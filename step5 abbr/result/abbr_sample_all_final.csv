before inlining happens remove unused code sees one use of inner c @$ which prevents its <PLACE_HOLDER> . after inlining it sees ` this instanceof inner c ` as the only use of inner c. make sure remove unused code recognizes that the value of inner c escapes .,test ( options @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,which prevents removal,success,obj,options <SEP> lines <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> lines <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str
use local <PLACE_HOLDER> for embedded spark <PLACE_HOLDER> when spark.master is not found,conf . set if missing ( __str__ @$ __str__ ) ; this . inner interpreter = load spark scala interpreter ( conf ) ; this . inner interpreter . open ( ) ; sc = this . inner interpreter . get spark context ( ) ; jsc = java spark context . from spark context ( sc ) ; spark version = spark version . from version string ( sc . version ( ) ) ; if ( enable supported version check && spark version . is unsupported version ( ) ) { throw new exception ( __str__ + spark version + __str__ + __str__ ) ; } sql context = this . inner interpreter . get sql context ( ) ; spark session = this . inner,mode spark context,fail,obj,conf <SEP> str <SEP> str <SEP> scala <SEP> conf <SEP> sc <SEP> sc <SEP> sc <SEP> supported <SEP> str <SEP> str <SEP> str <SEP> sql <SEP> sql
verify <PLACE_HOLDER> called or not,verify ( wal @$ expect append ? times ( __num__ ) : never ( ) ) . append data ( ( h region info ) any ( ) @$ ( wal key impl ) any ( ) @$ ( wal edit ) any ( ) ) ;,append called not,success,sub,wal <SEP> num <SEP> wal <SEP> impl <SEP> wal
use <PLACE_HOLDER> file attribute,use properties file = false ; properties file = __str__ ;,properties file attribute,success,sub,properties <SEP> properties <SEP> str
src inode and its subtree can not contain snapshottable <PLACE_HOLDER> with snapshots,fs dir snapshot op . check snapshot ( fsd @$ srciip @$ snapshottable dirs ) ;,inode contain directories,success,obj,fs <SEP> dir <SEP> fsd <SEP> srciip <SEP> snapshottable <SEP> dirs
the <PLACE_HOLDER> should have logging.properties,deque < property > result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; model node handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; string file name = null ;,address have logging.properties,success,sub,deque <SEP> deque <SEP> operations <SEP> str <SEP> last <SEP> contains <SEP> str <SEP> str <SEP> str <SEP> str <SEP> defined <SEP> defined <SEP> str
this statement can not be reached since the above method always throws an <PLACE_HOLDER> this is only here to silence the compiler and any warnings,return consumer records . empty ( ) ;,method throws exception,success,obj,records
throws obsolete version exception if another process has created a new <PLACE_HOLDER> already,store client . put ( new key . map value ( ) @$ new node ) ;,process created node,success,obj,None
param <PLACE_HOLDER> will not include 'this ',std type list param types = desc . get parameter types ( ) ; int sz param types = param types . size ( ) ;,"types include ""this""",success,sub,param <SEP> types <SEP> desc <SEP> types <SEP> int <SEP> sz <SEP> param <SEP> types <SEP> param <SEP> types
current <PLACE_HOLDER> already performs cleanup .,return ;,node performs cleanup,success,sub,None
test that the second parse build file <PLACE_HOLDER> repopulated the cache .,assert equals ( __str__ @$ __num__ @$ counter . calls ) ;,call repopulated cache,success,sub,equals <SEP> str <SEP> num <SEP> calls
check that slave satisfies min <PLACE_HOLDER> .,if ( cpus < cluster props . min cpu per node ( ) || mem < cluster props . min memory per node ( ) ) { log . log ( level . fine @$ __str__ @$ offer . get resources list ( ) ) ; return null ; } double total cpus = __num__ ; double total mem = __num__ ; double total disk = __num__ ;,slave satisfies requirements,success,obj,cpus <SEP> mem <SEP> str <SEP> resources <SEP> cpus <SEP> num <SEP> mem <SEP> num <SEP> num
if the receiver is not included in the contract @$ unfreeze frozen balance for this <PLACE_HOLDER> . otherwise @$ unfreeze delegated frozen balance provided this <PLACE_HOLDER> .,if ( ! array utils . is empty ( receiver address ) && dynamic store . supportdr ( ) ) { if ( arrays . equals ( receiver address @$ owner address ) ) { throw new contract validate exception ( __str__ ) ; } if ( ! decode util . address valid ( receiver address ) ) { throw new contract validate exception ( __str__ ) ; } account capsule receiver capsule = account store . get ( receiver address ) ; if ( dynamic store . get allow tvm constantinople ( ) == __num__ && receiver capsule == null ) { string readable receiver address = string util . create readable string ( receiver address ) ; throw new contract validate exception ( __str__ + readable,balance provided address,fail,obj,utils <SEP> supportdr <SEP> arrays <SEP> equals <SEP> str <SEP> util <SEP> str <SEP> tvm <SEP> constantinople <SEP> num <SEP> util <SEP> str
for every time when a user has not selected a <PLACE_HOLDER> but a basic block this breaks . as it does throw a null pointer exception .,first function . load ( ) ; second function . load ( ) ; final creation thread creation thread = new creation thread ( module @$ source block @$ target block @$ first function @$ second function ) ; progress dialog . show ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ @$ creation thread ) ; if ( ( ! ( creation thread . threw exception ( ) ) ) && ( creation thread . get created view ( ) == null ) ) { message box . show information ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ ) ; } else { new thread ( ) { @ override public void,user selected function,success,obj,second <SEP> second <SEP> dialog <SEP> plugin <SEP> str <SEP> created <SEP> plugin <SEP> str
collator.get keyword <PLACE_HOLDER> return the same contents for both commonly used true and false .,== all . length ) { match all = true ; for ( int j = __num__ ; j < pref . length ; j ++ ) { boolean found match = false ; for ( int k = __num__ ; k < all . length ; k ++ ) { if ( pref [ j ] . equals ( all [ k ] ) ) { found match = true ; break ; } } if ( ,key return contents,fail,sub,int <SEP> num <SEP> pref <SEP> int <SEP> num <SEP> pref <SEP> equals <SEP> errln <SEP> str <SEP> loc <SEP> str <SEP> arrays <SEP> str <SEP> arrays <SEP> pref
<PLACE_HOLDER> contains an object from another thread .,list . add ( obj from another thread . get ( ) ) ;,list contains object,success,sub,obj
the aws region provider chain that is used by default throws an <PLACE_HOLDER> instead of returning null when the region is not defined . for that reason @$ we have to support both throwing an <PLACE_HOLDER> and returning null as the region not being defined .,return null ;,chain throws exception,success,obj,None
if bolt has not finished <PLACE_HOLDER> or was not exactly once mode @$ just process the tuple immediately,if ( ! init || ( batch cache != null && ! batch cache . is exactly once mode ( ) ) ) { } else { pending batch batch = batch cache . get next pending batch ( last successful batch ) ; if ( batch != null ) { list < byte [ ] > pending msgs = batch . get tuples ( ) ; while ( pending msgs != null ) { for ( byte [ ] msg : pending msgs ) { receiver . deserialize tuple ( deserializer @$ msg @$ queue ) ; } pending msgs = batch . get tuples ( ) ; } } kryo input . set buffer ( data ) ; kryo input . set position ( __num__ ),bolt finished initialization,success,obj,init <SEP> last <SEP> msgs <SEP> tuples <SEP> msgs <SEP> msgs <SEP> deserialize <SEP> tuple <SEP> deserializer <SEP> msgs <SEP> tuples <SEP> kryo <SEP> kryo <SEP> num
this is tough for hotspot @$ but graal eliminates all <PLACE_HOLDER> .,consume ( el ) ;,graal eliminates calls,fail,obj,el
make sure the subject has a <PLACE_HOLDER>,assert false ( client principals . is empty ( ) ) ;,subject has principal,success,obj,principals
<PLACE_HOLDER> to the old behavior @$ allowing implementations to override .,map to map from id ( data @$ obj ) ;,objects allowing implementations,fail,sub,obj
the map reduce tokens are provided so that <PLACE_HOLDER> can spawn jobs if they wish to . the <PLACE_HOLDER> authenticate to the job tracker via the map reduce delegation tokens .,if ( system . getenv ( __str__ ) != null ) { conf . set ( __str__ @$ system . getenv ( __str__ ) ) ; } return conf ;,client spawn jobs,fail,sub,getenv <SEP> str <SEP> conf <SEP> str <SEP> getenv <SEP> str <SEP> conf
bad value prevents <PLACE_HOLDER> to next .,trigger action in cell editor ( key event . vk_tab ) ; assert is editing field ( __num__ @$ dt col num ) ;,value prevents move,success,obj,vktab <SEP> editing <SEP> num <SEP> num
turn the screen off . a black surface is already hiding the <PLACE_HOLDER> of the screen .,if ( m power state . get color fade level ( ) == __num__ ) { set screen state ( display . state_off ) ; m pending screen off = false ; m power state . dismiss color fade resources ( ) ; } else if ( perform screen off transition && m power state . prepare color fade ( m context @$ m color fade fades config ? color fade . mode_fade : color fade . mode_cool_down ) && m power state . get screen state ( ) != display . state_off ) { m color fade off animator . start ( ) ; } else { m color fade off animator . end ( ) ; },surface hiding area,fail,obj,num <SEP> stateoff <SEP> resources <SEP> fades <SEP> config <SEP> modefade <SEP> modecooldown <SEP> stateoff
get object can return <PLACE_HOLDER> if constraints were specified but not met,if ( s3 object == null ) return null ; output stream output stream = null ; try { output stream = new buffered output stream ( new file output stream ( destination file ) ) ; byte [ ] buffer = new byte [ __num__ * __num__ ] ; int bytes read ; while ( ( bytes read = s3 object . get object content ( ) . read ( buffer ) ) > - __num__ ) { output stream . write ( buffer @$ __num__ @$ bytes read ) ; } } catch ( io exception e ) { throw new sdk client exception ( __str__ + e . get message ( ) @$ e ) ; } finally { close quietly ( output stream @$,object return null,success,obj,s3 <SEP> buffered <SEP> num <SEP> num <SEP> int <SEP> bytes <SEP> bytes <SEP> s3 <SEP> content <SEP> num <SEP> num <SEP> bytes <SEP> io <SEP> sdk <SEP> str <SEP> close
<PLACE_HOLDER> means requery the adapter for this @$ it does n't have a valid width .,if ( ! lp . is decor && lp . height factor == __num__ ) { final item info ii = info for child ( child ) ; if ( ii != null ) { lp . height factor = ii . height factor ; lp . position = ii . position ; } },0 means adapter,success,sub,num <SEP> ii <SEP> ii <SEP> ii <SEP> ii
zero <PLACE_HOLDER> means a run,offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } while ( ip < ip bound ) { if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } } break ; } },address means run,fail,sub,ip <SEP> ip <SEP> ip <SEP> ip <SEP> ip <SEP> ip <SEP> ip
localized <PLACE_HOLDER> file name,args . filtered resources provider = new resources filter ( aapt target . with flavors ( internal flavor . of ( __str__ ) ) @$ filesystem @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ graph builder @$ immutable list . of ( resource1 . get res ( ) @$ resource2 . get res ( ) ) @$ immutable set . of ( ) @$ immutable set . of ( ) @$ null @$ resources filter . resource compression mode . disabled @$ filter resources steps . resource filter . empty_filter @$ optional . empty ( ) ) ;,string file name,success,sub,args <SEP> filtered <SEP> resources <SEP> resources <SEP> aapt <SEP> flavors <SEP> flavor <SEP> str <SEP> filesystem <SEP> resource1 <SEP> resource2 <SEP> resource1 <SEP> resource2 <SEP> resource1 <SEP> res <SEP> resource2 <SEP> res <SEP> resources <SEP> resources <SEP> steps <SEP> emptyfilter
clear references to <PLACE_HOLDER> @$ this should orphan the <PLACE_HOLDER> which should in turn trigger orphan removal logic .,team . set one vone player ( null ) ; team2 . set one vone player ( null ) ; tx . commit ( ) ; s . close ( ) ; s = open session ( ) ; tx = s . begin transaction ( ) ; count = ( ( long ) s . create query ( __str__ ) . iterate ( ) . next ( ) ) . int value ( ) ; assert equals ( __str__ + count @$ count @$ __num__ ) ; tx . commit ( ) ; s . close ( ) ;,references orphan players,success,obj,vone <SEP> team2 <SEP> vone <SEP> tx <SEP> close <SEP> tx <SEP> str <SEP> int <SEP> equals <SEP> str <SEP> num <SEP> tx <SEP> close
<PLACE_HOLDER> 1 sends a message to the bare jid of <PLACE_HOLDER> 0,chat chat = get connection ( __num__ ) . get chat manager ( ) . create chat ( get barejid ( __num__ ) @$ null ) ; chat . send message ( __str__ ) ; chat . send message ( __str__ ) ;,user sends message,success,sub,num <SEP> barejid <SEP> num <SEP> str <SEP> str
<PLACE_HOLDER> @$ for each set of paths that have a common input format @$ build a map of mappers to the paths they 're used for,for ( path path : paths ) { class < ? extends mapper > mapper class = mapper map . get ( path ) ; if ( ! mapper paths . contains key ( mapper class ) ) { mapper paths . put ( mapper class @$ new linked list < path > ( ) ) ; } mapper paths . get ( mapper class ) . add ( path ) ; },now build map,success,sub,paths <SEP> extends <SEP> mapper <SEP> mapper <SEP> mapper <SEP> mapper <SEP> paths <SEP> contains <SEP> mapper <SEP> mapper <SEP> paths <SEP> mapper <SEP> linked <SEP> mapper <SEP> paths <SEP> mapper
the text lines total height is larger than this view @$ snap <PLACE_HOLDER> to the top and bottom of the view .,if ( line1 height + line2 height > height ) { if ( child != m text line1 ) { vertical offset = height - line2 height ; } } else { vertical offset = ( height - line1 height - line2 height ) / __num__ ; if ( child == m text line2 ) { vertical offset += line1 height ; if ( m suggestion . has answer ( ) && m suggestion . get answer ( ) . get second line ( ) . has image ( ) ) { vertical offset += get resources ( ) . get dimension pixel offset ( r . dimen . omnibox_suggestion_answer_line2_vertical_spacing ) ; } } if ( line1 height != line2 height ) { vertical offset += ( line2,lines snap them,success,obj,line1 <SEP> line2 <SEP> line1 <SEP> line2 <SEP> line1 <SEP> line2 <SEP> num <SEP> line2 <SEP> line1 <SEP> second <SEP> resources <SEP> dimen <SEP> omniboxsuggestionanswerline2verticalspacing <SEP> line1 <SEP> line2 <SEP> line2
delete <PLACE_HOLDER> is complete before <PLACE_HOLDER> so not relevant @$ get next delete <PLACE_HOLDER>,delete range = delete range it . has next ( ) ? delete range it . next ( ) : null ; break ; case range1_completely_after_range2 :,range get range,success,sub,range1completelyafterrange2
using <PLACE_HOLDER> get an instance of document builder,from attribute ( el @$ __str__ ) ; throw new configuration changed exception ( __str__ + config version ) ; } } catch ( sax exception ex ) { logger . get logger ( notificationsxml response . class . get name ( ) ) . log ( level . severe @$ null @$ ex ) ; } catch ( io exception ex ) { logger . get logger ( notificationsxml response . class . get name ( ) ) . log ( level . severe @$ null @$ ex ) ; } catch ( parser configuration exception ex ) { logger . get logger ( notificationsxml response . class . get name ( ) ) . log ( level . severe @$ null @$ ex ) ; },factory get instance,success,sub,el <SEP> str <SEP> str <SEP> config <SEP> notificationsxml <SEP> io <SEP> notificationsxml <SEP> parser <SEP> notificationsxml
validate inode tree <PLACE_HOLDER> match given <PLACE_HOLDER> .,for ( mutable inode < ? > node : journaled ) { assert true ( tree entries . contains ( node . to journal entry ( ) ) ) ; },entries match entries,success,obj,inode <SEP> journaled <SEP> entries <SEP> contains
req.get remote host returns ip <PLACE_HOLDER> @$ try to resolve hostname to be consistent with raw protocol .,try { final inet address client address = inet address . get by name ( client host name ) ; client host name = client address . get host name ( ) ; } catch ( unknown host exception e ) { logger . info ( __str__ @$ client host name @$ e . get message ( ) ) ; },returns ip address,success,obj,inet <SEP> inet <SEP> str
in theory node name ca n't be null but better be careful who knows <PLACE_HOLDER> other implementations may be doing ? ...,if ( get node name ( ) == null ) { if ( arg . get node name ( ) != null ) { return false ; } } else if ( ! get node name ( ) . equals ( arg . get node name ( ) ) ) { return false ; } if ( get local name ( ) == null ) { if ( arg . get local name ( ) != null ) { return false ; } } else if ( ! get local name ( ) . equals ( arg . get local name ( ) ) ) { return false ; } if ( get namespaceuri ( ) == null ) { if ( arg . get namespaceuri ( ),implementations doing what,success,obj,arg <SEP> equals <SEP> arg <SEP> arg <SEP> equals <SEP> arg <SEP> namespaceuri <SEP> arg <SEP> namespaceuri
set timestamp before moving to <PLACE_HOLDER>root @$ so we can avoid race condition <PLACE_HOLDER> remove the file before setting timestamp,sum . equals ignore case ( checksum for ( cm path @$ fs ) ) ) { success = false ; } else { switch ( type ) { case move : { log . info ( __str__ @$ path . to string ( ) @$ cm path . to string ( ) ) ; success = fs . rename ( path @$ cm path ) ; break ; } case copy : { log . info ( __str__ @$ path . to string ( ) @$ cm path . to string ( ) ) ; success = file utils . copy ( fs @$ path @$ fs @$ cm path @$ false @$ true @$ conf ) ; break ; } default : break ; } },exception remove file,fail,sub,equals <SEP> fs <SEP> str <SEP> fs <SEP> str <SEP> utils <SEP> fs <SEP> fs <SEP> conf
export the data @$ <PLACE_HOLDER> causes a file chooser to be shown,execute on swing without blocking ( ( ) -> key binding utils . export key bindings ( options ) ) ; file selected file = find and test file chooser ( null @$ test_filename ) ; return selected file ;,which causes chooser,success,sub,blocking <SEP> utils <SEP> bindings <SEP> options <SEP> selected <SEP> testfilename <SEP> selected
<PLACE_HOLDER> should not impact result,current min = double min kudaf . aggregate ( null @$ current min ) ; assert that ( __num__ @$ equal to ( current min ) ) ;,null impact result,success,sub,kudaf <SEP> num
starting the job master should have read the <PLACE_HOLDER>,try { final completed checkpoint savepoint checkpoint = completed checkpoint store . get latest checkpoint ( false ) ; assert that ( savepoint checkpoint @$ matchers . not null value ( ) ) ; assert that ( savepoint checkpoint . get checkpointid ( ) @$ is ( savepoint id ) ) ; } finally { rpc utils . terminate rpc endpoint ( job master @$ testing timeout ) ; },master read savepoint,success,obj,completed <SEP> savepoint <SEP> completed <SEP> savepoint <SEP> matchers <SEP> savepoint <SEP> checkpointid <SEP> savepoint <SEP> rpc <SEP> utils <SEP> rpc <SEP> endpoint
verify that the archive rule has the correct <PLACE_HOLDER> : the object files from our sources .,rule . get native linkable input ( cxx platform @$ linker . linkable dep type . static @$ graph builder @$ unconfigured target configuration . instance ) ; build rule static rule = graph builder . get rule ( cxx description enhancer . create static library build target ( target @$ cxx platform . get flavor ( ) @$ pic type . pdc ) ) ; assert not null ( static rule ) ; assert equals ( immutable set . of ( cxx source rule factorypdc . create compile build target ( __str__ ) @$ cxx source rule factorypdc . create compile build target ( gen source name ) ) @$ static rule . get build deps ( ) . stream ( ) . map ( build rule,rule has deps,success,obj,linkable <SEP> cxx <SEP> linker <SEP> linkable <SEP> unconfigured <SEP> cxx <SEP> cxx <SEP> flavor <SEP> pdc <SEP> equals <SEP> cxx <SEP> factorypdc <SEP> str <SEP> cxx <SEP> factorypdc <SEP> deps
only the main task stack change notification requires a <PLACE_HOLDER> .,m handler . send message delayed ( msg @$ notify_task_stack_change_listeners_delay ) ;,notification requires delivery,fail,obj,delayed <SEP> notifytaskstackchangelistenersdelay
do abc tag <PLACE_HOLDER> does not have name,continue ;,abc tag plugin,fail,obj,None
our randomly generated <PLACE_HOLDER> significantly .,return name . length ( ) == __num__ ;,our generated name,fail,obj,num
no context class <PLACE_HOLDER> @$ try the current class <PLACE_HOLDER>,in = ss . get resource as stream ( cl @$ service ) ;,loader try loader,success,sub,None
this call does the right <PLACE_HOLDER> with a null transaction task queue,if ( ! m_complete msg . is restart ( ) ) { do commonspi complete actions ( ) ; log todr ( site connection . getdr gateway ( ) ) ; } else { m_txn state . set begin undo token ( site . k invalid undo token ) ; },call does thing,success,obj,mcomplete <SEP> commonspi <SEP> actions <SEP> todr <SEP> getdr <SEP> mtxn
add parameters from the configuration in the job trace the reason why the job configuration parameters @$ as seen in the jobconf file @$ are added first because the specialized <PLACE_HOLDER> obtained from rumen should override the job conf <PLACE_HOLDER> .,for ( map . entry < object @$ object > entry : job . get job properties ( ) . get value ( ) . entry set ( ) ) { job conf . set ( entry . get key ( ) . to string ( ) @$ entry . get value ( ) . to string ( ) ) ; },parameters override values,fail,sub,properties <SEP> conf
no more room to display the comments below ; do n't process <PLACE_HOLDER>,if ( total comments found > max display lines ) { return ; },comments process them,success,obj,comments <SEP> lines
let the hash builder <PLACE_HOLDER> reduce their accounted memory,partitions no longer needed . set ( null ) ; lock . write lock ( ) . lock ( ) ; try { arrays . fill ( partitions @$ null ) ; lookup source supplier = null ; close cached lookup sources ( ) ; } finally { lock . write lock ( ) . unlock ( ) ; },partitions reduce memory,fail,sub,partitions <SEP> longer <SEP> needed <SEP> arrays <SEP> partitions <SEP> close <SEP> cached <SEP> sources
illegal access error is expected note : <PLACE_HOLDER> stops context right after shutdown initiated . it is problematic to see log output system out could help,logger . warn ( __str__ + e . get message ( ) ) ;,user stops right,fail,sub,str
some binary files have <PLACE_HOLDER>,while ( c != __str__ ) { if ( c != __str__ ) sb . append ( c ) ; c = ( char ) buffer . get ( ) ; },files have newline,success,obj,str <SEP> str <SEP> sb
explicit <PLACE_HOLDER> provider implies <PLACE_HOLDER>,config . get ( key_authorizer ) . as string ( ) . if present ( value -> { if ( ! config . get ( key_authorize ) . exists ( ) ) { builder . authorize ( true ) ; } } ) ; return builder . build ( ) ;,provider implies atz,success,obj,config <SEP> keyauthorizer <SEP> config <SEP> keyauthorize <SEP> exists
bean may have acquired new weak <PLACE_HOLDER>,target = result . get node ( ) ; assert . assert equals ( count ++ @$ result . get value ( ) . int value ( ) ) ;,bean acquired affinity,success,obj,equals <SEP> int
<PLACE_HOLDER> must have the same type,if ( node1 . get class ( ) != node2 . get class ( ) ) { return false ; } string image1 = node1 . get image ( ) ; string image2 = node2 . get image ( ) ;,image have type,fail,sub,node1 <SEP> node2 <SEP> image1 <SEP> node1 <SEP> image2 <SEP> node2
iterate through all source <PLACE_HOLDER> to make sure we are generating a complete set of source folders for the source <PLACE_HOLDER> .,) . get parent ( ) ; if ( path elements . is empty ( ) ) { continue ; } while ( directory != null && directory . get file name ( ) != null && ! path elements . contains ( directory . get file name ( ) . to string ( ) ) ) { directory = directory . get parent ( ) ; } if ( directory == null || directory . get file name ( ) == null ) { continue ; } string directory path = directory . to string ( ) ; if ( ! directory path . ends with ( __str__ ) ) { directory path += __str__ ; } src folders . add ( directory path ) ; },paths generating set,success,sub,elements <SEP> elements <SEP> contains <SEP> ends <SEP> str <SEP> str <SEP> src <SEP> folders
quick exit : if the <PLACE_HOLDER> does not support encryption @$ we can exit early .,if ( ! is encryption supported ( ) ) { return device policy manager . encryption_status_unsupported ; },device support encryption,fail,sub,supported <SEP> encryptionstatusunsupported
make sure that the pattern matches a <PLACE_HOLDER> by one .,if ( ! b . is java constant ( ) ) { return null ; } java constant b cst = b . as java constant ( ) ; long b value ; if ( b cst . get java kind ( ) == java kind . int ) { b value = b cst . as int ( ) ; } else if ( b cst . get java kind ( ) == java kind . long ) { b value = b cst . as long ( ) ; } else { return null ; } if ( b value == - __num__ ) { return builder -> get arithmeticlir generator ( ) . emit get mask up to lowest set bit ( operand ( a,pattern matches subtraction,success,obj,int <SEP> int <SEP> num <SEP> arithmeticlir <SEP> lowest
some <PLACE_HOLDER> to not crash the chartbuilding with emtpy data,if ( m max elements == __num__ ) { m max elements = __num__ ; } if ( m mcount == __num__ ) { m mcount = __num__ ; } if ( m first element == m last element ) { m first element = __num__ ; m last element = __num__ ; } if ( m max cards == __num__ ) { m max cards = __num__ ; } return list . size ( ) > __num__ ;,adjustments crash chartbuilding,success,sub,elements <SEP> num <SEP> elements <SEP> num <SEP> mcount <SEP> num <SEP> mcount <SEP> num <SEP> last <SEP> num <SEP> last <SEP> num <SEP> cards <SEP> num <SEP> cards <SEP> num <SEP> num
1 st root <PLACE_HOLDER>,if ( parent == null ) { long next = ( ( long ) phase << phase_shift ) | adj ; if ( state . compare and set ( this @$ s @$ next ) ) break ; } else { main lock . lock ( ) ; try { if ( state == s ) { parent . do register ( __num__ ) ; do { phase = ( int ) ( root . state > > > phase_shift ) ; } while ( ! state . compare and set ( this @$ state @$ ( ( long ) phase << phase_shift ) | adj ) ) ; break ; } } finally { main lock . unlock ( ) ; } },st root registration,success,obj,phaseshift <SEP> adj <SEP> num <SEP> int <SEP> phaseshift <SEP> phaseshift <SEP> adj
bypass pe <PLACE_HOLDER> @$ but permit p es to complete ... valid docs wo n't care .,try { for ( ; ; ) { int c = getc ( ) ; if ( c == __str__ ) { c = getc ( ) ; if ( c != __str__ ) { if ( save comment text ) str tmp . append ( __str__ ) ; ungetc ( ) ; continue ; } next char ( __str__ @$ __str__ @$ null ) ; break one comment ; } if ( save comment text ) str tmp . append ( ( char ) c ) ; } } catch ( end of input exception e ) { if ( in . is internal ( ) ) { error ( __str__ @$ null ) ; } fatal ( __str__ ) ; },bypass pe tests,fail,obj,int <SEP> getc <SEP> str <SEP> getc <SEP> str <SEP> str <SEP> tmp <SEP> str <SEP> ungetc <SEP> str <SEP> str <SEP> str <SEP> tmp <SEP> str <SEP> str
setting volume on ui sounds stream <PLACE_HOLDER> also controls silent mode,if ( ( ( flags & audio manager . flag_allow_ringer_modes ) != __num__ ) || ( stream == get ui sounds stream type ( ) ) ) { set ringer mode ( get new ringer mode ( stream @$ index @$ flags ) @$ tag + __str__ @$ false ) ; },type controls mode,success,sub,flags <SEP> flagallowringermodes <SEP> num <SEP> sounds <SEP> flags <SEP> str
sequence <PLACE_HOLDER> compress writer,( fs @$ count @$ seed @$ block compressed file ) ; sort test ( fs @$ count @$ megabytes @$ factor @$ true @$ block compressed file ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; merge test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ false @$ factor @$ megabytes ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; merge test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ true @$ factor @$ megabytes ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; fs . close ( ) ;,block . file compress writer,fail,sub,fs <SEP> compressed <SEP> fs <SEP> megabytes <SEP> compressed <SEP> fs <SEP> compressed <SEP> fs <SEP> compressed <SEP> megabytes <SEP> fs <SEP> compressed <SEP> fs <SEP> compressed <SEP> megabytes <SEP> fs <SEP> compressed <SEP> fs <SEP> close
note that we can not do any size checking here @$ as the correct component count depends on the drawing step . gl should catch such <PLACE_HOLDER> then @$ and we will report them to the user .,if ( m values != null ) { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ __num__ ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m values ) ; } else { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ m vbo ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m offset ) ; },note catch exceptions,fail,obj,values <SEP> gles20 <SEP> gl <SEP> gles20 <SEP> glarraybuffer <SEP> num <SEP> gles20 <SEP> gl <SEP> attrib <SEP> components <SEP> values <SEP> gles20 <SEP> gl <SEP> gles20 <SEP> glarraybuffer <SEP> vbo <SEP> gles20 <SEP> gl <SEP> attrib <SEP> components
model name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( camelized name ) ) { final string model name = __str__ + camelized name ; logger . warn ( camelized name + __str__ + model name ) ; return model name ; },name use keyword,success,obj,camelized <SEP> str <SEP> camelized <SEP> camelized <SEP> str
copy local variables to event scope execution by value . this way @$ the event scope execution references a <PLACE_HOLDER> ' of the local variables,new sub process variable snapshotter ( ) . set variables snapshots ( sub process execution @$ event scope execution ) ;,execution references ' snapshot,fail,obj,process <SEP> snapshotter <SEP> variables <SEP> snapshots <SEP> process
the input method <PLACE_HOLDER> only throws security exceptions @$ so let 's log all others .,if ( ! ( e instanceof security exception ) ) { slog . wtf ( tag @$ __str__ @$ e ) ; } throw e ;,method throws exceptions,fail,sub,instanceof <SEP> str
<PLACE_HOLDER> account type,m ams . add account as user ( m mock account manager response @$ null @$ __str__ @$ null @$ true @$ null @$ user handle . user_system ) ;,response account type,success,sub,ams <SEP> str <SEP> usersystem
expression has n't nested <PLACE_HOLDER>,if ( tail == null ) { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( __str__ @$ offset @$ result ) ; } else { result . add ( new flat field descriptor ( offset @$ field type ) ) ; } } else { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( tail @$ offset @$ result ) ; } else { throw new invalid field reference exception ( __str__ + tail + __str__ + field type + __str__ ) ; } },expression nested field,success,obj,instanceof <SEP> fields <SEP> str <SEP> instanceof <SEP> fields <SEP> str <SEP> str <SEP> str
we need something that will measure the amount of time since our <PLACE_HOLDER> has seen a record ...,time since time since last record = threads . time since ( clock . system ) ;,thread seen record,fail,sub,last <SEP> threads
try to move focus to the next visible <PLACE_HOLDER> with a running activity if this <PLACE_HOLDER> is not covering the entire screen or is on a secondary display with no home <PLACE_HOLDER> .,if ( next focused stack != null ) { return m root activity container . resume focused stacks top activities ( next focused stack @$ prev @$ null ) ; },stack covering screen,success,sub,stacks <SEP> activities <SEP> prev
copy is necessary since the instance info builder just uses the original <PLACE_HOLDER> @$ and we do n't want to corrupt the global eureka copy of the object which may be used by other clients in our system,instance info copy = new instance info ( ii ) ; if ( is secure ) { ii = new instance info . builder ( copy ) . set secure port ( override port ) . build ( ) ; } else { ii = new instance info . builder ( copy ) . set port ( override port ) . build ( ) ; },builder uses one,fail,obj,ii <SEP> ii <SEP> ii
if declaration does not provide <PLACE_HOLDER> @$ there is no connection to make,if ( usage == null || declaration == null || declaration . length == __num__ ) return ; if ( usage . length != declaration . length ) return ;,declaration provide generics,success,obj,num
show that <PLACE_HOLDER> does work,do in hibernate ( this :: session factory @$ session -> { query query = session . create query ( __str__ @$ foo . class ) ; list < foo > list = query . get result list ( ) ; assert equals ( __num__ @$ list . size ( ) ) ; } ) ;,scan does work,fail,sub,str <SEP> foo <SEP> foo <SEP> equals <SEP> num
if event has this <PLACE_HOLDER> to filter on,if ( event dimension transformed . contains ( filter dimension transformed ) || filter dimension transformed . contains ( event dimension transformed ) ) { set < string > event dimension values set = new hash set < > ( event dimension values transformed ) ; event dimension values set . retain all ( filtered dimension values transformed ) ; if ( ! event dimension values set . is empty ( ) ) { filtered events . add ( event ) ; event added = true ; break ; } },event has dimension,success,obj,transformed <SEP> contains <SEP> transformed <SEP> transformed <SEP> contains <SEP> transformed <SEP> values <SEP> values <SEP> transformed <SEP> values <SEP> filtered <SEP> values <SEP> transformed <SEP> values <SEP> filtered <SEP> events <SEP> added
since hot swap <PLACE_HOLDER> run one file at a time @$ namespaces seen will not include any provides earlier than this current file .,if ( ! in hot swap && import type . must be ordered ( ) && ! namespaces seen . contains ( namespace ) ) { t . report ( call @$ late_provide_error @$ namespace ) ; },plugins run file,fail,sub,namespaces <SEP> contains <SEP> namespace <SEP> lateprovideerror <SEP> namespace
this test verifies the baseline <PLACE_HOLDER> used in subsequent tests . if this fails @$ the rest will fail .,project dependency graph graph = three projects depending ona single ( ) ; final list < maven project > sorted projects = graph . get sorted projects ( ) ; assert equals ( a project @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender1 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender2 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender3 @$ sorted projects . get ( __num__ ) ) ;,test verifies values,fail,obj,projects <SEP> depending <SEP> ona <SEP> projects <SEP> projects <SEP> equals <SEP> projects <SEP> num <SEP> equals <SEP> depender1 <SEP> projects <SEP> num <SEP> equals <SEP> depender2 <SEP> projects <SEP> num <SEP> equals <SEP> depender3 <SEP> projects <SEP> num
check:3 compare the key <PLACE_HOLDER>,for ( int i = __num__ ; i < prev count ; i ++ ) { value meta interface pre value = null ; for ( int j = __num__ ; j < rows . length ; j ++ ) { value meta interface v = rows [ j ] . search value meta ( key list . get ( j ) [ i ] ) ; if ( v == null ) { return false ; } if ( j != __num__ && v . get type ( ) != pre value . get type ( ) ) { log error ( __str__ ) ; return false ; } else { pre value = v ; } } },check:3 compare list,fail,obj,int <SEP> num <SEP> prev <SEP> int <SEP> num <SEP> rows <SEP> rows <SEP> num <SEP> str
backslash and double quote need double the <PLACE_HOLDER> for both java and haskell,special char replacements . remove ( __str__ ) ; special char replacements . remove ( __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ;,backslash double handling,fail,obj,replacements <SEP> str <SEP> replacements <SEP> str <SEP> replacements <SEP> str <SEP> str <SEP> replacements <SEP> str <SEP> str
record the sequence of seeks and reads which trigger a <PLACE_HOLDER> .,int [ ] seeks = new int [ __num__ ] ; int [ ] reads = new int [ __num__ ] ; try ( fs data input stream stm = get file system ( ) . open ( random seek file ) ) { for ( int i = __num__ ; i < limit ; i ++ ) { int seek off = r . next int ( buf . length ) ; int to read = r . next int ( math . min ( buf . length - seek off @$ __num__ ) ) ; seeks [ i % seeks . length ] = seek off ; reads [ i % reads . length ] = to read ; verify read ( stm @$ buf @$,which trigger split,fail,obj,int <SEP> seeks <SEP> int <SEP> num <SEP> int <SEP> reads <SEP> int <SEP> num <SEP> fs <SEP> stm <SEP> int <SEP> num <SEP> int <SEP> int <SEP> buf <SEP> int <SEP> int <SEP> buf <SEP> num <SEP> seeks <SEP> seeks <SEP> reads <SEP> reads <SEP> stm <SEP> buf
<PLACE_HOLDER> 3 items and ensure that <PLACE_HOLDER> pulls 2 from the database & 1 from the cache .,return of deleted entities ( true ) . multi load ( ids ( __num__ ) ) ; assert equals ( __num__ @$ entities . size ( ) ) ; simple entity deleted entity = entities . get ( __num__ ) ; assert not null ( deleted entity ) ; final entity entry entry = ( ( shared session contract implementor ) session ) . get persistence context ( ) . get entry ( deleted entity ) ; assert true ( entry . get status ( ) == status . deleted || entry . get status ( ) == status . gone ) ; assert true ( sql statement interceptor . get sql queries ( ) . get first ( ) . ends with ( __str__ ) ) ;,multiload pulls 2,success,sub,deleted <SEP> entities <SEP> ids <SEP> num <SEP> equals <SEP> num <SEP> entities <SEP> deleted <SEP> entities <SEP> num <SEP> deleted <SEP> shared <SEP> implementor <SEP> deleted <SEP> deleted <SEP> sql <SEP> sql <SEP> queries <SEP> ends <SEP> str
at normal playback speed @$ we stop <PLACE_HOLDER>ing when the <PLACE_HOLDER> reaches the minimum .,assert that ( load control . should continue loading ( min_buffer_us @$ speed ) ) . is false ( ) ;,load reaches minimum,fail,sub,minbufferus
test contents using contains element set <PLACE_HOLDER>,int size2 = __num__ ; int previous element = - __num__ ; for ( int element : set ) { assert true ( bs . get ( element ) ) ; size2 ++ ; assert true ( previous element < __num__ || ( ascending ? element - previous element > __num__ : element - previous element < __num__ ) ) ; previous element = element ; } assert equals ( size2 @$ size ) ;,contents contains iterator,success,obj,int <SEP> size2 <SEP> num <SEP> int <SEP> num <SEP> int <SEP> size2 <SEP> num <SEP> ascending <SEP> num <SEP> num <SEP> equals <SEP> size2
if the size of bloom filter is smaller than 1 mb @$ use <PLACE_HOLDER> max false positive probability,if ( num bits required <= max num bits ) { return default max false pos probability ; },value max probability,fail,sub,num <SEP> bits <SEP> required <SEP> num <SEP> bits <SEP> pos
transfer send some asset issue to default account @$ to test if this transaction use the creator <PLACE_HOLDER> .,assert . assert true ( public methed . transfer asset ( to address @$ asset account id . to byte array ( ) @$ __num__ @$ transfer asset address @$ transfer asset create key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ; asset creator net = public methed . get account net ( asset012 address @$ blocking stub full ) ; asset transfer net = public methed . get account net ( transfer asset address @$ blocking stub full ) ; long creator after net used = asset creator net . get net used ( ) ; long transfer after free net used = asset transfer net . get free net used ( ) ; logger,transaction use net,success,obj,methed <SEP> num <SEP> blocking <SEP> methed <SEP> blocking <SEP> methed <SEP> asset012 <SEP> blocking <SEP> methed <SEP> blocking <SEP> used <SEP> used <SEP> used <SEP> used
close <PLACE_HOLDER> 2 and pause so <PLACE_HOLDER> has a chance to close,close server ( server2 ) ; wait . pause ( __num__ * __num__ ) ; wait for cqs disconnected ( client @$ __str__ @$ __num__ ) ;,server has chance,success,sub,close <SEP> server2 <SEP> num <SEP> num <SEP> cqs <SEP> str <SEP> num
add view <PLACE_HOLDER> add view end,skeleton models . add ( new skeleton model builder ( ) . set start view ( btn3 ) . set end view ( btn4 ) . build ( ) ) ;,bottom add end,fail,sub,models <SEP> btn3 <SEP> btn4
<PLACE_HOLDER> write options to use in <PLACE_HOLDER> states .,write options write options = null ; linked hash map < string @$ rocksdb keyed state backend . rocks db kv state info > kv state information = new linked hash map < > ( ) ; rocksdb db = null ; abstract rocksdb restore operation restore operation = null ; rocks db ttl compact filters manager ttl compact filters manager = new rocks db ttl compact filters manager ( enable ttl compaction filter @$ ttl time provider ) ; resource guard rocksdb resource guard = new resource guard ( ) ; snapshot strategy < k > snapshot strategy ; priority queue set factory priority queue factory ; rocksdb serialized composite key builder < k > shared rocks key builder ;,the write options,success,sub,options <SEP> options <SEP> linked <SEP> rocksdb <SEP> keyed <SEP> backend <SEP> rocks <SEP> db <SEP> kv <SEP> kv <SEP> linked <SEP> rocksdb <SEP> db <SEP> rocksdb <SEP> rocks <SEP> db <SEP> ttl <SEP> filters <SEP> ttl <SEP> filters <SEP> rocks <SEP> db <SEP> ttl <SEP> filters <SEP> ttl <SEP> compaction <SEP> ttl <SEP> rocksdb <SEP> rocksdb <SEP> serialized <SEP> shared <SEP> rocks
ensure comments are stripped . <PLACE_HOLDER> cause syntax errors if not stripped .,final source file src file1 = source file . from code ( __str__ @$ line_joiner . join ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,these cause errors,success,sub,src <SEP> file1 <SEP> str <SEP> linejoiner <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str
if the incoming uri matches a single note <PLACE_HOLDER> @$ does the delete based on the incoming data @$ but modifies the where clause to restrict it to the particular note <PLACE_HOLDER> .,case main_id :,uri matches id,success,obj,mainid
pipeline should have ssl <PLACE_HOLDER> and server tls <PLACE_HOLDER>,iterator < map . entry < string @$ channel handler > > iterator = pipeline . iterator ( ) ; assert that ( iterator . next ( ) . get value ( ) ) . is instance of ( ssl handler . class ) ;,pipeline have handler,success,obj,iterator <SEP> iterator <SEP> iterator <SEP> iterator <SEP> ssl
unknown host exception happens if we ca n't resolve host<PLACE_HOLDER> into ip address . unknown host exception 's get message method returns just the host<PLACE_HOLDER> which is a useless message @$ so log the exception class <PLACE_HOLDER> to provide more info .,log . debug ( e . to string ( ) ) ; throw new helios exception ( __str__ + uri @$ e ) ; throw new helios exception ( __str__ + uri @$ e ) ;,hostname log name,success,obj,helios <SEP> str <SEP> helios <SEP> str
let 's not store 'raw ' <PLACE_HOLDER> but nodes,if ( value == null ) { value = null node ( ) ; },s store nulls,success,obj,None
get new display metrics based on the display adjustments given to the resources impl . update a <PLACE_HOLDER> if the compatibility info changed @$ because the resources impl object will handle the update internally .,display adjustments daj = r . get display adjustments ( ) ; if ( compat != null ) { daj = new display adjustments ( daj ) ; daj . set compatibility info ( compat ) ; } dm = get display metrics ( display id @$ daj ) ; if ( ! is default display ) { apply non default display metrics to configuration ( dm @$ tmp config ) ; } if ( has override configuration ) { tmp config . update from ( key . m override configuration ) ; } r . update configuration ( tmp config @$ dm @$ compat ) ; r . update configuration ( config @$ dm @$ compat ) ;,metrics update configuration,fail,obj,adjustments <SEP> daj <SEP> adjustments <SEP> compat <SEP> daj <SEP> adjustments <SEP> daj <SEP> daj <SEP> compat <SEP> metrics <SEP> daj <SEP> metrics <SEP> tmp <SEP> config <SEP> tmp <SEP> config <SEP> tmp <SEP> config <SEP> compat <SEP> config <SEP> compat
while visit the <PLACE_HOLDER> build elements @$ the probe element is null @$ and the <PLACE_HOLDER> build iterator would iterate all the <PLACE_HOLDER> build elements @$ so we return false during the second calling of this method .,this . unmatched build visited = true ; return true ;,unmatched build elements,success,sub,visited
the <PLACE_HOLDER> should contain just 1 character :,assert equals ( __num__ @$ straus . get len ( ) ) ; har file system . close ( ) ; local fs . delete ( tmp path @$ true ) ;,directory contain character,fail,sub,equals <SEP> num <SEP> straus <SEP> har <SEP> close <SEP> fs <SEP> tmp
start the two trigger <PLACE_HOLDER> . they will wait at the barrier,t1 . start ( ) ; t2 . start ( ) ;,two trigger threads,success,obj,t1 <SEP> t2
create calendar which omits <PLACE_HOLDER>,for ( action . handler action handler : action handlers ) { gregorian calendar cal = new gregorian calendar ( get time zone ( ) @$ get locale ( ) ) ; cal . clear ( ) ; cal . set ( current calendar . get ( java . util . calendar . year ) @$ current calendar . get ( java . util . calendar . month ) @$ current calendar . get ( java . util . calendar . date ) ) ; date start = cal . get time ( ) ; cal . add ( java . util . calendar . date @$ __num__ ) ; cal . add ( java . util . calendar . second @$ - __num__ ) ; date end,which omits keys,fail,obj,handlers <SEP> gregorian <SEP> gregorian <SEP> util <SEP> util <SEP> util <SEP> util <SEP> num <SEP> util <SEP> second <SEP> num
a filter with a start date of now should not accept the <PLACE_HOLDER>,filter = new log filter ( level . info @$ local date time . now ( ) @$ null ) ; assert that ( filter . accepts file ( path ) ) . is false ( ) ;,filter accept file,success,obj,accepts
method reports <PLACE_HOLDER> in nanoseconds across all processors .,cpu time /= __num__ * os . get available processors ( ) ; double cpu = __num__ ; if ( prev cpu time > __num__ ) { long cpu time diff = cpu time - prev cpu time ; cpu = math . min ( __num__ @$ ( double ) cpu time diff / metrics_update_freq ) ; },method reports cpu,fail,obj,num <SEP> processors <SEP> num <SEP> prev <SEP> num <SEP> diff <SEP> prev <SEP> num <SEP> diff <SEP> metricsupdatefreq
log a warning if the <PLACE_HOLDER> uses an unnecessary service error definition annotation,if ( service error def annotation != null && ! resource model . is any service error list defined ( ) ) { log . warn ( string . format ( __str__ + __str__ + __str__ @$ resource class . get name ( ) @$ service error def . class . get simple name ( ) @$ service errors . class . get simple name ( ) @$ param error . class . get simple name ( ) ) ) ; },user uses annotation,fail,sub,defined <SEP> str <SEP> str <SEP> str <SEP> errors <SEP> param
required <PLACE_HOLDER> expect activity launch options in,fail ( __str__ ) ;,features expect options,success,sub,str
2 range conditions are used on different columns @$ but not all sql <PLACE_HOLDER> properly optimize it . some <PLACE_HOLDER> can only use an index on one of the columns . an additional condition provides explicit knowledge that 'start ' can not be greater than 'end ' .,start ( ) . to string ( ) ) . bind ( __str__ @$ interval . get end ( ) . to string ( ) ) . map ( byte array mapper . first ) . fold ( new array list < > ( ) @$ new folder3 < list < data segment > @$ byte [ ] > ( ) { @ override public list < data segment > fold ( list < data segment > accumulator @$ byte [ ] payload @$ fold controller fold controller @$ statement context statement context ) { accumulator . add ( jackson utils . read value ( json mapper @$ payload @$ data segment . class ) ) ; return accumulator ; } } ) ; } } ) ;,databases use index,success,sub,str <SEP> mapper <SEP> folder3 <SEP> utils <SEP> json <SEP> mapper
these credentials must match <PLACE_HOLDER> in the default password file,string [ ] credentials = new string [ ] { __str__ @$ __str__ } ; cli_env . put ( __str__ @$ credentials ) ; jmxc = jmx connector factory . connect ( url @$ cli_env ) ; m bean server connection mbsc = jmxc . getm bean server connection ( ) ;,credentials match those,success,obj,str <SEP> str <SEP> clienv <SEP> str <SEP> jmxc <SEP> jmx <SEP> clienv <SEP> mbsc <SEP> jmxc <SEP> getm
update <PLACE_HOLDER> should not update any caches since they are both unregistered,m network score service . update scores ( new scored network [ ] { scored_network } ) ;,scores update caches,success,sub,scores <SEP> scored <SEP> scorednetwork
first copy the children as the call to copy.add will modify the <PLACE_HOLDER> we 're iterating on,enumeration < ? > enum from = node . children ( ) ; list < j meter tree node > tmp = new array list < > ( ) ; while ( enum from . has more elements ( ) ) { j meter tree node child = ( j meter tree node ) enum from . next element ( ) ; tmp . add ( child ) ; } for ( j meter tree node j meter tree node : tmp ) { copy . add ( j meter tree node ) ; } tree model . insert node into ( copy @$ target @$ index ++ ) ; nodes for removal . add ( node ) ; paths to select [ path position ++ ] =,call modify order,fail,obj,enum <SEP> children <SEP> tmp <SEP> enum <SEP> elements <SEP> enum <SEP> tmp <SEP> tmp <SEP> nodes <SEP> paths
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default remote profile = new crawl profile ( crawl_profile_remote @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ null @$ - __num__ @$ true @$ true @$ true @$ false @$ true @$ true @$ false @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . iffresh @$ __str__ + crawl_profile_remote @$ client identification . yacy internet crawler agent name @$ null @$ null @$ __num__ ) ;,ip match crawler,success,obj,crawlprofileremote <SEP> matchallstring <SEP> matchneverstring <SEP> matchallstring <SEP> matchneverstring <SEP> matchneverstring <SEP> matchneverstring <SEP> matchallstring <SEP> matchneverstring <SEP> matchallstring <SEP> matchneverstring <SEP> num <SEP> num <SEP> num <SEP> matchneverstring <SEP> iffresh <SEP> str <SEP> crawlprofileremote <SEP> yacy <SEP> num
evaluate any karate <PLACE_HOLDER> even on lhs will throw exception if variable does not exist,actual = eval karate expression ( expression @$ context ) ; if ( actual . is json like ( ) ) { path = var_root ; },expression throw exception,success,sub,eval <SEP> json <SEP> varroot
this needs to do a binary search @$ but a binary search is somewhat tough because the sequence <PLACE_HOLDER> involves two nodes .,int size = vec . size ( ) @$ i ; for ( i = size - __num__ ; i >= __num__ ; i -- ) { int child = vec . element at ( i ) ; if ( child == node ) { i = - __num__ ; break ; } dtm dtm = m_dtm mgr . getdtm ( node ) ; if ( ! dtm . is node after ( node @$ child ) ) { break ; } },test involves nodes,success,sub,int <SEP> vec <SEP> num <SEP> num <SEP> int <SEP> vec <SEP> num <SEP> dtm <SEP> dtm <SEP> mdtm <SEP> getdtm <SEP> dtm
box drawings light horizontal @$ so box drawings light <PLACE_HOLDER> @$ so box drawings light down and right @$ so box drawings light down and left @$ so box drawings light up and left @$ so box drawings light up and right @$ so box drawings light <PLACE_HOLDER> and right @$ so box drawings light down and horizontal @$ so box drawings light <PLACE_HOLDER>,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,drawings light vertical,success,obj,str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str
<PLACE_HOLDER> ls exception .,throw lse ; throw ( ls exception ) createls exception ( ls exception . serialize_err @$ e ) . fill in stack trace ( ) ; if ( fdom error handler != null ) { fdom error handler . handle error ( new dom error impl ( dom error . severity_fatal_error @$ e . get message ( ) @$ null @$ e ) ) ; } throw ( ls exception ) createls exception ( ls exception . serialize_err @$ e ) . fill in stack trace ( ) ;,rethrow ls exception,success,sub,ls <SEP> createls <SEP> ls <SEP> serializeerr <SEP> fdom <SEP> fdom <SEP> impl <SEP> severityfatalerror <SEP> ls <SEP> createls <SEP> ls <SEP> serializeerr
if we have <PLACE_HOLDER> combine their transforms,while ( parent node != null ) { if ( parent == current spatial ) { transform trans = new transform ( ) ; trans . set scale ( current spatial . get local scale ( ) ) ; shape transform . combine with parent ( trans ) ; parent node = null ; } else { shape transform . combine with parent ( current spatial . get local transform ( ) ) ; parent node = current spatial . get parent ( ) ; current spatial = parent node ; } },them combine transforms,fail,sub,None
<PLACE_HOLDER> added global foo,check symbol ( symbols [ __num__ ] @$ __str__ @$ true ) ;,both added foo,success,sub,symbols <SEP> num <SEP> str
create a table with a primary key named 'name ' @$ <PLACE_HOLDER> holds a string,new condition ( ) . with comparison operator ( comparison operator . gt . to string ( ) ) . with attribute value list ( new attribute value ( ) . withn ( __str__ ) ) ; scan filter . put ( __str__ @$ condition ) ; scan request scan request = new scan request ( tablename ) . with scan filter ( scan filter ) ; scan result scan result = dynamodb client . scan ( scan request ) ; logger . info ( __str__ + scan result ) ; } catch ( amazon service exception ase ) { logger . error ( __str__ + ase ) ; } catch ( amazon client exception ace ) { logger . error ( __str__ + ace ) ; },which holds string,success,sub,withn <SEP> str <SEP> str <SEP> tablename <SEP> dynamodb <SEP> str <SEP> ase <SEP> str <SEP> ase <SEP> str
most <PLACE_HOLDER> use the object number for p 2 @$ nut not all,area ) config . get device ( ) ; int mode = area . get mode for string ( command . to string ( ) ) ; if ( mode >= __num__ ) { cmd = omni link cmd . cmd_security_omni_disarm . get number ( ) + mode ; param1 = __num__ ; commands . add ( new omni link controller command ( cmd @$ param1 @$ param2 ) ) ; } } } break ; case button : { if ( command instanceof string type ) { cmd = omni link cmd . cmd_button . get number ( ) ; commands . add ( new omni link controller command ( cmd @$ param1 @$ param2 ) ) ; } } break ; default : break ; },commands use number,success,sub,config <SEP> int <SEP> num <SEP> cmd <SEP> cmd <SEP> cmdsecurityomnidisarm <SEP> param1 <SEP> num <SEP> commands <SEP> cmd <SEP> param1 <SEP> param2 <SEP> instanceof <SEP> cmd <SEP> cmd <SEP> cmdbutton <SEP> commands <SEP> cmd <SEP> param1 <SEP> param2
check that above <PLACE_HOLDER> did not change internal state of the policy qualifier info instance,assert true ( arrays . equals ( encoding @$ encoding ret1 ) ) ;,modification change state,success,sub,arrays <SEP> equals <SEP> encoding <SEP> encoding <SEP> ret1
the list of relational values should contain 2 or more values : the first represents the <PLACE_HOLDER> the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ jaxb any mapping . get name ( ) ) @$ origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source type source = new hibernate type source impl ( jaxb any mapping . get meta type ( ) ) ; private final relational value source relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > value mappings = new hash map < string @$ string > ( ) ; { for ( jaxb hbm any value,first represents discriminator,success,obj,sources <SEP> num <SEP> str <SEP> jaxb <SEP> discriminator <SEP> discriminator <SEP> impl <SEP> jaxb <SEP> sources <SEP> num <SEP> mappings <SEP> jaxb <SEP> hbm
in this case first call to end <PLACE_HOLDER> returns correct value @$ but a second thread has updated the source topic but since it 's a source topic @$ the second check should not fire hence no exception,consumer . add end offsets ( collections . singleton map ( topic partition @$ __num__ ) ) ; changelog reader . register ( new state restorer ( topic partition @$ restore listener @$ null @$ __num__ @$ true @$ __str__ @$ identity ( ) ) ) ; expect ( active . restoring task for ( topic partition ) ) . and return ( task ) ; replay ( active ) ; changelog reader . restore ( active ) ;,offsets returns value,success,sub,offsets <SEP> collections <SEP> num <SEP> changelog <SEP> num <SEP> str <SEP> restoring <SEP> changelog
metadata <PLACE_HOLDER> have no entropy,assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( entropy_marker ) ) ) ; assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( resolved_marker ) ) ) ;,streams have entropy,fail,sub,contains <SEP> entropymarker <SEP> contains <SEP> resolvedmarker
change resolutions and check that the selected date is not lost and that the <PLACE_HOLDER> has the correct resolution .,click ( resolution hour ) ; check header and body ( date time resolution . hour @$ false ) ; click ( resolution year ) ; check header and body ( date time resolution . year @$ false ) ; click ( resolution minute ) ; check header and body ( date time resolution . minute @$ false ) ;,day has resolution,fail,sub,minute <SEP> minute
<PLACE_HOLDER> map local work,local work = mrwork . get map red local work ( ) ; exec context . set local work ( local work ) ; mapred context . init ( true @$ new job conf ( jc ) ) ; mo . pass exec context ( exec context ) ; mo . initialize local work ( jc ) ; mo . initialize map operator ( jc ) ; if ( local work == null ) { return ; },initialize map work,success,sub,mrwork <SEP> mapred <SEP> init <SEP> conf <SEP> jc <SEP> jc <SEP> jc
the processor does n't taint the consumer <PLACE_HOLDER> which has already finished,span processor span = take span ( consumer spans ) ; assert that ( processor span . id ( ) ) . is not equal to ( consumer span . id ( ) ) ;,processor taint span,success,obj,spans
<PLACE_HOLDER> asked build interruption @$ let stop the build before trying to run another build step,if ( executor != null && executor . is interrupted ( ) ) { throw new interrupted exception ( ) ; },someone build interruption,success,sub,interrupted <SEP> interrupted
check that the memory <PLACE_HOLDER> got all segments back,if ( ! this . memman . verify empty ( ) ) { assert . fail ( __str__ ) ; } this . memman . shutdown ( ) ; this . memman = null ;,manager got segments,success,sub,memman<SEP> memman<SEP> memman
<PLACE_HOLDER> expected initial result .,int [ ] results cnt = new int [ ] { __num__ @$ __num__ @$ __num__ } ;,clients expected result,success,sub,cnt
dispatching to empty <PLACE_HOLDER> will not call back visitor @$ must call our visit empty <PLACE_HOLDER> explicitly,if ( finally statement instanceof empty statement ) { visit empty statement ( ( empty statement ) finally statement ) ; } else { finally statement . visit ( this ) ; },statement call visitor,success,sub,instanceof
<PLACE_HOLDER> does n't throw interrupted exception @$ but may return some bytes geq 0 or throw an exception,while ( ! thread . current thread ( ) . is interrupted ( ) || closed ) { if ( closed ) { throw new io exception ( __str__ ) ; } },connection throw exception,success,sub,interrupted <SEP> io <SEP> str
updates the port when the <PLACE_HOLDER> changes the security type . this allows us to show a reasonable default which the <PLACE_HOLDER> can change .,m security type view . set on item selected listener ( new adapter view . on item selected listener ( ) { @ override public void on item selected ( adapter view < ? > parent @$ view view @$ int position @$ long id ) { if ( m current security type view position != position ) { update port from security type ( ) ; validate fields ( ) ; } } @ override public void on nothing selected ( adapter view < ? > parent ) { } } ) ;,user changes type,success,sub,selected <SEP> selected <SEP> selected <SEP> int <SEP> fields <SEP> selected
we capture and set the context once the <PLACE_HOLDER> provided observable emits,assert true ( results . is context initialized observe on . get ( ) ) ;,user provided emits,success,sub,results <SEP> initialized
do n't let the <PLACE_HOLDER> intercept our events .,src . get parent ( ) . request disallow intercept touch event ( true ) ; return true ;,parent intercept events,success,sub,src
the final thread that should n't execute releases the latch once it has run so it is deterministic that the other two fill the thread <PLACE_HOLDER> until this one rejects,if ( ! should execute ) { success latch . count down ( ) ; },two fill pool,success,obj,None
required features expect activity launch <PLACE_HOLDER> in,fail ( __str__ ) ;,features expect options,success,obj,str
if this mapping has a valid file <PLACE_HOLDER> then we close it,if ( fd . valid ( ) ) { try { nd . close ( fd ) ; } catch ( io exception ignore ) { } },mapping has descriptor,success,obj,fd <SEP> nd <SEP> close <SEP> fd <SEP> io
if we do n't have an mx record @$ try the machine <PLACE_HOLDER>,if ( ( attr == null ) || ( attr . size ( ) == __num__ ) ) { attrs = ictx . get attributes ( host name @$ new string [ ] { __str__ } ) ; attr = attrs . get ( __str__ ) ; if ( attr == null ) { throw new naming exception ( base messages . get string ( pkg @$ __str__ @$ host name ) ) ; } },attributes try machine,fail,sub,attr <SEP> attr <SEP> num <SEP> attrs <SEP> ictx <SEP> attributes <SEP> str <SEP> attr <SEP> attrs <SEP> str <SEP> attr <SEP> naming <SEP> messages <SEP> str
not sure which thread gets the <PLACE_HOLDER> first so we add them to a map and verify that some thread had 4 threads waiting @$ 3 threads @$ etc .,assert that ( map . size ( ) @$ equal to ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ;,thread gets keys,fail,obj,num <SEP> num <SEP> num <SEP> num <SEP> num <SEP> num
for any <PLACE_HOLDER>s in the controller callback @$ test browser callback should have overridden the <PLACE_HOLDER> and call matching api in the callback proxy .,for ( int i = __num__ ; i < methods . length ; i ++ ) { assert not equals ( __str__ + methods [ i ] + __str__ @$ browser callback . class @$ methods [ i ] . get declaring class ( ) ) ; assert not equals ( __str__ + methods [ i ] + __str__ @$ controller callback . class @$ methods [ i ] . get declaring class ( ) ) ; },callback overridden method,success,obj,int <SEP> num <SEP> methods <SEP> equals <SEP> str <SEP> methods <SEP> str <SEP> methods <SEP> declaring <SEP> equals <SEP> str <SEP> methods <SEP> str <SEP> methods <SEP> declaring
crawler <PLACE_HOLDER> must match crawler <PLACE_HOLDER> must not match crawler ip must match crawler ip must not match crawler country must match crawler no depth limit match index <PLACE_HOLDER> must match index <PLACE_HOLDER> must not match index content must match index content must not match,profile = new crawl profile ( crawl_profile_snippet_global_text @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_snippet_global_text_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ true @$ false @$ true @$ true @$ true @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . ifexist @$ __str__ + crawl_profile_snippet_global_text @$ client identification . yacy intranet crawler agent name @$ null @$ null @$ __num__ ) ;,url match crawler,success,sub,crawlprofilesnippetglobaltext <SEP> matchallstring <SEP> matchneverstring <SEP> matchallstring <SEP> matchneverstring <SEP> matchneverstring <SEP> matchneverstring <SEP> matchallstring <SEP> matchneverstring <SEP> matchallstring <SEP> matchneverstring <SEP> num <SEP> recrawl <SEP> crawlprofilesnippetglobaltextrecrawlcycle <SEP> num <SEP> num <SEP> matchneverstring <SEP> ifexist <SEP> str <SEP> crawlprofilesnippetglobaltext <SEP> yacy <SEP> num
create a walker <PLACE_HOLDER> walks the tree in a dfs manner while maintaining the operator stack . the dispatcher generates the plan from the operator tree,put ( hive parser . tok_interval_day_literal @$ tf . get interval expr processor ( ) ) ; ast node to processor . put ( hive parser . tok_interval_hour_literal @$ tf . get interval expr processor ( ) ) ; ast node to processor . put ( hive parser . tok_interval_minute_literal @$ tf . get interval expr processor ( ) ) ; ast node to processor . put ( hive parser . tok_interval_second_literal @$ tf . get interval expr processor ( ) ) ; ast node to processor . put ( hive parser . tok_table_or_col @$ tf . get column expr processor ( ) ) ; ast node to processor . put ( hive parser . tok_subquery_expr @$ tf . get sub query expr processor ( ) ) ;,which walks tree,success,sub,parser <SEP> tokintervaldayliteral <SEP> tf <SEP> expr <SEP> parser <SEP> tokintervalhourliteral <SEP> tf <SEP> expr <SEP> parser <SEP> tokintervalminuteliteral <SEP> tf <SEP> expr <SEP> parser <SEP> tokintervalsecondliteral <SEP> tf <SEP> expr <SEP> parser <SEP> toktableorcol <SEP> tf <SEP> expr <SEP> parser <SEP> toksubqueryexpr <SEP> tf <SEP> expr
if a component specifies the file with a bad font @$ the corresponding <PLACE_HOLDER> will be initialized by default physical font . in such case find font 2 d may return composite font which can not be casted to physical font .,if ( ! component names [ slot ] . equals ignore case ( name ) ) { try { components [ slot ] = ( physical font ) fm . find font2d ( component names [ slot ] @$ style @$ font manager . physical_fallback ) ; } catch ( class cast exception cce ) { components [ slot ] = fm . get default physical font ( ) ; } },component find font,fail,sub,names <SEP> equals <SEP> components <SEP> font2d <SEP> names <SEP> physicalfallback <SEP> cce <SEP> components
how many header <PLACE_HOLDER> ?,int argnr = rep . count nr job entry attributes ( id_jobentry @$ __str__ ) ; allocate ( argnr ) ; for ( int a = __num__ ; a < argnr ; a ++ ) { header name [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; header value [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; },many header name,success,obj,int <SEP> argnr <SEP> attributes <SEP> idjobentry <SEP> str <SEP> argnr <SEP> int <SEP> num <SEP> argnr <SEP> idjobentry <SEP> str <SEP> idjobentry <SEP> str
because entries <PLACE_HOLDER> is empty @$ remove and decrement value,if ( entries . is empty ( ) ) { synchronized ( entries ) { if ( entries . is empty ( ) ) { if ( value to entries map . remove ( new key @$ entries ) ) { num index keys . decrement and get ( ) ; internal index stats . inc num keys ( - __num__ ) ; } } } },map empty value,fail,sub,entries <SEP> synchronized <SEP> entries <SEP> entries <SEP> entries <SEP> entries <SEP> num <SEP> keys <SEP> decrement <SEP> num <SEP> keys <SEP> num
assume properties contain <PLACE_HOLDER>,if ( property value == null ) { if ( require property ) { throw new illegal argument exception ( __str__ + property key + __str__ + property key + __str__ ) ; } else { return null ; } } property value = get slashy path ( property value ) ; property value = correct double slash ( property value @$ property index end @$ str ) ; result += property value ; property index end ++ ; property index start = property index end ;,properties contain paths,success,obj,str <SEP> str <SEP> str <SEP> slashy <SEP> str
each <PLACE_HOLDER> will have a unique consumer group id . in this case we have two queries and 3 consumers . so we should expect two results from the current consumption rate by <PLACE_HOLDER> call .,assert equals ( __num__ @$ consumption by query . size ( ) ) ;,query have group,success,sub,equals <SEP> num
the second line has a ' <PLACE_HOLDER> ' @$ so it needs more ascent and descent .,if ( m enabled ) { assert equals ( - __num__ * em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; } else { assert equals ( - em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; },line has b,success,obj,equals <SEP> num <SEP> num <SEP> equals <SEP> num <SEP> num <SEP> equals <SEP> num <SEP> equals <SEP> num <SEP> num
<PLACE_HOLDER> on parent would have precedence,attribute conversion info conversion = locate attribute conversion info ( property name ) ; if ( conversion != null ) { return conversion ; } return null ;,conversion have precedence,fail,sub,None
loading child elements modifies the <PLACE_HOLDER> of the attribute set 's underlying parser @$ so it needs to happen after obtaining attributes and extracting <PLACE_HOLDER>s .,if ( dr == null ) { int type ; while ( ( type = parser . next ( ) ) == xml pull parser . text ) { } if ( type != xml pull parser . start_tag ) { throw new xml pull parser exception ( parser . get position description ( ) + item_missing_drawable_error ) ; } if ( parser . get name ( ) . equals ( __str__ ) ) { dr = vector drawable compat . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else if ( sdk_int >= lollipop ) { dr = drawable . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else { dr = drawable,elements modifies state,success,obj,int <SEP> parser <SEP> parser <SEP> parser <SEP> starttag <SEP> parser <SEP> parser <SEP> itemmissingdrawableerror <SEP> parser <SEP> equals <SEP> str <SEP> drawable <SEP> compat <SEP> resources <SEP> parser <SEP> attrs <SEP> sdkint <SEP> drawable <SEP> resources <SEP> parser <SEP> attrs <SEP> drawable
whether device <PLACE_HOLDER> enforces camera restriction .,boolean disallow camera globally = false ; if ( is device owner ) { final active admin device owner = get device owner admin locked ( ) ; if ( device owner == null ) { return ; } user restrictions = device owner . user restrictions ; disallow camera globally = device owner . disable camera ; } else { final active admin profile owner = get profile owner admin locked ( user id ) ; user restrictions = profile owner != null ? profile owner . user restrictions : null ; },owner enforces restriction,success,sub,locked <SEP> restrictions <SEP> restrictions <SEP> locked <SEP> restrictions <SEP> restrictions
<PLACE_HOLDER> what cloud we think we are in . publish our health as well .,udp heartbeat . build_and_multicast ( cloud @$ hb ) ;,test publish health,fail,sub,udp <SEP> buildandmulticast <SEP> hb
extension registry may be either extension registry or extension registry lite . since the type we are parsing is a full message @$ only a full extension registry could possibly contain <PLACE_HOLDER> of it . otherwise we will treat the registry as if it were empty .,if ( type . is extension number ( field number ) ) { if ( extension registry instanceof extension registry ) { final extension registry . extension info extension = target . find extension by number ( ( extension registry ) extension registry @$ type @$ field number ) ; if ( extension == null ) { field = null ; } else { field = extension . descriptor ; default instance = extension . default instance ; if ( default instance == null && field . get java type ( ) == descriptors . field descriptor . java type . message ) { throw new illegal state exception ( __str__ + field . get full name ( ) ) ; } } } else { field =,registry contain extensions,success,obj,instanceof <SEP> descriptors <SEP> str
and that local <PLACE_HOLDER> always takes preference over remote <PLACE_HOLDER> .,) ) ; differencer . inject ( metadata to inject ) ; evaluation context evaluation context = evaluation context . new builder ( ) . set keep going ( false ) . set num threads ( __num__ ) . set event hander ( null event handler . instance ) . build ( ) ; assert that ( driver . evaluate ( immutable list . of ( action key1 @$ action key2 ) @$ evaluation context ) . has error ( ) ) . is false ( ) ; assert that ( new filesystem value checker ( null @$ null ) . get dirty action values ( evaluator . get values ( ) @$ null @$ modified file set . everything_modified ) ) . is empty ( ) ;,storage takes preference,fail,sub,differencer <SEP> num <SEP> threads <SEP> num <SEP> hander <SEP> key1 <SEP> key2 <SEP> filesystem <SEP> values <SEP> evaluator <SEP> values <SEP> modified <SEP> everythingmodified
this dead <PLACE_HOLDER> hushes warnings .,return ;,code hushes warnings,success,sub,None
no <PLACE_HOLDER> left so close the actual server the done handler needs to be executed on the context that calls close @$ not the context of the actual server,actual server . actual close ( context @$ completion ) ;,error left server,fail,sub,close
all locks clients should be stopped at this point @$ and all all locks should be released because none of the clients entered the prepare <PLACE_HOLDER>,lock count visitor lock count visitor = new lock count visitor ( ) ; locks . accept ( lock count visitor ) ; assert equals ( __num__ @$ lock count visitor . get lock count ( ) ) ;,none entered phase,success,obj,locks <SEP> equals <SEP> num
<PLACE_HOLDER> uses total duration ms locked @$ while total uses total time locked,dump timer ( proto @$ uid proto . sync . total @$ timer @$ raw realtime us @$ which ) ; dump timer ( proto @$ uid proto . sync . background @$ bg timer @$ raw realtime us @$ which ) ; proto . end ( sy token ) ;,background uses time,success,sub,uid <SEP> realtime <SEP> uid <SEP> bg <SEP> realtime <SEP> sy
not seen yet ; must add an entry @$ return it . for that @$ we need <PLACE_HOLDER>,object id generator < ? > generator = null ; if ( _object id generators == null ) { _object id generators = new array list < object id generator < ? > > ( __num__ ) ; } else { for ( int i = __num__ @$ len = _object id generators . size ( ) ; i < len ; ++ i ) { object id generator < ? > gen = _object id generators . get ( i ) ; if ( gen . can use for ( generator type ) ) { generator = gen ; break ; } } } if ( generator == null ) { generator = generator type . new for serialization ( this ) ; _object id generators .,not need none,fail,obj,generators <SEP> generators <SEP> num <SEP> int <SEP> num <SEP> generators <SEP> generators <SEP> can <SEP> generators
more complicated . vh 2 has a higher version @$ but has some <PLACE_HOLDER> that vh 1 does not have .,region version holder vh1 = new region version holder ( member ) ; region version holder vh2 = new region version holder ( member ) ; bit set bs1 = new bit set ( ) ; bs1 . set ( __num__ @$ __num__ ) ; bs1 . set ( __num__ @$ __num__ ) ; record versions ( vh1 @$ bs1 ) ; bit set bs2 = new bit set ( ) ; bs2 . set ( __num__ @$ __num__ ) ; bs2 . set ( __num__ @$ __num__ ) ; record versions ( vh2 @$ bs2 ) ;,vh has exceptions,success,obj,vh1 <SEP> vh2 <SEP> bs1 <SEP> bs1 <SEP> num <SEP> num <SEP> bs1 <SEP> num <SEP> num <SEP> versions <SEP> vh1 <SEP> bs1 <SEP> bs2 <SEP> bs2 <SEP> num <SEP> num <SEP> bs2 <SEP> num <SEP> num <SEP> versions <SEP> vh2 <SEP> bs2
assertion on auto onboard another dummy data <PLACE_HOLDER>,metadata source config another dummymd source = ds to onboards map . get ( __str__ ) . get ( __num__ ) . get metadata source config ( ) ; assert . assert equals ( another dummymd source . get class name ( ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . size ( ) @$ __num__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __num__ ) ;,assertion onboard source,success,obj,config <SEP> dummymd <SEP> ds <SEP> onboards <SEP> str <SEP> num <SEP> config <SEP> equals <SEP> dummymd <SEP> str <SEP> equals <SEP> dummymd <SEP> properties <SEP> num <SEP> equals <SEP> dummymd <SEP> properties <SEP> str <SEP> str <SEP> equals <SEP> dummymd <SEP> properties <SEP> str <SEP> num
if the scope has no <PLACE_HOLDER> try to remove it,if ( ! ( ( basic scope ) broadcast scope ) . has event listeners ( ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ ) ; } scope . remove child scope ( broadcast scope ) ; } log . debug ( __str__ @$ name ) ;,scope has listeners,success,obj,listeners <SEP> str <SEP> str
if the final number of <PLACE_HOLDER>s to scroll ends up being 0 @$ the view should still scroll at least one <PLACE_HOLDER> .,return capped scroll step != __num__ ? capped scroll step : direction ;,view scroll step,fail,obj,capped <SEP> num <SEP> capped
the keys of probe and build <PLACE_HOLDER> are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer join works well in this case .,final int probe vals per key = __num__ ;,keys build sides,success,obj,int <SEP> vals <SEP> num
we capture and set the context once the <PLACE_HOLDER> provided observable emits,assert true ( results . is context initialized observe on . get ( ) ) ;,user provided emits,success,sub,results <SEP> initialized
start <PLACE_HOLDER> root element,not empty ( prefix ) ) xtw . write namespace ( prefix @$ model . get namespaces ( ) . get ( prefix ) ) ; } xtw . write attribute ( type_language_attribute @$ schema_namespace ) ; xtw . write attribute ( expression_language_attribute @$ xpath_namespace ) ; if ( string utils . is not empty ( model . get target namespace ( ) ) ) { xtw . write attribute ( target_namespace_attribute @$ model . get target namespace ( ) ) ; } else { xtw . write attribute ( target_namespace_attribute @$ process_namespace ) ; } bpmnxml util . write custom attributes ( model . get definitions attributes ( ) . values ( ) @$ xtw @$ model . get namespaces ( ) @$ default attributes ) ;,definitions root element,success,sub,xtw <SEP> namespace <SEP> namespaces <SEP> xtw <SEP> typelanguageattribute <SEP> schemanamespace <SEP> xtw <SEP> expressionlanguageattribute <SEP> xpathnamespace <SEP> utils <SEP> namespace <SEP> xtw <SEP> targetnamespaceattribute <SEP> namespace <SEP> xtw <SEP> targetnamespaceattribute <SEP> processnamespace <SEP> bpmnxml <SEP> util <SEP> attributes <SEP> definitions <SEP> attributes <SEP> values <SEP> xtw <SEP> namespaces <SEP> attributes
this <PLACE_HOLDER> has a bad acl @$ so we are dismissing it early .,if ( err != keeper exception . code . ok . int value ( ) ) { dec in process ( ) ; reply header rh = new reply header ( request . cxid @$ __num__ @$ err ) ; try { request . cnxn . send response ( rh @$ null @$ null ) ; } catch ( io exception e ) { log . error ( __str__ @$ e ) ; } },request has acl,success,sub,int <SEP> process <SEP> rh <SEP> cxid <SEP> num <SEP> cnxn <SEP> rh <SEP> io <SEP> str
special <PLACE_HOLDER> : if there were no files to measure @$ use the containing j scroll pane 's width,if ( d . width == __num__ && get parent ( ) != null ) { if ( get parent ( ) . get parent ( ) instanceof j scroll pane ) { j scroll pane parent = ( j scroll pane ) get parent ( ) . get parent ( ) ; dimension parent size = parent . get size ( ) ; insets insets = parent . get insets ( ) ; d . width = parent size . width - ( insets != null ? insets . right + insets . left : __num__ ) ; } } else { d . width += default_icon_size + width_padding ; },case use width,success,sub,num <SEP> instanceof <SEP> insets <SEP> insets <SEP> insets <SEP> insets <SEP> insets <SEP> insets <SEP> num <SEP> defaulticonsize <SEP> widthpadding
equal class with one maybe a primitive @$ the later explicit cast arguments will solve this <PLACE_HOLDER>,continue ;,class solve case,success,obj,None
the finally clause will send an <PLACE_HOLDER> .,remove decoder ( imgd ) ; if ( thread . current thread ( ) . is interrupted ( ) || ! thread . current thread ( ) . is alive ( ) ) { error all consumers ( imgd . queue @$ true ) ; } else { error all consumers ( imgd . queue @$ false ) ; },clause send error,success,obj,imgd <SEP> interrupted <SEP> consumers <SEP> imgd <SEP> consumers <SEP> imgd
0 x 1002346 : p 1 repeatable <PLACE_HOLDER> contains p 2 repeatable <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ;,comment contains comment,success,sub,builder1 <SEP> str <SEP> str <SEP> repeatablecomment <SEP> builder2 <SEP> str <SEP> str <SEP> repeatablecomment
we wo n't compare the candidate display name against the current item . this is to prevent an validation warning if the <PLACE_HOLDER> sets the display name to what the existing display name is,if ( item . get name ( ) . equals ( current job name ) ) { continue ; } else if ( display name . equals ( item . get display name ( ) ) ) { return false ; },user sets name,success,sub,equals <SEP> equals
<PLACE_HOLDER> set image drawable internally,super . set image bitmap ( bitmap ) ;,constructor set drawable,fail,sub,None
components commonly have conditional <PLACE_HOLDER>s assigned . using the click <PLACE_HOLDER> matcher we can assert whether or not a given component has a <PLACE_HOLDER> attached to them . noinspection unchecked,assert that ( c @$ component ) . extracting sub component at ( __num__ ) . has ( sub component with ( c @$ test footer component . matcher ( c ) . click handler ( is null . < event handler < click event > > null value ( null ) ) . build ( ) ) ) ;,component has event,fail,obj,extracting <SEP> num <SEP> matcher
report all volumes as unmounted until we 've recorded that user 0 has <PLACE_HOLDER> . there are no guarantees that callers will see a consistent view of the volume before that point,final boolean system user unlocked = is system unlocked ( user handle . user_system ) ; final boolean user key unlocked ; final boolean storage permission ; final long token = binder . clear calling identity ( ) ; try { user key unlocked = is user key unlocked ( user id ) ; storage permission = m storage manager internal . has external storage ( uid @$ package name ) ; } finally { binder . restore calling identity ( token ) ; } boolean found primary = false ; final array list < storage volume > res = new array list < > ( ) ; synchronized ( m lock ) { for ( int i = __num__ ; i < m volumes . size (,user has access,fail,obj,usersystem <SEP> uid <SEP> res <SEP> synchronized <SEP> int <SEP> num <SEP> volumes
the <PLACE_HOLDER> can specify the hadoop memory,map < string @$ string > variables = new hash map < string @$ string > ( system . getenv ( ) ) ;,environment specify memory,fail,sub,variables <SEP> getenv
since the calling <PLACE_HOLDER> will push the type again @$ we better remove it here,os . remove ( __num__ ) ;,code push type,success,sub,num
as the annotation is legal on fields and methods only @$ javac <PLACE_HOLDER> will take care of printing an error message for this .,return ;,processing take care,fail,sub,None
<PLACE_HOLDER> @$ like all package groups @$ does n't have a configuration,) { package group package group = ( package group ) target ; target context target context = new target context ( analysis environment @$ target @$ config @$ prerequisite map . get ( dependency resolver . visibility_dependency ) @$ visibility ) ; return new package group configured target ( target context @$ package group ) ; } else if ( target instanceof environment group ) { target context target context = new target context ( analysis environment @$ target @$ config @$ immutable set . of ( ) @$ visibility ) ; return new environment group configured target ( target context ) ; } else { throw new assertion error ( __str__ + target . get class ( ) . get name ( ) ) ; },looks have configuration,fail,sub,config <SEP> resolver <SEP> visibilitydependency <SEP> configured <SEP> instanceof <SEP> config <SEP> configured <SEP> str
x attr exists @$ and replace it using <PLACE_HOLDER> replace flag .,fs . setx attr ( path @$ name1 @$ value1 @$ enum set . of ( x attr set flag . create ) ) ; fs . setx attr ( path @$ name1 @$ new value1 @$ enum set . of ( x attr set flag . create @$ x attr set flag . replace ) ) ; xattrs = fs . getx attrs ( path ) ; assert . assert equals ( xattrs . size ( ) @$ __num__ ) ; assert . assert array equals ( new value1 @$ xattrs . get ( name1 ) ) ; fs . removex attr ( path @$ name1 ) ;,attrs replace flag,fail,sub,fs <SEP> setx <SEP> attr <SEP> name1 <SEP> value1 <SEP> enum <SEP> attr <SEP> fs <SEP> setx <SEP> attr <SEP> name1 <SEP> value1 <SEP> enum <SEP> attr <SEP> attr <SEP> xattrs <SEP> fs <SEP> getx <SEP> attrs <SEP> equals <SEP> xattrs <SEP> num <SEP> equals <SEP> value1 <SEP> xattrs <SEP> name1 <SEP> fs <SEP> removex <SEP> attr <SEP> name1
user should not see anything so give unsatisfiable <PLACE_HOLDER>,return process payload builder . process definitions ( ) . with process definition key ( __str__ + uuid . randomuuid ( ) . to string ( ) ) . build ( ) ;,anything give condition,success,obj,process <SEP> process <SEP> definitions <SEP> process <SEP> str <SEP> uuid <SEP> randomuuid
synchronously end the animation @$ jumping to the end state . animator <PLACE_HOLDER> has synchronous listener behavior on all supported ap is .,if ( ! animated ) { current animation . end ( ) ; },animation has behavior,fail,sub,None
the <PLACE_HOLDER> will require scrolling to get to all the items . extend the height so that part of the hidden items is displayed .,if ( actual size < m overflow panel . get count ( ) ) { extension = ( int ) ( m line height * __num__ ) ; },overflow extend height,success,sub,int <SEP> num
relative paths have no <PLACE_HOLDER> .,return new gcs path ( fs @$ __str__ @$ component ) ;,paths have symbols,fail,obj,gcs <SEP> fs <SEP> str
verify exit <PLACE_HOLDER> matches exit state of script,assert . assert equals ( exit code @$ container status . get exit status ( ) ) ;,code matches state,fail,sub,equals
then : '' collected must remove <PLACE_HOLDER> '',assert that ( tap . block ( ) ) . contains exactly ( __num__ @$ __num__ @$ __num__ ) ;,collected remove duplicates,success,obj,contains <SEP> num <SEP> num <SEP> num
fire should not purge <PLACE_HOLDER>,assert equals ( __num__ @$ test harness . num keyed state entries ( ) ) ;,fire purge contents,success,obj,equals <SEP> num <SEP> num <SEP> keyed <SEP> entries
assert the package tracker triggered an <PLACE_HOLDER> .,check update check triggered ( new package versions ) ; check token token2 = m fake intent helper . capture and reset last token ( ) ;,tracker triggered update,success,obj,triggered <SEP> versions <SEP> token2 <SEP> last
lcn type need connection <PLACE_HOLDER>,dtx local context . make proxy ( ) ;,type need proxy,success,obj,dtx
output node local <PLACE_HOLDER>,local properties lp = p . get local properties ( ) ; writer . print ( __str__ ) ; if ( lp . get ordering ( ) != null ) { add property ( writer @$ __str__ @$ lp . get ordering ( ) . to string ( ) @$ true ) ; } else { add property ( writer @$ __str__ @$ __str__ @$ true ) ; } if ( lp . get grouped fields ( ) != null && lp . get grouped fields ( ) . size ( ) > __num__ ) { add property ( writer @$ __str__ @$ lp . get grouped fields ( ) . to string ( ) @$ false ) ; } else { add property ( writer @$ __str__,output node properties,success,obj,properties <SEP> properties <SEP> str <SEP> str <SEP> str <SEP> str <SEP> grouped <SEP> fields <SEP> grouped <SEP> fields <SEP> num <SEP> str <SEP> grouped <SEP> fields <SEP> str
synchronize on scanner read <PLACE_HOLDER>s so that nobody calculates get smallest read <PLACE_HOLDER> @$ before scanner read <PLACE_HOLDER>s is updated .,isolation level isolation level = scan . get isolation level ( ) ; long mvcc read point = package private field accessor . get mvcc read point ( scan ) ; synchronized ( scanner read points ) { if ( mvcc read point > __num__ ) { this . read pt = mvcc read point ; } else if ( nonce == h constants . no_nonce || rs services == null || rs services . get nonce manager ( ) == null ) { this . read pt = get read point ( isolation level ) ; } else { this . read pt = rs services . get nonce manager ( ) . get mvcc from operation context ( nonce group @$ nonce ) ; } scanner,synchronize read point,success,obj,mvcc <SEP> accessor <SEP> mvcc <SEP> synchronized <SEP> points <SEP> mvcc <SEP> num <SEP> mvcc <SEP> constants <SEP> nononce <SEP> rs <SEP> services <SEP> rs <SEP> services <SEP> rs <SEP> services <SEP> mvcc
group the <PLACE_HOLDER> into those that use indexes and those that do n't @$ so we can evaluate all the <PLACE_HOLDER> that do n't use indexes together in one iteration first get filter <PLACE_HOLDER>,filter filter operands = null ;,iteration get operands,success,obj,operands
wait until the connection used by res 1 is returned to the pool @$ so that the next <PLACE_HOLDER> reuses the connection .,while ( ! connection returned to pool ) { condition . await ( ) ; } lock . unlock ( ) ;,callback reuses connection,fail,sub,returned
user already specified schema <PLACE_HOLDER>,if ( vendor extensions . contains key ( codegen_vendor_extension_key ) ) { logger . info ( __str__ + base name + __str__ ) ; return ; },user specified values,success,obj,extensions <SEP> contains <SEP> codegenvendorextensionkey <SEP> str <SEP> str
verify that mock provider 2 contains a new <PLACE_HOLDER> named the same as the parent meta <PLACE_HOLDER> of the contact we just moved,contact group new grpp2 = mcl slick fixture . mock pres op setp2 . get server stored contact list root ( ) . get group ( mcl slick fixture . metap1 grp1 . get group name ( ) ) ; assert not null ( __str__ + mcl slick fixture . emilp2 . get display name ( ) + __str__ @$ new grpp2 ) ;,group named same,success,sub,grpp2 <SEP> mcl <SEP> pres <SEP> setp2 <SEP> stored <SEP> mcl <SEP> metap1 <SEP> grp1 <SEP> str <SEP> mcl <SEP> emilp2 <SEP> str <SEP> grpp2
the test passed @$ so just <PLACE_HOLDER> from main and harness will interepret this <PLACE_HOLDER> as a pass,return ;,return interepret return,success,sub,None
javax mail incorrectly adds the <PLACE_HOLDER> for the first boundary to the end of the preamble @$ so we trim,assert . assert equals ( javax mail multi partmime reader . _preamble != null ? javax mail multi partmime reader . _preamble . trim ( ) : null @$ expected preamble ) ;,mail adds crlf,success,obj,equals <SEP> javax <SEP> partmime <SEP> javax <SEP> partmime
all other operands require a <PLACE_HOLDER>,return register priority . must have register ;,operands require register,success,obj,None
the response object does n't contain any relevant <PLACE_HOLDER> so we have to create a copy of values being sent over the network in case m jp settings is modified while awaiting response,final jetpack settings model sent jp data = new jetpack settings model ( m jp settings ) ; ++ m save request count ; word press . get rest client utilsv1_1 ( ) . set jetpack settings ( m site . get site id ( ) @$ params @$ new rest request . listener ( ) { @ override public void on response ( json object response ) { app log . d ( app log . t . api @$ __str__ ) ; m remote jp settings . monitor active = sent jp data . monitor active ; m remote jp settings . jetpack protect enabled = sent jp data . jetpack protect enabled ; m remote jp settings . jetpack protect whitelist . clear ( ),object contain information,fail,obj,settings <SEP> settings <SEP> settings <SEP> utilsv11 <SEP> settings <SEP> params <SEP> json <SEP> str <SEP> settings <SEP> settings <SEP> settings <SEP> whitelist
override the reporter with our own <PLACE_HOLDER> collates the allocation sites .,close guard . set reporter ( new reporter ( ) { @ override public void report ( string message @$ throwable allocation site ) { close guard allocation sites . add ( allocation site ) ; } } ) ;,which collates sites,success,sub,close <SEP> throwable <SEP> close <SEP> sites
check if no <PLACE_HOLDER> has focus,view view = get current focus ( ) ; if ( view != null ) { input method manager input manager = ( input method manager ) get system service ( context . input_method_service ) ; if ( input manager != null ) input manager . hide soft input from window ( view . get window token ( ) @$ input method manager . hide_not_always ) ; },view has focus,success,sub,inputmethodservice <SEP> hidenotalways
for binary set the response op <PLACE_HOLDER>,if ( this . protocol == protocol . binary ) { reply . rewind ( ) ; reply . put ( position_opcode @$ buffer . get ( position_opcode ) ) ; reply . put int ( position_opaque @$ buffer . get int ( position_opaque ) ) ; if ( connection handler . get logger ( ) . finer enabled ( ) ) { connection handler . get logger ( ) . finer ( __str__ + reply + __str__ + command . bufferto string ( reply ) ) ; } } socket channel channel = this . socket . get channel ( ) ; if ( channel == null || ! channel . is open ( ) ) { throw new illegal state exception ( __str__ ) ; },binary set code,success,obj,positionopcode <SEP> positionopcode <SEP> int <SEP> positionopaque <SEP> int <SEP> positionopaque <SEP> finer <SEP> finer <SEP> str <SEP> str <SEP> bufferto <SEP> str
should use root entity <PLACE_HOLDER> by default,criteria executor criteria executor = new criteria executor ( ) { protected criteria get criteria ( session s ) { return s . create criteria ( enrolment . class @$ __str__ ) . create alias ( __str__ @$ __str__ @$ criteria . left_join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set projection ( projections . projection list ( ) . add ( projections . property ( __str__ ) ) . add ( projections . property ( __str__ ) ) ) . add order ( order . asc ( __str__ ) ) ; } } ;,use root transformer,success,obj,criteria <SEP> criteria <SEP> criteria <SEP> protected <SEP> criteria <SEP> criteria <SEP> criteria <SEP> str <SEP> str <SEP> str <SEP> criteria <SEP> leftjoin <SEP> str <SEP> str <SEP> projections <SEP> projections <SEP> str <SEP> projections <SEP> str <SEP> asc <SEP> str
<PLACE_HOLDER> can add internal system window,return create window ( parent @$ type @$ token @$ name @$ owner id @$ false ) ;,owner add window,success,sub,None
the first two <PLACE_HOLDER> have the same priority,final task task = noop task . create ( math . min ( __num__ @$ ( i - __num__ ) * __num__ ) ) ;,tasks have priority,success,sub,noop <SEP> num <SEP> num <SEP> num
make sure the master has <PLACE_HOLDER> of the reports,waiter . wait for ( test_util . get configuration ( ) @$ __num__ * __num__ @$ new predicate < exception > ( ) { @ override public boolean evaluate ( ) throws exception { map < region info @$ long > region sizes = quota manager . snapshot region sizes ( ) ; log . trace ( __str__ + region sizes ) ; return num regions == count regions for table ( tn @$ region sizes ) && table size <= get table size ( tn @$ region sizes ) ; } } ) ; map < table name @$ long > sizes = test_util . get admin ( ) . get space quota table sizes ( ) ; long size = sizes . get ( tn ),master has all,success,obj,testutil <SEP> num <SEP> num <SEP> throws <SEP> sizes <SEP> sizes <SEP> str <SEP> sizes <SEP> num <SEP> regions <SEP> regions <SEP> sizes <SEP> sizes <SEP> sizes <SEP> testutil <SEP> sizes <SEP> sizes
test once using the current correct hash function @$ expect no mispartitioned <PLACE_HOLDER>,client response cr = client . call procedure ( __str__ @$ ( object ) null ) ; volt table hashinator matches = cr . get results ( ) [ __num__ ] ; hashinator matches . advance row ( ) ; while ( hashinator matches . advance row ( ) ) { assert equals ( __num__ @$ hashinator matches . get long ( __str__ ) ) ; } volt table validate result = cr . get results ( ) [ __num__ ] ;,test expect rows,success,obj,cr <SEP> str <SEP> hashinator <SEP> matches <SEP> cr <SEP> results <SEP> num <SEP> hashinator <SEP> matches <SEP> row <SEP> hashinator <SEP> matches <SEP> row <SEP> equals <SEP> num <SEP> hashinator <SEP> matches <SEP> str <SEP> cr <SEP> results <SEP> num
make a copy of the graph to avoid concurrency problems . graph manipulations are not thread safe @$ and another thread can concurrently inline this <PLACE_HOLDER> .,final structured graph graph = ( structured graph ) method . compilation info . get graph ( ) . copy ( debug ) ; try ( debug context . scope s = debug . scope ( __str__ @$ graph @$ method @$ this ) ) { try { try ( indent in = debug . log and indent ( __str__ @$ method ) ) { boolean inlined = false ; for ( invoke invoke : graph . get invokes ( ) ) { if ( invoke instanceof invoke node ) { throw vm error . should not reach here ( __str__ + invoke . call target ( ) . target method ( ) . format ( __str__ ) + __str__ + ( graph . method ( ) ==,thread inline method,success,obj,structured <SEP> structured <SEP> str <SEP> str <SEP> inlined <SEP> invokes <SEP> instanceof <SEP> vm <SEP> str <SEP> str <SEP> str
return null since neither dm nor dls are shutting down can not call <PLACE_HOLDER> in progress because it 's abstract,return null ;,null call anything,fail,obj,None
null <PLACE_HOLDER> does n't actually retain the session,assert false ( cache . contains ( __str__ ) ) ;,properties retain session,fail,sub,contains <SEP> str
<PLACE_HOLDER> does n't recover any more the recovered <PLACE_HOLDER> should not call retain assignment @$ as it is not a clean startup .,assert false ( __str__ @$ mock load balancer . retain assign called ) ;,master recover master,success,obj,str <SEP> balancer <SEP> called
make sure the new <PLACE_HOLDER> has the same level as the old one,new comment . level = this . get ( index ) . level ; this . set ( index @$ new comment ) ; return true ;,comment has level,success,sub,None
queue c <PLACE_HOLDER> has sa and aq @$ both from parent,assert true ( c111 . has access ( queueacl . administer_queue @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . administer_queue @$ __str__ ) ) ; assert true ( c111 . has access ( queueacl . submit_applications @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . submit_applications @$ __str__ ) ) ; reset ( c ) ;,11 has sa,fail,sub,c111 <SEP> queueacl <SEP> administerqueue <SEP> queueacl <SEP> acl <SEP> infos <SEP> queueacl <SEP> administerqueue <SEP> str <SEP> c111 <SEP> queueacl <SEP> submitapplications <SEP> queueacl <SEP> acl <SEP> infos <SEP> queueacl <SEP> submitapplications <SEP> str
will be null when the feature to use minimized <PLACE_HOLDER> is disabled .,if ( minimized bitcode == null ) { return full bitcode ; } return minimized bitcode ;,feature use bitmap,fail,obj,minimized <SEP> bitcode <SEP> bitcode <SEP> minimized <SEP> bitcode
push twice will fail and temp dir <PLACE_HOLDER>,file out dir = new file ( string utils . format ( __str__ @$ config . get storage directory ( ) @$ segment path ) ) ; out dir . set read only ( ) ; try { pusher . push ( segment dir @$ segments [ i ] @$ false ) ; } catch ( io exception e ) { assert . fail ( __str__ ) ; },push fail cleaned,success,obj,dir <SEP> utils <SEP> str <SEP> config <SEP> dir <SEP> dir <SEP> segments <SEP> io <SEP> str
seek to the end just before the last page of stream to get the <PLACE_HOLDER> .,long last page search position = end position - ogg page header . max_page_size ; if ( last page search position > position before seek to end ) { return last page search position ; },page get duration,success,obj,last <SEP> ogg <SEP> maxpagesize <SEP> last <SEP> last
registers a callback to be invoked whenever a <PLACE_HOLDER> changes a preference .,get preference screen ( ) . get shared preferences ( ) . register on shared preference change listener ( this ) ;,user changes preference,success,sub,shared <SEP> preferences <SEP> shared
now reptable has 2000 <PLACE_HOLDER> and parttable has 252 <PLACE_HOLDER>,thread . sleep ( __num__ ) ; save tables with default nonce and path ( client ) ; wait for snapshot to finish ( client ) ;,reptable has rows,success,obj,num <SEP> tables
<PLACE_HOLDER> should remove visibility .,assert true ( m accounts db . delete de account ( acc id ) ) ;,display remove visibility,fail,sub,accounts <SEP> db <SEP> acc
make sure the <PLACE_HOLDER> outlives the gc,if ( ( flags & ( call_type_callback | call_type_global_value | call_type_struct_member ) ) > __num__ ) { o . retain ( ) ; },object outlives gc,success,sub,flags <SEP> calltypecallback <SEP> calltypeglobalvalue <SEP> calltypestructmember <SEP> num
<PLACE_HOLDER> only represent configuration details of the parent @$ and are not independent entities,return false ;,nodes represent details,fail,sub,None
object has default meta <PLACE_HOLDER> @$ so we need to replace it on demand,object = obj ;,object has class,success,obj,obj
the <PLACE_HOLDER> of relational values should contain 2 or more values : the first represents the discriminator the rest represent the fk,( jaxb many to any mapping . get id type ( ) ) ; private final list < relational value source > fk relational value sources = relational value sources . sub list ( __num__ @$ relational value sources . size ( ) ) ; @ override public hibernate type source get type source ( ) { return fk type source ; } @ override public list < relational value source > get relational value sources ( ) { return fk relational value sources ; } @ override public attribute path get attribute path ( ) { return plural attribute source . get attribute path ( ) ; } @ override public metadata building context get building context ( ) { return mapping document ; } } ;,list contain values,success,sub,jaxb <SEP> fk <SEP> sources <SEP> sources <SEP> num <SEP> sources <SEP> fk <SEP> sources <SEP> fk <SEP> sources
some systems send update <PLACE_HOLDER> before call is established to update call screening @$ if we do not process them and just send 501 we end the dialog and the call will fail to establish @$ so we just send ok,return false ;,systems send codecs,fail,obj,None
release app 2 's am <PLACE_HOLDER> on node 2 .,scheduler . handle ( app removed event2 ) ; assert equals ( __str__ @$ __num__ @$ queue1 . get am resource usage ( ) . get memory size ( ) ) ; scheduler . update ( ) ;,release app container,success,obj,removed <SEP> event2 <SEP> equals <SEP> str <SEP> num <SEP> queue1
no averagers returned <PLACE_HOLDER> . all buckets must be empty . skip this row .,return null ;,averagers returned nulls,fail,obj,None
check if we can use the fast path @$ resuming a session . we can do so iff we have a valid record for that session @$ and the cipher suite for that session was on the list which the <PLACE_HOLDER> requested @$ and if we 're not forgetting any needed authentication on the part of the <PLACE_HOLDER> .,__str__ + session identity alg ) ; } resuming session = false ; } } if ( resuming session ) { cipher suite suite = previous . get suite ( ) ; if ( ( is negotiable ( suite ) == false ) || ( mesg . get cipher suites ( ) . contains ( suite ) == false ) ) { resuming session = false ; } else { set cipher suite ( suite ) ; } } if ( resuming session ) { session = previous ; if ( debug != null && ( debug . is on ( __str__ ) || debug . is on ( __str__ ) ) ) { system . out . println ( __str__ + session ) ; } } },server requested which,fail,sub,str <SEP> alg <SEP> resuming <SEP> resuming <SEP> mesg <SEP> suites <SEP> contains <SEP> resuming <SEP> resuming <SEP> str <SEP> str <SEP> println <SEP> str
if the number of posts on this blog that use this <PLACE_HOLDER> is higher than previous @$ set this as the most popular <PLACE_HOLDER> @$ and set the second most popular <PLACE_HOLDER> to the current most popular <PLACE_HOLDER>,int post count = json this tag . opt int ( __str__ ) ; if ( post count > popular count ) { next most popular tag = most popular tag ; most popular tag = this tag name ; popular count = post count ; } else if ( next most popular tag == null ) { next most popular tag = this tag name ; },number set tag,success,obj,int <SEP> json <SEP> int <SEP> str
check that pause resume wo n't call the end <PLACE_HOLDER> prematurely,resp . pause ( ) ; resp . resume ( ) ;,resume call method,fail,obj,resp <SEP> resp
hint arrow has no <PLACE_HOLDER> @$ and always returns the current <PLACE_HOLDER>,if ( is in agility arena ( ) ) { world point new ticket position = client . get hint arrow point ( ) ; world point old tick position = last arena ticket position ; last arena ticket position = new ticket position ; if ( old tick position != null && new ticket position != null && ( old tick position . getx ( ) != new ticket position . getx ( ) || old tick position . gety ( ) != new ticket position . gety ( ) ) ) { log . debug ( __str__ @$ old tick position @$ new ticket position ) ; if ( config . notify agility arena ( ) ) { notifier . notify ( __str__ ) ; },arrow has position,fail,obj,last <SEP> last <SEP> getx <SEP> getx <SEP> gety <SEP> gety <SEP> str <SEP> config <SEP> notifier <SEP> str
pairs now contains a uniquified <PLACE_HOLDER> of the sorted inputs @$ with counts for how often that item appeared . now sort by how frequently they occur @$ and pick the most frequent . if the first place is tied between two @$ do n't pick any .,collections . sort ( pairs ) ; final pair first pair = pairs . get ( __num__ ) ; if ( pairs . size ( ) == __num__ ) return first pair . item ; final pair second pair = pairs . get ( __num__ ) ; if ( first pair . count > second pair . count ) return first pair . item ; check state ( first pair . count == second pair . count ) ; return __num__ ;,list pick any,success,sub,collections <SEP> pairs <SEP> pairs <SEP> num <SEP> pairs <SEP> num <SEP> second <SEP> pairs <SEP> num <SEP> second <SEP> second <SEP> num
the replica <PLACE_HOLDER> have different names ...,if ( ! this . replica sets by name . key set ( ) . equals ( prior state . replica sets by name . key set ( ) ) ) { return true ; },sets have names,success,sub,sets <SEP> equals <SEP> sets
report if multiple devices are matching the <PLACE_HOLDER> .,if ( ! quiet && devices . size ( ) > __num__ ) { print message ( __str__ + devices . size ( ) + __str__ ) ; },devices matching pattern,fail,obj,devices <SEP> num <SEP> str <SEP> devices <SEP> str
this all could probably be done more elegantly via a group extracted from a more comprehensive regexp . clean up any extra <PLACE_HOLDER> around the remainder of the line @$ which should be a view name .,return statement . substring ( matcher . end ( ) ) . trim ( ) ;,elegantly clean spaces,success,obj,substring <SEP> matcher
different properties means different <PLACE_HOLDER>,assert false ( objects . equals ( empty @$ finger ) ) ; assert false ( objects . equals ( empty @$ finger brand ) ) ; assert false ( objects . equals ( finger @$ finger brand ) ) ;,properties means results,success,obj,objects <SEP> equals <SEP> objects <SEP> equals <SEP> objects <SEP> equals
check <PLACE_HOLDER> reads eof,assert equals ( - __num__ @$ is . read ( ) ) ; assert true ( end point . is output shutdown ( ) ) ;,client reads eof,success,sub,equals <SEP> num
res <PLACE_HOLDER> the first found abstract method,method res = null ; for ( method mi : methods ) { if ( ! modifier . is abstract ( mi . get modifiers ( ) ) ) continue ; if ( mi . get annotation ( traits . implemented . class ) != null ) continue ; try { object . class . get method ( mi . get name ( ) @$ mi . get parameter types ( ) ) ; continue ; } catch ( no such method exception e ) { } if ( res != null ) return null ; res = mi ; },returns found method,fail,sub,res <SEP> methods <SEP> modifiers <SEP> traits <SEP> implemented <SEP> types <SEP> res <SEP> res
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution <PLACE_HOLDER> specifies the name of the attribute .,@$ { error msg . compiler_warning_key @$ __str__ } @$ { error msg . runtime_error_key @$ __str__ } @$ { error msg . invalid_qname_err @$ __str__ } @$ { error msg . invalid_ncname_err @$ __str__ } @$ { error msg . invalid_method_in_output @$ __str__ } @$ {,text specifies name,success,sub,compilerwarningkey <SEP> str <SEP> runtimeerrorkey <SEP> str <SEP> invalidqnameerr <SEP> str <SEP> invalidncnameerr <SEP> str <SEP> invalidmethodinoutput <SEP> str <SEP> jaxpgetfeaturenullname <SEP> str <SEP> jaxpsetfeaturenullname <SEP> str <SEP> jaxpunsupportedfeature <SEP> str <SEP> jaxpsecureprocessingfeature <SEP> str <SEP> outlineerrtrycatch <SEP> str <SEP> outlineerrunbalancedmarkers <SEP> str <SEP> outlineerrdeletedtarget <SEP> str <SEP> outlineerrmethodtoobig <SEP> str <SEP> deserializetransleterr <SEP> str
spellcheck <PLACE_HOLDER> contains valencian and general accentuation,assert equals ( __num__ @$ rule . match ( lang tool . get analyzed sentence ( __str__ ) ) . length ) ;,sentence contains valencian,fail,sub,equals <SEP> num <SEP> lang <SEP> analyzed <SEP> str
delete interpret odex for android o @$ directory change . fortunately @$ we do n't need to support android o interpret <PLACE_HOLDER> any more,log . i ( tag @$ __str__ ) ; share patch file util . delete dir ( patch version directory + __str__ + share constants . interpret_dex_optimize_path ) ;,o interpret sudo,fail,obj,str <SEP> util <SEP> dir <SEP> str <SEP> constants <SEP> interpretdexoptimizepath
if the flushed requests has <PLACE_HOLDER> @$ we should propagate it also and fail the checkpoint,check and propagate async error ( ) ;,requests has errors,success,obj,async
if no entry keep skipping rows until we come to the end @$ or find <PLACE_HOLDER> that is populated,while ( this . entry == null && this . row < this . length ) { this . entry = this . table [ this . row ] ; this . row ++ ; } return this . entry ;,entry keep one,success,obj,row <SEP> row <SEP> row
simple <PLACE_HOLDER> can cause troubles here because of how <PLACE_HOLDER> works e.g . between lists and sets .,return collection utils . is equal collection ( state objects @$ that . state objects ) ;,collections works e.g,fail,sub,utils <SEP> objects <SEP> objects
start <PLACE_HOLDER> create <PLACE_HOLDER>,event = ( activiti entity event ) listener . get events received ( ) . get ( __num__ ) ; assert equals ( activiti event type . entity_created @$ event . get type ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get process instance id ( ) ) ; assert not equals ( process instance . get id ( ) @$ event . get execution id ( ) ) ; assert equals ( process instance . get process definition id ( ) @$ event . get process definition id ( ) ) ;,event create event,success,sub,activiti <SEP> events <SEP> num <SEP> equals <SEP> activiti <SEP> entitycreated <SEP> equals <SEP> process <SEP> process <SEP> equals <SEP> process <SEP> equals <SEP> process <SEP> process <SEP> process
the second <PLACE_HOLDER> should only get the newest value and any later values .,live data . observe ( m lifecycle owner @$ new observer < string > ( ) { @ override public void on changed ( @ nullable string s ) { output2 . add ( s ) ; } } ) ; live data . remove observer ( m observer ) ; processor . on next ( __str__ ) ; assert that ( m live data output @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ; assert that ( output2 @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ;,observer get value,success,sub,live <SEP> lifecycle <SEP> nullable <SEP> output2 <SEP> live <SEP> str <SEP> live <SEP> arrays <SEP> str <SEP> str <SEP> output2 <SEP> arrays <SEP> str <SEP> str
append the work <PLACE_HOLDER> file path to the job input file,throw closer . rethrow ( t ) ;,input file path,fail,sub,closer <SEP> rethrow
<PLACE_HOLDER> to translators : xsltc could not find the stylesheet document with the name specified by the substitution text .,@$ { error msg . compiler_warning_key @$ __str__ } @$ { error msg . runtime_error_key @$ __str__ } @$ { error msg . invalid_qname_err @$ __str__ } @$ { error msg . invalid_ncname_err @$ __str__ } @$ { error msg . invalid_method_in_output @$ __str__ } @$ {,note find document,success,sub,compilerwarningkey <SEP> str <SEP> runtimeerrorkey <SEP> str <SEP> invalidqnameerr <SEP> str <SEP> invalidncnameerr <SEP> str <SEP> invalidmethodinoutput <SEP> str <SEP> jaxpgetfeaturenullname <SEP> str <SEP> jaxpsetfeaturenullname <SEP> str <SEP> jaxpunsupportedfeature <SEP> str <SEP> jaxpsecureprocessingfeature <SEP> str <SEP> outlineerrtrycatch <SEP> str <SEP> outlineerrunbalancedmarkers <SEP> str <SEP> outlineerrdeletedtarget <SEP> str <SEP> outlineerrmethodtoobig <SEP> str <SEP> deserializetransleterr <SEP> str
this property access would be an unknown property error unless the polymer <PLACE_HOLDER> had successfully parsed the element definition .,compiler compiler = compile ( options @$ new string [ ] { lines ( __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) } ) ;,pass parsed definition,success,sub,options <SEP> lines <SEP> str <SEP> lines <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str
no previous id @$ always accept this <PLACE_HOLDER>,if ( requested session id == null ) { requested session id = id ; session = s ; } else if ( requested session id . equals ( id ) ) { } else if ( session == null || ! is valid ( session ) ) { requested session id = id ; session = s ; } else { if ( s != null && is valid ( s ) ) throw new bad message exception ( __str__ + requested session id + __str__ + id ) ; },id accept one,success,obj,requested <SEP> requested <SEP> requested <SEP> equals <SEP> requested <SEP> str <SEP> requested <SEP> str
vm <PLACE_HOLDER> locks and frees key <PLACE_HOLDER>,vm1 . invoke ( new serializable runnable ( ) { @ override public void run ( ) { logger . info ( __str__ ) ; connect distributed system ( ) ; d lock service dls = ( d lock service ) d lock service . create ( dls name @$ get system ( ) @$ true @$ true @$ false ) ; assert that ( dls . lock ( key1 @$ - __num__ @$ - __num__ ) ) . is true ( ) ; logger . info ( __str__ ) ; dls . unlock ( key1 ) ; assert that ( dls . get token ( key1 ) ) . is not null ( ) ; dls . free resources ( key1 ) ; d lock token token,vm locks 1,success ,obj, dls <SEP> dls <SEP> dls <SEP> dls <SEP> dls <SEP> dls 
get the app log aggregation impl <PLACE_HOLDER> to crash,local dirs handler service mocked dir svc = mock ( local dirs handler service . class ) ; log aggregation service log aggregation service = new log aggregation service ( dispatcher @$ this . context @$ del srvc @$ mocked dir svc ) ; log aggregation service . init ( this . conf ) ; log aggregation service . start ( ) ; application id application1 = builder utils . new application id ( __num__ @$ __num__ ) ; log aggregation service . handle ( new log handler app started event ( application1 @$ this . user @$ null @$ this . acls ) ) ; log aggregation service . handle ( new log handler app finished event ( application1 ) ) ; dispatcher . await ( ),app log thread,success,obj,dirs <SEP> mocked <SEP> dir <SEP> svc <SEP> dirs <SEP> del <SEP> srvc <SEP> mocked <SEP> dir <SEP> svc <SEP> init <SEP> conf <SEP> application1 <SEP> utils <SEP> num <SEP> num <SEP> started <SEP> application1 <SEP> acls <SEP> application1
can happen if this <PLACE_HOLDER> does not carry forward the previous bucketing columns for e.g . another join <PLACE_HOLDER> which does not carry one of the sides ' key columns,list bucket cols . size ( ) <= col count ) { return false ; } expr node desc expr node desc = col expr map . get ( col name ) ; if ( expr node desc instanceof expr node column desc ) { if ( ( ( expr node column desc ) expr node desc ) . get column ( ) . equals ( list bucket cols . get ( col count ) ) ) { col count ++ ; } else { break ; } } if ( col count == parent col names . get ( __num__ ) . size ( ) ) { return ! strict || ( col count == list bucket cols . size ( ) ) ; } } },operator carry columns,success,sub,cols <SEP> expr <SEP> desc <SEP> expr <SEP> desc <SEP> expr <SEP> expr <SEP> desc <SEP> instanceof <SEP> expr <SEP> desc <SEP> expr <SEP> desc <SEP> expr <SEP> desc <SEP> equals <SEP> cols <SEP> names <SEP> num <SEP> cols
client sent an ack lds <PLACE_HOLDER> .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_lds @$ __str__ ) ) ) ;,client sent request,success,obj,eq <SEP> str <SEP> str <SEP> xds <SEP> impl <SEP> adstypeurllds <SEP> str
unlike pql @$ <PLACE_HOLDER> expects the group columns in select statements .,string group by columns = string utils . join ( _group columns @$ __str__ ) ;,csv expects columns,fail,sub,columns <SEP> utils <SEP> columns <SEP> str
rotate the queue so that each <PLACE_HOLDER> gets the descriptions in a different order,for ( filter filter : filters ) { mutating descriptions . add last ( mutating descriptions . poll first ( ) ) ; for ( description description : descriptions ) { if ( filter . should run ( description ) ) { add description for filter to map ( descriptions run @$ filter @$ description ) ; } } },aggregator gets descriptions,fail,sub,filters <SEP> mutating <SEP> descriptions <SEP> last <SEP> mutating <SEP> descriptions <SEP> descriptions <SEP> descriptions
this estimate will not take into account the <PLACE_HOLDER> saved by inlining the keys .,return versioned stats disklru region entry off heap object key . class ;,estimate take memory,success,obj,versioned <SEP> disklru
generate dates for selecting audits by date @$ making sure the <PLACE_HOLDER> will not contain the sample audit,string from date = sample_timestamp . minus days ( __num__ ) . format ( formatter ) ; string to date = sample_timestamp . minus days ( __num__ ) . format ( formatter ) ;,period contain audit,success,sub,sampletimestamp <SEP> days <SEP> num <SEP> formatter <SEP> sampletimestamp <SEP> days <SEP> num <SEP> formatter
unicode now contains the four hex digits which represents our unicode <PLACE_HOLDER>,if ( unicode . length ( ) == __num__ ) { int value = integer . parse int ( unicode . to string ( ) @$ __num__ ) ; out . write ( ( char ) value ) ; unicode . set length ( __num__ ) ; in unicode = false ; had slash = false ; },which represents character,success,obj,char
print suppressed <PLACE_HOLDER> @$ if any,for ( throwable se : get suppressed ( ) ) se . print enclosed stack trace ( s @$ trace @$ suppressed_caption @$ __str__ @$ deja vu ) ;,print suppressed exceptions,success,obj,se <SEP> se
user specified the <PLACE_HOLDER> for local mode hadoop run,console . print info ( __str__ + hadoop mem + __str__ ) ; variables . put ( hadoop_mem_key @$ string . value of ( hadoop mem ) ) ;,user specified memory,success,obj,mem<SEP> mem<SEP> mem
get data will activate deferred <PLACE_HOLDER> if necessary,byte [ ] the header = get data ( ic sig head ) ; int to big endian ( rendering intent @$ the header @$ ic hdr rendering intent ) ;,data activate profiles,success,obj,sig <SEP> int <SEP> endian <SEP> hdr
subsequent same key presses move the keyboard <PLACE_HOLDER> to the next object that starts with the same letter .,if ( ( prefix . length ( ) == __num__ ) && ( c == prefix . char at ( __num__ ) ) ) { starting row ++ ; } else { prefix = typed string ; },presses move focus,success,obj,num <SEP> num <SEP> starting <SEP> row <SEP> typed
note : this next bit <PLACE_HOLDER> the tree label @$ rather than creating a new tree node . beware !,ht . set label ( lf . new label ( ht . value ( ) + __str__ ) ) ;,bit changes,success,pre,ht <SEP> lf <SEP> ht <SEP> str
let handle timeout <PLACE_HOLDER> care of finishing the page,if ( ! timeout flag ) handler . post delayed ( new web view status checker ( ) @$ __num__ ) ;,timeout take,success,pre,delayed <SEP> num
m total length <PLACE_HOLDER> the padding already,child left = get padding left ( ) + right - left - m total length ; break ; case gravity . center_horizontal :,length contains,success,pre,centerhorizontal
client app <PLACE_HOLDER> the restore session dangling . we know that it ca n't be in the middle of an actual restore operation because the timeout is suspended while a restore is in progress . clean up now .,if ( backup manager service . get active restore session ( ) != null ) { slog . w ( tag @$ __str__ ) ; backup manager service . get active restore session ( ) . mark timed out ( ) ; post ( backup manager service . get active restore session ( ) . new end restore runnable ( backup manager service @$ backup manager service . get active restore session ( ) ) ) ; },app has,fail,pre,str <SEP> timed <SEP> runnable
flush enough files to get up to the threshold @$ does n't <PLACE_HOLDER> compactions,for ( int i = __num__ ; i < __num__ ; i ++ ) { put put = new put ( table name . to bytes ( ) ) . add column ( family @$ family @$ table name . to bytes ( ) ) ; region . put ( put ) ; fr = region . flush ( true ) ; assert true ( fr . is flush succeeded ( ) ) ; assert false ( fr . is compaction needed ( ) ) ; },files perform,fail,pre,int <SEP> num <SEP> num <SEP> bytes <SEP> bytes <SEP> succeeded <SEP> compaction <SEP> needed
pointers to <PLACE_HOLDER> columns,int [ ] polyp = new int [ ilbc_constants . enh_ups0 ] ;,pointers pv,fail,pre,int <SEP> int <SEP> ilbcconstants <SEP> enhups0
host must <PLACE_HOLDER> real keyboard focus .,if ( ! m host . is focused ( ) && ! m host . request focus ( ) ) { return false ; },host have,success,pre,None
the caller <PLACE_HOLDER> strings @$ we have to use an intermediary .,return new id enumeration ( collections . enumeration ( availablei ds ) ) ;,caller wants,fail,pre,collections <SEP> availablei <SEP> ds
renewing token and adding it to timer calls are separated purposefully if user <PLACE_HOLDER> incorrect token then it should not be added for renewal .,if ( ! token list . is empty ( ) ) { for ( delegation token to renew dtr : token list ) { delegation token to renew current dtr = all tokens . put if absent ( dtr . token @$ dtr ) ; if ( current dtr != null ) { current dtr . referring app ids . add ( application id ) ; app tokens . get ( application id ) . add ( current dtr ) ; } else { app tokens . get ( application id ) . add ( dtr ) ; set timer for token renewal ( dtr ) ; } } },user gave,fail,pre,dtr <SEP> dtr <SEP> tokens <SEP> dtr <SEP> dtr <SEP> dtr <SEP> dtr <SEP> referring <SEP> ids <SEP> tokens <SEP> dtr <SEP> tokens <SEP> dtr <SEP> dtr
flush those <PLACE_HOLDER> buffered events out .,produce synchronously to partition zero ( input @$ as list ( new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) ) ) ; verify output ( output raw @$ new hash set < > ( as list ( new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__,those sent,fail,pre,synchronously <SEP> str <SEP> str <SEP> scaled <SEP> num <SEP> str <SEP> str <SEP> scaled <SEP> num <SEP> str <SEP> str <SEP> scaled <SEP> num <SEP> str <SEP> num <SEP> scaled <SEP> num <SEP> str <SEP> num <SEP> scaled <SEP> num <SEP> str <SEP> num
rather than fight it @$ let root <PLACE_HOLDER> an alias,nodes . put ( __str__ @$ root ) ; nodes . put without digest ( root zookeeper @$ root ) ; root . add child ( proc child zookeeper ) ; nodes . put ( proc zookeeper @$ proc data node ) ; proc data node . add child ( quota child zookeeper ) ; nodes . put ( quota zookeeper @$ quota data node ) ; add config node ( ) ; node data size . set ( approximate data size ( ) ) ; try { data watches = watch manager factory . create watch manager ( ) ; child watches = watch manager factory . create watch manager ( ) ; } catch ( exception e ) { log . error ( __str__ @$ e,root have,success,pre,nodes <SEP> str <SEP> nodes <SEP> proc <SEP> nodes <SEP> proc <SEP> proc <SEP> proc <SEP> nodes <SEP> config <SEP> watches <SEP> watches <SEP> str
puff @$ everything <PLACE_HOLDER> ok,return return value ;,everything went,success,pre,None
verify that the text field <PLACE_HOLDER> the min address since the current offset is 0,assert equals ( program . get min address ( ) @$ ai . get address ( ) ) ; run swing ( ( ) -> ai . set value ( __str__ ) ) ; press button by text ( d . get component ( ) @$ __str__ ) ;,field has,success,pre,equals <SEP> str <SEP> str
if the user <PLACE_HOLDER> a custom view for the tab indicators @$ then do not draw the bottom strips .,if ( ! m draw bottom strips ) { return ; },user specified,success,pre,strips
create a view <PLACE_HOLDER> from the locator that conflicts with the installed view,member identifier locator member id = new internal distributed member ( __str__ @$ mock members [ mock members . length - __num__ ] . get membership port ( ) + __num__ ) ; locator member id . set vm kind ( cluster distribution manager . locator_dm_type ) ; list < member identifier > new member list = new array list < > ( members ) ; new member list . add ( locator member id ) ; gms membership view locator view = new gms membership view ( locator member id @$ installed view . get view id ( ) + __num__ @$ new member list ) ;,view object,fail,pre,distributed <SEP> str <SEP> members <SEP> members <SEP> num <SEP> num <SEP> vm <SEP> locatordmtype <SEP> members <SEP> gms <SEP> gms <SEP> installed <SEP> num
subsequent same key presses <PLACE_HOLDER> the keyboard focus to the next object that starts with the same letter .,if ( ( prefix . length ( ) == __num__ ) && ( c == prefix . char at ( __num__ ) ) ) { starting row ++ ; } else { prefix = typed string ; },presses move,success,pre,num <SEP> num <SEP> starting <SEP> row <SEP> typed
this fragment <PLACE_HOLDER> each execution node to create the files that will be written to during the snapshot,byte [ ] hashinator bytes = ( hashinator data != null ? hashinator data . m_ser data : null ) ; long hashinator version = ( hashinator data != null ? hashinator data . m_version : __num__ ) ; return create and execute sys proc plan ( sys proc fragment id . pf_create snapshot targets @$ sys proc fragment id . pf_create snapshot targets results @$ file path @$ file nonce @$ per partition txn ids @$ block @$ format . name ( ) @$ data @$ hashinator bytes @$ hashinator version @$ system . current time millis ( ) @$ path type ) ;,fragment uses,fail,pre,hashinator <SEP> bytes <SEP> hashinator <SEP> hashinator <SEP> mser <SEP> hashinator <SEP> hashinator <SEP> hashinator <SEP> mversion <SEP> num <SEP> sys <SEP> proc <SEP> sys <SEP> proc <SEP> pfcreate <SEP> targets <SEP> sys <SEP> proc <SEP> pfcreate <SEP> targets <SEP> results <SEP> txn <SEP> ids <SEP> hashinator <SEP> bytes <SEP> hashinator <SEP> millis
the user can <PLACE_HOLDER> the hadoop memory,map < string @$ string > variables = new hash map < string @$ string > ( system . getenv ( ) ) ;,user specify,success,pre,variables <SEP> getenv
nested view <PLACE_HOLDER> scrollable area under this point . let it be handled there .,if ( dy != __num__ && ! is gutter drag vertically ( m last motiony @$ dy ) && can scroll vertically ( this @$ false @$ ( int ) dy @$ ( int ) x @$ ( int ) y ) ) { m last motionx = x ; m last motiony = y ; m is unable to drag = true ; return false ; },view has,success,pre,dy <SEP> num <SEP> last <SEP> motiony <SEP> dy <SEP> can <SEP> int <SEP> dy <SEP> int <SEP> int <SEP> last <SEP> motionx <SEP> last <SEP> motiony
a node throwing the exception because of the can <PLACE_HOLDER> method of the attached faulty node property,dumb slave faulty agent = r . create online slave ( label expression . get ( __str__ ) ) ; faulty agent . get node properties ( ) . add ( new faulty node property ( ) ) ;,node create,fail,pre,str <SEP> properties
test that it works when the shutdown <PLACE_HOLDER> the outstanding request ...,test shutdown request outstanding ( __num__ @$ __num__ @$ remote invocation exception . class @$ timeout exception . class ) ;,shutdown kills,success,pre,num <SEP> num
if both old and new table does not <PLACE_HOLDER> the policy @$ then do n't dump the event .,log . info ( __str__ ) ; return false ;,table satisfy,fail,pre,str
let 's <PLACE_HOLDER> folder,data . sftpclient . create folder ( spool directory ) ; if ( is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ spool directory ) ) ; },'s create,success,pre,sftpclient <SEP> messages <SEP> str
make sure vm 1 did n't <PLACE_HOLDER> the bucket,assert that ( vm1 . invoke ( ( ) -> get bucket list ( partitioned region name ) ) ) . is empty ( ) ; vm1 . invoke ( ( ) -> get cache ( ) . close ( ) ) ; async invocation < void > create partitioned region onvm0 = vm0 . invoke async ( ( ) -> create partitioned region ( __num__ @$ - __num__ @$ __num__ @$ true ) ) ; create partitioned region onvm0 . join ( seconds . to millis ( __num__ ) ) ;,vm create,success,pre,vm1 <SEP> partitioned <SEP> vm1 <SEP> close <SEP> async <SEP> partitioned <SEP> onvm0 <SEP> vm0 <SEP> async <SEP> partitioned <SEP> num <SEP> num <SEP> num <SEP> partitioned <SEP> onvm0 <SEP> seconds <SEP> millis <SEP> num
recycler view <PLACE_HOLDER> also the same control to provide better performance .,m column header recycler view . set has fixed size ( has fixed width ) ;,view has,success,pre,recycler
closing the entity manager factory should <PLACE_HOLDER> the em,em . get entity manager factory ( ) . close ( ) ;,factory close,success,pre,close
required features <PLACE_HOLDER> activity launch,wait for latch ( latch ) ; verify ( m mock account manager response ) . on error ( account manager . error_code_invalid_response @$ account manager service test fixtures . error_message ) ; verify ( m mock account manager response @$ never ( ) ) . on result ( any ( bundle . class ) ) ;,features expect,success,pre,errorcodeinvalidresponse <SEP> fixtures <SEP> errormessage
this is a direct call to a method that the static analysis did not see as invoked . this can happen when the receiver is always null . in most cases @$ the method profile also <PLACE_HOLDER> a length of 0 and the below code to kill the invoke would trigger . but not all methods have profiles @$ for example methods with manually,if ( call target . invoke kind ( ) . is direct ( ) && ! ( ( hosted method ) call target . target method ( ) ) . get wrapped ( ) . is simply implementation invoked ( ) ) { unreachable invoke ( graph @$ invoke @$ call target ) ; continue ; },profile has,success,pre,hosted <SEP> invoked <SEP> unreachable
make sure the object <PLACE_HOLDER> the gc,if ( ( flags & ( call_type_callback | call_type_global_value | call_type_struct_member ) ) > __num__ ) { o . retain ( ) ; },object outlives,success,pre,flags <SEP> calltypecallback <SEP> calltypeglobalvalue <SEP> calltypestructmember <SEP> num
there is an existing user who only <PLACE_HOLDER> unsupported sign in methods,if ( task . is successful ( ) && last signed in providers . is empty ( ) && ! methods . is empty ( ) ) { return tasks . for exception ( new firebase ui exception ( error codes . developer_error ) ) ; },who has,success,pre,last <SEP> signed <SEP> providers <SEP> methods <SEP> tasks <SEP> firebase <SEP> codes <SEP> developererror
only the stream which was just added will <PLACE_HOLDER> parents . so we only need an array of size 1 .,list < parent changed event > events = new array list < parent changed event > ( __num__ ) ; connection state . take child ( new parent @$ false @$ events ) ; notify parent changed ( events ) ;,stream have,fail,pre,events <SEP> num <SEP> events <SEP> events
say that a null label <PLACE_HOLDER> no positive pattern @$ but any negated patern,if ( lab == null ) { return negated pattern ; } else { if ( basic cat ) { lab = basic cat function . apply ( lab ) ; } matcher m = pattern . matcher ( lab ) ; return m . find ( ) != negated pattern ; },label matches,success,pre,negated <SEP> matcher <SEP> matcher <SEP> negated
let authenticator <PLACE_HOLDER> the identity of the caller,login options . put int ( account manager . key_caller_uid @$ caller uid ) ; login options . put int ( account manager . key_caller_pid @$ binder . get calling pid ( ) ) ; if ( notify on auth failure ) { login options . put boolean ( account manager . key_notify_on_failure @$ true ) ; } long identity token = clear calling identity ( ) ; try { final byte [ ] caller pkg sig digest = calculate package signature digest ( caller pkg ) ; if ( ! custom tokens && permission granted ) { string auth token = read auth token internal ( accounts @$ account @$ auth token type ) ; if ( auth token != null ) { bundle result = new,authenticator change,fail,pre,options <SEP> int <SEP> keycalleruid <SEP> uid <SEP> options <SEP> int <SEP> keycallerpid <SEP> pid <SEP> auth <SEP> options <SEP> keynotifyonfailure <SEP> sig <SEP> tokens <SEP> auth <SEP> auth <SEP> accounts <SEP> auth <SEP> auth
this is technically a recursive constraint for cast @$ but type registry.can cast <PLACE_HOLDER> explicit handling for row to row cast,super ( cast @$ immutable list . of ( new type variable constraint ( __str__ @$ false @$ false @$ __str__ @$ immutable set . of ( new type signature ( __str__ ) ) @$ immutable set . of ( ) ) @$ with variadic bound ( __str__ @$ __str__ ) ) @$ immutable list . of ( ) @$ new type signature ( __str__ ) @$ immutable list . of ( new type signature ( __str__ ) ) @$ false ) ;,cast has,success,pre,str <SEP> str <SEP> str <SEP> variadic <SEP> str <SEP> str <SEP> str <SEP> str
match the container @$ to reduce the risk of issues . the preview should never be drawn while the surface <PLACE_HOLDER> this size .,if ( surface rect == null ) { surface view . layout ( __num__ @$ __num__ @$ get width ( ) @$ get height ( ) ) ; } else { surface view . layout ( surface rect . left @$ surface rect . top @$ surface rect . right @$ surface rect . bottom ) ; },surface has,success,pre,rect <SEP> num <SEP> num <SEP> rect <SEP> rect <SEP> rect <SEP> rect
repl load during migration @$ <PLACE_HOLDER> the explicit txn and start some internal txns . call release locks and commit or rollback to do the clean up .,if ( ! driver context . get txn manager ( ) . is txn open ( ) && driver context . get query state ( ) . get hive operation ( ) == hive operation . replload ) { release locks and commit or rollback ( false ) ; } else { },load enforce,fail,pre,txn <SEP> txn <SEP> replload <SEP> locks
drwho is not authorized to access test protocol 1 because it uses the default <PLACE_HOLDER> acl .,try { service authorization manager . authorize ( drwho @$ test protocol1 . class @$ conf @$ inet address . get by name ( address ) ) ; fail ( ) ; } catch ( authorization exception e ) { },default read,fail,pre,drwho <SEP> protocol1 <SEP> conf <SEP> inet
verify false constant <PLACE_HOLDER> the conditional with the else expression .,assert translation ( translation @$ __str__ ) ;,verify replaces,success,pre,str
register task manager success will <PLACE_HOLDER> monitoring heartbeat target between tm and jm,final task manager location task manager location1 = task manager location future . get ( timeout . to milliseconds ( ) @$ time unit . milliseconds ) ; assert that ( task manager location1 @$ equal to ( task manager location ) ) ;,success trigger,success,pre,location1 <SEP> milliseconds <SEP> milliseconds <SEP> location1
do n't let the user click restore if the words area <PLACE_HOLDER> the current wallet words @$ or are an invalid set @$ or if the date field is n't set @$ or if it 's in the future .,restore button . disable property ( ) . bind ( or ( or ( not ( validator . valid ) @$ equal ( orig words @$ words area . text property ( ) ) ) @$ date picker is invalid ) ) ;,area has,fail,pre,validator <SEP> orig <SEP> words <SEP> words
the value of call count can exceed 1 only if the callback thread <PLACE_HOLDER> the exception thrown by the first callback .,assert true ( call count . get ( ) > __num__ ) ; if ( watcher != null ) { watcher . stop ( ) ; watcher . wait for state ( file change watcher . state . stopped ) ; },thread throws,fail,pre,num <SEP> stopped
no need to sync because noone <PLACE_HOLDER> access to new info yet,new info . policy entries . add ( pe ) ;,noone has,success,pre,entries
if still null then the implementation does n't <PLACE_HOLDER> a presence operation set which is unacceptable for gibberish .,if ( op set pers presence2 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ; contact group root group1 = op set pers presence1 . get server stored contact list root ( ) ;,implementation offer,success,pre,pers <SEP> presence2 <SEP> str <SEP> str <SEP> str <SEP> group1 <SEP> pers <SEP> presence1 <SEP> stored
only one geometry @$ let 's not wrap it in another node unless the node <PLACE_HOLDER> children .,if ( primitives . length == __num__ && children == null ) { spatial = primitives [ __num__ ] ; } else { node node = new node ( ) ; for ( geometry primitive : primitives ) { node . attach child ( primitive ) ; } spatial = node ; },node has,success,pre,primitives <SEP> num <SEP> children <SEP> primitives <SEP> num <SEP> primitives
hregion <PLACE_HOLDER> this one,verify ( wal @$ times ( __num__ ) ) . sync ( any long ( ) ) ;,hregion calls,success,pre,wal <SEP> num
associate <PLACE_HOLDER> component virtual file with mocked <PLACE_HOLDER> component cls,psi class mocked generated component cls = mock ( psi class . class ) ; psi file mocked generated component file = mock ( psi file . class ) ; when ( mocked generated component cls . get containing file ( ) ) . then return ( mocked generated component file ) ; virtual file generated component virtual file = create present in scope virtual file ( ) ; when ( mocked generated component file . get virtual file ( ) ) . then return ( generated component virtual file ) ; virtual file present in scope virtual file = create present in scope virtual file ( ) ;,associate generated,success,pre,mocked <SEP> generated <SEP> cls <SEP> mocked <SEP> generated <SEP> mocked <SEP> generated <SEP> cls <SEP> containing <SEP> mocked <SEP> generated <SEP> generated <SEP> mocked <SEP> generated <SEP> generated
process command line property definitions these can potentially <PLACE_HOLDER> multiple times,list < cl option > cl options = parser . get arguments ( ) ; for ( cl option option : cl options ) { string name = option . get argument ( __num__ ) ; string value = option . get argument ( __num__ ) ; switch ( option . get descriptor ( ) . get id ( ) ) { case cl option . text_argument : throw new illegal argument exception ( __str__ + option . get argument ( ) ) ; case propfile2_opt : log . info ( __str__ @$ name ) ; try ( file input stream fis = new file input stream ( new file ( name ) ) ) { properties tmp = new properties ( ) ; tmp . load (,these occur,success,pre,options <SEP> parser <SEP> arguments <SEP> options <SEP> num <SEP> num <SEP> textargument <SEP> str <SEP> propfile2opt <SEP> str <SEP> fis <SEP> properties <SEP> tmp <SEP> properties <SEP> tmp
negative exit values are <PLACE_HOLDER> 256 :,for ( int exit : new int [ ] { - __num__ @$ - __num__ @$ - __num__ } ) { int expected = __num__ + exit ; string [ ] args = { __str__ @$ __str__ @$ __str__ + exit } ; bad exit status exception e = assert throws ( __str__ + expected @$ bad exit status exception . class @$ ( ) -> new command ( args ) . execute ( ) ) ; assert that ( e ) . has message that ( ) . is equal to ( __str__ + expected ) ; check command elements ( e @$ __str__ @$ __str__ @$ __str__ + exit ) ; termination status status = e . get result ( ) . get termination status (,values scans,fail,pre,int <SEP> int <SEP> num <SEP> num <SEP> num <SEP> int <SEP> num <SEP> args <SEP> str <SEP> str <SEP> str <SEP> throws <SEP> str <SEP> args <SEP> str <SEP> elements <SEP> str <SEP> str <SEP> str
user has <PLACE_HOLDER> a file @$ we can continue ...,try { file object file object = kettlevfs . get file object ( vfs filename @$ this ) ; if ( ! ( file object instanceof local file ) ) { throw new kettle exception ( __str__ + vfs filename + __str__ ) ; } string real filename = kettlevfs . get filename ( file object ) ; file file = new file ( real filename ) ; if ( ( file . exists ( ) && file . can read ( ) ) || is local infile ( ) == false ) { if ( log . is detailed ( ) ) { log detailed ( __str__ + real filename + __str__ ) ; } if ( connection != null ) { database db = new database,user specified,success,pre,kettlevfs <SEP> vfs <SEP> instanceof <SEP> str <SEP> vfs <SEP> str <SEP> kettlevfs <SEP> exists <SEP> can <SEP> infile <SEP> str <SEP> str <SEP> db
ensure that record <PLACE_HOLDER> corresponds to a data db record,if ( rec != null ) { if ( ! rec . has same schema ( datadb adapter . data_schema ) ) { return true ; } dt = code mgr . get data type ( rec ) ; if ( dt == null ) { msg . error ( this @$ __str__ + address ) ; } } else { dt = code mgr . get data type ( addr ) ; },record provided,success,pre,rec <SEP> rec <SEP> datadb <SEP> dataschema <SEP> rec <SEP> str <SEP> addr
metadata objects now <PLACE_HOLDER> their descriptor name,if ( cont type != container type . extended_content ) { out . write ( utils . get bytes ( get name ( ) @$ asf header . asf_charset ) ) ; out . write ( asf header . zero_term ) ; },objects have,fail,pre,extendedcontent <SEP> utils <SEP> bytes <SEP> asf <SEP> asfcharset <SEP> asf <SEP> zeroterm
if the id is a generated unique id then this could affect .q <PLACE_HOLDER> golden <PLACE_HOLDER>s for tests that run explain queries .,return __str__ + id + __str__ ;,.q do,fail,pre,str <SEP> str
the child case instance <PLACE_HOLDER> the plan item instance id as callback id stored . when the child case instance is finished @$ the plan item of the parent case needs to be triggered .,if ( case instance state . terminated . equals ( callback data . get new state ( ) ) || case instance state . completed . equals ( callback data . get new state ( ) ) ) { command context command context = command context util . get command context ( ) ; plan item instance entity plan item instance entity = command context util . get plan item instance entity manager ( command context ) . find by id ( callback data . get callback id ( ) ) ; if ( plan item instance entity != null ) { command context util . get agenda ( command context ) . plan trigger plan item instance operation ( plan item instance entity ) ; } },instance has,success,pre,terminated <SEP> equals <SEP> completed <SEP> equals <SEP> util <SEP> util <SEP> util
same as above @$ except this is a bit of stress testing . <PLACE_HOLDER> 5 database files and make sure they are all removed .,int n = __num__ ; array list < string > attached db files = new array list < string > ( n ) ; for ( int i = __num__ ; i < n ; i ++ ) { attached db files . add ( m database . get path ( ) + i ) ; } db obj = sq lite database . open or create database ( m database . get path ( ) @$ null ) ; db obj . execsql ( __str__ ) ; for ( int i = __num__ ; i < n ; i ++ ) { db obj . execsql ( __str__ + attached db files . get ( i ) + __str__ + i ) ; } assert true (,same exclude,fail,pre,int <SEP> num <SEP> db <SEP> files <SEP> int <SEP> num <SEP> db <SEP> files <SEP> db <SEP> obj <SEP> db <SEP> obj <SEP> execsql <SEP> str <SEP> int <SEP> num <SEP> db <SEP> obj <SEP> execsql <SEP> str <SEP> db <SEP> files <SEP> str
step 1 : ensure that user has write permissions to the process group . if not @$ then immediately fail . step 2 : <PLACE_HOLDER> flow from flow registry,if ( version control info != null && request process group entity . get versioned flow snapshot ( ) == null ) { final versioned flow snapshot flow snapshot = get flow from registry ( version control info ) ; service facade . discover compatible bundles ( flow snapshot . get flow contents ( ) ) ; service facade . resolve inherited controller services ( flow snapshot @$ group id @$ ni fi user utils . get ni fi user ( ) ) ; request process group entity . set versioned flow snapshot ( flow snapshot ) ; },not get,fail,pre,process <SEP> versioned <SEP> versioned <SEP> bundles <SEP> contents <SEP> inherited <SEP> services <SEP> fi <SEP> utils <SEP> fi <SEP> process <SEP> versioned
make the client <PLACE_HOLDER> client cache factory so it will have a default pool,this . durable clientvm . invoke ( ( ) -> cache server test util . create client cache ( get client pool ( get server host name ( ) @$ server1 port @$ true ) @$ region name @$ get client distributed system properties ( durable client id ) ) ) ;,client create,fail,pre,clientvm <SEP> util <SEP> server1 <SEP> distributed <SEP> properties
for posterity : the moment this user <PLACE_HOLDER> the easter egg,try { settings . system . put long ( cr @$ __str__ @$ system . current time millis ( ) ) ; } catch ( runtime exception e ) { log . e ( __str__ @$ __str__ @$ e ) ; },user use,fail,pre,settings <SEP> cr <SEP> str <SEP> millis <SEP> str <SEP> str
if the method <PLACE_HOLDER> parameters @$ skip it,try { if ( method . is empty ( ) || method . get parameter types ( ) . length != __num__ ) { continue ; } } catch ( not found exception e ) { continue ; },method has,success,pre,types <SEP> num
construct a new model with the intended architecture and print summary note : this architecture is constructed with the primary intent of demonstrating use of the transfer learning api @$ secondary to what might <PLACE_HOLDER> better results,computation graph vgg16 transfer = new transfer learning . graph builder ( vgg16 ) . fine tune configuration ( fine tune conf ) . set feature extractor ( feature extraction layer ) . n out replace ( __str__ @$ __num__ @$ weight init . xavier ) . remove vertex and connections ( __str__ ) . add layer ( __str__ @$ new dense layer . builder ( ) . activation ( activation . tanh ) . n in ( __num__ ) . n out ( __num__ ) . build ( ) @$ __str__ ) . add layer ( __str__ @$ new output layer . builder ( loss functions . loss function . negativeloglikelihood ) . activation ( activation . softmax ) . n in ( __num__ ) . n,secondary give,success,pre,vgg16 <SEP> vgg16 <SEP> conf <SEP> str <SEP> num <SEP> init <SEP> xavier <SEP> connections <SEP> str <SEP> str <SEP> tanh <SEP> num <SEP> num <SEP> str <SEP> str <SEP> functions <SEP> negativeloglikelihood <SEP> softmax <SEP> num
get all cookies that domain <PLACE_HOLDER> the uri,for ( map . entry < uri @$ list < http cookie > > entry : map . entry set ( ) ) { if ( uri . equals ( entry . get key ( ) ) ) { continue ; } list < http cookie > entry cookies = entry . get value ( ) ; for ( iterator < http cookie > i = entry cookies . iterator ( ) ; i . has next ( ) ; ) { http cookie cookie = i . next ( ) ; if ( ! http cookie . domain matches ( cookie . get domain ( ) @$ uri . get host ( ) ) ) { continue ; } if ( cookie . has expired ( ),domain matches,success,pre,equals <SEP> cookies <SEP> iterator <SEP> cookies <SEP> iterator <SEP> matches
even if some members have fallen behind . the config offset <PLACE_HOLDER>d to generate the assignment is included in the response so members that have fallen behind will not <PLACE_HOLDER> the assignment until they have caught up .,long max offset = null ; for ( map . entry < string @$ extended worker state > state entry : member configs . entry set ( ) ) { long member root offset = state entry . get value ( ) . offset ( ) ; if ( max offset == null ) max offset = member root offset ; else max offset = math . max ( max offset @$ member root offset ) ; } log . debug ( __str__ @$ max offset @$ coordinator . config snapshot ( ) . offset ( ) ) ; return max offset ;,members use,success,pre,configs <SEP> str <SEP> config
we will approve the request @$ which will <PLACE_HOLDER> the entity,variables = new hash map < > ( ) ; variables . put ( __str__ @$ boolean . true ) ; task task = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) @$ variables ) ;,which update,success,pre,variables <SEP> variables <SEP> str <SEP> process <SEP> process <SEP> variables
set max object size a little less than 1024 1024 @$ because the key of the segment result cache is long if set to 1024 1024 will cause memcached client <PLACE_HOLDER> max size error,memcached cache config . set max object size ( __num__ ) ; memcached cache config . set hosts ( config hosts ) ;,client written,fail,pre,memcached <SEP> config <SEP> num <SEP> memcached <SEP> config <SEP> hosts <SEP> config <SEP> hosts
the list of relational values should <PLACE_HOLDER> 2 or more values : the first represents the discriminator the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ plural attribute source . get attribute role ( ) . get full path ( ) ) @$ mapping document . get origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source discriminator type source = new hibernate type source impl ( jaxb many to any mapping . get meta type ( ) ) ; private final relational value source discriminator relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > discriminator value mapping = new hash,list contain,success,pre,sources <SEP> num <SEP> str <SEP> discriminator <SEP> discriminator <SEP> discriminator <SEP> impl <SEP> jaxb <SEP> discriminator <SEP> sources <SEP> num <SEP> discriminator
bg layout state future finished @$ let the ui thread complete layout . countdown happens here if test is successful . otherwise it will happen when the bg thread <PLACE_HOLDER> layout @$ which means ui thread still gets unblocked but assertion will fail .,unlockui thread layout . count down ( ) ;,thread finishes,fail,pre,unlockui
no reports right after we <PLACE_HOLDER> this table .,assert equals ( __num__ @$ get region reports for table ( quota manager . snapshot region sizes ( ) @$ tn ) ) ;,reports read,fail,pre,equals <SEP> num <SEP> reports <SEP> sizes
this loop may <PLACE_HOLDER> an io exception,while ( true ) { int bytes read = as . read ( loaded audio @$ loaded audio byte length @$ int len - loaded audio byte length ) ; if ( bytes read <= __num__ ) { as . close ( ) ; break ; } loaded audio byte length += bytes read ; },loop throw,success,pre,int <SEP> bytes <SEP> int <SEP> bytes <SEP> num <SEP> close <SEP> bytes
check new server <PLACE_HOLDER> listener for reconnected client .,try ( ignite new srv = start grid ( server count ( ) + __num__ ) ) { await partition map exchange ( ) ; lsnr . latch = new count down latch ( __num__ ) ; ignite cache < object @$ object > new srv cache = new srv . cache ( default_cache_name ) ; for ( integer key : primary keys ( new srv cache @$ __num__ ) ) new srv cache . put ( key @$ key ) ; assert true ( lsnr . latch . await ( __num__ @$ milliseconds ) ) ; } cur . close ( ) ;,server register,fail,pre,srv <SEP> num <SEP> lsnr <SEP> num <SEP> srv <SEP> srv <SEP> defaultcachename <SEP> keys <SEP> srv <SEP> num <SEP> srv <SEP> lsnr <SEP> num <SEP> milliseconds <SEP> close
existing <PLACE_HOLDER> id against the caller 's valid list .,if ( tbl == null ) { return null ; } m database mdb = null ; string cat name = tbl . is set cat name ( ) ? tbl . get cat name ( ) : get default catalog ( conf ) ; try { mdb = getm database ( cat name @$ tbl . get db name ( ) ) ; } catch ( no such object exception e ) { log . error ( __str__ @$ e ) ; throw new invalid object exception ( __str__ + database name . get qualified ( cat name @$ tbl . get db name ( ) ) + __str__ ) ; },existing checked,fail,pre,tbl <SEP> mdb <SEP> tbl <SEP> tbl <SEP> catalog <SEP> conf <SEP> mdb <SEP> getm <SEP> tbl <SEP> db <SEP> str <SEP> str <SEP> tbl <SEP> db <SEP> str
check request size will <PLACE_HOLDER> io exception if request is rejected,if ( is large request ( length ) ) { check request size when message received ( length ) ; si . set large request size ( length ) ; },size throw,success,pre,None
the is being initialize @$ <PLACE_HOLDER> the new value .,if ( n . has children ( ) ) { parent . remove child ( n ) ; node value = n . get first child ( ) ; n . remove child ( value ) ; node replacement = ir . assign ( n @$ value ) ; replacement . setjs doc info ( parent . getjs doc info ( ) ) ; replacement . use source info if missing from ( parent ) ; node statement = node util . new expr ( replacement ) ; grandparent . replace child ( parent @$ statement ) ; report code change ( __str__ @$ statement ) ; } else { if ( node util . is statement block ( grandparent ) ) { grandparent . remove child (,the copy,fail,pre,children <SEP> setjs <SEP> getjs <SEP> util <SEP> expr <SEP> str <SEP> util
update last <PLACE_HOLDER> tx id even in case of error @$ since some ops may have been successfully <PLACE_HOLDER> before the error .,last applied tx id = loader . get last applied tx id ( ) ;,update received,fail,pre,last <SEP> tx <SEP> loader <SEP> last <SEP> tx
case . it consists of two arrays of lines . the first array of lines is the test input @$ and the second one is the expected output . if the second array <PLACE_HOLDER> a single element starting with ! ! then it is expected that import orderer will throw a formatter exception with that message . if a line ends with \ then,string [ ] [ ] [ ] inputs outputs = { { { } @$ { } } @$ { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__,array has,success,pre,inputs <SEP> outputs <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str <SEP> str
set up the 2 d array used to hold the names . the first column <PLACE_HOLDER> the olson ids .,string [ ] [ ] result = new string [ available time zone ids . length ] [ __num__ ] ; for ( int i = __num__ ; i < available time zone ids . length ; ++ i ) { result [ i ] [ __num__ ] = available time zone ids [ i ] ; } long native start = system . current time millis ( ) ; fill zone strings ( locale . to string ( ) @$ result ) ; long native end = system . current time millis ( ) ; intern strings ( result ) ;,column contains,success,pre,ids <SEP> num <SEP> int <SEP> num <SEP> ids <SEP> num <SEP> ids <SEP> millis <SEP> strings <SEP> millis <SEP> strings
make sure the env <PLACE_HOLDER> bulk inserts with generated ids ...,if ( ! supports bulk insert id generation ( integer versioned . class ) ) { skip log . report skip ( __str__ @$ __str__ ) ; return ; } session s = open session ( ) ; transaction t = s . begin transaction ( ) ; integer versioned entity = new integer versioned ( __str__ ) ; s . save ( entity ) ; s . create query ( __str__ ) . list ( ) ; t . commit ( ) ; s . close ( ) ; long initial id = entity . get id ( ) ; int initial version = entity . get version ( ) ; s = open session ( ) ; t = s . begin transaction ( ) ;,env supports,success,pre,supports <SEP> versioned <SEP> str <SEP> str <SEP> versioned <SEP> versioned <SEP> str <SEP> str <SEP> close <SEP> int
total dirs <PLACE_HOLDER> root directory,assert equals ( dir count + __num__ @$ total dirs ) ; file status max file = collections . max ( written files . values ( ) @$ new comparator < file status > ( ) { @ override public int compare ( file status first @$ file status second ) { return first . get len ( ) < second . get len ( ) ? - __num__ : ( ( first . get len ( ) == second . get len ( ) ) ? __num__ : __num__ ) ; } } ) ; p = pattern . compile ( __str__ ) ; matcher = p . matcher ( output . to string ( __str__ ) ) ; assert true ( matcher . find ( ),dirs includes,success,pre,equals <SEP> dir <SEP> num <SEP> dirs <SEP> collections <SEP> files <SEP> values <SEP> comparator <SEP> int <SEP> second <SEP> second <SEP> num <SEP> second <SEP> num <SEP> num <SEP> str <SEP> matcher <SEP> matcher <SEP> str <SEP> matcher
no existing match found . <PLACE_HOLDER> a new one .,final list < virtual column > virtual columns = new array list < > ( ) ; if ( input . is direct column access ( ) ) { aggregator factory = new t digest sketch aggregator factory ( agg name @$ input . get direct column ( ) @$ compression ) ; } else { virtual column virtual column = virtual column registry . get or create virtual column for expression ( planner context @$ input @$ sql type name . float ) ; virtual columns . add ( virtual column ) ; aggregator factory = new t digest sketch aggregator factory ( agg name @$ virtual column . get output name ( ) @$ compression ) ; } return aggregation . create ( virtual columns @$,match create,success,pre,columns <SEP> agg <SEP> sql <SEP> columns <SEP> agg <SEP> columns
state change listener <PLACE_HOLDER> final query info and then clears scheduler when the query finishes .,sql query scheduler scheduler = query scheduler . get ( ) ; optional < query info > final query info = state machine . get final query info ( ) ; if ( final query info . is present ( ) ) { return final query info . get ( ) . get query stats ( ) . get total memory reservation ( ) ; } if ( scheduler == null ) { return new data size ( __num__ @$ byte ) ; } return succinct bytes ( scheduler . get total memory reservation ( ) ) ;,listener sets,success,pre,sql <SEP> num <SEP> bytes
normalization will <PLACE_HOLDER> these @$ so no uplevel references left,assert that ( create ( __str__ ) . contains uplevel references ( ) ) . is false ( ) ;,normalization remove,success,pre,str <SEP> contains <SEP> uplevel <SEP> references
now we know that both implementation <PLACE_HOLDER> same status,if ( got status == - __num__ ) { if ( ! got . to string ( ) . equals ( exp . to string ( ) ) ) { errln ( __str__ + refidna name + __str__ + uidna name + __str__ + exp + __str__ + got + __str__ + prettify ( src ) + __str__ + options ) ; } } else { logln ( __str__ + refidna name + __str__ + uidna name + __str__ + prettify ( src ) + __str__ + options ) ; },implementation return,fail,pre,num <SEP> equals <SEP> errln <SEP> str <SEP> refidna <SEP> str <SEP> uidna <SEP> str <SEP> str <SEP> str <SEP> src <SEP> str <SEP> options <SEP> logln <SEP> str <SEP> refidna <SEP> str <SEP> uidna <SEP> str <SEP> src <SEP> str <SEP> options
args can <PLACE_HOLDER> things like location macros @$ so extract any inputs we find .,for ( arg arg : preprocessor flags . get other flags ( ) . get all flags ( ) ) { buildable support . derive inputs ( arg ) . for each ( input consumer ) ; },args contain,success,pre,arg <SEP> arg <SEP> preprocessor <SEP> flags <SEP> flags <SEP> flags <SEP> buildable <SEP> inputs <SEP> arg
date time <PLACE_HOLDER> date,final plan phase effective plan phase = catalog . find phase ( plan phase . get name ( ) @$ effective date @$ last change plan date ) ; return compute usages ( is cancelled or blocked @$ effective plan phase ) ;,time created,fail,pre,catalog <SEP> last <SEP> usages <SEP> cancelled <SEP> blocked
last chance @$ look in the old hive config value . still <PLACE_HOLDER> defaults .,if ( reporters to start == null ) { reporters to start = conf . get ( metastore conf . conf vars . hive_metrics_reporter . get hive name ( ) ) ; if ( reporters to start == null ) { reporters to start = metastore conf . get var ( conf @$ metastore conf . conf vars . metrics_reporters ) ; } },chance need,fail,pre,reporters <SEP> reporters <SEP> conf <SEP> metastore <SEP> conf <SEP> conf <SEP> vars <SEP> hivemetricsreporter <SEP> reporters <SEP> reporters <SEP> metastore <SEP> conf <SEP> var <SEP> conf <SEP> metastore <SEP> conf <SEP> conf <SEP> vars <SEP> metricsreporters
first heartbeat which <PLACE_HOLDER> first resource .,localizer heartbeat response response = spy service . heartbeat ( stat ) ; assert equals ( __str__ @$ localizer action . live @$ response . get localizer action ( ) ) ;,which schedules,success,pre,localizer <SEP> equals <SEP> str <SEP> localizer <SEP> live <SEP> localizer
if multiple threads are <PLACE_HOLDER> the initialization at the same time it is not a problem because they will all get to the same result in the end .,if ( format == null ) { format = logging support . get simple format ( ) ; },threads doing,success,pre,None
value that has been set is new or modified bring the binary in sync so that the deserialization <PLACE_HOLDER> the correct result,if ( offset == modified_indicator_offset ) { update binary represenation ( ) ; offset = this . offsets [ field num ] ; },deserialization gives,success,pre,modifiedindicatoroffset <SEP> represenation <SEP> offsets <SEP> num
partition ca n't <PLACE_HOLDER> this name,reserved partition values . add ( hive conf . get var ( conf @$ conf vars . defaultpartitionname ) ) ; reserved partition values . add ( hive conf . get var ( conf @$ conf vars . default_zookeeper_partition_name ) ) ;,partition use,fail,pre,values <SEP> conf <SEP> var <SEP> conf <SEP> conf <SEP> vars <SEP> defaultpartitionname <SEP> values <SEP> conf <SEP> var <SEP> conf <SEP> conf <SEP> vars <SEP> defaultzookeeperpartitionname
first see if the file <PLACE_HOLDER> the regular expression !,if ( pattern != null ) { matcher matcher = pattern . matcher ( item . get name ( ) . geturi ( ) ) ; get it = matcher . matches ( ) ; } if ( patternexclude != null ) { matcher matcherexclude = patternexclude . matcher ( item . get name ( ) . geturi ( ) ) ; get itexclude = matcherexclude . matches ( ) ; } boolean take = take this file ( item @$ new file name ) ; if ( get it && ! get itexclude && take ) { if ( log . is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ item . get name ( ) .,file matches,success,pre,matcher <SEP> matcher <SEP> matcher <SEP> geturi <SEP> matcher <SEP> matches <SEP> patternexclude <SEP> matcher <SEP> matcherexclude <SEP> patternexclude <SEP> matcher <SEP> geturi <SEP> itexclude <SEP> matcherexclude <SEP> matches <SEP> itexclude <SEP> messages <SEP> str
note : each event loop <PLACE_HOLDER> its own connection pool .,factory with pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( true ) . build ( ) ; factory without pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( false ) . build ( ) ;,loop has,success,pre,pipelining <SEP> http1 <SEP> pipelining <SEP> pipelining <SEP> http1 <SEP> pipelining
fails with primitive classes ; <PLACE_HOLDER> the wrapper class . thanks @$ java .,a = ( e [ ] ) array . new instance ( afclz @$ splits . length ) ;,fails use,fail,pre,afclz <SEP> splits
when errors are not stored alongside values @$ transient errors that are recovered from do not <PLACE_HOLDER> the parent transient,if ( supports transient exceptions ) { assert that error info ( error info ) . is transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is not null ( ) ; } else { assert that error info ( error info ) . is not transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is null ( ) ; },errors affect,fail,pre,supports <SEP> exceptions
skip if this consumer already <PLACE_HOLDER> all the topic partitions it can get,if ( consumer partition count == all subscriptions . get ( consumer ) . size ( ) ) continue ;,consumer has,success,pre,subscriptions
only process if curi <PLACE_HOLDER> evidence of fetch attempt,return uri . contains data key ( a_fetch_began_time ) && uri . get recorder ( ) != null && uri . get recorder ( ) . get response content length ( ) >= get lower bound ( ) && uri . get recorder ( ) . get response content length ( ) <= get upper bound ( ) ;,curi contains,success,pre,contains <SEP> afetchbegantime <SEP> content <SEP> lower <SEP> content
os gi class loaders must <PLACE_HOLDER> bundle reference,final class loader class loader = info . get class loader ( ) ; settings . put ( org . hibernate . cfg . available settings . scanner @$ new osgi scanner ( ( ( bundle reference ) class loader ) . get bundle ( ) ) ) ; osgi class loader . add class loader ( class loader ) ; final class loader prevcl = thread . current thread ( ) . get context class loader ( ) ; try { thread . current thread ( ) . set context class loader ( class loader ) ; return bootstrap . get entity manager factory builder ( info @$ settings @$ new os gi class loader service impl ( osgi class loader @$ osgi service util ) ),loaders follow,fail,pre,loader <SEP> loader <SEP> loader <SEP> settings <SEP> org <SEP> cfg <SEP> settings <SEP> osgi <SEP> loader <SEP> osgi <SEP> loader <SEP> loader <SEP> loader <SEP> loader <SEP> prevcl <SEP> loader <SEP> loader <SEP> loader <SEP> settings <SEP> loader <SEP> impl <SEP> osgi <SEP> loader <SEP> osgi <SEP> util
but our test framework does n't <PLACE_HOLDER> that syntax directly .,session old kathmandu time zone offset session = session . builder ( this . session ) . set time zone key ( time_zone_key ) . set start time ( new date time ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ date_time_zone ) . get millis ( ) ) . build ( ) ; time zone key europe warsaw time zone key = get time zone key ( __str__ ) ; date time zone europe warsaw time zone = get date time zone ( europe warsaw time zone key ) ; session europe warsaw session winter = session . builder ( this . session ) . set time zone key ( europe warsaw time zone key ) . set start time ( new date,framework support,success,pre,kathmandu <SEP> timezonekey <SEP> num <SEP> num <SEP> num <SEP> num <SEP> num <SEP> num <SEP> datetimezone <SEP> millis <SEP> warsaw <SEP> str <SEP> warsaw <SEP> warsaw <SEP> warsaw <SEP> warsaw
check that net flow into a vertex <PLACE_HOLDER> zero @$ except at source and sink,if ( math . abs ( value + excess ( g @$ s ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ s ) ) ; system . err . println ( __str__ + value ) ; return false ; } if ( math . abs ( value - excess ( g @$ t ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ t ) ) ; system . err . println ( __str__ + value ) ; return false ; } for ( int v = __num__ ; v < g . v ( ) ; v ++ ) { if ( v == s || v == t,flow replaces,fail,pre,floatingpointepsilon <SEP> println <SEP> str <SEP> println <SEP> str <SEP> floatingpointepsilon <SEP> println <SEP> str <SEP> println <SEP> str <SEP> int <SEP> num
unknown host exception happens if we ca n't resolve hostname into ip address . unknown host exception 's get message method returns just the hostname which is a useless message @$ so <PLACE_HOLDER> the exception class name to provide more info .,log . debug ( e . to string ( ) ) ; throw new helios exception ( __str__ + uri @$ e ) ; throw new helios exception ( __str__ + uri @$ e ) ;,hostname log,success,pre,helios <SEP> str <SEP> helios <SEP> str
it will show all its tiles . in this case @$ the tiles have to be entered before the container is measured . any change in the tiles @$ should <PLACE_HOLDER> a remeasure .,final int num tiles = m records . size ( ) ; final int width = measure spec . get size ( width measure spec ) ; final int available width = width - get padding start ( ) - get padding end ( ) ; final int height mode = measure spec . get mode ( height measure spec ) ; if ( height mode == measure spec . unspecified ) { m rows = ( num tiles + m columns - __num__ ) / m columns ; } m cell width = ( available width - m side padding * __num__ - ( m cell margin horizontal * m columns ) ) / m columns ;,case cause,fail,pre,int <SEP> num <SEP> tiles <SEP> records <SEP> int <SEP> int <SEP> int <SEP> rows <SEP> num <SEP> tiles <SEP> columns <SEP> num <SEP> columns <SEP> num <SEP> columns <SEP> columns
setup the configurator to inject the pool config in the message driven component <PLACE_HOLDER> service,final message driven component description mdb component description = ( message driven component description ) mdb component configuration . get component description ( ) ; mdb component configuration . get create dependencies ( ) . add ( new pool injecting configurator ( mdb component description ) ) ;,component create,success,pre,mdb <SEP> mdb <SEP> mdb <SEP> dependencies <SEP> injecting <SEP> configurator <SEP> mdb
class we must guarantee that only one thread at a time <PLACE_HOLDER> our number format .,synchronized ( number format ) { return ( number format ) number format . clone ( ) ; },thread update,fail,pre,synchronized
there are no regions @$ or no samples available . just <PLACE_HOLDER> the entire range .,if ( sample row keys . is empty ( ) ) { log . info ( __str__ @$ this ) ; return collections . singleton list ( this ) ; } log . info ( __str__ @$ desired bundle size bytes @$ sample row keys . size ( ) @$ sample row keys . get ( __num__ ) ) ; immutable list . builder < bigtable source > splits = immutable list . builder ( ) ; for ( byte key range range : ranges ) { splits . add all ( split range based on samples ( desired bundle size bytes @$ sample row keys @$ range ) ) ; } return splits . build ( ) ;,samples return,fail,pre,row <SEP> keys <SEP> str <SEP> collections <SEP> str <SEP> desired <SEP> bytes <SEP> row <SEP> keys <SEP> row <SEP> keys <SEP> num <SEP> bigtable <SEP> splits <SEP> ranges <SEP> splits <SEP> samples <SEP> desired <SEP> bytes <SEP> row <SEP> keys <SEP> splits
add all <PLACE_HOLDER> cells that need to be inserted after this one .,add spanning cells ( ) ;,all skipped,fail,pre,spanning <SEP> cells
do n't add trust anchors if not already present @$ the builder will <PLACE_HOLDER> the anchors from its parent @$ and that 's where the trust anchors should be added .,if ( ! builder . has certificates entry refs ( ) ) { return ; } builder . add certificates entry refs ( debug config builder . get certificates entry refs ( ) ) ;,builder read,fail,pre,certificates <SEP> refs <SEP> certificates <SEP> refs <SEP> config <SEP> certificates <SEP> refs
this is called only if the peer <PLACE_HOLDER> junior status,this . seeddb . add potential ( peer ) ;,peer has,success,pre,seeddb
check can not <PLACE_HOLDER> tables that already exist .,response = client . post ( namespace path3 @$ null headers @$ new byte [ ] { } ) ; assert equals ( __num__ @$ response . get code ( ) ) ; response = client . post ( namespace path4 @$ constants . mimetype_protobuf @$ model4 . create protobuf output ( ) ) ; assert equals ( __num__ @$ response . get code ( ) ) ;,check create,fail,pre,namespace <SEP> path3 <SEP> headers <SEP> equals <SEP> num <SEP> namespace <SEP> path4 <SEP> constants <SEP> mimetypeprotobuf <SEP> model4 <SEP> protobuf <SEP> equals <SEP> num
check row <PLACE_HOLDER> comparator to check we are writing in order .,if ( ! check row ( cell ) ) { if ( start offset < __num__ ) { start offset = out . size ( ) ; } rows offsetbaos . write int ( out . size ( ) - start offset ) ; } last cell = cell ; return encoder . write ( cell ) ;,row uses,success,pre,row <SEP> num <SEP> rows <SEP> offsetbaos <SEP> int <SEP> last <SEP> encoder
as we only support limited concurrent checkpoints @$ after checkpoint <PLACE_HOLDER> more than the limits @$ the current periodic trigger would been assigned as null .,checkpoint coordinator . trigger checkpoint ( system . current time millis ( ) @$ false ) ; assert false ( checkpoint coordinator . is current periodic trigger available ( ) ) ; assert equals ( max concurrent checkpoints @$ checkpoint coordinator . get number of pending checkpoints ( ) ) ; checkpoint coordinator . abort pending checkpoints ( new checkpoint exception ( checkpoint failure reason . job_failover_region ) ) ;,checkpoint hits,fail,pre,millis <SEP> equals <SEP> checkpoints <SEP> checkpoints <SEP> checkpoints <SEP> jobfailoverregion
set text may <PLACE_HOLDER> the given text with spanned string . check the contents by casting to string .,try { tv . get text ( ) ; fail ( ) ; } catch ( illegal argument exception e ) { },text wrap,success,pre,None
if the caller did n't <PLACE_HOLDER> an explicit time tracker @$ we want to continue tracking under any it has .,if ( r . app time tracker == null && source record != null ) { r . app time tracker = source record . app time tracker ; },caller specify,success,pre,None
zap : <PLACE_HOLDER> the statement .,extension hook . get hook view ( ) . add option panel ( get options database panel ( ) ) ; extension hook . get hook view ( ) . add option panel ( get options jvm panel ( ) ) ;,zap added,success,pre,options <SEP> options <SEP> jvm
the message indicated some error trying to start : do call <PLACE_HOLDER> stop keepalive .,handle stop keepalive ( nai @$ slot @$ reason ) ;,call handle,success,pre,keepalive
we can know the size of the iterable . <PLACE_HOLDER> an encoding with a leading size field @$ followed by that many elements .,if ( iterable instanceof collection ) { collection < t > collection = ( collection < t > ) iterable ; observer . update ( __num__ ) ; for ( t elem : collection ) { element coder . register byte size observer ( elem @$ observer ) ; } } else { observer . update ( __num__ ) ; long count = __num__ ; for ( t elem : iterable ) { count += __num__ ; element coder . register byte size observer ( elem @$ observer ) ; } if ( count > __num__ ) { observer . update ( var int . get length ( count ) ) ; } observer . update ( __num__ ) ; },size use,success,pre,iterable <SEP> instanceof <SEP> iterable <SEP> num <SEP> elem <SEP> coder <SEP> elem <SEP> num <SEP> num <SEP> elem <SEP> iterable <SEP> num <SEP> coder <SEP> elem <SEP> num <SEP> var <SEP> int <SEP> num
test case 8 : 2 components @$ 2 instances for each comp 2 already finished . comp 1 <PLACE_HOLDER> a new instance finish @$ we should terminate the service,comp = create component ( service scheduler @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component1 instances = comp . get all component instances ( ) ; container status . set exit status ( - __num__ ) ; component comp2 = create component ( component instance . get component ( ) . get scheduler ( ) @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component2 instances = comp2 . get all component instances ( ) ; map <,components has,success,pre,org <SEP> hadoop <SEP> records <SEP> enum <SEP> num <SEP> num <SEP> component1 <SEP> instances <SEP> instances <SEP> num <SEP> comp2 <SEP> org <SEP> hadoop <SEP> records <SEP> enum <SEP> num <SEP> num <SEP> component2 <SEP> instances <SEP> comp2 <SEP> instances
note : parse offset pattern will <PLACE_HOLDER> the given pattern and throws illegal argument exception when pattern is not valid,object [ ] parsed items = parse offset pattern ( gmt offset patterns [ idx ] @$ t . required ( ) ) ; gmt offset pattern items [ idx ] = parsed items ;,pattern parse,fail,pre,parsed <SEP> items <SEP> patterns <SEP> idx <SEP> required <SEP> items <SEP> idx <SEP> parsed <SEP> items
create 100 threads @$ each will <PLACE_HOLDER> its own puts,putter [ ] all = new putter [ threads100 ] ;,each get,fail,pre,threads100
if maps @$ recursively <PLACE_HOLDER> the key and value types,if ( c1 . equals ( category . map ) ) { map object inspector mapoi1 = ( map object inspector ) o1 ; map object inspector mapoi2 = ( map object inspector ) o2 ; object inspector child key1 = mapoi1 . get map key object inspector ( ) ; object inspector child key2 = mapoi2 . get map key object inspector ( ) ; if ( compare types ( child key1 @$ child key2 ) ) { object inspector child val1 = mapoi1 . get map value object inspector ( ) ; object inspector child val2 = mapoi2 . get map value object inspector ( ) ; if ( compare types ( child val1 @$ child val2 ) ) { return true ; } } return,maps compare,success,pre,c1 <SEP> equals <SEP> mapoi1 <SEP> o1 <SEP> mapoi2 <SEP> o2 <SEP> key1 <SEP> mapoi1 <SEP> key2 <SEP> mapoi2 <SEP> types <SEP> key1 <SEP> key2 <SEP> val1 <SEP> mapoi1 <SEP> val2 <SEP> mapoi2 <SEP> types <SEP> val1 <SEP> val2
request did n't succeed because the token was revoked so we invalidate the token stored in the session and render the index page so that the user can <PLACE_HOLDER> the o auth flow again,if ( res2 . failed ( ) ) { ctx . session ( ) . destroy ( ) ; ctx . fail ( res2 . cause ( ) ) ; } else { user info . put ( __str__ @$ res2 . result ( ) . json array ( ) ) ; json object data = new json object ( ) . put ( __str__ @$ user info ) ; engine . render ( data @$ __str__ @$ res3 -> { if ( res3 . succeeded ( ) ) { ctx . response ( ) . put header ( __str__ @$ __str__ ) . end ( res3 . result ( ) ) ; } else { ctx . fail ( res3 . cause ( ) ) ; },user start,success,pre,res2 <SEP> ctx <SEP> ctx <SEP> res2 <SEP> str <SEP> res2 <SEP> json <SEP> json <SEP> json <SEP> str <SEP> str <SEP> res3 <SEP> res3 <SEP> succeeded <SEP> ctx <SEP> str <SEP> str <SEP> res3 <SEP> ctx <SEP> res3
do we need to pad the file so the stripe does n't <PLACE_HOLDER> a block boundary ?,long start = raw writer . get bytes written ( ) ; final long current stripe size = index size + data size + footer . get serialized size ( ) ; final long available = block size - ( start % block size ) ; final long overflow = current stripe size - adjusted stripe size ; final float avail ratio = ( float ) available / ( float ) default stripe size ; if ( avail ratio > __num__ && avail ratio < __num__ && avail ratio > padding tolerance ) { float correction = overflow > __num__ ? ( float ) overflow / ( float ) adjusted stripe size : __num__ ; correction = correction > padding tolerance ? padding tolerance : correction ; adjusted,stripe exceed,fail,pre,bytes <SEP> serialized <SEP> adjusted <SEP> num <SEP> num <SEP> num <SEP> adjusted <SEP> num <SEP> adjusted
center inside @$ but bitmap bounds <PLACE_HOLDER> the resize dimensions ... so change it to fit center .,if ( scale mode == scale mode . center inside ) { if ( resize width <= b . get width ( ) || resize height <= b . get height ( ) ) scale mode = scale mode . fit center ; },inside change,fail,pre,None
writing will always <PLACE_HOLDER> full date string,string time zone = new simple date format ( __str__ ) . format ( date ) ; assert equals ( __str__ + time zone @$ formatted ) ; assert equals ( date @$ result ) ;,writing return,fail,pre,str <SEP> equals <SEP> str <SEP> formatted <SEP> equals
the number of <PLACE_HOLDER> bytes,int got = skipped + cis . read ( result @$ __num__ @$ __num__ ) ;,number got,success,pre,int <SEP> skipped <SEP> num <SEP> num
default generator does n't <PLACE_HOLDER> any mechanism for emitting supporting files other than by a mustache template @$ so we 're obliged to serialize the caches to json strings and use templates to write them .,if ( load test data from file ) { try { if ( test data cache . root ( ) . is dirty ( ) ) { byte array output stream out = new byte array output stream ( ) ; test data cache . root ( ) . flush ( out ) ; string test data json = new string ( out . to byte array ( ) @$ __str__ ) ; objs . put ( __str__ @$ test data json ) ; supporting files . add ( new supporting file ( __str__ @$ test data file . get absolute path ( ) ) ) ; } } catch ( cache exception | unsupported encoding exception e ) { logger . error ( __str__ + test data,generator provide,success,pre,json <SEP> str <SEP> objs <SEP> str <SEP> json <SEP> files <SEP> str <SEP> encoding <SEP> str
reading the merged bag <PLACE_HOLDER> both the contents,assert that ( bag1 . read ( ) @$ contains in any order ( __str__ @$ __str__ @$ __str__ ) ) ; assert that ( bag2 . read ( ) @$ matchers . empty iterable ( ) ) ;,bag gets,success,pre,bag1 <SEP> contains <SEP> str <SEP> str <SEP> str <SEP> bag2 <SEP> matchers <SEP> iterable
if the size of bloom filter is smaller than 1 mb @$ use default <PLACE_HOLDER> false positive probability,if ( num bits required <= max num bits ) { return default max false pos probability ; },default max,success,pre,num <SEP> bits <SEP> required <SEP> num <SEP> bits <SEP> pos
ensure that per rfc 6265 @$ cookie.value <PLACE_HOLDER> syntax rules,syntax . require validrfc6265 cookie value ( _value ) ;,cookie.value follows,success,pre,validrfc6265
execute process does n't wait for finishing to drain error stream if it 's configure not to redirect stream . this causes test failure when draining the error stream did n't finish fast enough before the thread of this test case method <PLACE_HOLDER> the warn msg count . so @$ this loop wait for a while until the log msg count becomes expected number,final int expected warning messages = __num__ ; final int max retry = __num__ ; for ( int i = __num__ ; i < max retry && ( runner . get logger ( ) . get warn messages ( ) . size ( ) < expected warning messages ) ; i ++ ) { try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { } } final list < log message > warn messages = runner . get logger ( ) . get warn messages ( ) ; assert equals ( __str__ + __str__ @$ expected warning messages @$ warn messages . size ( ) ) ; final list < mock flow file > succeeded = runner . get flow files,thread reports,fail,pre,int <SEP> messages <SEP> num <SEP> int <SEP> num <SEP> int <SEP> num <SEP> messages <SEP> messages <SEP> num <SEP> interrupted <SEP> messages <SEP> messages <SEP> equals <SEP> str <SEP> str <SEP> messages <SEP> messages <SEP> succeeded <SEP> files
