peer 2 <PLACE_HOLDER> the tx but does not receive it yet .,inbound ( p2 @$ inv ) ; assert true ( outbound ( p2 ) instanceof get data message ) ; assert equals ( __num__ @$ tx . get confidence ( ) . num broadcast peers ( ) ) ; assert null ( event [ __num__ ] ) ;,peer sent,fail,pre
when partition could not be fetched from metastore @$ it is not known whether the partition was added . deleting the partition when aborting commit <PLACE_HOLDER> the risk of deleting partition not added in this transaction . not deleting the partition may leave garbage behind . the former is much more dangerous than the latter . therefore @$ the partition is not added to,batch completely added = false ;,commit has,success,pre
if we have parents <PLACE_HOLDER> their transforms,while ( parent node != null ) { if ( parent == current spatial ) { transform trans = new transform ( ) ; trans . set scale ( current spatial . get local scale ( ) ) ; shape transform . combine with parent ( trans ) ; parent node = null ; } else { shape transform . combine with parent ( current spatial . get local transform ( ) ) ; parent node = current spatial . get parent ( ) ; current spatial = parent node ; } },parents apply,fail,pre
do n't allow partitioned tables in subqueries . this restriction stems from the lack of confidence that the planner can reliably identify all cases of adequate and inadequate partition key <PLACE_HOLDER> criteria across different levels of correlated subqueries .,if ( ! m_partitioning . was specified as single ( ) ) { for ( abstract expression e : subquery exprs ) { assert ( e instanceof select subquery expression ) ; select subquery expression sub expr = ( select subquery expression ) e ; if ( ! sub expr . get subquery scan ( ) . get is replicated ( ) ) { m_recent error msg = in_exists_scalar_error_message ; return null ; } } },cases trim,fail,pre
bn <PLACE_HOLDER> a trailing slash : the web sphere class loader would return null for a raw directory name without it .,try { enumeration < url > urls = root . get resources ( bn ) ; if ( urls == null ) { return null ; } url visitor v = new url visitor ( ) { @ override public void visit ( string s ) { if ( s . ends with ( __str__ ) ) { string locstr = s . substring ( __num__ @$ s . length ( ) - __num__ ) ; names . add ( locstr ) ; } } } ; while ( urls . has more elements ( ) ) { url url = urls . next element ( ) ; url handler handler = url handler . get ( url ) ; if ( handler != null ) { handler,bn has,success,pre
if this project has <PLACE_HOLDER> reference @$ create value generator and produce the <PLACE_HOLDER> variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel ) ; },project correlated,success,pre
when visibility changes and the user <PLACE_HOLDER> a tab selected @$ unselect it and make sure their callback gets called .,if ( changed view == this && visibility != visible && m grabbed state != on trigger listener . no_handle ) { cancel grab ( ) ; },changes has,success,pre
match the container @$ to reduce the risk of issues . the preview should never be drawn while the surface <PLACE_HOLDER> this size .,if ( surface rect == null ) { surface view . layout ( __num__ @$ __num__ @$ get width ( ) @$ get height ( ) ) ; } else { surface view . layout ( surface rect . left @$ surface rect . top @$ surface rect . right @$ surface rect . bottom ) ; },surface has,success,pre
a window frame may <PLACE_HOLDER> just the start boundary or in the between style of expressing a window frame both boundaries are specified .,boundary spec start = process boundary ( ( ast node ) node . get child ( __num__ ) ) ; if ( node . get child count ( ) > __num__ ) { end = process boundary ( ( ast node ) node . get child ( __num__ ) ) ; },frame have,fail,pre
create a walker which <PLACE_HOLDER> the tree in a dfs manner while maintaining the operator stack .,map < rule @$ node processor > op rules = new linked hash map < rule @$ node processor > ( ) ; op rules . put ( new rule reg exp ( __str__ @$ app master event operator . get operator name ( ) + __str__ ) @$ new remove dynamic pruning by size ( ) ) ;,which walks,success,pre
see if the avro schema has any fields that are n't in the record schema @$ and if those fields <PLACE_HOLDER> a default value then we want to populate it in the generic record being produced,for ( final field field : avro schema . get fields ( ) ) { final optional < record field > record field = record schema . get field ( field . name ( ) ) ; if ( ! record field . is present ( ) && rec . get ( field . name ( ) ) == null && field . default val ( ) != null ) { rec . put ( field . name ( ) @$ field . default val ( ) ) ; } } return rec ;,fields have,success,pre
zip files <PLACE_HOLDER> 2 second resolution .,cal . set ( calendar . second @$ rand . next int ( __num__ ) * __num__ ) ;,files require,fail,pre
we 'll never have two dots nor will a type name end or begin with dot . so no need to <PLACE_HOLDER> dollars at the beginning @$ end @$ or adjacent to dots .,if ( index == __num__ || index == class name . length ( ) - __num__ || current . char at ( index - __num__ ) == __str__ || current . char at ( index + __num__ ) == __str__ ) { search start = index + __num__ ; continue ; } return index ;,need prefer,fail,pre
when layout is frozen @$ rv does not intercept the motion event . a child view e.g . a button may still <PLACE_HOLDER> the click .,if ( m layout frozen ) { return false ; },button get,success,pre
if execute long is used for the initial execution of the node and the uninitialized case is not checked then this execute long method might <PLACE_HOLDER> 0 l instead of 2 l. this test verifies that this particular case does not happen .,assert . assert equals ( __num__ @$ node . execute long ( truffle . get runtime ( ) . create virtual frame ( new object [ ] { __num__ } @$ new frame descriptor ( ) ) ) ) ;,method return,success,pre
table may or may not be <PLACE_HOLDER> metastore . only the ser de can tell us .,abstract ser de deserializer = null ; try { class < ? > clazz = conf . get class by name ( serde lib ) ; if ( ! abstract ser de . class . is assignable from ( clazz ) ) { return true ; } deserializer = reflection util . new instance ( conf . get class by name ( serde lib ) . as subclass ( abstract ser de . class ) @$ conf ) ; } catch ( exception ex ) { log . warn ( __str__ + serde lib + __str__ @$ ex ) ; return true ; },table using,success,pre
get base xml <PLACE_HOLDER> result set holder,j2d analyzer . read results ( basexml file name ) ; j2d analyzer . single result set holder basesrsh = ( j2d analyzer . single result set holder ) j2d analyzer . results . element at ( __num__ ) ; enumeration base enum_ = basesrsh . get key enumeration ( ) ; vector base keyvector = new vector ( ) ; while ( base enum_ . has more elements ( ) ) { base keyvector . add ( base enum_ . next element ( ) ) ; } string base keys [ ] = new string [ base keyvector . size ( ) ] ; base keyvector . copy into ( base keys ) ; j2d analyzer . sort ( base keys ) ;,xml based,fail,pre
client can still <PLACE_HOLDER> data even though server is closed ? ? ?,client . get output stream ( ) . write ( __num__ ) ;,client write,success,pre
gathering geometries in the sub graph . this must be done in the update phase as the gathering might <PLACE_HOLDER> a matparam override,targets . clear ( ) ; this . spatial . depth first traversal ( target locator ) ;,gathering have,fail,pre
if the task has no requested minimal size @$ we 'd like to enforce a minimal size so that the user can not <PLACE_HOLDER> the task too small to manipulate . we do n't need to do this for the pinned stack as the bounds are controlled by the system .,if ( ! in pinned windowing mode ( ) && m stack != null ) { final int default min size dp = m service . m root activity container . m default min size of resizeable task dp ; final activity display display = m service . m root activity container . get activity display ( m stack . m display id ) ; final float density = ( float ) display . get configuration ( ) . density dpi / display metrics . density_default ; final int default min size = ( int ) ( default min size dp * density ) ; if ( min width == invalid_min_size ) { min width = default min size ; } if ( min height == invalid_min_size ),user grab,fail,pre
set up an empty panel to be used as a gui place holder when a conflict in another merger already <PLACE_HOLDER> a choice that resolved the conflict .,empty conflict panel = new vertical choices panel ( ) ; empty conflict panel . clear ( ) ;,conflict views,fail,pre
only map <PLACE_HOLDER> ref and relationship attr when follow references is set to true,if ( entity with ext info == null ) { entity with ext info = entity graph retriever . to atlas entity with ext info ( guid @$ ! follow references ) ; if ( entity with ext info != null ) { context . cache ( entity with ext info ) ; if ( log . is debug enabled ( ) ) { log . debug ( __str__ @$ guid ) ; } } },map has,fail,pre
intern @$ as most entities have multiple surface forms <PLACE_HOLDER> the entry,dictionary . put ( surface form @$ link ) ; i += __num__ ;,forms add,success,pre
test that the module map action <PLACE_HOLDER> the header tree artifact as both the public header and part of the action inputs .,assert that ( module map action . get public headers ( ) ) . contains ( headers ) ; assert that ( module map action . get inputs ( ) ) . contains ( headers ) ; action execution context dummy action execution context = new action execution context ( null @$ null @$ action input prefetcher . none @$ action key context @$ null @$ null @$ null @$ immutable map . of ( ) @$ immutable map . of ( ) @$ dummy_artifact_expander @$ null @$ null ) ; byte array output stream module map stream = new byte array output stream ( ) ; byte array output stream umbrella header stream = new byte array output stream ( ) ; module map action . new,action has,fail,pre
blob <PLACE_HOLDER> integer,assert equals ( constants . sqlite_blob @$ st . column_type ( __num__ ) ) ;,blob contains,fail,pre
count timer <PLACE_HOLDER> events,int timer fired count = __num__ ; list < flowable event > events received = listener . get events received ( ) ; for ( flowable event event received : events received ) { if ( flowable engine event type . timer_fired == event received . get type ( ) ) { timer fired count ++ ; } } listener . clear events received ( ) ; assert equals ( __num__ @$ timer fired count ) ; process engine configuration . reset clock ( ) ;,timer fired,success,pre
maven <PLACE_HOLDER> empty tags into null entries @$ and its possible to have empty tags in jib.to.tags,session properties . put ( __str__ @$ __str__ ) ; try { test plugin configuration . get target image additional tags ( ) ; assert . fail ( ) ; } catch ( illegal argument exception ex ) { assert . assert equals ( __str__ @$ ex . get message ( ) ) ; },maven converts,fail,pre
this is used to set data type @$ which <PLACE_HOLDER> a python tuple of classes,string full suffix = suffix ; if ( __str__ . equals ( suffix ) ) { full suffix = __str__ + suffix ; } if ( model utils . is nullable ( p ) ) { full suffix = __str__ + suffix ; } if ( model utils . is free form object ( p ) && model utils . get additional properties ( p ) == null ) { return prefix + __str__ + full suffix ; } if ( ( model utils . is map schema ( p ) || p . get type ( ) == __str__ ) && model utils . get additional properties ( p ) != null ) { schema inner = model utils . get additional properties ( p ) ;,which creates,fail,pre
the code is written so that even completely incorrect approximations will still <PLACE_HOLDER> the correct answer eventually @$ but in practice this branch should almost never be entered @$ and even then the loop should not run more than once .,if ( approx cmp > __num__ ) { do { approx log10 -- ; approx pow = approx pow . divide ( big integer . ten ) ; approx cmp = approx pow . compare to ( x ) ; } while ( approx cmp > __num__ ) ; } else { big integer next pow = big integer . ten . multiply ( approx pow ) ; int next cmp = next pow . compare to ( x ) ; while ( next cmp <= __num__ ) { approx log10 ++ ; approx pow = next pow ; approx cmp = next cmp ; next pow = big integer . ten . multiply ( approx pow ) ; next cmp = next pow . compare to (,approximations give,fail,pre
no awb <PLACE_HOLDER> supported ? that 's unpossible !,if ( awb avail == null || awb avail . size ( ) == __num__ ) { log . w ( tag @$ __str__ ) ; awb avail = new array list < integer > ( __num__ ) ; awb avail . add ( control_awb_mode_auto ) ; },awb modes,success,pre
set the text to an invalid value @$ which will <PLACE_HOLDER> an exception and an error message,set offset ( offset @$ __str__ ) ; check offset fields enabled state ( info fields @$ true ) ; check offset fields enabled state ( offset buttons @$ false ) ;,which cause,fail,pre
if the table <PLACE_HOLDER> property external set @$ update table type accordingly,string table type = tbl . get table type ( ) ; boolean is external = boolean . parse boolean ( tbl . get parameters ( ) . get ( __str__ ) ) ; if ( table type . managed_table . to string ( ) . equals ( table type ) ) { if ( is external ) { table type = table type . external_table . to string ( ) ; } } if ( table type . external_table . to string ( ) . equals ( table type ) ) { if ( ! is external ) { table type = table type . managed_table . to string ( ) ; } } principal type owner principal type = tbl . get owner type ( ),table has,success,pre
first line : sentence index for all nodes ; we recover the words from the original tokens the first two tokens in this line <PLACE_HOLDER> : docid @$ sentence index,for ( indexed word node : graph . vertex set ( ) ) { if ( ! output header ) { string doc id = node . get ( core annotations . docid annotation . class ) ; if ( doc id != null && doc id . length ( ) > __num__ ) pw . print ( doc id ) ; else pw . print ( __str__ ) ; pw . print ( __str__ ) ; pw . print ( node . get ( core annotations . sentence index annotation . class ) ) ; output header = true ; } pw . print ( __str__ ) ; pw . print ( node . index ( ) ) ; if ( node . copy count ( ),tokens contains,fail,pre
server has <PLACE_HOLDER> data since last check,if ( last bytes written . compare and set ( previous bytes @$ current bytes ) ) { return false ; },server sent,success,pre
we are only running remotely if both the distribution is there and if the distribution is actually <PLACE_HOLDER> something .,clustered partitioning = trans meta . get slave step copy partition distribution ( ) != null && ! trans meta . get slave step copy partition distribution ( ) . get distribution ( ) . is empty ( ) ;,distribution doing,fail,pre
to change body of generated methods @$ choose tools <PLACE_HOLDER> templates .,return super . load symlinks ( ) ;,tools |,success,pre
composition <PLACE_HOLDER> one end to be the container .,if ( relationship category == relationship category . composition ) { throw new atlas base exception ( atlas error code . relationshipdef_composition_no_container @$ name ) ; } else if ( relationship category == relationship category . aggregation ) { throw new atlas base exception ( atlas error code . relationshipdef_aggregation_no_container @$ name ) ; },composition needs,success,pre
thread polling every 5 seconds to <PLACE_HOLDER> the word set seconds which is used in filter words bolt to filter the words,try { if ( ! poll ) { word set = parse file ( file name ) ; poll time = system . current time millis ( ) ; poll = true ; } else { if ( ( system . current time millis ( ) - poll time ) > __num__ ) { word set = parse file ( file name ) ; poll time = system . current time millis ( ) ; } } } catch ( io exception exp ) { throw new runtime exception ( exp ) ; } if ( word set != null && ! word set . contains ( word ) ) { collector . emit ( new values ( word ) ) ; },seconds reset,fail,pre
rows to <PLACE_HOLDER> line,label wl rows to skip = new label ( shell @$ swt . right ) ; wl rows to skip . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl rows to skip ) ; form data fdl rows to skip = new form data ( ) ; fdl rows to skip = new form data ( ) ; fdl rows to skip . left = new form attachment ( __num__ @$ __num__ ) ; fdl rows to skip . top = new form attachment ( last control @$ margin ) ; fdl rows to skip . right = new form attachment ( middle @$ - margin ) ; wl rows to skip . set layout,rows skip,success,pre
use the messages hostname as default domain when generating <PLACE_HOLDER> cookies,for ( http cookie c : msg . get response header ( ) . get http cookies ( msg . get request header ( ) . get host name ( ) ) ) session . get http state ( ) . add cookie ( convert cookie ( c ) ) ; return session ;,generating expected,fail,pre
original <PLACE_HOLDER> changed @$ but snapshot still <PLACE_HOLDER> old acl .,do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ; restart ( false ) ; do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ; restart ( true ) ; do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ;,snapshot has,success,pre
unix domain socket address . <PLACE_HOLDER> the underlying file for the unix domain socket .,if ( value . starts with ( unix_domain_socket_prefix ) ) { string file path = value . substring ( unix_domain_socket_prefix . length ( ) ) ; file file = new file ( file path ) ; if ( ! file . is absolute ( ) ) { throw new illegal argument exception ( __str__ + file path ) ; } try { if ( file . create new file ( ) ) { file . delete on exit ( ) ; } } catch ( io exception ex ) { throw new runtime exception ( ex ) ; } return new domain socket address ( file ) ; } else { string [ ] parts = value . split ( __str__ @$ __num__ ) ; if ( parts,address create,success,pre
new file @$ should <PLACE_HOLDER> name,string r = test col . get media ( ) . add file ( path ) ; assert equals ( __str__ @$ r ) ;,new have,fail,pre
since b and c <PLACE_HOLDER> the same score @$ we can expect them to appear in either order,assertion . has table section ( ) . has column ( __str__ ) . contains exactly ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,b have,success,pre
schedule low pri first . when high pri is scheduled @$ it takes away the duck from the low pri task . when the high pri finishes @$ low pri <PLACE_HOLDER> the duck back .,try { priority high pri = priority . new instance ( __num__ ) @$ low pri = priority . new instance ( __num__ ) ; tez task attemptid task1 = test task scheduler service wrapper . generate task attempt id ( ) @$ task2 = test task scheduler service wrapper . generate task attempt id ( ) ; ts wrapper . ts . update guaranteed count ( __num__ ) ; ts wrapper . control scheduler ( true ) ; ts wrapper . allocate task ( task1 @$ null @$ low pri @$ new object ( ) ) ; ts wrapper . await total task allocations ( __num__ ) ; task info ti1 = ts wrapper . ts . get task info ( task1 ) ; assert true (,pri puts,fail,pre
the full name should not <PLACE_HOLDER> the organization folder name,map links ; for ( map map : pipelines ) { assert . assert equals ( __str__ @$ map . get ( __str__ ) ) ; if ( map . get ( __str__ ) . equals ( __str__ ) ) { map . get ( __str__ ) . equals ( __str__ ) ; map . get ( __str__ ) . equals ( __str__ ) ; check links ( ( map ) map . get ( __str__ ) @$ __str__ ) ; } else if ( map . get ( __str__ ) . equals ( __str__ ) ) { map . get ( __str__ ) . equals ( __str__ ) ; map . get ( __str__ ) . equals ( __str__ ) ; check links ( ( map ),name match,fail,pre
distributed system.disconnect may have already <PLACE_HOLDER> the dls,distributed lock service . destroy ( abstract gateway sender . lock_service_name ) ;,system.disconnect destroyed,success,pre
begin @$ do and if increases <PLACE_HOLDER> depth,if ( ( __str__ . equals ( keyword . get text ( ) ) || __str__ . equals ( keyword . get text ( ) ) || __str__ . equals ( keyword . get text ( ) ) && ! __str__ . equals ( previous keyword . get text ( ) ) ) ) { context . increase block depth ( ) ; } else if ( __str__ . equals ( keyword . get text ( ) ) ) { context . decrease block depth ( ) ; },increases block,success,pre
rvv does not <PLACE_HOLDER> this entry so it is retained,result . add ( id ) ;,rvv have,fail,pre
the added node <PLACE_HOLDER> a new group .,return true ;,node adds,success,pre
nobody <PLACE_HOLDER> this value,throw new web service exception ( __str__ + lexical ) ;,nobody uses,fail,pre
normalization will <PLACE_HOLDER> these @$ so no uplevel references left,assert that ( create ( __str__ ) . contains uplevel references ( ) ) . is false ( ) ;,normalization remove,success,pre
update the switches <PLACE_HOLDER> the km 200 switch program service,e service . update errors ( node root ) ;,switches insode,success,pre
id types can <PLACE_HOLDER> int literals .,if ( type == scalars . graphqlid && string value . matches ( __str__ ) ) { return int value . new int value ( ) . value ( new big integer ( string value ) ) . build ( ) ; },types contain,fail,pre
without the metadata the status and health check ur ls will not be <PLACE_HOLDER> and the status page and health check url paths will not include the context path so <PLACE_HOLDER> them here,if ( string utils . has text ( management context path ) ) { instance . set health check url path ( management context path + instance . get health check url path ( ) ) ; instance . set status page url path ( management context path + instance . get status page url path ( ) ) ; },here set,success,pre
quick answers <PLACE_HOLDER> the js api .,return contextual search field trial . is quick answers enabled ( ) ;,answers needs,fail,pre
legacy security behavior : setup the security context if a sas current is available and invoke the component . one of the ejb security interceptors will <PLACE_HOLDER> and authorize the client .,security context legacy context = null ; if ( this . legacy security domain != null && ( identity principal != null || principal != null ) ) { final object final credential = identity principal != null ? this . sas current : credential ; final principal final principal = identity principal != null ? identity principal : principal ; if ( wild fly security manager . is checking ( ) ) { legacy context = access controller . do privileged ( ( privileged exception action < security context > ) ( ) -> { security context sc = security context factory . create security context ( this . legacy security domain ) ; sc . get util ( ) . create subject info ( final principal,behavior find,fail,pre
case insense . should <PLACE_HOLDER> root @$ html @$ body,elements p2 = doc . select ( __str__ ) ;,case have,fail,pre
note that this class does not <PLACE_HOLDER> forward mode .,if ( ! forward ) { if ( trusted pub key != null ) { prev pub key = trusted pub key ; } else { prev pub key = null ; } } else { throw new cert path validator exception ( __str__ ) ; },class support,success,pre
if the current wi is a quantity @$ we add it to the collector . if its the first word in a quantity @$ we <PLACE_HOLDER> index before it,if ( quantifiable . contains ( curr ner tag ) ) { if ( collector . is empty ( ) ) { before index = i - __num__ ; } collector . add ( wi ) ; } prev ner tag = curr ner tag ;,word add,fail,pre
server and cluster <PLACE_HOLDER> the same value,cluster props . clear ( ) ; server props . clear ( ) ; cluster props . set property ( __str__ @$ __str__ ) ; server props . set property ( __str__ @$ __str__ ) ; assert that ( gem fire cache impl . is mis configured ( cluster props @$ server props @$ __str__ ) ) . is false ( ) ; cluster props . set property ( __str__ @$ __str__ ) ; server props . set property ( __str__ @$ __str__ ) ; assert that ( gem fire cache impl . is mis configured ( cluster props @$ server props @$ __str__ ) ) . is false ( ) ;,server has,success,pre
make sure the given union tag <PLACE_HOLDER> a corresponding tuple tag in the schema .,int union tag = value . get union tag ( ) ; if ( schema . size ( ) <= union tag ) { throw new illegal state exception ( __str__ + union tag + __str__ ) ; } list < object > value list = ( list < object > ) value map . get ( union tag ) ; value list . add ( value . get value ( ) ) ;,tag has,success,pre
end playback @$ as we did n't manage to find a valid <PLACE_HOLDER> position .,if ( period position us == c . time_unset ) { set state ( player . state_ended ) ; reset internal ( false @$ false @$ true @$ false @$ true ) ; } else { long new period position us = period position us ; if ( period id . equals ( playback info . period id ) ) { media period holder playing period holder = queue . get playing period ( ) ; if ( playing period holder != null && playing period holder . prepared && new period position us != __num__ ) { new period position us = playing period holder . media period . get adjusted seek position us ( new period position us @$ seek parameters ) ; } if (,playback seek,success,pre
repl load during migration @$ <PLACE_HOLDER> the explicit txn and start some internal txns . call release locks and commit or rollback to do the clean up .,if ( ! driver context . get txn manager ( ) . is txn open ( ) && driver context . get query state ( ) . get hive operation ( ) == hive operation . replload ) { release locks and commit or rollback ( false ) ; } else { },load enforce,fail,pre
other task will now <PLACE_HOLDER> the process instance,org . flowable . task . api . task task = task service . create task query ( ) . process instance id ( pi . get id ( ) ) . task definition key ( __str__ ) . single result ( ) ; map < string @$ object > variables = new hash map < string @$ object > ( ) ; variables . put ( __str__ @$ __num__ ) ; task service . complete ( task . get id ( ) @$ variables ) ; assert equals ( __num__ @$ runtime service . create process instance query ( ) . process instance id ( pi . get id ( ) ) . count ( ) ) ;,task finish,success,pre
a volt <PLACE_HOLDER> extension to support indexed expressions and assume unique attribute .,c . set assume unique ( assume unique ) ; if ( has non column exprs ) { c = c . with expressions ( index exprs . to array ( new expression [ index exprs . size ( ) ] ) ) ; },volt db,success,pre
magic numbers @$ if anybody knows why @$ please <PLACE_HOLDER> me,datfiles . put ( dat file name @$ new byte [ ] { ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$,numbers give,fail,pre
if rm is <PLACE_HOLDER> about response id out of sync @$ force reset next time,if ( t instanceof invalid application master request exception ) { int response id = amrm client utils . parse expected response id from exception ( t . get message ( ) ) ; if ( response id != - __num__ ) { this . reset response id = response id ; log . info ( __str__ + response id + __str__ + allocate request . get response id ( ) + __str__ + this . app id + __str__ ) ; } else { log . warn ( __str__ + this . app id ) ; } } throw t ;,rm giving,fail,pre
special case @$ a collection with only a <PLACE_HOLDER> method we assume we can just add to the connection,if ( collection . class . is assignable from ( i . get property type ( ) ) ) { handled properties . add ( i . get name ( ) ) ; collection property value = ( collection ) i . read ( param ) ; if ( ! property value . is empty ( ) ) { list < deferred parameter > params = new array list < > ( ) ; for ( object c : property value ) { deferred parameter to add = load object instance ( c @$ existing @$ object . class ) ; params . add ( to add ) ; } setup steps . add ( new serialzation step ( ) { @ override public void handle ( method,case read,success,pre
some binary files <PLACE_HOLDER> newline,while ( c != __str__ ) { if ( c != __str__ ) sb . append ( c ) ; c = ( char ) buffer . get ( ) ; },files have,success,pre
after all class information has been read @$ now we can safely inspect <PLACE_HOLDER> information for errors . if we did this before all parsing was finished @$ we could get vicious circularities @$ since files can <PLACE_HOLDER> each others ' classes .,checked = - __num__ ;,files reference,fail,pre
to find the existing room created with op <PLACE_HOLDER> muc 1 @$ we will use op <PLACE_HOLDER> muc 2 to be sure the room will not be retrieved from op <PLACE_HOLDER> muc 1 cache,try { found room = op setmuc2 . find room ( test room name ) ; } catch ( exception ex ) { logger . warn ( ex ) ; } assert not null ( __str__ @$ found room ) ; assert equals ( __str__ @$ op set1 room . get name ( ) @$ found room . get name ( ) ) ;,cache set,success,pre
rename lambda method to reflect the new owner . not doing so confuses lambda desugaring if it 's run over this class again . lambda desugaring has already <PLACE_HOLDER> the method from its original name to include the interface name at this point .,if ( is lambda ) { return name + dependency collector . interface_companion_suffix ; },lambda mapped,fail,pre
for sake of simplicity <PLACE_HOLDER> any iterable in array,if ( result instanceof iterable < ? > ) { final list < object > list = new array list < object > ( ) ; for ( object o : ( iterable < object > ) result ) { list . add ( o ) ; } result = list . to array ( ) ; },sake replace,fail,pre
is this a <PLACE_HOLDER> subroutine ?,if ( branch target finder . is subroutine ( offset ) && branch target finder . is subroutine returning ( offset ) ) { if ( debug ) { system . out . println ( __str__ + instruction . to string ( offset ) ) ; } code attribute composer . append label ( offset ) ; } else { instruction . accept ( clazz @$ method @$ code attribute @$ offset @$ this ) ; },a returning,success,pre
which will <PLACE_HOLDER> the main activity to recreate this fragment,( new sign out word press com async ( get activity ( ) ) ) . execute on executor ( async task . thread_pool_executor ) ;,which trigger,fail,pre
check if send mono <PLACE_HOLDER> all notes off,channel . note on ( __num__ @$ __num__ ) ; soft . read ( __num__ ) ; assert true ( soft . find voice ( __num__ @$ __num__ ) != null ) ; smsg . set message ( short message . control_change @$ __num__ @$ __num__ @$ __num__ ) ; receiver . send ( smsg @$ - __num__ ) ; soft . read ( __num__ ) ; assert true ( soft . find voice ( __num__ @$ __num__ ) == null ) ; soft . close ( ) ;,mono triggers,success,pre
these colors should <PLACE_HOLDER> overridden .,intent intent = new custom tabs intent . builder ( ) . set toolbar color ( __num__ ) . set secondary toolbar color ( __num__ ) . set navigation bar color ( __num__ ) . set color scheme params ( color_scheme_light @$ light params ) . set color scheme params ( color_scheme_dark @$ dark params ) . build ( ) . intent ;,colors get,success,pre
step 2 : optional . dynamic runtime property change option we would like to utilize the benefits of obtaining dynamic property changes <PLACE_HOLDER> the dynamic property factory with our configuration source,dynamic property factory . init with configuration source ( my configuration ) ;,changes initialize,success,pre
if filter.g <PLACE_HOLDER> date parsing for quoted strings @$ we 'd need to verify there 's no type mismatch when string col is filtered by a string that looks like date .,if ( col type == filter type . date && val type == filter type . string ) { try { node value = meta store utils . partition_date_format . get ( ) . parse ( ( string ) node value ) ; val type = filter type . date ; } catch ( parse exception pe ) { } },filter.g supports,fail,pre
make sure no function at current and new location ca n't <PLACE_HOLDER> primary if it is there .,if ( is func ) { function manager . remove function ( mem addr ) ; function manager . remove function ( new addr ) ; },function change,success,pre
prev state cur state <PLACE_HOLDER> global counter exptected cur proc state seq <PLACE_HOLDER> block state expect notify,verify seq counter and interactions ( uid rec @$ process_state_foreground_service @$ process_state_service @$ __num__ @$ __num__ @$ network_state_unblock @$ true ) ;,state expected,success,pre
if all of the bits have been cleared in copy @$ that means bit set 1 <PLACE_HOLDER> at least all of the bits set that were set in bs 2,return copy . is empty ( ) ;,1 has,fail,pre
legacy herb flavors might <PLACE_HOLDER> this path before the caching logic corrects it @$ so treat this as disabled .,return false ;,flavors add,fail,pre
we found a simplification . <PLACE_HOLDER> the old filters and add new ones .,if ( range set . as ranges ( ) . size ( ) < filter list . size ( ) ) { for ( final bound dim filter bound : filter list ) { if ( ! new children . remove ( bound ) ) { throw new ise ( __str__ ) ; } } if ( range set . as ranges ( ) . is empty ( ) ) { new children . add ( filtration . match nothing ( ) ) ; } for ( final range < bound value > range : range set . as ranges ( ) ) { if ( ! range . has lower bound ( ) && ! range . has upper bound ( ) ) { new children .,simplification remove,success,pre
a single instance of hbase checksum failure <PLACE_HOLDER> the reader to switch off hbase checksum verification for the next 100 read requests . verify that this is correct .,for ( int i = __num__ ; i < h file block . checksum_verification_num_io_threshold + __num__ ; i ++ ) { b = hbr . read block data ( __num__ @$ - __num__ @$ pread @$ false @$ true ) ; assert true ( b . get buffer read only ( ) instanceof single byte buff ) ; assert equals ( __num__ @$ h file . get and reset checksum failures count ( ) ) ; },instance causes,success,pre
special case : if there were no files to measure @$ <PLACE_HOLDER> the containing j scroll pane 's width,if ( d . width == __num__ && get parent ( ) != null ) { if ( get parent ( ) . get parent ( ) instanceof j scroll pane ) { j scroll pane parent = ( j scroll pane ) get parent ( ) . get parent ( ) ; dimension parent size = parent . get size ( ) ; insets insets = parent . get insets ( ) ; d . width = parent size . width - ( insets != null ? insets . right + insets . left : __num__ ) ; } } else { d . width += default_icon_size + width_padding ; },case use,success,pre
should print <PLACE_HOLDER> diagnostic information ?,return __num__ ;,print saved,fail,pre
data should <PLACE_HOLDER> updated,watcher . wait for change ( ) ; assert equals ( __str__ @$ new string ( o . zk . getzk database ( ) . get data ( __str__ @$ stat @$ null ) ) ) ; assert equals ( __str__ @$ new string ( o . zk . getzk database ( ) . get data ( __str__ @$ stat @$ null ) ) ) ;,data get,success,pre
required for mdc based routing appender so that child threads can <PLACE_HOLDER> the mdc context,system . set property ( __str__ @$ __str__ ) ; configurator . initialize ( __str__ @$ llap_l4j2 . to string ( ) ) ; long end = system . current time millis ( ) ; log . debug ( __str__ @$ llap_l4j2 @$ ( end - start ) @$ async ) ; throw new runtime exception ( __str__ + __str__ + llap constants . log4j2_properties_file + __str__ ) ;,threads inform,fail,pre
record <PLACE_HOLDER> version and cluster name .,if ( target instanceof cluster info accessor ) { recorder . record attribute ( elasticsearch constants . args_version_annotation_key @$ ( ( cluster info accessor ) target ) . _$pinpoint$_get cluster info ( ) ) ; },record elasticsearch,success,pre
gesture selection helper <PLACE_HOLDER> logic that interprets a combination of motions and gestures in order to provide gesture driven selection support when used in conjunction with recycler view .,final gesture selection helper gesture helper = gesture selection helper . create ( tracker @$ m selection predicate @$ m recycler view @$ scroller @$ m monitor ) ;,helper create,fail,pre
verify the widget will now <PLACE_HOLDER> white text,attack plugin . on varbit changed ( new varbit changed ( ) ) ; warned skills = attack plugin . get warned skills ( ) ; assert true ( warned skills . contains ( skill . attack ) ) ; assert false ( attack plugin . is warned skill selected ( ) ) ;,widget show,fail,pre
bits consumed range : 0 to 7 @$ where 0 <PLACE_HOLDER> last byte fully consumed,if ( bits consumed == __num__ || bits consumed == __num__ ) { bits consumed = __num__ ; ++ offset ; },0 indicates,success,pre
someone <PLACE_HOLDER> the components tree holders while we were computing this range . we can just bail as another range will be computed .,synchronized ( this ) { if ( tree holders size != m component tree holders . size ( ) ) { return false ; } holder = m component tree holders . get ( index ) ; if ( holder . get render info ( ) . renders view ( ) ) { return true ; } children width spec = get actual children width spec ( holder ) ; children height spec = get actual children height spec ( holder ) ; },someone changed,fail,pre
turn the screen off . a black surface is already <PLACE_HOLDER> the contents of the screen .,if ( m power state . get color fade level ( ) == __num__ ) { set screen state ( display . state_off ) ; m pending screen off = false ; m power state . dismiss color fade resources ( ) ; } else if ( perform screen off transition && m power state . prepare color fade ( m context @$ m color fade fades config ? color fade . mode_fade : color fade . mode_cool_down ) && m power state . get screen state ( ) != display . state_off ) { m color fade off animator . start ( ) ; } else { m color fade off animator . end ( ) ; },surface stroking,fail,pre
since this link <PLACE_HOLDER> object files from another library @$ we know that library must be statically linked @$ so we need to look at include link static in lto indexing to decide whether to include its objects in the lto indexing for this target .,if ( include link static in lto indexing ) { for ( linker inputs . library to link lib : unique libraries ) { if ( ! lib . contains object files ( ) ) { continue ; } for ( artifact object file : lib . get object files ( ) ) { if ( compiled . contains ( object file ) ) { all bitcode . put ( object file . get exec path ( ) @$ object file ) ; } } } } for ( linker input input : object files ) { if ( this . lto compilation context . contains bitcode file ( input . get artifact ( ) ) ) { all bitcode . put ( input . get artifact (,link includes,success,pre
check cm when classifier <PLACE_HOLDER> only one class,checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ;,classifier has,fail,pre
create one file then delete it to trigger the file not <PLACE_HOLDER> exception when closing the file .,file sys . create ( new path ( __str__ ) ) ; file sys . delete ( new path ( __str__ ) @$ true ) ; dfs client dfs client = file sys . get client ( ) ;,file found,success,pre
first operator should <PLACE_HOLDER> final operator info,assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; operator stats = pipeline operator . get operator context ( ) . get nested operator stats ( ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ;,operator have,fail,pre
create one <PLACE_HOLDER> rule to generate <PLACE_HOLDER> config.java .,build rule params build config params = params ; optional < build rule > values file rule = values file . flat map ( graph builder :: get rule ) ; if ( values file rule . is present ( ) ) { build config params = build config params . copy appending extra deps ( values file rule . get ( ) ) ; } android build config android build config = new android build config ( build config build target @$ project filesystem @$ build config params @$ java package @$ values @$ values file @$ use constant expressions ) ; graph builder . add to index ( android build config ) ;,one build,success,pre
if the user <PLACE_HOLDER> a callback then we have to make sure the current realm has an events looper to deliver the results .,if ( ( on success != null || on error != null ) ) { shared realm . capabilities . check can deliver notification ( __str__ ) ; },user specified,fail,pre
should not <PLACE_HOLDER> : groups on basic type,long ds . group by ( __num__ ) ;,not work,success,pre
bind and start to <PLACE_HOLDER> incoming connections .,try { bootstrap server . register shutdown hook ( ) ; bootstrap server . start and block ( ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; } log . info ( __str__ ) ;,bind accept,success,pre
create and set iam role so that firehose service <PLACE_HOLDER> access to the s 3 buckets to put data . please check the trust policy document.json and permissions policy document.json files for the trust and permissions policies set for the role .,string iam role arn = create iam role ( s3 object prefix ) ; redshifts3 configuration . set rolearn ( iam role arn ) ; copy command copy command = new copy command ( ) ; copy command . with copy options ( copy options ) . with data table name ( data table name ) ; redshift destination configuration redshift destination configuration = new redshift destination configuration ( ) ; redshift destination configuration . with clusterjdbcurl ( clusterjdbc url ) . with rolearn ( iam role arn ) . with username ( username ) . with password ( password ) . with copy command ( copy command ) . withs3 configuration ( redshifts3 configuration ) ; create delivery stream request . set redshift destination configuration ( redshift,service has,success,pre
test that a match set with no tags applied <PLACE_HOLDER> no items filtered,vt session session = controller . get session ( ) ; list < vt match set > match sets = session . get match sets ( ) ; vt match set match set = match sets . get ( __num__ ) ; collection < vt match > matches = match set . get matches ( ) ; for ( vt match match : matches ) { assert true ( __str__ @$ tag filter . passes filter ( match ) ) ; } filter state filter state = tag filter . get filter state ( ) ; map < string @$ vt match tag > excluded tags = ( map < string @$ vt match tag > ) filter state . get ( tag filter . excluded_tags_key ) ;,match has,success,pre
each <PLACE_HOLDER> case .,int start = __num__ @$ total = __num__ @$ remain = __num__ ; for ( int i = __num__ ; i < gas . length ; ++ i ) { remain += gas [ i ] - cost [ i ] ; total += gas [ i ] - cost [ i ] ; if ( remain < __num__ ) { start = i + __num__ ; remain = __num__ ; } },each test,success,pre
call the api 's channels.list method to retrieve the resource that represents the authenticated user 's channel . in the api response @$ only include channel information needed for this use case . the channel 's content details part <PLACE_HOLDER> playlist i ds relevant to the channel @$ including the id for the list that <PLACE_HOLDER> videos uploaded to the channel .,you tube . channels . list channel request = youtube . channels ( ) . list ( __str__ ) ; channel request . set mine ( true ) ; channel request . set fields ( __str__ ) ; channel list response channel result = channel request . execute ( ) ; list < channel > channels list = channel result . get items ( ) ; if ( channels list != null ) { string upload playlist id = channels list . get ( __num__ ) . get content details ( ) . get related playlists ( ) . get uploads ( ) ; list < playlist item > playlist item list = new array list < playlist item > ( ) ; you tube . playlist items,part holds,fail,pre
extract character list @$ gold <PLACE_HOLDER> speaker and mention information from the xml document .,document doc = xml utils . read document from file ( file name ) ; node text = doc . get document element ( ) . get elements by tag name ( __str__ ) . item ( __num__ ) ; string doc text = get just text ( text ) ; annotation document = get annotated file ( doc text @$ file name @$ get processed corenlp properties ( ) ) ; list < core map > quotes = document . get ( core annotations . quotations annotation . class ) ; list < core label > tokens = document . get ( core annotations . tokens annotation . class ) ; list < gold quote info > gold list = new array list < > ( ),gold hold,fail,pre
a new directory also <PLACE_HOLDER> a copy of the parent 's default acl .,list < acl entry > default entries = child . is directory ( ) ? parent default entries : collections . < acl entry > empty list ( ) ; final fs permission new perm ; if ( ! acl util . is minimal acl ( access entries ) || ! default entries . is empty ( ) ) { child . add acl feature ( create acl feature ( access entries @$ default entries ) ) ; new perm = create fs permission for extended acl ( access entries @$ child perm ) ; } else { new perm = create fs permission for minimal acl ( access entries @$ child perm ) ; },directory has,fail,pre
0 <PLACE_HOLDER> no regions on master .,if ( master count == __num__ || master count == system_regions ) { assert equals ( master count @$ m actual count ) ; } else { check count ( master count @$ m actual count ) ; },0 means,success,pre
compare to make sure the created job <PLACE_HOLDER> the expected configuration @$ i.e . the configuration resulting from a merge of the json file and cli args .,final job actual job = json . read ( actual job config json @$ job . class ) ; final job . builder actual job builder = actual job . to builder ( ) ; builder . set name ( test job name ) . set version ( test job version ) . set image ( busybox ) . set env ( immutable map . of ( redundant env key @$ __str__ ) ) ; assert job equals ( builder . build ( ) @$ actual job builder . build ( ) ) ;,job has,success,pre
these values always <PLACE_HOLDER> non null .,while ( callback != null ) { future callback internal callback = this . callback ; exception e = this . e ; object result = this . result ; this . callback = null ; this . e = null ; this . result = null ; callback . on completed ( e @$ result @$ this ) ; },values produce,fail,pre
some exceptions do not <PLACE_HOLDER> a message ; fall back to the string value .,if ( message == null ) { message = thrown . to string ( ) ; },exceptions have,success,pre
d 2 has <PLACE_HOLDER> 0 to 50,for ( int i = __num__ ; i < __num__ ; ++ i ) { volt queuesql ( loadd2 @$ i @$ __str__ + string . value of ( i ) ) ; } volt executesql ( ) ;,d pkeys,success,pre
stop the inbound bridges when the foreign connection is dropped since the bridge <PLACE_HOLDER> no consumer and needs to be restarted once a new connection to the foreign side is made .,if ( this . foreign connection . compare and set ( connection @$ null ) ) { for ( destination bridge bridge : inbound bridges ) { try { bridge . stop ( ) ; } catch ( exception e ) { } } this . connection service . execute ( new runnable ( ) { @ override public void run ( ) { try { do initialize connection ( false ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; } } } ) ; } else if ( this . local connection . compare and set ( connection @$ null ) ) { for ( destination bridge bridge : outbound bridges ) { try { bridge . stop,bridge has,success,pre
parens <PLACE_HOLDER> interpretation as an expression .,node script = parse ( __str__ + js + __str__ ) ;,parens require,fail,pre
force default source if any param has <PLACE_HOLDER> storage,if ( ! get return ( ) . is valid ( ) ) { return source type . default ; } for ( parameter param : get parameters ( ) ) { if ( ! param . is valid ( ) ) { return source type . default ; } } return get stored signature source ( ) ; manager . lock . release ( ) ;,param defaulted,fail,pre
mock unregister m bean to throw the instance not <PLACE_HOLDER> exception @$ indicating that the m bean has already been unregistered,do throw ( new instance not found exception ( ) ) . when ( mockm bean server ) . unregisterm bean ( object name ) ; m beanjmx adapter m beanjmx adapter = spy ( new m beanjmx adapter ( dist member ) ) ; m beanjmx adapter . mbean server = mockm bean server ; m beanjmx adapter . unregisterm bean ( object name ) ;,m found,success,pre
if passed integer and in list <PLACE_HOLDER> numeric form else <PLACE_HOLDER> original value,try { int genre id = integer . parse int ( value ) ; if ( genre id < genre types . get max genre id ( ) ) { return bracket wrap ( string . value of ( genre id ) ) ; } else { return value ; } } catch ( number format exception nfe ) { integer genre id = genre types . get instance of ( ) . get id for name ( value ) ; if ( genre id != null ) { return bracket wrap ( string . value of ( genre id ) ) ; } if ( value . equals ignore case ( id3v2 extended genre types . rx . get description ( ) ) ) { value =,form return,fail,pre
get the set of component names to make sure the config only <PLACE_HOLDER> valid component name,set < string > component names = get component parallelism ( topology ) . key set ( ) ;,config has,fail,pre
these array types still need to be implemented . the superclass wo n't <PLACE_HOLDER> them so we return null here until we can code schema implementations for them .,return null ; default : if ( oid value == type registry . geometry oid ( ) ) { return geometry . builder ( ) ; } else if ( oid value == type registry . geography oid ( ) ) { return geography . builder ( ) ; } else if ( oid value == type registry . citext oid ( ) ) { return schema builder . string ( ) ; } else if ( oid value == type registry . geometry array oid ( ) ) { return schema builder . array ( geometry . builder ( ) . optional ( ) . build ( ) ) ; } else if ( oid value == type registry . hstore oid ( ) ) { return,superclass spot,fail,pre
create a long from the first 8 bytes of the digest this is fine as md 5 <PLACE_HOLDER> the avalanche property . paranoids could have xor folded the other 8 bytes in too .,long seed = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { seed = ( seed << __num__ ) + ( ( int ) digest [ i ] + __num__ ) ; } return seed ;,md uses,fail,pre
up to here @$ an error can <PLACE_HOLDER> a good message,log . error ( __str__ @$ client socket . get inet address ( ) @$ t ) ; try { write error to stream ( output stream @$ t ) ; } catch ( io exception e ) { },error give,success,pre
a field of executable stage which <PLACE_HOLDER> the p collection goes to worker side .,set < p collection node > executable stage outputs = new hash set < > ( ) ;,which includes,success,pre
owner can <PLACE_HOLDER> internal system window,return create window ( parent @$ type @$ token @$ name @$ owner id @$ false ) ;,owner add,success,pre
filter actually <PLACE_HOLDER> some processing @$ set the new chunk in and release the old chunk .,if ( ( filtered chunk != null ) && ( filtered chunk != orig chunk ) ) { body chunks . set ( i @$ filtered chunk ) ; final int ref cnt = orig chunk . ref cnt ( ) ; if ( ref cnt > __num__ ) { orig chunk . release ( ref cnt ) ; } },filter done,fail,pre
we only check for incomplete multi statement procedures right now <PLACE_HOLDER> a mandatory space..,incomplete stmt offset = i start ; incomplete stmt = string . copy value of ( buf @$ i start @$ i cur - i start ) ;,procedures use,fail,pre
make sure that the metadata was refreshed during the rebalance and thus subscriptions now <PLACE_HOLDER> two topics .,final set < string > updated subscription set = new hash set < > ( arrays . as list ( topic1 @$ topic2 ) ) ; assert equals ( updated subscription set @$ subscriptions . subscription ( ) ) ;,subscriptions have,fail,pre
propagating the amrm client <PLACE_HOLDER> token cache instance,nm client . setnm token cache ( rm client . getnm token cache ( ) ) ; nm client . init ( conf ) ; nm client . start ( ) ; assert not null ( nm client ) ; assert equals ( state . started @$ nm client . get service state ( ) ) ;,client sends,fail,pre
for types @$ the computed type <PLACE_HOLDER> the symbol 's type @$ except for two situations :,owntype = sym . type ; if ( owntype . has tag ( class ) ) { chk . check for bad auxiliary class access ( tree . pos ( ) @$ env @$ ( class symbol ) sym ) ; type own outer = owntype . get enclosing type ( ) ; if ( owntype . tsym . type . get type arguments ( ) . non empty ( ) ) { owntype = types . erasure ( owntype ) ; } else if ( own outer . has tag ( class ) && site != own outer ) { type norm outer = site ; if ( norm outer . has tag ( class ) ) { norm outer = types . as enclosing super (,type overrides,fail,pre
create four wheels and add them at their locations note that our fancy car actually <PLACE_HOLDER> backwards..,vector3f wheel direction = new vector3f ( __num__ @$ - __num__ @$ __num__ ) ; vector3f wheel axle = new vector3f ( - __num__ @$ __num__ @$ __num__ ) ; geometry wheel_fr = find geom ( car node @$ __str__ ) ; wheel_fr . center ( ) ; box = ( bounding box ) wheel_fr . get model bound ( ) ; wheel radius = box . gety extent ( ) ; float back_wheel_h = ( wheel radius * __num__ ) - __num__ ; float front_wheel_h = ( wheel radius * __num__ ) - __num__ ; player . add wheel ( wheel_fr . get parent ( ) @$ box . get center ( ) . add ( __num__ @$ - front_wheel_h @$ __num__ ) @$ wheel direction @$,car goes,success,pre
omitting <PLACE_HOLDER> 1 for the moment ; there seems to be no reason to allow it .,if ( params . get hash ( ) != hash type . sha256 && params . get hash ( ) != hash type . sha512 ) { throw new general security exception ( __str__ ) ; },omitting sha,success,pre
a corrupt mob file does n't <PLACE_HOLDER> the start of regions @$ so we can enable the table .,admin . enable table ( table ) ; h base fsck res = hbck testing util . doh file quarantine ( conf @$ table ) ; assert equals ( __num__ @$ res . get ret code ( ) ) ; h file corruption checker hfcc = res . geth filecorruption checker ( ) ; assert equals ( __num__ @$ hfcc . geth files checked ( ) ) ; assert equals ( __num__ @$ hfcc . get corrupted ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get failures ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get quarantined ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get missing,file prevent,fail,pre
second try should <PLACE_HOLDER> the client again,try { metastore . get all databases ( ) ; } catch ( runtime exception ignored ) { } assert equals ( mock client . get access count ( ) @$ __num__ ) ;,try cancel,fail,pre
the other one <PLACE_HOLDER> multiple durations @$ but we do n't . expand to multiple durations and copy over .,if ( other src . m durations != null ) { my src . make durations ( ) ; my src . m durations . add durations ( other src . m durations ) ; if ( my src . m active duration != __num__ ) { my src . m durations . add duration ( my src . m active proc state @$ my src . m active duration ) ; my src . m active duration = __num__ ; my src . m active proc state = process stats . state_nothing ; } } else if ( my src . m active duration != __num__ ) { if ( my src . m active proc state == other src . m active proc state ) {,one has,success,pre
date @$ note : these ms counts all presume pst jan @$ feb @$ mar @$ apr 2014 may @$ jun @$ jul @$ <PLACE_HOLDER> 2014 sep @$ oct @$ nov @$ dec 2014 jan 2016 @$ mar 2017 @$ jun 2018 @$ sep 2019 @$ dec 2020 jan @$ feb @$ mar @$ apr 2014 may @$ jun @$ jul @$ <PLACE_HOLDER> 2014,long [ ] exp = new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ;,date jan,fail,pre
not a comma . stop parsing the <PLACE_HOLDER> modifiers .,if ( ! configuration constants . argument_separator_keyword . equals ( next word ) ) { break ; },the include,fail,pre
kick off any lingering app transitions <PLACE_HOLDER> the move task to front operation @$ but only consider the top task and stack on that display .,if ( display . is top stack ( stack ) && top running activity . is state ( resumed ) ) { stack . execute app transition ( target options ) ; } else { resumed on display |= top running activity . make active if needed ( target ) ; },transitions perform,fail,pre
required for mdc based routing appender so that child threads can <PLACE_HOLDER> the mdc context,system . set property ( default thread context map . inheritable_map @$ __str__ ) ; configurator . initialize ( null @$ log4j file name ) ; log config location ( conf ) ; return __str__ + log4j config file + __str__ + async ;,threads inherit,success,pre
restarted rm <PLACE_HOLDER> the failed app info too .,rm2 . wait for state ( app1 . get application id ( ) @$ rm app state . failed ) ;,rm consumes,fail,pre
character functions <PLACE_HOLDER> number values :,add functions ( arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$,functions returning,success,pre
note : complex structure already <PLACE_HOLDER> a single undefined byte .,complex structure . add ( new byte data type ( ) ) ; complex structure . add ( new word data type ( ) ) ; complex structure . add ( new pointer32 data type ( data type . default ) @$ __num__ ) ;,structure contains,fail,pre
the suite made here will all be <PLACE_HOLDER> the tests from this class,multi config suite builder builder = new multi config suite builder ( test update deployment . class ) ;,suite using,success,pre
consuming segment should <PLACE_HOLDER> all instances in consuming state,for ( map . entry < string @$ string > entry : consuming segment instance state map . entry set ( ) ) { assert equals ( entry . get value ( ) @$ realtime segment online offline state model . consuming ) ; entry . set value ( realtime segment online offline state model . offline ) ; },segment have,success,pre
else can we handle iterator types if context node does n't <PLACE_HOLDER> event target ? ?,add event listener ( ) ;,node support,success,pre
then : the getters should <PLACE_HOLDER> properly parsed values,assert that ( builder . get bind address ( ) ) . is null ( ) ; assert that ( builder . get command ( ) ) . is equal to ( command . start ) ; assert that ( builder . get debug ( ) ) . is false ( ) ; assert that ( builder . get force ( ) ) . is false ( ) ; assert that ( builder . get help ( ) ) . is false ( ) ; assert that ( builder . get hostname for clients ( ) ) . is null ( ) ; assert that ( builder . get member name ( ) ) . is equal to ( __str__ ) ; assert that ( builder . get,getters return,success,pre
src inode and its subtree can not <PLACE_HOLDER> snapshottable directories with snapshots,fs dir snapshot op . check snapshot ( fsd @$ srciip @$ snapshottable dirs ) ;,inode create,fail,pre
the current item <PLACE_HOLDER> higher priority,final int current index = play queue . get index ( ) ; final play queue item current item = play queue . get item ( current index ) ; if ( current item == null ) return null ;,item has,success,pre
if spell checker is disabled @$ just return . the user should explicitly <PLACE_HOLDER> the spell checker .,if ( ! tsd . is spell checker enabled ( ) ) return ; if ( sci == null ) { sci = find avail system spell checker locked ( null @$ tsd ) ; set current spell checker locked ( sci @$ tsd ) ; } else { final string package name = sci . get package name ( ) ; final int change = is package disappearing ( package name ) ; if ( dbg ) slog . d ( tag @$ __str__ + package name ) ; if ( change == package_permanent_change || change == package_temporary_change ) { spell checker info avail sci = find avail system spell checker locked ( package name @$ tsd ) ; if ( avail sci == null || (,user set,fail,pre
add a test . this test <PLACE_HOLDER> the select statement and expects the result to be order by output .,dbc . add test ( new test config ( __str__ @$ __str__ @$ false @$ order by output ) ) ;,test runs,success,pre
we do n't yet know the next maximum value so <PLACE_HOLDER> the max of the address ranges .,if ( max == null ) { for ( int i = __num__ ; i < addr ranges . length ; i ++ ) { if ( addr ranges [ i ] != null ) { address check max addr = addr ranges [ i ] . get max address ( ) ; if ( ( max == null ) || ( max . compare to ( check max addr ) < __num__ ) ) { max = check max addr ; } } } },value get,success,pre
someone really <PLACE_HOLDER> this thread to die off .,thread . current thread ( ) . interrupt ( ) ;,someone expect,fail,pre
the last one should have <PLACE_HOLDER> a stream error,for ( int i = __num__ ; i < single partmime reader callbacks . size ( ) - __num__ ; i ++ ) { final single partmime exception reader callback impl current callback = single partmime reader callbacks . get ( i ) ; final body part current expected part = multi part mime body . get body part ( i ) ; final map < string @$ string > expected headers = new hash map < string @$ string > ( ) ; @ suppress warnings ( __str__ ) final enumeration < header > all headers = current expected part . get all headers ( ) ; while ( all headers . has more elements ( ) ) { final header header = all headers . next,one thrown,fail,pre
check that the backgrounds <PLACE_HOLDER> the same size of the components to which they are associated,assert that ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) . is equal to ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) ; assert that ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) . is equal to ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) ; final rect text layout bounds = layout state . get mountable output at ( __num__ ) . get bounds ( ) ; final rect text background bounds = layout state . get mountable output at ( __num__ ) . get bounds ( ) ; assert that,backgrounds have,success,pre
note down that the process has <PLACE_HOLDER> an activity and is in background activity starts grace period,if ( r . app != null ) { r . app . set last activity finish time if needed ( system clock . uptime millis ( ) ) ; } final long orig id = binder . clear calling identity ( ) ; try { boolean res ; final boolean finish with root activity = finish task == activity . finish_task_with_root_activity ; if ( finish task == activity . finish_task_with_activity || ( finish with root activity && r == rootr ) ) { res = m stack supervisor . remove task by id locked ( tr . task id @$ false @$ finish with root activity @$ __str__ ) ; if ( ! res ) { slog . i ( tag @$ __str__ ) ; } r,process finished,success,pre
use print writer.println @$ which <PLACE_HOLDER> correct platform line ending .,byte array output stream baos = new byte array output stream ( ) ; print writer node report str = new print writer ( new output stream writer ( baos @$ charset . for name ( __str__ ) ) ) ; node report node report = null ; for ( node report report : nodes report ) { if ( ! report . get node id ( ) . equals ( node id ) ) { continue ; } node report = report ; node report str . println ( __str__ ) ; node report str . print ( __str__ ) ; node report str . println ( node report . get node id ( ) ) ; node report str . print ( __str__ ) ; node,which uses,success,pre
initialize stats publishing table for noscan which <PLACE_HOLDER> only stats task the rest of mr task following stats task initializes it in exec driver.java,stats publisher stats publisher = factory . get stats publisher ( ) ; if ( ! stats publisher . init ( scc ) ) { throw new hive exception ( error msg . statspublisher_initialization_error . get error coded msg ( ) ) ; },which contains,fail,pre
generate for the <PLACE_HOLDER> method,super . abstract method ( symbol table @$ a @$ stream ) ;,the get,success,pre
a correct run should n't <PLACE_HOLDER> any exception,var counter = __num__ ; for ( var ex : res . get exception list ( ) ) { counter ++ ; logger . info ( ex ) ; } return counter ;,run throw,fail,pre
buck config should <PLACE_HOLDER> nonexistent targets without throwing .,buck config test utils . create with default filesystem ( temporary folder @$ reader ) ;,config delete,fail,pre
variants only <PLACE_HOLDER> audio .,if ( variants contain audio codecs ) { format [ ] audio formats = new format [ selected variants . size ( ) ] ; for ( int i = __num__ ; i < audio formats . length ; i ++ ) { format variant format = variants [ i ] . format ; audio formats [ i ] = derive audio format ( variant format @$ master playlist . muxed audio format @$ true ) ; } muxed track groups . add ( new track group ( audio formats ) ) ; } else { throw new illegal argument exception ( __str__ + codecs ) ; },variants contain,success,pre
catch the original exception which sounds like : java.lang.illegal argument exception : comparison method <PLACE_HOLDER> its general contract !,swing utilities . invoke later ( ( ) -> { thread . current thread ( ) . set uncaught exception handler ( new thread . uncaught exception handler ( ) { public void uncaught exception ( thread t @$ throwable e ) { e . print stack trace ( ) ; if ( e instanceof illegal argument exception ) { passed = false ; latch . count down ( ) ; } } } ) ; test dialog d = new test dialog ( ) ; d . add window focus listener ( new window adapter ( ) { public void window gained focus ( window event e ) { latch . count down ( ) ; } } ) ; d . set visible ( true ),method violates,success,pre
byte argument in gram key <PLACE_HOLDER> tie between equal grams,byte [ ] empty = new byte [ __num__ ] ; gram key [ ] input = { new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ empty ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ empty ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ foo ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . ngram ) @$ foo ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . ngram ) @$ empty ) @$ new gram key ( new,argument makes,fail,pre
health <PLACE_HOLDER> thread .,if ( is health checker configured ( ) ) { int sleep time = this . conf . get int ( h constants . health_chore_wake_freq @$ h constants . default_thread_wake_frequency ) ; health check chore = new health check chore ( sleep time @$ this @$ get configuration ( ) ) ; },health check,fail,pre
lazily initialized since most selectors do n't <PLACE_HOLDER> module installation .,if ( set name == null ) { set name = annotations . name of ( set key ) ; } return set name ;,selectors support,fail,pre
in case when remote tx has <PLACE_HOLDER> the current state before .,ignore ( ) ;,tx rejected,fail,pre
the suite made here will all be <PLACE_HOLDER> the tests from this class,multi config suite builder builder = new multi config suite builder ( testtpcc suite . class ) ;,suite using,success,pre
we did not demand online loading @$ therefore a failure does not mean that the missing snippet <PLACE_HOLDER> a rejection of this result this may happen during a remote search @$ because snippet loading is omitted to retrieve results faster,if ( cache strategy . must be offline ( ) ) { return page . make result entry ( this . query . get segment ( ) @$ this . peers @$ null ) ; } else { if ( this . snippet fetch words . contains ( segment . catchall string ) ) { return page . make result entry ( this . query . get segment ( ) @$ this . peers @$ null ) ; } final string reason = __str__ + snippet . get error code ( ) ; if ( this . delete if snippet fail ) { this . work tables . failur ls register missing word ( this . query . get segment ( ) . term index ( ) @$,snippet causes,success,pre
the output stream of this fs does n't <PLACE_HOLDER> hflush @$ so the below test will fail,conf . set var ( hive conf . conf vars . hive_blobstore_supported_schemes @$ __str__ ) ; strict delimited input writer writer = strict delimited input writer . new builder ( ) . with field delimiter ( __str__ ) . build ( ) ; try { hive streaming connection . new builder ( ) . with database ( db name ) . with table ( tbl name ) . with agent info ( __str__ + thread . current thread ( ) . get name ( ) ) . with record writer ( writer ) . with transaction batch size ( __num__ ) . with hive conf ( conf ) . connect ( ) ; assert . fail ( ) ; } catch ( connection error e ) { assert,stream support,success,pre
should not <PLACE_HOLDER> and block compilation,partial blocks < test element > partial blocks = block . get partial blocks ( ) ; assert not null ( partial blocks ) ; assert not null ( partial blocks . get block ranges ( ) ) ; assert equals ( __num__ @$ partial blocks . get block ranges ( ) . length ) ; assert equals ( group size @$ partial blocks . get block ranges ( ) [ __num__ ] ) ; assert not null ( partial blocks . get block targets ( ) ) ; assert equals ( __num__ @$ partial blocks . get block targets ( ) . length ) ; assert true ( partial blocks . get block targets ( ) [ __num__ ] . is valid ( ) ) ; assert,not trigger,success,pre
corresponding method count with the current stack may exceeds the stack trace . <PLACE_HOLDER> the count,if ( method count + stack offset > trace . length ) { method count = trace . length - stack offset - __num__ ; } for ( int i = method count ; i > __num__ ; i -- ) { int stack index = i + stack offset ; if ( stack index >= trace . length ) { continue ; } string builder builder = new string builder ( ) ; builder . append ( horizontal_line ) . append ( __str__ ) . append ( level ) . append ( get simple class name ( trace [ stack index ] . get class name ( ) ) ) . append ( __str__ ) . append ( trace [ stack index ] . get method name,trace extract,fail,pre
check <PLACE_HOLDER> server alias for rsa keys .,results rsa = km . choose server alias ( __str__ @$ null @$ null ) ; if ( results rsa == null ) { throw new exception ( __str__ ) ; } system . out . println ( __str__ ) ;,check choose,success,pre
new children added @$ qualified name <PLACE_HOLDER> recomputation,update child categories ( store object @$ new children . values ( ) @$ impacted categories @$ false ) ; break ;,name needs,success,pre
see if the file <PLACE_HOLDER> the regular expression !,try { if ( pattern != null ) { matcher matcher = pattern . matcher ( file . get name ( ) ) ; get it = matcher . matches ( ) ; } if ( get it ) { string local filename = return target filename ( file . get name ( ) ) ; if ( ( ! only getting new files ) || ( only getting new files && needs download ( local filename ) ) ) { if ( is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ file . get name ( ) @$ target directory ) ) ; } connection . download file ( file @$ return target filename ( file .,file matches,success,pre
define the <PLACE_HOLDER> fn that logs the value provider value .,p . apply ( create . of ( __num__ ) ) . apply ( par do . of ( new do fn < integer @$ integer > ( ) { @ process element public void process ( process context c ) { my options ops = c . get pipeline options ( ) . as ( my options . class ) ; log . info ( __str__ @$ ops . get string value ( ) ) ; } } ) ) ;,the do,success,pre
d <PLACE_HOLDER> 2 closes @$ but t xs with lower ids are still open,buffer . offer ( __num__ ) ; tx opened . increment and get ( ) ;,d had,fail,pre
note that this class does not <PLACE_HOLDER> forward mode .,if ( ! forward ) { if ( trusted pub key != null ) { prev pub key = trusted pub key ; } else { prev pub key = null ; } } else { throw new cert path validator exception ( __str__ ) ; },class support,success,pre
note that we do not need to update <PLACE_HOLDER> old value in events because that flag is only used during region initialization . otherwise we would need to,if ( ! ( this . region instanceof partitioned region ) ) { return ; },note set,fail,pre
noinspection <PLACE_HOLDER> allocation in loop,if ( is composite ( current index ) ) { new keys = new tree set < > ( ) ; for ( comparable current key : current keys ) { final list < orid > current result = get from composite index ( current key @$ current index ) ; new keys . add all ( prepare keys ( next index @$ current result ) ) ; } } else { final list < o identifiable > keys ; try ( stream < o raw pair < object @$ orid > > stream = current index . get internal ( ) . stream entries ( current keys @$ true ) ) { keys = stream . map ( ( pair ) -> pair . second ) .,noinspection object,success,pre
create a simple rule which just <PLACE_HOLDER> a file .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; rule key input rule key = new rule key ( __str__ ) ; path output = paths . get ( __str__ ) ; build rule rule = new input rule key build rule ( target @$ filesystem @$ params ) { @ override public immutable list < step > get build steps ( build context context @$ buildable context buildable context ) { buildable context . record artifact ( output ) ; return immutable list . of ( new write file step ( filesystem @$ __str__ @$ output @$ false ) ) ; } @ override public source path get,which writes,success,pre
add ejb <PLACE_HOLDER> handler service,boolean enable graceful shutdown = ejb3 subsystem root resource definition . enable_graceful_txn_shutdown . resolve model attribute ( context @$ model ) . as boolean ( ) ; final ejb suspend handler service ejb suspend handler service = new ejb suspend handler service ( enable graceful shutdown ) ; context . get service target ( ) . add service ( ejb suspend handler service . service_name @$ ejb suspend handler service ) . add dependency ( suspend controller service name @$ suspend controller . class @$ ejb suspend handler service . get suspend controller injected value ( ) ) . add dependency ( txn services . jboss_txn_local_transaction_context @$ local transaction context . class @$ ejb suspend handler service . get local transaction context injected value ( ) ) .,ejb abort,fail,pre
we need to keep the admin client instance separate in each callable @$ so that a <PLACE_HOLDER> of the client in one callable does not <PLACE_HOLDER> the admin client used by another callable .,admin client current admin client = admin store swapper . this . admin client ; int attempt = __num__ ; while ( attempt <= max_fetch_attempts ) { if ( attempt > __num__ ) { logger . info ( __str__ + attempt + __str__ + max_fetch_attempts + __str__ + node . brief to string ( ) + __str__ + wait_time_between_fetch_attempts + __str__ ) ; try { thread . sleep ( wait_time_between_fetch_attempts ) ; } catch ( interrupted exception e ) { throw new voldemort exception ( e ) ; } } logger . info ( __str__ + node . brief to string ( ) + __str__ + hadoop store dir to fetch ) ; try { return current admin client . readonly ops . fetch store ( node .,refresh affect,fail,pre
<PLACE_HOLDER> an int in a long register ? the callee has no clue whether the register <PLACE_HOLDER> an int @$ long or is unused . he always saves a long . here we know a long was saved @$ but we only want an int back . narrow the saved long to the int that the jvm wants .,if ( loc . holds int ( ) ) { if ( assert . asserts_enabled ) { assert . that ( loc . is register ( ) @$ __str__ ) ; } return new stack value ( value addr . getj long at ( __num__ ) & __num__ ) ; } else if ( loc . holds narrow oop ( ) ) { if ( loc . is register ( ) && vm . getvm ( ) . is big endian ( ) ) { return new stack value ( value addr . get comp oop handle at ( vm . getvm ( ) . get int size ( ) ) @$ __num__ ) ; } else { return new stack value ( value addr . get comp,register holds,success,pre
log the input value which <PLACE_HOLDER> exception so that it 's available for debugging . but when exposed through an error message it can leak sensitive information @$ even to the client application .,log . trace ( __str__ + tag + __str__ + utilities . format binary string ( value writable . get bytes ( ) @$ __num__ @$ value writable . get length ( ) ) + __str__ + value table desc [ tag ] . get properties ( ) ) ; throw new hive exception ( __str__ @$ e ) ;,which caused,success,pre
then @$ already tried this credential . remove any rfc 2617 credential since presence of a rfc 2617 credential serves as flag to frontier to requeue this curi and let the curi <PLACE_HOLDER> a natural death .,if ( extant != null ) { extant . detach all ( curi ) ; logger . warning ( __str__ + realm + __str__ + curi . to string ( ) ) ; } else { string server key = get server key ( curi ) ; crawl server server = server cache . get server for ( server key ) ; set < credential > store rfc2617 credentials = get credential store ( ) . subset ( curi @$ http authentication credential . class @$ server . get name ( ) ) ; if ( store rfc2617 credentials == null || store rfc2617 credentials . size ( ) <= __num__ ) { logger . fine ( __str__ + curi ) ; } else { http authentication credential,curi see,fail,pre
make sure that the nested custom layouts do not <PLACE_HOLDER> children,for ( element child : root node . children ( ) ) { assert that ( child . children ( ) . size ( ) @$ is ( __num__ ) ) ; },layouts have,fail,pre
remote node <PLACE_HOLDER> newer image age,if ( bn reg . get layout version ( ) < storage . get layout version ( ) || ( bn reg . get layout version ( ) == storage . get layout version ( ) && bn reg . getc time ( ) > storage . getc time ( ) ) ) msg = __str__ + bn reg . get address ( ) + __str__ + bn reg . get layout version ( ) + __str__ + bn reg . getc time ( ) + __str__ + storage . get layout version ( ) + __str__ + storage . getc time ( ) ;,node has,success,pre
lower half of the threads insert <PLACE_HOLDER> 0 @$ 1 @$2 ... uper half of the threads insert attributes 0 @$ 1 @$ 2 @$ ...,for ( int thread no = __num__ ; thread no < threads ; thread no ++ ) { graql insert query = thread no < threads / __num__ ? graql . parse ( __str__ + thread no + __str__ ) . as insert ( ) : graql . parse ( __str__ + ( thread no - threads / __num__ ) + __str__ ) . as insert ( ) ; completable future < void > async insert = completable future . supply async ( ( ) -> { transaction impl tx = ( transaction impl ) session . write transaction ( ) ; tx . execute ( query ) ; try { barrier . await ( ) ; } catch ( exception e ) { e . print stack,half values,fail,pre
node only has <PLACE_HOLDER> child which will be the minimum .,if ( right child index >= size ) { min index = left child index ; } else { t left child value = _values . get ( left child index ) ; t right child value = _values . get ( right child index ) ; if ( compare ( left child value @$ right child value ) <= __num__ ) { min index = left child index ; } else { min index = right child index ; } },node left,success,pre
mem sql <PLACE_HOLDER> catalogs instead of schemas,return result set . get string ( __str__ ) ;,sql uses,success,pre
sak : not calling this means that mouse and mouse motion listeners do n't <PLACE_HOLDER> installed . not a problem because the menu manager handles tracking for us .,( ( screen menu itemui ) ui ) . update listeners for screen menu item ( ) ;,listeners get,success,pre
k.o . criteria @$ start tag is not svg @$ <PLACE_HOLDER> parser on none svg,default :,criteria terminate,fail,pre
be more forgiving of not finding the <PLACE_HOLDER> listener method .,method method = introspector . find method ( source class @$ get listener method name @$ __num__ ) ; if ( method != null ) { set get listener method ( method ) ; },the get,success,pre
send enough coin to the apply account to make that account <PLACE_HOLDER> ability to apply become witness .,grpcapi . witness list witnesslist = blocking stub full . list witnesses ( grpcapi . empty message . new builder ( ) . build ( ) ) ; optional < witness list > result = optional . of nullable ( witnesslist ) ; grpcapi . witness list witness list = result . get ( ) ; if ( result . get ( ) . get witnesses count ( ) < __num__ ) { assert . assert true ( public methed . sendcoin ( low bal address @$ cost for create witness @$ from address @$ test key002 @$ blocking stub full ) ) ; assert . assert false ( create witness not broadcast ( low bal address @$ wrong url @$ test update witness key ) ) ;,account has,success,pre
does lfs <PLACE_HOLDER> the correct file length ?,assert equals ( lfs . get file status ( pathtotestfile1 ) . get len ( ) @$ testfile1 . length ( ) ) ;,lfs have,fail,pre
eof scanline <PLACE_HOLDER> the run,if ( j == scanline bytes - __num__ ) { if ( abs val == - __num__ ) { stream . write byte ( run count ) ; stream . write byte ( run val ) ; inc comp image size ( __num__ ) ; run count = __num__ ; } else { if ( abs val >= __num__ ) { stream . write byte ( __num__ ) ; stream . write byte ( abs val + __num__ ) ; inc comp image size ( __num__ ) ; for ( int a = __num__ ; a <= abs val ; a ++ ) { stream . write byte ( abs buf [ a ] ) ; inc comp image size ( __num__ ) ; } if ( !,scanline indicates,fail,pre
by the a 2 dp spec @$ srcs must <PLACE_HOLDER> the capture service . however if some device that do not @$ we try to match on some other class bits .,switch ( get device class ( ) ) { case device . audio_video_hifi_audio : case device . audio_video_set_top_box : case device . audio_video_vcr : return true ; default : return false ; },srcs contain,fail,pre
handle negatives @$ which <PLACE_HOLDER> last n characters,if ( start < __num__ ) { start = str . length ( ) + start ; },which remove,fail,pre
nodes must <PLACE_HOLDER> the same type,if ( node1 . get class ( ) != node2 . get class ( ) ) { return false ; } string image1 = node1 . get image ( ) ; string image2 = node2 . get image ( ) ;,nodes have,success,pre
jdk 11 <PLACE_HOLDER> the length to the error message @$ we can do that for all java versions to be consistent .,return new array index out of bounds exception ( __str__ + index + __str__ + length ) ;,jdk adds,fail,pre
check this ps is the master ps for this location @$ only master ps can <PLACE_HOLDER> the update,if ( ! request . is come from ps ( ) && ! is part master ps ( part loc ) ) { string log = __str__ + request + __str__ + request . get part key ( ) ; log . error ( log ) ; return new updater response ( response type . server_handle_failed @$ log ) ; } else { try { class < ? extends update func > func class = ( class < ? extends update func > ) class . for name ( request . get updater func class ( ) ) ; constructor < ? extends update func > constructor = func class . get constructor ( ) ; constructor . set accessible ( true ) ; update func func =,ps receive,fail,pre
stop the datanode <PLACE_HOLDER> b 4,data node dn = cluster . get data node ( dn list [ decommission nodes num ] . get ipc port ( ) ) ; cluster . stop data node ( dn list [ decommission nodes num ] . get xfer addr ( ) ) ; cluster . set data node dead ( dn . get datanode id ( ) ) ;,datanode node,fail,pre
to work around the fact that pmd <PLACE_HOLDER>es not yet <PLACE_HOLDER> full type analysis when it <PLACE_HOLDER>es @$ delete this,set < method name declaration > unique = new hash set < > ( ) ; set < string > sigs = new hash set < > ( ) ; for ( method name declaration mnd : methods . key set ( ) ) { string sig = mnd . get image ( ) + mnd . get parameter count ( ) + mnd . is varargs ( ) ; if ( ! sigs . contains ( sig ) ) { unique . add ( mnd ) ; } sigs . add ( sig ) ; } return unique ;,pmd do,success,pre
it is expected that every <PLACE_HOLDER> processes <PLACE_HOLDER> sleep count number of records .,try { context . set status ( __str__ + ( map sleep duration * ( map sleep count - count ) ) + __str__ ) ; long sleep time = sleep calc . calc sleep duration ( context . get task attemptid ( ) @$ count @$ map sleep count @$ map sleep duration ) ; thread . sleep ( sleep time ) ; } catch ( interrupted exception ex ) { throw ( io exception ) new io exception ( __str__ ) . init cause ( ex ) ; } ++ count ;,processes map,success,pre
android resources @$ views and extras <PLACE_HOLDER> special handling,if ( has injection points for annotation ( inject resource . class ) ) { bind listener ( matchers . any ( ) @$ resource listener ) ; } if ( has injection points for annotation ( inject extra . class ) ) { final extras listener extras listener = new extras listener ( context provider ) ; bind listener ( matchers . any ( ) @$ extras listener ) ; },resources require,success,pre
this creates lock contention for the delay <PLACE_HOLDER> command handlers inside the command handler . so move the actual execution of the post completion callbacks on the command executor thread .,post on main thread ( new add runnable ( this @$ command handler . create execution completed intent ( m context @$ work spec id @$ needs reschedule ) @$ default_start_id ) ) ;,contention run,fail,pre
password <PLACE_HOLDER> error,inputerror = __num__ ;,password read,fail,pre
only add to lexer if the function actually <PLACE_HOLDER> actions,if ( ! raf . actions . is empty ( ) && ! lexer . action funcs . contains key ( r ) ) { lexer . action funcs . put ( r @$ raf ) ; },function has,fail,pre
just return @$ stats gathering should not <PLACE_HOLDER> the main query,if ( stats publisher == null ) { log . error ( __str__ ) ; if ( is stats reliable ) { throw new hive exception ( error msg . statspublisher_not_obtained . get error coded msg ( ) ) ; } return ; },return block,success,pre
note : token only copied if the recipient does n't already <PLACE_HOLDER> one .,if ( token == null ) { token = o . token ; },recipient have,success,pre
the two columns <PLACE_HOLDER> different data type,if ( ! expected data type . get class ( ) . is instance ( actual data type ) ) { if ( expected data type instanceof unknown data type ) { return actual data type ; } if ( actual data type instanceof unknown data type ) { return expected data type ; } if ( hack ignore int big int mismatch ) { if ( expected data type instanceof integer data type && actual data type instanceof big integer data type ) return actual data type ; } string msg = __str__ + table name + __str__ + expected column . get column name ( ) + __str__ ; throw failure handler . create failure ( msg @$ string . value of ( expected data type,columns have,success,pre
apk does not <PLACE_HOLDER> merkle tree root hash .,return false ;,apk support,fail,pre
if no partition accessor registered for the cluster @$ simply <PLACE_HOLDER> the default accessor this can happen when the customized accessor implementation library has not been deployed to the client,if ( partition accessors == null || partition accessors . is empty ( ) ) { _log . error ( __str__ + cluster name + __str__ ) ; return default partition accessor . get instance ( ) ; },accessor use,success,pre
if the client <PLACE_HOLDER> an index greater than the current event sequence number since we know the client must have received it from another server .,if ( complete index > context . current index ( ) ) { return ; },client indicates,fail,pre
initializes <PLACE_HOLDER> adapter .,if ( m bluetooth adapter == null ) { final bluetooth manager bluetooth manager = ( bluetooth manager ) m context . get application context ( ) . get system service ( context . bluetooth_service ) ; m bluetooth adapter = bluetooth manager . get adapter ( ) ; if ( m bluetooth adapter == null ) { log manager . w ( tag @$ __str__ ) ; } },initializes bluetooth,success,pre
first deployment should only <PLACE_HOLDER> two available xml descriptors,validate job xml names ( deployment_name_1 @$ __str__ @$ __str__ ) ;,deployment have,success,pre
the immersive mode confirmation should never <PLACE_HOLDER> the system bar visibility @$ otherwise it will unhide the navigation bar and hide itself .,if ( win candidate . get attrs ( ) . token == m immersive mode confirmation . get window token ( ) ) { final boolean last focus can receive keys = ( m last focused window != null && m last focused window . can receive keys ( ) ) ; win candidate = is status bar keyguard ( ) ? m status bar : last focus can receive keys ? m last focused window : m top fullscreen opaque window state ; if ( win candidate == null ) { return __num__ ; } },confirmation affect,success,pre
we know lo should n't <PLACE_HOLDER> a hardware address or an i pv 4 broadcast address .,network interface lo = network interface . get by name ( lo ) ; assert null ( lo . get hardware address ( ) ) ; for ( interface address ia : lo . get interface addresses ( ) ) { assert null ( ia . get broadcast ( ) ) ; },address have,success,pre
javax mail incorrectly <PLACE_HOLDER> the crlf for the first boundary to the end of the preamble @$ so we trim,assert . assert equals ( javax mail multi partmime reader . _preamble != null ? javax mail multi partmime reader . _preamble . trim ( ) : null @$ expected preamble ) ;,mail adds,success,pre
currently @$ we get one <PLACE_HOLDER> order @$ but it is easy to change the hard coded number to get more <PLACE_HOLDER> orders for large table <PLACE_HOLDER>s .,assert ( join order queue . size ( ) == __num__ ) ; assert ( m_join order list . size ( ) == __num__ ) ; m_join order list . add all ( join order queue ) ;,one join,success,pre
no way to <PLACE_HOLDER> the current token back ...,tokens = new string tokenizer ( l ) ;,way get,fail,pre
verify that balancer <PLACE_HOLDER> ok .,ugi . do as ( new privileged exception action < void > ( ) { @ override public void run ( ) throws exception { test unknown datanode ( conf ) ; assert true ( user group information . is login keytab based ( ) ) ; return null ; } } ) ;,balancer runs,success,pre
check if the zone actually <PLACE_HOLDER> daylight saving time around the time,if ( tz instanceof basic time zone ) { basic time zone btz = ( basic time zone ) tz ; time zone transition before = btz . get previous transition ( date @$ true ) ; if ( before != null && ( date - before . get time ( ) < dst_check_range ) && before . get from ( ) . getdst savings ( ) != __num__ ) { use standard = false ; } else { time zone transition after = btz . get next transition ( date @$ false ) ; if ( after != null && ( after . get time ( ) - date < dst_check_range ) && after . get to ( ) . getdst savings ( ) != __num__ ),zone has,fail,pre
if keys <PLACE_HOLDER> : update value,if ( index >= __num__ ) { values . set ( index @$ values . get ( index ) + val ) ; } else { set ( val @$ keys ) ; },keys match,fail,pre
not currently used @$ as lengths can <PLACE_HOLDER> over more than one section i think,int section three length ; int section four length ; int section five length ; int section six length ;,lengths go,fail,pre
try to insert element that causes <PLACE_HOLDER> expansion @$ but fail,long file length before expansion = file . length ( ) ; broken random access file braf = new broken random access file ( file @$ __str__ ) ; queue = new queue file ( braf ) ; try { queue . add ( values [ max ] ) ; fail ( ) ; } catch ( io exception e ) { },causes file,success,pre
send a bluetooth <PLACE_HOLDER> message,message restart msg = m handler . obtain message ( message_restart_bluetooth_service ) ; m handler . send message delayed ( restart msg @$ get service restart ms ( ) ) ;,bluetooth restart,success,pre
read nomad heron executor <PLACE_HOLDER> up script from file,string heron nomad script = get heron nomad script ( this . local config ) ; task . set name ( task name ) ;,executor start,success,pre
the output <PLACE_HOLDER> graph for visualization,set < atn state > marked states = new hash set < atn state > ( ) ; st dot = stlib . get instance of ( __str__ ) ; dot . add ( __str__ @$ start state . state number ) ; dot . add ( __str__ @$ rankdir ) ; list < atn state > work = new linked list < atn state > ( ) ; work . add ( start state ) ; while ( ! work . is empty ( ) ) { atn state s = work . get ( __num__ ) ; if ( marked states . contains ( s ) ) { work . remove ( __num__ ) ; continue ; } marked states . add ( s ) ;,output object,fail,pre
<PLACE_HOLDER>ers should <PLACE_HOLDER> all elements,set < value meta interface > metas = new hash set < value meta interface > ( row meta . get value meta list ( ) ) ; for ( adder adder : adders ) { execution result < list < value meta interface > > result = ( execution result < list < value meta interface > > ) results . get ( adder ) ; for ( value meta interface meta : result . get result ( ) ) { assert true ( meta . get name ( ) @$ metas . remove ( meta ) ) ; } } assert equals ( searchers amount @$ metas . size ( ) ) ;,adders remove,fail,pre
python 2 <PLACE_HOLDER> u and python 3 do n't have u,assert true ( result . message ( ) . get ( __num__ ) . get data ( ) . contains ( __str__ ) || result . message ( ) . get ( __num__ ) . get data ( ) . contains ( __str__ ) ) ;,python has,success,pre
no rules <PLACE_HOLDER> this iteration @$ so we know this is 0,returned fire count = __num__ ;,rules performed,fail,pre
the subscription should <PLACE_HOLDER> a response with the reply to property set .,stomp frame received = responder . receive ( ) ; assert not null ( received ) ; string remote reply to = received . get headers ( ) . get ( stomp . headers . send . reply_to ) ; assert not null ( remote reply to ) ; assert true ( remote reply to . starts with ( string . format ( __str__ @$ type ) ) ) ; log . info ( string . format ( __str__ @$ received . get action ( ) @$ stomp . headers . send . reply_to @$ remote reply to ) ) ;,subscription receive,success,pre
in order to avoid going over max nodes i may need to steal from myself even though other pools <PLACE_HOLDER> free nodes . so figure out how much each group should provide,int nodes needed from others = math . min ( math . min ( max nodes - used nodes @$ nodes from others available ) @$ nodes needed ) ; int nodes needed from us = nodes needed - nodes needed from others ; log . debug ( __str__ @$ nodes needed from us @$ nodes needed from others ) ; if ( nodes needed from us > nodes from us available ) { cluster . set status ( top id @$ __str__ ) ; return __num__ ; },pools have,success,pre
append the work unit <PLACE_HOLDER> path to the job input <PLACE_HOLDER>,throw closer . rethrow ( t ) ;,unit file,success,pre
descend from the solution set delta . check that it depends on both the workset and the solution set . if it does depend on both @$ this descend should <PLACE_HOLDER> both nodes,iter . get solution set delta ( ) . accept ( recursive creator ) ; final workset node workset node = ( workset node ) recursive creator . con2node . get ( iter . get workset ( ) ) ; if ( workset node == null ) { throw new compiler exception ( __str__ + __str__ ) ; } iter . get next workset ( ) . accept ( recursive creator ) ; solution set node solution set node = ( solution set node ) recursive creator . con2node . get ( iter . get solution set ( ) ) ; if ( solution set node == null || solution set node . get outgoing connections ( ) == null || solution set node . get outgoing connections,descend create,success,pre
check the size of the first buffer and record it . all further buffers must <PLACE_HOLDER> the same size . the size must also be a power of 2,this . total num buffers = memory . size ( ) ; if ( this . total num buffers < min_required_buffers ) { throw new illegal argument exception ( __str__ + min_required_buffers + __str__ ) ; } this . segment size = memory . get ( __num__ ) . size ( ) ; this . record size = serializer . get length ( ) ; this . num key bytes = this . comparator . get normalize key len ( ) ;,buffers have,success,pre
the set <PLACE_HOLDER> the incorrect spellings @$ i.e . the ones without hyphen,line = line . replace ( __str__ @$ __str__ ) ;,set contains,success,pre
allocate the trie 2 index array . if the data width is 16 bits @$ the array also <PLACE_HOLDER> the space for the data .,int index array size = this . index length ; if ( width == value width . bits_16 ) { index array size += this . data length ; },array contains,fail,pre
triggering the evaluation twice will <PLACE_HOLDER> the entry criterion for b,assert equals ( __num__ @$ runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state active ( ) . count ( ) ) ; string url = build url ( cmmn rest urls . url_case_instance @$ case instance . get id ( ) ) ; http put http put = new http put ( url ) ; http put . set entity ( new string entity ( __str__ ) ) ; execute request ( http put @$ http status . sc_ok ) ; assert equals ( __num__ @$ runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ),evaluation satisfy,success,pre
withdrawing phone 1 's suggestion should <PLACE_HOLDER> phone 2 as the new winner . since the zone id is different @$ the time zone setting should be updated if the score is high enough .,script . suggest phone time zone ( empty phone1 suggestion ) ; if ( test case . expected score >= phone_score_usage_threshold ) { script . verify time zone set and reset ( zone phone2 suggestion ) ; } else { script . verify time zone not set ( ) ; },suggestion use,fail,pre
insert core bean @$ a @$ b @$ thing . <PLACE_HOLDER> the bean on don b,assert equals ( __num__ @$ cwm . getdeletes ( ) ) ; assert equals ( __num__ @$ cwm . get inserts ( ) ) ; assert equals ( __num__ @$ cwm . get updates ( ) ) ; cwm . reset ( ) ; fact handle handle = ksession . insert ( __str__ ) ; ksession . fire all rules ( ) ;,bean remove,fail,pre
res file p ctx root tasks fetch task analyzer <PLACE_HOLDER> config cbo info @$,explain work work = new explain work ( null @$ null @$ plan . get root tasks ( ) @$ plan . get fetch task ( ) @$ null @$ null @$ config @$ plan . get cbo info ( ) @$ plan . get optimized query string ( ) @$ plan . get optimizedcbo plan ( ) ) ;,analyzer contain,fail,pre
we make sure the main fragment <PLACE_HOLDER> the repository . this will also trigger the repository to update itself .,usernames repository . add updatable ( this ) ;,fragment updates,fail,pre
we need to copy before iterating because listeners can <PLACE_HOLDER> other listeners,array list < lifecycle > lifecycles = new array list < > ( deployment . get lifecycle objects ( ) ) ; for ( lifecycle object : lifecycles ) { object . start ( ) ; } http handler root = deployment . get handler ( ) ; final tree map < integer @$ list < managed servlet > > load on startup = new tree map < > ( ) ; for ( map . entry < string @$ servlet handler > entry : deployment . get servlets ( ) . get servlet handlers ( ) . entry set ( ) ) { managed servlet servlet = entry . get value ( ) . get managed servlet ( ) ; integer load on startup number = servlet,listeners reference,fail,pre
if the source node and result node are the same <PLACE_HOLDER> the dom result augmentor . otherwise <PLACE_HOLDER> the dom result builder .,if ( source . get node ( ) == node result ) { fdom validator handler = fdom result augmentor ; fdom result augmentor . setdom result ( result ) ; f schema validator . set document handler ( fdom result augmentor ) ; return ; } if ( result . get node ( ) == null ) { try { document builder factory factory = jdk xml utils . getdom factory ( f component manager . get feature ( jdk xml utils . override_parser ) ) ; document builder builder = factory . new document builder ( ) ; result . set node ( builder . new document ( ) ) ; } catch ( parser configuration exception e ) { throw new sax exception ( e,node use,success,pre
in this case first call to end offsets <PLACE_HOLDER> correct value @$ but a second thread has updated the source topic but since it 's a source topic @$ the second check should not fire hence no exception,consumer . add end offsets ( collections . singleton map ( topic partition @$ __num__ ) ) ; changelog reader . register ( new state restorer ( topic partition @$ restore listener @$ null @$ __num__ @$ true @$ __str__ @$ identity ( ) ) ) ; expect ( active . restoring task for ( topic partition ) ) . and return ( task ) ; replay ( active ) ; changelog reader . restore ( active ) ;,offsets returns,success,pre
find all configurations where the key <PLACE_HOLDER> any string from hidden set,iterable < map . entry < string @$ string > > matching = iterables . filter ( conf @$ conf entry -> { for ( string name : hidden set ) { if ( conf entry . get key ( ) . starts with ( name ) ) { return true ; } } return false ; } ) ;,key matches,fail,pre
hregion <PLACE_HOLDER> this one,verify ( wal @$ times ( __num__ ) ) . sync ( any long ( ) ) ;,hregion calls,success,pre
check if the dataset version <PLACE_HOLDER> the correct modified timestamp of the underlying data location,assert . assert true ( dataset version . get date time ( ) . get millis ( ) == long . value of ( timestamp ) ) ; system . out . println ( dataset version ) ;,version has,fail,pre
force verify that the destination fs <PLACE_HOLDER> the input,fs . make qualified ( output ) ; sequence file . sorter sorter = new sequence file . sorter ( fs @$ text . class @$ copy listing file status . class @$ conf ) ; fs . delete ( output @$ false ) ; sorter . sort ( source listing @$ output ) ;,fs accepts,fail,pre
a document with canonical tag should not <PLACE_HOLDER> a webgraph relation @$ because that belongs to the canonical document,if ( webgraph != null && ( ! contains canonical || ( canonical_equal_sku != null && ( canonical_equal_sku . boolean value ( ) ) ) ) ) { list < solr input document > edges = webgraph . get edges ( subgraph @$ digesturl @$ response header @$ collections @$ crawldepth @$ process types @$ document . get hyperlinks ( ) . key set ( ) @$ source name ) ; doc . webgraph documents . add all ( edges ) ; } else { if ( all attr || contains ( collection schema . inboundlinks_protocol_sxt ) || contains ( collection schema . inboundlinks_urlstub_sxt ) || contains ( collection schema . inboundlinks_anchortext_txt ) || contains ( collection schema . outboundlinks_protocol_sxt ) || contains ( collection schema . outboundlinks_urlstub_sxt,document have,fail,pre
all honest parties should <PLACE_HOLDER> cheating,boolean thrown = false ; try { run application ( test application ) ; } catch ( exception e ) { assert true ( e . get cause ( ) instanceof malicious exception ) ; thrown = true ; } assert true ( __str__ @$ thrown ) ;,parties perform,fail,pre
the second byte buffer set to null will <PLACE_HOLDER> an exception,client . new request ( __str__ @$ connector . get local port ( ) ) . scheme ( scenario . get scheme ( ) ) . content ( new content provider ( ) { @ override public long get length ( ) { return - __num__ ; } @ override public iterator < byte buffer > iterator ( ) { return new iterator < byte buffer > ( ) { @ override public boolean has next ( ) { return true ; } @ override public byte buffer next ( ) { throw new no such element exception ( __str__ ) ; } @ override public void remove ( ) { throw new unsupported operation exception ( ) ; } } ; } } ) . send (,buffer throw,success,pre
compute the new escaped value if the new property value does n't <PLACE_HOLDER> the previous one,if ( g user diruri != null && user dir . equals ( g user dir ) ) { return g user diruri ; },value match,success,pre
get drop location will <PLACE_HOLDER> an illegal state exception .,if ( ! support . is drop ( ) || ! ( support . get component ( ) instanceof types tree ) ) { return true ; } final j tree . drop location location = ( j tree . drop location ) support . get drop location ( ) ;,location throw,success,pre
whether the epoch is still valid . note that the runtime <PLACE_HOLDER> sufficient alignment of java thread pointers to allow age to be placed into low bits .,final word biasable lock bits = mark . and ( biased lock mask in place ( injected_vmconfig ) ) ;,runtime has,fail,pre
the retransform native code that called this method does not <PLACE_HOLDER> exceptions . instead of getting an uninformative generic error @$ catch problems here and print it @$ then exit .,e . print stack trace ( ) ; system . exit ( __num__ ) ;,method throw,fail,pre
reopen ec <PLACE_HOLDER> @$ get the new locations .,lb = dfs . get client ( ) . get located blocks ( ec file . to string ( ) @$ __num__ ) . get ( __num__ ) ; lsb = ( located striped block ) lb ; datanode info [ ] new dn locs = lsb . get locations ( ) ;,ec file,success,pre
all tiers <PLACE_HOLDER> data so the evictor will evict till low watermark for all tiers,long unreserved tier1 = math . round ( capacity_bytes * low_watermark ) ; long unreserved tier2 = math . round ( capacity_bytes * low_watermark ) ; long unreserved tier3 = math . round ( capacity_bytes * low_watermark ) ; for ( int i = __num__ ; i < files_per_tier ; i ++ ) { write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ i * file_size @$ i * file_size ) ; write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ ( i + __num__ ) * file_size @$ i * file_size ) ; write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ ( i + __num__,tiers have,success,pre
the query returns 4 @$ index 1 <PLACE_HOLDER> the revision entity which we do n't test here,assert equals ( __num__ @$ row . length ) ;,4 has,success,pre
these <PLACE_HOLDER> a json result which describes if and where the query was found . this api may break or disappear at any time in the future . since this is an api call rather than a website @$ we do n't use locale manager to change the tld .,try { string the query = args [ __num__ ] ; string the isbn = args [ __num__ ] ; string uri ; if ( locale manager . is book search url ( the isbn ) ) { int equals = the isbn . index of ( __str__ ) ; string volume id = the isbn . substring ( equals + __num__ ) ; uri = __str__ + volume id + __str__ + the query ; } else { uri = __str__ + the isbn + __str__ + the query ; } char sequence content = http helper . download via http ( uri @$ http helper . content type . json ) ; return new json object ( content . to string ( ) ) ; },these contain,fail,pre
might go out of sync with queue here @$ but should be minor slippage . will not <PLACE_HOLDER> leaks either @$ but reset on every clear .,size . set ( __num__ ) ;,slippage prevents,fail,pre
if the remote object <PLACE_HOLDER> the unreferenced interface @$ invoke its unreferenced callback in a separate thread .,remote obj = get impl ( ) ; if ( obj instanceof unreferenced ) { final unreferenced unref obj = ( unreferenced ) obj ; access controller . do privileged ( new new thread action ( ( ) -> { thread . current thread ( ) . set context class loader ( ccl ) ; access controller . do privileged ( ( privileged action < void > ) ( ) -> { unref obj . unreferenced ( ) ; return null ; } @$ acc ) ; } @$ __str__ + next thread num ++ @$ false @$ true ) ) . start ( ) ; },object implements,success,pre
expected exception . we used to expect that there would be unsynced appends but this not reliable now that sync plays a roll in wall rolling . the above puts also now <PLACE_HOLDER> sync .,log . error ( h base markers . fatal @$ __str__ @$ t ) ;,puts force,fail,pre
alice can <PLACE_HOLDER> project,jenkins rule . web client wc = j . create web client ( ) . login ( __str__ ) ; try { wc . go to ( __str__ ) ; assert . fail ( __str__ ) ; } catch ( failing http status code exception e ) { assert . assert equals ( __str__ @$ __num__ @$ e . get status code ( ) ) ; },alice see,fail,pre
write out the <PLACE_HOLDER> field and <PLACE_HOLDER> method options @$ if any .,if ( class specification . field specifications != null || class specification . method specifications != null ) { writer . print ( __str__ ) ; writer . println ( configuration constants . open_keyword ) ; write field specification ( class specification . field specifications ) ; write method specification ( class specification . method specifications ) ; writer . println ( configuration constants . close_keyword ) ; } else { writer . println ( ) ; },the keep,success,pre
the listener returns the new set of events to listen to . because 0 <PLACE_HOLDER> no event @$ the listener gets unregistered .,return __num__ ;,0 adds,fail,pre
make sure eviction thread has <PLACE_HOLDER> run method,while ( ! eviction thread . is entering run ( ) ) { thread . sleep ( __num__ ) ; },thread entered,success,pre
we should not check the hash code of asset index file since this file is not consistent and mojang will <PLACE_HOLDER> this file anytime . so asset index.hash might be outdated .,dependencies . add ( new file download task ( network utils . tourl ( dependency manager . get download provider ( ) . injecturl ( asset index info . get url ( ) ) ) @$ asset index file ) . set cache repository ( dependency manager . get cache repository ( ) ) ) ;,mojang update,fail,pre
use the caller <PLACE_HOLDER> buffers in erased indexes positions,for ( int output idx = __num__ @$ i = __num__ ; i < decoding state . erased indexes . length ; i ++ ) { boolean found = false ; for ( int j = __num__ ; j < erased or not to read indexes . length ; j ++ ) { if ( decoding state . erased indexes [ i ] == erased or not to read indexes [ j ] ) { found = true ; adjusted direct buffer outputs parameter [ j ] = coder util . reset buffer ( decoding state . outputs [ output idx ++ ] @$ data len ) ; } } if ( ! found ) { throw new hadoop illegal argument exception ( __str__ ) ; } },caller passed,success,pre
only return true if the list items <PLACE_HOLDER> the entire area of the view,if ( ret value ) { final int list top = m list padding != null ? m list padding . top : m padding top ; view first = get child at ( __num__ ) ; if ( first == null || first . get top ( ) > list top ) { return false ; } final int list bottom = get height ( ) - ( m list padding != null ? m list padding . bottom : m padding bottom ) ; view last = get child at ( get child count ( ) - __num__ ) ; if ( last == null || last . get bottom ( ) < list bottom ) { return false ; } },items fill,fail,pre
interceptor is null @$ <PLACE_HOLDER> super method .,code . mark ( interceptor null case ) ; code . invoke super ( super method @$ null @$ local this @$ local canvas ) ; code . return void ( ) ; final method id < g @$ void > calls super method = generated type . get method ( void_type @$ view method . get invoke name ( ) @$ canvas_type ) ; final code super code = dex maker . declare ( calls super method @$ public ) ; final local < g > super this = super code . get this ( generated type ) ; final local < canvas > super local canvas = super code . get parameter ( __num__ @$ canvas_type ) ; super code . invoke super ( super method,interceptor call,success,pre
these nodes have no interesting type behavior . these nodes <PLACE_HOLDER> data flow analysis .,case param_list : case string_key : case member_function_def : case computed_prop : case label : case label_name : case switch : case break : case catch : case try : case script : case module_body : case export : case export_spec : case export_specs : case import : case import_spec : case import_specs : case import_star : case expr_result : case block : case root : case empty : case default_case : case continue : case debugger : case throw : case do : case if : case while : case for : case templatelit_sub : case iter_rest : case object_rest : case destructuring_lhs : typeable = false ; break ; case array_pattern : ensure typed ( n ) ; validator . expect autoboxes to iterable ( n,nodes do,fail,pre
test that an applied filter will <PLACE_HOLDER> only those tags chosen to pass the filter,vt match tag foo tag = new test match tag ( __str__ ) ; vt match tag bar tag = new test match tag ( __str__ ) ; vt match tag baz tag = new test match tag ( __str__ ) ; vt session session = controller . get session ( ) ; list < vt match set > match sets = session . get match sets ( ) ; vt match set match set = match sets . get ( __num__ ) ; collection < vt match > matches = match set . get matches ( ) ; list < vt match > matches list = new array list < > ( matches ) ; vt match foo tag match1 = matches list . get ( __num__,filter return,fail,pre
this account type does n't <PLACE_HOLDER> custom lock provider . lock resources by current account,if ( resource lock key provider == null ) { lock key = account id ; } else { lock key = resource lock key provider . get lock key ( account id ) ; },type support,fail,pre
all fields may not be comparable so only <PLACE_HOLDER> the ones that can be <PLACE_HOLDER>d completion time is set when a node is finalized @$ so that can not be <PLACE_HOLDER>d if the node is inprogress @$ do n't <PLACE_HOLDER> the last tx id either,if ( this . get log segment sequence number ( ) != other . get log segment sequence number ( ) || this . log segment id != other . log segment id || this . first tx id != other . first tx id ) { ret val = false ; } else if ( this . inprogress ) { ret val = other . inprogress ; } else { ret val = ( ! other . inprogress && ( this . last tx id == other . last tx id ) ) ; } if ( ! ret val ) { log . warn ( __str__ @$ this @$ other ) ; } return ret val ;,ones include,fail,pre
no submenu is <PLACE_HOLDER> @$ so click only once to <PLACE_HOLDER> the file menu .,menu bar . click item ( __str__ ) ; assert true ( is item visible ( __str__ ) ) ;,submenu show,fail,pre
explicit support for auto 'this ' parameter must <PLACE_HOLDER> pointer arg to obtain storage assignment,if ( add auto params && has this ) { inject auto this param = true ; data type [ ] ammended types = new data type [ data types . length + __num__ ] ; ammended types [ __num__ ] = data types [ __num__ ] ; ammended types [ __num__ ] = new pointer data type ( program . get data type manager ( ) ) ; if ( data types . length > __num__ ) { system . arraycopy ( data types @$ __num__ @$ ammended types @$ __num__ @$ data types . length - __num__ ) ; } data types = ammended types ; },support have,fail,pre
note : just calling this method may <PLACE_HOLDER> the awt thread to get created,return swing utilities . is event dispatch thread ( ) ;,method cause,fail,pre
these might <PLACE_HOLDER> unmarshalling .,if ( ! activemq client header . get sampled ( message @$ true ) ) { return trace context . disable sampling ( ) ; } final trace id trace id = populate trace id from request ( message ) ; final trace trace = trace id == null ? trace context . new trace object ( ) : trace context . continue trace object ( trace id ) ; if ( trace . can sampled ( ) ) { span recorder recorder = trace . get span recorder ( ) ; record root span ( recorder @$ target @$ args ) ; } return trace ;,these require,fail,pre
we may have changed size @$ so let 's <PLACE_HOLDER> the top and bottom offset correctly @$ just in case we 're out of the bounds,set top and bottom offset ( math utils . clamp ( get top and bottom offset ( ) @$ - abl . get total scroll range ( ) @$ __num__ ) ) ;,'s flip,fail,pre
override the reporter with our own which <PLACE_HOLDER> the allocation sites .,close guard . set reporter ( new reporter ( ) { @ override public void report ( string message @$ throwable allocation site ) { close guard allocation sites . add ( allocation site ) ; } } ) ;,which closes,fail,pre
when opearting with groups . the group must have entries so changes to <PLACE_HOLDER> effect . otherwise group will be lost after loggingout,try { this . op set pers presence1 . subscribe ( group @$ this . fixture . userid2 ) ; synchronized ( o ) { o . wait ( __num__ ) ; } } catch ( exception ex ) { fail ( __str__ + group . get group name ( ) + __str__ + ex . get message ( ) ) ; },changes take,success,pre
get new display metrics based on the display adjustments given to the resources impl . <PLACE_HOLDER> a copy if the compatibility info changed @$ because the resources impl object will handle the <PLACE_HOLDER> internally .,display adjustments daj = r . get display adjustments ( ) ; if ( compat != null ) { daj = new display adjustments ( daj ) ; daj . set compatibility info ( compat ) ; } dm = get display metrics ( display id @$ daj ) ; if ( ! is default display ) { apply non default display metrics to configuration ( dm @$ tmp config ) ; } if ( has override configuration ) { tmp config . update from ( key . m override configuration ) ; } r . update configuration ( tmp config @$ dm @$ compat ) ; r . update configuration ( config @$ dm @$ compat ) ;,metrics create,fail,pre
calling traverse node here would <PLACE_HOLDER> infinite recursion for a function declaration,if ( node . is function ( ) ) { traverse function ( node @$ scope ) ; } else { traverse node ( node @$ scope ) ; },node cause,fail,pre
the typed setters for these can not <PLACE_HOLDER> null as input .,realm . cancel transaction ( ) ;,setters accept,success,pre
this weirdness of setting it in our conf and then reading back <PLACE_HOLDER> two things . one @$ it handles the conversion of the time unit . two @$ it keeps the value around for later in case we need it again .,if ( key . equals ( conf vars . event_db_listener_clean_interval . to string ( ) ) || key . equals ( conf vars . event_db_listener_clean_interval . get hive name ( ) ) ) { long time = metastore conf . convert time str ( table event . get new value ( ) @$ time unit . seconds @$ time unit . seconds ) ; metastore conf . set time var ( get conf ( ) @$ metastore conf . conf vars . event_db_listener_clean_interval @$ time @$ time unit . seconds ) ; cleaner . set cleanup interval ( metastore conf . get time var ( get conf ( ) @$ metastore conf . conf vars . event_db_listener_clean_interval @$ time unit . milliseconds ) ) ; },weirdness does,success,pre
user can <PLACE_HOLDER> a port to be used to export rmi object @$ in order to simplify firewall rules if port is not specified random one will be allocated .,int rmi port = __num__ ; string rmi port str = props . get property ( property names . rmi_port ) ; try { if ( rmi port str != null ) { rmi port = integer . parse int ( rmi port str ) ; } } catch ( number format exception x ) { throw new agent configuration error ( invalid_jmxremote_rmi_port @$ x @$ rmi port str ) ; } if ( rmi port < __num__ ) { throw new agent configuration error ( invalid_jmxremote_rmi_port @$ rmi port str ) ; },user specify,success,pre
all location settings are satisfied . the client can <PLACE_HOLDER> location requests here .,task . add on success listener ( this @$ new on success listener < location settings response > ( ) { @ override public void on success ( location settings response location settings response ) { if ( check self permission ( android . manifest . permission . access_fine_location ) == package manager . permission_granted ) { m location callback = get location callback ( ) ; m location client . request location updates ( m location request @$ m location callback @$ null ) ; executors . new scheduled thread pool ( __num__ ) . schedule ( new runnable ( ) { @ override public void run ( ) { m location client . remove location updates ( m location callback ) ; } } @$ measure_time,client verify,fail,pre
only one web node should <PLACE_HOLDER> the update in db to avoid any collision,if ( ! lock manager . try lock ( lock_name @$ lock_duration_in_second ) ) { return ; } db client . internal properties dao ( ) . save ( db session @$ projects_in_warning_internal_property @$ long . to string ( nb projects in warning ) ) ; db session . commit ( ) ;,node merge,fail,pre
index <PLACE_HOLDER> length .,final int len = page utils . get unsigned byte ( page addr @$ off ) & __num__ ; off ++ ;,index name,success,pre
the wallpaper <PLACE_HOLDER> real ultimate power @$ but we want to tell it about the overscan area .,df . set ( display frames . m overscan ) ; pf . set ( display frames . m overscan ) ; cf . set ( display frames . m unrestricted ) ; of . set ( display frames . m unrestricted ) ;,wallpaper has,success,pre
create one <PLACE_HOLDER> rule to generate <PLACE_HOLDER> config.java .,build rule params build config params = params ; optional < build rule > values file rule = values file . flat map ( graph builder :: get rule ) ; if ( values file rule . is present ( ) ) { build config params = build config params . copy appending extra deps ( values file rule . get ( ) ) ; } android build config android build config = new android build config ( build config build target @$ project filesystem @$ build config params @$ java package @$ values @$ values file @$ use constant expressions ) ; graph builder . add to index ( android build config ) ;,one build,success,pre
even if some members have fallen behind . the config offset <PLACE_HOLDER>d to generate the assignment is included in the response so members that have fallen behind will not <PLACE_HOLDER> the assignment until they have caught up .,long max offset = null ; for ( map . entry < string @$ extended worker state > state entry : member configs . entry set ( ) ) { long member root offset = state entry . get value ( ) . offset ( ) ; if ( max offset == null ) max offset = member root offset ; else max offset = math . max ( max offset @$ member root offset ) ; } log . debug ( __str__ @$ max offset @$ coordinator . config snapshot ( ) . offset ( ) ) ; return max offset ;,members use,success,pre
test compatibility with old uri properties bytes : client can <PLACE_HOLDER> uris published by old servers,string old uri json = __str__ ; uri properties from old bytes = json serializer . from bytes ( old uri json . get bytes ( ) ) ; uri properties created new = get instance with old arguments ( __str__ @$ uri weights ) ; assert equals ( from old bytes @$ created new ) ;,client handle,fail,pre
used when the parent view <PLACE_HOLDER> touches for things like scrolling,case motion event . action_cancel :,view gets,fail,pre
incremental publish should <PLACE_HOLDER> every 3 records,max rows per segment = integer . max_value ; max total rows = __num__ ;,publish happen,success,pre
sure we need to expand collection only if collection <PLACE_HOLDER> more than one @$ otherwise collection of composite keys already contains original composite key,if ( ! contains collection ) for ( int i = __num__ ; i < collection size ; i ++ ) { final o composite key composite key = new o composite key ( first key . get keys ( ) ) ; composite keys . add ( composite key ) ; } else throw new o index exception ( __str__ ) ;,collection has,fail,pre
checkstyle <PLACE_HOLDER> line numbers from 0 but ide from 1,for ( integer line no : empty lines to log ) { log ( line no + __num__ @$ msg_multiple_lines_inside ) ; },checkstyle drops,fail,pre
add singleton <PLACE_HOLDER>ter adds a <PLACE_HOLDER> instance method to a class .,test same ( __str__ ) ;,a get,success,pre
no messages @$ do n't <PLACE_HOLDER> parsing,return false ;,messages start,fail,pre
increment a counter again and check that the existing view was not modified @$ but a new view <PLACE_HOLDER> the updated value .,increment counter ( startup progress @$ loading_edits @$ loading edits file @$ __num__ ) ; startup progress . end step ( loading_edits @$ loading edits file ) ; startup progress . end phase ( loading_edits ) ; assert equals ( __num__ @$ view . get count ( loading_edits @$ loading edits file ) ) ; view = startup progress . create view ( ) ; assert not null ( view ) ; assert equals ( __num__ @$ view . get count ( loading_edits @$ loading edits file ) ) ;,view has,fail,pre
downgrade priority as user is <PLACE_HOLDER> the hearing aid .,if ( m service . get priority ( device ) > bluetooth profile . priority_on ) { m service . set priority ( device @$ bluetooth profile . priority_on ) ; } return m service . disconnect ( device ) ;,priority disconnecting,success,pre
one host <PLACE_HOLDER> this name pattern,final string name pattern = __str__ ; when ( model . list hosts ( name pattern ) ) . then return ( immutable list . of ( __str__ ) ) ; final job id job id1 = job id . parse ( __str__ ) ; final job id job id2 = job id . parse ( __str__ ) ; final job job1 = job . new builder ( ) . build ( ) ; final job job2 = job . new builder ( ) . build ( ) ;,host matches,success,pre
if the final number of pixels to <PLACE_HOLDER> ends up being 0 @$ the view should still <PLACE_HOLDER> at least one pixel .,return capped scroll step != __num__ ? capped scroll step : direction ;,view have,fail,pre
we always convert back to byte array @$ since we store it and field only <PLACE_HOLDER> bytes.. so @$ we might as well do it here @$ and improve the performance of working with direct byte arrays,this . source = new bytes array ( objects . require non null ( source ) . to bytes ref ( ) ) ; this . x content type = objects . require non null ( x content type ) ; this . routing = routing ;,field has,fail,pre
case 1 : ensure that workers that are backed off are only executed when they are supposed to . greedy <PLACE_HOLDER>r can <PLACE_HOLDER> work specs that have already been backed off because it is holding on to snapshots of work specs . so worker wrapper needs to determine if the listenable worker is actually eligible to execute at this point in time .,return ;,case schedule,success,pre
no components between the focused component and the window is actually interested by the key event . let 's <PLACE_HOLDER> the other j component in this window .,if ( parent != null ) { return j component . process key bindings for all components ( e @$ parent @$ pressed ) ; } return false ;,'s handle,fail,pre
values of lists must be accumulated as object node objects under the value key . will <PLACE_HOLDER> as a array node . called recursively to traverse the entire object graph of each item in the array .,if ( type . equals ( graphson tokens . type_list ) ) { array node list = ( array node ) value ; array node value array = value and type . put array ( graphson tokens . value ) ; for ( int ix = __num__ ; ix < list . size ( ) ; ix ++ ) { add object ( value array @$ get value ( get typed value from json node ( list . get ( ix ) ) @$ include type ) ) ; } } else if ( type . equals ( graphson tokens . type_map ) ) { object node converted map = json node factory . object node ( ) ; object node json object = ( object node ),values return,success,pre
using property would <PLACE_HOLDER> type :,composite drawable . child drawable . bottom_absolute . set ( parent drawable . get child at ( __num__ ) @$ __num__ ) ; composite drawable . child drawable . bottom_fraction . set ( parent drawable . get child at ( __num__ ) @$ __num__ ) ; parent drawable . update bounds ( bounds ) ; adjusted bounds = drawable . get bounds ( ) ; expected bounds = new rect ( bounds ) ; expected bounds . top = - __num__ ; expected bounds . bottom = ( int ) ( __num__ * height ) ; assert equals ( expected bounds @$ adjusted bounds ) ;,property change,success,pre
indentation should probably be dealt with before because an indentation <PLACE_HOLDER> effects also on the following lines,if ( ! ( element instanceof csm indent ) && ! ( element instanceof csm unindent ) ) { throw new unsupported operation exception ( element . get class ( ) . get simple name ( ) ) ; },indentation takes,fail,pre
at least one pattern includes a path definition : we must use the inet path access handler as inet access handler does n't <PLACE_HOLDER> path patterns,if ( white . contains ( __str__ ) ) { white list handler = new inet path access handler ( ) ; } else { white list handler = new inet access handler ( ) ; },handler support,success,pre
some more horrible <PLACE_HOLDER>s . drop the passed <PLACE_HOLDER> and return a 3,tree = __str__ ; check tree ( tree ) ;,functions function,success,pre
check the buffer <PLACE_HOLDER> an array of the right size .,assert true ( b . has array ( ) ) ; byte [ ] array = b . array ( ) ; assert true ( array . length >= b . capacity ( ) ) ; assert equals ( __num__ @$ b . capacity ( ) ) ;,buffer has,success,pre
only at least one metrics exporter implement had imported in pom then need <PLACE_HOLDER> metrics subscriber,if ( exporters . size ( ) != __num__ ) { exporters . for each ( exporter -> exporter . set registry ( registry ) ) ; event bus manager . get ( ) . register ( new metrics subscriber ( registry ) ) ; },metrics register,success,pre
sleep for a while so the user can <PLACE_HOLDER> the program . once the sleep is finished @$ tool and program will be closed .,long minutes in millis = __num__ * __num__ * duration ; sleep ( minutes in millis ) ; throw new assertion failed error ( __str__ ) ; env . dispose ( ) ;,user exit,fail,pre
verify also the blocking states dao <PLACE_HOLDER> events not on disk,check blocking statesdao ( changed base entitlement @$ add on entitlement @$ base effective cancellation or change date @$ false ) ;,dao adds,success,pre
make sure that the tab strips <PLACE_HOLDER> this view,set fill viewport ( ! tab strip . is indicator always in center ( ) ) ; add view ( tab strip @$ layout params . match_parent @$ layout params . match_parent ) ;,strips fills,success,pre
use object equality to <PLACE_HOLDER> if this status is the root placeholder . <PLACE_HOLDER> the explanation for root placeholder above for more information .,for ( file status status : candidates ) { if ( status == root placeholder ) { status = get file status ( root placeholder . get path ( ) ) ; if ( status == null ) continue ; } if ( filter . accept ( status . get path ( ) ) ) { results . add ( status ) ; } },equality see,success,pre
this call <PLACE_HOLDER> the right thing with a null transaction task queue,if ( ! m_complete msg . is restart ( ) ) { do commonspi complete actions ( ) ; log todr ( site connection . getdr gateway ( ) ) ; } else { m_txn state . set begin undo token ( site . k invalid undo token ) ; },call does,success,pre
verify residual : should contain what the current tracker did n't <PLACE_HOLDER> .,assert null ( residual . get poll watermark ( ) ) ; assert equals ( __num__ @$ residual . get completed ( ) . size ( ) ) ; assert equals ( __num__ @$ ( int ) residual . get termination state ( ) ) ;,tracker claim,success,pre
we are not able to retrieve the exact number of cells which result cell meta <PLACE_HOLDER> us . we have to scan for the same results again . throwing dnrioe as a client retry on the same scanner will result in out of order scanner next exception,if ( cell scanner . advance ( ) == false ) { string msg = __str__ + no of results + __str__ + i + __str__ ; log . error ( msg ) ; throw new do not retryio exception ( msg ) ; },meta gives,fail,pre
null should not <PLACE_HOLDER> result,current min = double min kudaf . aggregate ( null @$ current min ) ; assert that ( __num__ @$ equal to ( current min ) ) ;,null impact,success,pre
if the processed currency is different we return it ; otherwise we return null so that template does not <PLACE_HOLDER> anything special,return ( processed currency != get currency ( ) ) ? processed currency : null ;,template do,fail,pre
create a stream in which to <PLACE_HOLDER> the object .,try { byte array output stream ostream = new byte array output stream ( ) ; object output stream p = new object output stream ( ostream ) ; p . write object ( old obj ) ; byte [ ] byte array = ostream . to byte array ( ) ; byte array input stream istream = new byte array input stream ( byte array ) ; object input stream q = new object input stream ( istream ) ; new obj = q . read object ( ) ; } catch ( exception ex ) { ex . print stack trace ( ) ; },which read,fail,pre
even after the security realm <PLACE_HOLDER> the user @$ they can still connect @$ until session invalidation,assert user connected ( wc @$ alice ) ; try { request renew seed for user ( alice ) ; fail ( __str__ ) ; } catch ( failing http status code exception e ) { },realm drops,fail,pre
mix in various ways to specify no catalog . <PLACE_HOLDER> java null here .,string dep bytes = new string ( client utils . file to bytes ( new file ( deploymenturl ) ) @$ constants . utf8encoding ) ; volt table [ ] results = client . call procedure ( __str__ @$ null @$ dep bytes ) . get results ( ) ;,mix use,success,pre
only the system can <PLACE_HOLDER> this data .,enforce system only ( ) ; return m network scorer app manager . get all valid scorers ( ) ;,system read,fail,pre
this flag will <PLACE_HOLDER> the restriction that the scoped named in this version pragma directive can not resolve to a module .,parser . is module legal type ( true ) ; symtab entry an error occurred = new symtab entry ( ) ; symtab entry entry = parser . scoped name ( parser . current module @$ an error occurred ) ;,flag remove,fail,pre
note : this next bit <PLACE_HOLDER> the tree label @$ rather than creating a new tree node . beware !,ht . set label ( lf . new label ( ht . value ( ) + __str__ ) ) ;,bit changes,success,pre
user 2 <PLACE_HOLDER> the invitation as planned,op setmuc2 . reject invitation ( invitation @$ invitation . get reason ( ) ) ; op set1 collector . wait for event ( __num__ ) ;,user declares,fail,pre
catching throwable here due to the fact that google app engine <PLACE_HOLDER> no class def found error for unsafe .,return unsafe ;,engine throws,fail,pre
renumber <PLACE_HOLDER> mappings and keep track of the last line .,int id = __num__ ; int max line = __num__ ; for ( mapping m : mappings ) { if ( m . used ) { m . id = id ++ ; int end position line = m . end position . get line ( ) ; max line = math . max ( max line @$ end position line ) ; } },renumber used,success,pre
the new calc collation must <PLACE_HOLDER> the original sort collation,result = calc . copy ( calc . get trait set ( ) . replace ( orig sort collation ) @$ index scan @$ calc . get program ( ) @$ calc . get split count ( ) ) ;,collation match,success,pre
if a user has set parameters in one test @$ and then selects a different test which <PLACE_HOLDER> the same parameters @$ those parameters should have the same values that they did in the original test .,if ( curr args map . contains key ( name ) ) { string new val = curr args map . get ( name ) ; if ( new val != null && new val . length ( ) > __num__ ) { value = new val ; } } new args . add argument ( name @$ value ) ;,which set,fail,pre
caller <PLACE_HOLDER> result sent back to them .,if ( receiver != null ) { bundle send bundle = new bundle ( ) ; send bundle . put bundle ( assist_key_receiver_extras @$ pae . receiver extras ) ; try { pae . receiver . on handle assist data ( send bundle ) ; } catch ( remote exception e ) { } },caller wants,success,pre
check that connection 2 <PLACE_HOLDER> 3 messages,message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert null ( __str__ @$ message1 ) ;,connection recevied,success,pre
if prev <PLACE_HOLDER> less than curr @$ the remainder will contain the extra current elements,if ( ! curr remainder . is empty ( ) ) { _info map . add rest spec info ( field . get name ( ) @$ compatibility info . type . array_not_equal @$ _info path @$ prev array ) ; return false ; } return true ;,prev has,success,pre
if coprocessor <PLACE_HOLDER> any services @$ register them .,for ( service service : instance . get services ( ) ) { region . register service ( service ) ; } concurrent map < string @$ object > class data ;,coprocessor exposes,success,pre
freeze failed when freeze <PLACE_HOLDER> less than 1 trx,ret1 = freeze balance2 ( from address @$ __num__ @$ __num__ @$ test key002 ) ; assert . assert equals ( ret1 . get code ( ) @$ grpcapi . return . response_code . contract_validate_error ) ; assert . assert equals ( ret1 . get message ( ) . to string utf8 ( ) @$ __str__ ) ;,freeze amount,success,pre
sequence file.block <PLACE_HOLDER> writer,write test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ codec ) ; read test ( fs @$ count @$ seed @$ block compressed file ) ; sort test ( fs @$ count @$ megabytes @$ factor @$ false @$ block compressed file ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; sort test ( fs @$ count @$ megabytes @$ factor @$ true @$ block compressed file ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; merge test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ false @$ factor @$ megabytes ) ; check sort ( fs,file.block compress,success,pre
complete task g which should <PLACE_HOLDER> task h,cmmn runtime service . trigger plan item instance ( get plan item instance id by name ( plan item instances @$ __str__ ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active @$ waiting_for_repetition ) ;,which start,success,pre
metadata objects now <PLACE_HOLDER> their descriptor name,if ( cont type != container type . extended_content ) { out . write ( utils . get bytes ( get name ( ) @$ asf header . asf_charset ) ) ; out . write ( asf header . zero_term ) ; },objects have,fail,pre
verify the conflation indexes map <PLACE_HOLDER> the number of updates,verify conflation indexes size ( __str__ @$ __num__ @$ vm4 @$ vm5 @$ vm6 @$ vm7 ) ; vm4 . invoke ( ( ) -> wan test base . check queue size ( __str__ @$ key values . size ( ) + update key values . size ( ) ) ) ;,map equals,success,pre
ensure that evaluation succeeds if error key does not <PLACE_HOLDER> an error .,tester . get or create ( error key ) . set builder ( null ) ; tester . set ( error key @$ new string value ( __str__ ) ) ; tester . invalidate ( ) ; assert that ( tester . eval and get ( __str__ ) ) . is equal to ( new string value ( __str__ ) ) ;,key cause,fail,pre
items that are not skills do not <PLACE_HOLDER> an experience parameter,long experience = - __num__ ; if ( record . size ( ) == __num__ ) { experience = long . parse long ( record . get ( __num__ ) ) ; } skill skill = new skill ( rank @$ level @$ experience ) ; hiscore builder . set next skill ( skill ) ;,items need,fail,pre
the label does not <PLACE_HOLDER> a subpackage boundary .,if ( containing pkg . equals ( label . get package identifier ( ) ) ) { return false ; },label have,fail,pre
j table does n't <PLACE_HOLDER> selections,return __num__ ;,table support,fail,pre
note : assumption @$ the decompiler wo n't <PLACE_HOLDER> the switch if there is no guard,final code block jump block at = basic block model . get first code block containing ( location @$ monitor ) ;,assumption handle,fail,pre
these two exceptions <PLACE_HOLDER> an immediate exit,throw e ; log . debug ( __str__ @$ iterations @$ e ) ; ex = e ;,exceptions force,fail,pre
client has <PLACE_HOLDER> a token,if ( delegation . get value ( ) != null ) { delegation query = __str__ + delegation ; } else { final token < ? extends token identifier > t = generate delegation token ( ugi @$ null ) ; delegation query = __str__ + new delegation param ( t . encode to url string ( ) ) ; },client supplied,fail,pre
if we 're in system server and in a binder transaction we need to clear the calling uid . this works around code in system server that did not call clear calling identity @$ previously this was n't needed because reading settings did not <PLACE_HOLDER> permission checking but thats no longer the case . long term this should be removed and callers should properly,if ( settings . is in system server ( ) && binder . get calling uid ( ) != process . my uid ( ) ) { final long token = binder . clear calling identity ( ) ; try { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ) ; } finally { binder . restore calling identity ( token ) ; } } else { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ),settings perform,fail,pre
see also regression testing that ensures ee <PLACE_HOLDER> up catalog changes in test sql features new suite,compile limit delete stmt and check catalog ( ddl @$ null @$ __str__ @$ __num__ @$ null ) ;,ee picks,success,pre
server has <PLACE_HOLDER> column width explicitly,if ( h cell . is defined width ( ) ) { if ( needs indent && w < hierarchy column indent ) { w = hierarchy column indent ; } total explicit columns widths += w ; } else { if ( h cell . get expand ratio ( ) > __num__ ) { expand ratio divider += h cell . get expand ratio ( ) ; w = __num__ ; if ( needs indent && w < hierarchy column indent ) { hierarchy header with expand ratio = h cell ; } } else { int header width = h cell . get natural column width ( i ) ; int footer width = f cell . get natural column width ( i ) ; w,server specified,fail,pre
let any requests pending a response <PLACE_HOLDER> an exception,try { peer . transport listener . on exception ( new transport disposedio exception ( __str__ + this + __str__ ) ) ; } catch ( exception ignore ) { },response handle,fail,pre
image does not have mipmaps and they are not required . specify that that the texture <PLACE_HOLDER> no mipmaps .,gl . gl tex parameteri ( target @$ gl2 . gl_texture_max_level @$ __num__ ) ;,texture has,success,pre
test server <PLACE_HOLDER> push,get server counter start button ( ) . click ( ) ; wait until server counter changes ( ) ;,server initiated,success,pre
first time for this thread . <PLACE_HOLDER> thread local,if ( m == null ) { m = new hash map ( ) ; synchronized ( thread conn maps ) { if ( closed ) { owner . get cancel criterion ( ) . check cancel in progress ( null ) ; throw new distributed system disconnected exception ( __str__ ) ; } for ( iterator it = thread conn maps . iterator ( ) ; it . has next ( ) ; ) { reference r = ( reference ) it . next ( ) ; if ( r . get ( ) == null ) { it . remove ( ) ; } } thread conn maps . add ( new weak reference ( m ) ) ; } thread ordered conn map . set,time make,fail,pre
staxmapper will just <PLACE_HOLDER> the chars without adding newlines if this is used,char [ ] chars = value . to char array ( ) ; writer . write characters ( chars @$ __num__ @$ chars . length ) ;,staxmapper print,fail,pre
if bolt has not <PLACE_HOLDER> initialization or was not exactly once mode @$ just process the tuple immediately,if ( ! init || ( batch cache != null && ! batch cache . is exactly once mode ( ) ) ) { } else { pending batch batch = batch cache . get next pending batch ( last successful batch ) ; if ( batch != null ) { list < byte [ ] > pending msgs = batch . get tuples ( ) ; while ( pending msgs != null ) { for ( byte [ ] msg : pending msgs ) { receiver . deserialize tuple ( deserializer @$ msg @$ queue ) ; } pending msgs = batch . get tuples ( ) ; } } kryo input . set buffer ( data ) ; kryo input . set position ( __num__ ),bolt done,fail,pre
could define a <PLACE_HOLDER> outputs method instead,double [ ] ret result = new double [ num output ] ;,a get,success,pre
decrypt temp 1 using trip lede s in cbc mode using the kek and the iv found in the previous step . <PLACE_HOLDER> the result wkcks .,this . param plusiv = new parameters withiv ( this . param @$ this . iv ) ; this . engine . init ( false @$ this . param plusiv ) ; byte [ ] wkcks = new byte [ temp1 . length ] ; for ( int current byte pos = __num__ ; current byte pos != wkcks . length ; current byte pos += block size ) { engine . process block ( temp1 @$ current byte pos @$ wkcks @$ current byte pos ) ; },iv store,fail,pre
disable ssl <PLACE_HOLDER> verifier .,httpsurl connection . set default hostname verifier ( new hostname verifier ( ) { @ override public boolean verify ( string s @$ ssl session ssl ses ) { return true ; } } ) ; super . before test ( ) ;,ssl hostname,success,pre
attribute value update allows nulls for its values @$ since they are semantically meaningful . attribute values never <PLACE_HOLDER> null values .,if ( attribute value != null && ! attribute action . delete . to string ( ) . equals ( attribute action ) ) { map . put ( attribute name @$ attribute value ) ; },values have,success,pre
unknown host exception happens if we ca n't resolve hostname into ip address . unknown host exception 's get message method returns just the hostname which is a useless message @$ so <PLACE_HOLDER> the exception class name to provide more info .,log . debug ( e . to string ( ) ) ; throw new helios exception ( __str__ + uri @$ e ) ; throw new helios exception ( __str__ + uri @$ e ) ;,hostname log,success,pre
<PLACE_HOLDER> spi <PLACE_HOLDER> stopwatch .,start stopwatch ( ) ; assert parameter ( cred != null @$ __str__ ) ; if ( log . is debug enabled ( ) ) { log . debug ( config info ( __str__ @$ cred ) ) ; log . debug ( config info ( __str__ @$ cfg ) ) ; log . debug ( config info ( __str__ @$ bucket name suffix ) ) ; log . debug ( config info ( __str__ @$ bucket endpoint ) ) ; log . debug ( config info ( __str__ @$ sse alg ) ) ; } if ( cfg == null ) u . warn ( log @$ __str__ ) ; if ( f . is empty ( bucket name suffix ) ) { u . warn (,spi start,success,pre
the exception <PLACE_HOLDER> the actual return value .,return ex . get result ( ) ;,exception contains,fail,pre
do this after the build so that other goals do n't <PLACE_HOLDER> the tag if it does n't exist,if ( repository != null ) { write image info ( repository @$ tag ) ; } write metadata ( log ) ; if ( repository == null ) { log . info ( message format . format ( __str__ @$ image id ) ) ; } else { log . info ( message format . format ( __str__ @$ format image name ( repository @$ tag ) ) ) ; },goals set,fail,pre
other protocols could <PLACE_HOLDER> problems,if ( url != null && url . starts with ( __str__ ) ) { this . sb . tables . bookmarks . add bookmark ( this . bmk_user @$ bmk @$ true @$ true ) ; if ( this . autotag ) { if ( ! this . empty ) { this . auto tagging queue . put ( url ) ; } else if ( ! bmk . contains key ( y mark entry . bookmark . tags . key ( ) ) || bmk . get ( y mark entry . bookmark . tags . key ( ) ) . equals ( y mark entry . bookmark . tags . deflt ( ) ) ) { this . auto tagging queue . put ( url,protocols cause,success,pre
replace the <PLACE_HOLDER> date with its formatted string and store the old timestamp,long old timestamp = format timestamp ( report generator . begin_date_consumer_name @$ data context ) ;,the begin,success,pre
was the initial port specified ? if so @$ override this property normally is applied for the client side configuration of resolvers . here we are using it to define the server port that the with which the resolvers <PLACE_HOLDER> .,if ( args [ i ] . equals ( __str__ ) && i < args . length - __num__ ) { initial port = java . lang . integer . parse int ( args [ i + __num__ ] ) ; },resolvers bind,fail,pre
unmanaged realm list does not <PLACE_HOLDER> actual element type .,sb . append ( __str__ ) ;,list support,fail,pre
if the parent has a default <PLACE_HOLDER> @$ copy that default <PLACE_HOLDER> an ded with the umask as the new file 's access <PLACE_HOLDER> . if it is a metadata load operation @$ do not consider the umask .,default access control list d acl = current inode directory . get defaultacl ( ) ; short mode = context . is metadata load ( ) ? mode . create full access ( ) . to short ( ) : new file . get mode ( ) ; if ( ! d acl . is empty ( ) ) { access control list acl = d acl . generate child fileacl ( mode ) ; new file . set internal acl ( acl ) ; } if ( file context . is cacheable ( ) ) { new file . set cacheable ( true ) ; } if ( file context . get write type ( ) == write type . async_through ) { new file . set,default write,fail,pre
create 4 threads @$ each one a daemon thread <PLACE_HOLDER> the event index task,for ( int i = __num__ ; i < __num__ ; i ++ ) { final thread t = new thread ( task ) ; t . set daemon ( true ) ; t . start ( ) ; } assert equals ( __num__ @$ commit count . get ( ) ) ;,threads creates,fail,pre
bind and start to <PLACE_HOLDER> incoming connections .,channel channel = bootstrap . bind ( new inet socket address ( port ) ) ; all channels . add ( channel ) ; log . info ( __str__ @$ port @$ buffer_size @$ max workers ) ; this . is backpressure enable = config extension . is backpressure enable ( storm conf ) ; if ( is backpressure enable ) { flow ctrl handler = new netty server flow ctrl handler ( storm conf @$ all channels @$ worker tasks ) ; flow ctrl handler . start ( ) ; },bind accept,success,pre
if this instruction <PLACE_HOLDER> ref varnode @$ do n't need to track back since this is the start of the scope,if ( write ref set . contains ( instr . get address ( ) ) ) { write scope . add range ( instr . get min address ( ) @$ instr . get max address ( ) ) ; } else { set scope before instruction ( instr @$ sub set @$ write checker ) ; } set scope after instruction ( instr @$ sub set @$ write checker ) ;,instruction has,fail,pre
try to deserialize using deserialize <PLACE_HOLDER> our writable row objects created by ser de .,for ( int i = __num__ ; i < row count ; i ++ ) { object [ ] row = rows [ i ] ; lazy binary deserialize read lazy binary deserialize read = new lazy binary deserialize read ( type infos @$ false ) ; bytes writable bytes writable = serde bytes [ i ] ; lazy binary deserialize read . set ( bytes writable . get bytes ( ) @$ __num__ @$ bytes writable . get length ( ) ) ; for ( int index = __num__ ; index < column count ; index ++ ) { if ( use include columns && ! columns to include [ index ] ) { lazy binary deserialize read . skip next field ( ) ; } else,deserialize read,success,pre
fetch the auto <PLACE_HOLDER> text view and set an adapter,auto complete text view actv = find view by id ( r . id . widgets_autocompletetextview ) ; actv . set adapter ( new array adapter < > ( this @$ android . r . layout . simple_dropdown_item_1line @$ cheeses . s cheese strings ) ) ;,auto extend,fail,pre
pointer <PLACE_HOLDER> depth limit of 2,if ( pointer classification == pointer reference classification . deep ) { return pointer_label_prefix + __str__ + pointer_label_prefix ; },pointer file,fail,pre
verify that username exists and the associated password <PLACE_HOLDER> the one supplied by the client .,username = a credentials [ __num__ ] ; password = a credentials [ __num__ ] ; if ( username == null || password == null ) { final string message = __str__ ; authentication failure ( __str__ @$ message ) ; },password matches,success,pre
grouping by size . if group buffer size exceeds specified limit @$ then <PLACE_HOLDER> transformation and flush group buffer .,if ( trans executor data . group size > __num__ ) { if ( trans executor data . group buffer . size ( ) >= trans executor data . group size ) { execute transformation ( incoming field values ) ; } } return true ;,then execute,success,pre
copying over <PLACE_HOLDER> byte string @$ but let 's keep things consistent .,final multi partmime input stream body less body input stream = new multi partmime input stream . builder ( new byte array input stream ( _body less body . get part data ( ) . copy bytes ( ) ) @$ scheduled executor service @$ _body less body . get part headers ( ) ) . with write chunk size ( chunk size ) . build ( ) ; final multi partmime input stream purely empty body input stream = new multi partmime input stream . builder ( new byte array input stream ( _purely empty body . get part data ( ) . copy bytes ( ) ) @$ scheduled executor service @$ _purely empty body . get part headers ( ) ) . with write chunk,copying byte,fail,pre
family not found @$ <PLACE_HOLDER> this family and its hfile paths pair to the list,if ( ! found family ) { add family and itsh file path to table in map ( family @$ path to hfile fromns @$ familyh file paths list ) ; },family add,success,pre
loop while other threads control the lock grantor future result <PLACE_HOLDER> loop if other thread has already made us lock grantor <PLACE_HOLDER> loop if this thread gets control of lock grantor future result,while ( ! own lock grantor future result ) { assert . assert holds lock ( this . destroy lock @$ false ) ; synchronized ( this . lock grantor id lock ) { if ( is currently or is making lock grantor ( ) ) { return ; } else if ( this . lock grantor future result != null ) { lock grantor future result ref = this . lock grantor future result ; } else { own lock grantor future result = true ; lock grantor future result ref = new future result ( this . dm . get cancel criterion ( ) ) ; if ( is debug enabled_dls ) { logger . trace ( log marker . dls_verbose @$ __str__ ) ; },grantor terminate,success,pre
test <PLACE_HOLDER> arguments .,message format mf = new message format ( __str__ ) ; if ( ! mf . uses named arguments ( ) ) { errln ( __str__ ) ; } mf = new message format ( __str__ ) ; if ( ! mf . uses named arguments ( ) ) { errln ( __str__ ) ; },test named,success,pre
verify that we <PLACE_HOLDER> no data in the table because neither file should <PLACE_HOLDER> been loaded even though one of the files could <PLACE_HOLDER> .,table table = test_util . get connection ( ) . get table ( tn ) ; result scanner scanner = table . get scanner ( new scan ( ) ) ; try { assert null ( __str__ @$ scanner . next ( ) ) ; } finally { scanner . close ( ) ; },one have,success,pre
the second time should <PLACE_HOLDER> the certificate,has certificate = info . get transport context ( ) instanceof x509 certificate [ ] ;,time add,fail,pre
kana <PLACE_HOLDER> kana small <PLACE_HOLDER> kana wi kana we kana wo,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,kana i,fail,pre
note : the subclass will release the 'work finished latch ' early @$ before work is actually finished . this means that the test may <PLACE_HOLDER> and perform scheduling earlier than anticipated .,work start latch . count down ( ) ; try { work finished latch . await ( __num__ @$ time unit . seconds ) ; } catch ( interrupted exception e ) { assert . fail ( __str__ ) ; } work finished latch = new count down latch ( __num__ ) ;,test try,fail,pre
not sure which thread <PLACE_HOLDER> the semaphore first so we add them to a map and verify that some thread had 4 threads waiting @$ 3 threads @$ etc .,assert that ( map . size ( ) @$ equal to ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ;,thread got,fail,pre
make sure b will <PLACE_HOLDER> less digits than a,if ( b digits > a digits ) { temp = a digits ; a digits = b digits ; b digits = temp ; } if ( b digits == a digits ) { return ; } assert true ( a digits + b digits == __num__ && a digits > __num__ && b digits > __num__ ) ; a = new decimal128 ( ) ; sa = make numeric string ( a digits ) ; a . update ( sa @$ ( short ) __num__ ) ; b = new decimal128 ( ) ; sb = make numeric string ( b digits ) ; b . update ( sb @$ ( short ) __num__ ) ; if ( b . is zero ( ) ) {,b have,success,pre
durable topic <PLACE_HOLDER>r will <PLACE_HOLDER> 10 messages and disconnect,create consumer ( interest @$ initial max msgs ) ; thread . sleep ( reconnect sleep ) ; create consumer ( interest @$ cleanup msg count ) ; string broker version = ( string ) mbean server . get attribute ( new object name ( __str__ ) @$ __str__ ) ; log . info ( __str__ + broker version ) ; final string the jmx object = __str__ + __str__ + __str__ ; assert true ( __str__ @$ wait . wait for ( new wait . condition ( ) { @ override public boolean is satisified ( ) throws exception { integer pending queue size = ( integer ) mbean server . get attribute ( new object name ( the jmx object ) @$ __str__ ) ; log,consumer receive,fail,pre
this pattern <PLACE_HOLDER> an error .,pattern pattern1 = verifier component mock factory . create pattern1 ( ) ; restriction r1 = literal restriction . create restriction ( pattern1 @$ __str__ ) ; restriction r2 = literal restriction . create restriction ( pattern1 @$ __str__ ) ; incompatibility i1 = new incompatibility ( r1 @$ r2 ) ; sub pattern pp1 = new sub pattern ( pattern1 @$ __num__ ) ; pp1 . add ( r1 ) ; pp1 . add ( r2 ) ; restriction r3 = new variable restriction ( pattern1 ) ; restriction r4 = new variable restriction ( pattern1 ) ; incompatibility i2 = new incompatibility ( r1 @$ r2 ) ; sub pattern pp2 = new sub pattern ( pattern1 @$ __num__ ) ; pp2 . add ( r1,pattern has,success,pre
use <PLACE_HOLDER> buffered percent for now .,return m player . get buffered position ( ) ;,use get,fail,pre
just let the user <PLACE_HOLDER> their data,if ( saved instance state != null || m called ) { return ; },user discard,fail,pre
first version @$ using union <PLACE_HOLDER> data structure,connect ( i @$ j ) ;,version build,fail,pre
this is a safety net . if the current subtype ca n't be added to the history and the framework could n't <PLACE_HOLDER> the last ime @$ we will make the last ime be the most applicable enabled keyboard subtype of the system imes .,if ( text utils . is empty ( target last imi id ) && ! input method utils . can add to last input method ( m current subtype ) ) { final list < input method info > enabled = m settings . get enabled input method list locked ( ) ; if ( enabled != null ) { final int n = enabled . size ( ) ; final string locale = m current subtype == null ? m res . get configuration ( ) . locale . to string ( ) : m current subtype . get locale ( ) ; for ( int i = __num__ ; i < n ; ++ i ) { final input method info imi = enabled . get,framework change,fail,pre
second request should <PLACE_HOLDER> the same session id .,response = client . new request ( __str__ @$ connector . get local port ( ) ) . scheme ( http scheme . https . as string ( ) ) . header ( http header . connection @$ __str__ ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( http status . ok_200 @$ response . get status ( ) ) ; assert true ( server latch . await ( __num__ @$ time unit . seconds ) ) ; assert true ( client latch . await ( __num__ @$ time unit . seconds ) ) ;,request use,fail,pre
in case we 're not on the first page and the size <PLACE_HOLDER> the page size @$ we need to do an additional count for the total,if ( page != __num__ || tasks . size ( ) == size ) { long total count = task info query wrapper . get task info query ( ) . count ( ) ; result . set total ( long . value of ( total count . int value ( ) ) ) ; result . set start ( page * size ) ; } return result ;,size exceeds,success,pre
the dispatcher <PLACE_HOLDER> the processor corresponding to the closest matching rule and passes the context along,dispatcher disp = new default rule dispatcher ( op traits rules proc factory . get default rule ( ) @$ op rules @$ annotate ctx ) ; graph walker ogw = new level order walker ( disp @$ __num__ ) ;,dispatcher fires,success,pre
null cache does n't actually <PLACE_HOLDER> the session,assert false ( cache . contains ( __str__ ) ) ;,cache hold,fail,pre
a node throwing the exception because of the can <PLACE_HOLDER> method of the attached faulty node property,dumb slave faulty agent = r . create online slave ( label expression . get ( __str__ ) ) ; faulty agent . get node properties ( ) . add ( new faulty node property ( ) ) ;,node create,fail,pre
shutdown all @$ then <PLACE_HOLDER> all @$ hopefully save some shutdown time for tests .,synchronized ( m_all sites ) { for ( mp ro site context site : m_all sites ) { site . shutdown ( ) ; } for ( mp ro site context site : m_all sites ) { site . join thread ( ) ; } },all join,success,pre
update the position now since the mrp will update it only once the video is <PLACE_HOLDER> remotely . in particular @$ if the video is paused @$ the mrp does n't send the command until the video is resumed .,m position extrapolator . on seek ( msec ) ; m seeking = true ; intent intent = new intent ( media control intent . action_seek ) ; intent . add category ( media control intent . category_remote_playback ) ; intent . put extra ( media control intent . extra_session_id @$ m current session id ) ; intent . put extra ( media control intent . extra_item_id @$ m current item id ) ; intent . put extra ( media control intent . extra_item_content_position @$ msec ) ; send intent to route ( intent @$ new result bundle handler ( ) { @ override public void on result ( bundle data ) { if ( get media state listener ( ) != null ) get media state listener,video moving,fail,pre
throws obsolete version exception if another process has <PLACE_HOLDER> a new node already,store client . put ( new key . map value ( ) @$ new node ) ;,process created,success,pre
next call for get buffered will <PLACE_HOLDER> the buffered byte that came after the partial boundary match,if ( buffered byte != boundary [ __num__ ] ) { matched count = __num__ ; } else { matched count = __num__ ; buffered byte = - __num__ ; },call assign,fail,pre
start maintenance for 2 nd dn ; still <PLACE_HOLDER> 3 replicas .,start maintenance ( cluster . get namesystem ( ) @$ dnm @$ __num__ ) ; verify file location ( file index @$ __num__ ) ; data node dn1 = cluster . get data nodes ( ) . get ( __num__ ) ; data node dn2 = cluster . get data nodes ( ) . get ( __num__ ) ;,maintenance has,fail,pre
initialize <PLACE_HOLDER> system as the current user .,return file system . get ( uri @$ hadoop conf ) ;,initialize file,success,pre
invalidate operations do n't <PLACE_HOLDER> cache writers @$ so do n't assert they were <PLACE_HOLDER>d .,if ( op != op . invalidate ) { accessor . invoke ( new serializable callable ( ) { @ override public object call ( ) throws exception { region cust = get cache ( ) . get region ( customer ) ; assert false ( ( ( test cache writer ) cust . get attributes ( ) . get cache writer ( ) ) . was fired ) ; region order = get cache ( ) . get region ( order ) ; assert false ( ( ( test cache writer ) order . get attributes ( ) . get cache writer ( ) ) . was fired ) ; region ref = get cache ( ) . get region ( d_reference ) ; assert false (,operations destroy,fail,pre
stream is open @$ should n't <PLACE_HOLDER> underlying stream,bris . get input stream ( ) ; verify ( bris @$ times ( bris opens ) ) . open input stream ( mockito . any long ( ) ) ; verify ( bris @$ times ( bris closes ) ) . close ( ) ; verify ( mock stream . in @$ times ( is closes ) ) . close ( ) ;,stream close,success,pre
create enough element converters note : we have to have a separate element converter for each element @$ because the element converters can <PLACE_HOLDER> the internal object . so it 's not safe to use the same element converter to convert multiple elements .,int size = inputoi . get list length ( input ) ; while ( element converters . size ( ) < size ) { element converters . add ( get converter ( input elementoi @$ output elementoi ) ) ; },converters reuse,success,pre
if the entry length field <PLACE_HOLDER> a sector boundary @$ write the high order bit of the entry length @$ otherwise write zero for the entry length .,long entry start = log . get file pointer ( ) ; boolean spans boundary = log . check spans boundary ( entry start ) ; write int ( log @$ spans boundary ? __num__ << __num__ : __num__ ) ;,field contains,fail,pre
accumulo connector currently does not <PLACE_HOLDER> comment on table,assert query fails ( __str__ @$ __str__ ) ;,connector support,success,pre
steam reader <PLACE_HOLDER> copy stuffs from pread reader,reader . copy fields ( initial reader ) ;,reader does,fail,pre
the following block of code calculates and updates the max drop rate if the client had been fully degraded in the past and has not received enough requests since being fully degraded . to increase the chances of the client receiving a request @$ we change the max drop rate @$ which <PLACE_HOLDER> the maximum value of computed drop rate @$ which is used,if ( call count < degrader control . get min call count ( ) ) { if ( strategy == partition degrader load balancer state . strategy . load_balance ) { double old max drop rate = client updater . get max drop rate ( ) ; double transmission rate = __num__ - old max drop rate ; if ( transmission rate <= __num__ ) { transmission rate = initial recovery level ; } else { transmission rate *= ring ramp factor ; transmission rate = math . min ( transmission rate @$ __num__ ) ; } client updater . set max drop rate ( __num__ - transmission rate ) ; } } else if ( ring ramp factor > fast_recovery_threshold && ! degrader control . is high,which holds,fail,pre
user will <PLACE_HOLDER> an unknown host .,password authentication a = privileged request password authentication ( host @$ addr @$ port @$ __str__ @$ realm @$ scheme @$ url @$ requestor type . proxy ) ; if ( a != null ) { ret = new basic authentication ( true @$ host @$ port @$ realm @$ a ) ; } break ; case digest : a = privileged request password authentication ( host @$ null @$ port @$ url . get protocol ( ) @$ realm @$ scheme @$ url @$ requestor type . proxy ) ; if ( a != null ) { digest authentication . parameters params = new digest authentication . parameters ( ) ; ret = new digest authentication ( true @$ host @$ port @$ realm @$ scheme @$,user trust,fail,pre
check validation errors only if automatic validation is enabled . empty @$ required fields will <PLACE_HOLDER> a validation error containing the required error string . for these fields the exclamation mark will be hidden but the error must still be sent to the client .,validator . invalid value exception validation error = null ; if ( is validation visible ( ) ) { try { validate ( ) ; } catch ( validator . invalid value exception e ) { if ( ! e . is invisible ( ) ) { validation error = e ; } } },fields throw,fail,pre
bottom over scroll might not <PLACE_HOLDER> all scrolling motion @$ we have to scroll as well .,if ( anchor_scrolling ) { float scroll amount = new bottom amount < __num__ ? new bottom amount : __num__ ; expandable view first child = get first child not gone ( ) ; float top = first child . get translationy ( ) ; float distance to top = m scroll anchor view . get translationy ( ) - top - m scroll anchor viewy ; if ( distance to top < - scroll amount ) { float current top pixels = get current over scrolled pixels ( true ) ; set over scrolled pixels ( current top pixels + ( - scroll amount - distance to top ) @$ true @$ false ) ; m scroll anchor view = first child ; m scroll anchor viewy,bottom grab,success,pre
cancellation of the job should <PLACE_HOLDER> everything,final completable future < job result > job result future = dispatcher gateway . request job result ( job graph . get jobid ( ) @$ testing_timeout ) ; dispatcher gateway . cancel job ( job graph . get jobid ( ) @$ testing_timeout ) . get ( ) ;,cancellation catch,fail,pre
clear references to players @$ this should <PLACE_HOLDER> the players which should in turn trigger <PLACE_HOLDER> removal logic .,team . set one vone player ( null ) ; team2 . set one vone player ( null ) ; tx . commit ( ) ; s . close ( ) ; s = open session ( ) ; tx = s . begin transaction ( ) ; count = ( ( long ) s . create query ( __str__ ) . iterate ( ) . next ( ) ) . int value ( ) ; assert equals ( __str__ + count @$ count @$ __num__ ) ; tx . commit ( ) ; s . close ( ) ;,references orphan,success,pre
empty selection not allowed @$ <PLACE_HOLDER> old value,if ( ! is null selection allowed ( ) && s . is empty ( ) ) { mark as dirty ( ) ; return ; },selection keep,success,pre
failed emit will <PLACE_HOLDER> buffer 's elements,actual . on complete ( ) ;,emit discard,success,pre
some carriers will <PLACE_HOLDER> duplicate mms messages without this ack . when using the system sending method @$ apparently google does not do this for us . not sure why . you might have to have users manually enter their apn settings if you can not get them from the system somehow .,return null ;,carriers send,fail,pre
this thread <PLACE_HOLDER> the lock in server 1 first @$ then server 2 .,lock the locks ( server1 @$ server2 @$ count down latch ) ;,thread acquires,fail,pre
no need for a state <PLACE_HOLDER> array for the child since it only has two states,if ( indicator != null && indicator . is stateful ( ) ) { final int state set [ ] = pos . position . flat list pos == pos . group metadata . last child fl pos ? child_last_state_set : empty_state_set ; indicator . set state ( state set ) ; },need set,fail,pre
similar to the test above @$ except the threads will <PLACE_HOLDER> multiple page cursors opened at a time .,assert timeout preemptively ( of millis ( semi_long_timeout_millis ) @$ ( ) -> { final atomic boolean should stop = new atomic boolean ( ) ; final int cache pages = __num__ ; final int file pages = cache pages * __num__ ; final int thread count = __num__ ; final int page size = thread count * __num__ ; final int max cursors per thread = cache pages / ( __num__ + thread count ) ; assert that ( max cursors per thread * thread count @$ less than ( cache pages ) ) ; get page cache ( fs @$ cache pages @$ page cache tracer . null @$ page cursor tracer supplier . null ) ; try ( paged file paged file = page cache,threads have,success,pre
check stop requested inside the sync to prevent a race in which the wait <PLACE_HOLDER> the stopper 's notify .,if ( stop requested ( ) ) { return ; },wait invokes,fail,pre
make sure the home button <PLACE_HOLDER> an accurate content description for accessibility .,update home accessibility ( enable ) ;,button has,success,pre
if deletion failed then the directory scanner will <PLACE_HOLDER> the blocks eventually .,cleanup replica ( bpid @$ replica info ) ;,scanner clean,fail,pre
classify 1 <PLACE_HOLDER> entity a as an allowed type,assert true ( classify type1 . can apply to entity type ( entity typea ) ) ;,1 consumes,fail,pre
the edge that leaves the <PLACE_HOLDER> loop if the condition fails .,if ( ! cond . is true ( ) ) { create edge ( node @$ branch . on_false @$ compute follow node ( node @$ this ) ) ; },edge follow,fail,pre
shuffle the list so all clients do n't <PLACE_HOLDER> the same sentinel,if ( iter == null ) { list < redis client > clients = new array list < > ( sentinels . values ( ) ) ; collections . shuffle ( clients ) ; iter = clients . iterator ( ) ; },clients have,fail,pre
exopackage <PLACE_HOLDER> default to jar @$ otherwise @$ default to raw .,dex store default dex store = exopackage mode . enabled for secondary dexes ( exopackage modes ) ? dex store . jar : dex store . raw ; dex split strategy dex split strategy = args . get minimize primary dex size ( ) ? dex split strategy . minimize_primary_dex_size : dex split strategy . maximize_primary_dex_size ; return new dex split mode ( args . get use split dex ( ) @$ dex split strategy @$ args . get dex compression ( ) . or else ( default dex store ) @$ args . get linear alloc hard limit ( ) @$ args . get dex group lib limit ( ) @$ args . get primary dex patterns ( ) @$ args . get primary dex classes file,exopackage enabled,fail,pre
test key store only <PLACE_HOLDER> key pair entries .,if ( is cert entry == true ) { throw new runtime exception ( __str__ + __str__ + __str__ + alias ) ; } boolean is key entry = input key store . is key entry ( alias ) ; key key = null ; if ( is key entry ) { key = input key store . get key ( alias @$ in key pass . to char array ( ) ) ; } else { throw new runtime exception ( __str__ + alias ) ; } output key store . set key entry ( alias @$ key @$ out key pass . to char array ( ) @$ certs ) ;,store contain,success,pre
both threads have terminated and cancel should have <PLACE_HOLDER> the insert statement .,result set res = st . execute query ( __str__ ) ; assert false ( res . next ( ) ) ; try { st . close ( ) ; st . cancel ( ) ; fail ( __str__ ) ; } catch ( sql exception e ) { },cancel failed,fail,pre
subsequent same key presses <PLACE_HOLDER> the keyboard focus to the next object that starts with the same letter .,if ( ( prefix . length ( ) == __num__ ) && ( c == prefix . char at ( __num__ ) ) ) { starting row ++ ; } else { prefix = typed string ; },presses move,success,pre
check if application <PLACE_HOLDER> more resource @$ skip if it does n't need more .,if ( reserved container == null ) { if ( ! application . has pending resource request ( candidates . get partition ( ) @$ scheduling mode ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + application . get application attempt id ( ) + __str__ + scheduling mode . name ( ) + __str__ + candidates . get partition ( ) ) ; } activities logger . app . record skipped app activity without allocation ( activities manager @$ node @$ application @$ null @$ activity diagnostic constant . application_do_not_need_resource @$ activity level . app ) ; return cs assignment . skip_assignment ; } for ( scheduler request key scheduler key : application . get scheduler,application need,fail,pre
iterate over all macros <PLACE_HOLDER> in the string @$ expanding each <PLACE_HOLDER> macro .,int last end = __num__ ; macro finder automaton matcher = new macro finder automaton ( blob ) ; while ( matcher . has next ( ) ) { macro match result match result = matcher . next ( ) ; combiner . add string ( blob . substring ( last end @$ match result . get start index ( ) ) ) ; if ( match result . is escaped ( ) ) { combiner . add string ( blob . substring ( match result . get start index ( ) + ( resolve escaping ? __num__ : __num__ ) @$ match result . get end index ( ) ) ) ; } else { macro replacer < t > replacer = replacers . get ( match,iterate found,success,pre
record should <PLACE_HOLDER> a schema that indicates that the 'child ' is a choice of 2 different record types,assert true ( first outer schema . get data type ( __str__ ) . get ( ) . get field type ( ) == record field type . choice ) ; final list < data type > first sub types = ( ( choice data type ) first outer schema . get data type ( __str__ ) . get ( ) ) . get possible sub types ( ) ; assert equals ( __num__ @$ first sub types . size ( ) ) ; assert equals ( __num__ @$ first sub types . stream ( ) . filter ( type -> type . get field type ( ) == record field type . record ) . count ( ) ) ;,record have,success,pre
the partial solution was on the path here . check whether the channel requires certain properties that are met @$ or whether the channel <PLACE_HOLDER> new properties,found = true ;,channel requires,fail,pre
ordinary bmp code point @$ excluding leading surrogates . bmp <PLACE_HOLDER> a single level lookup . bmp index starts at offset 0 in the trie 2 index . 32 bit data is stored in the index array itself .,if ( code point < __num__ || ( code point > __num__ && code point <= __num__ ) ) { ix = index [ code point > > utrie2_shift_2 ] ; ix = ( ix << utrie2_index_shift ) + ( code point & utrie2_data_mask ) ; value = data32 [ ix ] ; return value ; },bmp uses,success,pre
simulate node status updater impl sending c <PLACE_HOLDER> signal containers event,signal container request signal req = signal container request . new instance ( c id @$ command ) ; list < signal container request > reqs = new array list < > ( ) ; reqs . add ( signal req ) ; container manager . handle ( new c mgr signal containers event ( reqs ) ) ; final argument captor < container signal context > signal context captor = argument captor . for class ( container signal context . class ) ; if ( signal . equals ( signal . null ) ) { verify ( exec @$ never ( ) ) . signal container ( signal context captor . capture ( ) ) ; } else { verify ( exec @$ timeout ( __num__ ),c maintain,fail,pre
handler which do n't <PLACE_HOLDER> an ancestor,sys prop configa . eth62 = new eth62 ( ) { @ override protected void process get block headers ( get block headers message msg ) { if ( msg . get max headers ( ) == __num__ ) { super . process get block headers ( msg ) ; return ; } list < block header > headers = new array list < > ( ) ; for ( int i = __num__ ; i < mainb1b10 . size ( ) ; i ++ ) { headers . add ( mainb1b10 . get ( i ) . get header ( ) ) ; } block headers message response = new block headers message ( headers ) ; send message ( response ) ; } } ;,which have,fail,pre
permission bits from server response will <PLACE_HOLDER> the priority for accuracy .,if ( this . permission != null ) { perm arg = this . permission ; },bits take,fail,pre
memory merge may have <PLACE_HOLDER> the changed code units we are working with .,address set view result set = result pgm . get memory ( ) ; this . latest set = latest changes . get register address set ( ) . intersect ( result set ) ; this . my set = my changes . get register address set ( ) . intersect ( result set ) ; original context = original pgm . get program context ( ) ; latest context = latest pgm . get program context ( ) ; my context = my pgm . get program context ( ) ; result context = result pgm . get program context ( ) ; registers = my context . get registers ( ) ; try { diff original latest = new program diff ( original pgm @$ latest,merge removed,fail,pre
client configuration setting <PLACE_HOLDER> a precedence over system property,system . set property ( __str__ @$ __str__ ) ; config = new client configuration ( ) . with non proxy hosts ( __str__ ) ; assert equals ( __str__ @$ config . get non proxy hosts ( ) ) ; config . set protocol ( protocol . http ) ; assert equals ( __str__ @$ config . get non proxy hosts ( ) ) ; system . clear property ( __str__ ) ;,setting has,success,pre
event bus goes here . this method is only called if one of the binding <PLACE_HOLDER>rs <PLACE_HOLDER> a binding for the given 'item name ' . see the readme.md for the item formatting for the isy link .,this . logger . debug ( __str__ @$ item name @$ command ) ; isy binding config config = get binding config by name ( item name ) ; if ( config != null ) { process command ( config @$ command ) ; } else { this . logger . warn ( __str__ @$ item name ) ; },one provide,success,pre
secs since midnight now <PLACE_HOLDER> hours,secs since midnight -= minutes ;,secs has,fail,pre
color filters always <PLACE_HOLDER> tint filters .,final color filter color filter = ( m color filter == null ? m tint filter : m color filter ) ;,filters override,success,pre
wait until vm 1 has <PLACE_HOLDER> the tx,try { pausibletx . class . wait ( ) ; } catch ( interrupted exception ie ) { fail ( __str__ + ie ) ; },vm commit,fail,pre
analyzer has already <PLACE_HOLDER> this instructions,continue ;,analyzer seen,fail,pre
noinspection <PLACE_HOLDER> allocation in loop,if ( rid . equals ( new o record id ( configuration1 . get schema record id ( ) ) ) && rid . equals ( new o record id ( configuration2 . get schema record id ( ) ) ) ) continue ; if ( rid . get cluster id ( ) == __num__ && rid . get cluster position ( ) == __num__ ) { if ( ! storage type1 . equals ( storage type2 ) ) continue ; },noinspection object,success,pre
return true if child views should not <PLACE_HOLDER> this event .,if ( should consume event ) { return true ; },views consume,fail,pre
output <PLACE_HOLDER> local properties,local properties lp = p . get local properties ( ) ; writer . print ( __str__ ) ; if ( lp . get ordering ( ) != null ) { add property ( writer @$ __str__ @$ lp . get ordering ( ) . to string ( ) @$ true ) ; } else { add property ( writer @$ __str__ @$ __str__ @$ true ) ; } if ( lp . get grouped fields ( ) != null && lp . get grouped fields ( ) . size ( ) > __num__ ) { add property ( writer @$ __str__ @$ lp . get grouped fields ( ) . to string ( ) @$ false ) ; } else { add property ( writer @$ __str__,output node,success,pre
for enumerations <PLACE_HOLDER> the create,if ( collection utils . is not empty ( types def . get enum defs ( ) ) ) { for ( atlas enum def enum def : types def . get enum defs ( ) ) { atlas enum def created def = enum def store . create ( enum def @$ null ) ; ttr . update guid ( created def . get name ( ) @$ created def . get guid ( ) ) ; ret . get enum defs ( ) . add ( created def ) ; } },enumerations do,fail,pre
use a local copy in case another thread <PLACE_HOLDER> things,last mapping < configuration property name > last = this . last mapped configuration property name ; if ( last != null && last . is from ( configuration property name ) ) { return last . get mapping ( ) ; } string converted name = configuration property name . to string ( ) ; property mapping [ ] mapping = { new property mapping ( converted name @$ configuration property name ) } ; this . last mapped configuration property name = new last mapping < > ( configuration property name @$ mapping ) ; return mapping ;,thread changes,success,pre
get the width of a task view so that we know how wide to <PLACE_HOLDER> the header bar .,if ( use grid layout ) { task grid layout algorithm grid layout = m dummy stack view . get grid algorithm ( ) ; grid layout . initialize ( window rect ) ; task view width = ( int ) grid layout . get transform ( __num__ @$ stack . get task count ( ) @$ new task view transform ( ) @$ stack layout ) . rect . width ( ) ; } else { rect task view bounds = stack layout . get untransformed task view bounds ( ) ; if ( ! task view bounds . is empty ( ) ) { task view width = task view bounds . width ( ) ; } },wide put,fail,pre
set domain file for newly opened domain object note : some domain object implementations may <PLACE_HOLDER> runtime exceptions so cleanup is required in those cases,try { domain obj . set domain file ( get domain file ( ) ) ; } catch ( exception e ) { domain obj . release ( consumer ) ; file manager . clear domain object ( get pathname ( ) ) ; throwable cause = e . get cause ( ) ; if ( cause instanceof io exception ) { throw ( io exception ) cause ; } else if ( cause instanceof version exception ) { throw ( version exception ) cause ; } throw new io exception ( e . get message ( ) @$ e ) ; },implementations throw,success,pre
screenshot does not <PLACE_HOLDER> rotation !,final display display = display content . get display ( ) ; int original rotation = display . get rotation ( ) ; final int original width ; final int original height ; display info display info = display content . get display info ( ) ; if ( fixed to user rotation ) { m force default orientation = true ; original width = display content . m base display width ; original height = display content . m base display height ; } else { original width = display info . logical width ; original height = display info . logical height ; },screenshot support,fail,pre
without any read permission the anon <PLACE_HOLDER> access to the user list,jenkins rule . web client wc = j . create web client ( ) ; wc . get options ( ) . set throw exception on failing status code ( false ) ; wc . get options ( ) . set redirect enabled ( false ) ; page page = wc . go to ( __str__ @$ null ) ; check page is redirected to login ( page ) ; assert that ( page . get web response ( ) . get content as string ( ) @$ not ( contains string ( __str__ ) ) ) ; assert request was not blocked ( ) ; page = wc . go to ( __str__ @$ null ) ; assert equals ( __num__ @$ page . get web response,anon has,fail,pre
because this function <PLACE_HOLDER> a totally random spi @$ it really should n't ever fail to allocate an spi ; we simply need this because the exception is checked .,throw new resource unavailable exception ( __str__ ) ;,function allocates,success,pre
no good way to verify that we have an array ... although could i guess check via json parser . so let 's <PLACE_HOLDER> everything is working fine @$ for now .,if ( _injectables != null ) { inject values ( ctxt @$ bean ) ; } final settable bean property [ ] props = _ordered properties ; int i = __num__ ; final int prop count = props . length ; while ( true ) { if ( p . next token ( ) == json token . end_array ) { return bean ; } if ( i == prop count ) { break ; } settable bean property prop = props [ i ] ; if ( prop != null ) { try { prop . deserialize and set ( p @$ ctxt @$ bean ) ; } catch ( exception e ) { throw wrap and throw ( e @$ bean @$ prop . get name,way assume,success,pre
the internal ci adapters <PLACE_HOLDER> negative connection ids and are n't included in public stats .,for ( map . entry < long @$ client interface handle manager > e : m_cihm . entry set ( ) ) { if ( e . get key ( ) > __num__ ) { long admin mode = e . get value ( ) . is admin ? __num__ : __num__ ; long read wait = e . get value ( ) . connection . read stream ( ) . data available ( ) ; long write wait = e . get value ( ) . connection . write stream ( ) . get outstanding message count ( ) ; long outstanding txns = e . get value ( ) . get outstanding txns ( ) ; client_stats . put ( e . get key ( ),adapters contain,fail,pre
if we have n't configured the standard dialect @$ preprocessing inlined expressions <PLACE_HOLDER> no sense and in any case we would be in risk of not knowing what we are doing ...,if ( ! this . dialect set configuration . is standard dialect present ( ) ) { return false ; },expressions makes,success,pre
check to make sure that the runtime defined tags <PLACE_HOLDER> all the template tags .,set < string > runtime tag keys = new hash set < > ( tags . key set ( ) ) ; runtime tag keys . add all ( config ( ) . tags ( ) . key set ( ) ) ; set < string > template tag keys = template . tags ( ) ; if ( ! runtime tag keys . equals ( template tag keys ) ) { throw new illegal argument exception ( __str__ + template . name ( ) + __str__ + __str__ + runtime tag keys . to string ( ) + __str__ + template tag keys . to string ( ) ) ; } return this . metric name ( template . name ( ) @$ template . group (,tags contains,fail,pre
need to put this here @$ since vanilla <PLACE_HOLDER> this state after the vignette was rendered .,if ( pre ( vignette ) ) { gl state manager . enable depth test ( ) ; gl state manager . blend func separate ( gl state manager . source factor . src_alpha @$ gl state manager . dest factor . one_minus_src_alpha @$ gl state manager . source factor . one @$ gl state manager . dest factor . zero ) ; return ; },vanilla binds,fail,pre
the response object does n't <PLACE_HOLDER> any relevant info so we have to create a copy of values being sent over the network in case m jp settings is modified while awaiting response,final jetpack settings model sent jp data = new jetpack settings model ( m jp settings ) ; ++ m save request count ; word press . get rest client utilsv1_1 ( ) . set jetpack settings ( m site . get site id ( ) @$ params @$ new rest request . listener ( ) { @ override public void on response ( json object response ) { app log . d ( app log . t . api @$ __str__ ) ; m remote jp settings . monitor active = sent jp data . monitor active ; m remote jp settings . jetpack protect enabled = sent jp data . jetpack protect enabled ; m remote jp settings . jetpack protect whitelist . clear ( ),object contain,success,pre
this input reference is part of a join expression that refers an expression that comes from either node . to resolve it we need to find its index in the inner node 's expression list <PLACE_HOLDER> the inner node projection,if ( resolving outer || resolving inner ) { preconditions . check state ( expr input indx < m_program . get project list ( ) . size ( ) ) ; final rex local ref input local ref = m_program . get project list ( ) . get ( expr input indx ) ; input idx = input local ref . get index ( ) ; rex node expr = m_program . get expr list ( ) . get ( input idx ) ; if ( expr . isa ( sql kind . cast ) ) { expr = ( ( rex call ) expr ) . get operands ( ) . get ( __num__ ) ; } final abstract expression volt expr = expr . accept (,node perform,fail,pre
if we 're restarting this transaction @$ and we only have local work @$ add some dummy remote work so that we can avoid injecting a borrow task into the local buddy site before the complete transaction message with the restart flag reaches it . right now @$ any read on a replicated table which <PLACE_HOLDER> no distributed work will generate these null fragments,boolean used null fragment = false ; if ( m_is restart && m_remote work == null ) { used null fragment = true ; m_remote work = new fragment task message ( m_local work . get initiatorhs id ( ) @$ m_local work . get coordinatorhs id ( ) @$ m_local work . get txn id ( ) @$ m_local work . get unique id ( ) @$ m_local work . is read only ( ) @$ false @$ false @$ m_n part txn @$ m_restart timestamp ) ; m_remote work . set empty for restart ( get next dependency id ( ) ) ; if ( ! m_have distributed init task && ! is for replay ( ) && ! is read only ( ) ) {,which has,success,pre
if user has <PLACE_HOLDER> a stopword file other than the template,try { if ( ! conf . get ( __str__ ) . equals ( __str__ ) ) { stop words = new array list < string > ( ) ; string stop word ; buffered reader br = new buffered reader ( conf . get conf resource as reader ( ( conf . get ( __str__ ) ) ) ) ; while ( ( stop word = br . read line ( ) ) != null ) { stop words . add ( stop word ) ; } log . info ( __str__ @$ conf . get ( __str__ ) ) ; } int [ ] ngram arr = retrieve ngrams ( conf ) ; int mingram = ngram arr [ __num__ ] ; int maxgram = ngram,user specified,success,pre
we need to return the connection back to the jdbc pool . in order to do that we need to close it @$ to keep the remaining code readable one can <PLACE_HOLDER> a headers end handler to close the connection .,routing context . add headers end handler ( done -> conn . close ( v -> { } ) ) ; routing context . next ( ) ;,one add,success,pre
in case when remote tx has <PLACE_HOLDER> the current state before .,ignore ( ) ;,tx rejected,fail,pre
create 100 threads @$ each will <PLACE_HOLDER> its own puts,putter [ ] all = new putter [ threads100 ] ;,each get,fail,pre
peek does n't <PLACE_HOLDER> buffer,assert equals ( __num__ @$ buffer . position ( ) ) ;,peek touch,fail,pre
given that a second user has <PLACE_HOLDER> the bubble clock face,when ( m mock settings wrapper . get lock screen custom clock face ( secondary_user_id ) ) . then return ( bubble_clock ) ;,user set,fail,pre
fall through to skip since comment <PLACE_HOLDER> no children .,case ls parser filter . filter_skip : default :,comment has,success,pre
first @$ reduce to account asset balance . else ca n't <PLACE_HOLDER> this test case .,account capsule to account = db manager . get account store ( ) . get ( byte array . from hex string ( to_address ) ) ; to account . reduce asset amount ( byte string . copy from utf8 ( asset_name ) . to byte array ( ) @$ total_supply - __num__ ) ; db manager . get account store ( ) . put ( to account . get address ( ) . to byte array ( ) @$ to account ) ; participate asset issue actuator actuator = new participate asset issue actuator ( ) ; actuator . set chain base manager ( db manager . get chain base manager ( ) ) . set any ( get contract ( __num__ ) ) ; transaction result,first complete,success,pre
do n't remove if user <PLACE_HOLDER> one or is recorded in previous run,if ( partition . get has user specified high watermark ( ) || this . work unit state . get prop ( configuration keys . work_unit_state_actual_high_water_mark_key ) != null ) { return false ; } return true ;,user specified,fail,pre
visual index is now the visual length minus the bi di controls @$ which should <PLACE_HOLDER> the length of the bidi test.txt ordering .,if ( is ok && ordering count != visual index ) { errln ( __str__ + ordering count + __str__ + visual index ) ; is ok = false ; } if ( ! is ok ) { print error line ( ) ; string builder eord = new string builder ( __str__ ) ; for ( i = __num__ ; i < ordering count ; ++ i ) { eord . append ( __str__ ) . append ( ( char ) ( __str__ + ordering [ i ] ) ) ; } string builder aord = new string builder ( __str__ ) ; for ( i = __num__ ; i < result length ; ++ i ) { int logical index = ubidi . get logical index,which represent,fail,pre
for batch mode @$ the max watermark should <PLACE_HOLDER> the bundle to close,if ( watermark . is equal ( bounded window . timestamp_max_value ) ) { finish bundle ( emitter ) ; },watermark cause,fail,pre
the block would have been decremented for the scan case as it was wrapped before even the post next hook <PLACE_HOLDER> executed . giving some time for the block to be decremented,thread . sleep ( __num__ ) ; iterator < cached block > iterator = cache . iterator ( ) ; boolean used blocks found = false ; int ref count = __num__ ; while ( iterator . has next ( ) ) { cached block next = iterator . next ( ) ; block cache key cache key = new block cache key ( next . get filename ( ) @$ next . get offset ( ) ) ; if ( cache instanceof bucket cache ) { ref count = ( ( bucket cache ) cache ) . get rpc ref count ( cache key ) ; } else if ( cache instanceof combined block cache ) { ref count = ( ( combined block cache ) cache,hook gets,success,pre
user 2 <PLACE_HOLDER> his availability to away,muc2 . change availability status ( __str__ @$ presence . mode . away ) ; thread . sleep ( __num__ ) ;,user shuts,fail,pre
if this does n't throw then the clues map does n't <PLACE_HOLDER> duplicate keys,new coordinate clue ( __str__ @$ new world point ( __num__ @$ __num__ @$ __num__ ) @$ null ) ;,map contain,fail,pre
if no child <PLACE_HOLDER> a more specific decision @$ see if we have a value for the property,if ( ! decision found && property map . contains key ( property name ) ) { property value value = property map . get ( property name ) ; list < string > decision path = get decision path ( ) ; decision set . add decision ( new decision ( value . value @$ decision path @$ value . source ) ) ; decision found = true ; } return decision found ;,child fulfills,fail,pre
if an instance of media tray @$ fall thru returning all media <PLACE_HOLDER> areas,if ( ! ( media name instanceof media size name ) ) { media name = null ; },instance showing,fail,pre
should not <PLACE_HOLDER> anything when cursor is null,matcher . cursor = null ; matcher . next is ( opcodes . nop ) ;,not do,success,pre
flushing and minor compaction <PLACE_HOLDER> delete markers,region . flush ( true ) ; region . compact ( false ) ; assert equals ( __num__ @$ count delete markers ( region ) ) ; region . compact ( true ) ;,compaction adds,fail,pre
all partitioned tables get <PLACE_HOLDER> crud procs,add shim procedure ( prefix + __str__ @$ table @$ null @$ true @$ partition index @$ partitioncolumn @$ false ) ;,tables generated,fail,pre
let client <PLACE_HOLDER> the same request @$ add pending commit to sync later,if ( ! from read ) { commit ctx commit ctx = new commit ctx ( commit offset @$ channel @$ xid @$ pre op attr ) ; pending commits . put ( commit offset @$ commit ctx ) ; },client send,fail,pre
method <PLACE_HOLDER> time in nanoseconds across all processors .,cpu time /= __num__ * os . get available processors ( ) ; double cpu = __num__ ; if ( prev cpu time > __num__ ) { long cpu time diff = cpu time - prev cpu time ; cpu = math . min ( __num__ @$ ( double ) cpu time diff / metrics_update_freq ) ; },method reports,success,pre
here we restore our wallet from a seed with no passphrase . also <PLACE_HOLDER> a look at the backup to mnemonic seed.java example that shows how to backup a wallet by creating a mnemonic sentence .,string seed code = __str__ ; string passphrase = __str__ ; long creationtime = __num__ ; deterministic seed seed = new deterministic seed ( seed code @$ null @$ passphrase @$ creationtime ) ;,here have,success,pre
robolectric does n't <PLACE_HOLDER> this library,if ( ! test util . are robolectric tests running ( ) ) { ok http builder . add interceptor ( new chuck interceptor ( application . get application context ( ) ) ) ; },robolectric have,fail,pre
client <PLACE_HOLDER> the connection,socket s1 = new socket ( ) ; s1 . connect ( isa ) ;,client opens,fail,pre
check if the caller <PLACE_HOLDER> the right to invoke 'query m beans ',if ( sm != null ) { checkm bean permission ( ( string ) null @$ null @$ null @$ __str__ ) ; set < object instance > list = querym beans impl ( name @$ null ) ; set < object instance > allowed list = new hash set < object instance > ( list . size ( ) ) ; for ( object instance oi : list ) { try { checkm bean permission ( oi . get class name ( ) @$ null @$ oi . get object name ( ) @$ __str__ ) ; allowed list . add ( oi ) ; } catch ( security exception e ) { } } return filter list of object instances ( allowed list @$ query ),caller has,success,pre
x sends x crossing to all hierarchy so if the edge of child equals to ancestor and mouse enters child @$ the ancestor will <PLACE_HOLDER> an event too . from java point the event is bogus as ancestor is obscured @$ so if the child can <PLACE_HOLDER> java event itself @$ we skip it on ancestor .,long child wnd = xce . get_subwindow ( ) ; if ( child wnd != x constants . none ) { x base window child = x toolkit . window tox window ( child wnd ) ; if ( child != null && child instanceof x window && ! child . is event disabled ( xev ) ) { return ; } },child send,fail,pre
if queue <PLACE_HOLDER> default label expression @$ and rr does n't have @$ use the default label expression of queue,if ( label exp == null && queue info != null && resource request . any . equals ( res req . get resource name ( ) ) ) { log . debug ( __str__ @$ queue info . get default node label expression ( ) ) ; label exp = queue info . get default node label expression ( ) ; },queue has,success,pre
has to be tied for best break if we 've found one in ` best arborescence ` at this point all edges in ` candidates ` <PLACE_HOLDER> equal weight @$ and if one of them is in ` best arborescence ` it will be first,final exclusive edge best edge = candidates . remove first ( ) ;,edges have,success,pre
first renderer <PLACE_HOLDER> english .,map < string @$ integer > first renderer mapped capabilities = new hash map < > ( ) ; first renderer mapped capabilities . put ( english . id @$ format_handled ) ; first renderer mapped capabilities . put ( german . id @$ format_unsupported_subtype ) ; renderer capabilities first renderer capabilities = new fake mapped renderer capabilities ( c . track_type_text @$ first renderer mapped capabilities ) ;,renderer handles,success,pre
split 0 times should <PLACE_HOLDER> iae,try { parts = bytes . split ( low @$ high @$ __num__ ) ; assert true ( __str__ @$ false ) ; } catch ( illegal argument exception iae ) { },split throw,success,pre
higher order functions <PLACE_HOLDER> functions .,return true ; case switch : case case :,functions override,fail,pre
share ur is <PLACE_HOLDER> the content : scheme when able @$ which looks bad when displayed in the url bar .,if ( service . is download openable in browser ( is off the record @$ mime type ) ) { uri file uri = uri . from file ( file ) ; uri share uri = get uri for item ( file ) ; string normalized mime type = intent . normalize mime type ( mime type ) ; intent intent = get media viewer intent for download item ( file uri @$ share uri @$ normalized mime type ) ; intent handler . start activity for trusted intent ( intent @$ context ) ; return true ; },ur using,fail,pre
each replica could <PLACE_HOLDER> own end point factory resolver,string end point factory = sub config . has path ( end_point_factory_class ) ? sub config . get string ( end_point_factory_class ) : default_end_point_factory_class ; end point factory factory = end point factory resolver . resolve class ( end point factory ) . new instance ( ) ; this . replicas . add ( factory . build replica ( sub config @$ replica name @$ this . selection config ) ) ;,replica have,success,pre
moved region <PLACE_HOLDER> operation to a guarded block . if a region is getting created it wo n't allow it to destroy any region .,synchronized ( region op lock ) { object name object name = m beanjmx adapter . get regionm bean name ( internal cache . get distributed system ( ) . get distributed member ( ) @$ region . get full path ( ) ) ; try { regionm bean regionm bean = ( regionm bean ) service . get local regionm bean ( region . get full path ( ) ) ; if ( regionm bean != null ) { regionm bean . stop monitor ( ) ; } } catch ( management exception e ) { if ( logger . is debug enabled ( ) ) { logger . debug ( e . get message ( ) @$ e ) ; } return ; } service .,region destroy,fail,pre
make all cache elements for this guy <PLACE_HOLDER> stale .,remove stale entries ( class value ) ;,elements name,fail,pre
card array is cloned in deck task @$ which <PLACE_HOLDER> attention to memory pressure,m undo . add ( new object [ ] { type @$ o [ __num__ ] } ) ; break ; default : timber . e ( __str__ @$ type ) ; break ;,which takes,fail,pre
only the main task stack change notification <PLACE_HOLDER> a delay .,m handler . send message delayed ( msg @$ notify_task_stack_change_listeners_delay ) ;,notification has,fail,pre
java <PLACE_HOLDER>es not <PLACE_HOLDER> this right imo . the number of bytes in a file is a long @$ but the length of a string is an int . why ?,line guess = ( int ) ( f . length ( ) / ( long ) __str__ . length ( ) ) ; line guess += ( line guess / __num__ ) ;,java do,success,pre
use a flag bc we need to handle is encap nonce <PLACE_HOLDER> either way,boolean failed = false ;,nonce failed,fail,pre
no content length check is performed when the md 5 check is enabled @$ since a correct md 5 check would <PLACE_HOLDER> a correct content length .,try { message digest digest = message digest . get instance ( __str__ ) ; is = new digest validation input stream ( is @$ digest @$ server side hash ) ; } catch ( no such algorithm exception e ) { log . warn ( __str__ + __str__ @$ e ) ; },check generate,fail,pre
create a view <PLACE_HOLDER> from the locator that conflicts with the installed view,member identifier locator member id = new internal distributed member ( __str__ @$ mock members [ mock members . length - __num__ ] . get membership port ( ) + __num__ ) ; locator member id . set vm kind ( cluster distribution manager . locator_dm_type ) ; list < member identifier > new member list = new array list < > ( members ) ; new member list . add ( locator member id ) ; gms membership view locator view = new gms membership view ( locator member id @$ installed view . get view id ( ) + __num__ @$ new member list ) ;,view object,fail,pre
client 1 <PLACE_HOLDER> all,client1 . invoke ( ( ) -> { region < string @$ ticker data > region = get cache ( ) . get region ( region name ) ; do put all ( region @$ __str__ @$ one_hundred * __num__ ) ; assert that ( region . size ( ) ) . is equal to ( one_hundred * __num__ ) ; } ) ;,client put,success,pre
third volume @$ again with 3 <PLACE_HOLDER> free space .,volumes . add ( mockito . mock ( fs volume spi . class ) ) ; mockito . when ( volumes . get ( __num__ ) . get available ( ) ) . then return ( __num__ * __num__ * __num__ ) ;,volume mb,success,pre
msc.sync all will <PLACE_HOLDER> a npe,if ( spt == null ) break ;,all throw,fail,pre
force eof if a read <PLACE_HOLDER> place at this position,curr buf idx -- ; buf position = buffer_size ; curr buf = file . get buffer ( curr buf idx ) ; buf position = __num__ ; long buflen = length - buf start ; buf length = buflen > buffer_size ? buffer_size : ( int ) buflen ;,read took,fail,pre
note : number format used by date format only uses int numbers . remainder operation on 32 bit platform using long is significantly slower than int . so @$ this method <PLACE_HOLDER> long number into int .,int number = ( int ) numberl ; int limit = decimal buf . length < max int digits ? decimal buf . length : max int digits ; int index = limit - __num__ ; while ( true ) { decimal buf [ index ] = digits [ ( number % __num__ ) ] ; number /= __num__ ; if ( index == __num__ || number == __num__ ) { break ; } index -- ; } int padding = min int digits - ( limit - index ) ; for ( ; padding > __num__ ; padding -- ) { decimal buf [ -- index ] = digits [ __num__ ] ; } int length = limit - index ; to append to . append (,method converts,fail,pre
insert the video . the command <PLACE_HOLDER> three arguments . the first specifies which information the api request is setting and which information the api response should return . the second argument is the video resource that contains metadata about the new video . the third argument is the actual video content .,you tube . videos . insert video insert = youtube . videos ( ) . insert ( __str__ @$ video object defining metadata @$ media content ) ;,command takes,fail,pre
\u 00 a 5 and \uffe 5 are actually the same symbol @$ just different code points . but the ri <PLACE_HOLDER> the \uffe 5 and android <PLACE_HOLDER> those with \u 00 a 5,string [ ] yen = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; string [ ] dollar = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } ;,ri breaks,fail,pre
if error while generating pojo do not <PLACE_HOLDER> compile rule as they very likely depends hence fail too .,if ( has errors ( ) ) { return ; },pojo apply,fail,pre
! drained if dispatch <PLACE_HOLDER> new events on this dispatcher,synchronized ( mutex ) { drained = queue . is empty ( ) ; },dispatch produced,fail,pre
we only care about it if the image name <PLACE_HOLDER> the current base name,if ( variable declarator id . has image equal to ( base name ) ) { boolean allocation found = declarator . get first descendant of type ( ast allocation expression . class ) != null ; boolean iterator = is iterator ( ) || is factory ( declarator ) ; boolean for loop = is for loop ( declarator ) ; assignments . add ( new assignment ( declarator . get begin line ( ) @$ allocation found @$ iterator @$ for loop ) ) ; },name matches,success,pre
check notification <PLACE_HOLDER> css style which describes notification type,if ( get attribute ( __str__ ) . contains ( notif type ) ) { return entry . get value ( ) ; },notification has,success,pre
the old request should <PLACE_HOLDER> no effect on new tracker,request . record miss ( ) ; assert equals ( __num__ @$ tracker . get total hit count ( ) ) ; assert equals ( __num__ @$ tracker . get total miss count ( ) ) ; assert equals ( __num__ @$ tracker . get total miss match count ( ) ) ; assert equals ( __num__ @$ tracker . get total eviction count ( ) ) ; assert equals ( __num__ @$ tracker . get total invalidation count ( ) ) ; assert equals ( __num__ @$ tracker . get total load success count ( ) ) ; assert equals ( __num__ @$ tracker . get total load exception count ( ) ) ; assert equals ( __num__ @$ tracker . get total retrieval time ( ),request have,success,pre
test the same data and model with prior @$ should <PLACE_HOLDER> the same model except for the intercept,glm = new glm ( params ) ; model4 = glm . train model ( ) . get ( ) ; assert equals ( __str__ + model3 . _output . _training_metrics . _mse + __str__ + model4 . _output . _training_metrics . _mse @$ model3 . _output . _training_metrics . _mse @$ model4 . _output . _training_metrics . _mse @$ __num__ ) ; assert equals ( __str__ + ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev + __str__ + ( ( model metrics binomialglm ) model4 . _output . _training_metrics ) . _res dev @$ ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev @$ ( ( model metrics binomialglm ) model4 . _output .,data get,success,pre
cancel currently executing tasks <PLACE_HOLDER> a while for tasks to respond to being cancelled,if ( ! executor . await termination ( __num__ @$ time unit . seconds ) ) system . out . println ( __str__ ) ;,tasks wait,success,pre
more complicated race ! ! some client managed to acquire the provider and release it before the removal was completed . <PLACE_HOLDER> the removal @$ and abort the next remove message .,prc . remove pending = false ; final i binder j binder = prc . holder . provider . as binder ( ) ; provider ref count existing prc = m provider ref count map . get ( j binder ) ; if ( existing prc == prc ) { m provider ref count map . remove ( j binder ) ; } for ( int i = m provider map . size ( ) - __num__ ; i >= __num__ ; i -- ) { provider client record pr = m provider map . value at ( i ) ; i binder my binder = pr . m provider . as binder ( ) ; if ( my binder == j binder ) { m provider map,race defer,fail,pre
verify resource request sent for map <PLACE_HOLDER> appropriate node label expression as per the configuration,validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ; validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ; validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ;,request has,fail,pre
british pronounce h in this word americans <PLACE_HOLDER> it ' h ' for the name @$ no ' h ' for the plant,if ( string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) || string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ ) || string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) { if ( ( m_current == __num__ ) && string at ( m_current @$ __num__ @$ __str__ @$ __str__ ) ) { if ( m_encode vowels ) { metaph add ( __str__ @$ __str__ ) ; } else { metaph add ( __str__ @$ __str__ ) ; } } else if ( ( m_current == __num__ ) || m_encode vowels ) { metaph add ( __str__ ) ; } m_current ++ ;,h give,success,pre
all project related messages which <PLACE_HOLDER> the view,string current etag = string . format ( __str__ @$ objects . hash ( get last modified ( ) @$ get project messages ( ) @$ get env ( ) . get date for last index run ( ) != null ? get env ( ) . get date for last index run ( ) . get time ( ) : __num__ @$ info . get version ( ) ) ) ;,which change,fail,pre
mix in various ways to specify no catalog . <PLACE_HOLDER> empty string here .,dep bytes = new string ( client utils . file to bytes ( new file ( deploymenturl ) ) @$ constants . utf8encoding ) ; results = client . call procedure ( __str__ @$ __str__ @$ dep bytes ) . get results ( ) ; assert true ( results . length == __num__ ) ; thread . sleep ( __num__ ) ;,mix use,success,pre
assert that the disk has <PLACE_HOLDER> all of the updates,region version vector rvv = getrvv ( vm1 ) ; region version vector diskrvv = get diskrvv ( vm1 ) ; assert samervv ( rvv @$ diskrvv ) ;,disk received,fail,pre
the new tree root node id must <PLACE_HOLDER> the original one to be able to reconnect the subtrees,new tree . set id ( sub tree . get id ( ) ) ; tree permutations . add ( new tree ) ;,id match,success,pre
we want to keep track of the last 1000 events in the files so that we can add them to 'ring buffer ' . however @$ we do n't want to add them directly to ring buffer @$ because once they are added to ring buffer @$ they are available in query results . as a result @$ we can have the issue where,final ring buffer < provenance event record > latest records = new ring buffer < > ( __num__ ) ;,records have,fail,pre
if already setup <PLACE_HOLDER> the view,if ( null == this . crouton view ) { initialize crouton view ( ) ; } return crouton view ;,setup create,fail,pre
all <PLACE_HOLDER> @$ no commit or rollback from tm,if ( xa resource . xa_rdonly == response . get result ( ) ) { super . on response ( command ) ; },all fail,fail,pre
conversions on parent would <PLACE_HOLDER> precedence,attribute conversion info conversion = locate attribute conversion info ( property name ) ; if ( conversion != null ) { return conversion ; } return null ;,conversions take,fail,pre
m total length <PLACE_HOLDER> the padding already,child top = m padding top + bottom - top - m total length ; break ;,length contains,success,pre
lie data type <PLACE_HOLDER> data type .,byte i71 [ ] = { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ;,type invaid,success,pre
check if the .job file <PLACE_HOLDER> an underlying job template,if ( job config . has path ( gobblin_job_template_key ) ) { uri job template relative uri = new uri ( job config . get string ( gobblin_job_template_key ) ) ; if ( ! job template relative uri . get scheme ( ) . equals ( fs_scheme ) ) { throw new runtime exception ( __str__ + fs_scheme + __str__ + flow template diruri . get scheme ( ) ) ; } path full job template path = path utils . merge paths ( new path ( template catalog dir ) @$ new path ( job template relative uri ) ) ; job config = job config . with fallback ( load hocon file at path ( full job template path ) ) ; } job templates .,file has,success,pre
we do n't care as long as nobody <PLACE_HOLDER> the method :,compilation helper . set args ( arrays . as list ( __str__ @$ temporary folder . get root ( ) . get absolute path ( ) @$ __str__ ) ) . add source lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . do test ( ) ;,nobody calls,success,pre
join to next if parent of p paragraph <PLACE_HOLDER> another element after p paragraph @$ and it is n't a leaf .,element parent = p paragraph . get parent element ( ) ; int p paragraph index = parent . get element index ( offset ) ; if ( ( p paragraph index + __num__ ) < parent . get element count ( ) && ! parent . get element ( p paragraph index + __num__ ) . is leaf ( ) ) { last start spec . set direction ( element spec . join next direction ) ; },parent has,success,pre
but they are not . however @$ i bet someone will try and treat it like a target @$ so find the owning cell if necessary @$ and then fully <PLACE_HOLDER> the path against the owning cell 's root .,matcher matcher = include_path_pattern . matcher ( include ) ; preconditions . check state ( matcher . matches ( ) ) ; optional < string > cell name = optional . of nullable ( matcher . group ( __num__ ) ) ; string include path = matcher . group ( __num__ ) ; return cell path resolver . get cell path ( cell name ) . map ( cell path -> cell path . resolve ( include path ) ) . or else get ( ( ) -> cell . get filesystem ( ) . resolve ( include path ) ) ;,then include,fail,pre
according to david ackerman @$ the compression type can <PLACE_HOLDER> the endianness of the document .,if ( compression type . equals ( __str__ ) ) { aiff header . set endian ( aiff audio header . endian . little_endian ) ; } bytes left -= __num__ ; compression name = aiff util . read pascal string ( raf ) ; bytes left -= compression name . length ( ) + __num__ ;,type change,success,pre
hope the next local variable would <PLACE_HOLDER> a smaller count .,continue ;,hope give,fail,pre
if the structure size indicates there are more fields @$ we are dealing with a newer version of the structure . each size check <PLACE_HOLDER> a new version of the structure .,if ( reader . get pointer index ( ) - index < size ) { security cookie = read pointer ( reader ) ; se handler table = read pointer ( reader ) ; se handler count = read pointer ( reader ) ; } if ( reader . get pointer index ( ) - index < size ) { guard cfc check function pointer = read pointer ( reader ) ; guard cf dispatch function pointer = read pointer ( reader ) ; guard cf function table = read pointer ( reader ) ; guard cf function count = read pointer ( reader ) ; guard flags = new guard flags ( reader . read next int ( ) ) ; } if ( reader . get,check represents,success,pre
push queries only <PLACE_HOLDER> value columns @$ but query metadata schema includes key and meta :,final logical schema stored schema = query metadata . get logical schema ( ) ; final builder actual schema builder = logical schema . builder ( ) . no implicit columns ( ) ; stored schema . value ( ) . for each ( actual schema builder :: value column ) ; return streamed row . header ( no_query_id @$ actual schema builder . build ( ) ) ;,queries have,fail,pre
block types other than data blocks always <PLACE_HOLDER> data block encoding.none . to avoid false negative cache misses @$ only perform this check if cached block is a data block .,if ( cached block . get block type ( ) . is data ( ) && ! actual data block encoding . equals ( expected data block encoding ) ) { if ( ! expected data block encoding . equals ( data block encoding . none ) && ! actual data block encoding . equals ( data block encoding . none ) ) { log . info ( __str__ + cache key + __str__ + __str__ + expected data block encoding + __str__ + actual data block encoding + __str__ + path ) ; return and evict block ( cache @$ cache key @$ cached block ) ; } return null ; },types return,fail,pre
when amrmp roxy ha is enabled @$ nmss should not <PLACE_HOLDER> the uam token @$ it should be in registry,assert . assert false ( recovered data map . contains key ( sc entry ) ) ;,nmss have,success,pre
wait for auth <PLACE_HOLDER> event from client 's event thread .,try ( zoo keeper ignored = create client ( new my watcher ( ) @$ host port ) ) { auth failed . await ( ) ; },wait failed,success,pre
find all aliases <PLACE_HOLDER> current word,if ( ! common utils . is empty ( request . get active query ( ) . get text ( ) ) && ! common utils . is empty ( word part ) ) { if ( word part . index of ( request . get context ( ) . get syntax manager ( ) . get struct separator ( ) ) != - __num__ || word part . equals ( all_columns_pattern ) ) { return ; } sql dialect sql dialect = sql utils . get dialect from data source ( request . get context ( ) . get data source ( ) ) ; string table name pattern = get table name pattern ( sql dialect ) ; string table alias pattern = get table alias,aliases using,fail,pre
our protocol does not <PLACE_HOLDER> an epilogue or a preamble .,final multi partmime writer . builder attachments builder = new multi partmime writer . builder ( ) ; for ( final object data source : streaming attachments ) { assert ( data source instanceof rest li attachment data source writer || data source instanceof rest li data source iterator ) ; if ( data source instanceof rest li attachment data source writer ) { attachment utils . append single attachment to builder ( attachments builder @$ ( rest li attachment data source writer ) data source ) ; } else { attachment utils . append multiple attachments to builder ( attachments builder @$ ( rest li data source iterator ) data source ) ; } } final multi partmime writer multi partmime writer = attachment utils . create,protocol support,fail,pre
compiler bug : a strictfp class <PLACE_HOLDER> all methods to strictfp,flags &= ~ code constants . acc_strict ;,class contains,fail,pre
we need to log these independently of cards @$ as one side may <PLACE_HOLDER> more card templates,_log rem ( ids @$ consts . rem_note ) ; m db . execute ( __str__ + strids ) ;,side write,fail,pre
only allocate the array if there are enough bytes available . this only works for byte array input stream . the assignment below ensures that buffer <PLACE_HOLDER> the required type .,byte array input stream array input = buffer ; if ( array input . available ( ) < length ) { throw new io exception ( __str__ ) ; } byte [ ] bytes = new byte [ length ] ; array input . read ( bytes ) ; if ( is constructed ( ) ) { der input stream in = new der input stream ( bytes @$ __num__ @$ bytes . length @$ buffer . allowber ) ; bytes = null ; while ( in . available ( ) != __num__ ) { bytes = append ( bytes @$ in . get octet string ( ) ) ; } } return bytes ;,buffer has,success,pre
caller might <PLACE_HOLDER> different units,convert temps ( main @$ units ) ;,caller set,fail,pre
check if user is member of group since api does n't <PLACE_HOLDER> typed exception,if ( identity service . create user query ( ) . member of group ( group . get id ( ) ) . user id ( member ship . get user id ( ) ) . count ( ) > __num__ ) { throw new flowable conflict exception ( __str__ + member ship . get user id ( ) + __str__ + group . get id ( ) + __str__ ) ; } identity service . create membership ( member ship . get user id ( ) @$ group . get id ( ) ) ; response . set status ( http status . created . value ( ) ) ; return rest response factory . create membership response ( member ship . get user id ( ),api return,success,pre
remap <PLACE_HOLDER> tokens to the token id we created for the first instance of any <PLACE_HOLDER> token name .,if ( remapping indexes . not empty ( ) ) { remapping indexes . for each key value ( ( index @$ creating index ) -> ids [ index ] = ids [ creating index ] ) ; } return created tokens ;,remap created,fail,pre
the fractional seconds may <PLACE_HOLDER> 1 through 6 digits but no more and no less .,parse date fails ( __str__ ) ; parse date fails ( __str__ ) ;,seconds have,success,pre
google camera <PLACE_HOLDER> the preview surface as well as capture surface @$ for still capture,still builder . add target ( image reader . get surface ( ) ) ; if ( image reader raw != null ) still builder . add target ( image reader raw . get surface ( ) ) ; capture session . stop repeating ( ) ;,camera adds,success,pre
consider our cached version dirty since app code now <PLACE_HOLDER> a reference to it,if ( m drawable == m recycleable bitmap drawable ) { m recycleable bitmap drawable = null ; },version has,success,pre
verify that mock provider 2 contains a new group <PLACE_HOLDER> the same as the parent meta group of the contact we just moved,contact group new grpp2 = mcl slick fixture . mock pres op setp2 . get server stored contact list root ( ) . get group ( mcl slick fixture . metap1 grp1 . get group name ( ) ) ; assert not null ( __str__ + mcl slick fixture . emilp2 . get display name ( ) + __str__ @$ new grpp2 ) ;,group works,fail,pre
add g 1 and all <PLACE_HOLDER> their projects,auth entity = auth entity factory . apply ( null ) ; auth entity . set for groups ( new tree set < > ( ) ) ; auth entity . set for groups ( __str__ ) ; auth entity . load ( new tree map < > ( ) ) ; assert equals ( new tree set < > ( arrays . as list ( new string [ ] { __str__ @$ __str__ } ) ) @$ auth entity . for groups ( ) ) ; assert equals ( new tree set < > ( arrays . as list ( new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ) ) @$ auth entity . for projects,g managed,fail,pre
minimal <PLACE_HOLDER> buffer size .,final silent server ss = new silent server ( __num__ ) ;,minimal buffered,fail,pre
divide space according to expansion ratios if any span <PLACE_HOLDER> a ratio,int total expansion = __num__ ; for ( int i = __num__ ; i < span size ; i ++ ) { int item index = span start index + i ; total expansion += expansion ratios [ item index ] ; } for ( int i = __num__ ; i < span size ; i ++ ) { int item index = span start index + i ; int expansion ; if ( total expansion == __num__ ) { expansion = needed extra space / span size ; } else { expansion = ( int ) ( needed extra space * expansion ratios [ item index ] / total expansion ) ; } dimensions [ item index ] += expansion ; allocated extra space += expansion ;,span has,success,pre
if state machine instance not <PLACE_HOLDER> stop retry,if ( framework error code . state machine instance not exists . equals ( e . get errcode ( ) ) ) { return branch status . phase two_ committed ; },instance exists,success,pre
create a bitmap of the icon which is what the widget 's remoteview <PLACE_HOLDER> .,icon . set color filter ( m icon utilities . get disabled color filter ( ) ) ; return m icon utilities . create icon bitmap ( icon ) ;,remoteview says,fail,pre
user 10 does n't <PLACE_HOLDER> package 3 @$ so no callback .,verify ( c10 @$ times ( __num__ ) ) . on shortcuts changed ( eq ( calling_package_3 ) @$ any ( list . class ) @$ any ( user handle . class ) ) ; assert shortcut ids ( shortcuts . get value ( ) @$ __str__ ) ; assert equals ( start_time + __num__ @$ find shortcut ( shortcuts . get value ( ) @$ __str__ ) . get last changed timestamp ( ) ) ;,user call,fail,pre
regression test @$ used to throw a class cast exception because the job launcher <PLACE_HOLDER> a query result instead of row count result,long [ ] row counts = execute ( __str__ @$ $$ ( $ ( __str__ ) ) ) ; assert that ( row counts . length @$ is ( __num__ ) ) ;,launcher returns,fail,pre
center inside @$ but bitmap bounds <PLACE_HOLDER> the resize dimensions ... so change it to fit center .,if ( scale mode == scale mode . center inside ) { if ( resize width <= b . get width ( ) || resize height <= b . get height ( ) ) scale mode = scale mode . fit center ; },inside change,fail,pre
when the trailer contains an orca report @$ callbacks for both listeners will be invoked . both listener will <PLACE_HOLDER> the same orca report instance @$ which means deserialization happens only once .,trailer . put ( orca reporting tracer factory . orca_endpoint_load_metrics_key @$ orca load report . get default instance ( ) ) ; child tracer . inbound trailers ( trailer ) ; argument captor < orca load report > parent report cap = argument captor . for class ( null ) ; argument captor < orca load report > child report cap = argument captor . for class ( null ) ; verify ( orca listener1 ) . on load report ( parent report cap . capture ( ) ) ; verify ( orca listener2 ) . on load report ( child report cap . capture ( ) ) ; assert that ( parent report cap . get value ( ) ) . is equal to ( orca load,listener receive,success,pre
java serialization only <PLACE_HOLDER> ootb if not defaulting to json if java serliazation needed together with json defaulting then add to custom post variable types,if ( ! serializepoj os in variables to json ) { variable types . add type ( new byte array type ( ) ) ; variable types . add type ( new serializable type ( serializable variable type track deserialized objects ) ) ; variable types . add type ( new custom object type ( __str__ @$ item instance . class ) ) ; variable types . add type ( new custom object type ( __str__ @$ message instance . class ) ) ; } if ( custom post variable types != null ) { for ( variable type custom variable type : custom post variable types ) { variable types . add type ( custom variable type ) ; } },serialization supports,fail,pre
take from q 1 even though mux <PLACE_HOLDER> q 0 @$ since q 0 empty,assert equals ( call @$ fcq . take ( ) ) ; assert equals ( __num__ @$ fcq . size ( ) ) ;,mux requests,fail,pre
the summary section displays up to two text <PLACE_HOLDER> side by side .,m summary layout = new linear layout ( get context ( ) ) ; m summary layout . add view ( m summary left text view @$ left layout params ) ; m summary layout . add view ( m summary right text view @$ right layout params ) ; main section layout . add view ( m summary layout @$ new linear layout . layout params ( layout params . match_parent @$ layout params . wrap_content ) ) ; set summary text ( null @$ null ) ; create main section content ( main section layout ) ; return main section layout ;,displays views,success,pre
ensure fc 2 <PLACE_HOLDER> directory,assert . assert true ( is dir ( fc2 @$ path ) ) ; assert . assert true ( exists ( fc2 @$ path ) ) ; assert . assert false ( is file ( fc2 @$ path ) ) ;,fc has,success,pre
catcher also <PLACE_HOLDER> the exception @$ prepended :,catch args . add ( __num__ @$ thrown ) ; helper . assert called ( __str__ @$ catch args ) ; asserteq ( cast . apply ( catch args ) @$ returned ) ;,catcher wraps,fail,pre
the acceptor event loop thread needs to be from a different pool otherwise can <PLACE_HOLDER> lags in accepted connections under a lot of load,acceptor event loop group = transport . event loop group ( transport . acceptor_event_loop_group @$ __num__ @$ acceptor event loop thread factory @$ __num__ ) ; metrics = initialise metrics ( options ) ; int worker pool size = options . get worker pool size ( ) ; executor service worker exec = new thread pool executor ( worker pool size @$ worker pool size @$ __num__ @$ time unit . milliseconds @$ new linked transfer queue < > ( ) @$ new vertx thread factory ( __str__ @$ checker @$ true @$ options . get max worker execute time ( ) @$ options . get max worker execute time unit ( ) ) ) ; pool metrics worker pool metrics = metrics != null ? metrics .,event leave,fail,pre
skip if statements as they have oplog file <PLACE_HOLDER> checks .,if ( line . starts with ( __str__ ) ) { continue ; } else if ( line . contains ( windows script generator . exit_marker ) ) { break ; } else { begin index = line . last index of ( __str__ ) + __num__ ; end index = line . index of ( windows script generator . robocopy_no_job_header @$ begin index ) - __num__ ; oplog name = line . substring ( begin index @$ end index ) . trim ( ) ; add oplog line ( oplog name @$ line ) ; },statements file,fail,pre
do not <PLACE_HOLDER> interface methods with instance methods do not <PLACE_HOLDER> private methods note : private methods from parent classes are not shown here @$ but when doing the multimethod connection step @$ we <PLACE_HOLDER> methods of the parent class with methods of a subclass and in that case we want to keep the private methods,if ( match . is private ( ) || ( ! is non real method ( match ) && match . get declaring class ( ) . is interface ( ) && ! method . get declaring class ( ) . is interface ( ) ) ) { } else { cached class methodc = method . get declaring class ( ) ; cached class matchc = match . get declaring class ( ) ; if ( methodc == matchc ) { if ( is non real method ( method ) ) { return method ; } } else if ( ! methodc . is assignable from ( matchc . get the class ( ) ) ) { return method ; } },methods overwrite,success,pre
the data matrix will not fit in this size @$ so the matrix should <PLACE_HOLDER> back bigger,int too small = __num__ ; data matrix writer writer = new data matrix writer ( ) ; bit matrix matrix = writer . encode ( __str__ @$ barcode format . data_matrix @$ too small @$ too small @$ null ) ; assert not null ( matrix ) ; assert true ( too small < matrix . get width ( ) ) ; assert true ( too small < matrix . get height ( ) ) ;,matrix come,success,pre
if tools not found @$ no point in trying a url class loader so <PLACE_HOLDER> the original exception .,if ( ! file . exists ( ) ) throw e ; url [ ] urls = { file . touri ( ) . tourl ( ) } ; trace ( fine @$ urls [ __num__ ] . to string ( ) ) ; cl = url class loader . new instance ( urls ) ; ref tool class loader = new weak reference < class loader > ( cl ) ;,point throw,fail,pre
replace values if better probability <PLACE_HOLDER> sum @$ if same edit distance or one space difference,if ( ( i == max segmentation word length ) || ( ( ( compositions [ circular index ] . distance sum + top ed == compositions [ destination index ] . distance sum ) || ( compositions [ circular index ] . distance sum + separator length + top ed == compositions [ destination index ] . distance sum ) ) && ( compositions [ destination index ] . probability log sum < compositions [ circular index ] . probability log sum + top probability log ) ) || ( compositions [ circular index ] . distance sum + separator length + top ed < compositions [ destination index ] . distance sum ) ) { compositions [ destination index ] . segmented string = compositions [,probability log,success,pre
fail verification which should <PLACE_HOLDER> an alert that is send back to the client .,return new trust manager [ ] { new x509 trust manager ( ) { @ override public void check client trusted ( x509 certificate [ ] x509 certificates @$ string s ) throws certificate exception { throw new certificate exception ( ) ; } @ override public void check server trusted ( x509 certificate [ ] x509 certificates @$ string s ) { } @ override public x509 certificate [ ] get accepted issuers ( ) { return empty arrays . empty_x509_certificates ; } } } ;,which cause,fail,pre
this is a code name @$ ensure this document <PLACE_HOLDER> the same code name .,if ( ! character . is digit ( library min sdk version . char at ( __num__ ) ) ) { if ( ! library min sdk version . equals ( get min sdk version ( default_sdk_version ) ) ) { merging report . add message ( get source file ( ) @$ merging report . record . severity . error @$ string . format ( __str__ + __str__ @$ get min sdk version ( default_sdk_version ) @$ library min sdk version @$ lower priority document . get source file ( ) . print ( false ) ) ) ; return ; } },document uses,success,pre
sanity check : get only element of a set @$ to ensure all segments <PLACE_HOLDER> the same data source .,final string data source = iterables . get only element ( stream support . stream ( segments . spliterator ( ) @$ false ) . map ( segment id :: get data source ) . collect ( collectors . to set ( ) ) ) ; final multiple specific segment spec query segment spec = new multiple specific segment spec ( stream support . stream ( segments . spliterator ( ) @$ false ) . map ( segment id :: to descriptor ) . collect ( collectors . to list ( ) ) ) ; final segment metadata query segment metadata query = new segment metadata query ( new table data source ( data source ) @$ query segment spec @$ new all column includerator ( ) @$,segments have,success,pre
list actually <PLACE_HOLDER> the list,naming context data store impl = ( naming context data store ) this ; synchronized ( impl ) { impl . list ( how_many @$ bl @$ bi ) ; } if ( debug && bl . value != null ) dprint ( __str__ + how_many + __str__ + bl . value . length + __str__ + bi . value ) ;,list generates,success,pre
when the node <PLACE_HOLDER> only one child,if ( parent == x ) { x = parent . left == null ? parent . right : parent . left ; } else if ( x . left == null ) { if ( parent . left == x ) parent . left = x . right ; else parent . right = x . right ; } else { if ( parent . left == x ) parent . left = x . left ; else parent . right = x . left ; },node has,success,pre
if the incoming uri <PLACE_HOLDER> a single note id @$ does the delete based on the incoming data @$ but modifies the where clause to restrict it to the particular note id .,case main_id :,uri contains,fail,pre
an estimate of how much extra memory is needed before we can go ahead and expand the hash table . this <PLACE_HOLDER> the new capacity for values @$ group ids @$ and values by group id as well as the size of the current page,preallocated memory in bytes = ( new capacity - hash capacity ) * ( long ) ( long . bytes + integer . bytes ) + ( calculate max fill ( new capacity ) - max fill ) * long . bytes + current page size in bytes ; if ( ! update memory . update ( ) ) { return false ; },estimate includes,success,pre
direct column access on a complex column @$ expressions can not operate on complex columns @$ only postaggs <PLACE_HOLDER> the column access in a field access postagg so that other postaggs can use it,if ( post aggregator complex direct column is ok ( input row signature @$ post aggregator expression @$ post aggregator rex node ) ) { final post aggregator post aggregator = new field access post aggregator ( post aggregator visitor . get output name prefix ( ) + post aggregator visitor . get and increment counter ( ) @$ post aggregator expression . get direct column ( ) ) ; post aggregator visitor . add post agg ( post aggregator ) ; row order . add ( post aggregator . get name ( ) ) ; } else if ( post aggregator direct column is ok ( input row signature @$ post aggregator expression @$ post aggregator rex node ) ) { row order . add ( post,postaggs allow,fail,pre
array <PLACE_HOLDER> local node & rack node,test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; cluster . sort by distance ( data nodes [ __num__ ] @$ test nodes @$ test nodes . length ) ; assert true ( test nodes [ __num__ ] == data nodes [ __num__ ] ) ; assert true ( test nodes [ __num__ ] == data nodes [ __num__ ] ) ;,array contains,success,pre
if the user <PLACE_HOLDER> their own we use it even if persistence is disabled since we do n't know anything about their implementation .,if ( job scheduler store == null && ! has start exception ( ) ) { if ( ! is persistent ( ) ) { this . job scheduler store = new in memory job scheduler store ( ) ; configure service ( job scheduler store ) ; return this . job scheduler store ; } try { persistence adapter pa = get persistence adapter ( ) ; if ( pa != null ) { this . job scheduler store = pa . create job scheduler store ( ) ; job scheduler store . set directory ( get scheduler directory file ( ) ) ; configure service ( job scheduler store ) ; return this . job scheduler store ; } } catch ( io exception e ),user specified,fail,pre
setup <PLACE_HOLDER> mapper .,message receiver s ; if ( sponge . get game ( ) . is server available ( ) ) { s = sponge . get server ( ) . get console ( ) ; } else { s = new client message reciever ( ) ; },setup broadcast,fail,pre
<PLACE_HOLDER> proj rel only projects one expression <PLACE_HOLDER> this project only projects one expression @$ i.e . scalar subqueries .,list < rex node > proj exprs = project . get projects ( ) ; if ( proj exprs . size ( ) != __num__ ) { return ; },expression function,fail,pre
we do not resolve a vanilla name starting with a lower case letter try to resolve against a default import @$ because we know that the default packages do not <PLACE_HOLDER> classes like these,test default imports &= ! ( type instanceof lower case class ) ; if ( test default imports ) { for ( int i = __num__ @$ size = default_imports . length ; i < size ; i ++ ) { string package prefix = default_imports [ i ] ; string name = type . get name ( ) ; constructed class with package tmp = new constructed class with package ( package prefix @$ name ) ; if ( resolve ( tmp @$ false @$ false @$ false ) ) { type . set redirect ( tmp . redirect ( ) ) ; return true ; } } string name = type . get name ( ) ; if ( name . equals ( __str__ ) ),packages contain,success,pre
put the band into the correct location on the page . once the band is <PLACE_HOLDER>d we translate the device transform so that the band will <PLACE_HOLDER> down the page on the next iteration of the loop .,band graphics . set transform ( uniform transform ) ; band graphics . transform ( device transform ) ; device transform . translate ( __num__ @$ - band height ) ;,band scroll,fail,pre
this test will <PLACE_HOLDER> rows 0 and 1 from the parent before calling split @$ so we expect the primary <PLACE_HOLDER> to start at offset 2 .,when ( fake storage client . read rows ( read rows request . new builder ( ) . set read position ( stream position . new builder ( ) . set stream ( stream . new builder ( ) . set name ( __str__ ) ) . set offset ( __num__ ) ) . build ( ) ) ) . then return ( new fake big query server stream < > ( parent responses . sub list ( __num__ @$ __num__ ) ) ) ;,primary read,success,pre
set to unlimited if global memstore size already <PLACE_HOLDER> lower limit,if ( flush pressure >= __num__ ) { max throughput to set = double . max_value ; } else { max throughput to set = max throughput lower bound + ( max throughput upper bound - max throughput lower bound ) * flush pressure ; },size exceeds,success,pre
while partition has been spilled @$ relocation <PLACE_HOLDER> filter bits for current bucket @$ and build <PLACE_HOLDER> filter with hashcode .,if ( status == bucket_status_in_filter ) { this . bloom filter . set bits location ( bucket @$ bucket in segment pos + bucket_header_length ) ; this . bloom filter . add hash ( hash code ) ; },relocation oom,fail,pre
svn repo <PLACE_HOLDER> history . if there is a cheap test @$ then this code can be refined @$ boosting performance .,return true ;,repo has,success,pre
the tables should <PLACE_HOLDER> the same size and be dividable by 3,for ( int i = __num__ ; i < vertex table . length ; i += __num__ ) { vector3f vert = new vector3f ( vertex table [ i ] @$ vertex table [ i + __num__ ] @$ vertex table [ i + __num__ ] ) ; vector3f norm = vert to normal map . get ( vert ) ; if ( norm == null ) { norm = new vector3f ( normal table [ i ] @$ normal table [ i + __num__ ] @$ normal table [ i + __num__ ] ) ; vert to normal map . put ( vert @$ norm ) ; } else { norm . add local ( normal table [ i ] @$ normal table [ i + __num__,tables have,success,pre
constant <PLACE_HOLDER> constructor matcher 2 @$,match get member ( clazz @$ method @$ code attribute @$ offset @$ instruction @$ null @$ get constructor matcher2 @$ false @$ false @$ class constants . method_name_init @$ null ) ;,constant get,success,pre
populate the new <PLACE_HOLDER> rule graph <PLACE_HOLDER>er with all of the usable rules from the last <PLACE_HOLDER> rule graph <PLACE_HOLDER>er for incremental action graph generation .,action graph cache . populate action graph builder with cached rules ( event bus @$ target graph @$ graph builder ) ;,new build,success,pre
the object that we 're modifying here is a copy of the original ! so let 's <PLACE_HOLDER> the filename from relative to absolute by grabbing the file object ... in case the name of the file comes from previous steps @$ forget about this !,try { list < string > new filenames = new array list < string > ( ) ; if ( ! is in fields ( ) ) { file input list file list = get files ( space ) ; if ( file list . get files ( ) . size ( ) > __num__ ) { for ( file object file object : file list . get files ( ) ) { if ( file object . exists ( ) ) { new filenames . add ( file object . get name ( ) . get path ( ) ) ; } } set file name ( new filenames . to array ( new string [ new filenames . size ( ) ] ) ) ; set,let change,success,pre
fire the presence <PLACE_HOLDER> event,while ( presences . has next ( ) ) { listener . presence changed ( presences . next ( ) ) ; },presence changed,success,pre
if the request sequence number is greater than the next sequence number @$ that indicates a command is missing . <PLACE_HOLDER> the command request and return a future to be completed once commands are properly sequenced . if the session 's current sequence number is too far beyond the last known sequence number @$ reject the command to force it to be resent by,if ( sequence number > session . next request sequence ( ) ) { if ( session . get commands ( ) . size ( ) < max_pending_commands ) { log . trace ( __str__ @$ sequence number @$ session . next request sequence ( ) ) ; session . register command ( request . sequence number ( ) @$ new pending command ( request @$ future ) ) ; return future ; } else { return completable future . completed future ( log response ( command response . builder ( ) . with status ( raft response . status . error ) . with error ( raft error . type . command_failure ) . with last sequence ( session . get request sequence ( ) ) .,future prepare,fail,pre
copy resource into a byte array . this is necessary because several browsers consider class.get resource a security risk since it can be used to load additional classes . class.get resource as stream just <PLACE_HOLDER> raw bytes @$ which we can convert to a sound .,byte [ ] buffer = access controller . do privileged ( new privileged action < byte [ ] > ( ) { public byte [ ] run ( ) { try { input stream resource = basic look and feel . this . get class ( ) . get resource as stream ( sound file ) ; if ( resource == null ) { return null ; } buffered input stream in = new buffered input stream ( resource ) ; byte array output stream out = new byte array output stream ( __num__ ) ; byte [ ] buffer = new byte [ __num__ ] ; int n ; while ( ( n = in . read ( buffer ) ) > __num__ ) { out .,stream returns,success,pre
only fill in value if the argument <PLACE_HOLDER> a value .,code . begin control flow ( __str__ @$ i ) ;,argument has,success,pre
record the failure persistently @$ and upload to uma when the library successfully <PLACE_HOLDER> next time .,precacheuma . record ( precacheuma . event . precache_task_load_library_fail ) ; break ; default : break ;,library compiles,fail,pre
patch throwing pc into return address so that deoptimization <PLACE_HOLDER> the right debug info,patch return address ( exception pc ) ; word handler pc = exception handler for pc ( exception_handler_for_pc @$ thread ) ; if ( logging ) { printf ( __str__ @$ word . object to tracked pointer ( exception ) . raw value ( ) @$ exception pc . raw value ( ) @$ handler pc . raw value ( ) ) ; decipher ( handler pc . raw value ( ) ) ; printf ( __str__ ) ; },deoptimization has,fail,pre
wait until main thread <PLACE_HOLDER> signalled of the semaphore,synchronized ( locka ) { while ( handshake . get waiter count ( ) == __num__ ) { utils . go sleep ( __num__ ) ; } handshake . semav ( ) ; try { blocked thread . wait until blocked ( ) ; system . out . println ( __str__ ) ; utils . check thread state ( itself @$ thread . state . runnable ) ; check stack ( itself @$ examiner stack @$ es depth ) ; system . out . println ( __str__ + __str__ ) ; utils . check thread state ( blocked thread @$ thread . state . blocked ) ; check stack ( blocked thread @$ blocked stack @$ bs depth ) ; } catch ( exception e ) { e,thread gets,success,pre
the index population <PLACE_HOLDER> conflicts on the fly @$ however for updates coming in we 're in a position where we can not detect conflicts while applying @$ but instead afterwards .,if ( descriptor . is unique ( ) && can check conflicts without store access ( ) ) { updater = new deferred conflict checking index updater ( updater @$ this :: new reader @$ descriptor ) ; },population introduces,fail,pre
report if multiple devices are <PLACE_HOLDER> the filter .,if ( ! quiet && devices . size ( ) > __num__ ) { print message ( __str__ + devices . size ( ) + __str__ ) ; },devices matching,success,pre
if the expression does n't <PLACE_HOLDER> any package @$ we 'll look up the package in the imports . if not found there @$ it 's a local package .,if ( expression . contains ( dot ) ) { int last dot index = expression . last index of ( __str__ ) ; messages package = expression . substring ( __num__ @$ last dot index ) ; } else { string package name = imported classes . get ( expression ) ; if ( package name == null ) { messages package = class package ; } else { messages package = package name ; } },expression specify,fail,pre
closing the writable log channel @$ then the underlying channel is what physical log file <PLACE_HOLDER>,channel . close ( ) ; store channel . close ( ) ;,file does,success,pre
we overrode this method here to clear the picked state of edges and vertices if we ever get a released event when the user is <PLACE_HOLDER> somewhere that is not an edge or vertex,if ( ! is dragging ( ) && vertex == null && edge == null ) { maybe clear picked state ( e ) ; } super . mouse released ( e ) ;,user dragging,fail,pre
test if gdi can <PLACE_HOLDER> the transform,boolean direct togdi = ( ( transform type != affine transform . type_general_transform ) && ( ( transform type & affine transform . type_flip ) == __num__ ) ) ; if ( ! direct togdi ) { return __num__ ; },gdi apply,fail,pre
verify that we can read in all the transactions that we have written . if there were any corruptions @$ it is likely that the reading in of these transactions will <PLACE_HOLDER> an exception .,for ( storage directory sd : fsimage . get storage ( ) . dir iterable ( name node dir type . edits ) ) { file edit file = new file ( sd . get current dir ( ) @$ log file name ) ; system . out . println ( __str__ + edit file ) ; fs edit log loader loader = new fs edit log loader ( namesystem @$ start tx id ) ; long num edits this log = loader . loadfs edits ( new edit log file input stream ( edit file ) @$ start tx id ) ; system . out . println ( __str__ + num edits this log ) ; assert true ( num edits == - __num__ || num edits,the throw,success,pre
we only need to create a customized image @$ if we 're running on linux @$ as docker on mac os and windows does n't <PLACE_HOLDER> users from the host into the container anyway .,if ( os . get current ( ) != os . linux ) { return base image ; } return image map . compute if absent ( base image @$ ( image ) -> { reporter . handle ( event . info ( __str__ + image + __str__ ) ) ; string work dir = path fragment . create ( __str__ ) . get relative ( exec root . get base name ( ) ) . get path string ( ) ; string builder dockerfile = new string builder ( ) ; dockerfile . append ( string . format ( __str__ @$ image ) ) ; dockerfile . append ( string . format ( __str__ @$ work dir ) ) ; if ( gid > __num__ ) {,docker dispatch,fail,pre
lazy check that we get 5 rows back @$ <PLACE_HOLDER> off checking contents,assert equals ( __num__ @$ results [ __num__ ] . get row count ( ) ) ;,check skip,fail,pre
make sure this call happens before check <PLACE_HOLDER> application class as servlet ! ! !,boolean has boot = has boot classes ( webdata ) ; resteasy deployment data . set boot classes ( has boot ) ; class < ? > declared application class = check declared application class as servlet ( webdata @$ class loader ) ;,check declared,success,pre
the call to build may also <PLACE_HOLDER> unsupported version exception @$ if there are essential fields that can not be represented in the chosen version .,do send ( client request @$ is internal request @$ now @$ builder . build ( version ) ) ;,call throw,success,pre
unlike message consumers @$ we try current span before trying extraction . this is the proper order because the span in scope should <PLACE_HOLDER> precedence over a potentially stale header entry . note : brave instrumentation used properly does not result in stale header entries @$ as we always clear message headers after reading .,span span ; if ( maybe parent == null ) { trace context or sampling flags extracted = kafka tracing . extract and clear headers ( extractor @$ request @$ record . headers ( ) ) ; span = kafka tracing . next messaging span ( sampler @$ request @$ extracted ) ; } else { span = tracer . new child ( maybe parent ) ; },span take,success,pre
table <PLACE_HOLDER> these split points,byte [ ] [ ] table split keys = new byte [ ] [ ] { bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) } ;,table contains,fail,pre
before sending the request @$ check if the request <PLACE_HOLDER> a global session and what we have is a local session . if so do an upgrade .,if ( check for upgrade ) { request upgrade request = null ; try { upgrade request = zks . check upgrade session ( request ) ; } catch ( keeper exception ke ) { if ( request . get hdr ( ) != null ) { request . get hdr ( ) . set type ( op code . error ) ; request . set txn ( new error txn ( ke . code ( ) . int value ( ) ) ) ; } request . set exception ( ke ) ; log . warn ( __str__ @$ ke ) ; } catch ( io exception ie ) { log . error ( __str__ @$ ie ) ; } if ( upgrade request != null ),request has,fail,pre
test <PLACE_HOLDER> coin .,account send account = public methed . query account ( send account key @$ blocking stub full ) ; long send account before balance = send account . get balance ( ) ; assert . assert true ( send account before balance == __num__ ) ; account receipt account = public methed . query account ( receipt account key @$ blocking stub full ) ; long receipt account before balance = receipt account . get balance ( ) ; assert . assert true ( receipt account before balance == __num__ ) ;,test send,success,pre
copy the existing queue into the new one . first <PLACE_HOLDER> the oldest entry if no new ones exist .,if ( m event records . remaining capacity ( ) == __num__ ) { event record record = m event records . poll ( ) ; if ( record != null ) { m call event record map . remove ( record . get record entry ( ) ) ; } },one remove,success,pre
close the column family <PLACE_HOLDER> first @$ then the db last,for ( column family handle cfh : column family handles . values ( ) ) { safe close ( cfh @$ exception reference ) ; } logger . info ( __str__ ) ; safe close ( rocksdb @$ exception reference ) ; rocksdb = null ; closed = true ; if ( exception reference . get ( ) != null ) { throw new io exception ( exception reference . get ( ) ) ; } db write lock . unlock ( ) ;,family handle,fail,pre
open d <PLACE_HOLDER> consumer,cons . open ( ) ;,d lock,fail,pre
the implementation of fragments do n't <PLACE_HOLDER> to be beans themselves,spring data repository creator repository creator = new spring data repository creator ( class output @$ composite index @$ ( n ) -> { additional beans . produce ( additional bean build item . unremovable of ( n ) ) ; } ) ;,implementation need,success,pre
load properties <PLACE_HOLDER> loaded properties from the input set @$ so the remaining ones were not on the node,final int iterator properties with no value = additional properties to load . int iterator ( ) ; while ( properties with no value . has next ( ) ) { put ( properties with no value . next ( ) @$ no_value ) ; },properties extends,fail,pre
all existing types should <PLACE_HOLDER> a dedicated runner,for ( scenario simulation model . type value : scenario simulation model . type . values ( ) ) { final scenario runner provider retrieved = abstract scenario runner . get specific runner provider ( value ) ; assert not null ( retrieved ) ; },types have,success,pre
specifically only include relevant bits according to the env var setting @$ rather than strictly overlaying config so that if the config file <PLACE_HOLDER> a setting for master endpoints @$ the file does n't override the env var if it 's a domain : as master endpoints takes precedence over domains .,final uri uri = new uri ( master ) ; config config from env var = config factory . empty ( ) ;,file defines,fail,pre
handle possible indirection indirect flow should be to a data pointer which <PLACE_HOLDER> code .,if ( ref type == ref type . indirection ) { instruction dest instr = listing . get instruction containing ( ref . get to address ( ) ) ; int cnt = __num__ ; if ( dest instr == null && follow indirect flows ) { if ( instr == null || ! instr . get min address ( ) . equals ( from addr ) ) { instr = listing . get instruction at ( from addr ) ; } cnt = follow indirection ( block ref queue @$ include externals @$ block @$ ref @$ instr . get flow type ( ) . is call ( ) ? ref type . computed_call : ref type . computed_jump @$ monitor ) ; } if ( cnt,which contains,fail,pre
empty error messages <PLACE_HOLDER> success,if ( original host id == cluster save file state . error_code ) { long host id = savefile_data [ __num__ ] . get long ( __str__ ) ; string host name = savefile_data [ __num__ ] . get string ( __str__ ) ; string warning msg = savefile_data [ __num__ ] . get string ( __str__ ) ; warnings . add ( __str__ + host id + __str__ + host name + __str__ + warning msg ) ; },messages means,fail,pre
second observer will <PLACE_HOLDER> only last event,second observer . assert value ( __num__ ) ;,observer receive,success,pre
let task 1 <PLACE_HOLDER> and cancel task 2,task2 handle queue . add ( handle ) ;,task complete,fail,pre
the test data is constructed such that the merge join zig zag <PLACE_HOLDER> an early out @$ leaving elements on the dynamic path input unconsumed,data set < path > edges = env . from elements ( new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) ) ; iterative data set < path > current paths = edges . iterate ( __num__ ) ; data set < path > new paths = current paths . join ( edges @$ join hint . repartition_sort_merge,zag has,success,pre
update the per <PLACE_HOLDER> checksum,if ( file check sum generator != null ) { file check sum generator . update ( buffer @$ __num__ @$ read ) ; },per file,success,pre
functions within i <PLACE_HOLDER> cause syntax errors on safari .,assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ;,functions begin,fail,pre
if comment does n't <PLACE_HOLDER> a post title @$ set it to the passed one and save to comment table,if ( m comment != null && m comment . get post title ( ) == null ) { m comment . set post title ( post title ) ; m dispatcher . dispatch ( comment action builder . new update comment action ( m comment ) ) ; },comment have,success,pre
add a listener to know when android has done another measurement pass . the listener automatically <PLACE_HOLDER> itself to prevent triggering the animation multiple times .,m layout . add on layout change listener ( new on layout change listener ( ) { @ override public void on layout change ( view v @$ int left @$ int top @$ int right @$ int bottom @$ int old left @$ int old top @$ int old right @$ int old bottom ) { m layout . remove on layout change listener ( this ) ; start animator ( callback ) ; } } ) ;,listener inflates,fail,pre
copy & <PLACE_HOLDER> decimal point,int decimal point = end ; for ( int i = start @$ j = offset ; i < end ; i ++ @$ j ++ ) { buf [ i ] = value [ j ] ; if ( buf [ i ] == __str__ && i < decimal point ) { decimal point = i ; } },& set,fail,pre
test chrome driver is not <PLACE_HOLDER> headless request ; using chrome driver instead,super . driver = new chrome driver ( options ) ; driver = ( chrome driver ) super . driver ; driver . get ( pages . clicks page ) ; assert that ( check permission ( driver @$ clipboard_read ) ) . is equal to ( __str__ ) ; assert that ( check permission ( driver @$ clipboard_write ) ) . is equal to ( __str__ ) ; driver . set permission ( clipboard_read @$ __str__ ) ; driver . set permission ( clipboard_write @$ __str__ ) ; assert that ( check permission ( driver @$ clipboard_read ) ) . is equal to ( __str__ ) ; assert that ( check permission ( driver @$ clipboard_write ) ) . is equal to ( __str__ ) ;,driver showing,fail,pre
produce a message to broker 1 's test queue and verify that broker 1 's memory usage <PLACE_HOLDER> increased @$ but broker 2 still <PLACE_HOLDER> no memory usage .,send messages ( __str__ @$ test queue @$ __num__ ) ; assert true ( broker1 test queue . get memory usage ( ) . get usage ( ) > __num__ ) ; assert equals ( __num__ @$ broker2 test queue . get memory usage ( ) . get usage ( ) ) ;,broker has,success,pre
do one last placeholder substitution @$ this is useful as we do n't stop the build when a library <PLACE_HOLDER> a placeholder substitution @$ but the element might have been overridden so the problem was transient . however @$ with the final document ready @$ all placeholders values must have been provided .,if ( ! m optional features . contains ( invoker . feature . no_placeholder_replacement ) ) { perform place holder substitution ( loaded main manifest info @$ xml document optional . get ( ) @$ merging report builder ) ; if ( merging report builder . has errors ( ) ) { return merging report builder . build ( ) ; } },library does,fail,pre
the partition must <PLACE_HOLDER> a current consumer,string consumer = current partition consumer . get ( partition ) ; if ( consumer == null ) log . error ( __str__ @$ partition ) ; if ( prev assignment . contains key ( partition ) && current assignment . get ( consumer ) . size ( ) > current assignment . get ( prev assignment . get ( partition ) . consumer ) . size ( ) + __num__ ) { reassign partition ( partition @$ current assignment @$ sorted current subscriptions @$ current partition consumer @$ prev assignment . get ( partition ) . consumer ) ; reassignment performed = true ; modified = true ; continue ; },partition have,success,pre
only some of the node memories <PLACE_HOLDER> special serialization handling so we iterate over all of them and process only those that <PLACE_HOLDER> it,for ( base node base node : context . sinks . values ( ) ) { memory memory = memories . peek node memory ( base node ) ; if ( memory != null ) { protobuf messages . node memory _node = null ; switch ( memory . get node type ( ) ) { case node type enums . query element node : { _node = write query element node memory ( base node . get id ( ) @$ memory @$ wm ) ; break ; } } if ( _node != null ) { _ksb . add node memory ( _node ) ; } } },some need,fail,pre
the actual <PLACE_HOLDER> method,secu filter . process authentication ( filter context @$ client builder @$ method security @$ tracing . atn tracing ( ) ) ; assert that ( filter context . is should finish ( ) @$ is ( true ) ) ; assert that ( secu context . user ( ) @$ is ( optional . empty ( ) ) ) ;,actual tested,success,pre
note : batch <PLACE_HOLDER> the timestamps of dataset if the event traffic spike,dataset config manager . update last refresh time ( dataset @$ event . get high watermark ( ) ) ; thirdeye metrics util . processed trigger event counter . inc ( ) ; log . debug ( __str__ + event . get dataset name ( ) ) ;,batch updates,fail,pre
direct call to the relation service shall not <PLACE_HOLDER> a relation not found exception for an internal relation,if ( relation serv call flg ) { try { relation serv . send role update notification ( my rel id @$ new role @$ old role value ) ; } catch ( relation not found exception exc ) { throw new runtime exception ( exc . get message ( ) ) ; } } else { object [ ] params = new object [ __num__ ] ; params [ __num__ ] = my rel id ; params [ __num__ ] = new role ; params [ __num__ ] = old role value ; string [ ] signature = new string [ __num__ ] ; signature [ __num__ ] = __str__ ; signature [ __num__ ] = __str__ ; signature [ __num__ ] = __str__ ; try {,call throw,success,pre
updated golden value since we <PLACE_HOLDER> a different serial version uid in open jdk .,string s = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; tree map < string @$ string > map = new tree map < string @$ string > ( string . case_insensitive_order ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; sorted map < string @$ string > sub map = map . sub map ( __str__ @$ __str__ ) ; new serialization tester < sorted map < string @$ string > > ( sub map @$ s ) { @,value have,success,pre
check number of transactions @$ should only <PLACE_HOLDER> one,log entry reader log entry reader = new version aware log entry reader ( new test command reader factory ( ) ) ; assert that ( log files . get lowest log version ( ) @$ is ( log files . get highest log version ( ) ) ) ; long version = log files . get highest log version ( ) ; try ( log versioned store channel channel = log files . open for version ( version ) ; read ahead log channel read ahead log channel = new read ahead log channel ( channel ) ; log entry cursor cursor = new log entry cursor ( log entry reader @$ read ahead log channel ) ) { log entry entry ; long number of transactions,number have,success,pre
create the <PLACE_HOLDER> option panel .,final j panel allow option panel = new j panel ( layout ) ; allow option panel . set border ( border factory . create titled border ( etched border @$ msg ( __str__ ) ) ) ; allow option panel . add ( tip ( allow shrinking check box @$ __str__ ) @$ constraints last stretch ) ; allow option panel . add ( tip ( allow optimization check box @$ __str__ ) @$ constraints last stretch ) ; allow option panel . add ( tip ( allow obfuscation check box @$ __str__ ) @$ constraints last stretch ) ;,the allow,success,pre
the front end will know which <PLACE_HOLDER> is the active <PLACE_HOLDER> ; just use the first <PLACE_HOLDER> for the test,final workspace active workspace = workspaces [ __num__ ] ; swing utilities . invoke and wait ( new runnable ( ) { @ override public void run ( ) { running tool = active workspace . run tool ( tool config ) ; } } ) ; assert not null ( running tool ) ; swing utilities . invoke and wait ( new runnable ( ) { @ override public void run ( ) { running tool . close ( ) ; } } ) ;,one one,success,pre
adapter views <PLACE_HOLDER> their children in a frame so we need to go one layer deeper here .,if ( parent instanceof adapter view animator ) { vg = ( view group ) vg . get child at ( __num__ ) ; } if ( vg == null ) return ; remote response response = null ; int child count = vg . get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { object tag = vg . get child at ( i ) . get tag ( com . android . internal . r . id . fill in intent ) ; if ( tag instanceof remote response ) { response = ( remote response ) tag ; break ; } } if ( response == null ) return ; response . handle,views place,fail,pre
use batch <PLACE_HOLDER> option when performing seeded operation,input configurator . set batch scan ( accumulo input format . class @$ conf @$ true ) ; add iterators ( accumulo store @$ conf @$ context . get user ( ) @$ operation ) ; add ranges ( accumulo store @$ conf @$ operation ) ; final java pairrdd < element @$ null writable > pairrdd = spark context . newapi hadooprdd ( conf @$ element input format . class @$ element . class @$ null writable . class ) ; final javardd < element > rdd = pairrdd . map ( new first element ( ) ) ; return rdd ;,batch scan,success,pre
transfer send some asset issue to default account @$ to test if this transaction <PLACE_HOLDER> the transaction free net .,assert . assert true ( public methed . transfer asset ( to address @$ asset account id . to byte array ( ) @$ __num__ @$ transfer asset address @$ transfer asset create key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ; asset creator net = public methed . get account net ( asset014 address @$ blocking stub full ) ; asset transfer net = public methed . get account net ( transfer asset address @$ blocking stub full ) ; long creator after net used = asset creator net . get net used ( ) ; long transfer after free net used = asset transfer net . get free net used ( ) ; logger,transaction use,success,pre
e.g . binary operator extends bi function @$ binary operator <PLACE_HOLDER> no abstract method @$ but it is really a sam,if ( type . is interface ( ) ) { methods = type . redirect ( ) . get all declared methods ( ) ; } else { methods = type . get methods ( ) ; },operator has,fail,pre
dw suggested buffer size specifies the suggested buffer size for reading the file . generally @$ this size should be large enough to contain the largest chunk in the file . if set to zero @$ or if it is too small @$ the playback software will have to reallocate memory during playback @$ which will <PLACE_HOLDER> performance . for an interleaved file @$,video track vt = null ; int width = __num__ @$ height = __num__ ;,which improve,fail,pre
csv can not <PLACE_HOLDER> null and empty string .,if ( value == null ) { current line . add ( __str__ ) ; } else if ( value instanceof string ) { current line . add ( ( string ) value ) ; } else { current line . add ( value . to string ( ) ) ; },csv return,fail,pre
configuration for publishing jars containing sources for <PLACE_HOLDER> classes to the project artifacts for including in the ivy.xml,if ( _generate sources jar task == null ) { configuration container configurations = project . get configurations ( ) ; configuration generated sources = configurations . maybe create ( __str__ ) ; configuration test generated sources = configurations . maybe create ( __str__ ) ; test generated sources . extends from ( generated sources ) ; _generate sources jar task = project . get tasks ( ) . create ( __str__ @$ jar . class @$ jar task -> { jar task . set group ( java base plugin . documentation_group ) ; jar task . set description ( __str__ ) ; jar task . set classifier ( __str__ ) ; } ) ; project . get artifacts ( ) . add ( __str__ @$ _generate sources,configuration generated,success,pre
if the op set is null then the implementation does n't <PLACE_HOLDER> a presence operation set which is unacceptable for gibberish .,if ( operation set presence1 == null ) { throw new null pointer exception ( __str__ + __str__ + __str__ ) ; },implementation offer,success,pre
all notes with this template must <PLACE_HOLDER> at least two cards @$ or we could end up creating orphaned notes,sql = __str__ + utils . ids2str ( cids ) + __str__ ; if ( m col . get db ( ) . query scalar ( sql ) != __num__ ) { return false ; },notes have,success,pre
keys which <PLACE_HOLDER> similar strings as the desired key,list < string > possible matches = new array list < > ( ) ;,which have,success,pre
set up things @$ the order <PLACE_HOLDER> matter as if it is messed up then the set up should fail because of the precondition checks in the respective set up methods for creating a customized task queue see test realtime index task failure test,task storage = set up task storage ( ) ; handoff notifier factory = set up segment hand off notifier factory ( ) ; data segment pusher = set up data segment pusher ( ) ; mdc = set up metadata storage coordinator ( ) ; tb = set up task toolbox factory ( data segment pusher @$ handoff notifier factory @$ mdc ) ; task runner = set up thread pool task runner ( tb ) ; task queue = set up task queue ( task storage @$ task runner ) ;,order does,success,pre
this is expected @$ void type options <PLACE_HOLDER> no args .,if ( option definition . get type ( ) . equals ( void . class ) ) { } else if ( next args . has next ( ) ) { unconverted value = next args . next ( ) ; command line form . append ( __str__ ) . append ( unconverted value ) ; } else { throw new options parsing exception ( __str__ + arg ) ; },options have,success,pre
socket timeout exceptions are interrupted io exceptions ; however they do not signify an external interruption @$ but simply a failed download due to some server timing out . so <PLACE_HOLDER> them as ordinary io exceptions .,throw new io exception ( e ) ;,download rethrow,success,pre
do not start any application as super dev mode will <PLACE_HOLDER> the page once done compiling,if ( super dev mode . enable based on parameter ( ) ) { return ; },mode edit,fail,pre
will be null in the <PLACE_HOLDER> path or when we run out opf attempts..,if ( notification time != null ) { try { janitor queue . record future notification ( notification time @$ key @$ user token @$ account record id @$ tenant record id ) ; } catch ( final io exception e ) { log . warn ( __str__ @$ payment transaction id @$ e . get message ( ) ) ; } },the create,fail,pre
first lets check that a fully opaque foreground <PLACE_HOLDER> sufficient contrast,double test ratio = contrast calculator . calculate contrast ( foreground @$ background @$ __num__ ) ; if ( test ratio < min contrast ratio ) { return - __num__ ; },foreground has,success,pre
we hide the sms label until we know if the chat <PLACE_HOLDER> sms .,sms button . set visible ( false ) ; center panel . add ( sms button @$ constraints ) ;,chat declares,fail,pre
we then use the <PLACE_HOLDER> up service to assert that it behaves as expected @$ just like in any other test .,map < com . oracle . truffle . api . source . source @$ coverage > coverage map = coverage instrument . get coverage map ( ) ; assert . assert equals ( __num__ @$ coverage map . size ( ) ) ; coverage map . for each ( ( com . oracle . truffle . api . source . source s @$ coverage v ) -> { set < integer > not yet covered line numbers = coverage instrument . non covered line numbers ( s ) ; object [ ] expected = new integer [ ] { __num__ @$ __num__ @$ __num__ } ; assert . assert array equals ( expected @$ not yet covered line numbers . stream ( ) . sorted ( ) .,the looked,success,pre
we know an exists only ever <PLACE_HOLDER> one child @$ and the previous algorithm <PLACE_HOLDER> confirmed the child is an or,final group element or = ( group element ) parent . get children ( ) . get ( __num__ ) ; parent . set type ( group element . not ) ; parent . get children ( ) . clear ( ) ; final group element and = group element factory . new and instance ( ) ; for ( rule condition element rule condition element : or . get children ( ) ) { final group element new not = group element factory . new not instance ( ) ; new not . add child ( rule condition element ) ; and . add child ( new not ) ; } parent . add child ( and ) ; parent . pack ( ) ;,exists has,success,pre
the expression must <PLACE_HOLDER> no children,return check parameter count ( expr @$ keywords . none ) ;,expression have,success,pre
then the last task should be cleared @$ and the system should <PLACE_HOLDER> lock task mode,verify ( tr1 ) . perform clear task locked ( ) ; assert false ( m lock task controller . is task locked ( tr1 ) ) ; assert equals ( lock_task_mode_none @$ m lock task controller . get lock task mode state ( ) ) ; verify lock task stopped ( times ( __num__ ) ) ;,system retain,fail,pre
arc to treats the sweep angle mod 360 @$ so check for that @$ since we think 360 means <PLACE_HOLDER> the entire oval,if ( sweep < __num__ && sweep > - __num__ ) { ring path . set fill type ( path . fill type . even_odd ) ; ring path . move to ( x + radius @$ y ) ; ring path . line to ( x + radius + thickness @$ y ) ; ring path . arc to ( bounds @$ __num__ @$ sweep @$ false ) ; ring path . arc to ( inner bounds @$ sweep @$ - sweep @$ false ) ; ring path . close ( ) ; } else { ring path . add oval ( bounds @$ path . direction . cw ) ; ring path . add oval ( inner bounds @$ path . direction . ccw ) ;,means draw,success,pre
the wait is necessary to have the poll function <PLACE_HOLDER> and propagate the changes from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one three = database onecode node . get comments ( ) . get global code node comment ( ) ; final list < i comment > two three = database twocode node . get comments ( ) . get global code node comment ( ) ; assert equals ( one two size @$ one three . size ( ) ) ; assert equals ( two two size @$ two three . size ( ) ) ; assert equals ( one three @$ two three ) ; assert equals ( __str__ @$ iterables . get last ( one three ) . get comment ( ),function complete,success,pre
user 6 <PLACE_HOLDER> other global permission,user dto user6 = db . users ( ) . insert user ( with email ( __str__ ) ) ; db . users ( ) . insert permission on user ( organization1 @$ user6 @$ administer_quality_profiles ) ;,user has,success,pre
create the base list of classes which <PLACE_HOLDER> possible methods to be overloaded,this . class list = new linked hash set < class > ( ) ; this . class list . add ( super class ) ; if ( generate delegate field ) { class list . add ( delegate class ) ; collections . add all ( this . class list @$ delegate class . get interfaces ( ) ) ; } if ( interfaces != null ) { collections . add all ( this . class list @$ interfaces ) ; } this . proxy name = proxy name ( ) ; this . empty body = empty body ;,which have,success,pre
grantor <PLACE_HOLDER> release of lock ...,if ( is debug enabled_dls ) { if ( my reply . reply code == d lock release reply message . ok ) { logger . trace ( log marker . dls_verbose @$ __str__ @$ this . object name @$ my reply . service name ) ; } else if ( my reply . reply code == d lock release reply message . not_grantor ) { logger . trace ( log marker . dls_verbose @$ __str__ @$ my reply . get sender ( ) @$ my reply . service name ) ; } },grantor confirm,fail,pre
indent:0 exp:12 <PLACE_HOLDER> indent:8 exp:8 indent:8 exp:8 indent:8 exp:8,system . get property ( __str__ ) ;,exp:12 warn,success,pre
spawn a thread to put on server @$ which will <PLACE_HOLDER> a lock on entry,set client server observer for before interest recovery ( ) ; server1 . invoke ( ( ) -> start server ( ) ) ; verify dead and live servers ( __num__ @$ __num__ ) ; wait for before interest recovery call back ( ) ;,which hold,fail,pre
always log the error here @$ we might not <PLACE_HOLDER> the error in the caller,log . error ( __str__ @$ db err ) ;,error know,fail,pre
any object literal definition <PLACE_HOLDER> warning .,used ( lines ( __str__ @$ __str__ ) ) ;,definition causes,fail,pre
they are in different functions @$ find out which <PLACE_HOLDER> the other,if ( has larger parent ( outer2 @$ section length ) ) { outer = outer2 ; } else { },which encloses,success,pre
if user is <PLACE_HOLDER> headers support fragment @$ swap to empty fragment and wait <PLACE_HOLDER> finishes .,if ( is showing headers ( ) && grid view != null && grid view . get scroll state ( ) != recycler view . scroll_state_idle ) { get child fragment manager ( ) . begin transaction ( ) . replace ( r . id . scale_frame @$ new fragment ( ) ) . commit ( ) ; grid view . remove on scroll listener ( m wait scroll finish and commit main fragment ) ; grid view . add on scroll listener ( m wait scroll finish and commit main fragment ) ; } else { commit main fragment ( ) ; },user scrolling,success,pre
client <PLACE_HOLDER> these protocols .,return protocols ;,client wants,fail,pre
the input method manager only <PLACE_HOLDER> security exceptions @$ so let 's log all others .,if ( ! ( e instanceof security exception ) ) { slog . wtf ( tag @$ __str__ @$ e ) ; } throw e ;,manager throws,success,pre
axes <PLACE_HOLDER> vertical axis,int x = left margin - __num__ ; int y = top margin ; font metrics fm = g . get font metrics ( ) ; g . draw line ( x @$ y @$ x @$ y + h ) ; int n = __num__ ; if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ),axes use,fail,pre
alias can be used in js types . types <PLACE_HOLDER> node type string and not name so we <PLACE_HOLDER> to use their name as string .,string node name = n . is string ( ) || n . is import star ( ) ? n . get string ( ) : preprocessor symbol table . get qualified name ( n ) ;,types have,success,pre
the message was not sent using async send @$ so we should only ack the local broker when we get confirmation that the remote broker has <PLACE_HOLDER> the message .,response callback callback = new response callback ( ) { public void on completion ( future response future ) { try { response response = future . get result ( ) ; if ( response . is exception ( ) ) { exception response er = ( exception response ) response ; service local exception ( er . get exception ( ) ) ; } else { dequeue counter . increment and get ( ) ; local broker . oneway ( new message ack ( md @$ message ack . standard_ack_type @$ __num__ ) ) ; } } catch ( io exception e ) { service local exception ( e ) ; } } } ; remote broker . async request ( message @$ callback ) ;,broker received,success,pre
note param.always on top will be null if the user has n't <PLACE_HOLDER> a preference yet,get check box always on top ( ) . set selected ( param . get always on top ( ) != boolean . false ) ; get check box in scope only ( ) . set selected ( param . is in scope only ( ) ) ; get button mode ( ) . set selected index ( param . get button mode ( ) - __num__ ) ;,user provided,fail,pre
if we have a callback to call @$ schedule it on a different thread . who knows what this thread is <PLACE_HOLDER> .,if ( callback != null ) { queue buffer . executor . submit ( new callable < void > ( ) { public void call ( ) throws exception { callback . on success ( result ) ; return null ; } } ) ; },thread doing,success,pre
notify the target <PLACE_HOLDER> listeners .,notify ( event @$ true ) ; if ( event . is stopped ( ) ) return event . is cancelled ( ) ;,target capture,success,pre
no @$ we do n't <PLACE_HOLDER> a code set,if ( code set == __num__ ) { switch ( new code set ) { case code_code_a : pattern index = code_start_a ; break ; case code_code_b : pattern index = code_start_b ; break ; default : pattern index = code_start_c ; break ; } } else { pattern index = new code set ; },no have,success,pre
the work spec <PLACE_HOLDER> constraints . once the execution of the worker is complete @$ we might need to disable constraint proxies which were previously enabled for this work spec . hence @$ trigger a constraints changed command .,if ( m has constraints ) { intent intent = command handler . create constraints changed intent ( m context ) ; m dispatcher . post on main thread ( new system alarm dispatcher . add runnable ( m dispatcher @$ intent @$ m start id ) ) ; },spec has,fail,pre
did some instances <PLACE_HOLDER> their configuration ?,if ( last known instance configs . size ( ) != current relevant instance configs . size ( ) ) { logger . info ( __str__ @$ table name @$ last known instance configs . size ( ) @$ current relevant instance configs . size ( ) ) ; return true ; },instances change,fail,pre
if there is no container acl but the user can still access the container @$ the only possibility is that the user has the admin role . in this case @$ the user should <PLACE_HOLDER> 0700 mode to the swift container .,if ( mode == __num__ && m access . get token ( ) != null ) { mode = ( short ) __num__ ; } m account mode = mode ;,user set,fail,pre
this ensures that incremental compilation only <PLACE_HOLDER> the root that 's been swapped out .,typed scope scope = t . get typed scope ( ) ; if ( ! scope . is block scope ( ) && ! scope . is module scope ( ) ) { infer scope ( t . get current node ( ) @$ scope ) ; },compilation affects,fail,pre
now <PLACE_HOLDER> the values in the row !,for ( int i = __num__ ; i < row meta . size ( ) ; i ++ ) { value meta interface v = row meta . get value meta ( i ) ; object object = data [ i ] ; try { set value ( ps @$ v @$ object @$ i + __num__ ) ; } catch ( kettle database exception e ) { throw new kettle database exception ( __str__ + row meta @$ e ) ; } },now set,success,pre
skip if any aggregation <PLACE_HOLDER> a order by,if ( node . has orderings ( ) ) { return context . default rewrite ( node @$ optional . empty ( ) ) ; },aggregation contains,success,pre
connectivity service should have <PLACE_HOLDER> the wake on lan supported to true,wifi lp . set wake on lan supported ( true ) ; assert equals ( wifi lp @$ m service . get active link properties ( ) ) ;,service enabled,fail,pre
response <PLACE_HOLDER> gateway receiver,if ( cluster member != null ) { cluster . gateway receiver gateway receiver = cluster member . get gateway receiver ( ) ; boolean is gateway = false ; if ( gateway receiver != null ) { responsejson . put ( __str__ @$ true ) ; responsejson . put ( __str__ @$ gateway receiver . get listening port ( ) ) ; responsejson . put ( __str__ @$ gateway receiver . get link throughput ( ) ) ; responsejson . put ( __str__ @$ gateway receiver . get avg batch processing time ( ) ) ; } else { responsejson . put ( __str__ @$ false ) ; } cluster . gateway sender [ ] gateway senders = cluster member . get member gateway senders ( ),response object,fail,pre
the week <PLACE_HOLDER> a year boundary .,time temp = new time ( this ) ; temp . month day += s thursday offset [ week day ] ; temp . normalize ( true ) ;,week contains,fail,pre
process schema <PLACE_HOLDER> schemas first @$ so that unnamed classes will belong to the defining class instead of the current class,final list < named data schema > includes = schema . get include ( ) ; for ( named data schema included schema : includes ) { process schema ( included schema @$ null @$ null ) ; } final map < custom info spec @$ object > custom info map = new identity hash map < custom info spec @$ object > ( schema . get fields ( ) . size ( ) * __num__ ) ; for ( record data schema . field field : schema . get fields ( ) ) { final class template spec field class = process schema ( field . get type ( ) @$ record class @$ field . get name ( ) ) ; final record template spec .,schema include,fail,pre
note that test xml ca n't check if they are same because enabling dynamic regions <PLACE_HOLDER> a meta region to be produced .,test xml ( cache @$ false ) ; assert equals ( true @$ dynamic region factory . get ( ) . is open ( ) ) ; assert equals ( f . get absolute file ( ) @$ dynamic region factory . get ( ) . get config ( ) . get disk dir ( ) ) ; region dr = get cache ( ) . get region ( __str__ ) ; if ( dr != null ) { dr . local destroy region ( ) ; },note causes,success,pre
` <PLACE_HOLDER> & & whatever ` never returns <PLACE_HOLDER> itself @$ so it is safe . ` whatever & & <PLACE_HOLDER> ` may return <PLACE_HOLDER> @$ so return outer context .,return first ? link ( parent @$ true ) : this ; case or :,returns match,fail,pre
check can not <PLACE_HOLDER> tables that already exist .,response = client . post ( namespace path3 @$ null headers @$ new byte [ ] { } ) ; assert equals ( __num__ @$ response . get code ( ) ) ; response = client . post ( namespace path4 @$ constants . mimetype_protobuf @$ model4 . create protobuf output ( ) ) ; assert equals ( __num__ @$ response . get code ( ) ) ;,check create,fail,pre
existing <PLACE_HOLDER> id against the caller 's valid list .,if ( tbl == null ) { return null ; } m database mdb = null ; string cat name = tbl . is set cat name ( ) ? tbl . get cat name ( ) : get default catalog ( conf ) ; try { mdb = getm database ( cat name @$ tbl . get db name ( ) ) ; } catch ( no such object exception e ) { log . error ( __str__ @$ e ) ; throw new invalid object exception ( __str__ + database name . get qualified ( cat name @$ tbl . get db name ( ) ) + __str__ ) ; },existing checked,fail,pre
transformation must <PLACE_HOLDER> the order,assert equals ( a @$ and1 . get children ( ) . get ( __num__ ) ) ; assert equals ( c @$ and1 . get children ( ) . get ( __num__ ) ) ; final group element and2 = ( group element ) parent . get children ( ) . get ( __num__ ) ; assert equals ( b @$ and2 . get children ( ) . get ( __num__ ) ) ; assert equals ( c @$ and2 . get children ( ) . get ( __num__ ) ) ;,transformation preserve,fail,pre
do n't send an error response here @$ unlike the base authentication filter implementation . this request did not <PLACE_HOLDER> kerberos auth . instead @$ we will send an error response in pre response authorization check filter to allow other authenticator implementations to check the request .,if ( authentication ex == null ) { filter chain . do filter ( request @$ response ) ; } else { http response . send error ( err code @$ authentication ex . get message ( ) ) ; },request require,fail,pre
change resolutions and check that the selected date is not lost and that the calendar <PLACE_HOLDER> the correct resolution .,click ( resolution hour ) ; check header and body ( date time resolution . hour @$ false ) ; click ( resolution year ) ; check header and body ( date time resolution . year @$ false ) ; click ( resolution minute ) ; check header and body ( date time resolution . minute @$ false ) ;,calendar has,success,pre
print <PLACE_HOLDER> entries that are set using the entry processor above .,print cache entries ( cache ) ;,print outs,success,pre
verify exit status <PLACE_HOLDER> exit state of script,assert . assert equals ( exit code @$ container status . get exit status ( ) ) ;,status matches,success,pre
no apps can <PLACE_HOLDER> dnd,assert equals ( __num__ @$ m helper . get apps bypassing dnd count ( user ) ) ;,apps bypass,success,pre
app <PLACE_HOLDER> a visible activity ; only upgrade adjustment .,if ( adj > process list . visible_app_adj ) { adj = process list . visible_app_adj ; app . adj type = __str__ ; if ( debug_oom_adj_reason || log uid == app uid ) { report oom adj message locked ( tag_oom_adj @$ __str__ + app ) ; } } if ( proc state > process state cur top ) { proc state = process state cur top ; app . adj type = __str__ ; if ( debug_oom_adj_reason || log uid == app uid ) { report oom adj message locked ( tag_oom_adj @$ __str__ + app ) ; } } if ( sched group < process list . sched_group_default ) { sched group = process list . sched_group_default ; } app . cached = false ;,app has,success,pre
this method might <PLACE_HOLDER> the file on disk . use segment lock to prevent race condition,lock segment lock = segment locks . get segment lock ( table name with type @$ segment name ) ; try { segment lock . lock ( ) ; final file segment dir = new file ( _fetcher and loader . get segment local directory ( table name with type @$ segment name ) ) ; if ( segment dir . exists ( ) ) { file utils . delete quietly ( segment dir ) ; _logger . info ( __str__ @$ segment dir ) ; } } catch ( final exception e ) { _logger . error ( __str__ + segment name + __str__ + e . get message ( ) @$ e ) ; utils . rethrow exception ( e ) ; } finally { segment,method modify,success,pre
verify that the archive rule <PLACE_HOLDER> the correct deps : the object files from our sources .,rule . get native linkable input ( cxx platform @$ linker . linkable dep type . static @$ graph builder @$ unconfigured target configuration . instance ) ; build rule static rule = graph builder . get rule ( cxx description enhancer . create static library build target ( target @$ cxx platform . get flavor ( ) @$ pic type . pdc ) ) ; assert not null ( static rule ) ; assert equals ( immutable set . of ( cxx source rule factorypdc . create compile build target ( __str__ ) @$ cxx source rule factorypdc . create compile build target ( gen source name ) ) @$ static rule . get build deps ( ) . stream ( ) . map ( build rule,rule has,success,pre
node 1 still <PLACE_HOLDER> node 2 in readers map .,assert true ( e1 . readers ( ) . contains ( n2 . id ( ) ) ) ; assert not null ( cache1 . get and put ( __num__ @$ __str__ ) ) ; final grid dht cache entry e1f = e1 ; grid test utils . wait for condition ( new grid abs predicate ( ) { @ override public boolean apply ( ) { try { return ! e1f . readers ( ) . contains ( n2 . id ( ) ) ; } catch ( grid cache entry removed exception ignored ) { return true ; } catch ( exception e ) { throw new runtime exception ( e ) ; } } } @$ __num__ ) ;,node has,success,pre
update the key.current <PLACE_HOLDER> . first create tmp <PLACE_HOLDER> and do move of tmp to current so that the operation is atomic .,path tmp symlink = tmp symlink location ( ) ; path target of symlink = construct blob with version file name ( base dir @$ get key ( ) @$ version ) ; log . debug ( __str__ @$ tmp symlink @$ target of symlink ) ; files . create symbolic link ( tmp symlink @$ target of symlink ) ; path current sym link = get current symlink path ( ) ; files . move ( tmp symlink @$ current sym link @$ atomic_move ) ;,tmp symlink,success,pre
swap in our scheme if the filtered fs is <PLACE_HOLDER> a different scheme,if ( swap scheme != null ) { try { fq path = new path ( new uri ( swap scheme @$ fq path . to uri ( ) . get scheme specific part ( ) @$ null ) ) ; } catch ( uri syntax exception e ) { throw new illegal argument exception ( e ) ; } },fs using,success,pre
while still holding the lock <PLACE_HOLDER> the timer by transitioning . this simulates a race where the callback goes to <PLACE_HOLDER> the timer while the timer is trying to run .,ees . become active ( ) ;,lock invalidate,fail,pre
otherwise @$ adjust <PLACE_HOLDER> state .,cancel caption animator ( ) ; if ( enabled ) { helper text view = new app compat text view ( context ) ; helper text view . set id ( r . id . textinput_helper_text ) ; if ( version . sdk_int >= __num__ ) { helper text view . set text alignment ( view . text_alignment_view_start ) ; } if ( typeface != null ) { helper text view . set typeface ( typeface ) ; } helper text view . set visibility ( view . invisible ) ; view compat . set accessibility live region ( helper text view @$ view compat . accessibility_live_region_polite ) ; set helper text appearance ( helper text text appearance ) ; set helper text view text color ( helper,adjust closed,fail,pre
it was whitespace @$ try again . the assumption is that it must be a word start if the last one <PLACE_HOLDER> whitespace following it .,word position = words . next ( ) ; if ( word position != break iterator . done ) { offs = line start + word position - seg . offset ; if ( offs != line end ) { return offs ; } } segment cache . release shared segment ( seg ) ; return break iterator . done ;,one had,success,pre
check that method under test <PLACE_HOLDER> npe,try { epki . get key spec ( ( key ) null @$ __str__ ) ; fail ( get name ( ) + __str__ ) ; } catch ( null pointer exception ok ) { },method throws,success,pre
the id lists must <PLACE_HOLDER> the same size,this . operatori ds . add ( operatorid . from job vertexid ( this . id ) ) ; this . operator ids alternatives . add ( null ) ;,lists have,success,pre
both <PLACE_HOLDER> the same client id @$ so there 's no lock conflict . not necessarily ideal @$ but how the system currently works .,utils . io result ( future reader1 ) ; utils . io result ( future reader2 ) ; dlm0 . close ( ) ; dlm1 . close ( ) ;,both have,fail,pre
the user 1 queue should <PLACE_HOLDER> the configurations from the root queue,fs leaf queue user queue = scheduler . get queue manager ( ) . get leaf queue ( __str__ @$ true ) ; assert equals ( __num__ @$ user queue . get num runnable apps ( ) ) ; assert equals ( __num__ @$ user queue . get min share preemption timeout ( ) ) ; assert equals ( __num__ @$ user queue . get fair share preemption timeout ( ) ) ; assert equals ( __num__ @$ user queue . get fair share preemption threshold ( ) @$ __num__ ) ;,queue inherit,success,pre
make sure the thread <PLACE_HOLDER> the correct state,assert true ( thread success . get ( ) ) ;,thread has,fail,pre
the 2 i indexes have to be stored in the appositely <PLACE_HOLDER> strong consistent scans bucket type : this however has to be done only if the user actually <PLACE_HOLDER> it ! so @$ if the latter does n't exist @$ then the scan transactions will not be performed at all .,if ( strong consistency && ! strong consistent scans bucket type . is empty ( ) ) { bucket type2i = strong consistent scans bucket type ; perform strong consistent scans = true ; } else { bucket type2i = bucket type ; perform strong consistent scans = false ; },user defined,fail,pre
set character <PLACE_HOLDER> info for this section,curr section core map . set ( core annotations . character offset begin annotation . class @$ section start token . get ( core annotations . character offset begin annotation . class ) ) ;,character offset,success,pre
time of creation we use a <PLACE_HOLDER> privileged with the access control context that was in place when this was created .,if ( acc == null && system . get security manager ( ) != null ) { throw new security exception ( __str__ ) ; } return access controller . do privileged ( new privileged action < object > ( ) { public object run ( ) { try { class < ? > c ; object cl ; if ( table == null || ! ( ( cl = table . get ( __str__ ) ) instanceof class loader ) ) { cl = thread . current thread ( ) . get context class loader ( ) ; if ( cl == null ) { cl = class loader . get system class loader ( ) ; } } reflect util . check package access ( class,a do,success,pre
skeleton <PLACE_HOLDER> unmarshal exception if it does not recognize the operation number ; this is consistent with the case of an unrecognized method hash .,p . pln ( __str__ + id unmarshal exception + __str__ ) ; p . p oln ( __str__ ) ;,skeleton throws,success,pre
clearing focus <PLACE_HOLDER> the dialog to commit any pending changes @$ e.g . typed text in a number picker .,m time picker . clear focus ( ) ; dismiss ( ) ;,focus forces,success,pre
full recurse into class hierarchy @$ should <PLACE_HOLDER> all available transformers @$ so all internal maps and lists are transformed too,return immutable list . of ( pojoizer . convert to pojo ( rclass . prop1 ) ) ;,recurse use,fail,pre
if we are currently performing a binary search on the input @$ do n't forward the results currently this value is set when a query is optimized using a compact index . the map reduce job responsible for scanning and filtering the index <PLACE_HOLDER> this value . it remains set throughout the binary search executed by the hive binary search record resder until a,if ( io context . is binary searching ( ) ) { return ; } boolean ret = ( boolean ) condition inspector . get primitive java object ( condition ) ; if ( boolean . true . equals ( ret ) ) { forward ( row @$ row inspector ) ; },job disables,fail,pre
if member does not exists defined for this region then <PLACE_HOLDER> a member,if ( null == member ) { member = new cluster . member ( ) ; member . set name ( member name ) ; cluster . get membersh map ( ) . put ( member name @$ member ) ; },region create,success,pre
client authorization required exception is caught by o auth 2 authorization request redirect web filter which <PLACE_HOLDER> authorization,if ( authorization grant type . authorization_code . equals ( context . get client registration ( ) . get authorization grant type ( ) ) && context . get authorized client ( ) == null ) { return mono . error ( ( ) -> new client authorization required exception ( context . get client registration ( ) . get registration id ( ) ) ) ; },which initiates,success,pre
check whether the broadcast inputs <PLACE_HOLDER> the same plan candidate at the branching point,for ( int i = __num__ ; i < broadcast channels combination . size ( ) ; i ++ ) { named channel nc = broadcast channels combination . get ( i ) ; plan node bc source = nc . get source ( ) ; if ( ! are branch compatible ( bc source @$ input source ) ) { valid combination = false ; break ; } for ( int k = __num__ ; k < i ; k ++ ) { plan node other bc source = broadcast channels combination . get ( k ) . get source ( ) ; if ( ! are branch compatible ( bc source @$ other bc source ) ) { valid combination = false ; break ; },inputs use,success,pre
negative ty value <PLACE_HOLDER> an upward movement so subtracting ty <PLACE_HOLDER> expanding the panel .,set clamped panel height ( m initial panel height - ty ) ; request update ( ) ;,ty means,success,pre
the use of 'flat map ' at notification sender does not <PLACE_HOLDER> actual entities order . use typed qname map to assert regardless of ordering .,final map < string @$ referenceable > entities = message . get entities ( ) . stream ( ) . collect ( collectors . to map ( ref -> atlas utils . to typed qualified name ( ref . get type name ( ) @$ ( string ) ref . get ( attr_qualified_name ) ) @$ ref -> ref ) ) ; boolean has flow path seen = false ; for ( int i = __num__ ; i < expects . length ; i ++ ) { final referenceable expect = expects [ i ] ; final string type name = expect . get type name ( ) ; final referenceable actual = entities . get ( atlas utils . to typed qualified name ( type name @$,use alter,fail,pre
user <PLACE_HOLDER> a to transfer token to b,byte string asset account dev = public methed . query account ( dev001 address @$ blocking stub full ) . get asset issuedid ( ) ; byte string fake token id = byte string . copy from utf8 ( long . to string ( long . value of ( asset account dev . to string utf8 ( ) ) + __num__ ) ) ; string param = __str__ + fake token id . to string utf8 ( ) + __str__ ; final string trigger txid = public methed . trigger contract ( transfer token contract address @$ __str__ @$ param @$ false @$ __num__ @$ __num__ @$ __str__ @$ __num__ @$ dev001 address @$ dev001 key @$ blocking stub full ) ; public methed . wait produce next,user trigger,success,pre
we load the url from the tab rather than directly from the content view so the tab <PLACE_HOLDER> a chance of using a prerenderer page is any .,int load type = native load url ( m native tab android @$ params . get url ( ) @$ params . get verbatim headers ( ) @$ params . get post data ( ) @$ params . get transition type ( ) @$ params . get referrer ( ) != null ? params . get referrer ( ) . get url ( ) : null @$ params . get referrer ( ) != null ? params . get referrer ( ) . get policy ( ) : __num__ @$ params . get is renderer initiated ( ) @$ params . get should replace current entry ( ) @$ params . get intent received timestamp ( ) @$ params . get has user gesture ( ) ) ;,tab has,success,pre
synchronization provided by controller gauge to make sure that only one thread <PLACE_HOLDER> the gauge,_controller metrics . set value of table gauge ( table name with type @$ controller gauge . number_of_replicas @$ n replicas external ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . percent_of_replicas @$ ( n replicas ideal max > __num__ ) ? ( n replicas external * __num__ / n replicas ideal max ) : __num__ ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . segments_in_error_state @$ n errors ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . percent_segments_available @$ ( n segments > __num__ ) ? ( __num__ - ( n offline * __num__ / n segments,thread manipulates,fail,pre
this is solely for testing . it checks if the test has <PLACE_HOLDER> the looped value to false @$ and if so remembers that and then <PLACE_HOLDER>s it to true at the end . we have to check here first to make sure we go through a complete iteration of the loop before re<PLACE_HOLDER>ting it .,do { boolean set looped = ! looped . get ( ) ; txn store . mutexapi . lock handle handle = null ; long started at = - __num__ ; try { handle = txn handler . get mutexapi ( ) . acquire lock ( txn store . mutex_key . cleaner . name ( ) ) ; started at = system . current time millis ( ) ; long min open txn id = txn handler . find min open txn id ( ) ; for ( compaction info compaction info : txn handler . find ready to clean ( ) ) { clean ( compaction info @$ min open txn id ) ; } } catch ( throwable t ) { log . error ( __str__,test set,success,pre
step 3 : lookup mounted <PLACE_HOLDER> systems,byte [ ] dir = path . as byte array ( ) ; for ( unix mount entry entry : fs . get mount entries ( ) ) { if ( arrays . equals ( dir @$ entry . dir ( ) ) ) return entry ; } throw new io exception ( __str__ ) ;,mounted file,success,pre
unrecognized attributes do not <PLACE_HOLDER> an exception,return ;,attributes cause,success,pre
if the user did n't <PLACE_HOLDER> a ser de @$ we use the default .,string ser de class name ; if ( get ser name ( ) == null ) { if ( storage handler == null ) { ser de class name = plan utils . get default ser de ( ) . get name ( ) ; log . info ( __str__ + ser de class name + __str__ + table name ) ; } else { ser de class name = storage handler . get ser de class ( ) . get name ( ) ; log . info ( __str__ + ser de class name + __str__ + table name ) ; } } else { ser de class name = get ser name ( ) ; ddl utils . validate ser de ( ser de class name,user specify,success,pre
system arraycopy does the boundary checks <PLACE_HOLDER> @$ no need to check extra,system . arraycopy ( this . memory @$ index @$ dst @$ offset @$ length ) ;,checks anyways,success,pre
open jdk 8 does not <PLACE_HOLDER> iv parameter spec for gcm .,system . out . println ( __str__ + ex . to string ( ) ) ; return ;,jdk support,success,pre
evaluate batch 1 so that temporary arrays in the expression <PLACE_HOLDER> residual values to interfere in later computation,or expr . evaluate ( batch1 ) ;,arrays have,success,pre
the timestamp given by golden gate does not <PLACE_HOLDER> nanoseconds accuracy needed by oracle timestamp,string corrected timestamp = time stamp . append ( __str__ ) . to string ( ) ;,timestamp have,success,pre
we check only permission because igfs client <PLACE_HOLDER> username and group name explicitly .,assert equals ( props . get ( igfs utils . prop_permission ) @$ igfs . info ( dir ) . properties ( ) . get ( igfs utils . prop_permission ) ) ;,client adds,success,pre
set the values which <PLACE_HOLDER> no defaults and compare,set annotation member value ( impl1 @$ __str__ @$ annotation . boolean1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . byte1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . short1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . char1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . int1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . long1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . float1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . double1 ( ) ) ;,which have,success,pre
the test is complete when the client <PLACE_HOLDER> the other object instance .,if ( object == server test object . get other object ( ) ) stop end points ( ) ;,client returns,fail,pre
path stack <PLACE_HOLDER> top of stack at front,if ( path stack . is empty ( ) ) throw new config exception . bug or broken ( __str__ ) ; else return new path ( path stack . descending iterator ( ) ) ;,stack has,success,pre
foo <PLACE_HOLDER> the position and length of ` super `,node super dotg replacement = prototype . get first child ( ) ; assert node ( super dotg replacement ) . matches qualified name ( __str__ ) . has equal source info to ( source super ) ;,foo has,success,pre
multiply by 2 because each <PLACE_HOLDER> a value @$ and map.entry,int expected size = ( map results . size ( ) + duplicates ) * __num__ ; assert equals ( __str__ + joiner . on ( __str__ ) . join ( other matches ) @$ expected size @$ other matches size ) ; assert true ( entry set match ) ; assert true ( entry set javax match ) ; assert true ( map provider match ) ; assert true ( map javax provider match ) ; assert true ( collection of providers of entry of provider match ) ; assert true ( collection of javax providers of entry of provider match ) ; assert equals ( allow duplicates @$ map set match ) ; assert equals ( allow duplicates @$ map set provider match ) ; assert,each has,success,pre
the above method will <PLACE_HOLDER> the new as long as the user does not cancel the request,if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . open_policy_file ) == __num__ ) { tool dialog td = new tool dialog ( policy tool . get message ( __str__ ) @$ tool @$ tw @$ true ) ; td . display user save ( tool dialog . open ) ; } else if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . save_policy_file ) == __num__ ) { string filename = ( ( j text field ) tw . get component ( tool window . mw_filename_textfield ) ) . get text ( ) ; if ( filename == null || filename . length ( ) ==,method open,fail,pre
create a native linkable dep and have it list the fake <PLACE_HOLDER> rule above as a link time dependency .,native linkable input native linkable input = native linkable input . of ( immutable list . of ( source path arg . of ( fake build rule . get source path to output ( ) ) ) @$ immutable set . of ( ) @$ immutable set . of ( ) ) ; fake native linkable group native linkable = create native linkable ( __str__ @$ native linkable input @$ native linkable input ) ;,fake build,success,pre
check that ot ns ordering is not <PLACE_HOLDER> serialization,ksession = serialization helper . get serialised stateful knowledge session ( ksession @$ true ) ; ksession . fire all rules ( ) ;,ns repeating,fail,pre
no requirement to <PLACE_HOLDER> an update from the distributed cache anymore .,just elected primary node = false ;,requirement generate,fail,pre
connection acquisition <PLACE_HOLDER> more than 0 ms in a real system,stub data source . set connection acquistion time ( connection acquisition time ms ) ; final executor service thread pool = new fixed thread pool ( thread count ) ; final count down latch all threads done = new count down latch ( iterations ) ; for ( int i = __num__ ; i < iterations ; i ++ ) { thread pool . submit ( ( ) -> { if ( ref . get ( ) == null ) { quietly sleep ( rest time ms ) ; try ( connection c2 = ds . get connection ( ) ) { quietly sleep ( work time ms ) ; } catch ( exception e ) { ref . set ( e ) ; } } all threads,acquisition takes,success,pre
method does not <PLACE_HOLDER> an exception .,verify read xml returns expected signatures ( __str__ @$ package parser . signing details . signature scheme version . unknown ) ;,method throw,success,pre
otherwise @$ the user <PLACE_HOLDER> specific test targets to build and run @$ so build a graph around these .,log . debug ( __str__ @$ get arguments ( ) ) ; target graph creation result = params . get parser ( ) . build target graph without top level configuration targets ( parsing context @$ parse arguments as target node specs ( params . get cell ( ) @$ params . get client working dir ( ) @$ get arguments ( ) @$ params . get buck config ( ) ) @$ params . get target configuration ( ) ) ; explicit build targets = target graph creation result . get build targets ( ) ; log . debug ( __str__ @$ explicit build targets ) ; immutable set . builder < build target > test targets builder = immutable set . builder ( ) ; for,user provided,fail,pre
this all could probably be done more elegantly via a group extracted from a more comprehensive regexp . <PLACE_HOLDER> up any extra spaces around the remainder of the line @$ which should be a view name .,return statement . substring ( matcher . end ( ) ) . trim ( ) ;,elegantly clean,success,pre
both columns <PLACE_HOLDER> same data type @$ return any one of them,return expected data type ;,columns have,success,pre
user has <PLACE_HOLDER> a file @$ we can continue ...,try { file object file object = kettlevfs . get file object ( vfs filename @$ this ) ; if ( ! ( file object instanceof local file ) ) { throw new kettle exception ( __str__ + vfs filename + __str__ ) ; } string real filename = kettlevfs . get filename ( file object ) ; file file = new file ( real filename ) ; if ( ( file . exists ( ) && file . can read ( ) ) || is local infile ( ) == false ) { if ( log . is detailed ( ) ) { log detailed ( __str__ + real filename + __str__ ) ; } if ( connection != null ) { database db = new database,user specified,success,pre
the first vertex <PLACE_HOLDER> a duplicate id from a vertex in the graph and should not be added to the new graph,vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; graph = graph . add vertices ( vertices ) ; data set < vertex < long @$ long > > data = graph . get vertices ( ) ; list < vertex < long @$ long > > result = data . collect ( ) ; expected result = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; compare result as tuples ( result @$ expected result ) ;,vertex has,success,pre
find out if the class which <PLACE_HOLDER> this field reference has access to the class which declares the public or protected field .,if ( source class == ctx class ) { class definition declarer = field . get class definition ( ) ; if ( declarer . is package private ( ) && ! declarer . get name ( ) . get qualifier ( ) . equals ( source class . get name ( ) . get qualifier ( ) ) ) { field = member definition . make proxy member ( field @$ clazz @$ env ) ; } },which contains,success,pre
if the path <PLACE_HOLDER> match @$ then pass on to the subclass implementation for specific checks :,if ( path matches ( path pattern @$ requesturi ) ) { if ( log . is trace enabled ( ) ) { log . trace ( __str__ + path pattern + __str__ + encode . for html ( requesturi ) + __str__ + __str__ ) ; } return filter chain manager . proxy ( original chain @$ path pattern ) ; },path matches,fail,pre
check if the node <PLACE_HOLDER> the bucket passed as filter,region function context impl rfc = ( region function context impl ) context ; partitioned region pr = ( partitioned region ) rfc . get data set ( ) ; int [ ] bucketi ds = rfc . get local bucket array ( pr ) ; pr . get gem fire cache ( ) . get logger ( ) . fine ( __str__ + bucketi ds ) ; result sender < integer > rs = context . < integer > get result sender ( ) ; if ( ! pr . get data store ( ) . are all buckets hosted ( bucketi ds ) ) { throw new assertion error ( __str__ + bucketi ds + __str__ ) ; } else { for ( int i =,node contains,success,pre
base connector config <PLACE_HOLDER> 13 fields @$ connector 's configs add 2 @$ and 2 producer overrides,assert equals ( __num__ @$ result . values ( ) . size ( ) ) ; assert true ( result . values ( ) . stream ( ) . any match ( config info -> ack config key . equals ( config info . config value ( ) . name ( ) ) && ! config info . config value ( ) . errors ( ) . is empty ( ) ) ) ; assert true ( result . values ( ) . stream ( ) . any match ( config info -> sasl config key . equals ( config info . config value ( ) . name ( ) ) && config info . config value ( ) . errors ( ) . is empty ( ),config has,success,pre
check that large slices <PLACE_HOLDER> something sane,string tree = __str__ ; check tree ( tree ) ;,slices generate,fail,pre
clear the current suggestions as the server response always includes the new ones . exception is when filtering @$ then we need to retain the value if the user does not <PLACE_HOLDER> any of the options matching the filter .,if ( ! widget . waiting for filtering response ) { widget . current suggestion = null ; widget . suggestion popup . menu . clear items ( ) ; popup open and cleared = widget . suggestion popup . is attached ( ) ; },user select,success,pre
the specified network connection is not available . <PLACE_HOLDER> error message .,web view my web view = find view by id ( r . id . webview ) ; my web view . load data ( get resources ( ) . get string ( r . string . connection_error ) @$ __str__ @$ null ) ;,network show,fail,pre
free hash <PLACE_HOLDER> memory @$ but not release back to memory manager,mutable object iterator < tuple2 < binary row @$ binary row > > iterator = sorter . getkv iterator ( ) ; tuple2 < binary row @$ binary row > kv ; while ( ( kv = iterator . next ( ) ) != null ) { binary row key = kv . f0 ; binary row value = kv . f1 ; fallback input . replace ( key @$ value ) ; if ( last key == null ) { last key = key . copy ( ) ; agg sum is null = true ; agg sum = - __num__ ; } else if ( key . get size in bytes ( ) != last key . get size in bytes ( ) || ! (,hash allocated,fail,pre
a match with both pos and word labeled should <PLACE_HOLDER> both attributes on the same node,string pattern = __str__ ;,match have,success,pre
dexopts have to match exactly since aspect only <PLACE_HOLDER> archives for listed ones,return normalize dexopts ( iterables . filter ( tokenized dexopts @$ predicates . in ( get android config ( rule context ) . get dexopts supported in incremental dexing ( ) ) ) ) ;,aspect generates,fail,pre
it will show all its tiles . in this case @$ the tiles have to be entered before the container is measured . any change in the tiles @$ should <PLACE_HOLDER> a remeasure .,final int num tiles = m records . size ( ) ; final int width = measure spec . get size ( width measure spec ) ; final int available width = width - get padding start ( ) - get padding end ( ) ; final int height mode = measure spec . get mode ( height measure spec ) ; if ( height mode == measure spec . unspecified ) { m rows = ( num tiles + m columns - __num__ ) / m columns ; } m cell width = ( available width - m side padding * __num__ - ( m cell margin horizontal * m columns ) ) / m columns ;,case cause,fail,pre
simple lv hangul simple lvt hangul lvtt @$ last jamo <PLACE_HOLDER> for search llvvvtt @$ every jamo <PLACE_HOLDER> for search 0 x ac 01 as conjoining jamo 0 x ac 01 as compatibility jamo 0 x ac 0 f as conjoining jamo ; last <PLACE_HOLDER> for search 0 x afff as conjoining jamo ; all expand for search small letter ae @$ <PLACE_HOLDER> small,string tsce text = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ;,lvtt expands,success,pre
is true if either the resources <PLACE_HOLDER> increased or execution type updated from opportunistic to guaranteed,boolean is increase = false ; if ( ! current resource . equals ( target resource ) ) { is resource change = true ; is increase = resources . fits in ( current resource @$ target resource ) && ! resources . fits in ( target resource @$ current resource ) ; } else if ( ! current exec type . equals ( target exec type ) ) { is exec type update = true ; is increase = current exec type == execution type . opportunistic && target exec type == execution type . guaranteed ; } if ( is increase ) { org . apache . hadoop . yarn . api . records . container increased container = null ; if ( is resource change,resources get,fail,pre
note that this might miss collisions @$ <PLACE_HOLDER> internal tick callback to check for collision on every tick . see internal tick test on how to implement it .,contacts . clear ( ) ;,collision use,success,pre
do n't send the <PLACE_HOLDER>d db name @$ as this method will <PLACE_HOLDER> itself .,ret = get_partitions_ps_with_auth ( db_name @$ tbl_name @$ part_vals @$ max_parts @$ null @$ null ) ; ret = filter utils . filter partitions if enabled ( is server filter enabled @$ filter hook @$ ret ) ; end function ( __str__ @$ ret != null @$ ex @$ tbl_name ) ;,method initialize,fail,pre
otherwise @$ we return the generic placeholder of this library @$ that dependents can use get the real <PLACE_HOLDER> rules via querying the action graph .,prebuilt cxx library paths paths = get paths ( build target @$ args ) ; return new prebuilt cxx library ( build target @$ project filesystem @$ params ) { private final transitive cxx preprocessor input cache transitive cxx preprocessor input cache = new transitive cxx preprocessor input cache ( this ) ; cxx deps all exported deps = cxx deps . builder ( ) . add deps ( args . get exported deps ( ) ) . add platform deps ( args . get exported platform deps ( ) ) . build ( ) ; private boolean has headers ( cxx platform cxx platform ) { if ( ! args . get exported headers ( ) . is empty ( ) ) { return true ; },real build,success,pre
pure symbol reference : the data <PLACE_HOLDER> the symbol 's address @$ load it,if ( data info . is symbol reference ( ) ) { register result register = as register ( result ) ; a arch64 address address = a arch64 address . create scaled immediate address ( result register @$ __num__ ) ; masm . adrp ldr ( __num__ @$ result register @$ address ) ; crb . compilation result . record data patch ( before @$ new c global data reference ( data info ) ) ; } else { a arch64 address address = masm . get placeholder ( before ) ; masm . load address ( as register ( result ) @$ address @$ __num__ ) ; crb . compilation result . record data patch ( before @$ new c global data reference ( data info,data contains,success,pre
previous call to read text file has already <PLACE_HOLDER> the text to draw,if ( text to use != file_text ) { set text to draw ( text @$ range @$ user @$ null ) ; } set font params ( name @$ size @$ style @$ transform ) ; set transformg2 ( g2transform ) ;,call set,success,pre
a special native server to <PLACE_HOLDER> basic sanity,if ( mode != - __num__ ) { if ( ns ( - __num__ ) . wait for ( ) != __num__ ) { proc . d ( __str__ ) ; mode = - __num__ ; } },server check,success,pre
cache size is 3 @$ so 4 th access should <PLACE_HOLDER> first function,assert equals ( __num__ @$ evicted from cache . size ( ) ) ;,access exhaust,fail,pre
set default <PLACE_HOLDER> flag to cluster only if it is true .,if ( dflt enable val ) baseline auto adjust enabled . propagate ( dflt enable val ) ;,default enabled,fail,pre
if we are in the middle of <PLACE_HOLDER>ing process do not fire events @$ will do it later when the method <PLACE_HOLDER> and login finishes its work,if ( err != null && err . get condition ( ) == stream error . condition . conflict ) { synchronized ( connect and login lock ) { if ( in connect and login ) { event during login = new registration state change event ( protocol provider service jabber impl . this @$ get registration state ( ) @$ registration state . unregistered @$ registration state change event . reason_multiple_logins @$ __str__ ) ; return ; } } fire registration state changed ( get registration state ( ) @$ registration state . unregistered @$ registration state change event . reason_multiple_logins @$ __str__ ) ; disconnect and clean connection ( ) ; return ; },method connect,success,pre
the handler <PLACE_HOLDER> a slice of the slice and is now done with it .,dup2 . release ( ) ;,handler created,success,pre
black color @$ <PLACE_HOLDER> a lightbulb off :,if ( color . equals ( color . black ) ) { set icon ( image utilities . load image icon ( __str__ @$ false ) ) ; } else { set icon ( new color icon ( ) ) ; },color show,success,pre
no averagers <PLACE_HOLDER> anything . all buckets must be empty . skip this row .,return null ;,averagers does,fail,pre
any exceptions thrown inside an event listener will not <PLACE_HOLDER> propagation of the event,timber . w ( e @$ __str__ ) ;,exceptions stop,success,pre
legacy application listeners only <PLACE_HOLDER> one file at a time,for ( final file file : files ) { final application event ae = new application event ( toolkit . get default toolkit ( ) @$ file . get absolute path ( ) ) ; send event to each listener until handled ( ae @$ new event dispatcher ( ) { public void dispatch event ( final application listener listener ) { listener . handle open file ( ae ) ; } } ) ; },listeners open,fail,pre
else : mtp <PLACE_HOLDER> the object as part of cleanup . do n't send an event .,break ; default : return false ;,mtp leaves,fail,pre
class.for name and class.get constructor are supposed to never return null though a broken class loader could <PLACE_HOLDER> the contract . just in case we introduce this specific catch to avoid polluting the logs with np es .,logger . log ( warning @$ string . format ( __str__ @$ pm class name ) ) ; logger . log ( warning @$ string . format ( __str__ @$ pm class name ) ) ; logger . log ( warning @$ string . format ( __str__ @$ pm class name ) @$ e ) ;,loader follow,fail,pre
second cancellation should n't <PLACE_HOLDER> additional callbacks,server stream . cancel ( status ) ; do ping pong ( server listener ) ;,cancellation trigger,success,pre
create a display which only <PLACE_HOLDER> 2 stacks .,final activity display display = add new activity display at ( activity display . position_top ) ; final activity stack stack1 = create fullscreen stack with simple activity at ( display ) ; final activity stack stack2 = create fullscreen stack with simple activity at ( display ) ;,which contains,success,pre
assume that this zone <PLACE_HOLDER> dst,if ( tz . getn transitions ( ) > __num__ ) { tz . setdst type ( timezone . x_dst ) ; long time = time . get local time ( max year @$ month . january @$ __num__ @$ __num__ ) ; time -= zrec . get gmt offset ( ) ; tz . add transition ( time @$ tz . get offset index ( gmt offset ) @$ tz . get dst offset index ( __num__ ) ) ; used zone = true ; } else { tz . setdst type ( timezone . no_dst ) ; },zone has,fail,pre
add to all instant messaging operation sets the message listener which handles all <PLACE_HOLDER> messages .,im . add message listener ( this ) ;,all received,success,pre
have the nn redundancy monitor compute the reco<PLACE_HOLDER>truction and invalidation commands to send d <PLACE_HOLDER> every second .,conf . set int ( dfs config keys . dfs_namenode_redundancy_interval_seconds_key @$ __num__ ) ;,d ns,success,pre
if no identity was found then the user does not <PLACE_HOLDER> a reserved room nickname,return null ;,user have,success,pre
serialized version 0 progressive actions did not <PLACE_HOLDER> keys .,boolean is legacy progressive = version == __num__ && download request . type_progressive . equals ( type ) ; list < stream key > keys = new array list < > ( ) ; if ( ! is legacy progressive ) { int key count = input . read int ( ) ; for ( int i = __num__ ; i < key count ; i ++ ) { keys . add ( read key ( type @$ version @$ input ) ) ; } },version contain,success,pre
update once at end of iteration to reduce heap <PLACE_HOLDER> traffic,cursor = i ; last ret = i - __num__ ; if ( mod count != expected mod count ) throw new concurrent modification exception ( ) ;,heap write,success,pre
add key as int here as to have the importer <PLACE_HOLDER> the token id,while ( store property cursor . next ( ) ) { visitor . property ( store property cursor . property key ( ) @$ store property cursor . property value ( ) . as object ( ) ) ; },importer remember,fail,pre
do n't include auto <PLACE_HOLDER> tasks that are finished or finishing .,if ( tr . auto remove recents && tr . get top activity ( ) == null ) { if ( debug_recents ) { slog . d ( tag_recents @$ __str__ + tr ) ; } continue ; },auto remove,success,pre
this loop may <PLACE_HOLDER> an io exception,while ( true ) { int bytes read = as . read ( loaded audio @$ loaded audio byte length @$ int len - loaded audio byte length ) ; if ( bytes read <= __num__ ) { as . close ( ) ; break ; } loaded audio byte length += bytes read ; },loop throw,success,pre
randomly choose the app directory the chance of picking a directory is proportional to the available space on the directory . firstly <PLACE_HOLDER> the sum of all available space on these directories,for ( string local dir : local dirs ) { path cur base = get application dir ( new path ( local dir ) @$ user @$ app id ) ; long space = __num__ ; try { space = get disk free space ( cur base ) ; } catch ( io exception e ) { log . warn ( __str__ @$ cur base @$ e ) ; } available on disk [ i ++ ] = space ; total available += space ; },chance get,fail,pre
for binary <PLACE_HOLDER> the response op code,if ( this . protocol == protocol . binary ) { reply . rewind ( ) ; reply . put ( position_opcode @$ buffer . get ( position_opcode ) ) ; reply . put int ( position_opaque @$ buffer . get int ( position_opaque ) ) ; if ( connection handler . get logger ( ) . finer enabled ( ) ) { connection handler . get logger ( ) . finer ( __str__ + reply + __str__ + command . bufferto string ( reply ) ) ; } } socket channel channel = this . socket . get channel ( ) ; if ( channel == null || ! channel . is open ( ) ) { throw new illegal state exception ( __str__ ) ; },binary add,fail,pre
now reload the reloadable top class and introduce that field i which will <PLACE_HOLDER> the one in the supertype,r top . load new version ( __str__ @$ retrieve rename ( top @$ top + __str__ ) ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; run on instance ( c top @$ i top @$ __str__ @$ __num__ ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; is mgr fa = get field accessor ( i top ) ; assert contains ( __str__ @$ fa . to string ( ) ) ; assert does,which shadow,success,pre
in the absence of sorted by clause @$ the sorted dynamic partition insert should <PLACE_HOLDER> the ordering of records provided by order by in select statement,reduce sink operator parentrs op = operator utils . find single operator upstream ( parent @$ reduce sink operator . class ) ; if ( parentrs op != null && parse ctx . get query properties ( ) . has outer order by ( ) ) { string parentrs op order = parentrs op . get conf ( ) . get order ( ) ; string parentrs op null order = parentrs op . get conf ( ) . get null order ( ) ; if ( parentrs op order != null && ! parentrs op order . is empty ( ) && sort positions . is empty ( ) ) { key cols . add all ( parentrs op . get conf ( ) . get key,insert change,fail,pre
b is maxed out on capacity @$ so this move should <PLACE_HOLDER> the session,future = wm . apply move session async ( sessiona3 @$ __str__ ) ; assert not null ( future . get ( ) ) ; assert false ( future . get ( ) ) ; wm . add test event ( ) . get ( ) ; while ( sessiona3 . is open ( ) ) { thread . sleep ( __num__ ) ; } assert null ( sessiona3 . get pool name ( ) ) ; assert equals ( __str__ @$ sessiona3 . get reason for kill ( ) ) ; assert equals ( __num__ @$ all session providers . get ( __str__ ) . get sessions ( ) . size ( ) ) ; assert equals ( __num__ @$ all session providers . get ( __str__,move close,fail,pre
does this observer <PLACE_HOLDER> the target user ?,if ( target user handle == user handle . user_all || entry . user handle == user handle . user_all || target user handle == entry . user handle ) { if ( leaf ) { if ( ( flags & content resolver . notify_skip_notify_for_descendants ) != __num__ && entry . notify for descendants ) { if ( debug ) slog . d ( tag @$ __str__ + entry . observer + __str__ ) ; continue ; } } else { if ( ! entry . notify for descendants ) { if ( debug ) slog . d ( tag @$ __str__ + entry . observer + __str__ ) ; continue ; } } if ( debug ) slog . d ( tag @$ __str__ + entry .,observer allow,fail,pre
if neither are present then <PLACE_HOLDER> empty string in parameter slot,final constant pool gen cpg = class gen . get constant pool ( ) ; final instruction list il = method gen . get instruction list ( ) ; il . append ( new push ( cpg @$ constants . emptystring ) ) ;,neither store,success,pre
the second stream will <PLACE_HOLDER> the first picker,verify ( picker @$ timeout ( __num__ ) ) . pick subchannel ( args2 ) ;,stream see,success,pre
check if this string does not <PLACE_HOLDER> any uppercase characters .,if ( lowercased ) { return this ; } final byte [ ] new value = platform dependent . allocate uninitialized array ( length ( ) ) ; for ( i = __num__ @$ j = array offset ( ) ; i < new value . length ; ++ i @$ ++ j ) { new value [ i ] = to lower case ( value [ j ] ) ; } return new ascii string ( new value @$ false ) ;,string contain,success,pre
if the declaring class of the field <PLACE_HOLDER> missing classes a ` no class def found error ` can be thrown . we intrinsify ` it here .,method intrinsic = get intrinsic ( analysis @$ hosted @$ b @$ exception synthesizer . throw no class def found error method ) ; if ( intrinsic == null ) { return false ; } throw no class def found error ( b @$ target method @$ e . get message ( ) ) ;,class references,success,pre
stack allocation <PLACE_HOLDER> an allocation size that is a compile time constant @$ so we split the byte array up in multiple chunks and write them separately .,final int chunk size = __num__ ; final c char pointer bytes = stack value . get ( chunk size ) ; int chunk offset = offset ; int input length = length ; while ( input length > __num__ ) { int chunk length = math . min ( input length @$ chunk size ) ; for ( int i = __num__ ; i < chunk length ; i ++ ) { int index = chunk offset + i ; byte b ; if ( value instanceof string ) { b = ( byte ) char at ( ( string ) value @$ index ) ; } else if ( value instanceof char [ ] ) { b = ( byte ) ( ( char [ ],allocation has,fail,pre
skip if this consumer already <PLACE_HOLDER> all the topic partitions it can get,if ( consumer partition count == all subscriptions . get ( consumer ) . size ( ) ) continue ;,consumer has,success,pre
leader migration has <PLACE_HOLDER> the leader @$ send the request to the new leader,m_mailbox . send ( new leader @$ counter . get open message ( ) ) ;,migration mapped,fail,pre
other topics should not <PLACE_HOLDER> exception @$ but they should clear existing exception,metadata . update ( metadata response @$ time . milliseconds ( ) ) ; metadata . maybe throw exception for topic ( __str__ ) ; metadata . maybe throw any exception ( ) ;,topics throw,success,pre
<PLACE_HOLDER> recognition locked can possibly <PLACE_HOLDER> the list of models,array list < model data > model datas = new array list < model data > ( m model data map . values ( ) ) ; for ( model data model data : model datas ) { update recognition locked ( model data @$ is allowed @$ notify ) ; },recognition update,success,pre
create a dummy <PLACE_HOLDER> to <PLACE_HOLDER> all columns,gen select plan ( pctx @$ map join op ) ; return map join op ;,dummy join,fail,pre
32 mb default <PLACE_HOLDER> index in memory,kahadb persistence adapter . set index cache size ( __num__ ) ; kahadb persistence adapter . set index write batch size ( __num__ ) ; kahadb persistence adapter . set enable index recovery file ( false ) ; kahadb persistence adapter . set enable index disk syncs ( false ) ; broker . add connector ( __str__ ) ; broker . start ( ) ; string options = __str__ ; connection factory = new activemq connection factory ( broker . get transport connectors ( ) . get ( __num__ ) . get connect uri ( ) + options ) ;,default allocate,fail,pre
if the user mouses around the neutral zone @$ then start the close timer . the timer will be reset if the user <PLACE_HOLDER> the popup .,close timer . start ( ) ;,user clicks,fail,pre
hash aggregates and partial aggregates <PLACE_HOLDER> the index ordering . so @$ we will need an orderby node .,if ( number hash aggregates > __num__ ) { return true ; } else if ( number window functions == __num__ ) { if ( index use . get window function uses index ( ) == window function scoreboard . no_index_use ) { return true ; } else { assert ( index use . get window function uses index ( ) == window function scoreboard . statement_level_order_by_index ) ; return number receive nodes > __num__ ; } } else if ( number window functions == __num__ ) { return ! ( index use . get window function uses index ( ) == __num__ && index use . is window function compatible with order by ( ) ) ; } else { return true ; },aggregates maintain,fail,pre
check if graphics card does n't <PLACE_HOLDER> depth textures,if ( img . get format ( ) . is depth format ( ) && ! caps . contains ( caps . depth texture ) ) { throw new renderer exception ( __str__ ) ; } if ( target == gl . gl_texture_cube_map ) { int cube size = limits . get ( limits . cubemap size ) ; if ( img . get width ( ) > cube size || img . get height ( ) > cube size ) { throw new renderer exception ( __str__ + img + __str__ + cube size ) ; } if ( img . get width ( ) != img . get height ( ) ) { throw new renderer exception ( __str__ ) ; } } else { int,card support,success,pre
pattern <PLACE_HOLDER> more and match,if ( pp < plen && ( s . char at ( ps ) == p . char at ( pp ) || p . char at ( pp ) == __str__ ) ) { ++ pp ; ++ ps ; } else if ( pp < plen && p . char at ( pp ) == __str__ ) { old ps = ps ; old pp = pp ; met star = true ; ++ pp ; } else { if ( met star ) { ++ old ps ; ps = old ps ; pp = old pp ; ++ pp ; } else { return false ; } },pattern has,success,pre
no actions @$ hide the row @$ <PLACE_HOLDER> out the view,if ( m actions == null ) { m action row . set visibility ( view . gone ) ; m current view . set slice actions ( null ) ; m current view . set insets ( get padding start ( ) @$ get padding top ( ) @$ get padding end ( ) @$ get padding bottom ( ) ) ; return ; },actions fill,fail,pre
vector should <PLACE_HOLDER> rawname value pairs,int i = __num__ ; while ( i < annotation local attrs . size ( ) ) { string rawname = ( string ) annotation local attrs . element at ( i ++ ) ; int colon index = rawname . index of ( __str__ ) ; string prefix @$ localpart ; if ( colon index == - __num__ ) { prefix = __str__ ; localpart = rawname ; } else { prefix = rawname . substring ( __num__ @$ colon index ) ; localpart = rawname . substring ( colon index + __num__ ) ; } string uri = schema doc . f namespace support . geturi ( f symbol table . add symbol ( prefix ) ) ; local str buffer . append ( rawname ),vector contain,success,pre
validate that every inherited interface must <PLACE_HOLDER> pipeline options except for pipeline options itself .,validate inherited interfaces extend pipeline options ( iface ) ; @ suppress warnings ( __str__ ) set < class < ? extends pipeline options > > combined pipeline options interfaces = fluent iterable . from ( validated pipeline options interfaces ) . append ( iface ) . to set ( ) ;,interface extend,success,pre
request did n't succeed because the token was revoked so we invalidate the token stored in the session and render the index page so that the user can <PLACE_HOLDER> the o auth flow again,if ( res2 . failed ( ) ) { ctx . session ( ) . destroy ( ) ; ctx . fail ( res2 . cause ( ) ) ; } else { user info . put ( __str__ @$ res2 . result ( ) . json array ( ) ) ; json object data = new json object ( ) . put ( __str__ @$ user info ) ; engine . render ( data @$ __str__ @$ res3 -> { if ( res3 . succeeded ( ) ) { ctx . response ( ) . put header ( __str__ @$ __str__ ) . end ( res3 . result ( ) ) ; } else { ctx . fail ( res3 . cause ( ) ) ; },user start,success,pre
run the timing method first <PLACE_HOLDER> up the method to make sure it gets compiled,if ( argv [ __num__ ] . equals ( __str__ ) ) { for ( int i = __num__ ; i < warmup_loops ; i ++ ) { timeit ( __num__ @$ __num__ @$ __num__ ) ; } system . out . println ( __str__ ) ; timeit ( timing_trials @$ get_timing_loops @$ set_timing_loops ) ; } else if ( argv [ __num__ ] . equals ( __str__ ) ) { max_milliseconds = stress_milliseconds ; } else { throw new runtime exception ( __str__ + argv [ __num__ ] + __str__ ) ; },method wake,fail,pre
test suite <PLACE_HOLDER> standalone wal iterator to verify pds content .,grid test utils . add test if needed ( suite @$ ignite wal reader test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds exchange during checkpoint test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds reserve wal segments test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds reserve wal segments with compaction test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite wal replaying after restart test . class @$ ignored tests ) ;,suite uses,success,pre
check that active primary <PLACE_HOLDER> a newer version so that peer recovery works,if ( primary != null ) { return is version compatible allocating replica ( allocation . routing nodes ( ) @$ primary . current node id ( ) @$ node @$ allocation ) ; } else { return allocation . decision ( decision . yes @$ name @$ __str__ ) ; },primary has,success,pre
create data with 2 chunks : the 2 nd chunk <PLACE_HOLDER> half of the block size,long first chunk length = block_size * blocks per chunk ; long second chunk length = block_size / __num__ ; dfs test util . create file ( fs @$ src data @$ buffer len @$ first chunk length @$ block_size @$ repl factor @$ src seed ) ; dfs test util . append file new block ( ( distributed file system ) fs @$ src data @$ ( int ) second chunk length ) ; dfs test util . create file ( fs @$ new path ( target base + filename + __str__ + first chunk length ) @$ buffer len @$ first chunk length @$ block_size @$ repl factor @$ dst seed ) ; dfs test util . create file ( fs @$ new path ( target,chunk holds,fail,pre
if a component specifies the file with a bad font @$ the corresponding slot will be initialized by default physical font . in such case <PLACE_HOLDER> font 2 d may return composite font which can not be casted to physical font .,if ( ! component names [ slot ] . equals ignore case ( name ) ) { try { components [ slot ] = ( physical font ) fm . find font2d ( component names [ slot ] @$ style @$ font manager . physical_fallback ) ; } catch ( class cast exception cce ) { components [ slot ] = fm . get default physical font ( ) ; } },slot find,success,pre
permission controller <PLACE_HOLDER> default permission granting and role management @$ so it 's a critical part of the core system .,m required permission controller package = get required permission controllerl pr ( ) ;,controller has,fail,pre
verify that both consumers <PLACE_HOLDER> a prefetch of 10,assert equals ( __num__ @$ brokerb . get destination ( advisory topic ) . get consumers ( ) . get ( __num__ ) . get prefetch size ( ) ) ; assert equals ( __num__ @$ brokerb . get destination ( topic1 ) . get consumers ( ) . get ( __num__ ) . get prefetch size ( ) ) ; assert deq inflight ( __num__ @$ __num__ ) ;,consumers have,success,pre
important : first add factory methods ; then constructors @$ so latter can <PLACE_HOLDER> former !,_add factory creators ( ctxt @$ bean desc @$ vchecker @$ intr @$ creators @$ creator defs ) ;,latter override,success,pre
each thread <PLACE_HOLDER> the connection from the existing pool . if no connection is available @$ it requests one and waits for it to be created . but a thread that requests the connection and before it could wait @$ other thread could steal that connection . the connections in progress tries to avoid these edge cases by not requesting a connection from the,if ( connections in progress . get ( ) == __num__ ) { resource = non blocking get ( key @$ pool ) ; } if ( resource == null ) { connections in progress . increment and get ( ) ; try { attempt grow ( key @$ this . object factory @$ pool ) ; resource = non blocking get ( key @$ pool ) ; } finally { connections in progress . decrement and get ( ) ; } } return resource ;,thread gets,success,pre
let the fragment <PLACE_HOLDER> the back button if it implements our on parent back pressed listener,fragment fragment = m bottom nav . get active fragment ( ) ; if ( fragment instanceof on activity back pressed listener ) { boolean handled = ( ( on activity back pressed listener ) fragment ) . on activity back pressed ( ) ; if ( handled ) { return ; } } if ( is task root ( ) && device utils . get instance ( ) . is chromebook ( this ) ) { return ; },fragment handle,success,pre
write them to disk so that configuration.get password will <PLACE_HOLDER> them,provider . flush ( ) ;,password read,fail,pre
this maybe not take effect @$ when not every consume queue has <PLACE_HOLDER> file .,if ( is ext addr ( tags code ) ) { max ext addr = tags code ; },queue extend,success,pre
default display should <PLACE_HOLDER> system decors,assert true ( m target . should show system decors locked ( m primary display ) ) ;,display show,success,pre
this is the last split in the list @$ the filters <PLACE_HOLDER> the range from the previous split to the current split and also the current split to the end,if ( i == split keys . size ( ) - __num__ ) { range filter = string . format ( __str__ + __str__ @$ lowest bound @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; range filter = string . format ( __str__ @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; } else { range filter = string . format ( __str__ + __str__ @$ lowest bound @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; },filters define,success,pre
if we did n't find an existing policy <PLACE_HOLDER> a new one,if ( found policy == null ) { final string uuid seed = resource + action ; final access policy . builder builder = new access policy . builder ( ) . identifier generate from seed ( uuid seed ) . resource ( resource ) . add user ( user identifier ) ; if ( action . equals ( read_code ) ) { builder . action ( request action . read ) ; } else if ( action . equals ( write_code ) ) { builder . action ( request action . write ) ; } else { throw new illegal state exception ( __str__ + action ) ; } final access policy access policy = builder . build ( ) ; final policy jaxb policy = createjaxb,policy create,success,pre
the current group has <PLACE_HOLDER> all its rows,if ( current group position == current group size ) { memory size in bytes -= current group size in bytes ; current group position = __num__ ; current rows = next grouped rows ( ) ; continue ; },group published,fail,pre
now resource should <PLACE_HOLDER> 0 permit .,assert . assert equals ( __num__ @$ lr . sem . available permits ( ) ) ;,resource have,success,pre
region <PLACE_HOLDER> will <PLACE_HOLDER> dls and manual free resources only,if ( dlock service == null ) { dlock service = d lock service . create ( get full path ( ) @$ get system ( ) @$ true @$ false @$ false ) ; },destroy destroy,success,pre
let 's also <PLACE_HOLDER> back the result rows ...,node result rows node = xml handler . get sub node ( node @$ xml_rows_tag ) ; list < node > result nodes = xml handler . get nodes ( result rows node @$ row meta . xml_data_tag ) ; if ( ! result nodes . is empty ( ) ) { row meta row meta = new row meta ( xml handler . get sub node ( result rows node @$ row meta . xml_meta_tag ) ) ; for ( node result node : result nodes ) { object [ ] row data = row meta . get row ( result node ) ; rows . add ( new row meta and data ( row meta @$ row data ) ) ; } },'s send,fail,pre
the number is just to <PLACE_HOLDER> the origin of the fieldname,set < string > key set = fields . key set ( ) ; list < string > entries = new array list < string > ( key set ) ; string [ ] field names = entries . to array ( new string [ entries . size ( ) ] ) ; const . sort strings ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ;,number maintain,fail,pre
check that the jstl bundle is not already included in the pattern @$ and include it if it is not beca<PLACE_HOLDER> subsequent classes such as os gi web inf configuration <PLACE_HOLDER> this pattern to determine which jars are considered to be on the container classpath,if ( jstl bundle != null ) { if ( pattern == null ) { pattern = pattern . compile ( jstl bundle . get symbolic name ( ) ) ; deployment manager . set context attribute ( os gi web inf configuration . container_bundle_pattern @$ jstl bundle . get symbolic name ( ) ) ; } else if ( ! ( pattern . matcher ( jstl bundle . get symbolic name ( ) ) . matches ( ) ) ) { string s = tmp + __str__ + jstl bundle . get symbolic name ( ) ; pattern = pattern . compile ( s ) ; deployment manager . set context attribute ( os gi web inf configuration . container_bundle_pattern @$ s ) ; } } for,classes use,success,pre
the object that we 're modifying here is a copy of the original ! so let 's <PLACE_HOLDER> the filename from relative to absolute by grabbing the file object ... in case the name of the file comes from previous steps @$ forget about this !,try { if ( ! accepting filenames ) { for ( int i = __num__ ; i < file name . length ; i ++ ) { file object file object = kettlevfs . get file object ( space . environment substitute ( file name [ i ] ) @$ space ) ; file name [ i ] = resource naming interface . name resource ( file object @$ space @$ utils . is empty ( file mask [ i ] ) ) ; } } return null ; } catch ( exception e ) { throw new kettle exception ( e ) ; },let change,success,pre
check whether the path <PLACE_HOLDER> an existing leaf node .,for ( string part : parts ) { if ( ! create new branch && node != root && node . children . is empty ( ) ) { return this ; } if ( node . children . contains key ( part ) ) { node = node . children . get ( part ) ; } else { create new branch = true ; node tmp = new node ( ) ; node . children . put ( part @$ tmp ) ; node = tmp ; } },path contains,fail,pre
grouping shuffle reader <PLACE_HOLDER> value in empty windows . for now @$ we count the element at least once to keep the current counter behavior .,if ( windows size == __num__ ) { element count . add value ( __num__ ) ; } else { element count . add value ( windows size ) ; },reader adds,fail,pre
length protocol version authentication result code server host <PLACE_HOLDER> connection <PLACE_HOLDER>,int offset = __num__ + __num__ + __num__ + __num__ + __num__ ;,host peer,fail,pre
unlock <PLACE_HOLDER> profile with unified lock,for ( user info profile : m user manager . get profiles ( user id ) ) { if ( tied managed profile ready to unlock ( profile ) ) { try { unlock child profile ( profile . id @$ false @$ challenge type @$ challenge @$ reset lockouts ) ; } catch ( remote exception e ) { log . d ( tag @$ __str__ @$ e ) ; } } if ( ! already unlocked ) { long ident = clear calling identity ( ) ; try { maybe show encryption notification for user ( profile . id ) ; } finally { restore calling identity ( ident ) ; } } },unlock managed,success,pre
every language <PLACE_HOLDER> itself :,assert true ( new german ( ) . equals consider variants if specified ( new german ( ) ) ) ; assert true ( new germany german ( ) . equals consider variants if specified ( new germany german ( ) ) ) ; assert true ( new english ( ) . equals consider variants if specified ( new english ( ) ) ) ; assert true ( new american english ( ) . equals consider variants if specified ( new american english ( ) ) ) ;,language replaces,fail,pre
now we have a list . let ' <PLACE_HOLDER> this list .,m upload store . register post model ( post @$ media list ) ;,' populate,fail,pre
actual and values <PLACE_HOLDER> the same elements but are they in the same order ?,if ( ! diff . differences found ( ) ) { int i = __num__ ; for ( object element from actual : actual ) { if ( ! are equal ( element from actual @$ values [ i ] ) ) { throw failures . failure ( info @$ elements differ at index ( element from actual @$ values [ i ] @$ i @$ comparison strategy ) ) ; } i ++ ; } return ; },actual have,success,pre
a polygon should <PLACE_HOLDER> more than 2 corners,preconditions . check argument ( abscissa . length > __num__ ) ;,polygon have,success,pre
noinspection <PLACE_HOLDER> lint clickable view accessibility,note block holder . m avatar image view . set on touch listener ( m on gravatar touch listener ) ; if ( site id == user id ) { note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_no ) ; } else { note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_yes ) ; } note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_no ) ; note block holder . m avatar image view . set content description ( null ) ; note block holder . m avatar image view . set on click listener ( null ) ;,noinspection android,success,pre
balancer <PLACE_HOLDER> initial response .,lb response observer . on next ( build initial response ( ) ) ;,balancer sends,success,pre
if we have no record of this environment @$ that means the current rule implicitly uses the defaults for this group . so explicitly <PLACE_HOLDER> that group 's defaults into the refined set before trying to remove specific items .,if ( env to prune == null ) { for ( environment with group default env : get defaults ( refined environment to prune @$ dep environments . get refined environments ( ) ) ) { refined environments so far . add ( default env ) ; labels to environments . put ( default env . environment ( ) @$ default env ) ; } env to prune = verify . verify not null ( labels to environments . get ( refined environment to prune ) ) ; },rule put,fail,pre
no space for eo <PLACE_HOLDER> record in the file .,if ( file size < zip_eocd_rec_min_size ) { return null ; },space dfd,fail,pre
closing window should <PLACE_HOLDER> tex field,openw . click ( ) ; text field element text field = $ ( text field element . class ) . first ( ) ; window element window = $ ( window element . class ) . first ( ) ; window . close ( ) ; assert equals ( text field . get wrapped element ( ) @$ get focused element ( ) ) ;,window hide,fail,pre
now <PLACE_HOLDER> a finishable state .,r2 . set can update finishable ( ) ; task wrapper . finishable state updated ( false ) ; task wrapper2 = task executor service . preemption queue . peek ( ) ; assert not null ( task wrapper2 ) ; assert true ( task wrapper . is in preemption queue ( ) ) ; r2 . complete ( ) ; r2 . await end ( ) ; task executor service . shut down ( false ) ;,now set,fail,pre
all plan item instances are created . now <PLACE_HOLDER> them .,command context util . get agenda ( command context ) . plan activate plan item instance operation ( entry dependent plan item instance @$ satisfied criterion . get id ( ) ) ; for ( int i = parent plan item instances to activate . size ( ) - __num__ ; i >= __num__ ; i -- ) { plan item instance entity parent plan item instance = parent plan item instances to activate . get ( i ) ; if ( parent plan item instance == null ) { command context util . get agenda ( command context ) . plan create plan item instance operation ( parent plan item instance ) ; } command context util . get agenda ( command context ) . plan activate,plan create,fail,pre
since we send the empty password to the client @$ if the client has not <PLACE_HOLDER> the password then we do change it,if ( ! string utils . is empty ( user info . get password ( ) ) ) { user . set password ( user info . get password ( ) ) ; } user . set description ( user info . get description ( ) ) ; return user ;,client changed,fail,pre
this hook is for <PLACE_HOLDER> purpose only .,if ( boolean . get boolean ( cli strings . ignore_interceptors ) ) { return result model . create info ( cli strings . shutdown__msg__shutdown_entire_ds ) ; } response response = read yes no ( cli strings . shutdown__msg__warn_user @$ response . yes ) ; if ( response == response . no ) { return result model . create error ( cli strings . shutdown__msg__aborting_shutdown ) ; } else { return result model . create info ( cli strings . shutdown__msg__shutdown_entire_ds ) ; },hook testing,success,pre
so let 's <PLACE_HOLDER> pipe cloner to make that possible,return new pipe cloner impl ( ) . copy ( p ) ;,'s use,fail,pre
note that regular ser de does n't <PLACE_HOLDER> fewer columns .,list < object > deserialized row ; if ( do write fewer columns ) { deserialized row = ( list < object > ) serde_fewer . deserialize ( bytes writable ) ; } else { deserialized row = ( list < object > ) serde . deserialize ( bytes writable ) ; } object [ ] row = rows [ i ] ; for ( int index = __num__ ; index < write column count ; index ++ ) { object expected = row [ index ] ; object object = deserialized row . get ( index ) ; if ( expected == null || object == null ) { if ( expected != null || object != null ) { fail ( __str__ ) ; } },de write,fail,pre
i found that crimson does n't show the proper stack trace when a runtime exception happens inside a schema builder . the following code <PLACE_HOLDER> the actual exception that happened .,if ( e . get cause ( ) instanceof sax exception ) { sax exception se = ( sax exception ) e . get cause ( ) ; if ( se . get exception ( ) != null ) se . get exception ( ) . print stack trace ( ) ; } throw e ;,code shows,success,pre
scale fractional digits @$ dot @$ <PLACE_HOLDER> digits .,final int scale = fast scale ; final boolean is zero fast1 and fast2 = ( fast1 == __num__ && fast2 == __num__ ) ; final boolean is zero fast2 = ( fast2 == __num__ ) ; int lower longword scale = __num__ ; int middle longword scale = __num__ ; int high longword scale = __num__ ; long long word = fast0 ; if ( scale > __num__ ) { lower longword scale = math . min ( scale @$ longword_decimal_digits ) ; for ( int i = __num__ ; i < lower longword scale ; i ++ ) { scratch buffer [ index -- ] = ( byte ) ( byte_digit_zero + long word % __num__ ) ; long word /= __num__ ; } if (,digits inherit,fail,pre
make sure nongreedy mech cut off does n't <PLACE_HOLDER> this alt,lexer grammar lg = new lexer grammar ( __str__ + __str__ + __str__ + __str__ ) ;,mech support,fail,pre
test renaming a source to a path which <PLACE_HOLDER> a file as a directory .,source file = new file ( test_dir @$ __str__ ) ; assert . assert true ( source file . create new file ( ) ) ; file bad target = new file ( target file @$ __str__ ) ; try { nativeio . rename to ( source file @$ bad target ) ; assert . fail ( ) ; } catch ( nativeio exception e ) { if ( path . windows ) { assert . assert equals ( string . format ( __str__ ) @$ e . get message ( ) ) ; } else { assert . assert equals ( errno . enotdir @$ e . get errno ( ) ) ; } },which contains,fail,pre
starting the case instance starts the process task . the process <PLACE_HOLDER> an async job at the beginning,cmmn engine . get cmmn runtime service ( ) . create case instance builder ( ) . case definition key ( __str__ ) . start ( ) ; job job = process engine . get management service ( ) . create job query ( ) . single result ( ) ; assert null ( job . get scope type ( ) ) ; job test helper . wait for job executor to process all jobs ( process engine . get process engine configuration ( ) @$ process engine . get management service ( ) @$ __num__ @$ __num__ ) ;,process triggers,fail,pre
total overlap do n't <PLACE_HOLDER> either one @$ get next range 1 and range 2,range1 = range1 it . has next ( ) ? range1 it . next ( ) : null ; range2 = range2 it . has next ( ) ? range2 it . next ( ) : null ; break ; case range1_starts_at_range2_ends_after_range2 :,overlap affect,fail,pre
nested view <PLACE_HOLDER> scrollable area under this point . let it be handled there .,if ( dy != __num__ && ! is gutter drag vertically ( m last motiony @$ dy ) && can scroll vertically ( this @$ false @$ ( int ) dy @$ ( int ) x @$ ( int ) y ) ) { m last motionx = x ; m last motiony = y ; m is unable to drag = true ; return false ; },view has,success,pre
already exist . only update <PLACE_HOLDER> commands .,connected controller record record = m controller records . get ( saved info ) ; record . allowed commands = commands ;,update allowed,success,pre
break @$ next read will <PLACE_HOLDER> larger buffer .,break ;,read return,fail,pre
make sure test invoice plugin api will <PLACE_HOLDER> an additional tax item,test invoice plugin api . add tax item ( new tax invoice item ( uuid . randomuuid ( ) @$ null @$ null @$ account . get id ( ) @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ new local date ( __num__ @$ __num__ @$ __num__ ) @$ new local date ( __num__ @$ __num__ @$ __num__ ) @$ __str__ @$ big decimal . one @$ account . get currency ( ) @$ null @$ null ) ) ; test invoice plugin api . add tax item ( new tax invoice item ( uuid . randomuuid ( ) @$ null @$ null @$ account . get id ( ) @$ null @$ null @$,api return,success,pre
the touchable region should not <PLACE_HOLDER> the bounds of its container .,apply op to region by bounds ( touchable region @$ this @$ region . op . intersect ) ; final view parent parent = get parent ( ) ; if ( parent != null ) { parent . subtract obscured touchable region ( touchable region @$ this ) ; },region intersect,fail,pre
we know that the parameters <PLACE_HOLDER> the same name @$ since this is what the descriptor 's hash code & equality are based on . the only thing that may be different is the description . and since the proposed parameter does not <PLACE_HOLDER> a description @$ we want to use whatever is currently set .,return old parameter == null ? descriptor : old parameter . get descriptor ( ) ;,parameter have,success,pre
if the comment is <PLACE_HOLDER> approval @$ offer moderation actions,if ( note . get comment status ( ) == comment status . unapproved ) { if ( note . can moderate ( ) ) { add comment approve action for comment notification ( context @$ builder @$ note id ) ; } } else { if ( note . can like ( ) ) { add comment like action for comment notification ( context @$ builder @$ note id ) ; } },comment perform,fail,pre
error getting active data <PLACE_HOLDER> keeperexception,try { mockito . when ( mockzk . get data ( mockito . eq ( zk_lock_name ) @$ mockito . eq ( false ) @$ any ( ) ) ) . then throw ( new keeper exception . auth failed exception ( ) ) ; elector . get active data ( ) ; assert . fail ( __str__ ) ; } catch ( keeper exception . auth failed exception ke ) { mockito . verify ( mockzk @$ mockito . times ( __num__ ) ) . get data ( mockito . eq ( zk_lock_name ) @$ mockito . eq ( false ) @$ any ( ) ) ; },error throws,fail,pre
generating portfolio <PLACE_HOLDER> array to be populated across the pr 's & local regions,portfolio data [ ] portfolio data = create portfolio data ( start_portfolio_data_index @$ total_data_size ) ;,portfolio object,success,pre
count all nodes @$ how many groups each node <PLACE_HOLDER> each,configuration group config = configuration . with batch size ( config @$ neo store . get relationship group store ( ) . get records per page ( ) ) ; stats provider memory usage = new memory usage stats provider ( neo store @$ group cache ) ; execute stage ( new count groups stage ( group config @$ from store @$ group cache @$ memory usage ) ) ; long from node id = __num__ ; while ( from node id < high node id ) { long to node id = group cache . prepare ( from node id ) ; monitor . defragmenting node range ( from node id @$ to node id ) ; execute stage ( new scan and cache groups stage (,groups has,success,pre
owner <PLACE_HOLDER> group and permission,verify set acl ( test_user_1 @$ test_dir_uri @$ null @$ test_user_2 . get group ( ) @$ ( short ) __num__ @$ true ) ; file info file info = m file system master . get file info ( m file system master . get file id ( new alluxiouri ( test_dir_file_uri ) ) ) ; assert equals ( test_user_2 . get group ( ) @$ file info . get group ( ) ) ; assert equals ( ( short ) __num__ @$ file info . get mode ( ) ) ;,owner owns,fail,pre
handle the case where the last call to write actually <PLACE_HOLDER> an error in the log,if ( ( ledger writer . is log segment in error ( ) || force recovery ) && reset on error ) { completable future < void > close future ; if ( ledger writer . is log segment in error ( ) ) { close future = ledger writer . async abort ( ) ; } else { close future = ledger writer . async close ( ) ; } return close future . then compose ( new function < void @$ completion stage < bk log segment writer > > ( ) { @ override public completable future < bk log segment writer > apply ( void result ) { remove cached log writer ( ) ; if ( ledger writer . is log segment in,call had,fail,pre
make sure the state we just read in for columns @$ rows @$ and scrollbar visibility <PLACE_HOLDER> legal values,if ( columns < __num__ ) { columns = __num__ ; } if ( rows < __num__ ) { rows = __num__ ; } if ( ( scrollbar visibility < scrollbars_both ) || ( scrollbar visibility > scrollbars_none ) ) { this . scrollbar visibility = scrollbars_both ; } if ( text area serialized data version < __num__ ) { set focus traversal keys ( keyboard focus manager . forward_traversal_keys @$ forward traversal keys ) ; set focus traversal keys ( keyboard focus manager . backward_traversal_keys @$ backward traversal keys ) ; },state has,success,pre
the line <PLACE_HOLDER> a paragraph boundary,if ( get paragraph index ( start ) != get paragraph index ( limit - __num__ ) ) { throw new illegal argument exception ( ) ; },line overlaps,fail,pre
check that union <PLACE_HOLDER> only a single successor,if ( this . get outgoing connections ( ) . size ( ) > __num__ ) { throw new compiler exception ( __str__ ) ; } boolean children skipped due to replicated input = false ;,union has,success,pre
we compute what the d<PLACE_HOLDER>tance between each cluster and its closest neighbor <PLACE_HOLDER> to set a proportional d<PLACE_HOLDER>tance threshold for points that should be involved in calculating the centroid .,closest cluster distances . clear ( ) ; for ( vector center : centroids ) { vector closest other cluster = centroids . search first ( center @$ true ) . get value ( ) ; closest cluster distances . add ( distance measure . distance ( center @$ closest other cluster ) ) ; },distance is,success,pre
create a media <PLACE_HOLDER> name,string file name = string . format ( __str__ @$ new simple date format ( __str__ ) . format ( new date ( ) ) ) ; if ( type == media_type_image ) { file name = string . format ( __str__ @$ file name ) ; } else if ( type == media_type_video ) { file name = string . format ( __str__ @$ file name ) ; } else { log . e ( tag @$ __str__ + type ) ; return null ; } return new file ( string . format ( __str__ @$ storage dir . get path ( ) @$ file . separator @$ file name ) ) ;,media file,success,pre
reduce to subinterfaces this does not need to be recursive since we make a copy @$ and that copy <PLACE_HOLDER> all super types for the whole hierarchy,for ( ct class intf : alter map . values ( ) ) { ct class [ ] interfaces ; try { interfaces = intf . get interfaces ( ) ; } catch ( not found exception e ) { throw new runtime exception ( e ) ; } for ( ct class c : interfaces ) alter map . remove ( c . get name ( ) ) ; } return alter map ;,copy includes,fail,pre
exception will <PLACE_HOLDER> error,new certificate manager ( key @$ cert ) ;,exception set,success,pre
the glow <PLACE_HOLDER> more on the velocity @$ and therefore starts out nearly invisible .,m glow alpha start = __num__ ; m glow scaley start = math . max ( m glow scaley @$ __num__ ) ;,glow depends,success,pre
find the actual first which <PLACE_HOLDER> the filter .,boolean bool = super . internal first ( ) ; if ( p == null ) { return bool ; } while ( bool ) { if ( p . evaluate ( this ) ) { break ; } bool = super . internal next ( ) ; } return bool ;,which matches,success,pre
initial board <PLACE_HOLDER> a grid like the following in the center : wb bw,int middle row = board . length / __num__ ; int middle column = board [ middle row ] . length / __num__ ; board [ middle row ] [ middle column ] = new piece ( color . white ) ; board [ middle row + __num__ ] [ middle column ] = new piece ( color . black ) ; board [ middle row + __num__ ] [ middle column + __num__ ] = new piece ( color . white ) ; board [ middle row ] [ middle column + __num__ ] = new piece ( color . black ) ; black count = __num__ ; white count = __num__ ;,board has,success,pre
start definitions <PLACE_HOLDER> element,xtw . write start element ( bpmn2_prefix @$ element_definitions @$ bpmn2_namespace ) ; xtw . set default namespace ( bpmn2_namespace ) ; xtw . write default namespace ( bpmn2_namespace ) ; xtw . write namespace ( bpmn2_prefix @$ bpmn2_namespace ) ; xtw . write namespace ( xsi_prefix @$ xsi_namespace ) ; xtw . write namespace ( xsd_prefix @$ schema_namespace ) ; xtw . write namespace ( activiti_extensions_prefix @$ activiti_extensions_namespace ) ; xtw . write namespace ( bpmndi_prefix @$ bpmndi_namespace ) ; xtw . write namespace ( omgdc_prefix @$ omgdc_namespace ) ; xtw . write namespace ( omgdi_prefix @$ omgdi_namespace ) ; for ( string prefix : model . get namespaces ( ) . key set ( ) ) { if ( ! default namespaces . contains ( prefix,definitions root,success,pre
let 's <PLACE_HOLDER> the principal the administration permission @$ without granting access,mutable acl acl first deny = new acl impl ( identity @$ __num__ @$ acl authorization strategy @$ new console audit logger ( ) ) ; acl first deny . insert ace ( __num__ @$ base permission . administration @$ new principal sid ( auth ) @$ false ) ;,'s give,success,pre
workaround : if the abstract value type does not <PLACE_HOLDER> any parent @$ a vector containing value base should be returned instead of an empty vector,if ( v . derived from ( ) . size ( ) == __num__ ) stream . print ( __str__ ) ; else { symtab entry parent ; for ( int i = __num__ ; i < v . derived from ( ) . size ( ) ; i ++ ) { if ( i == __num__ ) stream . print ( __str__ ) ; else stream . print ( __str__ ) ; parent = ( symtab entry ) v . derived from ( ) . element at ( i ) ; stream . print ( util . java name ( parent ) ) ; } },type define,fail,pre
an event <PLACE_HOLDER> multiple filters,event filter dimension map . put ( __str__ @$ lists . new array list ( __str__ ) ) ; filtered events = event filter . apply dimension filter ( all events @$ event filter dimension map ) ; assert . assert equals ( filtered events . size ( ) @$ __num__ ) ; assert . assert equals ( filtered events . get ( __num__ ) . get name ( ) @$ __str__ ) ;,event has,fail,pre
asif : <PLACE_HOLDER> the cartesian of the different group junction results .,compiled value iter operands to send = null ; if ( ! iter operands . is empty ( ) ) { int size = iter operands . size ( ) ; compiled value cv [ ] = new compiled value [ size ] ; for ( int k = __num__ ; k < size ; ++ k ) { cv [ k ] = ( compiled value ) this . iter operands . get ( k ) ; } if ( cv . length == __num__ ) { iter operands to send = cv [ __num__ ] ; } else { iter operands to send = new compiled junction ( cv @$ this . operator ) ; } },asif go,fail,pre
<PLACE_HOLDER> result in this we should try 5 count <PLACE_HOLDER> the result,int count = __num__ ; exception error = null ; while ( count > __num__ ) { if ( command . is cancel ) { if ( command . m listener != null ) command . m listener . on cancel ( ) ; break ; } try { command . m result = i_command . command ( command . m id @$ command . m timeout @$ command . m parameters ) ; if ( command . m listener != null ) command . m listener . on completed ( command . m result ) ; break ; } catch ( exception e ) { error = e ; count -- ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e1,count get,success,pre
like many fully successful operations @$ a single row <PLACE_HOLDER> counts as 2 logical row operations @$ one for locating the row and one for retrieving it .,assert equals ( __num__ @$ m_ee . m_calls fromee ) ; assert equals ( long opthreshold @$ m_ee . m_last tuples accessed ) ; assert true ( __num__ < m_ee . m_curr memory in bytes ) ; assert true ( __num__ > m_ee . m_curr memory in bytes ) ; assert true ( __num__ < m_ee . m_peak memory in bytes ) ; assert true ( __num__ > m_ee . m_peak memory in bytes ) ; assert true ( m_ee . m_peak memory in bytes >= m_ee . m_curr memory in bytes ) ;,row fetch,success,pre
jdbc should <PLACE_HOLDER> strings ...,if ( data instanceof string ) { r . deliver ( data ) ; } else if ( data instanceof long ) { long indexes = ( ( long ) data ) . long value ( ) ; r . deliver ( convert set value ( column @$ indexes @$ options ) ) ; },jdbc return,success,pre
another call to close should not <PLACE_HOLDER> an exception,zis . close ( ) ;,call throw,fail,pre
after the delay the lfo should <PLACE_HOLDER> oscillate let make sure output is accurate enough,double p_step = ( __num__ / control_rate ) * math . exp ( ( freq - __num__ ) * ( math . log ( __num__ ) / __num__ ) ) ; double p = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { p += p_step ; double predicted_output = __num__ + math . sin ( p * __num__ * math . pi ) * __num__ ; if ( math . abs ( predicted_output - lfo_output [ __num__ ] ) > __num__ ) throw new exception ( __str__ + predicted_output + __str__ + lfo_output [ __num__ ] + __str__ ) ; lfo . process control logic ( ) ; },lfo handle,fail,pre
7900040069 ea 030000000000 8300 c 20003006 aea 03005 c 003800223 aab <PLACE_HOLDER> a 393276617 b 41 a 0030 d 506 e 3000414010250 bf 630007000000000000000000000000000000 c 3107209000089050000000000006 bea 03005 c 003800403 aab <PLACE_HOLDER> a 393276617 b 41 a 0030 d 506 e 3000414010250 bf 630007000000000000000000000000000000 c 3107209000089050000000000006 cea 03005 c 0038005 e 3 aab <PLACE_HOLDER> a 393276617 b 41 a 0030 d 506 e 3000414010250,apel protocol decoder decoder = new apel protocol decoder ( null ) ;,aab bf,fail,pre
app : <PLACE_HOLDER> api 10 dev : <PLACE_HOLDER> api 20,verify compute target sdk version ( older_version @$ released @$ true @$ older_version ) ;,app released,success,pre
we used to destroy the buffers on exiting fs mode @$ this is no longer needed since fs change will <PLACE_HOLDER> a surface data replacement,synchronized ( peer ) { exit full screen exclusive ( screen @$ peer ) ; },change generate,fail,pre
the value of call count can <PLACE_HOLDER> 1 only if the callback thread survives the exception thrown by the first callback .,assert true ( call count . get ( ) > __num__ ) ; if ( watcher != null ) { watcher . stop ( ) ; watcher . wait for state ( file change watcher . state . stopped ) ; },value go,fail,pre
if still null then the implementation does n't <PLACE_HOLDER> a presence operation set which is unacceptable for jabber .,if ( op set pers presence2 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ;,implementation offer,success,pre
first preference for stack goes to the task id set in the activity options . <PLACE_HOLDER> the stack associated with that if possible .,if ( task id != invalid_task_id ) { options . set launch task id ( invalid_task_id ) ; final task record task = any task for id ( task id @$ match_task_in_stacks_or_recent_tasks_and_restore @$ options @$ on top ) ; options . set launch task id ( task id ) ; if ( task != null ) { return task . get stack ( ) ; } },preference restore,fail,pre
start from relation type <PLACE_HOLDER> to a role player check the role player 's type <PLACE_HOLDER> to the other role player check two players are different check the role player 's type check the subtypes,assert that ( plan @$ contains in any order ( instance of ( label fragment . class ) @$ instance of ( label fragment . class ) @$ instance of ( label fragment . class ) @$ instance of ( in isa fragment . class ) @$ instance of ( out role player fragment . class ) @$ instance of ( out isa fragment . class ) @$ instance of ( out role player fragment . class ) @$ instance of ( neq fragment . class ) @$ instance of ( out isa fragment . class ) @$ instance of ( out sub fragment . class ) ) ) ;,start go,success,pre
this is the actual creation of the file system . return true <PLACE_HOLDER> a success,return su . execute ( listener @$ root username @$ root password @$ new create ( listener @$ home @$ uid @$ gid @$ user name ) ) ;,system indicates,fail,pre
for some reason in some corner cases nodes are n't having sentence index set do a pass and make sure all nodes <PLACE_HOLDER> sentence index set,semantic graph sg = sentence . get ( semantic graph core annotations . collapsed dependencies annotation . class ) ; if ( sg != null ) { for ( indexed word iw : sg . vertex set ( ) ) { if ( iw . get ( core annotations . sentence index annotation . class ) == null && sentence . get ( core annotations . sentence index annotation . class ) != null ) { iw . set sent index ( sentence . get ( core annotations . sentence index annotation . class ) ) ; } } },nodes have,success,pre
latvian does not <PLACE_HOLDER> upper first,string [ ] data = { __str__ @$ __str__ @$ __str__ @$ __str__ } ; generic locale starter ( new locale ( __str__ @$ __str__ ) @$ data ) ;,latvian make,fail,pre
exclude if this bulletin does n't <PLACE_HOLDER> a source id or if it does n't match,if ( bulletin query . get source id pattern ( ) != null ) { if ( bulletin . get source id ( ) == null || ! bulletin query . get source id pattern ( ) . matcher ( bulletin . get source id ( ) ) . find ( ) ) { return false ; } },bulletin have,success,pre
if 1 <PLACE_HOLDER> sibling @$ return parent,if ( result == null ) { result = node . get parent node ( ) ; return result ; },1 has,fail,pre
assert that all threads <PLACE_HOLDER> back the same reference,for ( future < map < bounded window @$ long > > result : results ) { assert equals ( value @$ result . get ( ) ) ; for ( map . entry < bounded window @$ long > entry : result . get ( ) . entry set ( ) ) { assert same ( value . get ( entry . get key ( ) ) @$ entry . get value ( ) ) ; } },threads got,success,pre
rejoin <PLACE_HOLDER>es not <PLACE_HOLDER> paused mode .,list < string > rejoin cmd ln str = rejoin cmd ln . create command line ( ) ; string cmd line full = __str__ ; for ( string element : rejoin cmd ln str ) { cmd line full += __str__ + element ; } log . info ( cmd line full ) ; m_proc builder . command ( ) . clear ( ) ; m_proc builder . command ( ) . add all ( rejoin cmd ln str ) ; process proc = m_proc builder . start ( ) ; start = system . current time millis ( ) ;,rejoin do,success,pre
make sure we do n't think anyone else is <PLACE_HOLDER> the lock,if ( got ) { if ( blackboard . get is locked ( ) ) { string msg = __str__ + service name + __str__ + object name + __str__ + ( ( d lock service ) service ) . get lock grantor id ( ) + __str__ + service . is lock grantor ( ) ; system . out . println ( __str__ + msg ) ; fail ( msg ) ; } blackboard . set is locked ( true ) ; long count = blackboard . get count ( ) ; system . out . println ( __str__ + count + __str__ + service name + __str__ + object name ) ; thread . sleep ( hold time ) ; blackboard . inc count ( ),anyone holding,success,pre
no service for this class @$ must <PLACE_HOLDER> the configuration by selection,p conf . as node list ( ) . get ( ) . stream ( ) . filter ( this :: not reserved provider key ) . for each ( provider specific conf -> { if ( ! provider specific . compare and set ( null @$ provider specific conf ) ) { throw new security exception ( __str__ + __str__ + provider specific . get ( ) . key ( ) + __str__ + provider specific conf . key ( ) ) ; } } ) ;,service pass,fail,pre
the user has <PLACE_HOLDER> a new node,if ( node != null ) { this . target = node ; if ( node . get start node ( ) != null ) { session session = model . get singleton ( ) . get session ( ) ; list < context > contexts = session . get contexts for node ( node . get start node ( ) ) ; for ( context context : contexts ) { ctx names . add ( context . get name ( ) ) ; } } else if ( node . get context ( ) != null ) { ctx names . add ( node . get context ( ) . get name ( ) ) ; } },user started,fail,pre
invalid time is now since the session <PLACE_HOLDER> the window cutoff time .,expected stats . expiration time elapsed = now ; expected stats . execution time in window ms = __num__ * minute_in_millis ; expected stats . bg job count in window = __num__ ; expected stats . execution time in max period ms = __num__ * minute_in_millis ; expected stats . bg job count in max period = __num__ ; expected stats . session count in window = __num__ ; m quota controller . update execution stats locked ( __num__ @$ __str__ @$ input stats ) ; assert equals ( expected stats @$ input stats ) ; input stats . window size ms = expected stats . window size ms = __num__ * minute_in_millis ;,session straddles,success,pre
exopackage <PLACE_HOLDER> default to jar @$ otherwise @$ default to raw .,dex store default dex store = exopackage mode . enabled for secondary dexes ( exopackage modes ) ? dex store . jar : dex store . raw ; dex split strategy dex split strategy = args . get minimize primary dex size ( ) ? dex split strategy . minimize_primary_dex_size : dex split strategy . maximize_primary_dex_size ; return new dex split mode ( args . get use split dex ( ) @$ dex split strategy @$ args . get dex compression ( ) . or else ( default dex store ) @$ args . get linear alloc hard limit ( ) @$ args . get dex group lib limit ( ) @$ args . get primary dex patterns ( ) @$ args . get primary dex classes file,exopackage enabled,fail,pre
will be null when the feature to <PLACE_HOLDER> minimized bitcode is disabled .,if ( minimized bitcode == null ) { return full bitcode ; } return minimized bitcode ;,feature get,fail,pre
proto source root . this ensures that protos can <PLACE_HOLDER> either the full path or the short path when including other protos .,command line . add all ( vector arg . of ( transitive imports ) . mapped ( new expand import args fn ( output directory @$ direct proto source roots ) ) ) ; if ( protos in direct dependencies != null ) { if ( ! protos in direct dependencies . is empty ( ) ) { command line . add all ( __str__ @$ vector arg . join ( __str__ ) . each ( protos in direct dependencies ) . mapped ( new expand to path fn with imports ( output directory @$ direct proto source roots ) ) ) ; } else { command line . add ( __str__ ) ; } },protos return,fail,pre
start event <PLACE_HOLDER> event,event = ( activiti entity event ) listener . get events received ( ) . get ( __num__ ) ; assert equals ( activiti event type . entity_created @$ event . get type ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get process instance id ( ) ) ; assert not equals ( process instance . get id ( ) @$ event . get execution id ( ) ) ; assert equals ( process instance . get process definition id ( ) @$ event . get process definition id ( ) ) ;,event create,success,pre
if aod is showing @$ the ime should be hidden . however @$ sometimes the aod is considered hidden because it 's in the process of hiding @$ but it 's still being shown on screen . in that case @$ we want to continue hiding the ime until the windows have <PLACE_HOLDER> drawing . this way @$ we know that the ime can,final boolean hide ime = win . is input method window ( ) && ( m aod showing || ! m default display policy . is window manager draw complete ( ) ) ; if ( hide ime ) { return true ; } final boolean show ime over keyguard = ime target != null && ime target . is visible lw ( ) && ( ime target . can show when locked ( ) || ! can be hidden by keyguard lw ( ime target ) ) ;,windows finished,fail,pre
sets the peer <PLACE_HOLDER> signature algorithm to use in km temporarily .,session . set peer supported signature algorithms ( supported sign algs ) ;,sets supported,success,pre
if we 're in a chunk and the item <PLACE_HOLDER> fragments,if ( in block && special chunk ) { in block = false ; block size index = - __num__ ; block size position = - __num__ ; start_block ( ) ; special chunk = false ; },item contains,fail,pre
target address may not <PLACE_HOLDER> connected address,dc . send ( bb @$ p . get socket address ( ) ) ;,address contain,fail,pre
if we have operation set persistent presence skip sending initial presence while login is executed @$ the operation set will <PLACE_HOLDER> care of it,if ( get operation set ( operation set persistent presence . class ) != null ) conf conn . set send presence ( false ) ;,set take,success,pre
weird case : someone has <PLACE_HOLDER> us their own custom i intent sender @$ and now they have someone else trying to send to it but of course this is n't really a pending intent @$ so there is no base intent @$ and the caller is n't supplying an intent ... but we never want to dispatch a null intent to a receiver,if ( intent == null ) { slog . wtf ( tag @$ __str__ ) ; intent = new intent ( intent . action_main ) ; },someone given,success,pre
sep ; ascii ' 4 ' single quotes <PLACE_HOLDER> header ncols,parse setup setup = new parse setup ( xls_info @$ ( byte ) __num__ @$ true @$ parse setup . no_header @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ ctypes @$ null @$ null @$ null ) ;,quotes have,fail,pre
second call must not <PLACE_HOLDER> another message,assert that ( under test . is operational ( ) ) . is false ( ) ; assert that ( memory appender . events ) . extracting ( i logging event :: get level @$ i logging event :: get message ) . contains only once ( tuple ( level . info @$ __str__ ) ) ;,call add,fail,pre
fuzzy k <PLACE_HOLDER> clusterer,throw new unsupported operation exception ( __str__ ) ;,k has,fail,pre
one <PLACE_HOLDER> measure,if ( timeto measure iterations == __num__ ) { long start = system . current time millis ( ) ; ksession . execute ( command factory . new insert elements ( arrays . as list ( data ) ) ) ; system . out . println ( __str__ + message + __str__ + ( system . current time millis ( ) - start ) ) ; assert true ( list . size ( ) > __num__ ) ; } else { long start = system . current time millis ( ) ; long end = start + timeto measure iterations ; int count = __num__ ; while ( system . current time millis ( ) < end ) { stateless kie session sess2 = create stateless knowledge session,one start,fail,pre
tree indexes <PLACE_HOLDER> a 40 byte overhead per row .,isize . width min += tree_map_entry_overhead + tuple_ptr_size ; isize . width max += tree_map_entry_overhead + tuple_ptr_size ;,indexes have,success,pre
then it is an attribute or its not an attribute name nor operation name and the below invocation will <PLACE_HOLDER> a attribute not found exception .,result = do attribute operation ( mbsc @$ instance @$ sub command @$ attribute info ) ;,invocation raise,fail,pre
the following should <PLACE_HOLDER> the same result as the previous @$ since it has the same restrictions applied in reverse order .,s = open session ( ) ; s . get transaction ( ) . begin ( ) ; root criteria = s . create criteria ( order . class @$ __str__ ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; root criteria . create alias ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; result = ( order ) root criteria . unique result ( ) ; assert equals ( order1 . get order id ( ) @$ result . get order id ( ) ) ; assert equals ( __num__ @$ result,following have,success,pre
res stores the first <PLACE_HOLDER> abstract method,method res = null ; for ( method mi : methods ) { if ( ! modifier . is abstract ( mi . get modifiers ( ) ) ) continue ; if ( mi . get annotation ( traits . implemented . class ) != null ) continue ; try { object . class . get method ( mi . get name ( ) @$ mi . get parameter types ( ) ) ; continue ; } catch ( no such method exception e ) { } if ( res != null ) return null ; res = mi ; },stores implemented,fail,pre
only the exception case would have <PLACE_HOLDER> the proxy regions . we can safely proceed here .,repo . put entry in monitoring region map ( member @$ proxy monitoring region ) ; repo . put entry in notif region map ( member @$ proxy notification region ) ; try { if ( ! running ) { return ; } proxy factory . create all proxies ( member @$ proxy monitoring region ) ; management cache listener . mark ready ( ) ; notif listener . mark ready ( ) ; } catch ( exception e ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ e ) ; } throw new management exception ( e ) ; },case created,fail,pre
have one cycle of successful calls to verify valid tracker clients returned . <PLACE_HOLDER> load balancing on this update state @$ need to update state before forcing the strategy .,tracker client resulttc = get tracker client ( strategy @$ request @$ new request context ( ) @$ __num__ @$ clients ) ; strategy . set strategy ( default_partition_id @$ partition degrader load balancer state . strategy . load_balance ) ; resulttc = get tracker client ( strategy @$ request @$ new request context ( ) @$ __num__ @$ clients ) ; assert not null ( resulttc @$ __str__ ) ; for ( int j = __num__ ; j < num_checks ; j ++ ) { cc list . add ( client1 . get call tracker ( ) . start call ( ) ) ; cc list . add ( client2 . get call tracker ( ) . start call ( ) ) ; } clock . add,returned keep,fail,pre
drag & <PLACE_HOLDER> manager,recycler view drag drop manager drag drop manager = new recycler view drag drop manager ( ) ; drag drop manager . set dragging item shadow drawable ( ( nine patch drawable ) context . get resources ( ) . get drawable ( r . drawable . shadow_8dp ) ) ; recycler view . adapter adapter = new quick search adapter ( ) ; adapter . set has stable ids ( true ) ; adapter = drag drop manager . create wrapped adapter ( adapter ) ;,& drop,success,pre
as the annotation is legal on fields and methods only @$ javac itself will <PLACE_HOLDER> care of printing an error message for this .,return ;,itself take,success,pre
wait @$ the client no longer <PLACE_HOLDER> access to the display .,if ( ! m window manager internal . is uid allowed on display ( cs . self reported display id @$ cs . uid ) ) { return input bind result . invalid_display_id ; },client has,success,pre
caution : fist delete tasks then jobs @$ as task <PLACE_HOLDER> a foreign key .,try { int job retention days = monitor task info . get default retention days ( ) ; int deleted tasks = dao_registry . get taskdao ( ) . delete records older than days ( job retention days ) ; int deleted jobs = dao_registry . get jobdao ( ) . delete records older than days ( job retention days ) ; log . info ( __str__ @$ deleted tasks @$ deleted jobs @$ job retention days ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; },tasks has,success,pre
if we are using osvr <PLACE_HOLDER> the eye info here,if ( environment . getvr hardware ( ) instanceof osvr ) { ( ( osvr ) environment . getvr hardware ( ) ) . get eye info ( ) ; },osvr download,fail,pre
so that other persistence context can also <PLACE_HOLDER> new entity,employee = sfsb1 . get employee notx ( __num__ ) ;,context add,fail,pre
function <PLACE_HOLDER> 2 x @$ cancel never,verify ( on next function @$ times ( __num__ ) ) . apply ( any ( ) ) ; verify ( subscription @$ never ( ) ) . cancel ( ) ;,function calls,fail,pre
then the repository <PLACE_HOLDER> a success status and usernames are available,assert false ( usernames repository . is error ( ) ) ; assert true ( usernames repository . get ( ) . length == usernames . length ) ;,repository has,success,pre
find the old generation which <PLACE_HOLDER> low memory detection,list iterator iter = pools . list iterator ( ) ; while ( iter . has next ( ) ) { memory poolmx bean p = ( memory poolmx bean ) iter . next ( ) ; if ( p . get type ( ) == memory type . heap && p . is usage threshold supported ( ) ) { mpool = p ; if ( trace ) { system . out . println ( __str__ + __str__ ) ; memory util . print memory pool ( mpool ) ; } break ; } } test listener listener = new test listener ( ) ; sensor listener l2 = new sensor listener ( ) ; notification emitter emitter = ( notification emitter ) mm ; emitter .,which supports,success,pre
register the chat window menu <PLACE_HOLDER> item .,container filter . put ( container . container_id @$ container . container_chat_menu_bar . getid ( ) ) ; bundle context . register service ( plugin component factory . class . get name ( ) @$ new otr plugin component factory ( container . container_chat_menu_bar ) @$ container filter ) ;,menu bar,success,pre
but if statements and loops <PLACE_HOLDER> blocks automagically .,assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ; assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ; assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ;,statements evaluate,fail,pre
last task to exit when shutdown <PLACE_HOLDER> resources,int remaining = thread exit ( this @$ replace me ) ; if ( remaining == __num__ && is shutdown ( ) ) impl close ( ) ;,shutdown releases,fail,pre
release compute engine startup method which will <PLACE_HOLDER> startup exception,compute engine . release startup ( ) ; while ( ce server . get status ( ) == monitored . status . down ) { },which throw,success,pre
deque <PLACE_HOLDER> a boolean that when true indicates that the value in that position of the window is missing .,return new aggregate function < t @$ integer > ( ) { private final array deque < boolean > queue = new array deque < > ( ) ; private int missing count = __num__ ; @ override public void remove left most ( ) { boolean removed missing value = queue . remove ( ) ; if ( removed missing value ) { missing count -- ; } } @ override public void add right most ( t new value ) { queue . add ( false ) ; } @ override public void add right most missing ( ) { queue . add ( true ) ; missing count ++ ; } @ override public integer get value ( ) { return queue . size (,deque has,fail,pre
if this is a transactional <PLACE_HOLDER> all @$ we will not have version information as it is only generated at commit so treat transactional <PLACE_HOLDER> all as if the server is not versioned . if we have no storage then act as if the server is not versioned .,final boolean server is versioned = proxy result != null && proxy result . region is versioned ( ) && ! istx ( ) && get data policy ( ) . with storage ( ) ; if ( ! server is versioned && ! partial result ) { proxy result = null ; },transactional write,fail,pre
if user typed the 'quit ' command @$ wait until the server <PLACE_HOLDER> the connection .,while ( ( line = command input stream . read line ( ) ) != null ) { if ( __str__ . equals ( line . to lower case ( ) ) ) { log . info ( __str__ ) ; channel . close ( ) ; break ; } message base msg = produce message base on user input ( line @$ ( short ) random . next int ( short . max_value ) ) ; if ( msg == null ) { continue ; } send ( msg ) ; },server closes,success,pre
if the job is <PLACE_HOLDER> shell job @$ the entry point class name is <PLACE_HOLDER> gate way server . otherwise @$ the entry point class of <PLACE_HOLDER> job is <PLACE_HOLDER> driver,if ( entry point class == null ) { entry point class = __str__ ; } if ( jar file path == null ) { throw new illegal argument exception ( __str__ ) ; } jar file = get jar file ( jar file path ) ;,name crashing,fail,pre
for remote output port @$ it 's possible that multiple processors are connected . in that case @$ the received flow file is cloned and passed to each connection . so we need to create multiple data <PLACE_HOLDER> refs .,final list < connection status > connections = nifi flow . get outgoing connections ( port process id ) ; if ( connections == null || connections . is empty ( ) ) { logger . warn ( __str__ @$ new object [ ] { event } ) ; return ; },data point,fail,pre
owid with the given value <PLACE_HOLDER> ! searching now for row id ...,if ( pos >= __num__ ) { key = compressed owids [ pos ] ; if ( row id < row ids [ key . from index ] || row id > row ids [ key . to index - __num__ ] ) { return false ; } if ( arrays . binary search ( row ids @$ key . from index @$ key . to index @$ row id ) >= __num__ ) { return true ; } },owid failed,fail,pre
rather than fight it @$ let root <PLACE_HOLDER> an alias,nodes . put ( __str__ @$ root ) ; nodes . put ( root zookeeper @$ root ) ; root . add child ( proc child zookeeper ) ; nodes . put ( proc zookeeper @$ proc data node ) ; proc data node . add child ( quota child zookeeper ) ; nodes . put ( quota zookeeper @$ quota data node ) ;,root have,success,pre
compressed streams do not <PLACE_HOLDER> position until after sync marker @$ so we can hit eof here .,try { close ( ) ; } catch ( final exception e2 ) { log . warn ( e . get message ( ) @$ e2 ) ; } return false ;,streams clear,fail,pre
means <PLACE_HOLDER> all database @$ so no need to add filter,if ( db name != null && ! __str__ . equals ( db name ) ) { append simple condition ( filter builder @$ __str__ @$ new string [ ] { db name } @$ parameter vals ) ; },means matched,fail,pre
the client must not <PLACE_HOLDER> the trailers .,assert false ( trailers latch . await ( __num__ @$ time unit . seconds ) ) ;,client send,success,pre
comparing with itself <PLACE_HOLDER> 0,assert true ( comp . compare ( p1 @$ p1 ) == __num__ ) ;,itself returns,success,pre
if the flushed requests <PLACE_HOLDER> errors @$ we should propagate it also and fail the checkpoint,check and propagate async error ( ) ;,requests has,success,pre
functions that are n't ctors or interfaces <PLACE_HOLDER> no instance type .,return null ;,functions have,success,pre
3 regions 3 replicas <PLACE_HOLDER> some data to the table,table ht = test_util . get connection ( ) . get table ( table name ) ; list < put > puts = new array list < > ( ) ; byte [ ] qualifier = bytes . to bytes ( __str__ ) ; put put = new put ( new byte [ ] { ( byte ) __str__ } ) ; put . add column ( cf @$ qualifier @$ bytes . to bytes ( __str__ ) ) ; puts . add ( put ) ; put = new put ( new byte [ ] { ( byte ) __str__ } ) ; put . add column ( cf @$ qualifier @$ bytes . to bytes ( __str__ ) ) ; puts . add ( put ),regions write,success,pre
do init transactions <PLACE_HOLDER> sender.do once three times @$ only two requests are sent @$ so we should only poll twice,verify ( client @$ times ( __num__ ) ) . poll ( eq ( retry_backoff_ms ) @$ any long ( ) ) ;,transactions check,fail,pre
the user might <PLACE_HOLDER> the file chooser dialog in this case we should not close the test plan,if ( gui package . is dirty ( ) ) { return false ; },user cancel,success,pre
note : the above limiter could <PLACE_HOLDER> and delay the caller 's callback,callback . on error ( t ) ; latch . count down ( ) ;,limiter block,success,pre
seek with no offset epoch requires no validation no matter what the current leader <PLACE_HOLDER>,state . seek unvalidated ( tp0 @$ new subscription state . fetch position ( __num__ @$ optional . of ( __num__ ) @$ new metadata . leader and epoch ( broker1 @$ optional . of ( __num__ ) ) ) ) ; assert false ( state . has valid position ( tp0 ) ) ; assert true ( state . awaiting validation ( tp0 ) ) ; state . seek unvalidated ( tp0 @$ new subscription state . fetch position ( __num__ @$ optional . empty ( ) @$ new metadata . leader and epoch ( broker1 @$ optional . of ( __num__ ) ) ) ) ; assert true ( state . has valid position ( tp0 ) ) ; assert false ( state . awaiting validation,leader is,success,pre
this rename never <PLACE_HOLDER> the dest so files deleted and collected are irrelevant .,return create rename result ( fsd @$ renameiip @$ false @$ null ) ;,rename changes,fail,pre
the empty class <PLACE_HOLDER> a default constructor .,assert that ( class info . get ( ) . declared members ( ) ) . has size ( __num__ ) ; assert that ( class info . get ( ) . declared members ( ) . iterator ( ) . next ( ) . member name ( ) ) . is equal to ( __str__ ) ;,class has,success,pre
if text field <PLACE_HOLDER> only one match @$ use that match .,equate row object match = get match from table ( equate from filter ) ; if ( match != null ) { return match ; } return null ;,field has,fail,pre
measure the component with undefined width spec @$ as the contents of the hscroll <PLACE_HOLDER> unlimited horizontal space .,child component tree . set root and size spec ( content props @$ size spec . make size spec ( __num__ @$ unspecified ) @$ height spec @$ content size ) ; content props . measure ( context @$ size spec . make size spec ( __num__ @$ unspecified ) @$ height spec @$ content size ) ; measured width = content size . width ; measured height = content size . height ; measured component width . set ( measured width ) ; measured component height . set ( measured height ) ;,contents have,success,pre
when errors are not stored alongside values @$ transient errors that are recovered from do not <PLACE_HOLDER> the parent transient,if ( supports transient exceptions ) { assert that error info ( error info ) . is transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is not null ( ) ; } else { assert that error info ( error info ) . is not transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is null ( ) ; },errors affect,fail,pre
comment must immeiately <PLACE_HOLDER> the parameter attribute,parm entry . comment ( token . comment ) ; param attribute ( parm entry ) ; parm entry . type ( param type spec ( entry ) ) ; parm entry . name ( token . name ) ; match ( token . identifier ) ; if ( isnt in list ( entry . parameters ( ) @$ parm entry . name ( ) ) ) entry . add parameter ( parm entry ) ;,comment identify,fail,pre
when generating <PLACE_HOLDER> plan @$ the authority of the output path will be determined based on this hostname configuration .,server configuration . set ( property key . master_hostname @$ __str__ ) ; list < transform plan > plans = m catalog . get transform plan ( db name @$ table name @$ transform_definition ) ; map < string @$ layout > transformed layouts = maps . new hash map with expected size ( plans . size ( ) ) ; plans . for each ( plan -> transformed layouts . put ( plan . get base layout ( ) . get spec ( ) @$ plan . get transformed layout ( ) ) ) ; m catalog . complete transform table ( noop journal context . instance @$ db name @$ table name @$ transform_definition . get definition ( ) @$ transformed layouts ) ; table .,generating transform,success,pre
reduce ipc client connection <PLACE_HOLDER> times and interval time,configuration client conf = new configuration ( false ) ; client conf . set int ( common configuration keys . ipc_client_connect_max_retries_key @$ __num__ ) ; client conf . set int ( common configuration keys . ipc_client_connect_retry_interval_key @$ __num__ ) ;,connection retry,success,pre
ok now we are sure to have both a valid post and show gutenberg flag @$ let 's <PLACE_HOLDER> the editing session tracker,create post editor analytics session tracker ( m show gutenberg editor @$ m edit post repository . get post ( ) @$ m site @$ m is new post ) ;,'s create,fail,pre
a function parameter can not be replaced with a direct inlined value if it is referred to by an inner function . the inner function can out live the call we are replacing @$ so inner function must <PLACE_HOLDER> a unique name . this approach does not work within loop bodies so those are forbidden elsewhere .,if ( n . is function ( ) ) { in inner function = true ; },function have,fail,pre
case 2 : on api 23 @$ we double scheduler workers be<PLACE_HOLDER> job scheduler prefers batching . so is the work is periodic @$ we only need to execute it once per interval . also potential bugs in the platform may <PLACE_HOLDER> a job to run more than once .,if ( m work spec . is periodic ( ) || m work spec . is backed off ( ) ) { long now = system . current time millis ( ) ; boolean is first run = m work spec . period start time == __num__ ; if ( ! is first run && now < m work spec . calculate next run time ( ) ) { logger . get ( ) . debug ( tag @$ string . format ( __str__ + __str__ @$ m work spec . worker class name ) ) ; resolve ( true ) ; return ; } },bugs use,fail,pre
this region does not <PLACE_HOLDER> the scan @$ so skip it .,if ( ! range . overlaps ( byte key range . of ( last end key @$ response end key ) ) ) { last offset = response offset ; last end key = response end key ; continue ; },region contain,fail,pre
after emitting each pane @$ it will continue accumulating the elements so that each approximation <PLACE_HOLDER> all of the previous data in addition to the newly arrived data .,p collection < table row > speculative results = flow info . apply ( __str__ @$ window . < kv < string @$ integer > > into ( fixed windows . of ( duration . standard minutes ( window duration ) ) ) . triggering ( repeatedly . forever ( after processing time . past first element in pane ( ) . plus delay of ( one_minute ) ) ) . accumulating fired panes ( ) . with allowed lateness ( one_day ) ) . apply ( new total flow ( __str__ ) ) ;,approximation contains,fail,pre
there is an existing user who only <PLACE_HOLDER> unsupported sign in methods,if ( task . is successful ( ) && last signed in providers . is empty ( ) && ! methods . is empty ( ) ) { return tasks . for exception ( new firebase ui exception ( error codes . developer_error ) ) ; },who has,success,pre
agree on the consensus if this node does not see the client at all or if this node <PLACE_HOLDER> the client however the timeout is out,if ( found client == null || is timeout exceeded ( found client @$ watchdog client retry timeout ) ) { client disconnected consensus = true ; },node found,fail,pre
first copy the children as the call to copy.add will <PLACE_HOLDER> the collection we 're iterating on,enumeration < ? > enum from = node . children ( ) ; list < j meter tree node > tmp = new array list < > ( ) ; while ( enum from . has more elements ( ) ) { j meter tree node child = ( j meter tree node ) enum from . next element ( ) ; tmp . add ( child ) ; } for ( j meter tree node j meter tree node : tmp ) { copy . add ( j meter tree node ) ; } tree model . insert node into ( copy @$ target @$ index ++ ) ; nodes for removal . add ( node ) ; paths to select [ path position ++ ] =,call update,fail,pre
now do <PLACE_HOLDER> closest on created mapfile .,map file . reader reader = new map file . reader ( qualified dir name @$ conf ) ; try { assert equals ( null @$ reader . mid key ( ) ) ; } finally { reader . close ( ) ; },now get,success,pre
update processor config : note that we want to run a generation update even if the processor config <PLACE_HOLDER> no changes ; we still need to handle changes in the exported tables,update processor config ( connectors ) ; if ( ! requires new export generation ) { if ( m_generation . get ( ) != null ) { m_generation . get ( ) . update generation id ( catalog context . m_gen id ) ; } export log . info ( __str__ ) ; return ; },config has,success,pre
as it turns out @$ starting a new media player on the completion of a previous player <PLACE_HOLDER> up slightly overlapping the two playbacks @$ so slightly delaying the start of the next player gives a better user experience,if ( m next player != null ) { system clock . sleep ( __num__ ) ; m next player . start ( ) ; },player picks,fail,pre
make cdi work in containers with implicit archive <PLACE_HOLDER> disabled,write optional ( output folder @$ new supporting file ( __str__ @$ __str__ @$ __str__ ) ) ;,containers file,fail,pre
fallback if <PLACE_HOLDER> not a valid path .,changes . put ( file @$ file ) ;,fallback has,fail,pre
the file should <PLACE_HOLDER> just 1 character :,assert equals ( __num__ @$ straus . get len ( ) ) ; har file system . close ( ) ; local fs . delete ( tmp path @$ true ) ;,file have,fail,pre
check if the sequence at the current position <PLACE_HOLDER> locale digits .,for ( int i = __num__ ; i < __num__ ; i ++ ) { int digit str len = locale digits [ i ] . length ( ) ; if ( str . region matches ( start @$ locale digits [ i ] @$ __num__ @$ digit str len ) ) { dec val [ __num__ ] = i ; return digit str len ; } },sequence matches,success,pre
inflater <PLACE_HOLDER> a bit of slack,long size = get entry size ( jzentry ) + __num__ ;,inflater gives,fail,pre
user should not see anything so <PLACE_HOLDER> unsatisfiable condition,return process payload builder . process definitions ( ) . with process definition key ( __str__ + uuid . randomuuid ( ) . to string ( ) ) . build ( ) ;,anything give,success,pre
check for region <PLACE_HOLDER> event from server .,client1 . invoke ( new cache serializable runnable ( __str__ ) { @ override public void run2 ( ) throws cache exception { region local region = get cache ( ) . get region ( __str__ + regions [ __num__ ] ) ; if ( local region != null ) { wait . pause ( __num__ * __num__ ) ; cq query [ ] cqs = get cache ( ) . get query service ( ) . get cqs ( ) ; if ( cqs != null && cqs . length > __num__ ) { assert true ( cqs [ __num__ ] . is closed ( ) ) ; } assert null ( __str__ + __str__ @$ get cache ( ) . get region ( __str__ + regions,check destroyed,success,pre
some legacy mode devices do not <PLACE_HOLDER> af off,if ( is hardware level limited or better ( ) ) { check true for key ( key @$ __str__ @$ modes list . contains ( camera metadata . control_af_mode_off ) ) ; },devices support,success,pre
no cluster in cfg : <PLACE_HOLDER> the default one,if ( ! clusters . contains field ( i cluster name ) ) cfg = clusters . field ( all_wildcard ) ; else cfg = clusters . field ( i cluster name ) ;,cluster use,fail,pre
for job props @$ deletion in local folder <PLACE_HOLDER> inheritance from ancestor folder and reschedule .,load new common config and handle new job ( path @$ job scheduler . action . reschedule ) ; return ;,deletion prevents,fail,pre
spellcheck dictionary <PLACE_HOLDER> valencian and general accentuation,assert equals ( __num__ @$ rule . match ( lang tool . get analyzed sentence ( __str__ ) ) . length ) ;,dictionary contains,success,pre
see if the alias <PLACE_HOLDER> a lateral view . if so @$ chain the lateral view operator on,list < ast node > lateral views = alias to lateral views . get ( alias ) ; if ( lateral views != null ) { operator op = e . get value ( ) ; for ( ast node lateral view tree : alias to lateral views . get ( alias ) ) { op = gen lateral view plan ( qb @$ op @$ lateral view tree ) ; } e . set value ( op ) ; },alias has,success,pre
remove any <PLACE_HOLDER> columns that do not exist in this segment .,for ( string column name : immutable list . copy of ( column names ) ) { if ( index . get column holder ( column name ) == null ) { column names . remove ( column name ) ; } },any returned,fail,pre
propagate tracing headers @$ so remote service can <PLACE_HOLDER> current span as its parent,map < string @$ list < string > > tracing headers = tracing headers ( tracer @$ current span ) ; map < string @$ list < string > > outbound headers = tracer provider . map ( provider -> provider . update outbound headers ( current span @$ tracer @$ parent span . or else ( null ) @$ tracing headers @$ inbound headers ) ) . or else ( tracing headers ) ;,service have,fail,pre
since tests are always running on a single host keep the resolution timeout low as otherwise people running with strange network configurations will <PLACE_HOLDER> very slow tests,conf . put ( property key . network_host_resolution_timeout_ms @$ __str__ ) ;,people get,fail,pre
hint arrow <PLACE_HOLDER> no plane @$ and always returns the current plane,if ( is in agility arena ( ) ) { world point new ticket position = client . get hint arrow point ( ) ; world point old tick position = last arena ticket position ; last arena ticket position = new ticket position ; if ( old tick position != null && new ticket position != null && ( old tick position . getx ( ) != new ticket position . getx ( ) || old tick position . gety ( ) != new ticket position . gety ( ) ) ) { log . debug ( __str__ @$ old tick position @$ new ticket position ) ; if ( config . notify agility arena ( ) ) { notifier . notify ( __str__ ) ; },arrow has,success,pre
okay @$ we <PLACE_HOLDER> the data . now <PLACE_HOLDER> the agent do the restore .,stage . close ( ) ; m backup data = parcel file descriptor . open ( m backup data name @$ parcel file descriptor . mode_read_only ) ; m new state = parcel file descriptor . open ( m new state name @$ parcel file descriptor . mode_read_write | parcel file descriptor . mode_create | parcel file descriptor . mode_truncate ) ;,now lets,fail,pre
same dead <PLACE_HOLDER> all of the time .,int max try = config . get node repetitions ( ) + __num__ ; if ( max try < __num__ ) { max try = __num__ ; } return new ketama iterator ( k @$ max try @$ get ketama nodes ( ) @$ hash alg ) ;,dead try,fail,pre
make a copy of the graph to avoid concurrency problems . graph manipulations are not thread safe @$ and another thread can concurrently <PLACE_HOLDER> this method .,final structured graph graph = ( structured graph ) method . compilation info . get graph ( ) . copy ( debug ) ; try ( debug context . scope s = debug . scope ( __str__ @$ graph @$ method @$ this ) ) { try { try ( indent in = debug . log and indent ( __str__ @$ method ) ) { boolean inlined = false ; for ( invoke invoke : graph . get invokes ( ) ) { if ( invoke instanceof invoke node ) { throw vm error . should not reach here ( __str__ + invoke . call target ( ) . target method ( ) . format ( __str__ ) + __str__ + ( graph . method ( ) ==,thread execute,fail,pre
first heartbeat which <PLACE_HOLDER> first resource .,localizer heartbeat response response = spy service . heartbeat ( stat ) ; assert equals ( __str__ @$ localizer action . live @$ response . get localizer action ( ) ) ;,which schedules,success,pre
reset the flow scope 's syntactic scope to the function block @$ rather than the function <PLACE_HOLDER> itself . this allows pulling out local vars from the function by name to verify their types .,if ( cfg root . is function ( ) ) { return scope = rtn state . get in ( ) . with syntactic scope ( scope creator . create scope ( cfg root . get last child ( ) ) ) ; } else { return scope = rtn state . get in ( ) ; },function function,fail,pre
the <PLACE_HOLDER> options to use in the states .,write options write options = null ; linked hash map < string @$ rocksdb keyed state backend . rocks db kv state info > kv state information = new linked hash map < > ( ) ; rocksdb db = null ; abstract rocksdb restore operation restore operation = null ; rocks db ttl compact filters manager ttl compact filters manager = new rocks db ttl compact filters manager ( enable ttl compaction filter @$ ttl time provider ) ; resource guard rocksdb resource guard = new resource guard ( ) ; snapshot strategy < k > snapshot strategy ; priority queue set factory priority queue factory ; rocksdb serialized composite key builder < k > shared rocks key builder ;,the write,success,pre
instance klass array is sorted by name . <PLACE_HOLDER> binary search,int low = __num__ ; int high = tmp klasses . length - __num__ ; int mid = - __num__ ; while ( low <= high ) { mid = ( low + high ) > > __num__ ; instance klass mid val = tmp klasses [ mid ] ; int cmp = mid val . get name ( ) . as string ( ) . compare to ( class name ) ; if ( cmp < __num__ ) { low = mid + __num__ ; } else if ( cmp > __num__ ) { high = mid - __num__ ; } else { return tmp klasses [ mid ] ; } },klass use,fail,pre
currently @$ this will always be null because the manifest does n't <PLACE_HOLDER> any useful information,if ( jersey version == null ) jersey version = __str__ ;,manifest provide,fail,pre
stop <PLACE_HOLDER> chain .,return true ;,stop link,fail,pre
style rules should always <PLACE_HOLDER> the lowest priority .,return - __num__ ;,rules have,success,pre
note : for plugins that depend on other plugin artifacts the plugin realm <PLACE_HOLDER> more than one plugin descriptor . however @$ only the first descriptor is of interest .,if ( ! first descriptor ) { return ; } first descriptor = false ; if ( ! plugin artifact . get group id ( ) . equals ( plugin descriptor . get group id ( ) ) ) { errors . add ( __str__ + plugin descriptor . get group id ( ) ) ; } if ( ! plugin artifact . get artifact id ( ) . equals ( plugin descriptor . get artifact id ( ) ) ) { errors . add ( __str__ + plugin descriptor . get artifact id ( ) ) ; } if ( ! plugin artifact . get base version ( ) . equals ( plugin descriptor . get version ( ) ) ) { errors . add ( __str__,realm has,fail,pre
this is a direct call to a method that the static analysis did not see as invoked . this can happen when the receiver is always null . in most cases @$ the method profile also has a length of 0 and the below code to kill the invoke would trigger . but not all methods <PLACE_HOLDER> profiles @$ for example methods with manually,if ( call target . invoke kind ( ) . is direct ( ) && ! ( ( hosted method ) call target . target method ( ) ) . get wrapped ( ) . is simply implementation invoked ( ) ) { unreachable invoke ( graph @$ invoke @$ call target ) ; continue ; },methods use,fail,pre
synchronously end the animation @$ jumping to the end state . animator set <PLACE_HOLDER> synchronous listener behavior on all supported ap is .,if ( ! animated ) { current animation . end ( ) ; },set enables,fail,pre
null <PLACE_HOLDER> a 'thunk ' function,return instruction != null ;,null indicates,fail,pre
make sure link properties represents the latest private dns status . this does not need to be done before update dnses because the link properties are not the source of the private dns configuration . update dnses will <PLACE_HOLDER> the private dns configuration from dns manager .,m dns manager . update private dns status ( net id @$ new lp ) ; if ( is default network ( network agent ) ) { handle apply default proxy ( new lp . get http proxy ( ) ) ; } else { update proxy ( new lp @$ old lp ) ; } update wake on lan ( new lp ) ;,dnses recompute,fail,pre
did not find a child to receive the event . assign the pointer to the least recently <PLACE_HOLDER> target .,if ( new touch target == null && m first touch target != null ) { new touch target = m first touch target ; while ( new touch target . next != null ) { new touch target = new touch target . next ; } new touch target . pointer id bits |= id bits to assign ; },pointer touched,fail,pre
now <PLACE_HOLDER> a rebalance @$ but cancel it in the middle,vm0 . invoke ( ( ) -> { count down latch rebalancing cancelled = new count down latch ( __num__ ) ; count down latch rebalancing finished = new count down latch ( __num__ ) ; internal resource manager manager = get cache ( ) . get internal resource manager ( ) ; internal resource manager . set resource observer ( new resource observer adapter ( ) { @ override public void rebalancing or recovery started ( region region ) { try { rebalancing cancelled . await ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; } } @ override public void rebalancing or recovery finished ( region region ) { rebalancing finished . count,now perform,fail,pre
this function <PLACE_HOLDER> one sync future only .,return __num__ ;,function maintains,fail,pre
existing <PLACE_HOLDER> id against the caller 's valid list .,if ( part == null ) { return null ; } if ( mt == null ) { throw new invalid object exception ( __str__ ) ; },existing generated,fail,pre
note that we are always delivering a small message to the transport here which may incur transport framing overhead as it may be sent separately to the contents of the grpc frame . the final message may not be completely written because we do not flush the last buffer . do not <PLACE_HOLDER> the last message as sent .,sink . deliver frame ( writeable header @$ false @$ false @$ messages buffered - __num__ ) ; messages buffered = __num__ ;,note see,fail,pre
end <PLACE_HOLDER> check,break ;,end enabled,success,pre
counting source.unbounded <PLACE_HOLDER> very good splitting behavior,assert that ( initial inputs @$ has size ( num splits ) ) ; int read per split = __num__ ; int total size = num splits * read per split ; set < long > expected outputs = contiguous set . create ( range . closed open ( __num__ @$ ( long ) total size ) @$ discrete domain . longs ( ) ) ; collection < long > read items = new array list < > ( total size ) ; for ( committed bundle < ? > initial input : initial inputs ) { committed bundle < unbounded source shard < long @$ ? > > shard bundle = ( committed bundle < unbounded source shard < long @$ ? > > ) initial input,source.unbounded gives,fail,pre
this method always <PLACE_HOLDER> activation exception,user service . activate user ( __str__ ) ;,method throws,success,pre
verify that the vertices are all in the same slot <PLACE_HOLDER> group,slot sharing group group1 ; slot sharing group group2 ;,vertices shared,fail,pre
<PLACE_HOLDER> the label field @$ so that if it contains inlined labels then it 's free . otherwise <PLACE_HOLDER> the dynamic labels in another data structure and point into it .,long label field = node cursor . get label field ( ) ; boolean has inlined labels = ! node labels field . field points to dynamic record of labels ( node cursor . get label field ( ) ) ; if ( labels == null ) { has inlined labels = true ; label field = no_labels_field . long value ( ) ; },cache save,fail,pre
if its <PLACE_HOLDER> no further action,if ( stage steps collection completed ) { return ; },its doing,fail,pre
call resource <PLACE_HOLDER> ctor so that it does all validations .,new health monitor ( deployment . get systemsettings ( ) @$ new dummy snmp trap sender ( ) ) ;,resource create,fail,pre
0 xc 1 a 5 <PLACE_HOLDER> 12 e @$ 0 x 40358874 @$ 0 x 64 d 4 e<PLACE_HOLDER> 0 d @$ 0 xc 0089309,masm . subsd ( xmm5 @$ xmm2 ) ; masm . andl ( rdx @$ __num__ ) ; masm . shrl ( rdx @$ __num__ ) ; masm . movdqu ( xmm0 @$ new amd64 address ( r11 @$ rdx @$ amd64 address . scale . times1 @$ - __num__ ) ) ; masm . movdqu ( xmm4 @$ record external address ( crb @$ coeff16 ) ) ;,5 fe,fail,pre
client will always <PLACE_HOLDER> 'initiating ' state first .,if ( request . get options ( ) . get run async ( ) ) { return new backup status ( backup id @$ backup state . initiating ) ; },client send,fail,pre
check if its instance of index @$ in that the case <PLACE_HOLDER> index exists exception .,if ( ind instanceof index ) { if ( remotely originated ) { return ( index ) ind ; } throw new index name conflict exception ( string . format ( __str__ @$ index name ) ) ; } future task < index > old index future task = ( future task < index > ) ind ; index index = null ; boolean interrupted = false ; try { if ( old index future task == null ) { index future task . run ( ) ; index = index future task . get ( ) ; if ( index != null ) { this . indexes . put ( index task @$ index ) ; partitioned index pr index = ( partitioned index ) index ;,case throws,fail,pre
stream 1 <PLACE_HOLDER> another message,buffer = create message frame ( fake message ) ; frame handler ( ) . data ( false @$ __num__ @$ buffer @$ message frame length ) ; verify ( frame writer @$ timeout ( time_out_ms ) ) . window update ( eq ( __num__ ) @$ eq ( ( long ) __num__ * message frame length ) ) ;,stream receives,success,pre
the anonymous click listener since that changes the model state @$ instead our anonymous click listener should <PLACE_HOLDER> the hash code of the <PLACE_HOLDER>r 's click listener,model click listener model click listener = new model click listener ( ) ; view click listener view click listener = new view click listener ( ) ; test controller controller = new test controller ( ) ; adapter data observer observer mock = mock ( adapter data observer . class ) ; controller . get adapter ( ) . register adapter data observer ( observer mock ) ; model with click listener_ model = new model with click listener_ ( ) ; controller . set model ( model ) ; controller . request model build ( ) ; verify ( observer mock ) . on item range inserted ( eq ( __num__ ) @$ eq ( __num__ ) ) ; model = new model with click listener_,listener reference,fail,pre
transaction <PLACE_HOLDER> info,log provider . raw message matcher ( ) . assert contains ( __str__ ) ; log provider . raw message matcher ( ) . assert contains ( __str__ ) ; management service . shutdown ( ) ;,transaction commit,fail,pre
do an update @$ which will <PLACE_HOLDER> up the large version,region . put ( __num__ @$ __num__ ) ;,which pick,success,pre
for tabs above the focused tab <PLACE_HOLDER> them up to 0 .,if ( i < focus index ) { add animation ( set @$ tab @$ scroll_offset @$ tab . get scroll offset ( ) @$ tab . get scroll offset ( ) - m height - spacing @$ tab_focused_animation_duration @$ __num__ ) ; } else if ( i > focus index ) { float covering tab position = layout tab . gety ( ) ; float distance to border = math utils . clamp ( m height - covering tab position @$ __num__ @$ m height ) ; float delay = tab_focused_max_delay * distance to border / m height ; add animation ( set @$ tab @$ y_in_stack_offset @$ tab . gety in stack offset ( ) @$ tab . gety in stack offset ( ) + m,tabs push,fail,pre
verify that server 1 's event queue <PLACE_HOLDER> the default value,server1 . invoke ( ( ) -> { internal cache cache = cluster startup rule . get cache ( ) ; async event queue queue = cache . get async event queue ( __str__ ) ; assert that ( queue . get batch size ( ) ) . is equal to ( gateway sender . default_batch_size ) ; assert that ( queue . get batch time interval ( ) ) . is equal to ( default_batch_time_interval ) ; assert that ( queue . get maximum queue memory ( ) ) . is equal to ( gateway sender . default_maximum_queue_memory ) ; } ) ; gfsh . execute and assert that ( __str__ + altered_batch_size + __str__ + altered_batch_time_interval + __str__ + altered_maximum_queue_memory ) . status is success (,queue has,success,pre
add 1@$1 should <PLACE_HOLDER> 1@$1,try { inc ( usage @$ suffix @$ resource . new instance ( __num__ @$ __num__ ) @$ label ) ; check ( __num__ @$ __num__ @$ get ( usage @$ suffix @$ label ) ) ; } catch ( no such method exception e ) { },1 increase,fail,pre
creation <PLACE_HOLDER> sync,post destroy action ( ) ;,creation does,fail,pre
each interpreter group <PLACE_HOLDER> one session,for ( interpreter group interpreter group : interpreter setting . get all interpreter groups ( ) ) { assert equals ( __num__ @$ interpreter group . get session num ( ) ) ; },group has,success,pre
do the <PLACE_HOLDER> out magic,instructions . add ( reil helpers . create and ( base offset ++ @$ byte size @$ __str__ @$ byte size @$ is zero condition seven @$ byte size @$ shifter carry out tmp1 ) ) ; instructions . add ( reil helpers . create bsh ( base offset ++ @$ d word size @$ register node value1 @$ d word size @$ minus thirty one set @$ byte size @$ tmp var5 ) ) ; instructions . add ( reil helpers . create and ( base offset ++ @$ byte size @$ tmp var5 @$ byte size @$ is zero condition four @$ byte size @$ shifter carry out tmp2 ) ) ; instructions . add ( reil helpers . create sub ( base offset ++ @$,the kick,fail,pre
rehash the table if the number of entries would <PLACE_HOLDER> the number of buckets .,if ( f num >= f table size ) { rehash ( ) ; bucket = hash % f table size ; } else if ( collision count >= max_hash_collisions && key instanceof string ) { rebalance ( ) ; bucket = hash ( key ) % f table size ; },number exceed,success,pre
most likely client browser <PLACE_HOLDER> socket,if ( t instanceof socket exception ) { get logger ( ) . info ( __str__ + __str__ ) ; return ; },browser closed,success,pre
add modules files as inputs . instead they rely on input discovery to recognize the needed ones . however @$ orphan detection runs before input discovery and thus module files would be discarded as orphans . this is strictly better than marking all transitive modules as inputs @$ which would also effectively <PLACE_HOLDER> orphan detection for .pcm files .,if ( output file . is file type ( cpp file types . cpp_module ) ) { return immutable set . of ( output file ) ; } return super . get mandatory outputs ( ) ;,which force,fail,pre
let system <PLACE_HOLDER> blame for date change events .,final work source work source = null ;,system generate,fail,pre
add all <PLACE_HOLDER> cells that need to be inserted after this one .,add spanning cells ( ) ;,all skipped,fail,pre
a list which will <PLACE_HOLDER> the handles for the column families once the db is opened,list < column family handle > columns = new array list < > ( ) ; m db = rocksdb . open ( m db opts @$ m db path @$ cf descriptors @$ columns ) ; m checkpoint = checkpoint . create ( m db ) ; for ( int i = __num__ ; i < columns . size ( ) - __num__ ; i ++ ) { m column handles . get ( i ) . set ( columns . get ( i + __num__ ) ) ; },which contain,fail,pre
for whatever reason this activity is being launched into a new task ... yet the caller has <PLACE_HOLDER> a result back . well @$ that is pretty messed up @$ so instead immediately send back a cancel and let the new task continue launched as normal without a dependency on its originator .,if ( source stack != null && ( m launch flags & flag_activity_new_task ) != __num__ ) { slog . w ( tag @$ __str__ ) ; source stack . send activity result locked ( - __num__ @$ m start activity . result to @$ m start activity . result who @$ m start activity . request code @$ result_canceled @$ null ) ; m start activity . result to = null ; },caller provided,fail,pre
null pkg <PLACE_HOLDER> user switch,cancel all notifications by list locked ( m notification list @$ calling uid @$ calling pid @$ pkg @$ true @$ channel id @$ flag checker @$ false @$ user id @$ false @$ reason @$ listener name @$ true ) ;,pkg indicates,success,pre
the file name should <PLACE_HOLDER> an extension separator .,if ( file name . last index of ( extension_separator ) == - __num__ ) { return false ; },name have,fail,pre
pod does not <PLACE_HOLDER> any volume that references pvc,if ( volume to che volume name . is empty ( ) ) { continue ; },pod have,success,pre
we check that access control exceptions <PLACE_HOLDER> absolute paths .,path parent = p . get parent ( ) ; assert true ( parent . is uri path absolute ( ) ) ; assert true ( e . get message ( ) . contains ( parent . to string ( ) ) ) ; return false ;,exceptions return,fail,pre
client & server <PLACE_HOLDER> the same protocol,if ( server methods == null ) { return true ; },client have,success,pre
because users do n't have a backup of media @$ it 's safer to import new data and rely on them <PLACE_HOLDER> a media db check to get rid of any unwanted media . in the future we might also want to duplicate this step import media,hash map < string @$ string > name to num = new hash map < > ( ) ; hash map < string @$ string > num to name = new hash map < > ( ) ; file media map file = new file ( dir . get absolute path ( ) @$ __str__ ) ; if ( media map file . exists ( ) ) { json reader jr = new json reader ( new file reader ( media map file ) ) ; jr . begin object ( ) ; string name ; string num ; while ( jr . has next ( ) ) { num = jr . next name ( ) ; name = jr . next string ( ) ; name,them using,fail,pre
new settings for set <PLACE_HOLDER> location ui no longer initiated here .,if ( upgrade version == __num__ ) { upgrade version = __num__ ; },settings install,success,pre
given : thread which <PLACE_HOLDER> segment .,segment aware aware = new segment aware ( __num__ @$ false ) ; aware . check can read archive or reserve work segment ( __num__ ) ; try { aware . release work segment ( __num__ ) ; } catch ( assertion error e ) { return ; } fail ( __str__ ) ;,which awaited,success,pre
coalesce can <PLACE_HOLDER> this more succinctly,list < string > nick names = entity manager . create query ( __str__ + __str__ @$ string . class ) . get result list ( ) ;,coalesce output,fail,pre
tree <PLACE_HOLDER> stuff .,menu = new j menu ( __str__ ) ; menu bar . add ( menu ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new add action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new insert action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new reload action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action,tree list,fail,pre
normal operation <PLACE_HOLDER> the site thread on the sitetasker queue .,while ( m_should continue ) { m_txn state = null ; site tasker task = m_scheduler . take ( ) ; task . run ( get site procedure connection ( ) ) ; },operation blocks,success,pre
this thread is being killed @$ do not <PLACE_HOLDER> anything,if ( t instanceof thread death ) { } else { system . err . println ( __str__ + thread . current thread ( ) . get name ( ) + __str__ + t . get class ( ) . get canonical name ( ) ) ; try { t . print stack trace ( ) ; } catch ( throwable ignored ) { } },thread do,fail,pre
all key input columns are repeating . <PLACE_HOLDER> key once . lookup once . since the key is repeated @$ we must use entry 0 regardless of selected in use .,if ( all key input columns repeating ) { key vector serialize write . set output ( current key output ) ; key vector serialize write . serialize write ( batch @$ __num__ ) ; join util . join result join result ; if ( key vector serialize write . get has any nulls ( ) ) { join result = join util . join result . nomatch ; } else { byte [ ] key bytes = current key output . get data ( ) ; int key length = current key output . get length ( ) ; join result = hash multi set . contains ( key bytes @$ __num__ @$ key length @$ hash multi set results [ __num__ ] ) ; } if,input generate,success,pre
no good way to <PLACE_HOLDER> ambiguous time at transition @$ but following code work in most case .,tz . get offset ( local millis @$ true @$ offsets ) ; if ( tztype == time type . standard && offsets [ __num__ ] != __num__ || tztype == time type . daylight && offsets [ __num__ ] == __num__ ) { tz . get offset ( local millis - ( __num__ * __num__ * __num__ * __num__ ) @$ true @$ offsets ) ; },way catch,fail,pre
set a small time unit as cookie max age so that the server <PLACE_HOLDER> a 401,hive conf . set time var ( conf vars . hive_server2_thrift_http_cookie_max_age @$ __num__ @$ time unit . seconds ) ; hive conf . set bool var ( conf vars . hive_support_concurrency @$ false ) ; minihs2 = mini hive kdc . get minihs2 with kerb ( mini hive kdc @$ hive conf ) ; minihs2 . start ( new hash map < string @$ string > ( ) ) ;,server returns,fail,pre
if vertex low time is same as <PLACE_HOLDER> time then this is start vertex for strongly connected component . keep popping vertices out of stack still you find current vertex . they are all part of one strongly connected component .,if ( visited time . get ( vertex ) == low time . get ( vertex ) ) { set < vertex < integer > > strongly connected componenet = new hash set < > ( ) ; vertex v ; do { v = stack . poll first ( ) ; on stack . remove ( v ) ; strongly connected componenet . add ( v ) ; } while ( ! vertex . equals ( v ) ) ; result . add ( strongly connected componenet ) ; },then visited,success,pre
if the evaluate yields true then <PLACE_HOLDER> all rows else <PLACE_HOLDER> 0 rows,if ( pred instanceof expr node generic func desc ) { expr node generic func desc gen func = ( expr node generic func desc ) pred ; for ( expr node desc leaf : gen func . get children ( ) ) { if ( leaf instanceof expr node generic func desc ) { long new num rows = __num__ ; for ( expr node desc child : gen func . get children ( ) ) { new num rows = evaluate child expr ( stats @$ child @$ asp ctx @$ needed cols @$ op @$ num rows ) ; } return num rows - new num rows ; } else if ( leaf instanceof expr node constant desc ) { expr node constant desc encd,true pass,success,pre
no need to call <PLACE_HOLDER> all futures @$ since future drawables will change layout direction when they are prepared .,final int count = m num children ; final drawable [ ] drawables = m drawables ; for ( int i = __num__ ; i < count ; i ++ ) { if ( drawables [ i ] != null ) { boolean child changed = false ; if ( android . os . build . version . sdk_int >= android . os . build . version_codes . m ) { child changed = drawables [ i ] . set layout direction ( layout direction ) ; } if ( i == current index ) { changed = child changed ; } } } m layout direction = layout direction ; return changed ;,need clear,fail,pre
let 's <PLACE_HOLDER> an invalid frame,handshake ( client @$ sock -> { buffer buff = buffer . buffer ( ) ; buff . append byte ( ( byte ) ( __num__ ) ) . append byte ( ( byte ) __num__ ) ; sock . write ( buff ) ; } ) ;,'s send,fail,pre
create a simple rule which just <PLACE_HOLDER> a file .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; path output = paths . get ( __str__ ) ; immutable set < source path > inputs before = immutable set . of ( ) ; dep file build rule rule = new dep file build rule ( target @$ filesystem @$ params ) { @ add to rule key private final source path path = path source path . of ( filesystem @$ input file ) ; @ override public immutable list < step > get build steps ( build context context @$ buildable context buildable context ) { return immutable list . of ( new write file step,which writes,success,pre
some variables do n't <PLACE_HOLDER> names @$ but use the name of their datatype,if ( datatype != null ) { return datatype . get name ( ) ; } string signature = get signature ( true ) ; string fixed = symbol utilities . replace invalid chars ( signature @$ true ) ; return fixed ;,variables have,success,pre
recalculate <PLACE_HOLDER> time .,woke = ( woke == - __num__ ) ? system . current time millis ( ) : woke ; wait time = this . period - ( woke - start time ) ;,recalculate wait,success,pre
and <PLACE_HOLDER> the records ...,try { assert source record match ( actual record @$ expected record @$ ignorable fields :: contains @$ comparators by field name @$ comparators by schema name ) ; } catch ( assertion error e ) { result . error ( ) ; string msg = __str__ + schema util . as string ( actual record . key ( ) ) + __str__ + e . get message ( ) ; testing . debug ( msg ) ; throw new mismatch record exception ( e @$ msg @$ actual record history @$ expected record history ) ; },and compare,success,pre
our internal version does not <PLACE_HOLDER> this rule,if ( ! analysis mock . is this bazel ( ) ) { return ; },version support,fail,pre
no class <PLACE_HOLDER> corresponding to the system property passed in from bazel,if ( suite == null ) { if ( args . length == __num__ && suite class name != null ) { system . err . printf ( __str__ @$ suite class name ) ; return __num__ ; } },class defined,fail,pre
sp plans which have an index which can <PLACE_HOLDER> the window function ordering do n't create an order by node .,if ( ! ( child instanceof order by plan node ) ) { return plan ; } order by plan node onode = ( order by plan node ) child ; child = onode . get child ( __num__ ) ;,which modify,fail,pre
make sure the developer is <PLACE_HOLDER> the rules .,if ( ( node . get text ( ) == null ) && ( node . get content description ( ) == null ) ) { throw new runtime exception ( __str__ + __str__ ) ; } node . get bounds in parent ( temp parent rect ) ; if ( temp parent rect . equals ( invalid_parent_bounds ) ) { throw new runtime exception ( __str__ + __str__ ) ; } final int actions = node . get actions ( ) ; if ( ( actions & accessibility node info . action_accessibility_focus ) != __num__ ) { throw new runtime exception ( __str__ + __str__ ) ; } if ( ( actions & accessibility node info . action_clear_accessibility_focus ) != __num__ ) { throw new runtime exception,developer following,success,pre
most subclasses will <PLACE_HOLDER> these :,sampler sampler = jmctx . get current sampler ( ) ; bindings . put ( __str__ @$ sampler ) ;,subclasses need,success,pre
process <PLACE_HOLDER> query i ds,list < string > named query ids = list named queries result . get named query ids ( ) ;,process named,success,pre
the plugin state has been <PLACE_HOLDER> while a configuration network call is going on @$ we need to dispatch another configure plugin action since we do n't allow multiple configure actions to happen at the same time this might happen either because user <PLACE_HOLDER> the state or a remove plugin action has started,if ( is plugin state changed since last configuration dispatch ( ) ) { dispatch configure plugin action ( false ) ; } else if ( m is removing plugin && ! m plugin . is active ( ) ) { dispatch remove plugin action ( ) ; m is active = m plugin . is active ( ) ; m switch active . set checked ( m is active ) ; },user changed,success,pre
test once using the current correct hash function @$ <PLACE_HOLDER> no mispartitioned rows,client response cr = client . call procedure ( __str__ @$ ( object ) null ) ; volt table hashinator matches = cr . get results ( ) [ __num__ ] ; hashinator matches . advance row ( ) ; while ( hashinator matches . advance row ( ) ) { assert equals ( __num__ @$ hashinator matches . get long ( __str__ ) ) ; } volt table validate result = cr . get results ( ) [ __num__ ] ;,test check,fail,pre
if the current component <PLACE_HOLDER> an action @$ run that one,if ( component . has ( __str__ ) ) { json object action = component . getjson object ( __str__ ) ; ( ( jason view activity ) context ) . call ( action . to string ( ) @$ new json object ( ) . to string ( ) @$ __str__ @$ v . get context ( ) ) ; } else { view cursor = v ; while ( cursor . get parent ( ) != null ) { json object item = ( json object ) ( ( ( view ) cursor . get parent ( ) ) . get tag ( ) ) ; if ( item != null && ( item . has ( __str__ ) || item . has ( __str__ ),component has,fail,pre
let the local destroy or processing of transfer <PLACE_HOLDER> the clear,return ;,destroy do,success,pre
the new area tiles <PLACE_HOLDER> a wall,if ( ( collision data flags [ x ] [ checky ] & y wall flags east ) != __num__ ) { return false ; },tiles contains,success,pre
create a simple rule which just <PLACE_HOLDER> a file .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; source path input = path source path . of ( filesystem @$ filesystem . get root path ( ) . get file system ( ) . get path ( __str__ ) ) ; filesystem . touch ( path resolver . get relative path ( input ) ) ; path output = build target paths . get gen path ( filesystem @$ target @$ __str__ ) ; dep file build rule rule = new dep file build rule ( target @$ filesystem @$ params ) { @ add to rule key private final source path path = input ; @ override,which writes,success,pre
lets <PLACE_HOLDER> a look at the operator memory requirements .,dispatcher disp = null ; final set < map join operator > map joins = new linked hash set < map join operator > ( ) ; linked hash map < rule @$ node processor > rules = new linked hash map < rule @$ node processor > ( ) ; rules . put ( new rule reg exp ( __str__ @$ map join operator . get operator name ( ) + __str__ ) @$ new node processor ( ) { @ override public object process ( node nd @$ stack < node > stack @$ node processor ctx proc ctx @$ object ... node outputs ) { map joins . add ( ( map join operator ) nd ) ; return null ; } } ) ;,lets have,fail,pre
if the reparent window token <PLACE_HOLDER> previous display 's last focus window @$ means it will end up to gain window focus on the target display @$ so it should not be notified that it lost focus from the previous display .,if ( token . has child ( prev dc . m last focus ) ) { prev dc . m last focus = null ; },token has,fail,pre
run the counter job @$ outputs to a single <PLACE_HOLDER> task and file,log . info ( __str__ ) ; try { boolean success = counter . wait for completion ( true ) ; if ( ! success ) { string message = __str__ + counter . get status ( ) . get state ( ) + __str__ + counter . get status ( ) . get failure info ( ) ; log . error ( message ) ; throw new runtime exception ( message ) ; } } catch ( io exception | interrupted exception | class not found exception e ) { log . error ( __str__ @$ e ) ; throw e ; } log . info ( __str__ ) ;,job join,fail,pre
if no restrictions were saved @$ dpm method should <PLACE_HOLDER> an empty bundle as per java doc .,return bundle != null ? new bundle ( bundle ) : new bundle ( ) ;,method return,success,pre
get the start index ; the assertions below will fail if the assignment logic does not <PLACE_HOLDER> correct contracts,int num consumers = kafka topic partition assigner . assign ( mock get all partitions for topics return . get ( __num__ ) @$ num subtasks ) ; for ( int subtask index = __num__ ; subtask index < mock get all partitions for topics return . size ( ) ; subtask index ++ ) { test partition discoverer partition discoverer = new test partition discoverer ( topics descriptor @$ subtask index @$ mock get all partitions for topics return . size ( ) @$ test partition discoverer . create mock get all topics sequence from fixed return ( collections . singleton list ( test_topic ) ) @$ test partition discoverer . create mock get all partitions from topics sequence from fixed return ( mock get all partitions,logic meet,success,pre
change the mob compaction <PLACE_HOLDER> size,conf . set long ( mob constants . mob_compaction_mergeable_threshold @$ merge size ) ; common policy test logic ( __str__ @$ mob compact partition policy . weekly @$ false @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ true ) ;,compaction merge,success,pre
otherwise we saw a transaction <PLACE_HOLDER> our coins @$ but we did n't try and <PLACE_HOLDER> them ourselves yet . the outputs are already marked as spent by the connect call above @$ so check if there are any more for us to use . move if not .,if ( result == transaction input . connection result . success ) { transaction connected = check not null ( input . get connected transaction ( ) ) ; log . info ( __str__ @$ input . get outpoint ( ) @$ tx . get tx id ( ) ) ; maybe move pool ( connected @$ __str__ ) ; if ( output . is mine or watched ( this ) ) { check state ( my unspents . remove ( output ) ) ; } },transaction send,fail,pre
end if found better end for left rules end for left state <PLACE_HOLDER> right restricted rules,for ( int right state = __num__ ; right state < num states ; right state ++ ) { int narrowl = narrowl extent_end [ right state ] ; if ( narrowl <= start ) { continue ; } binary rule [ ] right rules = bg . split rules withrc ( right state ) ; for ( binary rule rule : right rules ) { int left child = rule . left child ; int narrowr = narrowr extent_start [ left child ] ; if ( narrowr > narrowl ) { continue ; } int min2 = widel extent_end [ right state ] ; int min = ( narrowr > min2 ? narrowr : min2 ) ; int max1 = wider extent_start [ left child ] ;,end do,success,pre
check that obj literal <PLACE_HOLDER> only names that are present in the message text,for ( string ph name : ph names ) { if ( ! used placeholders . contains ( ph name ) ) { throw new malformed exception ( __str__ + ph name @$ node ) ; } },literal had,fail,pre
current node already <PLACE_HOLDER> cleanup .,return ;,node performs,success,pre
if the <PLACE_HOLDER> directly <PLACE_HOLDER> the interface or extends it @$ return true,if ( iface . is assignable from ( get class ( ) ) ) { return true ; } else if ( iface . is assignable from ( delegate . get class ( ) ) ) { return true ; } else if ( wrapper . class . is assignable from ( delegate . get class ( ) ) ) { return ( ( wrapper ) unwrapp6 spy proxy ( ) ) . is wrapper for ( iface ) ; },proxy implements,fail,pre
instance info in response from peer will <PLACE_HOLDER> local registry call,create peer eureka node ( ) . heartbeat ( instance info . get app name ( ) @$ instance info . get id ( ) @$ instance info @$ null @$ false ) ; expect request type ( request type . batch ) ;,info enforce,fail,pre
key <PLACE_HOLDER> reset here,byte [ ] bytes = sortk1 . to byte array ( ) ; do assert ( bytes . length == __num__ && bytes [ __num__ ] == __num__ && bytes [ __num__ ] == __num__ && bytes [ __num__ ] == __num__ @$ __str__ ) ;,key set,fail,pre
initialize <PLACE_HOLDER> local work,local work = mrwork . get map red local work ( ) ; exec context . set local work ( local work ) ; mapred context . init ( true @$ new job conf ( jc ) ) ; mo . pass exec context ( exec context ) ; mo . initialize local work ( jc ) ; mo . initialize map operator ( jc ) ; if ( local work == null ) { return ; },initialize map,success,pre
check whether surface flinger spontaneously <PLACE_HOLDER> modes out from under us . schedule traversals to ensure that the correct state is reapplied if necessary .,if ( m active mode id != __num__ && m active mode id != active record . m mode . get mode id ( ) ) { m active mode invalid = true ; send traversal request locked ( ) ; } boolean records changed = records . size ( ) != m supported modes . size ( ) || modes added ;,flinger moves,fail,pre
if this path <PLACE_HOLDER> a root component the given path 's root must match,if ( ! this . root . equals ignore case ( other . root ) ) { return false ; },path has,success,pre
arbitrary and not worth documenting @$ as activity manager will <PLACE_HOLDER> this process shortly anyway .,return __num__ ;,manager kill,success,pre
force a 100 <PLACE_HOLDER> response .,jetty request . get http channel ( ) . send response ( http generator . continue_100_info @$ null @$ false ) ;,100 continue,success,pre
create a web hdfs <PLACE_HOLDER> system instance,uri uri = new uri ( __str__ + string . value of ( port ) ) ; configuration conf = new configuration ( ) ; web hdfs file system webfs = ( web hdfs file system ) file system . get ( uri @$ conf ) ; content summary summary = webfs . get content summary ( new path ( __str__ ) ) ; verify content summary ( sym link summary for dir contains fromdfs @$ summary ) ;,hdfs file,success,pre
the class in which <PLACE_HOLDER> widget is implemented,if ( connector bundle . is connected component connector ( type ) ) { j class type create widget class = connector bundle . find inherited method ( type @$ __str__ ) . get enclosing type ( ) ; j method get widget = connector bundle . find inherited method ( type @$ __str__ ) ; j class type widget type = get widget . get return type ( ) . is class ( ) ; if ( create widget class . get qualified source name ( ) . equals ( abstract component connector . class . get canonical name ( ) ) ) { if ( get widget . get enclosing type ( ) . get qualified source name ( ) . equals ( abstract component connector,which create,success,pre
if the user set a thread <PLACE_HOLDER> size at thread creation @$ then use that .,if ( stack size != __num__ ) { chosen stack size = stack size ; } else { final int default thread stack size = ( int ) x options . get xss ( ) . get value ( ) ; if ( default thread stack size != __num__ ) { chosen stack size = default thread stack size ; } },thread stack,success,pre
for dependencies that are missing we canonicalize and remap the target so we do n't suggest private <PLACE_HOLDER> labels .,set < jar owner > canonicalized missing = missing targets . stream ( ) . filter ( owner -> owner . label ( ) . is present ( ) ) . sorted ( comparator . comparing ( ( jar owner owner ) -> owner . label ( ) . get ( ) ) ) . map ( owner -> owner . with label ( owner . label ( ) . map ( label -> canonicalize target ( label ) ) ) ) . collect ( to immutable set ( ) ) ;,private build,success,pre
dw suggested buffer size specifies how large a buffer should be <PLACE_HOLDER>d to read this stream . typically @$ this contains a value corresponding to the largest chunk present in the stream . using the correct buffer size makes playback more efficient . <PLACE_HOLDER> zero if you do not know the correct buffer size .,d . write int ( tr . quality ) ;,present use,success,pre
call matrix <PLACE_HOLDER> listener if needed,if ( m matrix change listener != null ) { rectf display rect = get display rect ( matrix ) ; if ( display rect != null ) { m matrix change listener . on matrix changed ( display rect ) ; } },matrix changed,success,pre
the second line <PLACE_HOLDER> a ' b ' @$ so it needs more ascent and descent .,if ( m enabled ) { assert equals ( - __num__ * em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; } else { assert equals ( - em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; },line has,success,pre
current time should <PLACE_HOLDER> rotate of older active file,rotate . maybe rotate ( current time ) ;,time trigger,success,pre
mock the second split <PLACE_HOLDER> stream call .,when ( fake storage client . split read stream ( split read stream request . new builder ( ) . set original stream ( streams . get ( __num__ ) ) . set fraction ( __num__ ) . build ( ) ) ) . then return ( split read stream response . new builder ( ) . set primary stream ( streams . get ( __num__ ) ) . set remainder stream ( stream . new builder ( ) . set name ( __str__ ) ) . build ( ) ) ;,split read,success,pre
create the module provider ; this class <PLACE_HOLDER> a tile loader that actually loads the tile from the map file .,module provider = new maps forge tile module provider ( simple register receiver @$ from files @$ tile writer ) ;,class provides,success,pre
instant apps <PLACE_HOLDER> permission to create foreground services .,if ( r . app info . is instant app ( ) ) { final int mode = m am . m app ops service . check operation ( app ops manager . op_instant_app_start_foreground @$ r . app info . uid @$ r . app info . package name ) ; switch ( mode ) { case app ops manager . mode_allowed : break ; case app ops manager . mode_ignored : slog . w ( tag @$ __str__ + r . app info . package name + __str__ + __str__ ) ; return ; case app ops manager . mode_errored : throw new security exception ( __str__ + r . app info . package name + __str__ ) ; default : m am . enforce permission (,apps require,fail,pre
clear the underlying <PLACE_HOLDER> bits .,clear finished bits ( entry . get value ( ) ) ;,the finished,success,pre
set <PLACE_HOLDER> wal dir to true and use default values for other options .,util . start mini cluster ( start mini cluster option . builder ( ) . createwal dir ( true ) . build ( ) ) ;,set create,success,pre
fake ftp server <PLACE_HOLDER> timestamp precision in minutes . specify milliseconds precision so that test does not need to wait for minutes .,runner . set property ( list file . target_system_timestamp_precision @$ list file . precision_millis ) ; runner . assert valid ( ) ;,server expects,fail,pre
super call <PLACE_HOLDER> the connection preface we need to flush to send it this is called only on the client,ctx . flush ( ) ;,call receives,fail,pre
heartbeat <PLACE_HOLDER> no body,if ( message type != protocol constants . msgtype_heartbeat_request && message type != protocol constants . msgtype_heartbeat_response ) { codec codec = codec factory . get codec ( rpc message . get codec ( ) ) ; body bytes = codec . encode ( rpc message . get body ( ) ) ; compressor compressor = compressor factory . get compressor ( rpc message . get compressor ( ) ) ; body bytes = compressor . compress ( body bytes ) ; full length += body bytes . length ; },heartbeat has,success,pre
if the user specified a custom view for the tab indicators @$ then do not <PLACE_HOLDER> the bottom strips .,if ( ! m draw bottom strips ) { return ; },then draw,success,pre
gets set once @$ so doc and descendants <PLACE_HOLDER> first only,assert equals ( __str__ @$ doc . base uri ( ) ) ;,doc check,fail,pre
concurrent hash map does not <PLACE_HOLDER> synch . here,for ( abstract thread group thread group : groups ) { thread group . wait threads stopped ( ) ; },map need,success,pre
we accept property with targets type to be compatible with old jdks <PLACE_HOLDER> 6607163,if ( targets getter . is executed ( ) && ! targets getter . is disposed ( ) && ( targets getter . get actual type ( ) == x atom . xa_atom || targets getter . get actual type ( ) == x data transferer . targets_atom . get atom ( ) ) && targets getter . get actual format ( ) == __num__ ) { int count = targets getter . get number of items ( ) ; if ( count > __num__ ) { long atoms = targets getter . get data ( ) ; formats = new long [ count ] ; for ( int index = __num__ ; index < count ; index ++ ) { formats [ index ] = native .,jdks see,success,pre
if the first argument is const then just <PLACE_HOLDER> the flag and continue,if ( col name == null ) { is const = true ; prev const = ( ( expr node constant desc ) leaf ) . get value ( ) ; continue ; },argument mark,fail,pre
chrome has been killed @$ <PLACE_HOLDER> a download info .,if ( m download info == null ) { m download info = new download info . builder ( ) . set file name ( title ) . set description ( c . get string ( c . get column index ( download manager . column_description ) ) ) . set mime type ( c . get string ( c . get column index ( download manager . column_media_type ) ) ) . set content length ( long . parse long ( c . get string ( c . get column index ( download manager . column_total_size_bytes ) ) ) ) . build ( ) ; },chrome recreate,fail,pre
last parameter to copy from referenced method @$ <PLACE_HOLDER> final var args,int last = local context . needs var args conversion ( ) ? impl size - __num__ : impl size ;,parameter has,fail,pre
reset the stream @$ but set a lower limit . writing beyond the limit should <PLACE_HOLDER> an exception,stream . reset ( size - __num__ ) ; assert true ( __str__ @$ ( stream . get limit ( ) == size - __num__ ) ) ; caught exception = false ; try { stream . write ( input @$ __num__ @$ size ) ; } catch ( exception e ) { caught exception = true ; } assert true ( __str__ @$ caught exception ) ;,limit throw,success,pre
other errors <PLACE_HOLDER> a full log .,log . error ( __str__ @$ service != null ? ( service . to string ( ) + __str__ + service . get service state ( ) ) : __str__ @$ thrown ) ; exit exception = convert to exit exception ( thrown ) ;,errors generate,fail,pre
selection <PLACE_HOLDER> navigation,change selection to navigate ( table @$ __num__ @$ __num__ ) ; run swing ( ( ) -> get providers ( ) [ __num__ ] . close component ( ) ) ; assert equals ( addr ( __str__ ) @$ cb . get current address ( ) ) ;,selection affects,fail,pre
wait until the connection used by res 1 is returned to the pool @$ so that the next request <PLACE_HOLDER> the connection .,while ( ! connection returned to pool ) { condition . await ( ) ; } lock . unlock ( ) ;,request reassigns,fail,pre
mimic a task failure ; setting up the task for cleanup simulates the abort protocol to be played . without checks in the framework @$ this will fail as the committer will <PLACE_HOLDER> a commit to happen for the cleanup task .,task . set task cleanup task ( ) ; my umbilical umbilical = new my umbilical ( ) ; task . run ( job @$ umbilical ) ; assert true ( __str__ @$ umbilical . task done ) ;,committer auction,fail,pre
if only implicit defaults exist @$ <PLACE_HOLDER> a null default trump default primitives . this makes it so if there is a nullable object and a primitive in a group @$ the default value will be to null out the object .,if ( default attribute == null || has explicit default ( attribute ) || attribute . has set nullability ( ) ) { default attribute = attribute ; },primitives let,fail,pre
at this point @$ the configuration 's size needs to be taken into account as not all configurations <PLACE_HOLDER> all values .,int screen layout = __num__ ; int ui mode = __num__ ; int smallest screen width dp = __num__ ; int screen width dp = __num__ ; int screen height dp = __num__ ; byte [ ] locale script = new byte [ __num__ ] ; byte [ ] locale variant = new byte [ __num__ ] ; byte screen layout2 = __num__ ; byte screen config pad1 = __num__ ; short screen config pad2 = __num__ ; if ( size >= screen_config_min_size ) { screen layout = unsigned bytes . to int ( buffer . get ( ) ) ; ui mode = unsigned bytes . to int ( buffer . get ( ) ) ; smallest screen width dp = buffer . get short ( ),configurations use,fail,pre
view <PLACE_HOLDER> low padding area :,if ( view min < padding min ) { first view = view ; if ( m focus scroll strategy == base grid view . focus_scroll_page ) { while ( prepend one column visible items ( ) ) { circular int array positions = m grid . get item positions in rows ( m grid . get first visible index ( ) @$ pos ) [ row ] ; first view = find view by position ( positions . get ( __num__ ) ) ; if ( view max - get view min ( first view ) > client size ) { if ( positions . size ( ) > __num__ ) { first view = find view by position ( positions . get ( __num__ ) ),view has,fail,pre
check that resource exhaustion <PLACE_HOLDER> an exception,try { m ip sec service . reserve net id ( ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { },exhaustion throws,fail,pre
completing the human task a should <PLACE_HOLDER> the stage as completable,cmmn task service . complete ( taska . get id ( ) ) ; plan item instance stage1 plan item instance = cmmn runtime service . create plan item instance query ( ) . plan item instance name ( __str__ ) . single result ( ) ; assert that ( stage1 plan item instance . is completable ( ) ) . is true ( ) ; assert that ( cmmn runtime service . create plan item instance query ( ) . stage instance id ( stage1 plan item instance . get id ( ) ) . list ( ) ) . is not empty ( ) ;,a mark,success,pre
if this is a parent pr @$ create the bucket @$ possibly going over redundancy . we need to do this so that we can create the child region in this member . this member may <PLACE_HOLDER> the latest data for the child region .,if ( has persistent child region ( ) ) { result = partitioned region . get data store ( ) . grab bucket ( bid @$ get distribution manager ( ) . get distribution manager id ( ) @$ true @$ true @$ false @$ null @$ true ) ; } else { if ( this . partitioned region . is shadowpr ( ) && this . partitioned region . get colocated with ( ) != null ) { partitioned region colocated region = colocation helper . get colocated region ( this . partitioned region ) ; if ( this . partitioned region . get data policy ( ) . with persistence ( ) && ! objects . require non null ( colocated region ) . get data,member have,success,pre
currently there is no way to stop the meta store service . it will be stopped when the test <PLACE_HOLDER> exits . this is how other tests are also using meta store server .,hive server2 . stop ( ) ; set started ( false ) ; try { if ( llap cluster != null ) { llap cluster . stop ( ) ; } if ( mr != null ) { mr . shutdown ( ) ; mr = null ; } if ( dfs != null ) { dfs . shutdown ( ) ; dfs = null ; } } catch ( io exception e ) { },test test,fail,pre
some will <PLACE_HOLDER> 1 more than everything else .,int num high = ( int ) ( total - ( math . floor ( mean ) * count ) ) ; int num low = ( int ) ( count - num high ) ; min = ( num high * ( math . ceil ( mean ) - mean ) ) + ( num low * ( mean - math . floor ( mean ) ) ) ;,some return,fail,pre
make sure our set text call did not <PLACE_HOLDER> the window to be created,matching window = text field . get active matching window ( ) ; assert null ( __str__ @$ matching window ) ;,call trigger,success,pre
build up the state index . the bg & ug both <PLACE_HOLDER> a set count of states .,build state index ( ) ; build grammars ( ) ;,bg have,fail,pre
check the <PLACE_HOLDER> up key having only one memory segment,check argument ( key . get segments ( ) . length == __num__ ) ; final int hash code1 = key . hash code ( ) ; int new pos = hash code1 & num buckets mask ;,the look,fail,pre
if the layer does n't <PLACE_HOLDER> a drawable or unresolved theme attribute for a drawable @$ attempt to parse one from the child element . if multiple child elements exist @$ we 'll only use the first one .,if ( layer . m drawable == null && ( layer . m theme attrs == null ) ) { while ( ( type = parser . next ( ) ) == xml pull parser . text ) { } if ( type != xml pull parser . start_tag ) { throw new xml pull parser exception ( parser . get position description ( ) + __str__ + __str__ ) ; } layer . m drawable = drawable . create from xml inner for density ( r @$ parser @$ attrs @$ m layer state . m src density override @$ theme ) ; layer . m drawable . set callback ( this ) ; state . m children changing configurations |= layer . m drawable . get,layer have,success,pre
to be . note however that in the context of this function @$ the block can claim to be as difficult as it wants to be ... . if somebody was able to take control of our network connection and fork us onto a different chain @$ they could send us valid blocks with ridiculously easy difficulty and this function would <PLACE_HOLDER> them .,big integer target = get difficulty target as integer ( ) ; big integer h = get hash ( ) . to big integer ( ) ; if ( h . compare to ( target ) > __num__ ) { if ( throw exception ) throw new verification exception ( __str__ + get hash as string ( ) + __str__ + target . to string ( __num__ ) ) ; else return false ; },function handle,fail,pre
if this is a dead notification @$ then <PLACE_HOLDER> the controller if it 's really failed,if ( status event . get state ( ) == z wave node state . dead ) { z controller . request is failed node ( node . get node id ( ) ) ; },then notify,fail,pre
previous diff should <PLACE_HOLDER> .text to the view,invoke later ( prev diff ) ; view set . add range ( addr ( __str__ ) @$ addr ( __str__ ) ) ; assert equals ( addr ( __str__ ) @$ cb . get current address ( ) ) ; assert equals ( addr ( __str__ ) @$ get diff address ( ) ) ; assert equals ( cb . get current selection ( ) @$ new program selection ( addr ( __str__ ) @$ addr ( __str__ ) ) ) ; assert equals ( view set @$ cb . get view ( ) ) ;,diff add,success,pre
wer <PLACE_HOLDER> a successful connection,c . connect blocking ( ) ; c . close blocking ( ) ; http headers . put ( __str__ @$ __str__ ) ; c = new example client ( new uri ( __str__ ) @$ http headers ) ;,wer gets,fail,pre
bind <PLACE_HOLDER> method,c = new method closure ( this @$ __str__ ) ; super . set variable ( __str__ @$ c ) ;,bind generated,fail,pre
var has constant value @$ <PLACE_HOLDER> a literal .,if ( var . get constant value ( ) != null ) { return tree util . new literal ( var . get constant value ( ) @$ type util ) ; },var return,success,pre
generating cpu quota before the query finishes . this assertion <PLACE_HOLDER> cpu update during quota generation .,root . generate cpu quota ( __num__ ) ; stream . of ( root @$ child ) . for each ( group -> assert within cpu limit ( group @$ __num__ ) ) ;,assertion requires,fail,pre
enable table @$ use <PLACE_HOLDER> assignment to assign regions .,admin . enable table ( table name ) . join ( ) ; list < h region location > regions2 = async meta table accessor . get tableh region locations ( meta table @$ table name ) . get ( ) ;,use retain,success,pre
loaded key must <PLACE_HOLDER> stored one,byte [ ] bytesa2 = store . key util ( ) . identity key to bytes ( keya2 ) ; assert true ( __str__ @$ arrays . equals ( bytesa1 @$ bytesa2 ) ) ;,key match,fail,pre
iteration of inodes <PLACE_HOLDER> exclusive lock,begin write ( ) ;,iteration awaiting,fail,pre
the service group <PLACE_HOLDER> an unknown cluster !,if ( candidate services == null ) { _log . error ( __str__ + colo cluster variant ) ; return exception_exit_code ; },group had,success,pre
if put if absent fails @$ opportunistically <PLACE_HOLDER> its return value,for ( ; ; ) { v new value = remapping function . apply ( key @$ old value ) ; if ( new value != null ) { if ( old value != null ) { if ( replace ( key @$ old value @$ new value ) ) return new value ; } else if ( ( old value = put if absent ( key @$ new value ) ) == null ) return new value ; else continue have old value ; } else if ( old value == null || remove ( key @$ old value ) ) { return null ; } continue retry ; },opportunistically use,success,pre
same stream @$ <PLACE_HOLDER> sequences .,if ( stream . equals ( other start . stream ) ) { final map < partition id type @$ sequence offset type > new map = new hash map < > ( partition sequence number map ) ; new map . put all ( other start . partition sequence number map ) ; final set < partition id type > new exclusive partitions = new hash set < > ( ) ; partition sequence number map . for each ( ( partition id @$ sequence offset ) -> { if ( exclusive partitions . contains ( partition id ) && ! other start . partition sequence number map . contains key ( partition id ) ) { new exclusive partitions . add ( partition id ) ;,stream consider,fail,pre
only the legacy token <PLACE_HOLDER> impact on that capability,generate new token ( wc @$ __str__ @$ __str__ ) ; check combination with config and method for legacy token creation ( config @$ wc @$ user ) ;,token has,fail,pre
a serializable function can only return an object type @$ so if the <PLACE_HOLDER> fn parameter is a primitive type @$ then box it for the return . the return type will be unboxed before being forwarded to the <PLACE_HOLDER> fn parameter .,if ( output type . get raw type ( ) . is primitive ( ) ) { output type = type descriptor . of ( primitives . wrap ( output type . get raw type ( ) ) ) ; },the do,success,pre
if the length value is wrong or not all data made it to disk this read will not complete correctly . there could be overflow @$ underflow etc . so use a try finally block to indicate that all partitions are now corrupt . the enclosing exception handlers will <PLACE_HOLDER> the right thing wrt to propagating the error and closing the file .,boolean completed read = false ; int checksum start position = __num__ ; int row count = __num__ ; try { c . b ( ) . clear ( ) ; if ( is compressed ( ) ) { c . b ( ) . limit ( next chunk length + m_table header . capacity ( ) + __num__ ) ; } else { c . b ( ) . limit ( ( next chunk length - __num__ ) + m_table header . capacity ( ) ) ; } m_table header . position ( __num__ ) ; c . b ( ) . put ( m_table header ) ; c . b ( ) . position ( c . b ( ) . position ( ) + __num__,handlers do,success,pre
client endpoint <PLACE_HOLDER> eof and shutdown input as result,assert equals ( - __num__ @$ client . get input stream ( ) . read ( ) ) ; client . shutdown input ( ) ;,endpoint sends,fail,pre
seek to the end just before the last page of stream to <PLACE_HOLDER> the duration .,long last page search position = end position - ogg page header . max_page_size ; if ( last page search position > position before seek to end ) { return last page search position ; },page get,success,pre
open tab may <PLACE_HOLDER> an exception when the tab is not found .,outer . open tab ( __str__ ) ;,tab throw,success,pre
add parameters from the configuration in the job trace the reason why the job configuration parameters @$ as seen in the jobconf file @$ are added first because the specialized values obtained from rumen should <PLACE_HOLDER> the job conf values .,for ( map . entry < object @$ object > entry : job . get job properties ( ) . get value ( ) . entry set ( ) ) { job conf . set ( entry . get key ( ) . to string ( ) @$ entry . get value ( ) . to string ( ) ) ; },values override,success,pre
scroll down and verify that the old elements do n't <PLACE_HOLDER> the stylename any more,get grid element ( ) . get row ( __num__ ) ; assert false ( has css class ( row2 @$ __str__ ) ) ; assert false ( has css class ( cell4_2 @$ __str__ ) ) ;,elements have,success,pre
start definitions <PLACE_HOLDER> element,xtw . write start element ( element_definitions ) ; xtw . set default namespace ( cmmn_namespace ) ; xtw . write default namespace ( cmmn_namespace ) ; xtw . write namespace ( xsi_prefix @$ xsi_namespace ) ; xtw . write namespace ( flowable_extensions_prefix @$ flowable_extensions_namespace ) ; xtw . write namespace ( cmmndi_prefix @$ cmmndi_namespace ) ; xtw . write namespace ( omgdc_prefix @$ omgdc_namespace ) ; xtw . write namespace ( omgdi_prefix @$ omgdi_namespace ) ; for ( string prefix : model . get namespaces ( ) . key set ( ) ) { if ( ! default namespaces . contains ( prefix ) && string utils . is not empty ( prefix ) ) xtw . write namespace ( prefix @$ model . get namespaces (,definitions root,success,pre
in addition to changing the containing scope @$ inlining function declarations also <PLACE_HOLDER> the function name scope from the containing scope to the inner scope .,if ( is function declaration ) { compiler . report change to change scope ( value ) ; compiler . report change to enclosing scope ( value . get parent ( ) ) ; },declarations change,fail,pre
do n't create future with sequenced future manager . otherwise session would receive discontinued sequence number @$ and it would make future work item 'keeping call sequence when session <PLACE_HOLDER> commands ' impossible .,return session result . create future with result ( result_error_permission_denied ) ;,session execute,success,pre
target address may not <PLACE_HOLDER> connected address,dc . send ( bb @$ p . get socket address ( ) ) ;,address contain,fail,pre
l l e s <PLACE_HOLDER> j \u 0000 m i,byte [ ] expected = { ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__,s t,success,pre
we do n't know what the last varbit <PLACE_HOLDER> @$ so we just hit the end @$ then set it for future iterations,log . debug ( __str__ @$ i ) ; num varbits = i ; break ;,varbit is,success,pre
running the 'reset expired ' logic should <PLACE_HOLDER> no effect @$ the lock time is not yet passed,expired jobs = management service . execute command ( new find expired jobs cmd ( expired jobs pages size @$ job service configuration . get job entity manager ( ) ) ) ; assert equals ( __num__ @$ expired jobs . size ( ) ) ; assert job details ( true ) ;,logic have,success,pre
id @$ input string @$ <PLACE_HOLDER> string,string data [ ] = { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ;,id expected,fail,pre
avoid some images do not <PLACE_HOLDER> a size .,if ( size > __num__ ) { point . x = __num__ ; point . y = __num__ ; },images have,success,pre
unbuffered . will not <PLACE_HOLDER> naughty tricks with the file position .,data input stream metadata = new data input stream ( stream ) ;,unbuffered do,fail,pre
store the checksum for thw <PLACE_HOLDER> batch so that on recovery we know if we have a consistent <PLACE_HOLDER> batch on disk .,recovery file . write long ( checksum . get value ( ) ) ;,checksum write,success,pre
the processor does n't <PLACE_HOLDER> the consumer span which has already finished,span processor span = take span ( consumer spans ) ; assert that ( processor span . id ( ) ) . is not equal to ( consumer span . id ( ) ) ;,processor take,fail,pre
the parsed string <PLACE_HOLDER> a relative part,if ( ! relative contributors . is empty ( ) ) { array memory relative = array memory . create hashed ( relative_contributor_names . length ) ; array . ref of index ( __str__ ) . assign ( relative ) ; arrays . stream ( relative_contributor_names ) . for each ( name -> { long value = relative contributors . get or default ( name @$ __num__ ) ; relative . ref of index ( name ) . assign ( value ) ; } ) ; arrays . stream ( relative_bool_contributor_names ) . filter ( relative contributors :: contains key ) . for each ( name -> relative . ref of index ( name ) . assign ( memory . true ) ) ; } return array ;,string has,success,pre
problem when shutdown is called on the cloud eureka client where the application info manager bean is requested but wont be allowed beca<PLACE_HOLDER> we are shutting down . to avoid this we <PLACE_HOLDER> the object directly .,application info manager app manager ; if ( aop utils . is aop proxy ( manager ) ) { app manager = proxy utils . get target object ( manager ) ; } else { app manager = manager ; } cloud eureka client cloud eureka client = new cloud eureka client ( app manager @$ config @$ this . optional args @$ this . context ) ; cloud eureka client . register health check ( health check handler ) ; return cloud eureka client ;,problem use,success,pre
if prehash <PLACE_HOLDER> same call with sha 1 to check expected code .,if ( pre hash && ! proc name . equals ( __str__ ) ) { params . put ( __str__ @$ get hashed password forhttp var ( password @$ client auth scheme . hash_sha1 ) ) ; call proc overjson raw ( params @$ http port @$ expected code @$ session id ) ; },prehash make,success,pre
if this is a property on an object literal @$ always <PLACE_HOLDER> an initialization somewhere,if ( name . get parent ( ) . is object literal ( ) ) { return true ; },somewhere do,fail,pre
read default <PLACE_HOLDER> object indicator,boolean called default write object = read boolean ( ) ; read object state . begin unmarshal custom value ( this @$ called default write object @$ ( current class desc . read object method != null ) ) ; if ( current class desc . has read object ( ) ) set state ( in_read_object_remote_not_custom_marshaled ) ;,default write,success,pre
try to connect client 2 with no credentials verify that the creation of region <PLACE_HOLDER> security exception,if ( gen . class code ( ) . equals ( credential generator . class code . ssl ) ) { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ noforce_authreq_exception ) ) ; client2 . invoke ( ( ) -> do puts ( __num__ @$ other_exception ) ) ; } else { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ authreq_exception ) ) ; },creation throws,success,pre
if the user <PLACE_HOLDER> the api token @$ we can delete it,if ( revoked . is legacy ( ) ) { p . api token = null ; },user invalidated,fail,pre
volatile image we immediately punt in subclasses . if this poses a problem we 'll <PLACE_HOLDER> a more sophisticated detection algorithm @$ or api .,this ( buffer_strategy_specified_off ) ;,image find,fail,pre
clear memory which corresponds to all code <PLACE_HOLDER> no need to remove context since this will happen when instruction is added,if ( overwrite ) { for ( address range range : instruction set . get address set ( ) ) { clear code units ( range . get min address ( ) @$ range . get max address ( ) @$ false @$ task monitor adapter . dummy_monitor ) ; } } else { check instruction set ( instruction set @$ skip delay slots ) ; },memory cuts,fail,pre
collect all metadata @$ newer values <PLACE_HOLDER> older values,if ( is newer ( res @$ val ) ) { meta = merge meta ( val . get meta data ( ) @$ meta ) ; res . set ( val ) ; } else { meta = merge meta ( meta @$ val . get meta data ( ) ) ; },values override,success,pre
after setting the playback speed @$ reset m is <PLACE_HOLDER> flag .,m is rewinding = false ;,m creating,fail,pre
selection and tap can <PLACE_HOLDER> cursor from this tap position .,final float eventx = event . getx ( ) ; final float eventy = event . gety ( ) ; final boolean is mouse = event . is from source ( input device . source_mouse ) ; switch ( event . get action masked ( ) ) { case motion event . action_down : if ( extracted text mode will be started ( ) ) { hide ( ) ; } else { m min touch offset = m max touch offset = m text view . get offset for position ( eventx @$ eventy ) ; if ( m gesture stayed in tap region ) { if ( m tap state == tap_state_double_tap || m tap state == tap_state_triple_click ) { final float deltax = eventx -,selection move,success,pre
make the consumer <PLACE_HOLDER> a single message for each partition,when ( consumer mock . poll ( any long ( ) ) ) . then return ( new consumer records < > ( collections . singleton map ( partition that will be revoked @$ spout with mocked consumer setup helper . create records ( partition that will be revoked @$ __num__ @$ __num__ ) ) ) ) . then return ( new consumer records < > ( collections . singleton map ( assigned partition @$ spout with mocked consumer setup helper . create records ( assigned partition @$ __num__ @$ __num__ ) ) ) ) . then return ( new consumer records < > ( collections . empty map ( ) ) ) ;,consumer exce,fail,pre
this conditional is for test acl <PLACE_HOLDER> application where app is rejected and no app is added .,if ( scheduler . get scheduler applications ( ) . contains key ( id . get application id ( ) ) ) { scheduler . add application attempt ( id @$ false @$ false ) ; } list < resource request > ask = new array list < > ( requests ) ; rm app rm app = mock ( rm app . class ) ; rm app attempt rm app attempt = mock ( rm app attempt . class ) ; when ( rm app . get current app attempt ( ) ) . then return ( rm app attempt ) ; when ( rm app attempt . getrm app attempt metrics ( ) ) . then return ( new rm app attempt metrics ( id @$,acl submit,success,pre
test that it works when the request timeout <PLACE_HOLDER> the outstanding request ...,test shutdown request outstanding ( __num__ @$ __num__ @$ remote invocation exception . class @$ exception . class ) ;,timeout kills,success,pre
closing region with colocated regions will <PLACE_HOLDER> an exception and the region will not be closed .,accessor . invoke ( ( ) -> pr colocationd unit test . close region with colocated regions ( customer partitioned region name @$ false ) ) ;,region throw,success,pre
this is to test that the background thread created by the adapter is <PLACE_HOLDER> the stream objects properly . if it were not @$ the program would crash after this method exits and its outer autorelease pool drains .,assert not null ( stream ) ;,thread collecting,fail,pre
middle word <PLACE_HOLDER> integer rounding ; lower longword is cleared .,if ( abs round power < two_x_longword_decimal_digits ) { final int adjusted abs power = abs round power - longword_decimal_digits ; final long round factor = power of ten table [ adjusted abs power ] ; result0 = __num__ ; result1 = ( ( fast1 / round factor ) * round factor ) ; result2 = fast2 ; } else { final int adjusted abs power = abs round power - two_x_longword_decimal_digits ; final long round factor = power of ten table [ adjusted abs power ] ; result0 = __num__ ; result1 = __num__ ; result2 = ( ( fast2 / round factor ) * round factor ) ; },word gets,success,pre
the user asked for stats to be collected . some stats like number of rows <PLACE_HOLDER> a scan of the data however @$ some other stats @$ like number of files @$ do not <PLACE_HOLDER> a complete scan update the stats which do not <PLACE_HOLDER> a complete scan .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },which require,success,pre
get the point which are near the cell <PLACE_HOLDER> the polygon and outside of the polygon,m_points for polygons . add ( geography point value . normalize lng lat ( center longitude + radius in degrees @$ center latitude + radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude - radius in degrees @$ center latitude + radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude - radius in degrees @$ center latitude - radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude + radius in degrees @$ center latitude - radius in degrees ) ) ;,which intersecting,fail,pre
value that has been set is new or modified bring the binary in sync so that the deserialization <PLACE_HOLDER> the correct result,if ( offset == modified_indicator_offset ) { update binary represenation ( ) ; offset = this . offsets [ field num ] ; },deserialization gives,success,pre
suppress all application not <PLACE_HOLDER> exception for now .,continue ;,suppress found,success,pre
if the qualifier expression <PLACE_HOLDER> problems @$ give up attribution of method reference,if ( expr type . is erroneous ( ) ) { result = that . type = expr type ; return ; },expression has,fail,pre
total dirs <PLACE_HOLDER> root directory,assert equals ( dir count + __num__ @$ total dirs ) ; file status max file = collections . max ( written files . values ( ) @$ new comparator < file status > ( ) { @ override public int compare ( file status first @$ file status second ) { return first . get len ( ) < second . get len ( ) ? - __num__ : ( ( first . get len ( ) == second . get len ( ) ) ? __num__ : __num__ ) ; } } ) ; p = pattern . compile ( __str__ ) ; matcher = p . matcher ( output . to string ( __str__ ) ) ; assert true ( matcher . find ( ),dirs includes,success,pre
sometimes safari does not <PLACE_HOLDER> checkbox correctly when attaching . setting the visibility to hidden and a bit later restoring will make everything just fine .,if ( browser info . get ( ) . is safari ( ) ) { get element ( ) . get style ( ) . set visibility ( style . visibility . hidden ) ; scheduler . get ( ) . schedule finally ( ( ) -> { get element ( ) . get style ( ) . set visibility ( style . visibility . visible ) ; } ) ; },safari render,success,pre
<PLACE_HOLDER> integer & null objects <PLACE_HOLDER> integer & null objects,bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection,integer copy,fail,pre
only un<PLACE_HOLDER> <PLACE_HOLDER> entries .,reader = new filtered data entry reader ( new data entry name filter ( new extension matcher ( __str__ ) ) @$ zip reader @$ reader ) ;,unzip apk,fail,pre
methods with 1 implementations do not <PLACE_HOLDER> a vtable because invokes can be done as direct calls without the <PLACE_HOLDER> for a vtable . methods with 0 implementations are unreachable .,if ( method . wrapped . is invoked ( ) || method . wrapped . is implementation invoked ( ) ) { if ( method . implementations . length > __num__ ) { int slot = find slot ( method @$ vtables map @$ used slots map @$ vtables slots ) ; method . vtable index = slot ; assign implementations ( method . get declaring class ( ) @$ method @$ slot @$ vtables map ) ; } },methods reference,fail,pre
phreak must also <PLACE_HOLDER> the lt from the rule network evaluator,if ( match . get rule agenda item ( ) != null ) { if ( left tuple . get memory ( ) != null ) { left tuple . get memory ( ) . remove ( left tuple ) ; } },phreak remove,success,pre
if the request capacity does not <PLACE_HOLDER> reallocation @$ just update the length of the memory .,if ( ! chunk . unpooled ) { if ( new capacity > length ) { if ( new capacity <= max length ) { length = new capacity ; return this ; } } else if ( new capacity > max length > > > __num__ && ( max length > __num__ || new capacity > max length - __num__ ) ) { length = new capacity ; trim indices to capacity ( new capacity ) ; return this ; } },capacity need,fail,pre
change the mob compaction <PLACE_HOLDER> size,conf . set long ( mob constants . mob_compaction_mergeable_threshold @$ merge size ) ; common policy test logic ( __str__ @$ mob compact partition policy . monthly @$ false @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } @$ true ) ;,compaction merge,success,pre
note : another sanitize because camelize can <PLACE_HOLDER> an invalid name,return sanitize kotlin specific names ( modified ) ;,camelize give,fail,pre
current logic <PLACE_HOLDER> at 30 @$ so set value to max,if ( count >= __num__ ) { return m max sleep ms ; } else { int sleep ms = m base sleep time ms * ( thread local random . current ( ) . next int ( __num__ << count @$ __num__ << ( count + __num__ ) ) ) ; return math . min ( abs ( sleep ms @$ m max sleep ms ) @$ m max sleep ms ) ; },logic sleep,fail,pre
store it because need elements of it to <PLACE_HOLDER> header in output,stored header = line ;,elements put,fail,pre
should only clear this if the last schema <PLACE_HOLDER> identity constraints .,if ( ! f may match field map . is empty ( ) ) { f may match field map . clear ( ) ; },schema violated,fail,pre
ensure the invariant that either all main accounts <PLACE_HOLDER> a password set @$ or none .,if ( account one password set != string utils . is not empty ( account two password ) || account one password set != string utils . is not empty ( account three password ) ) { throw new illegal argument exception ( ) ; },accounts have,success,pre
now the real test ! <PLACE_HOLDER> up clients to be able to pick up correct tokens .,security util . set security info providers ( new custom security info ( ) ) ; token < client toam token identifier > token = converter utils . convert from yarn ( original client toam token @$ am . address ) ;,! set,success,pre
host must <PLACE_HOLDER> real keyboard focus .,if ( ! m host . is focused ( ) && ! m host . request focus ( ) ) { return false ; },host have,success,pre
since this was sorted in reverse order @$ index ' 0 ' contains the highest ai @$ and index ' 1 ' <PLACE_HOLDER> us the index of the corresponding sentence in its array .,ay eye ray [ sentence number ] = all anonymity indices . get ( sentence number ) [ quality rank ] [ __num__ ] ;,ai tells,success,pre
verify that new job requests <PLACE_HOLDER> no issues .,job runnable = submit concurrent jobs ( __num__ @$ config @$ false @$ false @$ submit job helper . get delayed resonse answer ( __num__ @$ __num__ ) @$ kill job helper . get delayed resonse answer ( __num__ @$ status bean ) @$ __str__ ) ; assert true ( job runnable . exception == null ) ;,requests have,success,pre
aggregates with more aggregate functions <PLACE_HOLDER> a bit more,float multiplier = __num__ + ( float ) agg calls . size ( ) * __num__ ; for ( aggregate call agg call : agg calls ) { if ( agg call . get aggregation ( ) . get name ( ) . equals ( __str__ ) ) { multiplier += __num__ ; } },aggregates need,fail,pre
make sure id cache is filled so that initial allocations wo n't <PLACE_HOLDER> high id unnecessarily .,try ( indexed id generator freelist = new indexed id generator ( page cache @$ directory . file ( __str__ ) @$ immediate ( ) @$ id type . node @$ false @$ ( ) -> __num__ @$ long . max_value @$ false ) ) { freelist . maintenance ( ) ; race race = new race ( ) ; work sync < indexed id generator @$ ids > work sync = new work sync < > ( freelist ) ; for ( int t = __num__ ; t < threads ; t ++ ) { int thread = t ; race . add contestant ( throwing ( ( ) -> { int cursor = __num__ ; for ( int i = __num__ ; i < transactions_per_thread ;,allocations project,fail,pre
the system bars are visible . <PLACE_HOLDER> any desired adjustments to your ui @$ such as showing the action bar or other navigational controls .,mainui . set immersive mode ( false ) ; set immersive timer ( ) ; if ( my debug . log ) log . d ( tag @$ __str__ ) ;,system make,success,pre
create a second instance @$ which will <PLACE_HOLDER> up the same value,try ( s3a file system second delegate = news3a instance ( uri @$ conf ) ) { assert bound todt ( second delegate @$ token kind ) ; if ( encryption test enabled ( ) ) { assert not null ( __str__ @$ second delegate . get server side encryption algorithm ( ) ) ; assert equals ( __str__ @$ fs . get server side encryption algorithm ( ) @$ second delegate . get server side encryption algorithm ( ) ) ; } contract test utils . assert deleted ( second delegate @$ test path @$ true ) ; assert not null ( __str__ @$ second delegate . get delegation token ( __str__ ) ) ; },which pick,success,pre
read field <PLACE_HOLDER> constants,access_flags_offset = db . lookup int constant ( __str__ ) . int value ( ) ; name_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; signature_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; initval_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; low_offset = db . lookup int constant ( __str__ ) . int value ( ) ; high_offset = db . lookup int constant ( __str__ ) . int value ( ) ; field_slots = db . lookup int constant ( __str__ ) . int value ( ) ; fieldinfo_tag_size = db . lookup int constant ( __str__ ) . short value ( ) ;,field set,fail,pre
important : first <PLACE_HOLDER> factory methods ; then constructors @$ so latter can override former !,_add factory creators ( ctxt @$ bean desc @$ vchecker @$ intr @$ creators @$ creator defs ) ;,first add,success,pre
instead @$ the calling code is responsible for the synchronization . <PLACE_HOLDER> 6784816 for details .,return get components_ no client code ( ) ;,synchronization see,success,pre
the address should <PLACE_HOLDER> logging.properties,deque < property > result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; model node handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; string file name = null ;,address have,success,pre
<PLACE_HOLDER>nt <PLACE_HOLDER> anything,;,dont do,success,pre
now we know the bytes <PLACE_HOLDER> something we need to rewrite :,class reader file reader = new class reader ( bytes ) ; rewrite class adaptor class adaptor = new rewrite class adaptor ( type registry ) ; try { file reader . accept ( class adaptor @$ class reader . skip_frames ) ; } catch ( dont rewrite exception drex ) { return bytes ; },bytes represents,fail,pre
we found the id . deleting the item via the content provider will also <PLACE_HOLDER> the file,if ( c . move to first ( ) ) { long id = c . get long ( c . get column index or throw ( media store . images . media . _id ) ) ; uri delete uri = content uris . with appended id ( media store . images . media . external_content_uri @$ id ) ; content resolver . delete ( delete uri @$ null @$ null ) ; } else { },id remove,success,pre
must use raw local because the checksummer does n't <PLACE_HOLDER> flushes .,file system fs = file system . get local ( conf ) . get raw ( ) ; object inspector inspector ; synchronized ( test orc file . class ) { inspector = object inspector factory . get reflection object inspector ( my row . class @$ object inspector factory . object inspector options . java ) ; } properties tbl props = new properties ( ) ; tbl props . set property ( __str__ @$ __str__ ) ; tbl props . set property ( __str__ @$ __str__ ) ; hive conf . set int var ( conf @$ hive conf . conf vars . hive_orc_base_delta_ratio @$ __num__ ) ; acid output format . options options = new acid output format . options ( conf ) . filesystem,checksummer honor,success,pre
when entity expansion limit is set by the application @$ we need to check for the entity expansion limit set by the parser @$ if number of entity expansions exceeds the entity expansion limit @$ parser will <PLACE_HOLDER> fatal error . note that this represents the nesting level of open entities .,f entity expansion count ++ ; if ( f limit analyzer != null ) { f limit analyzer . add value ( entity expansion index @$ name @$ __num__ ) ; } if ( f security manager != null && f security manager . is over limit ( entity expansion index @$ f limit analyzer ) ) { f security manager . debug print ( f limit analyzer ) ; f error reporter . report error ( xml message formatter . xml_domain @$ __str__ @$ new object [ ] { f security manager . get limit value by index ( entity expansion index ) } @$ xml error reporter . severity_fatal_error ) ; f entity expansion count = __num__ ; },parser return,fail,pre
the main thread will <PLACE_HOLDER> a notification,work unit wu = e . get current work unit ( ) ; if ( wu . is main work ( ) ) { if ( problems == null ) { future . set ( executable ) ; e . get owner ( ) . task completed ( e @$ task @$ duration ) ; } else { future . set ( problems ) ; e . get owner ( ) . task completed with problems ( e @$ task @$ duration @$ problems ) ; } },thread send,success,pre
if event <PLACE_HOLDER> this dimension to filter on,if ( event dimension transformed . contains ( filter dimension transformed ) || filter dimension transformed . contains ( event dimension transformed ) ) { set < string > event dimension values set = new hash set < > ( event dimension values transformed ) ; event dimension values set . retain all ( filtered dimension values transformed ) ; if ( ! event dimension values set . is empty ( ) ) { filtered events . add ( event ) ; event added = true ; break ; } },event has,success,pre
client <PLACE_HOLDER> an eds request containing all clusters being watched to management server .,verify ( request observer ) . on next ( arg that ( new discovery request matcher ( __str__ @$ immutable list . of ( __str__ @$ __str__ ) @$ xds client impl . ads_type_url_eds @$ __str__ ) ) ) ;,client sends,success,pre
the automation service can <PLACE_HOLDER> other services .,final user state user state = get user state locked ( resolved user id ) ; if ( m ui automation manager . suppressing accessibility services locked ( ) ) { return collections . empty list ( ) ; } final list < accessibility service connection > services = user state . m bound services ; final int service count = services . size ( ) ; final list < accessibility service info > result = new array list < > ( service count ) ; for ( int i = __num__ ; i < service count ; ++ i ) { final accessibility service connection service = services . get ( i ) ; if ( ( service . m feedback type & feedback type ) !=,service access,fail,pre
remove any that are constants @$ or expressions where all params exactly <PLACE_HOLDER> a group by expression :,if ( ! unmatched selects . is empty ( ) ) { throw new ksql exception ( __str__ + unmatched selects ) ; } final set view < column reference exp > unmatched selects agg = sets . difference ( aggregate analysis . get aggregate select fields ( ) @$ group by exprs ) ; if ( ! unmatched selects agg . is empty ( ) ) { throw new ksql exception ( __str__ + __str__ + unmatched selects agg ) ; } final set < column reference exp > having columns = aggregate analysis . get non aggregate having fields ( ) ; final set < column reference exp > having only = sets . difference ( having columns @$ group by exprs ) ; if (,params match,success,pre
there should be at least two iterations of this loop because reset thread loopers <PLACE_HOLDER> 'quit ' on background loopers once @$ which also resets the scheduler .,for ( int i = __num__ ; i < __num__ ; i ++ ) { assert that ( shadow of ( background looper ) . get scheduler ( ) . size ( ) ) . is equal to ( __num__ ) ; assert that ( shadow of ( background looper ) . get scheduler ( ) . get current time ( ) ) . is equal to ( __num__ ) ; handler . post ( empty ) ; handler . post delayed ( empty @$ __num__ ) ; shadow of ( background looper ) . run to end of tasks ( ) ; assert that ( shadow of ( background looper ) . get scheduler ( ) . get current time ( ) ) . is equal to,loopers calls,success,pre
second input parameter but 3 <PLACE_HOLDER> column .,return get column param string ( __num__ @$ arg1 column ) + __str__ + get column param string ( __num__ @$ arg3 column ) ;,parameter rd,success,pre
constructor is not visible and config seems to be null at this point @$ hence we can not use the <PLACE_HOLDER> method here,constructor < transfer manager builder > tmb constructor = transfer manager builder . class . get declared constructor ( config . class @$ transfer manager . class ) ; tmb constructor . set accessible ( true ) ; return tmb constructor . new instance ( config @$ transfer manager ) . with feature ( read after write consistent . class ) . as default ( ) ;,the get,fail,pre
only my parameter signature changed so auto <PLACE_HOLDER> my parameters .,get merge my ( ) . replace function parameters ( entry @$ monitor ) ;,auto merge,success,pre
add a new feed all new feeds will have the most recent item <PLACE_HOLDER> as unplayed,feed item most recent = new feed . get most recent item ( ) ; if ( most recent != null ) { most recent . set new ( ) ; } new feeds list . add ( new feed ) ; result feeds [ feed idx ] = new feed ; log . d ( tag @$ __str__ + new feed . get title ( ) + __str__ ) ; collections . sort ( new feed . get items ( ) @$ new feed item pubdate comparator ( ) ) ; if ( new feed . get page nr ( ) == saved feed . get page nr ( ) ) { if ( saved feed . compare with other ( new feed ) ) { log,feeds seen,fail,pre
note : parser not working properly here @$ use url note : raw live format could <PLACE_HOLDER> exoplayer,m listener . on dash url ( uri . parse ( m dashmpd url . to string ( ) ) ) ;,note give,fail,pre
n.b . missing format width exception is the only kind of illegal format exception whose constructor can <PLACE_HOLDER> and display arbitrary error message @$ hence its use below .,int length = pattern . length ( ) ; int arg length = arguments . size ( ) ; int i = __num__ ;,constructor throw,fail,pre
bypass <PLACE_HOLDER> expansion @$ but <PLACE_HOLDER>rmit p es to complete ... valid docs wo n't care .,try { for ( ; ; ) { int c = getc ( ) ; if ( c == __str__ ) { c = getc ( ) ; if ( c != __str__ ) { if ( save comment text ) str tmp . append ( __str__ ) ; ungetc ( ) ; continue ; } next char ( __str__ @$ __str__ @$ null ) ; break one comment ; } if ( save comment text ) str tmp . append ( ( char ) c ) ; } } catch ( end of input exception e ) { if ( in . is internal ( ) ) { error ( __str__ @$ null ) ; } fatal ( __str__ ) ; },bypass do,fail,pre
if the peer <PLACE_HOLDER> an initializing id @$ they are also not online .,if ( initializingid != null ) { remove newer persistentid ( offline members @$ initializingid ) ; } handle partially destroyed region ( offline members @$ members to wait for @$ persistentid @$ initializingid @$ disk storeid ) ;,peer defines,fail,pre
the handler <PLACE_HOLDER> a slice of the slice and is now done with it .,dup2 . release ( ) ;,handler created,success,pre
cancel currently executing tasks <PLACE_HOLDER> a while for tasks to respond to being cancelled,if ( ! executor . await termination ( __num__ @$ time unit . seconds ) ) { log . warn ( __str__ ) ; },tasks wait,success,pre
the slow loading resource on that page <PLACE_HOLDER> 6 seconds to return @$ but with 'none ' page loading strategy 'refresh ' operation should not wait .,assert that ( duration ) . as ( __str__ ) . is less than ( __num__ ) ;,resource takes,success,pre
without limiting to be inside the parent bounds @$ the out screen size should <PLACE_HOLDER> relative to the input bounds .,final activity record . compat display insets compat intsets = new activity record . compat display insets ( display content ) ; task . compute config resource overrides ( in out config @$ parent config @$ compat intsets ) ; assert equals ( ( short side - status bar height ) * density_default / parent config . density dpi @$ in out config . screen height dp ) ; assert equals ( long side * density_default / parent config . density dpi @$ in out config . screen width dp ) ; assert equals ( configuration . orientation_landscape @$ in out config . orientation ) ;,size stay,fail,pre
note : the python version <PLACE_HOLDER> slices which return an empty list when indexed beyond what the list contains . since we ca n't slice out an empty sublist in java @$ we must check if we 've reached the end and clear the fnames list manually .,if ( cnt == fnames . size ( ) ) { fnames . clear ( ) ; } else { fnames = fnames . sub list ( cnt @$ fnames . size ( ) ) ; } m con . publish progress ( string . format ( anki droid app . get app resources ( ) . get string ( r . string . sync_media_downloaded_count ) @$ m download count ) ) ;,version contains,fail,pre
now get the operator class which <PLACE_HOLDER> the operation,final class < ? extends driver < s @$ ot > > driver class = this . config . get driver ( ) ; this . driver = instantiation util . instantiate ( driver class @$ driver . class ) ; string head name = get environment ( ) . get task info ( ) . get task name ( ) . split ( __str__ ) [ __num__ ] . trim ( ) ; this . metrics = get environment ( ) . get metric group ( ) . get or add operator ( head name . starts with ( __str__ ) ? head name . substring ( __num__ ) : head name ) ; this . metrics . getio metric group ( ) . reuse input metrics,which performs,fail,pre
raw types <PLACE_HOLDER> this branch as well,if ( type instanceof class ) { return for class ( ( class < ? > ) type ) ; } else if ( type instanceof parameterized type ) { final parameterized type parameterized type = ( parameterized type ) type ; final type [ ] type arguments = parameterized type . get actual type arguments ( ) ; final java type definition [ ] generic bounds = new java type definition [ type arguments . length ] ; for ( int i = __num__ ; i < type arguments . length ; i ++ ) { generic bounds [ i ] = resolve type definition ( type arguments [ i ] @$ method @$ method type args ) ; } return for class ( ( class <,types take,success,pre
create the reveal animations that will run when the retreat <PLACE_HOLDER> them,for ( int i = __num__ ; i < steps ; i ++ ) { reveal animations [ i ] = new pending reveal animator ( was - i @$ new leftward start predicate ( dot centerx [ was - i ] ) ) ; dots to hide [ i ] = was - i ; } add update listener ( new animator update listener ( ) { @ override public void on animation update ( value animator value animator ) { retreating joinx2 = ( float ) value animator . get animated value ( ) ; if ( build . version . sdk_int >= build . version_codes . jelly_bean ) { post invalidate on animation ( ) ; } else { post invalidate ( ) ; },retreat passes,success,pre
check the base <PLACE_HOLDER> permission first . only do the recursion if base znode 's perms are not correct .,try { list < acl > actual acls = recoverable zoo keeper . get acl ( znode paths . basez node @$ new stat ( ) ) ; if ( ! is base znode acl setup ( actual acls ) ) { log . info ( __str__ ) ; set znode acls recursive ( znode paths . basez node ) ; } } catch ( keeper exception . no node exception nne ) { return ; } catch ( interrupted exception ie ) { interrupted exception no throw ( ie @$ false ) ; } catch ( io exception | keeper exception e ) { log . warn ( __str__ @$ e ) ; },base nn,fail,pre
first record <PLACE_HOLDER> header strings,if ( use headers ) { for ( int i = __num__ ; i < record . size ( ) && i < serieses . length ; i ++ ) { serieses [ i ] . set name ( record . get ( i ) ) ; } record = it . next ( ) ; },record contains,success,pre
compute the missing segments if there are at least two segments and the table <PLACE_HOLDER> time column,int num missing segments = __num__ ; int num segments = offline segmentzk metadata list . size ( ) ; segments validation and retention config validation config = table config . get validation config ( ) ; if ( segment interval utils . eligible for missing segment check ( num segments @$ validation config ) ) { list < interval > segment intervals = new array list < > ( num segments ) ; int num segments with invalid intervals = __num__ ; for ( offline segmentzk metadata offline segmentzk metadata : offline segmentzk metadata list ) { interval time interval = offline segmentzk metadata . get time interval ( ) ; if ( time interval != null && time utils . is valid time interval ( time,table has,success,pre
do abc <PLACE_HOLDER> 1 does not have name,continue ;,abc def,fail,pre
should we change the current code ? do we <PLACE_HOLDER> a code set ?,if ( code set == __num__ ) { switch ( new code set ) { case code_code_a : pattern index = code_start_a ; break ; case code_code_b : pattern index = code_start_b ; break ; default : pattern index = code_start_c ; break ; } } else { pattern index = new code set ; },code have,success,pre
if none of the arguments <PLACE_HOLDER> a name specified @$ we just send all the values as the entity body,if ( get send parameter values as post body ( ) ) { has entity body = true ; arguments arguments = get arguments ( ) ; string builder entity body content = new string builder ( arguments . get argument count ( ) * __num__ ) ; for ( j meter property j meter property : arguments ) { http argument arg = ( http argument ) j meter property . get object value ( ) ; if ( charset != null ) { entity body content . append ( arg . get encoded value ( charset ) ) ; } else { entity body content . append ( arg . get encoded value ( ) ) ; } } string entity request entity = new string,none have,success,pre
last name should <PLACE_HOLDER> no tooltip,check tooltip ( $ ( text field element . class ) . get ( __num__ ) @$ null ) ;,name have,success,pre
p 12 keystore currently does not <PLACE_HOLDER> separate store and entry passwords,if ( p12keystore . equals ignore case ( ks . get type ( ) ) ) { throw une2 ; } else { pkey = get key passwd ( alias @$ null @$ null ) ; pp = new password protection ( pkey ) ; entry = ks . get entry ( alias @$ pp ) ; },keystore support,success,pre
if actual dimensions do n't <PLACE_HOLDER> the declared size @$ reset everything .,if ( this . s width > __num__ && this . s height > __num__ && ( this . s width != s width || this . s height != s height ) ) { reset ( false ) ; if ( bitmap != null ) { if ( ! bitmap is cached ) { bitmap . recycle ( ) ; } bitmap = null ; if ( on image event listener != null && bitmap is cached ) { on image event listener . on preview released ( ) ; } bitmap is preview = false ; bitmap is cached = false ; } } this . decoder = decoder ; this . s width = s width ; this . s height = s height ;,dimensions match,success,pre
verify escaped partition names do n't <PLACE_HOLDER> partitions,exception thrown = false ; try { string bad part name = __str__ ; client . get partition ( db name @$ tbl name @$ bad part name ) ; } catch ( no such object exception e ) { exception thrown = true ; } assert true ( __str__ @$ exception thrown ) ; path part path = new path ( part . get sd ( ) . get location ( ) ) ; assert true ( fs . exists ( part path ) ) ; client . drop partition ( db name @$ tbl name @$ part . get values ( ) @$ true ) ; assert false ( fs . exists ( part path ) ) ;,names affect,fail,pre
dw size <PLACE_HOLDER> the size of the data chunk @$ in bytes .,idx1 chunk . finish ( ) ;,size specifies,success,pre
do bind <PLACE_HOLDER> all four flavors of binding,naming context data store impl = ( naming context data store ) this ; do bind ( impl @$ n @$ nc @$ false @$ binding type . ncontext ) ;,bind implements,success,pre
a task normally slowly <PLACE_HOLDER> scheduled time in a level and then moves to the next @$ but if the split had a particularly long quanta @$ accrue time to each level as if it had run in that level up to the level limit .,for ( int current level = old level ; current level < new level ; current level ++ ) { long time accrued to level = math . min ( seconds . to nanos ( level_threshold_seconds [ current level + __num__ ] - level_threshold_seconds [ current level ] ) @$ remaining level contribution ) ; add level time ( current level @$ time accrued to level ) ; remaining level contribution -= time accrued to level ; remaining task time -= time accrued to level ; } add level time ( new level @$ remaining level contribution ) ; long new level min priority = get level min priority ( new level @$ scheduled nanos ) ; return new priority ( new level @$ new level min priority,task divides,fail,pre
x attr exists @$ and <PLACE_HOLDER> it using create| <PLACE_HOLDER> flag .,fs . setx attr ( path @$ name1 @$ value1 @$ enum set . of ( x attr set flag . create ) ) ; fs . setx attr ( path @$ name1 @$ new value1 @$ enum set . of ( x attr set flag . create @$ x attr set flag . replace ) ) ; xattrs = fs . getx attrs ( path ) ; assert . assert equals ( xattrs . size ( ) @$ __num__ ) ; assert . assert array equals ( new value1 @$ xattrs . get ( name1 ) ) ; fs . removex attr ( path @$ name1 ) ;,create| set,fail,pre
its important that we not store the stack itself in the weak order queue as the stack also is used in the weak hash map as key . so just store the enclosed atomic integer which should <PLACE_HOLDER> to have the stack itself g ced .,head = new head ( stack . available shared capacity ) ; head . link = tail ; interval = stack . interval ; handle recycle count = interval ;,which affect,fail,pre
both sides <PLACE_HOLDER> only one value,join function . join ( firstv1 @$ firstv2 @$ collector ) ;,sides have,fail,pre
creating the file <PLACE_HOLDER> the link work,create and write file ( file abs ) ; wrapper . get file status ( link ) ;,file does,fail,pre
circular reveal library <PLACE_HOLDER> absolute coordinates setup animation,support animator anim = io . codetail . animation . view animation utils . create circular reveal ( view @$ centerx @$ centery @$ start radius @$ end radius ) ; anim . set duration ( ( int ) duration ) ; anim . set interpolator ( interpolator ) ;,library supports,fail,pre
convert text type to table type . we assume the text type must be the sql output . this assumption is correct for now . ideally livy should <PLACE_HOLDER> table type . we may do it in the future release of livy .,for ( interpreter result message message : result . message ( ) ) { if ( message . get type ( ) == interpreter result . type . text ) { list < string > rows = parsesql output ( message . get data ( ) ) ; result2 . add ( interpreter result . type . table @$ string utils . join ( rows @$ __str__ ) ) ; if ( rows . size ( ) >= ( max result + __num__ ) ) { result2 . add ( result messages . get exceeds limit rows message ( max result @$ zeppelin_livy_spark_sql_max_result ) ) ; } } else { result2 . add ( message . get type ( ) @$ message . get data ( ) ),livy support,fail,pre
now to test the case when rm already <PLACE_HOLDER> cleanup @$ and nm suddenly realizes that the container is running .,log . info ( __str__ + __str__ ) ; container statuses . clear ( ) ; container status list . clear ( ) ; container status list . add ( builder utils . new container status ( conts . get ( __num__ ) . get id ( ) @$ container state . running @$ __str__ @$ __num__ @$ conts . get ( __num__ ) . get resource ( ) ) ) ; container statuses . put ( app . get application id ( ) @$ container status list ) ; resp = nm1 . node heartbeat ( container statuses @$ true ) ;,rm performs,fail,pre
make sure the object <PLACE_HOLDER> the gc,if ( ( flags & ( call_type_callback | call_type_global_value | call_type_struct_member ) ) > __num__ ) { o . retain ( ) ; },object outlives,success,pre
if the last table does not <PLACE_HOLDER> any records we still need to mark the last processed event as the last one,if ( snapshot context . last table ) { snapshot context . offset . mark last snapshot record ( ) ; },table contain,success,pre
ensure that the notifier <PLACE_HOLDER> the entry,assert equals ( v1 @$ r . get entry ( k3 ) . get value ( ) ) ;,notifier captures,fail,pre
this call is <PLACE_HOLDER> the private field access in the reloaded method has been changed to use the accessors into the type that can access the field from outside,method = st . get clazz ( ) . get method ( __str__ ) ; string = ( string ) method . invoke ( object ) ; assert equals ( __str__ @$ string ) ;,call assuming,fail,pre
tab 2 <PLACE_HOLDER> a custom view @$ but no text or icon,tab = tab layout . get tab at ( __num__ ) ; assert null ( tab . get text ( ) ) ; assert null ( tab . get icon ( ) ) ; assert not null ( tab . get custom view ( ) ) ; assert equals ( r . id . my_custom_tab @$ tab . get custom view ( ) . get id ( ) ) ;,tab has,success,pre
have server <PLACE_HOLDER> frame .,try ( blockhead connection server conn = server conn fut . get ( timeouts . connect @$ timeouts . connect_unit ) ) { byte [ ] payload = new byte [ buffer size / __num__ ] ; arrays . fill ( payload @$ ( byte ) __str__ ) ; byte buffer server frame = buffer util . allocate ( buffer size ) ; buffer util . flip to fill ( server frame ) ; server frame . put ( ( byte ) ( __num__ | __num__ ) ) ; server frame . put ( ( byte ) __num__ ) ; server frame . put ( ( byte ) ( payload . length > > __num__ ) ) ; server frame . put ( ( byte ) ( payload,server close,fail,pre
no need to sync because noone <PLACE_HOLDER> access to new info yet,new info . policy entries . add ( pe ) ;,noone has,success,pre
while statement always <PLACE_HOLDER> a complexity of at least 1,bool comp while ++ ; entry stack . peek ( ) . bump decision points ( bool comp while ) ; super . visit ( node @$ data ) ; logger . exiting ( class_name @$ __str__ ) ; return data ;,statement has,success,pre
first callback <PLACE_HOLDER> items based on a query,data provider < person @$ ? > data provider = data provider . from callbacks ( query -> { int offset = query . get offset ( ) ; int limit = query . get limit ( ) ; list < person > persons = get person service ( ) . fetch persons ( offset @$ limit ) ; return persons . stream ( ) ; } @$ query -> get person service ( ) . get person count ( ) ) ;,callback fetch,fail,pre
these exist just to support the 'old ' lombok.experimental.builder @$ which <PLACE_HOLDER> these properties . lombok.builder no longer has them .,boolean fluent = to boolean ( annotation . get actual expression ( __str__ ) @$ true ) ; boolean chain = to boolean ( annotation . get actual expression ( __str__ ) @$ true ) ; string builder method name = builder instance . builder method name ( ) ; string build method name = builder instance . build method name ( ) ; string builder class name = builder instance . builder class name ( ) ; string to builder method name = __str__ ; boolean to builder = builder instance . to builder ( ) ; java . util . list < name > type args for to builder = null ; if ( builder method name == null ) builder method name = __str__ ;,which had,success,pre
we want to differentiate between io exceptions thrown by the repository and io exceptions thrown from processor code . as a result @$ as have the flow file access input stream that <PLACE_HOLDER> io exception from the repository and translates into either flow file access exception or content not found exception . we keep track of any content not found exception because if it,final flow file access input stream ffais = new flow file access input stream ( counting in @$ source @$ curr claim ) ; final flow file access output stream ffaos = new flow file access output stream ( counting out @$ source ) ; boolean cnfe thrown = false ; try { writer . process ( create task termination stream ( ffais ) @$ create task termination stream ( ffaos ) ) ; } catch ( final content not found exception cnfe ) { cnfe thrown = true ; throw cnfe ; } finally { written to flow file = counting out . get bytes written ( ) ; this . bytes written += written to flow file ; this . bytes read += counting in .,code catches,success,pre
put the call args back on stack so the method call can <PLACE_HOLDER> them,if ( should backup ) { restore stack ( backup args indices @$ arg types @$ is static call ) ; },call dispose,fail,pre
if another thread concurrently <PLACE_HOLDER> the only remaining value from the entry @$ this put and get will return null @$ since the entry is about to be removed from the map . in that case @$ we obtain a fresh entry from the map and do the put on it .,while ( result == null ) { result = get or create entry ( key ) . put and get ( val ) ; },thread removed,fail,pre
note that this insertion method is worthwhile since the vast majority mark group keys will <PLACE_HOLDER> only one value .,if ( o == null ) { mark groups . put ( token @$ token ) ; } else if ( o instanceof token entry ) { list < token entry > l = new array list < > ( ) ; l . add ( ( token entry ) o ) ; l . add ( token ) ; mark groups . put ( token @$ l ) ; } else { @ suppress warnings ( __str__ ) list < token entry > l = ( list < token entry > ) o ; l . add ( token ) ; } last hash = __num__ ; for ( int end = math . max ( __num__ @$ i - min + __num__ ) ; i >,keys have,success,pre
schedule monitor only if the job <PLACE_HOLDER> auto interruptable functionality,try { if ( context . get job detail ( ) . get job data map ( ) . get boolean ( auto_interruptible ) ) { job interrupt monitor plugin monitor plugin = ( job interrupt monitor plugin ) context . get scheduler ( ) . get context ( ) . get ( job_interrupt_monitor_key ) ; long job data delay = default_max_runtime ; if ( context . get job detail ( ) . get job data map ( ) . get ( max_run_time ) != null ) { job data delay = context . get job detail ( ) . get job data map ( ) . get long ( max_run_time ) ; } future = monitor plugin . schedule job interrupt monitor ( context . get job,job has,fail,pre
dummy out <PLACE_HOLDER> target argument @$ since we do n't care about target .,if ( arg . equals ( __str__ ) ) { get arg value ( args @$ arg ) ; } else if ( platform_module_system_options . contains ( arg ) ) { add platform module system options ( arg @$ get arg value ( args @$ arg ) ) ; } else if ( arg . starts with ( batch_processing_max_flag ) ) { } else if ( obsolete flags . contains ( arg ) ) { } else if ( arg . starts with ( __str__ ) ) { usage ( __str__ + arg ) ; } else if ( name table . is valid class name ( arg ) && ! has known file suffix ( arg ) ) { entry classes . add ( arg ) ;,dummy get,fail,pre
synchronize so default value does n't <PLACE_HOLDER> other default value,synchronized ( session ) { object result = session . get attribute ( name ) ; if ( result == null && default value != null ) { session . set attribute ( name @$ default value ) ; result = default value ; } return result ; },value affect,fail,pre
when hive.metastore.transactional.event.listeners is set @$ a failed event should not <PLACE_HOLDER> a new notification,dummy raw store fail event . set event succeed ( false ) ; try { ms client . alter_partition ( default db name @$ tbl name @$ new part @$ null ) ; fail ( __str__ ) ; } catch ( exception ex ) { },event create,success,pre
wait for all non <PLACE_HOLDER> tasks to be completed,try { boolean acquired = sem . try acquire ( max_waiting_time @$ time unit . milliseconds ) ; assert . assert true ( __str__ @$ acquired ) ; background executor . cancel all ( __str__ @$ true ) ; assert . assert equals ( __str__ @$ list . size ( ) @$ nb_add ) ; for ( int i = __num__ ; i < nb_add ; i ++ ) { assert . assert true ( __str__ @$ i < nb_add ) ; } } catch ( interrupted exception e ) { assert . assert false ( __str__ @$ true ) ; },non join,fail,pre
different properties <PLACE_HOLDER> different results,assert false ( objects . equals ( empty @$ finger ) ) ; assert false ( objects . equals ( empty @$ finger brand ) ) ; assert false ( objects . equals ( finger @$ finger brand ) ) ;,properties give,fail,pre
optionally check the byte after this frame <PLACE_HOLDER> sync word .,if ( ! try read ( pes buffer @$ adts scratch . data @$ __num__ ) ) { return true ; } adts scratch . set position ( __num__ ) ; int frame size = adts scratch . read bits ( __num__ ) ; if ( frame size <= __num__ ) { return false ; },frame covers,fail,pre
lower android versions <PLACE_HOLDER> a reference table with 1024 entries only,for ( int i = __num__ ; i < __num__ ; i ++ ) { int [ ] ints = jni test . return int array ( ) ; assert not null ( ints ) ; },versions have,success,pre
assert true here to make test tool <PLACE_HOLDER> this test case into account,assert . assert true ( true ) ;,tool take,success,pre
setting heartbeat interval to 1 hour to prevent bp service actor <PLACE_HOLDER> heartbeat periodically to nn during running test case @$ and bp service actor only <PLACE_HOLDER> heartbeat once after startup,conf . set time duration ( dfs_heartbeat_interval_key @$ __num__ @$ time unit . hours ) ; minidfs cluster cluster = new minidfs cluster . builder ( conf ) . nn topology ( minidfsnn topology . simpleha federated topology ( __num__ ) ) . build ( ) ; cluster . wait active ( ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; metrics record builder rb = get metrics ( dn . get metrics ( ) . name ( ) ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$,actor sends,success,pre
already have demand @$ so do n't need to do anything @$ the current demand will <PLACE_HOLDER> the stream .,if ( requested ( ) != __num__ ) { return ; },demand complete,success,pre
defaults to <PLACE_HOLDER> 6,fla version = fla version . cs6 ;,defaults win,fail,pre
if it is a dynamic queue mapping @$ we can safely assume leaf queue name does not have ' . ' in it validate if parent queue is specified @$ then parent queue exists and an instance of auto <PLACE_HOLDER> enabled parent queue,queue mapping entity new mapping = validate and get auto created queue mapping ( queue manager @$ mapping @$ queue path ) ; if ( new mapping != null ) { new mappings . add ( new mapping ) ; } else { new mappings . add ( mapping ) ; },instance created,fail,pre
when table could not be fetched from metastore @$ it is not known whether the table was added . deleting the table when aborting commit <PLACE_HOLDER> the risk of deleting table not added in this transaction . not deleting the table may leave garbage behind . the former is much more dangerous than the latter . therefore @$ the table is not considered added,if ( ! done ) { throw e ; },commit has,success,pre
base 64 <PLACE_HOLDER> the payload and then convert to a string for the header .,encoded buf = base64 . encode ( buf @$ url_safe ) ; return encoded buf . to string ( utf_8 ) ; release ( buf ) ; release ( encoded buf ) ;,base encodes,fail,pre
batch 2 <PLACE_HOLDER> a write based on the read,volt queuesql ( write @$ expect_scalar_match ( __num__ ) @$ current + __num__ @$ pkey ) ; volt executesql ( ) ;,batch does,success,pre
disable reorder of columns . allowing this would <PLACE_HOLDER> the table to become unstable ; we rely on knowing that mnemonics are always in the first column @$ and that operand columns are in a particular order .,this . get table header ( ) . set reordering allowed ( false ) ;,reorder cause,success,pre
replication scope <PLACE_HOLDER> replacement @$ and does not require empty directories,if ( replication spec . is in replication scope ( ) ) { return ; },scope includes,fail,pre
client 1 did not <PLACE_HOLDER> interest,await ( ) . until asserted ( ( ) -> { assert that ( client1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ - __num__ ) ; assert that ( client2 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ - __num__ ) ; assert that ( server1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ - __num__ ) ; assert that,client register,success,pre
flush those <PLACE_HOLDER> buffered events out .,produce synchronously to partition zero ( input @$ as list ( new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) ) ) ; verify output ( output raw @$ new hash set < > ( as list ( new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__,those sent,fail,pre
remove timestamp should also be <PLACE_HOLDER> the last modified timestamp,m exif . remove timestamp ( ) ; assert that ( m exif . get last modified timestamp ( ) ) . is equal to ( exif . invalid_timestamp ) ;,timestamp returning,fail,pre
inbuilt assumption that the testdir <PLACE_HOLDER> only one output file .,path di test = new path ( tmppath @$ testdir ) ; if ( ! fs . exists ( di test ) ) { throw new runtime exception ( tmpdir + file . separator + testdir + __str__ ) ; } if ( ! shim loader . get hadoop shims ( ) . is directory ( fs . get file status ( di test ) ) ) { throw new runtime exception ( tmpdir + file . separator + testdir + __str__ ) ; } fs data input stream fi test = fs . open ( ( fs . list status ( di test ) ) [ __num__ ] . get path ( ) ) ; file input stream fi gold = new file input stream ( new,testdir has,success,pre
bind <PLACE_HOLDER> method,c = new method closure ( this @$ __str__ ) ; super . set variable ( __str__ @$ c ) ;,bind generated,fail,pre
the number of <PLACE_HOLDER> bytes,int got = cis . read ( result ) ;,number got,success,pre
configure a virtual destination that forwards messages from topic test <PLACE_HOLDER> name,remote session . create consumer ( included ) ; message producer included producer = local session . create producer ( included ) ; message test = local session . create text message ( __str__ ) ; final destination statistics destination statistics = local broker . get destination ( included ) . get destination statistics ( ) ;,messages queue,success,pre
if a partition has multiple partition keys @$ we make the assumption that make part name with one key will <PLACE_HOLDER> a substring of the name made with both all the keys .,string escaped name fragment = warehouse . make part name ( part key to val @$ false ) ; if ( key count == __num__ ) { params . put ( param name @$ escaped name fragment ) ; fltr . append ( __str__ ) . append ( is eq ? __str__ : __str__ ) . append ( param name ) ; } else if ( key pos + __num__ == key count ) { params . put ( param name @$ __str__ + escaped name fragment ) ; fltr . append ( is eq ? __str__ : __str__ ) . append ( __str__ ) . append ( param name ) . append ( __str__ ) ; } else if ( key pos == __num__ ) { params,name return,success,pre
until the scan manager <PLACE_HOLDER> a new configuration @$ we 're going to work with a default result log config object .,config = new result log config ( ) ; config . set memory max records ( mem capacity ) ; config . set log file name ( get log file name ( false ) ) ; config . set log file max records ( file capacity ) ;,manager returns,fail,pre
a tree on the left @$ an editor on the right : <PLACE_HOLDER> them in a sash form ... right below the editor we display the error messages ...,sash form = new sash form ( shell @$ swt . horizontal ) ; sash form . set layout ( new fill layout ( ) ) ; form data fd sash form = new form data ( ) ; fd sash form . left = new form attachment ( __num__ @$ __num__ ) ; fd sash form . right = new form attachment ( __num__ @$ __num__ ) ; fd sash form . top = new form attachment ( __num__ @$ __num__ ) ; fd sash form . bottom = new form attachment ( buttons composite @$ - __num__ ) ; sash form . set layout data ( fd sash form ) ; form data fdbc = new form data ( ) ; fdbc . left = new,tree fill,fail,pre
roll back so next write buffer can <PLACE_HOLDER> its job on the next call but at the first write we 're at 0 .,if ( m write index == __num__ && m write buf index != __num__ ) { m write index = m chunk size ; m write buf index -- ; },buffer do,success,pre
remove the filter popup if the user has <PLACE_HOLDER> all text,if ( showing && length == __num__ ) { dismiss popup ( ) ; m filtered = false ; },user selected,fail,pre
the current txn is either in open or aborted state . <PLACE_HOLDER> the write ids state as per the txn state .,invalid write id list . add ( write id ) ; if ( valid txn list . is txn aborted ( txn id ) ) { aborted bits . set ( invalid write id list . size ( ) - __num__ ) ; } else { min open write id = math . min ( min open write id @$ write id ) ; },either check,fail,pre
unconditionally reduce the amount of memory required for flow control because there is no object allocation costs associated with doing so and the stream will not <PLACE_HOLDER> any more local flow control state to keep track of anymore .,stream . set property ( state key @$ reduced_flow_state ) ;,stream have,success,pre
all types must <PLACE_HOLDER> the same root,for ( logical type type : normalized types ) { if ( type . get type root ( ) != type root ) { return null ; } if ( type . get children ( ) . size ( ) != number of children ) { return null ; } },types have,success,pre
if a run instances request does n't <PLACE_HOLDER> a client token @$ fill one in @$ otherwise retries could result in unwanted instances being launched in the customer 's account .,if ( original request instanceof run instances request ) { run instances request run instances request = ( run instances request ) original request ; if ( run instances request . get client token ( ) == null ) { request . add parameter ( __str__ @$ uuid . randomuuid ( ) . to string ( ) ) ; } } else if ( original request instanceof modify reserved instances request ) { modify reserved instances request modify reserved instances request = ( modify reserved instances request ) original request ; if ( modify reserved instances request . get client token ( ) == null ) { request . add parameter ( __str__ @$ uuid . randomuuid ( ) . to string ( ) ) ; } },request specify,success,pre
invalidate both <PLACE_HOLDER> journals .,invalidate edits dir at index ( __num__ @$ true @$ false ) ; invalidate edits dir at index ( __num__ @$ true @$ false ) ;,invalidate edits,success,pre
it <PLACE_HOLDER>s some sense to return true @$ as no filter implies all shall pass the filter @$ but if this returns true @$ then any other filters can be used as the source of the data to filter @$ which does n't <PLACE_HOLDER> sense if this is meant to only be used by itself .,return false ;,which make,success,pre
let 's <PLACE_HOLDER> these strange cases @$ as shortening the text might leave us behind with invalid markup etc,if ( context . length ( ) > max_context_length ) { continue ; },'s skip,success,pre
verify that the second job <PLACE_HOLDER> the prober that was passed to its builder .,assert that ( override prober . probed ( ) @$ is ( true ) ) ;,job contains,fail,pre
note : adjust the column size after loading the initial model so the table <PLACE_HOLDER> the column model generated,this . get column model ( ) . get column ( __num__ ) . set max width ( __num__ ) ;,table has,success,pre
start the enable procedure & & <PLACE_HOLDER> the executor,long proc id = proc exec . submit procedure ( new enable table procedure ( proc exec . get environment ( ) @$ table name ) ) ; int last step = __num__ ;,procedure kill,success,pre
should n't ever throw cancelled since this method <PLACE_HOLDER> a dummy monitor .,msg . error ( this @$ __str__ + e . get message ( ) @$ e ) ;,method creates,fail,pre
splits must be stable @$ and can not change during consecutive executions for example : kafka should not <PLACE_HOLDER> partitions if more then one topic is read .,for ( int i = __num__ ; i < num splits ; i ++ ) { result . add ( new microbatch source < > ( splits . get ( i ) @$ max read time @$ __num__ @$ num records [ i ] @$ i @$ source id @$ reader cache interval ) ) ; },kafka reorder,fail,pre
as we only support limited concurrent checkpoints @$ after checkpoint <PLACE_HOLDER> more than the limits @$ the current periodic trigger would been assigned as null .,checkpoint coordinator . trigger checkpoint ( system . current time millis ( ) @$ false ) ; assert false ( checkpoint coordinator . is current periodic trigger available ( ) ) ; assert equals ( max concurrent checkpoints @$ checkpoint coordinator . get number of pending checkpoints ( ) ) ; checkpoint coordinator . abort pending checkpoints ( new checkpoint exception ( checkpoint failure reason . job_failover_region ) ) ;,checkpoint hits,fail,pre
dmn engine does n't <PLACE_HOLDER> the whole object when no entry of the decision table match,if ( result raw != null ) { for ( expression element expression element : elements without class ) { if ( ! ( result raw instanceof map ) ) { throw new scenario exception ( __str__ ) ; } map < string @$ object > result = ( map < string @$ object > ) result raw ; result raw = result . get ( expression element . get step ( ) ) ; } } class < ? > result class = result raw != null ? result raw . get class ( ) : null ; object expected result raw = expected result . get raw value ( ) ; return get result wrapper ( fact mapping . get class name ( ) @$ expected,engine change,fail,pre
no state <PLACE_HOLDER> @$ check when we should remove the delay flag from the shards the next time . if cluster state <PLACE_HOLDER> @$ we can leave the scheduling of the next delay up to the cluster <PLACE_HOLDER> event this should not be needed @$ but we want to be extra safe here,if ( old state == new state ) { schedule if needed ( current nano time ( ) @$ new state ) ; },state change,fail,pre
we use the default <PLACE_HOLDER> configuration,if ( build time config . default backend . version . is present ( ) ) { add config ( property collector @$ engine settings . default_backend @$ hibernate search elasticsearch recorder . default_backend ) ; },default requires,fail,pre
strip the noisy <PLACE_HOLDER> trace elements .,string [ ] exclusions = excluded methods . get ( ) ; for ( int k = __num__ ; k < exclusions . length ; k += __num__ ) { if ( exclusions [ k ] . equals ( element . get class name ( ) ) && exclusions [ k + __num__ ] . equals ( element . get method name ( ) ) ) { continue out ; } } buf . append ( __str__ ) ; buf . append ( element . to string ( ) ) ; buf . append ( newline ) ;,noisy exclude,fail,pre
ok. now i <PLACE_HOLDER> my mocked up server & region server services and dodgy wal @$ go ahead with test .,file system fs = file system . get ( conf ) ; path root dir = new path ( dir + get name ( ) ) ; dodgyfs log dodgywal = new dodgyfs log ( fs @$ root dir @$ get name ( ) @$ conf ) ; dodgywal . init ( ) ; path originalwal = dodgywal . get current file name ( ) ;,ok. have,success,pre
check if there have been any changes if we have n't built the media db yet @$ do so on this sync . <PLACE_HOLDER> note at the top of this class about this difference to the original .,try { if ( m col . get media ( ) . need scan ( ) ) { m con . publish progress ( r . string . sync_media_find ) ; m col . log ( __str__ ) ; m col . get media ( ) . find changes ( ) ; } int last usn = m col . get media ( ) . last usn ( ) ; json object ret = m server . begin ( ) ; int srv usn = ret . get int ( __str__ ) ; if ( ( last usn == srv usn ) && ! ( m col . get media ( ) . have dirty ( ) ) ) { return __str__ ; } m col . log,check see,success,pre
a new call will <PLACE_HOLDER> a new transport,client call < string @$ integer > call2 = oob1 . new call ( method @$ call options . default ) ; call2 . start ( mock call listener2 @$ headers ) ; client call < string @$ integer > call3 = oob1 . new call ( method @$ call options . default . with wait for ready ( ) ) ; call3 . start ( mock call listener3 @$ headers ) ; verify ( mock transport factory @$ times ( __num__ ) ) . new client transport ( eq ( socket address ) @$ eq ( new client transport options ( ) . set authority ( __str__ ) . set user agent ( user_agent ) ) @$ isa ( channel logger . class ) ) ; transport,call start,fail,pre
while this is more general than what stapler can <PLACE_HOLDER> on these types @$ the above is the only criterion for stapler to attempt dispatch . therefore prohibit this as a regular getter .,if ( parameter types . length > __num__ && parameter types [ __num__ ] == string . class ) { return false ; },stapler do,fail,pre
use a short 100 ms sleep since this could be done inline with a rs startup even if we fail @$ other region servers can <PLACE_HOLDER> care of it,this . executor = new thread pool executor ( nb workers @$ nb workers @$ __num__ @$ time unit . milliseconds @$ new linked blocking queue < > ( ) ) ; thread factory builder tfb = new thread factory builder ( ) ; tfb . set name format ( __str__ ) ; tfb . set daemon ( true ) ; this . executor . set thread factory ( tfb . build ( ) ) ; this . latest paths = new hash map < > ( ) ; this . replication for bulk load data enabled = conf . get boolean ( h constants . replication_bulkload_enable_key @$ h constants . replication_bulkload_enable_default ) ; this . sleep for retries = this . conf . get long ( __str__,servers take,success,pre
test that a click on an item <PLACE_HOLDER> the registered listener,int index to click = expected count - __num__ ; on data ( all of ( is ( instance of ( string . class ) ) @$ is ( expected content [ index to click ] ) ) ) . in root ( is dialog ( ) ) . perform ( click ( ) ) ; verify ( on click listener @$ times ( __num__ ) ) . on click ( m alert dialog @$ index to click ) ;,click invokes,success,pre
close <PLACE_HOLDER> connections .,list < completable future < void > > connection close futures = new array list < > ( m connections . size ( ) ) ; for ( connection connection : m connections ) { connection close futures . add ( connection . close ( ) ) ; } m connections . clear ( ) ; return completable future . all of ( connection close futures . to array ( new completable future [ __num__ ] ) ) ;,close created,success,pre
first char is <PLACE_HOLDER> duration next : are rest duration @$ pitch @$ <PLACE_HOLDER> duration,int channel = __num__ ; int velocity = __num__ ; int track = __num__ ; final int resolution = __num__ ; final int resolution delta = resolution / __num__ ; int index = __num__ ;,char adjusting,fail,pre
end of string @$ <PLACE_HOLDER> last run .,run end = end ;,end close,fail,pre
check job config for overrides @$ otherwise <PLACE_HOLDER> the default server value .,long job val = job conf . get long ( var . varname @$ - __num__ ) ; return ( job val != - __num__ ) ? job val : hive conf . get long var ( daemon conf @$ var ) ;,job use,success,pre
when application explicitly <PLACE_HOLDER> segments ; timestamp based purge is only used to cleanup log segments that have been marked for truncation,if ( ( l . is truncated ( ) || ! conf . get explicit truncation by application ( ) ) && ! l . is in progress ( ) && ( l . get completion time ( ) < min timestamp to keep ) ) { purge list . add ( l ) ; } else { break ; },application removenrs,fail,pre
jjt <PLACE_HOLDER> child resizes it 's child node list according to known indexes . going backwards makes sure the first time it gets the right size avoiding copies .,for ( int i = parameter types . length - __num__ ; i >= __num__ ; i -- ) { ast formal parameter formal parameter = new ast formal parameter ( java parser tree constants . jjtformalparameter ) ; formal parameters . jjt add child ( formal parameter @$ i ) ; formal parameter . jjt set parent ( formal parameters ) ; ast variable declarator id variable declarator id = new ast variable declarator id ( java parser tree constants . jjtvariabledeclaratorid ) ; variable declarator id . set image ( __str__ + i ) ; formal parameter . jjt add child ( variable declarator id @$ __num__ ) ; variable declarator id . jjt set parent ( formal parameter ) ; ast type type = new,jjt add,success,pre
note : we do n't break because other branches may <PLACE_HOLDER> useful state information,if ( m ) { match = true ; },branches provide,fail,pre
create a text <PLACE_HOLDER> output step,string test file output name = name ; text file output meta text file output meta = new text file output meta ( ) ; string text file input pid = registry . get plugin id ( step plugin type . class @$ text file output meta ) ; step meta text file output step = new step meta ( text file input pid @$ test file output name @$ text file output meta ) ;,text file,success,pre
r <PLACE_HOLDER> java package,android resource android resource = new android resource ( build target . with appended flavors ( aar_android_resource_flavor ) @$ project filesystem @$ android resource params @$ graph builder @$ immutable sorted set . < build rule > natural order ( ) . add ( assemble assets directories ) . add ( assemble resource directories ) . add all ( original build rule params . get declared deps ( ) . get ( ) ) . build ( ) @$ assemble resource directories . get source path to output ( ) @$ immutable sorted map . of ( ) @$ null @$ assemble assets directories . get source path to output ( ) @$ immutable sorted map . of ( ) @$ manifest . get source path to output,r android,fail,pre
app <PLACE_HOLDER> the rate limit .,if ( ! is under timing session count quota && stats . session count in window < stats . session count limit ) { in quota time elapsed = math . max ( in quota time elapsed @$ stats . session rate limit expiration time elapsed ) ; },app hit,success,pre
management server <PLACE_HOLDER> back a cds response without cluster for the requested resource .,list < any > clusters = immutable list . of ( any . pack ( build cluster ( __str__ @$ null @$ false ) ) @$ any . pack ( build cluster ( __str__ @$ null @$ false ) ) ) ; discovery response response = build discovery response ( __str__ @$ clusters @$ xds client impl . ads_type_url_cds @$ __str__ ) ; response observer . on next ( response ) ;,server sends,success,pre
sadly @$ motion event does n't <PLACE_HOLDER> equals @$ so we compare references .,assert true ( expected == m last context event ) ;,event support,fail,pre
assert that the correct dir does not <PLACE_HOLDER> the violation .,assert true ( pairs correct . stream ( ) . none match ( p -> p . get path ( ) . equals ( cwd correct ) ) ) ;,dir contain,success,pre
if a shape <PLACE_HOLDER> 2 or less points it can not be reduced,if ( tolerance <= __num__ || n < __num__ ) { return shape ; } boolean [ ] marked = new boolean [ n ] ;,shape has,success,pre
next tag does n't correctly <PLACE_HOLDER> dt ds,if ( event == xml stream constants . start_document ) { while ( ! fast infoset stream reader . is start element ( ) ) event = fast infoset stream reader . next ( ) ; },tag handle,success,pre
table should not <PLACE_HOLDER> any footers at initialization,assert null ( table . get column footer ( __str__ ) ) ; assert null ( table . get column footer ( __str__ ) ) ; assert null ( table . get column footer ( __str__ ) ) ;,table have,fail,pre
note on quoting : it would be wrong here @$ since argv will be passed to runtime.exec @$ which should not <PLACE_HOLDER> arguments or split on whitespace .,argv . add ( __str__ + name + __str__ + props . get property ( name ) ) ;,which remove,fail,pre
mandatory server hello <PLACE_HOLDER> message,upcoming states . add ( hs_server_hello_done ) ;,hello done,success,pre
the main wf and the sub wf should be in <PLACE_HOLDER> state,workflow = workflow execution service . get execution status ( workflow id @$ true ) ; assert not null ( workflow ) ; assert equals ( running @$ workflow . get status ( ) ) ; assert equals ( __num__ @$ workflow . get tasks ( ) . size ( ) ) ; assert equals ( correlation id @$ workflow . get correlation id ( ) ) ; assert equals ( __str__ @$ workflow . get input ( ) . get ( __str__ ) ) ; assert equals ( __str__ @$ workflow . get input ( ) . get ( __str__ ) ) ; sub workflow = workflow execution service . get execution status ( sub workflow id @$ true ) ; assert not null ( sub workflow,wf running,success,pre
any application must <PLACE_HOLDER> one of the package declaration methods .,package declaration ( off @$ nm . id ) ;,application override,success,pre
update scripts using the now complete goog module <PLACE_HOLDER> global state and unspool the script descriptions that were queued up by all the recording .,for ( node c : script nodes ) { push script ( script descriptions . remove first ( ) ) ; if ( ! c . is from externs ( ) || node util . is from type summary ( c ) ) { node traversal . traverse ( compiler @$ c @$ new script updater ( ) ) ; } pop script ( ) ; } declare synthetic externs ( ) ;,scripts verify,fail,pre
bitwise or combines the sign bits so any negative value <PLACE_HOLDER> the check .,if ( ( index | size | bytes . length - index - size ) < __num__ ) { throw new array index out of bounds exception ( string . format ( __str__ @$ bytes . length @$ index @$ size ) ) ; } int offset = index ; final int limit = offset + size ;,value fails,success,pre
the following flags <PLACE_HOLDER> startup time and are acceptable only for dev purposes,args . add ( __str__ ) ; if ( ! is preventnoverify ( ) ) { args . add ( __str__ ) ; },flags reduce,success,pre
simple check to make sure things <PLACE_HOLDER> ok ...,for ( table catalog_tbl : catalog_db . get tables ( ) ) { string builder sb = new string builder ( ) ; catalog schema tools . to schema ( sb @$ catalog_tbl @$ null @$ false @$ null @$ null ) ; string sql = sb . to string ( ) ; assert true ( sql . starts with ( __str__ + catalog_tbl . get type name ( ) ) ) ; for ( column catalog_col : catalog_tbl . get columns ( ) ) { assert true ( sql . index of ( catalog_col . get type name ( ) ) != - __num__ ) ; } for ( constraint catalog_const : catalog_tbl . get constraints ( ) ) { constraint type const_type = constraint type .,things look,success,pre
wild card <PLACE_HOLDER> 6 single broadcast <PLACE_HOLDER> 6 address : fe 80 : xx : xx ... loopback <PLACE_HOLDER> 6 address site local <PLACE_HOLDER> 6 address : fec 0 : xx : xx ...,if ( inet addr . is any local address ( ) || inet addr . is link local address ( ) || inet addr . is loopback address ( ) || inet addr . is site local address ( ) ) { return true ; },broadcast pv,fail,pre
and calculate the difference . expandable will <PLACE_HOLDER> precedence .,if ( child instanceof i sectionable && has header ( child ) ) { i header header = get header of ( child ) ; if ( ! ( header instanceof i expandable ) ) { return get global position of ( child ) - get global position of ( header ) - __num__ ; } } return get siblings of ( child ) . index of ( child ) ;,expandable take,fail,pre
we deal with xml file <PLACE_HOLDER> xml file,if ( meta . getxml source file ( ) ) { file object xmlfile validator = kettlevfs . get file object ( xml fieldvalue ) ; if ( xmlfile validator == null || ! xmlfile validator . exists ( ) ) { log error ( base messages . get string ( pkg @$ __str__ @$ xml fieldvalue ) ) ; throw new kettle step exception ( base messages . get string ( pkg @$ __str__ @$ xml fieldvalue ) ) ; } sourcexml = new stream source ( xmlfile validator . get content ( ) . get input stream ( ) ) ; },file file,fail,pre
since the overlay panel manager can <PLACE_HOLDER> panels without request panel <PLACE_HOLDER> being called @$ the flag for the panel being <PLACE_HOLDER>n should be set to true here .,m panel shown = true ; super . peek panel ( reason ) ;,manager show,success,pre
create child <PLACE_HOLDER> the entire component hierarchy,component root = element == null ? null : design context . read design ( element ) ;,child covers,fail,pre
<PLACE_HOLDER>er must <PLACE_HOLDER> strip html media on passed val,string [ ] split = val . split ( __str__ @$ __num__ ) ; if ( split . length != __num__ ) { return null ; } string mid = split [ __num__ ] ; val = split [ __num__ ] ; string csum = long . to string ( utils . field checksum ( val ) ) ; list < long > nids = new array list < > ( ) ; cursor cur = null ; try { cur = m col . get db ( ) . get database ( ) . query ( __str__ @$ new string [ ] { mid @$ csum } ) ; long nid = cur . get long ( __num__ ) ; string flds = cur . get string,caller handle,fail,pre
do n't let cp initialization errors <PLACE_HOLDER> the master,if ( this . cp host != null ) { try { this . cp host . post start master ( ) ; } catch ( io exception ioe ) { log . error ( __str__ @$ ioe ) ; } },errors start,fail,pre
the system apk may have been updated with an older version of the one on the data partition @$ but which <PLACE_HOLDER> a new system permission that it did n't have before . in this case we do want to allow the app to now get the new permission if the ancestral apk is privileged to get it .,if ( disabled ps != null && disabled pkg != null && is package requesting permission ( disabled pkg @$ perm ) && ( ( privileged permission && disabled ps . is privileged ( ) ) || ( oem permission && disabled ps . is oem ( ) && can grant oem permission ( disabled ps @$ perm ) ) ) ) { allowed = true ; },which resets,fail,pre
we catch any kind of problem here . some linux distros <PLACE_HOLDER> problems accessing the clipboard .,return new data flavor [ __num__ ] ;,distros have,success,pre
mpi is <PLACE_HOLDER> deps per partition hsid . we need to make sure we write ours into the message getting sent to the mpi,resp . set executor site id ( m_mailbox . geths id ( ) ) ; m_mailbox . send ( counter . m_destination id @$ resp ) ; voltdb . crash global voltdb ( __str__ @$ true @$ null ) ;,mpi handling,fail,pre
no parameter @$ just <PLACE_HOLDER> 0,if ( params == null || params . size ( ) == __num__ ) return o common const . empty_byte_array ;,parameter assume,fail,pre
the long press on the original item should not have triggered a long press on the item touch helper . this check is a stand in for a better way to see if the item touch helper 's internal long press callback method was called . m callback.m <PLACE_HOLDER> drag flag will only be 0 if the item touch helper 's long press call,assert equals ( __num__ @$ m callback . m has drag flag . size ( ) ) ;,callback.m has,success,pre
by some reason many hive errors <PLACE_HOLDER> this sql state,if ( sql state . sql_08s01 . get code ( ) . equals ( sql state ) ) { return error type . normal ; },errors use,fail,pre
we 're letting the window listener <PLACE_HOLDER> control of this,this . set default close operation ( j frame . do_nothing_on_close ) ;,listener take,success,pre
the first entry <PLACE_HOLDER> turn flags for car and foot only,assert equals ( __num__ @$ cartc enc . get decimal ( false @$ tc flags ) @$ __num__ ) ; assert equals ( __num__ @$ trucktc enc . get decimal ( false @$ tc flags ) @$ __num__ ) ; assert true ( double . is infinite ( biketc enc . get decimal ( false @$ tc flags ) ) ) ; tc storage . read flags ( tc flags @$ gh utility . get edge ( graph @$ __num__ @$ __num__ ) . get edge ( ) @$ __num__ @$ gh utility . get edge ( graph @$ __num__ @$ __num__ ) . get edge ( ) ) ; assert equals ( __num__ @$ cartc enc . get decimal ( false @$ tc flags ) @$ __num__,entry provides,success,pre
the user asked for stats to be collected . some stats like number of rows require a scan of the data however @$ some other stats @$ like number of files @$ do not require a complete scan <PLACE_HOLDER> the stats which do not require a complete scan .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },scan fetch,fail,pre
empty point <PLACE_HOLDER> exception,assert invalid multi point ( __str__ @$ __str__ ) ; assert invalid multi point ( __str__ @$ __str__ @$ __str__ ) ;,point raises,success,pre
a new occupant has <PLACE_HOLDER> the room,if ( ! is user status modification ) { for ( participant status listener listener : participant status listeners ) { listener . joined ( from ) ; } },occupant joined,success,pre
now <PLACE_HOLDER> the 1 big write .,file . seek ( wb . offset ) ; if ( max stat > __num__ ) { if ( stat idx < max stat ) { stats [ stat idx ++ ] = sequence . get length ( ) ; } else { long all = __num__ ; for ( ; stat idx > __num__ ; ) { all += stats [ -- stat idx ] ; } system . err . println ( __str__ + all / max stat ) ; } } file . write ( sequence . get data ( ) @$ sequence . get offset ( ) @$ sequence . get length ( ) ) ; replication target replication target = journal . get replication target ( ) ; if ( replication target !=,now do,success,pre
ze time <PLACE_HOLDER> the steps in 100 increment,int steps = activity user . get steps goal ( ) / __num__ ;,time counts,fail,pre
check am 2 <PLACE_HOLDER> the nm token from am 1 .,assert . assert equals ( expectednm tokens . size ( ) @$ register response . getnm tokens from previous attempts ( ) . size ( ) ) ; for ( int i = __num__ ; i < expectednm tokens . size ( ) ; i ++ ) { assert . assert true ( expectednm tokens . get ( i ) . equals ( register response . getnm tokens from previous attempts ( ) . get ( i ) ) ) ; },2 got,fail,pre
consumer ca n't <PLACE_HOLDER> the odd producer index,if ( e == null ) { if ( index != lv producer index ( ) ) { e = spin wait for element ( buffer @$ offset ) ; } else { return null ; } },consumer get,fail,pre
each test must <PLACE_HOLDER> the schema access strategy and schema @$ and enable the writer cs,runner . set property ( jolt transform record . record_writer @$ __str__ ) ;,test set,success,pre
load should also <PLACE_HOLDER> error,binder . read bean ( person ) ; assert null ( name field . get component error ( ) ) ;,load report,fail,pre
once the repl load is successful @$ the this config should be unset or else @$ the subsequent repl load will also <PLACE_HOLDER> those tables which will cause data loss .,load with clause = collections . empty list ( ) ;,load drop,success,pre
we currently do not validate the elements on the ancestors @$ assuming they 've already been validated . this also means some <PLACE_HOLDER>s such as unique ids might not be <PLACE_HOLDER> all situations .,for ( annotation handler annotation handler : environment . get handlers ( ) ) { if ( ! annotation handler . is enabled ( ) ) { continue ; } string validator simple name = annotation handler . get class ( ) . get simple name ( ) ; string annotation name = annotation handler . get target ( ) ; set < ? extends element > annotated elements = extracted model . get root annotated elements ( annotation name ) ; set < element > validated annotated elements = new linked hash set < > ( ) ; validating holder . put root annotated elements ( annotation name @$ validated annotated elements ) ; if ( ! annotated elements . is empty ( ) ) { logger,checks allow,fail,pre
skip children which already <PLACE_HOLDER> their type assigned,if ( current child . get type ( ) == null ) { if ( current child . jjt get last token ( ) . to string ( ) . equals ( __str__ ) ) { if ( previous child != null ) { current child . set type definition ( previous child . get type definition ( ) ) ; } else { ast class or interface declaration type declaration = current child . get first parent of type ( ast class or interface declaration . class ) ; if ( type declaration != null ) { current child . set type definition ( type declaration . get type definition ( ) ) ; } } } else if ( current child . jjt get last token,which have,success,pre
this branch coordinates fragment task or completed transaction task @$ <PLACE_HOLDER> the tasks until all the sites on the node receive the task . task with newer sp handle will,if ( task . need coordination ( ) && m_scoreboard enabled ) { coordinated task queue offer ( task ) ; } else { task queue offer ( task ) ; },task queue,fail,pre
extract the resource context that contains projection information for root <PLACE_HOLDER> entities @$ metadata and paging .,final resource context resource context = routing result . get context ( ) ;,information resource,fail,pre
update the keys in vm 0 until the entry version rolls over . this means that if we did a conflict check @$ vm 0 's key will <PLACE_HOLDER> a lower entry version than vm 1 @$ which would cause us to prefer vm 1 's value,vm0 . invoke ( ( ) -> { internal region region = ( internal region ) get cache ( ) . get region ( region name ) ; region . put ( __num__ @$ __str__ ) ; region entry region entry = region . get region entry ( __num__ ) ; version tag tag = region entry . get version stamp ( ) . as version tag ( ) ; tag . set entry version ( tag . get entry version ( ) - __num__ ) ; region entry . get version stamp ( ) . set versions ( tag ) ; } ) ;,key have,success,pre
produced properties <PLACE_HOLDER> relevant input,global properties gp = new global properties ( ) ; gp . set hash partitioned ( new field list ( __num__ ) ) ; local properties lp = local properties . for grouping ( new field list ( __num__ @$ __num__ ) ) ; requested global properties rgp = new requested global properties ( ) ; rgp . set hash partitioned ( new field list ( __num__ ) ) ; requested local properties rlp = new requested local properties ( ) ; rlp . set grouped fields ( new field list ( __num__ ) ) ; to join1 . set required global props ( rgp ) ; to join1 . set required local props ( rlp ) ; to join1 . set ship strategy ( ship strategy type,properties match,success,pre
register <PLACE_HOLDER> types and fields before canonicalization can optimize them .,register used elements ( ) ; canonicalizer phase . create ( ) . apply ( graph @$ bb . get providers ( ) ) ;,register object,fail,pre
perform a topological sort which <PLACE_HOLDER> the state descriptor classes in their priority .,list < graph vertex < class < ? extends state descriptor > > > sorted successors = topological sort ( preference graph . values ( ) ) ;,which sees,fail,pre
all symbols that came from goog.module are collected separately because they will have to be processed first . <PLACE_HOLDER> explanation below .,list < symbol > types = new array list < > ( ) ; list < symbol > goog module export types = new array list < > ( ) ; list < symbol > module types = new array list < > ( ) ;,symbols see,success,pre
provide compatibility with legacy applications which may <PLACE_HOLDER> boolean values in bind args .,if ( arg instanceof boolean ) { native bind long ( m connection ptr @$ statement ptr @$ i + __num__ @$ ( ( boolean ) arg ) . boolean value ( ) ? __num__ : __num__ ) ; } else { native bind string ( m connection ptr @$ statement ptr @$ i + __num__ @$ arg . to string ( ) ) ; },which pass,success,pre
pdx @$ alias @$ <PLACE_HOLDER> queries,return new object [ ] { new object [ ] { __str__ @$ true @$ true @$ true } @$ new object [ ] { __str__ @$ true @$ true @$ false } @$ new object [ ] { __str__ @$ true @$ false @$ true } @$ new object [ ] { __str__ @$ true @$ false @$ false } @$ new object [ ] { __str__ @$ false @$ true @$ true } @$ new object [ ] { __str__ @$ false @$ true @$ false } @$ new object [ ] { __str__ @$ false @$ false @$ true } @$ new object [ ] { __str__ @$ false @$ false @$ false } @$ new object [ ] { __str__ @$ true @$ true,pdx nested,success,pre
call the g rpc <PLACE_HOLDER> service suggestion,echo . echo response response = echo service grpc . new blocking stub ( channel ) . echo ( echo . echo request . new builder ( ) . set message ( __str__ ) . build ( ) ) ; assert that ( response . get message ( ) @$ is ( __str__ ) ) ; ( ( managed channel ) channel ) . shutdown ( ) . await termination ( __num__ @$ time unit . seconds ) ;,rpc echo,success,pre
transfer the location info from the incoming event to the event factory so that the event we ask it to generate for us <PLACE_HOLDER> the same location info,xml event factory . set location ( start element . get location ( ) ) ; return xml event factory . create start element ( new q name ( mapping xsd support . instance . hbm xsd ( ) . get namespace uri ( ) @$ start element . get name ( ) . get local part ( ) ) @$ start element . get attributes ( ) @$ target namespaces . iterator ( ) ) ;,event has,success,pre
second dfs client does <PLACE_HOLDER> lease,final dfs client mock client2 = create mock client ( ) ; mockito . do return ( true ) . when ( mock client2 ) . renew lease ( ) ; assert same ( renewer @$ lease renewer . get instance ( fake_authority @$ fake_ugi_a @$ mock client2 ) ) ; renewer . put ( mock client2 ) ;,client renew,success,pre
note : m result handler <PLACE_HOLDER> main looper @$ so this must not be blocked .,m result handler = new handler ( context . get main looper ( ) ) ; m next seq number = __num__ ; m pending commands = new array map < > ( ) ; m requested command seq numbers = new array set < > ( ) ; boolean connect requested ; if ( token . get type ( ) == type_session ) { m service connection = null ; connect requested = request connect to session ( connection hints ) ; } else { m service connection = new session service connection ( connection hints ) ; connect requested = request connect to service ( ) ; } if ( ! connect requested ) { close ( ) ; },handler uses,success,pre
orc file does n't <PLACE_HOLDER> extension by linkedin 's convention .,return compare iterators ( keys1 @$ keys2 @$ ( key1 @$ key2 ) -> { if ( ! remove extension ( key1 ) . equals ( key2 ) ) { log . error ( string . format ( __str__ @$ key1 @$ key2 ) ) ; return false ; } orc row iterator it1 = expected . get ( key1 ) ; orc row iterator it2 = observed . get ( key2 ) ; if ( ! it1 . get type info ( ) . equals ( it2 . get type info ( ) ) ) { log . error ( string . format ( __str__ @$ key1 @$ key2 ) ) ; return false ; } boolean result = true ; while ( it1 . has next,file have,success,pre
someone else <PLACE_HOLDER> the race and created the file,throw x ;,someone lost,fail,pre
ui 1 has not yet been added in ui.init where logging <PLACE_HOLDER> place,assert equals ( __str__ @$ get log row ( __num__ ) ) ; string url = get testurl ( getui class ( ) ) . replace ( __str__ @$ __str__ ) ; driver . get ( url ) ;,logging takes,success,pre
g <PLACE_HOLDER> g,for ( int j = __num__ ; j < dimension ; j ++ ) { gg += g [ j ] * g [ j ] ; dgg += ( xi [ j ] + g [ j ] ) * xi [ j ] ; },g contains,fail,pre
client 1 <PLACE_HOLDER> two paths to secret 1,acldao . enroll client ( jooq context . configuration ( ) @$ client1 . get id ( ) @$ group1 . get id ( ) ) ; acldao . enroll client ( jooq context . configuration ( ) @$ client1 . get id ( ) @$ group2 . get id ( ) ) ; acldao . allow access ( jooq context . configuration ( ) @$ secret1 . get id ( ) @$ group1 . get id ( ) ) ; acldao . allow access ( jooq context . configuration ( ) @$ secret1 . get id ( ) @$ group2 . get id ( ) ) ; set < sanitized secret > secret = acldao . get sanitized secrets for ( client1 ) ; assert that (,client has,success,pre
in case this worker <PLACE_HOLDER> no instance on this node,if ( left > right ) { log . debug ( __str__ + nid ) ; this . node pos start [ __num__ * nid + __num__ ] = left ; this . node pos end [ __num__ * nid + __num__ ] = right ; log . debug ( string . format ( __str__ @$ __num__ * nid + __num__ @$ left @$ right ) ) ; this . node pos start [ __num__ * nid + __num__ ] = left ; this . node pos end [ __num__ * nid + __num__ ] = right ; log . debug ( string . format ( __str__ @$ __num__ * nid + __num__ @$ left @$ right ) ) ; return ; },worker has,success,pre
first time store <PLACE_HOLDER> one log file @$ next id will be 2,mockito . do answer ( ans ) . when ( m store ) . roll writer ( __num__ ) ;,store has,success,pre
since nm 's and rm 's token sequence no is different @$ response should <PLACE_HOLDER> system credentials for apps,assert equals ( __num__ @$ response1 . get token sequence no ( ) ) ; assert equals ( __num__ @$ response1 . get system credentials for apps ( ) . size ( ) ) ; resource tracker service . close ( ) ;,response have,fail,pre
this will fail for all users currently <PLACE_HOLDER> signal verified,if ( check valid string ( signal username ) && signal username . equals ( preferences . get verified signal username ( ) ) ) { find preference ( preference manager . verify_signal ) . set summary ( r . string . verification_dialog_summary_verified ) ; } else { find preference ( preference manager . verify_signal ) . set summary ( r . string . verification_dialog_summary ) ; },users have,fail,pre
user 1 <PLACE_HOLDER> the room,try { muc = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc . create ( __str__ ) ; form form = new form ( form . type_submit ) ; form field field = new form field ( __str__ ) ; field . set type ( __str__ ) ; form . add field ( field ) ; form . set answer ( __str__ @$ arrays . as list ( __str__ ) ) ; muc . send configuration form ( form ) ; } catch ( exception e ) { e . print stack trace ( ) ; fail ( e . get message ( ) ) ; },user joins,fail,pre
kill zoo keeper 0 @$ removing <PLACE_HOLDER> 3 .,try { fail site ( __num__ ) ; } catch ( exception e ) { fail ( ) ; } while ( monitor for manager4 . m_members . size ( ) != __num__ && ! monitor for manager4 . has identical membership ( monitor for manager2 ) ) { thread . yield ( ) ; },removing thread,fail,pre
the suite made here will all be <PLACE_HOLDER> the tests from this class,multi config suite builder builder = new multi config suite builder ( test index scan count suite . class ) ;,suite using,success,pre
any application must <PLACE_HOLDER> one of the define field methods .,identifier arg ids [ ] = null ; identifier exp ids [ ] = null ; if ( args != null ) { arg ids = new identifier [ args . length ] ; for ( int i = __num__ ; i < args . length ; i ++ ) { arg ids [ i ] = args [ i ] . id ; } } if ( exp != null ) { exp ids = new identifier [ exp . length ] ; for ( int i = __num__ ; i < exp . length ; i ++ ) { exp ids [ i ] = exp [ i ] . id ; } } define field ( where @$ doc @$ mod @$ t @$ nm,application override,success,pre
at normal playback speed @$ we stop buffering when the buffer <PLACE_HOLDER> the minimum .,assert that ( load control . should continue loading ( min_buffer_us @$ speed ) ) . is false ( ) ;,buffer reaches,success,pre
bypass <PLACE_HOLDER> transaction constructor .,set transaction type ( cardio2e transaction types . get ) ; super . set object type ( cardio2e object types . zones_bypass ) ; set object number ( object number ) ;,bypass get,success,pre
add column families <PLACE_HOLDER> a set of keys,list < column family descriptor builder > family builders = new array list < > ( ) ; sorted map < byte [ ] @$ integer > map = new tree map < > ( bytes . bytes_comparator ) ; visit bulkh files ( fs @$ hfof dir @$ new bulkh file visitor < column family descriptor builder > ( ) { @ override public column family descriptor builder bulk family ( byte [ ] family name ) { column family descriptor builder builder = column family descriptor builder . new builder ( family name ) ; family builders . add ( builder ) ; return builder ; } @ override public void bulkh file ( column family descriptor builder builder @$ file status hfile status ) throws,families specifying,fail,pre
check for offline members @$ if the region <PLACE_HOLDER> persistence even if we intend to replace offline data @$ we still need to make sure the bucket is n't completely offline,if ( ! replace offline data || redundancy == - __num__ ) { bucket persistence advisor persist advisor = get persistence advisor ( ) ; if ( persist advisor != null ) { if ( ! persist advisor . was hosting ( ) && advisor . get had primary ( ) ) { final persistent membership view membership view = persist advisor . get membership view ( ) ; if ( membership view == null ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . partitioned region . getpr id ( ) @$ partitioned region . bucket_id_separator @$ bid ) ; } return false ; } set < persistent memberid > offline members = membership view .,region has,success,pre
per javadoc log exceptions but still go online . note : this does not include errors @$ which <PLACE_HOLDER> a fatal problem,logger . log ( warning @$ string . format ( __str__ @$ cl . get class ( ) ) @$ e ) ;,which indicate,success,pre
get the app <PLACE_HOLDER> aggregation impl thread to crash,local dirs handler service mocked dir svc = mock ( local dirs handler service . class ) ; log aggregation service log aggregation service = new log aggregation service ( dispatcher @$ this . context @$ del srvc @$ mocked dir svc ) ; log aggregation service . init ( this . conf ) ; log aggregation service . start ( ) ; application id application1 = builder utils . new application id ( __num__ @$ __num__ ) ; log aggregation service . handle ( new log handler app started event ( application1 @$ this . user @$ null @$ this . acls ) ) ; log aggregation service . handle ( new log handler app finished event ( application1 ) ) ; dispatcher . await ( ),app log,success,pre
invoke the method to have the subclass <PLACE_HOLDER> entities,try { write entities ( tl conf @$ manager @$ context ) ; } finally { manager . close ( ) ; },subclass write,success,pre
depth first search from root directory for all application <PLACE_HOLDER> dirs,remote iterator < file status > iter = list ( dirpath ) ; while ( iter . has next ( ) ) { file status stat = iter . next ( ) ; path child path = stat . get path ( ) ; if ( stat . is directory ( ) ) { application id app id = parse application id ( child path . get name ( ) ) ; if ( app id != null ) { app log dir present . set true ( ) ; if ( should clean app log dir ( child path @$ now @$ fs @$ retain millis ) ) { delete dir ( child path ) ; } } else { clean app log dir ( child path,search log,success,pre
double precision only <PLACE_HOLDER> 52 bits of mantissa,if ( level > __num__ ) return point crossings for line ( px @$ py @$ x0 @$ y0 @$ x1 @$ y1 ) ; double xmid = ( xc0 + xc1 ) / __num__ ; double ymid = ( yc0 + yc1 ) / __num__ ; xc0 = ( x0 + xc0 ) / __num__ ; yc0 = ( y0 + yc0 ) / __num__ ; xc1 = ( xc1 + x1 ) / __num__ ; yc1 = ( yc1 + y1 ) / __num__ ; double xc0m = ( xc0 + xmid ) / __num__ ; double yc0m = ( yc0 + ymid ) / __num__ ; double xmc1 = ( xmid + xc1 ) / __num__ ; double ymc1 = ( ymid + yc1 ),precision has,success,pre
godin : note that other channels <PLACE_HOLDER> method delete in order to do the same thing,tmp builder . set length ( __num__ ) ; return true ;,channels call,fail,pre
test case 8 : 2 components @$ 2 instances for each comp 2 already finished . comp 1 <PLACE_HOLDER> a new instance finish @$ we should terminate the service,comp = create component ( service scheduler @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component1 instances = comp . get all component instances ( ) ; container status . set exit status ( - __num__ ) ; component comp2 = create component ( component instance . get component ( ) . get scheduler ( ) @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component2 instances = comp2 . get all component instances ( ) ; map <,components has,success,pre
check this tree <PLACE_HOLDER> this method by having properly populated core label 's,list < tree > leaves = this . get leaves ( ) ; if ( ! ( leaves . get ( __num__ ) . label ( ) instanceof core label ) ) { throw new illegal argument exception ( __str__ ) ; } else if ( ( ( core label ) leaves . get ( __num__ ) . label ( ) ) . word ( ) == null ) { throw new illegal argument exception ( __str__ ) ; } else if ( ( ( core label ) leaves . get ( __num__ ) . label ( ) ) . after ( ) == null ) { throw new illegal argument exception ( __str__ ) ; } list < core label > core labels = this . get,tree overrides,fail,pre
inflater <PLACE_HOLDER> a bit of slack,long size = get entry size ( jzentry ) + __num__ ;,inflater gives,fail,pre
check whether the buffer <PLACE_HOLDER> the data to be read,if ( cur < bar ) { pos ++ ; return buff [ cur ] ; },buffer contains,success,pre
<PLACE_HOLDER> container collection that we get back is immutable so we keep track of which containers need to be deleted after they 've been folded into the janus graph step and then remove them from their step using <PLACE_HOLDER> container.remove <PLACE_HOLDER> container,final list < has container > removable has containers = new array list < > ( ) ; final set < string > step labels = current step . get labels ( ) ; has container holder . get has containers ( ) . for each ( has container -> { if ( graph step . process has container ids ( graph step @$ has container ) ) { step labels . for each ( janusgraph step :: add label ) ; removable has containers . add ( has container ) ; } } ) ;,collection has,success,pre
do n't calculate cost if the server does n't <PLACE_HOLDER> enough space or is loading the segment,if ( proposal segment size > server . get available size ( ) || server . is loading segment ( proposal segment ) ) { return double . positive_infinity ; },server have,success,pre
do we need to pad the file so the stripe does n't <PLACE_HOLDER> a block boundary ?,long start = raw writer . get bytes written ( ) ; final long current stripe size = index size + data size + footer . get serialized size ( ) ; final long available = block size - ( start % block size ) ; final long overflow = current stripe size - adjusted stripe size ; final float avail ratio = ( float ) available / ( float ) default stripe size ; if ( avail ratio > __num__ && avail ratio < __num__ && avail ratio > padding tolerance ) { float correction = overflow > __num__ ? ( float ) overflow / ( float ) adjusted stripe size : __num__ ; correction = correction > padding tolerance ? padding tolerance : correction ; adjusted,stripe exceed,fail,pre
no alias to <PLACE_HOLDER> no local task,if ( localwork . get alias to fetch work ( ) . is empty ( ) ) { new local work . set has staged alias ( false ) ; curr task . set backup task ( local task . get backup task ( ) ) ; curr task . set backup children tasks ( local task . get backup children tasks ( ) ) ; return ; },alias fetch,fail,pre
unless an accept gzip parameter is explicitly set to false @$ respond that this server <PLACE_HOLDER> gzip,if ( ! boolean . to string ( false ) . equals ignore case ( request . get parameter ( __str__ ) ) ) { response . set header ( __str__ @$ __str__ ) ; },server accepts,success,pre
we need to check if the class <PLACE_HOLDER> a custom timer name,return class timed == null || class timed . name ( ) . is empty ( ) ? metric registry . name ( clazz @$ method name ) : metric registry . name ( class timed . name ( ) @$ method name ) ;,class has,success,pre
element <PLACE_HOLDER> end if policy element found,return false ;,element found,fail,pre
we need to run the subprocedure even if we have no relevant regions . the coordinator <PLACE_HOLDER> participation in the procedure and without sending message the snapshot attempt will hang and fail .,log . debug ( __str__ + snapshot . get name ( ) + __str__ + snapshot . get table ( ) + __str__ + snapshot . get type ( ) ) ; foreign exception dispatcher exn dispatcher = new foreign exception dispatcher ( snapshot . get name ( ) ) ; configuration conf = rss . get configuration ( ) ; long timeout millis = conf . get long ( snapshot_timeout_millis_key @$ snapshot_timeout_millis_default ) ; long wake millis = conf . get long ( snapshot_request_wake_millis_key @$ snapshot_request_wake_millis_default ) ; switch ( snapshot . get type ( ) ) { case flush : snapshot subprocedure pool task manager = new snapshot subprocedure pool ( rss . get server name ( ) . to string ( ) @$ conf @$,coordinator expects,success,pre
this method will only <PLACE_HOLDER> full resource plan when activating one @$ to give the caller the result atomically with the activation .,wm full resource plan full plan after alter = getms ( ) . alter resource plan ( request . get resource plan name ( ) @$ request . get ns ( ) @$ request . get resource plan ( ) @$ request . is is enable and activate ( ) @$ request . is is force deactivate ( ) @$ request . is is replace ( ) ) ; if ( full plan after alter != null ) { response . set full resource plan ( full plan after alter ) ; } return response ;,method update,fail,pre
if the filter does not <PLACE_HOLDER> bound values @$ execute query using full engine,if ( ! needs bound value ( filter expression ) ) { materialized result result = runner . execute ( __str__ + filter ) ; assert equals ( result . get types ( ) . size ( ) @$ __num__ ) ; boolean query result ; if ( result . get materialized rows ( ) . is empty ( ) ) { query result = false ; } else { assert equals ( result . get materialized rows ( ) . size ( ) @$ __num__ ) ; query result = ( boolean ) iterables . get only element ( result . get materialized rows ( ) ) . get field ( __num__ ) ; } results . add ( query result ) ; } return results ;,filter need,success,pre
is ok when considering two zero duration inervals this is the simplest case @$ as the two intervals either <PLACE_HOLDER> a gap or not if not @$ then they are equal and abut,interval test0808 = new interval ( __num__ @$ __num__ ) ; interval test1010 = new interval ( __num__ @$ __num__ ) ;,intervals have,success,pre
this intentionally does disconnect without locking the vaadin session to avoid deadlocks where the server <PLACE_HOLDER> a lock for the websocket connection,new thread ( ( ) -> { set push connection ( null ) ; } ) . start ( ) ;,server has,fail,pre
if the decision point list <PLACE_HOLDER> either of the parent rows @$ update it to include the new row as well,if ( ( decision point list . contains ( new integer ( old row num ) ) || decision point list . contains ( new integer ( new row num ) ) ) && ! decision point list . contains ( new integer ( combined row num ) ) ) { decision point list . add element ( new integer ( combined row num ) ) ; },list contains,success,pre
we know that the parameters <PLACE_HOLDER> the same name @$ since this is what the descriptor 's hash code & equality are based on . the only thing that may be different is the description . and since the proposed parameter does not <PLACE_HOLDER> a description @$ we want to use whatever is currently set .,return old parameter == null ? descriptor : old parameter . get descriptor ( ) ;,parameter have,success,pre
when tag compression is been used in this file @$ tag compression context will have a not null value <PLACE_HOLDER> .,if ( tag compression context != null ) { tag compression context . uncompress tags ( source @$ dest @$ tags length ) ; } else { byte buffer utils . copy from stream to buffer ( dest @$ source @$ tags length ) ; },context set,fail,pre
we need to pass the meta from the pod in the deployment as that is what <PLACE_HOLDER> machine name,final object meta template meta = to create . get spec ( ) . get template ( ) . get metadata ( ) ; store starting machine ( created pod @$ template meta @$ machine configs @$ server resolver ) ;,what generates,fail,pre
first describe what is <PLACE_HOLDER> a long time .,switch ( m_fragment context ) { default : case ro_batch : case rw_batch : sb . append ( __str__ + m_current procedure name ) ; break ; case catalog_update : sb . append ( __str__ ) ; break ; case catalog_load : sb . append ( __str__ ) ; break ; },what taking,success,pre
attention : only update the <PLACE_HOLDER> values for sorted storage method,double [ ] values ; if ( vector storage utils . use int key ( vector ) ) { values = get int double vector ( ) . get storage ( ) . get values ( ) ; } else { values = get long double vector ( ) . get storage ( ) . get values ( ) ; } for ( int i = __num__ ; i < values . length ; i ++ ) { values [ i ] = func . update ( ) ; },attention exist,success,pre
map types <PLACE_HOLDER> as key enum no default as value,ret . add ( get attribute def ( attr name prefix @$ atlas base type def . get map type name ( attribute type @$ enum_def_with_no_default . get name ( ) ) ) ) ;,types have,fail,pre
get transform record ; if no transform is found @$ will <PLACE_HOLDER> illegal argument exception,transform record info = user record . m transform records . get resource or throw ( resource id ) ;,record throw,success,pre
the status from error response overwrites the one in <PLACE_HOLDER> response . however @$ results and errors are not expected to have overlapping key . see batch <PLACE_HOLDER> response builder .,if ( error data != null ) { update data . put ( __str__ @$ error data . get ( __str__ ) ) ; update data . put ( __str__ @$ error data ) ; },batch write,fail,pre
remember that these elements <PLACE_HOLDER> updated,for ( element element : instance . non zeroes ( ) ) { int j = element . index ( ) ; update steps . set quick ( j @$ get step ( ) ) ; update counts . increment quick ( j @$ __num__ ) ; } next step ( ) ;,elements got,success,pre
remove the linkstamp objects from inputs so that create linkstamp <PLACE_HOLDER> action does n't cause a circular dependency .,immutable set < artifact > expanded linker artifacts no linkstamps = sets . difference ( expanded linker artifacts @$ linkstamp object artifacts ) . immutable copy ( ) ; cc toolchain variables variables ; try { immutable list . builder < string > user link flags = immutable list . < string > builder ( ) . add all ( linkopts ) . add all ( cpp configuration . get linkopts ( ) ) ; if ( is lto indexing && cpp configuration . use standalone lto indexing command lines ( ) ) { user link flags . add all ( cpp configuration . get lto index options ( ) ) ; } variables = link build variables . setup variables ( get link type ( ) .,linkstamp build,fail,pre
<PLACE_HOLDER> tree <PLACE_HOLDER> name iterator returns all paths in the directory tree which <PLACE_HOLDER> name is build <PLACE_HOLDER> name strip out build <PLACE_HOLDER> name to get a path pointing to package root folder all paths are relative,package roots = immutable sorted set . copy of ( iterators . transform ( file tree file name iterator . of ( file tree @$ build file name ) @$ path -> more paths . get parent or empty ( path ) ) ) ; break ; default : throw new illegal state exception ( ) ;,which file,success,pre
the window manager will <PLACE_HOLDER> us a valid window token,if ( m show dialog for submenu ) { m sub menu helper = new menu dialog helper ( sub menu ) ; m sub menu helper . show ( null ) ; return true ; },manager give,success,pre
the following should <PLACE_HOLDER> over a second,secure random rand = new secure random ( ) ; rand . next bytes ( new byte [ __num__ ] ) ;,following take,success,pre
note : a valid authorization response <PLACE_HOLDER> either a 'code ' or 'error ' parameter .,http servlet response response = mock ( http servlet response . class ) ; filter chain filter chain = mock ( filter chain . class ) ; this . filter . do filter ( request @$ response @$ filter chain ) ; verify ( filter chain ) . do filter ( any ( http servlet request . class ) @$ any ( http servlet response . class ) ) ;,response contains,success,pre
the caller <PLACE_HOLDER> a data node,for ( string datanode uuid : data node uuids ) { boolean found = false ; for ( int index = __num__ ; index < info . length ; index ++ ) { if ( info [ index ] . get datanode uuid ( ) . equals ( datanode uuid ) ) { data node names . add ( info [ index ] . get xfer addr ( ) ) ; datanode infos . add ( name node adapter . get datanode ( cluster . get namesystem ( nn index ) @$ info [ index ] ) ) ; found = true ; break ; } } if ( ! found ) { throw new io exception ( __str__ + datanode uuid ) ; } },caller reached,fail,pre
note that this might miss collisions @$ use internal tick callback to check for collision on every tick . <PLACE_HOLDER> internal tick test on how to implement it .,contacts . clear ( ) ;,note see,success,pre
known previous uid @$ so we know which package <PLACE_HOLDER> to check,for ( string pkg : package names ) { hash set < string > set = m backup participants . get ( old uid ) ; if ( set != null && set . contains ( pkg ) ) { remove package from set locked ( set @$ pkg ) ; if ( set . is empty ( ) ) { if ( more_debug ) slog . v ( tag @$ __str__ ) ; m backup participants . remove ( old uid ) ; } } },package has,fail,pre
make sure dn 's jmx <PLACE_HOLDER> the failed volume,final string [ ] expected failed volumes = { dn1 vol1 . get absolute path ( ) } ; data node test utils . trigger heartbeat ( dn ) ; fs dataset spi < ? > fsd = dn . getfs dataset ( ) ; assert equals ( expected failed volumes . length @$ fsd . get num failed volumes ( ) ) ; assert array equals ( expected failed volumes @$ convert to absolute paths ( fsd . get failed storage locations ( ) ) ) ;,jmx has,fail,pre
this filter <PLACE_HOLDER> all children to stay in the view,set filter text ( __str__ ) ; tree path first visible path = scroll to ( __num__ ) ; set filter text ( __str__ ) ; tree path new first visible path = get last visible path ( ) ; assert close enough ( first visible path @$ new first visible path ) ;,filter requires,fail,pre
if all the others <PLACE_HOLDER> parens @$ this is likely a bug,assert equals ( __str__ @$ native meta . get modify column statement ( __str__ @$ new value meta string ( __str__ @$ __num__ @$ __num__ ) @$ __str__ @$ false @$ __str__ @$ false ) ) ;,others use,fail,pre
if any provider <PLACE_HOLDER> been disabled @$ clear all last locations for all providers . this is to be on the safe side in case a provider <PLACE_HOLDER> location derived from this disabled provider .,if ( ! m useable ) { m last location . clear ( ) ; m last location coarse interval . clear ( ) ; },provider has,success,pre
client 1 <PLACE_HOLDER> listener and put all,async invocation < void > register interest and put all in client1 = client1 . invoke async ( ( ) -> { region < string @$ ticker data > region = get cache ( ) . get region ( region name ) ; region . get attributes mutator ( ) . add cache listener ( new counting cache listener < > ( ) ) ; region . register interest ( __str__ ) ; do put all ( region @$ title @$ one_hundred ) ; } ) ;,client add,success,pre
same as above @$ except this is a bit of stress testing . <PLACE_HOLDER> 5 database files and make sure they are all removed .,int n = __num__ ; array list < string > attached db files = new array list < string > ( n ) ; for ( int i = __num__ ; i < n ; i ++ ) { attached db files . add ( m database . get path ( ) + i ) ; } db obj = sq lite database . open or create database ( m database . get path ( ) @$ null ) ; db obj . execsql ( __str__ ) ; for ( int i = __num__ ; i < n ; i ++ ) { db obj . execsql ( __str__ + attached db files . get ( i ) + __str__ + i ) ; } assert true (,same exclude,fail,pre
release only tickets that have been flushed . <PLACE_HOLDER> the rest .,iterator < flush ticket > ticket iterator = m ticket set . iterator ( ) ; while ( ticket iterator . has next ( ) ) { flush ticket ticket = ticket iterator . next ( ) ; ticket iterator . remove ( ) ; if ( ticket . get target counter ( ) <= m flush counter . get ( ) ) { ticket . set completed ( ) ; } else { ticket . set error ( exc ) ; } },release skip,fail,pre
since the 200 was committed @$ the 500 did not <PLACE_HOLDER> the chance to be written,assert that ( __str__ @$ response . get status ( ) @$ is ( __num__ ) ) ; assert that ( __str__ @$ response . get content ( ) @$ is ( __str__ ) ) ; assuming that ( http version == http version . http_1_1 @$ ( ) -> assert that ( response @$ contains header value ( __str__ @$ __str__ ) ) ) ;,500 get,success,pre
only append if target does not <PLACE_HOLDER> the key,if ( ! target . contains key ( key ) ) { target . put ( key @$ entry . get value ( ) . get value ( profiles ) ) ; },target contain,success,pre
at the moment @$ web assembly only <PLACE_HOLDER> one memory instance @$ thus the only valid memory index is 0 .,assert . assert int equal ( mem index @$ __num__ @$ __str__ ) ; long data offset = __num__ ; byte instruction ; do { instruction = read1 ( ) ; switch ( instruction ) { case instructions . i32_const : data offset = read signed int32 ( ) ; break ; case instructions . global_get : read global index ( ) ; throw new wasm exception ( __str__ ) ; case instructions . end : break ; default : assert . fail ( string . format ( __str__ @$ instruction ) ) ; } } while ( instruction != instructions . end ) ;,assembly supports,success,pre
some adjustments to not <PLACE_HOLDER> the chartbuilding with empty data,if ( m max elements == __num__ ) { m max elements = __num__ ; } if ( m mcount == __num__ ) { m mcount = __num__ ; } if ( m first element == m last element ) { m first element = __num__ ; m last element = __num__ ; } if ( m max cards == __num__ ) m max cards = __num__ ; meta info . setm dynamic axis ( true ) ; meta info . setm has colored cumulative ( true ) ; meta info . setm type ( type ) ; meta info . setm title ( r . string . stats_forecast ) ; meta info . setm backwards ( true ) ; meta info . setm value labels ( m,adjustments crash,success,pre
replica <PLACE_HOLDER> name with no address ...,host and port = server address . default host ( ) ;,replica has,fail,pre
if any of the declared signatures <PLACE_HOLDER> the package signature @$ it 's valid .,for ( signature signature : provider . signatures ) { if ( signature . equals ( package info . signatures [ __num__ ] ) ) return true ; } return false ;,any matches,fail,pre
negative transition @$ which <PLACE_HOLDER> a duplicated local time range,if ( ( ( duplicated time opt & std_dst_mask ) == local_std && dst to std ) || ( ( duplicated time opt & std_dst_mask ) == local_dst && std to dst ) ) { transition += offset after ; } else if ( ( ( duplicated time opt & std_dst_mask ) == local_std && std to dst ) || ( ( duplicated time opt & std_dst_mask ) == local_dst && dst to std ) ) { transition += offset before ; } else if ( ( duplicated time opt & former_latter_mask ) == local_former ) { transition += offset before ; } else { transition += offset after ; },which makes,success,pre
join keys <PLACE_HOLDER> difference sizes ?,if ( k1 . size ( ) != k2 . size ( ) ) { return k1 . size ( ) - k2 . size ( ) ; } if ( comparators . length == __num__ ) { return __num__ ; },keys have,success,pre
last chance @$ look in the old hive config value . still <PLACE_HOLDER> defaults .,if ( reporters to start == null ) { reporters to start = conf . get ( metastore conf . conf vars . hive_metrics_reporter . get hive name ( ) ) ; if ( reporters to start == null ) { reporters to start = metastore conf . get var ( conf @$ metastore conf . conf vars . metrics_reporters ) ; } },chance need,fail,pre
iterator should <PLACE_HOLDER> an item when dir is not empty,it = fs . list status iterator ( d ) ; assert true ( it . has next ( ) ) ; it . next ( ) ; assert false ( it . has next ( ) ) ; for ( int i = __num__ ; i < list limit * __num__ ; i ++ ) { p = new path ( d @$ __str__ + i ) ; assert . assert true ( fs . create new file ( p ) ) ; },iterator have,success,pre
this will be picked up by janitor to figure out what really happened and correct the state if needed note that the default case <PLACE_HOLDER> the null status,case undefined : default : return transaction status . unknown ;,case contains,fail,pre
move over a bit so the oval does n't <PLACE_HOLDER> the text,int label padding = __num__ ; location . x -= label padding * __num__ ; location . y -= label padding * __num__ ;,oval touch,fail,pre
<PLACE_HOLDER> this loop forever in case a hang in the first client prevents the listener from ever starting . this loop should <PLACE_HOLDER> the test timeout and thus we 'll get a thread dump,while ( true ) { try { s = new socket ( __str__ @$ node1 port ) ; break ; } catch ( connect exception ce ) { thread . sleep ( __num__ ) ; } } input stream in = s . get input stream ( ) ; final long end = system . current time millis ( ) + __num__ ; while ( system . current time millis ( ) < end ) { log ( __str__ ) ; thread . sleep ( __num__ ) ; try { if ( in . read ( ) < __num__ ) { break ; } } catch ( io exception ioe ) { break ; } } if ( system . current time millis ( ) > end ),loop make,success,pre
all <PLACE_HOLDER> ; now <PLACE_HOLDER> up the ipc and launch the agent,set up pipes ( ) ; m agent = m backup manager service . bind to agent synchronous ( m target app @$ full backup . key_value_data_token . equals ( info . domain ) ? application thread constants . backup_mode_incremental : application thread constants . backup_mode_restore_full ) ; m agent package = pkg ;,all set,success,pre
all types must <PLACE_HOLDER> the same number of children,if ( type . get children ( ) . size ( ) != number of children ) { return null ; },types have,success,pre
n.b . do n't <PLACE_HOLDER> regexp field to empty to keep regexp,this . regexp data field . set text ( __str__ ) ;,n.b set,success,pre
check <PLACE_HOLDER> content and bucket id .,map < file @$ string > contents = test utils . get file content by path ( out dir ) ; for ( map . entry < file @$ string > file contents : contents . entry set ( ) ) { integer bucket id = integer . parse int ( file contents . get key ( ) . get parent file ( ) . get name ( ) ) ; assert . assert true ( bucket id >= __num__ && bucket id <= __num__ ) ; assert . assert equals ( string . format ( __str__ @$ bucket id @$ bucket id ) @$ file contents . get value ( ) ) ; },check file,success,pre
another listener should not have <PLACE_HOLDER> any notifications,assert that ( another queue . is empty ( ) @$ is ( true ) ) ; kv state registry . unregister kv state ( job id @$ job vertex id @$ key group range @$ registration name @$ kv stateid ) ; assert that ( state deregistration notifications . poll ( ) @$ equal to ( job id ) ) ;,listener received,success,pre
same type @$ request code and intent action <PLACE_HOLDER> equality .,assert that ( pending intent . get activity ( context @$ __num__ @$ new intent ( __str__ ) @$ flag_no_create ) ) . is same instance as ( pending intent ) ;,code implies,success,pre
priority 4 : find all <PLACE_HOLDER> java installations @$ and use the newest .,list < file > java home dirs = java finder . find supported java home from installations ( java config @$ java filter ) ; if ( ! java home dirs . is empty ( ) ) { java home dir = java home dirs . iterator ( ) . next ( ) ; if ( save ) { java config . save java home ( java home dir ) ; } system . out . println ( java home dir ) ; return exit_success ; } return exit_failure ;,all supported,success,pre
get the annotation <PLACE_HOLDER> witht the local attr decl,if ( attr decl . get attribute node ( schema symbols . att_ref ) == null ) { attr use . f annotations = attribute . get annotations ( ) ; } else { xs object list annotations ; if ( annotation != null ) { annotations = new xs object list impl ( ) ; ( ( xs object list impl ) annotations ) . addxs object ( annotation ) ; } else { annotations = xs object list impl . empty_list ; } attr use . f annotations = annotations ; },annotation object,fail,pre
buffers are only discarded after they are acked . discarding them here would <PLACE_HOLDER> the sender to generate too much work for the receiver .,m_future . set ( true ) ;,them cause,success,pre
use a new redirect each time as different clients could be <PLACE_HOLDER> the console with different host names,final redirect handler redirect handler = new redirect handler ( location ) ; redirect handler . handle request ( exchange ) ;,time entering,fail,pre
initialize the default <PLACE_HOLDER> mode .,get view ( ) . add view mode ( m_edit mode ) ; m_edit mode listener = new internal edit mode listener < node type @$ edge type > ( m_graph listeners ) ; m_edit mode . add listener ( m_edit mode listener ) ;,default edit,success,pre
a volt db extension to <PLACE_HOLDER> indexed expressions and partial indexes,expression predicate = ( expression ) arguments [ __num__ ] ; @ suppress warnings ( __str__ ) java . util . list < expression > index exprs = ( java . util . list < expression > ) arguments [ __num__ ] ; boolean assume unique = ( ( boolean ) arguments [ __num__ ] ) . boolean value ( ) ; if ( index exprs != null ) { table works . add expr index ( index columns @$ index exprs . to array ( new expression [ index exprs . size ( ) ] ) @$ name @$ unique @$ migrating @$ predicate ) . set assume unique ( assume unique ) ; break ; } org . hsqldb_voltpatches . index . index added index =,extension support,success,pre
check access in case the principal <PLACE_HOLDER> no authorization rights,try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_general ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { } try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_auditing ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { } try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_ownership ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { },principal has,success,pre
caller did not <PLACE_HOLDER> any root cause @$ so just use our own .,if ( throwable == null ) { return new cache closed exception ( reason @$ disconnect cause ) ; },caller specify,success,pre
delegate to the old behavior @$ <PLACE_HOLDER> implementations to override .,map to map from id ( data @$ obj ) ;,delegate allows,fail,pre
compensate for the <PLACE_HOLDER> delimiter,s . request ( __num__ ) ; g . on next ( t ) ;,the get,fail,pre
this function also <PLACE_HOLDER> new entity,scan entity reference ( f content buffer ) ;,function creates,fail,pre
snapshot directory @$ as some file system implementations do not <PLACE_HOLDER> the parent directory 's mod time when there are new sub items @$ for example @$ s 3 .,file status [ ] snapshot dirs = fs utils . list status ( fs @$ snapshot dir @$ p -> ! p . get name ( ) . equals ( snapshot description utils . snapshot_tmp_dir_name ) ) ;,implementations set,fail,pre
ensure that committed watermarks <PLACE_HOLDER> exactly the input rows because we shutdown in an orderly manner .,assert . assert true ( one record extractor . validate watermarks ( true @$ external watermark storage ) ) ;,watermarks match,success,pre
make sure the client <PLACE_HOLDER> the response correctly,test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( __str__ + client conn . get data events buffer ( ) . last written scn ( ) ) ; return client conn . get data events buffer ( ) . last written scn ( ) == __num__ ; } } @$ __str__ + consumer . get sequences ( ) @$ __num__ @$ log ) ; test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( __str__ + consumer . get event num ( ) ) ; return stats . get total stats ( ) . get num data,client processes,success,pre
insert fake stats with the correct modification time . the call should <PLACE_HOLDER> the fake stats,format = new dummy file input format ( ) ; format . set file path ( temp dir ) ; format . configure ( new configuration ( ) ) ; file base statistics out dated fake stats = new file base statistics ( math . min ( math . min ( mod time1 @$ mod time2 ) @$ mod time3 ) - __num__ @$ fake_size @$ base statistics . avg_record_bytes_unknown ) ; base statistics re gathered = format . get statistics ( out dated fake stats ) ; assert . assert equals ( __str__ @$ total @$ re gathered . get total input size ( ) ) ;,call return,success,pre
animated image drawable <PLACE_HOLDER> post process and release only if m post processor exists .,if ( decoder . m animated ) { image decoder post process ptr = decoder . m post processor == null ? null : decoder ; decoder . check state ( true ) ; drawable d = new animated image drawable ( decoder . m native ptr @$ post process ptr @$ decoder . m desired width @$ decoder . m desired height @$ decoder . get color space ptr ( ) @$ decoder . check for extended ( ) @$ src density @$ src . compute dst density ( ) @$ decoder . m crop rect @$ decoder . m input stream @$ decoder . m asset fd ) ; decoder . m input stream = null ; decoder . m asset fd = null ; return,drawable set,fail,pre
the data is locked in the cache @$ or we 're ignoring the cache . <PLACE_HOLDER> the cache and read from upstream .,if ( next span == null ) { next data source = upstream data source ; next data spec = new data spec ( uri @$ http method @$ null @$ read position @$ read position @$ bytes remaining @$ key @$ flags ) ; } else if ( next span . is cached ) { uri file uri = uri . from file ( next span . file ) ; long file position = read position - next span . position ; long length = next span . length - file position ; if ( bytes remaining != c . length_unset ) { length = math . min ( length @$ bytes remaining ) ; } next data spec = new data spec ( file uri @$,cache enforce,fail,pre
eap 7 does not <PLACE_HOLDER> the clone operation,profile clone eap7x ( ) ;,eap have,success,pre
check the next check <PLACE_HOLDER> backoff @$ because there 's no point of sending a request immediately only to get a 'connection refused ' error .,final stopwatch stopwatch = stopwatch . create started ( ) ; health check request logs . take ( ) ; assert that ( stopwatch . elapsed ( time unit . milliseconds ) ) . is greater than ( retry_interval . to millis ( ) * __num__ / __num__ ) ;,check has,fail,pre
use a custom <PLACE_HOLDER> function for put values .,super ( ( old item @$ new item ) -> new git hub repository ( old item ) . overwrite ( new item ) @$ git hub repository :: get id @$ repository -> repository != null ? repository : git hub repository . none ( ) @$ git hub repository :: none ) ;,custom overwrite,fail,pre
set throwable to holder first because asynchronous fork : :put record can <PLACE_HOLDER> the throwable when it detects fork state.failed status .,fork throwable holder holder = task . get fork throwable holder ( this . broker ) ; holder . set throwable ( this . get index ( ) @$ t ) ; this . fork state . set ( fork state . failed ) ; this . logger . error ( string . format ( __str__ @$ this . index @$ this . task id @$ holder ) @$ t ) ;,record set,fail,pre
this test can be very liberal . too liberal will just <PLACE_HOLDER> some extra refreshes . too conservative will display obsolete info .,if ( upper . index of ( __str__ ) > - __num__ || upper . index of ( __str__ ) > - __num__ || upper . index of ( __str__ ) > - __num__ ) { direct refresh tree ( ) ; },liberal show,fail,pre
if the proxy host requires authentication then <PLACE_HOLDER> the host credentials to the credentials provider,final auth configuration auth = proxy . get auth ( ) ; if ( auth != null ) { if ( credentials provider == null ) { credentials provider = new basic credentials provider ( ) ; } auth scope auth scope = new auth scope ( http host @$ auth . get realm ( ) @$ auth . get auth scheme ( ) ) ; credentials credentials = configure credentials ( auth ) ; credentials provider . set credentials ( auth scope @$ credentials ) ; },authentication pass,fail,pre
if another engine has <PLACE_HOLDER> the async history session factory already @$ there 's no need to do it again .,if ( ! session factories . contains key ( async history session . class ) ) { async history session factory async history session factory = new async history session factory ( ) ; if ( async history listener == null ) { init default async history listener ( ) ; } async history session factory . set async history listener ( async history listener ) ; session factories . put ( async history session . class @$ async history session factory ) ; } ( ( async history session factory ) session factories . get ( async history session . class ) ) . register job data types ( cmmn async history constants . ordered_types ) ;,engine initialized,fail,pre
the super implementation does not <PLACE_HOLDER> the following parameters,params . set endpoint identification algorithm ( identification protocol ) ; params . set algorithm constraints ( algorithm constraints ) ; if ( sni matchers . is empty ( ) && ! no sni matcher ) { params . setsni matchers ( null ) ; } else { params . setsni matchers ( sni matchers ) ; },implementation handle,success,pre
the system ui <PLACE_HOLDER> the state of app ops and updates the location icon accordingly .,intent intent = new intent ( location manager . high_power_request_change_action ) ; m context . send broadcast as user ( intent @$ user handle . all ) ;,ui updates,fail,pre
its not unique @$ <PLACE_HOLDER> a suitable index ...,string stub = name . substring ( __num__ @$ name . length ( ) - ext . length ( ) - __num__ ) ; int index = __num__ ; do { index ++ ; name = stub + __str__ + index + __str__ + ext ; } while ( this . get script impl ( name ) != null ) ; return name ;,its add,success,pre
this call generally deletes any files at locations that are declared outputs of the action @$ although some actions perform additional work @$ while others intentionally <PLACE_HOLDER> previous outputs in place .,try ( silent closeable d = profiler . profile ( profiler task . info @$ __str__ ) ) { action . prepare ( action execution context . get exec root ( ) ) ; } catch ( io exception e ) { throw to action execution exception ( __str__ @$ e @$ action @$ null ) ; },others take,fail,pre
ensure wrapper view child <PLACE_HOLDER> a header,wrapper view wrapper view child = ( wrapper view ) child ; if ( ! wrapper view child . has header ( ) ) { continue ; },child has,success,pre
each volume <PLACE_HOLDER> 2 blocks,int initial block count = num volumes * __num__ ; create file ( test file @$ initial block count ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; final fs dataset spi < ? extends fs volume spi > data = dn . data ; dn . data = mockito . spy ( data ) ; final int new volume count = __num__ ; list < thread > add volume delayed threads = collections . synchronized list ( new array list < > ( ) ) ; atomic boolean add volume error = new atomic boolean ( false ) ; atomic boolean list storage error = new atomic boolean ( false ) ; count down latch add volume,volume has,success,pre
first let 's <PLACE_HOLDER> all children which do n't belong in the parents,array list < expandable notification row > to remove = new array list < > ( ) ; for ( int i = __num__ ; i < m list container . get container child count ( ) ; i ++ ) { view view = m list container . get container child at ( i ) ; if ( ! ( view instanceof expandable notification row ) ) { continue ; } expandable notification row parent = ( expandable notification row ) view ; list < expandable notification row > children = parent . get notification children ( ) ; list < expandable notification row > ordered children = m tmp child order map . get ( parent ) ; if ( children != null ) { to,'s remove,success,pre
everything <PLACE_HOLDER> a type even arrays and null,size += __num__ ;,everything has,success,pre
the list of remote printers got updated @$ so update the cached list printers which <PLACE_HOLDER> both local and network printers,if ( ! arrays . equals ( prev remote printers @$ current remote printers ) ) { refresh services ( ) ; prev remote printers = current remote printers ; },which contains,fail,pre
call the <PLACE_HOLDER> method to recursively fetch the resource,if ( obj != null ) { obj = obj . get ( a key @$ aliases visited @$ requested ) ; },the get,success,pre
make sure that in the working dir abc.jar <PLACE_HOLDER> the new content,assert that ( file utils . read file to string ( jar1 . to file ( ) @$ __str__ ) ) . is equal to ( __str__ ) ; verify ( config region @$ times ( __num__ ) ) . put ( eq ( __str__ ) @$ argument captor . capture ( ) @$ eq ( __str__ ) ) ; configuration = argument captor . get value ( ) ; assert that ( configuration . get jar names ( ) ) . contains exactly in any order ( __str__ @$ __str__ ) ;,abc.jar has,success,pre
if both <PLACE_HOLDER> @$ take longest,if ( got positive && got negative ) { if ( positive suffix . length ( ) > negative suffix . length ( ) ) { got negative = false ; } else if ( positive suffix . length ( ) < negative suffix . length ( ) ) { got positive = false ; } },both exist,fail,pre
1 because a send <PLACE_HOLDER> the queue to be recreated again which sends a new demand advisory,assert remote advisory count ( advisory consumer @$ __num__ ) ; assert advisory broker counts ( __num__ @$ __num__ @$ __num__ ) ;,send triggers,fail,pre
<PLACE_HOLDER> collision with keyword in another release . <PLACE_HOLDER> collision with keyword in letter @$ but not in case .,if ( ( token . equals ( token . identifier ) || token . equals ( token . macro identifier ) ) && ! token . is escaped ( ) ) { if ( token . collides with keyword ( ) ) parse exception . warning ( scanner @$ util . get message ( __str__ @$ token . name ) ) ; },identifier try,fail,pre
unregister should <PLACE_HOLDER> dropwizard listeners,registry . remove ( metric name . name ( __str__ ) . build ( ) ) ; mockito . verify ( listener ) . on gauge removed ( mockito . eq ( __str__ ) ) ; assert equals ( __num__ @$ registry . get gauges ( ) . size ( ) ) ; assert equals ( __num__ @$ registry . get metric registry ( ) . get gauges ( ) . size ( ) ) ;,unregister remove,fail,pre
test that both spawns <PLACE_HOLDER> the local tag attached as a execution info,assert that ( spawn . get execution info ( ) ) . contains key ( __str__ ) ; action execution context context = invocation . get argument ( __num__ ) ; file out err out err = context . get file out err ( ) ; called . add ( out err ) ; if ( spawn . get output files ( ) . size ( ) != __num__ ) { try ( output stream stream = out err . get output stream ( ) ) { stream . write ( __str__ . get bytes ( utf_8 ) ) ; stream . write ( ( test log helper . header_delimiter + __str__ ) . get bytes ( utf_8 ) ) ; stream . write ( __str__ . get,spawns have,success,pre
we are dragging directly over a card @$ make sure that we also catch the gesture even if nobody else <PLACE_HOLDER> the touch event .,if ( m callback . get child at position ( ev ) != null ) { on intercept touch event ( ev ) ; return true ; } else { cancel long press ( ) ; return false ; },nobody consumes,fail,pre
intercept the record @$ which can be potentially modified ; this method does not <PLACE_HOLDER> exceptions,producer record < k @$ v > intercepted record = this . interceptors . on send ( record ) ; return do send ( intercepted record @$ callback ) ;,method throw,success,pre
aaa entity only <PLACE_HOLDER> initialized when a persistent property is accessed,assert equals ( ( short ) __num__ @$ aaa entity . get field ina mapped superclass ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert true ( hibernate . is initialized ( aaa entity ) ) ; assert equals ( true @$ aaa entity . get field ina entity ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert equals ( __str__ @$ aaa entity . get field inaa entity ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert equals ( __num__ @$ aaa entity . get field inaaa entity ( ) ) ; assert equals,entity gets,success,pre
could n't find an external in latest that we think matches this one in my . so just <PLACE_HOLDER> this one and give it a conflict name if necessary .,if ( latest external location == null ) { try { external location result external location = add external ( my external location @$ monitor ) ; external location [ ] external locations = new external location [ ] { result external location @$ latest external location @$ my external location @$ null } ; adjustid maps for add ( external locations @$ result external location @$ my ) ; } catch ( duplicate name exception e ) { msg . error ( this @$ __str__ + my external location . get symbol ( ) . get name ( true ) + __str__ + e . get message ( ) ) ; } catch ( invalid input exception e ) { msg . error ( this @$ __str__,external resolve,fail,pre
copy <PLACE_HOLDER> explicit attributes to the beginning of the array .,system . arraycopy ( state @$ __num__ @$ new state @$ __num__ @$ n ) ; state = new state ;,copy moved,fail,pre
create three link tree objects . one will change the dependencies @$ and one just <PLACE_HOLDER> destination subdirs to make sure that 's taken into account,python symlink tree first sym link tree build rule = new python symlink tree ( __str__ @$ build target @$ project filesystem @$ output path @$ immutable map . of ( paths . get ( __str__ ) @$ path source path . of ( project filesystem @$ more paths . relativize ( tmp dir . get root ( ) @$ file1 ) ) ) @$ first merge directories @$ graph builder ) ; python symlink tree second sym link tree build rule = new python symlink tree ( __str__ @$ build target @$ project filesystem @$ output path @$ immutable map . of ( paths . get ( __str__ ) @$ path source path . of ( project filesystem @$ more paths . relativize ( tmp dir .,one changes,success,pre
the order of the next two fields is important @$ as a change listener of the transport combo <PLACE_HOLDER> the proxy port field to its default,connection panel . set selected transport ( preferred transport ) ; connection panel . set proxy port ( proxy port ) ; security panel . load account ( sip acc reg . get security registration ( ) ) ; presence panel . reinit ( ) ; presence panel . set presence enabled ( enable presence ) ; presence panel . set force peer to peer mode ( forcep2p ) ; presence panel . set poll period ( polling period ) ; presence panel . set subscription expiration ( subscription period ) ; if ( ! enable presence ) { presence panel . set presence options enabled ( enable presence ) ; } connection panel . set keep alive method ( keep alive method ) ; connection panel .,listener sets,success,pre
and that local files always <PLACE_HOLDER> preference over remote files .,action lookup key action lookup key = new action lookup key ( ) { @ override public sky function name function name ( ) { return sky function name . for_testing ; } } ; sky key action key1 = action lookup data . create ( action lookup key @$ __num__ ) ; sky key action key2 = action lookup data . create ( action lookup key @$ __num__ ) ; artifact out1 = create derived artifact ( __str__ ) ; artifact out2 = create derived artifact ( __str__ ) ; map < sky key @$ sky value > metadata to inject = new hash map < > ( ) ; metadata to inject . put ( action key1 @$ action value with remote artifact ( out1 @$,files take,fail,pre
check for overflow @$ if overflow is detected <PLACE_HOLDER> the current interval to the max interval .,if ( current interval millis >= max interval millis / multiplier ) { current interval millis = max interval millis ; } else { current interval millis *= multiplier ; },check reduce,fail,pre
if element <PLACE_HOLDER> children @$ then serialize them @$ otherwise serialize en empty tag .,if ( elem . has child nodes ( ) ) { state = enter element state ( null @$ null @$ tag name @$ preserve space ) ; child = elem . get first child ( ) ; while ( child != null ) { serialize node ( child ) ; child = child . get next sibling ( ) ; } end elementio ( tag name ) ; } else { if ( ! is document state ( ) ) { state . after element = true ; state . empty = false ; } },element has,success,pre
1 is excluded via property @$ 3 already <PLACE_HOLDER> stats @$ so we only expect two updates .,drain work queue ( su @$ nonstat_part_count - __num__ ) ; for ( int i = __num__ ; i < nonstat_part_count ; ++ i ) { verify stats up to date ( __str__ @$ __str__ + i @$ lists . new array list ( __str__ ) @$ ms client @$ i != excluded_part ) ; } verify stats up to date ( __str__ @$ __str__ + excluded_part @$ lists . new array list ( __str__ ) @$ ms client @$ false ) ; ms client . close ( ) ;,3 has,success,pre
no standard <PLACE_HOLDER> key listener as we want to catch ctrl globally no matter of focus,keyboard focus manager . get current keyboard focus manager ( ) . add key event post processor ( new key event post processor ( ) { @ override public boolean post process key event ( key event e ) { if ( e . getid ( ) == key event . key_pressed ) { la . key pressed ( e ) ; } if ( e . getid ( ) == key event . key_released ) { la . key released ( e ) ; } if ( e . getid ( ) == key event . key_typed ) { la . key typed ( e ) ; } return false ; } } ) ;,standard press,fail,pre
the host has <PLACE_HOLDER> masters @$ go ahead to move them,if ( m_cartographer . get master count ( ihid ) > __num__ ) { migrate partition leader message message = new migrate partition leader message ( ihid @$ integer . min_value ) ; for ( integer integer : live hids ) { m_mailbox . send ( core utils . geths id from host and site ( integer @$ host messenger . client_interface_site_id ) @$ message ) ; } message = new migrate partition leader message ( ihid @$ integer . min_value ) ; message . set start task ( ) ; message . set stop node service ( ) ; m_mailbox . send ( core utils . geths id from host and site ( ihid @$ host messenger . client_interface_site_id ) @$ message ) ; },host closed,fail,pre
chrome version does not necessarily <PLACE_HOLDER> the desired version because of auto updates ...,if ( browser util . is chrome ( get desired capabilities ( ) ) ) { browser identifier = get expected user agent string ( get desired capabilities ( ) ) + __str__ ; } else if ( browser util . is firefox ( get desired capabilities ( ) ) ) { browser identifier = get expected user agent string ( get desired capabilities ( ) ) + __str__ ; } else { browser identifier = get expected user agent string ( desired capabilities ) + desired capabilities . get version ( ) ; },version match,success,pre
we assume here that programs do n't <PLACE_HOLDER> the value of the keyword undefined to something other than the value undefined .,if ( __str__ . equals ( name ) || __str__ . equals ( name ) ) { return ternary value . false ; } else if ( __str__ . equals ( name ) ) { return ternary value . true ; } else { return ternary value . unknown ; },programs change,success,pre
indicate that the active request has <PLACE_HOLDER> the update,active request . update performed ( ) ; return generate ok response ( response entity ) . build ( ) ;,request performed,success,pre
if this project has <PLACE_HOLDER> reference @$ create value generator and produce the <PLACE_HOLDER> variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel @$ frame ) ; },project correlated,success,pre
if not websocket @$ then just <PLACE_HOLDER> the message,return msg ;,then return,success,pre
<PLACE_HOLDER> view start <PLACE_HOLDER> view end,skeleton models . add ( new skeleton model builder ( ) . set start view ( btn3 ) . set end view ( btn4 ) . build ( ) ) ;,start closed,fail,pre
now we look if user 2 <PLACE_HOLDER> an event,assert equals ( __str__ @$ __num__ @$ op set2 collector . collected events . size ( ) ) ;,user returned,fail,pre
short circuit the no time <PLACE_HOLDER> case .,if ( timeout < __num__ ) { try { processor . process ( now @$ region @$ mutations @$ wal edit ) ; } catch ( io exception e ) { string row = processor . get rows to lock ( ) . is empty ( ) ? __str__ : __str__ + bytes . to string binary ( processor . get rows to lock ( ) . iterator ( ) . next ( ) ) + __str__ ; log . warn ( __str__ + processor . get class ( ) . get name ( ) + __str__ + row @$ e ) ; throw e ; } return ; },circuit lock,fail,pre
web view inside browser does n't <PLACE_HOLDER> initial focus to be set .,settings . set need initial focus ( false ) ;,view require,fail,pre
the kerberos <PLACE_HOLDER> rpc .,token token = logged in user . do as ( new privileged exception action < token > ( ) { @ override public token run ( ) throws io exception { get delegation token request request = records . new record ( get delegation token request . class ) ; request . set renewer ( renewer string ) ; return hs service . get delegation token ( request ) . get delegation token ( ) ; } } ) ; return token ;,kerberos recurrency,fail,pre
if no service has the capability to introspect screen @$ we do not register callback in the window manager for window changes @$ so we have to ask the window manager what the focused window is to update the active one . the active window also <PLACE_HOLDER> events from which windows are delivered .,synchronized ( m lock ) { if ( m windows for accessibility callback == null ) { m focused window id = get focused window id ( ) ; if ( window id == m focused window id ) { m active window id = window id ; } } },window holds,fail,pre
all trackers should <PLACE_HOLDER> the node available with data one,assert not null ( local tracker . get data ( false ) ) ; assert not null ( local tracker . block until available ( ) ) ; assert true ( bytes . equals ( local tracker . get data ( false ) @$ data one ) ) ; assert not null ( second tracker . get data ( false ) ) ; assert not null ( second tracker . block until available ( ) ) ; assert true ( bytes . equals ( second tracker . get data ( false ) @$ data one ) ) ; assert true ( thread . has data ) ; assert true ( bytes . equals ( thread . tracker . get data ( false ) @$ data one ) ),trackers have,success,pre
basic key chain can <PLACE_HOLDER> output script types .,if ( ( result = basic . find key from pub hash ( pub key hash ) ) != null ) return result ; if ( chains != null ) { for ( deterministic key chain chain : chains ) { if ( script type != null && script type != chain . get output script type ( ) ) continue ; if ( ( result = chain . find key from pub hash ( pub key hash ) ) != null ) return result ; } },chain support,fail,pre
one <PLACE_HOLDER> request for session 1,request req1 = create write request ( __num__ @$ __num__ ) ; process request with wait ( req1 ) ;,one write,success,pre
check if ping command does not <PLACE_HOLDER> any errors,if ( process . exit value ( ) == __num__ ) { shell_result = parse ( process ) ; } else { shell_result = su_busyboox_ping ( params [ __num__ ] . address ) ; },command throw,fail,pre
3 sentries @$ each completion should <PLACE_HOLDER> the milestone,for ( int i = __num__ ; i < __num__ ; i ++ ) { case instance case instance = cmmn runtime service . create case instance builder ( ) . case definition key ( __str__ ) . start ( ) ; list < plan item instance > plan item instances = cmmn runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state ( plan item instance state . active ) . order by name ( ) . asc ( ) . list ( ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; cmmn runtime service . trigger plan item instance ( plan,sentries complete,fail,pre
need to convert value to number this happens because json <PLACE_HOLDER> 1 as an integer even if the field is supposed to be a long,if ( value instanceof number ) { number number value = ( number ) value ; class < ? > clazz = field . get type ( ) ; if ( clazz == integer . class || clazz == int . class ) { return number value . int value ( ) ; } else if ( clazz == long . class || clazz == long . class ) { return number value . long value ( ) ; } else if ( clazz == double . class || clazz == double . class ) { return number value . double value ( ) ; } else if ( clazz == float . class || clazz == float . class ) { return number value . float value,json returns,fail,pre
when we get this far @$ we 're exiting @$ so no need to <PLACE_HOLDER> the property .,system . set property ( __str__ @$ factory class ) ; transformer factory transformer factory = transformer factory . new instance ( ) ; transformer trans = transformer factory . new transformer ( ) ; trans . set output property ( output keys . omit_xml_declaration @$ __str__ ) ; trans . set output property ( output keys . indent @$ __str__ ) ; trans . set output property ( __str__ @$ __str__ ) ;,need save,fail,pre
we have a bucket so <PLACE_HOLDER> the frist left tuple and return,if ( list != null ) { tuple = list . get first ( ) ; return tuple ; },bucket assign,success,pre
get object can <PLACE_HOLDER> null if constraints were specified but not met,if ( s3 object == null ) return null ; output stream output stream = null ; try { output stream = new buffered output stream ( new file output stream ( destination file ) ) ; byte [ ] buffer = new byte [ __num__ * __num__ ] ; int bytes read ; while ( ( bytes read = s3 object . get object content ( ) . read ( buffer ) ) > - __num__ ) { output stream . write ( buffer @$ __num__ @$ bytes read ) ; } } catch ( io exception e ) { throw new sdk client exception ( __str__ + e . get message ( ) @$ e ) ; } finally { close quietly ( output stream @$,object return,success,pre
configure how content <PLACE_HOLDER> information will appear .,assistant . enable auto activation ( store . get boolean ( sql preference constants . enable_auto_activation ) ) ; assistant . set auto activation delay ( store . get int ( sql preference constants . auto_activation_delay ) ) ; assistant . set proposal popup orientation ( i content assistant . proposal_overlay ) ; assistant . set sorter ( new sql completion sorter ( ) ) ; assistant . set information control creator ( get information control creator ( source viewer ) ) ;,content dismiss,fail,pre
the caller <PLACE_HOLDER> strings @$ we have to use an intermediary .,return new id enumeration ( collections . enumeration ( availablei ds ) ) ;,caller wants,fail,pre
test server can <PLACE_HOLDER> data written by client,server . invoke ( client server host name verification distributed test :: do server region test ) ;,server see,success,pre
if not active or if the flow catalog is not up yet then ca n't <PLACE_HOLDER> config changes,if ( ! is active || ! this . flow catalog . is running ( ) ) { log . warn ( __str__ @$ this . is active ) ; return false ; } return true ;,then consider,fail,pre
a volt db extension to <PLACE_HOLDER> the third parameter optional,parse list alt = new short [ ] { tokens . openbracket @$ tokens . question @$ tokens . comma @$ tokens . question @$ tokens . x_option @$ __num__ @$ tokens . comma @$ tokens . question @$ tokens . closebracket } ;,extension make,success,pre
and adding an old one will not <PLACE_HOLDER> any error,cache . check and store ( now @$ a1 ) ;,one cause,fail,pre
the values in the linear axis will not <PLACE_HOLDER> values after the decimal point .,vertical axis . set label format ( __str__ ) ; linear axis horizontal axis = new linear axis ( ) ; horizontal axis . set label format ( __str__ ) ; scatter point series . set vertical axis ( vertical axis ) ; scatter point series . set horizontal axis ( horizontal axis ) ;,values have,success,pre
pipeline <PLACE_HOLDER> pipeline base,add pipeline base extends ( __str__ @$ pipeline transform . class ) ;,pipeline extends,success,pre
check if the array is really too big . this is an optimistic check because the heap probably <PLACE_HOLDER> other objects in it @$ so the next collection will throw an out of memory error if this object is allocated and survives .,if ( size . above or equal ( heap policy . get large array threshold ( ) ) ) { if ( size . above or equal ( heap policy . get maximum heap size ( ) ) ) { throw array allocation too large ; } unaligned heap chunk . unaligned header u chunk = heap chunk provider . get ( ) . produce unaligned chunk ( size ) ; result = allocate large array ( hub @$ length @$ size @$ u chunk @$ tlab @$ remembered set ) ; } else { aligned header new chunk = prepare new allocation chunk ( tlab ) ; result = allocate small array ( hub @$ length @$ size @$ tlab @$ remembered set @$ new chunk ),heap has,success,pre
to avoid duplication only first subtask keeps track of next transactional id hint . otherwise all of the subtasks would <PLACE_HOLDER> exactly same information .,if ( get runtime context ( ) . get index of this subtask ( ) == __num__ && semantic == semantic . exactly_once ) { check state ( next transactional id hint != null @$ __str__ ) ; long next free transactional id = next transactional id hint . next free transactional id ; if ( get runtime context ( ) . get number of parallel subtasks ( ) > next transactional id hint . last parallelism ) { next free transactional id += get runtime context ( ) . get number of parallel subtasks ( ) * kafka producers pool size ; } next transactional id hint state . add ( new next transactional id hint ( get runtime context ( ) . get number of parallel,track write,success,pre
check that net flow into a vertex <PLACE_HOLDER> zero @$ except at source and sink,if ( math . abs ( value + excess ( g @$ s ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ s ) ) ; system . err . println ( __str__ + value ) ; return false ; } if ( math . abs ( value - excess ( g @$ t ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ t ) ) ; system . err . println ( __str__ + value ) ; return false ; } for ( int v = __num__ ; v < g . v ( ) ; v ++ ) { if ( v == s || v == t,flow replaces,fail,pre
remove all elements . should <PLACE_HOLDER> array attribute to null,partitions . clear ( ) ; table entity . set attribute ( __str__ @$ partitions ) ; init ( ) ; response = entity store . create or update ( new atlas entity stream ( entities info ) @$ false ) ; updated table = response . get first updated entity by type name ( table_type ) ; validate entity ( entities info @$ get entity from store ( updated table ) ) ;,elements set,success,pre
okay @$ it 's showtime . let 's <PLACE_HOLDER> the role through live ddl,try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; } try { admin client . update application catalog ( null @$ new file ( path to deployment ) ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; },'s set,fail,pre
test no balance to <PLACE_HOLDER> coin . query balance before <PLACE_HOLDER> coin .,long deplay account before balance = public methed . query account ( delay account3 address @$ blocking stub full ) . get balance ( ) ; long recevier account before balance = public methed . query account ( receiver account4 address @$ blocking stub full ) . get balance ( ) ; logger . info ( __str__ + deplay account before balance ) ; logger . info ( __str__ + recevier account before balance ) ; assert . assert false ( public methed . sendcoin delayed ( receiver account4 address @$ send coin amount @$ delay second @$ delay account3 address @$ delay account3 key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ;,balance send,success,pre
they may have changed the document and the variables via instrumentation so <PLACE_HOLDER> the reference to it,execution input = execution input . transform ( builder -> builder . variables ( parse result . get variables ( ) ) ) ; execution input ref . set ( execution input ) ; log not safe . debug ( __str__ @$ query ) ; final list < validation error > errors = validate ( execution input @$ document @$ graphql schema @$ instrumentation state ) ; if ( ! errors . is empty ( ) ) { log not safe . warn ( __str__ @$ query ) ; return new preparsed document entry ( errors ) ; } return new preparsed document entry ( document ) ;,document remember,fail,pre
check request size will <PLACE_HOLDER> io exception if request is rejected,if ( is large request ( length ) ) { check request size when message received ( length ) ; si . set large request size ( length ) ; },size throw,success,pre
at this point @$ suggestion key <PLACE_HOLDER> the new selection key,if ( update prompt and selection if match found ) { if ( ! suggestion key . equals ( selected option key ) || suggestion . get replacement string ( ) . equals ( tb . get text ( ) ) || force update text ) { set text ( suggestion . get replacement string ( ) ) ; selected option key = suggestion key ; } },key equals,fail,pre
script output <PLACE_HOLDER> ambiguous node attributes,string script content = __str__ + __str__ + __str__ ; write node attribute script file ( script content @$ true ) ; node attributes provider . init ( get conf for node attribute script ( ) ) ; node attributes provider . start ( ) ;,output contains,success,pre
this case <PLACE_HOLDER> only the polymer calls that are inside a function which is an arg to goog.load module,check state ( is function arg in goog load module ( enclosing node ) ) ; node enclosing script = node util . get enclosing script ( enclosing node ) ; node insertion point = get node for insertion ( enclosing script ) ; insertion point . add child to front ( declaration code ) ; compiler . report change to change scope ( insertion point ) ;,case represents,success,pre
if timestamps are equal then <PLACE_HOLDER> images did not correctly <PLACE_HOLDER> up the image info and image proxy,preconditions . check argument ( ! min image info timestamp . equals ( min image proxy timestamp ) ) ; if ( min image info timestamp > min image proxy timestamp ) { for ( int i = m pending images . size ( ) - __num__ ; i >= __num__ ; i -- ) { if ( m pending images . key at ( i ) < min image info timestamp ) { image proxy image proxy = m pending images . value at ( i ) ; image proxy . close ( ) ; m pending images . remove at ( i ) ; } } } else { for ( int i = m pending image infos . size ( ) - __num__ ; i,images pick,fail,pre
clear annotation database finder <PLACE_HOLDER> hierarchy filter,guice . set annotation database package names ( null ) ; guice . set hierarchy traversal filter factory ( new hierarchy traversal filter factory ( ) ) ;,finder object,fail,pre
start the prepared thread so that it is writing znodes while the follower is restarting . on the first restart @$ the follow should <PLACE_HOLDER> txnlog to catchup . for subsequent restart @$ the follower should <PLACE_HOLDER> a diff to catchup .,if ( i == __num__ ) { mytestfoo thread . start ( ) ; log . info ( __str__ @$ index ) ; qu . restart ( index ) ; thread . sleep ( __num__ ) ; log . info ( __str__ @$ index ) ; qu . shutdown ( index ) ; thread . sleep ( __num__ ) ; log . info ( __str__ @$ index ) ; qu . restart ( index ) ; log . info ( __str__ @$ index ) ; },follower use,success,pre
validate <PLACE_HOLDER> itself .,file report file = new file ( location ) ; assert . assert true ( __str__ + location @$ report file . exists ( ) ) ; validate jdr report contents ( report file ) ;,validate report,success,pre
do n't publish directly @$ ensure that the user explicitly <PLACE_HOLDER> and approves content first .,idempotent executor . execute and swallowio exceptions ( title @$ title @$ ( ) -> get or create blogger service ( auth data ) . posts ( ) . insert ( blog id @$ post ) . set is draft ( true ) . execute ( ) . get id ( ) ) ;,user guards,fail,pre
load balancer <PLACE_HOLDER> all sorts of callbacks,transport info1 . listener . transport ready ( ) ; verify ( state listener1 @$ times ( __num__ ) ) . on subchannel state ( state info captor . capture ( ) ) ; assert same ( connecting @$ state info captor . get all values ( ) . get ( __num__ ) . get state ( ) ) ; assert same ( ready @$ state info captor . get all values ( ) . get ( __num__ ) . get state ( ) ) ; verify ( state listener2 ) . on subchannel state ( state info captor . capture ( ) ) ; assert same ( connecting @$ state info captor . get value ( ) . get state ( ) ) ; resolver . listener,balancer triggers,fail,pre
namespace declarations parameter <PLACE_HOLDER> no effect if namespaces is false .,if ( ! f namespace declarations && f namespace aware ) { int len = attributes . get length ( ) ; for ( int i = len - __num__ ; i >= __num__ ; -- i ) { if ( xml symbols . prefix_xmlns == attributes . get prefix ( i ) || xml symbols . prefix_xmlns == attributes . getq name ( i ) ) { attributes . remove attribute at ( i ) ; } } } super . start element ( element @$ attributes @$ augs ) ;,declarations has,success,pre
the candidate name did n't <PLACE_HOLDER> any items so the name is unique,if ( null == item ) { return true ; } else if ( item . get name ( ) . equals ( current job name ) ) { return true ; } else { return false ; },name match,fail,pre
the super implementation does not <PLACE_HOLDER> the following parameters,identification protocol = params . get endpoint identification algorithm ( ) ; algorithm constraints = params . get algorithm constraints ( ) ; prefer local cipher suites = params . get use cipher suites order ( ) ; list < sni server name > sni names = params . get server names ( ) ; if ( sni names != null ) { no sni extension = sni names . is empty ( ) ; server names = sni names ; } collection < sni matcher > matchers = params . getsni matchers ( ) ; if ( matchers != null ) { no sni matcher = matchers . is empty ( ) ; sni matchers = matchers ; } if ( ( handshaker != null ) &&,implementation handle,success,pre
the key'algo ' <PLACE_HOLDER> the shortcut @$ 'algorithm ' is the long version,final string algo full name = readkv ( __str__ ) ;,key'algo indicates,fail,pre
we <PLACE_HOLDER> n't validate the string but let the parser <PLACE_HOLDER> the work for us it throws a validation exception,validate optional ( key @$ is optional @$ v -> { logical type t = logical type parser . parse ( v ) ; if ( t . get type root ( ) == logical type root . unresolved ) { throw new validation exception ( __str__ + v + __str__ ) ; } } ) ;,parser do,success,pre
set background @$ if your root layout does n't <PLACE_HOLDER> one,final drawable window background = get window ( ) . get decor view ( ) . get background ( ) ; top blur view . setup with ( root ) . set frame clear drawable ( window background ) . set blur algorithm ( new support render script blur ( this ) ) . set blur radius ( radius ) . set has fixed transformation matrix ( true ) ; bottom blur view . setup with ( root ) . set frame clear drawable ( window background ) . set blur algorithm ( new support render script blur ( this ) ) . set blur radius ( radius ) . set has fixed transformation matrix ( true ) ; int initial progress = ( int ) ( radius,layout have,success,pre
component <PLACE_HOLDER> a baseline,if ( constraints . ascent >= __num__ ) { int baseline = constraints . ascent ; constraints . descent = h - constraints . ascent + constraints . insets . bottom ; constraints . ascent += constraints . insets . top ; constraints . baseline resize behavior = c . get baseline resize behavior ( ) ; constraints . center padding = __num__ ; if ( constraints . baseline resize behavior == component . baseline resize behavior . center_offset ) { int next baseline = c . get baseline ( w @$ h + __num__ ) ; constraints . center offset = baseline - h / __num__ ; if ( h % __num__ == __num__ ) { if ( baseline != next baseline ) { constraints . center,component has,success,pre
raptor connector currently does not <PLACE_HOLDER> comment on table,assert query fails ( __str__ @$ __str__ ) ;,connector support,success,pre
why do we have a separate field for the singular form ? <PLACE_HOLDER> keys and <PLACE_HOLDER> key .,if ( finder schema . has assoc key ( ) ) { string part key = finder schema . get assoc key ( ) ; compound key key = ( compound key ) generate key ( ) ; finder . assoc key ( part key @$ key . get part ( part key ) ) ; },key join,fail,pre
not a list @$ but not a primtive @$ <PLACE_HOLDER> the nested group type,return g . get group ( field index @$ __num__ ) ;,list try,fail,pre
version 2 layout <PLACE_HOLDER> outer circle bigger than inner,if ( controller . get version ( ) == time picker dialog . version . version_1 ) { m text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_outer ) ) ; m inner text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_inner ) ) ; } else { m text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_outer_v2 ) ) ; m inner text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_inner_v2 ) ) ; } m inner text grid heights = new float [ __num__ ] ;,version makes,fail,pre
size of metadata has <PLACE_HOLDER> @$ the most complex situation @$ more atoms affected,int additional space required for metadata = new ilst size - old ilst size ;,size reverted,fail,pre
o auth <PLACE_HOLDER> some characters differently :,return url decoder . decode ( url @$ __str__ ) . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) ;,auth encodes,success,pre
the service that detects <PLACE_HOLDER> standby master nodes,get executor service ( ) . submit ( new heartbeat thread ( heartbeat context . master_lost_master_detection @$ new lost master detection heartbeat executor ( ) @$ ( int ) server configuration . get ms ( property key . master_standby_heartbeat_interval ) @$ server configuration . global ( ) @$ m master context . get user state ( ) ) ) ; get executor service ( ) . submit ( new heartbeat thread ( heartbeat context . master_log_config_report_scheduling @$ new log config report heartbeat executor ( ) @$ ( int ) server configuration . get ms ( property key . master_log_config_report_heartbeat_interval ) @$ server configuration . global ( ) @$ m master context . get user state ( ) ) ) ; if ( server configuration . get boolean (,detects lost,success,pre
management server <PLACE_HOLDER> back another rds response containing the route configuration for the requested resource .,route configs = immutable list . of ( any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) @$ build virtual host ( immutable list . of ( __str__ @$ __str__ ) @$ __str__ ) ) ) ) ) ; response = build discovery response ( __str__ @$ route configs @$ xds client impl . ads_type_url_rds @$ __str__ ) ; response observer . on next ( response ) ;,server sends,success,pre
this is dangerous : new instance can <PLACE_HOLDER> checked exceptions . this is dangerous : config is mutable .,if ( ! runnable before . is interface ( ) ) { runnable before . new instance ( ) . run ( get configuration ( ) ) ; },instance throw,success,pre
invalidate throws entry not found exception then <PLACE_HOLDER> region,try ( ignored exception ignored1 = add ignored exception ( entry not found exception . class . get name ( ) ) ; ignored exception ignored2 = add ignored exception ( reply exception . class . get name ( ) ) ) { for ( int i = invalidate range_2 start ; i <= invalidate range_2 end ; i ++ ) { string key = integer . to string ( i ) ; string value = integer . to string ( i ) ; assert that thrown by ( ( ) -> region . invalidate ( key ) ) . is instance of ( entry not found exception . class ) ; region . create ( key @$ value ) ; } },exception terminate,fail,pre
properties javadoc <PLACE_HOLDER> use of put so we use set property,if ( name instanceof string && value instanceof string ) { props . set property ( ( string ) name @$ ( string ) value ) ; },javadoc makes,fail,pre
always use the smallest radius to make sure the rounded corners will completely <PLACE_HOLDER> the display .,return math . min ( top radius @$ bottom radius ) ;,corners fill,fail,pre
the two methods follow different code paths to determine what the <PLACE_HOLDER> target for the result should be and we want to test both of them .,try ( fixture fixture = create synchronous execution fixture ( __str__ ) ) { cell cell = fixture . get cell ( ) ; path root build file path = cell . get filesystem ( ) . resolve ( __str__ ) ; path a build file path = cell . get filesystem ( ) . resolve ( __str__ ) ; fixture . get target node parse pipeline ( ) . get all requested target nodes ( cell @$ root build file path @$ optional . empty ( ) ) ; optional < build file manifest > root raw nodes = fixture . get raw node parse pipeline cache ( ) . lookup computed node ( cell @$ root build file path @$ event bus ) ; fixture .,the build,success,pre
unlike list popup window @$ popup menu does n't have an api to check whether it is showing . <PLACE_HOLDER> a custom matcher to check the visibility of the drop down list view instead .,on view ( with class name ( matchers . is ( drop_down_class_name ) ) ) . in root ( is platform popup ( ) ) . check ( matches ( is displayed ( ) ) ) ;,api use,success,pre
expected exception as the function is <PLACE_HOLDER> down the target members and the result collector will get member departed exception,return __str__ ;,function showing,fail,pre
transition to fatal error since we <PLACE_HOLDER> unresolved batches .,sender . run once ( ) ;,transition have,success,pre
asserts <PLACE_HOLDER> equality on the <PLACE_HOLDER> graph .,assert true ( copya . get object ( ) == copyc . get object ( ) ) ; assert true ( copya . get objects ( ) . get ( __num__ ) == copyc . get objects ( ) . get ( __num__ ) ) ; assert true ( copya == copyc . get objects ( ) . get ( __num__ ) ) ; assert true ( copyc == copya . get objects ( ) . get ( __num__ ) ) ;,asserts object,success,pre
we do a remote call to have an io exception if the connection is broken . <PLACE_HOLDER> the bug 4939578,return connection . get connection id ( ) ;,4939578 see,success,pre
the list of relational values should contain 2 or more values : the first <PLACE_HOLDER> the discriminator the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ jaxb any mapping . get name ( ) ) @$ origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source type source = new hibernate type source impl ( jaxb any mapping . get meta type ( ) ) ; private final relational value source relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > value mappings = new hash map < string @$ string > ( ) ; { for ( jaxb hbm any value,first represents,success,pre
creation of an isa will <PLACE_HOLDER> a resolve .,inet socket address initial isa = new inet socket address ( hostname @$ port ) ; if ( initial isa . get address ( ) == null ) { throw new illegal argument exception ( __str__ + initial isa ) ; } final list < blocking service and interface > sai = new array list < > ( __num__ ) ;,creation force,success,pre
the keys of probe and build sides are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer <PLACE_HOLDER> works well in this case .,final int probe_vals_per_key = __num__ ;,side join,success,pre
set capacity will <PLACE_HOLDER> length of underlying byte array,buf . set capacity ( start size ) ; assert equals ( start size @$ buf . capacity ( ) ) ; check buffer ( buf @$ empty ) ;,capacity change,success,pre
find out @$ which <PLACE_HOLDER> number the connection to the parent,int num ; for ( num = __num__ ; num < this . outgoing connections . size ( ) ; num ++ ) { if ( this . outgoing connections . get ( num ) == to parent ) { break ; } } if ( num >= this . outgoing connections . size ( ) ) { throw new compiler exception ( __str__ + __str__ ) ; },which provided,fail,pre
the spy <PLACE_HOLDER> an io exception when writing to the second directory,do answer ( new faulty save image ( false ) ) . when ( spy image ) . savefs image ( any ( ) @$ any ( ) @$ any ( ) ) ; should fail = false ; break ; case save_all_fsimages :,spy throws,success,pre
fetch the cr ls via ldap . ldap cert store <PLACE_HOLDER> its own caching mechanism @$ see the class description for more info . safe cast since xsel is an x 509 certificate selector .,try { return ( collection < x509crl > ) ldap cert store . getcr ls ( xsel ) ; } catch ( cert store exception cse ) { throw new pkix . cert store type exception ( __str__ @$ cse ) ; },store has,success,pre
buck modules do not <PLACE_HOLDER> versions,return ( __ @$ ___ ) -> true ;,modules support,success,pre
server <PLACE_HOLDER> settings,assert equals ( ( long ) expected settings . get max header list size ( ) @$ new settings . max header list size ( ) ) ; assert equals ( ( integer ) expected settings . get max frame size ( ) @$ new settings . max frame size ( ) ) ; assert equals ( ( integer ) expected settings . get initial window size ( ) @$ new settings . initial window size ( ) ) ; assert equals ( ( long ) expected settings . get max concurrent streams ( ) @$ new settings . max concurrent streams ( ) ) ; assert equals ( null @$ new settings . header table size ( ) ) ; complete ( ) ; break ; default,server received,fail,pre
in the second rebalance the new member gets its assignment and this member <PLACE_HOLDER> no assignments or revocations,expect rebalance ( __num__ @$ collections . empty list ( ) @$ collections . empty list ( ) ) ; member . poll ( easy mock . any int ( ) ) ; power mock . expect last call ( ) ; power mock . replay all ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ double . positive_infinity ) ; herder . tick ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; herder . tick ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; power mock . verify all ( ),assignment has,success,pre
we use a runnable to make sure this work . because if the list view is <PLACE_HOLDER> data @$ this might not work .,m view files . post ( new runnable ( ) { @ override public void run ( ) { if ( should be selected idx >= __num__ && should be selected idx < m file adapter . get count ( ) ) { m view files . set selection ( should be selected idx ) ; } else if ( ! m file adapter . is empty ( ) ) m view files . set selection ( __num__ ) ; } } ) ;,view showing,fail,pre
the first two tasks <PLACE_HOLDER> the same priority,final task task = noop task . create ( math . min ( __num__ @$ ( i - __num__ ) * __num__ ) ) ;,tasks have,success,pre
we should show the promo view only when the panel has <PLACE_HOLDER> the exact expanded height .,if ( percentage == __num__ ) { show promo view ( ) ; } else { hide promo view ( ) ; },panel reached,success,pre
no fallback should <PLACE_HOLDER> more than 2 steps .,if ( new style2 != null ) { if ( fallback cache [ new style2 . ordinal ( ) ] != null ) { throw new illegal state exception ( __str__ ) ; } },fallback try,fail,pre
strip <PLACE_HOLDER> token from result .,result . remove ( account manager . key_authtoken ) ; if ( log . is loggable ( tag @$ log . verbose ) ) { log . v ( tag @$ get class ( ) . get simple name ( ) + __str__ + response ) ; },strip auth,success,pre
only the first request ever generated should <PLACE_HOLDER> an install event .,if ( succeeded && sending install request ) { m send install event = false ; register new request ( current timestamp ) ; generate and post request ( current timestamp @$ sessionid ) ; },request send,fail,pre
lazily <PLACE_HOLDER> snapshots of the notification .,status bar notification sbn = r . sbn ; status bar notification old sbn = ( old != null ) ? old . sbn : null ; trim cache trim cache = new trim cache ( sbn ) ; for ( final managed service info info : get services ( ) ) { boolean sbn visible = is visible to listener ( sbn @$ info ) ; boolean old sbn visible = old sbn != null ? is visible to listener ( old sbn @$ info ) : false ; if ( ! old sbn visible && ! sbn visible ) { continue ; } if ( r . is hidden ( ) && info . target sdk version < build . version_codes . p ) { continue,lazily keep,fail,pre
configure the atomic rename directories key so every folder will <PLACE_HOLDER> atomic rename applied .,conf . set ( azure native file system store . key_atomic_rename_directories @$ __str__ ) ; azure blob storage test account test account = azure blob storage test account . create ( conf ) ; assume not null ( test account ) ; test statistics with account ( test account ) ;,folder have,success,pre
otherwise we can get into an infinite loop because the object type hash code method <PLACE_HOLDER> this one .,return objects . hash code ( properties . key set ( ) ) ;,method calls,success,pre
filter projects only to groups which <PLACE_HOLDER> project 's name,set < group > copy = group . matching ( project @$ groups ) ;,which match,success,pre
check mapping a different element <PLACE_HOLDER> the expected result .,bounded window input window2 = new interval window ( instant . now ( ) @$ duration . standard minutes ( __num__ ) ) ; assert equals ( global window . instance @$ window mapping fn . get side input window ( input window2 ) ) ; assert equals ( input window2 @$ ( ( kv ) test sdk harness . get input values ( ) . get ( __num__ ) . get value ( ) ) . get value ( ) ) ;,check gives,fail,pre
update button visibility only triggers when a scroll is completed . so a user might <PLACE_HOLDER> the button when the animation is still ongoing potentially pushing the target position outside of the bounds of the day picker view,if ( position >= __num__ && position < day picker view . get count ( ) ) { day picker view . smooth scroll to position ( position ) ; update button visibility ( position ) ; },user remove,fail,pre
since value may <PLACE_HOLDER> integer limits @$ use stock parser which checks for this .,if ( length >= __num__ ) { value = integer . parse int ( text . sub sequence ( position @$ position += length ) . to string ( ) ) ; } else { int i = position ; if ( negative ) { i ++ ; } try { value = text . char at ( i ++ ) - __str__ ; } catch ( string index out of bounds exception e ) { return ~ position ; } position += length ; while ( i < position ) { value = ( ( value << __num__ ) + ( value << __num__ ) ) + text . char at ( i ++ ) - __str__ ; } if ( negative ) { value = -,value exceed,success,pre
if system time has <PLACE_HOLDER> backwards increase original by 1 ms to maintain uniqueness,last time = ( now < last time ) ? last time + __num__ : now ; last count = short . min_value ; done = true ;,time goed,fail,pre
query the data <PLACE_HOLDER> the service,query data ( ) ;,data find,fail,pre
test show <PLACE_HOLDER> view,string expected sql = format sql text ( format ( __str__ @$ get session ( ) . get catalog ( ) . get ( ) @$ get session ( ) . get schema ( ) . get ( ) @$ __str__ @$ query ) ) . trim ( ) ; actual = compute actual ( __str__ ) ; assert equals ( get only element ( actual . get only column as set ( ) ) @$ expected sql ) ; actual = compute actual ( format ( __str__ @$ get session ( ) . get catalog ( ) . get ( ) @$ get session ( ) . get schema ( ) . get ( ) ) ) ; assert equals ( get only element ( actual .,show create,success,pre
pattern order is used downstream so that we can know what order the text is in instead of encoding it in the string @$ which would <PLACE_HOLDER> more processing later to remove it pre feature selection .,map < string @$ integer > pattern order = new hash map < > ( ) ; int order = __num__ ; if ( has option ( from_option [ __num__ ] ) ) { patterns . add ( mail processor . from_prefix ) ; pattern order . put ( mail options . from @$ order ++ ) ; } if ( has option ( to_option [ __num__ ] ) ) { patterns . add ( mail processor . to_prefix ) ; pattern order . put ( mail options . to @$ order ++ ) ; } if ( has option ( references_option [ __num__ ] ) ) { patterns . add ( mail processor . refs_prefix ) ; pattern order . put ( mail options . refs @$,which require,success,pre
total jobs number is <PLACE_HOLDER> num 3,assert equals ( __str__ @$ threads num * jobs per task @$ stolen . get ( ) + none stolen . get ( ) ) ; assert false ( __str__ @$ stolen . get ( ) == __num__ ) ; for ( ignite g : g . all grids ( ) ) assert true ( __str__ @$ nodes . contains ( g . name ( ) ) ) ; assert true ( __str__ + stolen + __str__ + none stolen + __str__ @$ math . abs ( stolen . get ( ) - __num__ * none stolen . get ( ) ) <= __num__ ) ;,jobs threads,success,pre
request a list of upload ur ls for the <PLACE_HOLDER> files,if ( ! file params . is empty ( ) ) { get rpc proxy ( file drop target rpc . class ) . drop ( file params ) ; } event . prevent default ( ) ; event . stop propagation ( ) ;,the dropped,success,pre
wrong ur <PLACE_HOLDER> missing slashes @$ however accepted by digest url and multi protocol url constructors,test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; digesturl java url = new digesturl ( java url str ) ; string java hash result = ascii . string ( java url . hash ( ) ) ;,ur contains,fail,pre
in theory @$ the subclasses of stream source may <PLACE_HOLDER> the bounded one input interface @$ so we still need the following call to end the input,if ( ! is canceled or stopped ( ) ) { synchronized ( locking object ) { operator chain . end head operator input ( __num__ ) ; } },subclasses implement,success,pre
log any changes to what is currently <PLACE_HOLDER> the brightness setting .,if ( ! m brightness reason temp . equals ( m brightness reason ) || brightness adjustment flags != __num__ ) { slog . v ( tag @$ __str__ + brightness + __str__ + m brightness reason temp . to string ( brightness adjustment flags ) + __str__ + m brightness reason + __str__ ) ; m brightness reason . set ( m brightness reason temp ) ; },what showing,fail,pre
presto only uses get reflection object inspector here @$ in a test method . therefore @$ we choose to work around this issue by synchronizing this method . before synchronizing this method @$ test in this class <PLACE_HOLDER> approximately 1 out of 10 runs on travis .,return get reflection object inspector ( type @$ object inspector options . java ) ;,test takes,fail,pre
if the thread context classloader is set @$ then <PLACE_HOLDER> its hierarchy to find a matching context,class loader tccl = thread . current thread ( ) . get context class loader ( ) ; loader = tccl ; if ( loader != null ) { if ( log . is debug enabled ( ) ) log . debug ( __str__ ) ; synchronized ( __context map ) { while ( ctx == null && loader != null ) { ctx = get context for class loader ( loader ) ; if ( ctx == null && loader != null ) loader = loader . get parent ( ) ; } if ( ctx == null ) { ctx = new naming context ( obj @$ tccl @$ env @$ name @$ name ctx ) ; __context map . put ( tccl @$ ctx ),then walk,fail,pre
each <PLACE_HOLDER> the version increments as we walk the list @$ that counts as a successful operation,long version = - __num__ ; for ( int i = __num__ ; i < number of operations ; i ++ ) { if ( operations . get ( i ) . version ( ) >= version ) { count ++ ; version = operations . get ( i ) . version ( ) ; } },each get,fail,pre
each transform also <PLACE_HOLDER> its own group,list < string > expected groups = arrays . as list ( connector config . common_group @$ connector config . transforms_group @$ connector config . error_group @$ __str__ @$ __str__ ) ; assert equals ( expected groups @$ result . groups ( ) ) ; assert equals ( __num__ @$ result . error count ( ) ) ;,transform gets,success,pre
create some <PLACE_HOLDER> entries so the gc service is populated,serializable callable create = new serializable callable ( __str__ ) { @ override public object call ( ) { region factory f = get cache ( ) . create region factory ( get region attributes ( ) ) ; cc region = ( local region ) f . create ( name ) ; return cc region . get distribution manager ( ) . get distribution manager id ( ) ; } } ;,some destroyed,success,pre
now based on that floating point percentage <PLACE_HOLDER> the real scroller value .,final int visible amt = f scroll bar . get visible amount ( ) ; final int max = f scroll bar . get maximum ( ) ; final int min = f scroll bar . get minimum ( ) ; final int extent = max - min ;,now calculate,fail,pre
check if the line <PLACE_HOLDER> at least one character or digit,for ( string line : lines ) { if ( character_digits_pattern . matcher ( line ) . matches ( ) ) { stderr . append ( line ) ; } },line matches,fail,pre
dep 1 should <PLACE_HOLDER> only dep 1 abi dependency,dex produced from java library java dep1 dex rule = ( dex produced from java library ) graph builder . get rule ( build target factory . new instance ( __str__ ) ) ; assert not null ( java dep1 dex rule ) ; assert that ( java dep1 dex rule . get desugar deps ( ) @$ has size ( __num__ ) ) ; assert that ( java dep1 dex rule . get desugar deps ( ) @$ has item ( java dep2 abi . get source path to output ( ) ) ) ; assert that ( java dep1 dex rule . get build deps ( ) @$ all of ( has item ( java dep2 abi ) @$ not ( has item ( java dep1,dep have,success,pre
make the client <PLACE_HOLDER> client cache factory so it will have a default pool,this . durable clientvm . invoke ( ( ) -> cache server test util . create client cache ( get client pool ( get server host name ( ) @$ server1 port @$ true ) @$ region name @$ get client distributed system properties ( durable client id ) ) ) ;,client create,fail,pre
error while invoking <PLACE_HOLDER> constructor @$ use empty constructor,result = ( i persistable ) the class . new instance ( ) ; result . deserialize ( in ) ;,invoking get,fail,pre
finishing task 2 should <PLACE_HOLDER> the stage,cmmn runtime service . trigger plan item instance ( plan item instances . get ( __num__ ) . get id ( ) ) ; plan item instances = cmmn runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state ( plan item instance state . active ) . order by name ( ) . asc ( ) . list ( ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; expected names = new string [ ] { __str__ } ; for ( int i = __num__ ; i < plan item instances . size ( ) ; i ++ ) { assert equals,task complete,success,pre
validate inode tree entries <PLACE_HOLDER> given entries .,for ( mutable inode < ? > node : journaled ) { assert true ( tree entries . contains ( node . to journal entry ( ) ) ) ; },entries contains,fail,pre
fastpath : do not <PLACE_HOLDER> a new string if the src is a string and is already normalized .,if ( src instanceof string ) { int span length = span quick check yes ( src ) ; if ( span length == src . length ( ) ) { return ( string ) src ; } string builder sb = new string builder ( src . length ( ) ) . append ( src @$ __num__ @$ span length ) ; return normalize second and append ( sb @$ src . sub sequence ( span length @$ src . length ( ) ) ) . to string ( ) ; },fastpath create,fail,pre
second combine should <PLACE_HOLDER> contents ; should read existing data @$ and write final data to disk .,current time += second_in_millis ; reader . reset ( ) ; rotate . combine active ( reader @$ writer ( __str__ ) @$ current time ) ; reader . assert read ( __str__ ) ; assert read all ( rotate @$ __str__ ) ;,combine reuse,fail,pre
do bind <PLACE_HOLDER> all four flavors of binding,naming context data store impl = ( naming context data store ) this ; do bind ( impl @$ n @$ nc @$ true @$ binding type . ncontext ) ;,bind implements,success,pre
one output reader closes @$ the other still <PLACE_HOLDER> the output,output consumer binding . dispose ( ) ; consumed out . reset ( ) ; consumed out2 . reset ( ) ; context . eval ( source ) ; engine output = engine output + full output ; assert equals ( engine output @$ to unix string ( engine out ) ) ; assert equals ( __num__ @$ consumed out . size ( ) ) ; assert true ( consumed out2 . size ( ) > __num__ ) ; from out reader2 = new buffered reader ( new input stream reader ( new byte array input stream ( consumed out2 . to byte array ( ) ) ) ) ; assert equals ( full lines @$ read lines list ( from out reader2 ) ) ;,other consumes,fail,pre
if right becomes root parent means rotation happened at lower level . so just return right so that nodes at upper level can <PLACE_HOLDER> their child correctly,if ( right == root . parent ) { return right ; },nodes set,success,pre
if child is not <PLACE_HOLDER> yet call measure,if ( child != null && child . get visibility ( ) != view . gone ) { if ( child . get measured height ( ) == __num__ || child . get measured width ( ) == __num__ ) child . measure ( measure spec . make measure spec ( width @$ measure spec . at_most ) @$ measure spec . make measure spec ( height @$ measure spec . at_most ) ) ; layout params lp = ( layout params ) child . get layout params ( ) ; final int child width = child . get measured width ( ) ; final int child height = child . get measured height ( ) ; if ( child top + child height + lp . top,child managing,fail,pre
for now the call bellow will <PLACE_HOLDER> type unknown,type = compiler . resolve type ref ( model @$ bkm @$ bkm @$ null ) ;,bellow return,success,pre
table row <PLACE_HOLDER> default @$ match anything character class .,if ( table el . f char class == __num__ ) { break ; },row has,fail,pre
very simple format that shows <PLACE_HOLDER> resolution,try { return long . to string ( d . get time ( ) ) ; } catch ( exception ignore ) { return __str__ ; },shows timezone,fail,pre
ensure the work handler has <PLACE_HOLDER> the scores inside the cache listener of wifi tracker,wait for handlers to process currently enqueued messages ( tracker ) ;,handler loaded,fail,pre
0 x 100231 d : p 1 and p 2 <PLACE_HOLDER> same note bookmarks .,program builder1 . create bookmark ( __str__ @$ bookmark type . note @$ __str__ @$ __str__ ) ; program builder2 . create bookmark ( __str__ @$ bookmark type . note @$ __str__ @$ __str__ ) ;,1 have,success,pre
by default @$ parse <PLACE_HOLDER> the schema @$ which is what we want .,schema schema = new schema . parser ( ) . parse ( metadata . get schema string ( ) ) ; assert equals ( __num__ @$ schema . get fields ( ) . size ( ) ) ;,parse extract,fail,pre
camera device should <PLACE_HOLDER> configure outputs and have it finish before constructing us,if ( configure success ) { m state callback . on configured ( this ) ; if ( debug ) log . v ( tag @$ m id string + __str__ ) ; m configure success = true ; } else { m state callback . on configure failed ( this ) ; m closed = true ; log . e ( tag @$ m id string + __str__ ) ; m configure success = false ; },device write,fail,pre
only the stream which was just added will <PLACE_HOLDER> parents . so we only need an array of size 1 .,list < parent changed event > events = new array list < parent changed event > ( __num__ ) ; connection state . take child ( new parent @$ false @$ events ) ; notify parent changed ( events ) ;,stream have,fail,pre
does the list already <PLACE_HOLDER> that id ?,return ! cached device list . contains ( id ) ;,list contain,success,pre
optimization : the special case where we 're a message of size 1 @$ and the other factor is <PLACE_HOLDER> the message @$ and of size 2,if ( domain . size ( ) == __num__ && ( result domain . size ( ) == other domain . size ( ) ) && result domain . size ( ) == __num__ ) { return other . multiply ( this ) ; } else { int [ ] mapping = new int [ result . neighbor indices . length ] ; int [ ] other mapping = new int [ result . neighbor indices . length ] ; for ( int i = __num__ ; i < result . neighbor indices . length ; i ++ ) { mapping [ i ] = domain . index of ( result . neighbor indices [ i ] ) ; other mapping [ i ] = other domain .,factor completing,fail,pre
should use <PLACE_HOLDER> object @$ with original bit array size,bit array array3 = new bit array ( __num__ ) ; array3 = matrix . get row ( __num__ @$ array3 ) ; assert equals ( __num__ @$ array3 . get size ( ) ) ; for ( int x = __num__ ; x < __num__ ; x ++ ) { boolean on = ( x & __num__ ) == __num__ ; assert equals ( on @$ array . get ( x ) ) ; assert equals ( on @$ array2 . get ( x ) ) ; assert equals ( on @$ array3 . get ( x ) ) ; },use object,fail,pre
all symbols must <PLACE_HOLDER> declaration nodes .,check not null ( decl node ) ; return declare symbol ( sym . get name ( ) @$ get type ( sym ) @$ is type inferred ( sym ) @$ scope @$ decl node @$ sym . getjs doc info ( ) ) ;,symbols have,success,pre
assert that the log <PLACE_HOLDER> the correct message .,html page log = j . create web client ( ) . get page ( proj @$ __str__ ) ; string logastext = log . as text ( ) ; assert true ( logastext . contains ( __str__ + abstract project . workspace offline reason . all_suitable_nodes_are_offline . name ( ) + __str__ ) ) ;,log contains,success,pre
since the link is relative and the holder is not even being documented @$ this must be an inherited link . redirect it . the current class either <PLACE_HOLDER> the referenced member or inherits it automatically .,if ( see . text ( ) . trim ( ) . starts with ( __str__ ) && ! ( containing . is public ( ) || util . is linkable ( containing @$ configuration ) ) ) { if ( this instanceof class writer impl ) { containing = ( ( class writer impl ) this ) . get class doc ( ) ; } else if ( ! containing . is public ( ) ) { configuration . get doclet specific msg ( ) . warning ( see . position ( ) @$ __str__ @$ tag name @$ containing . qualified name ( ) ) ; } else { configuration . get doclet specific msg ( ) . warning ( see . position ( ) @$,class accesses,fail,pre
if no values in this block were <PLACE_HOLDER> @$ we can just <PLACE_HOLDER> its index to be the same as some other block with no values <PLACE_HOLDER> @$ assuming we 've seen one yet .,if ( ! block touched [ plane ] [ i ] && i untouched != - __num__ ) { indices [ plane ] [ i ] = i untouched ; } else { int j block start = limit compacted * blockcount ; if ( i > limit compacted ) { system . arraycopy ( values [ plane ] @$ i block start @$ values [ plane ] @$ j block start @$ blockcount ) ; } if ( ! block touched [ plane ] [ i ] ) { i untouched = ( short ) j block start ; } indices [ plane ] [ i ] = ( short ) j block start ; limit compacted ++ ; },block set,success,pre
after tx state proxy committed @$ <PLACE_HOLDER> lock will <PLACE_HOLDER> the lock for the oldtx but caller should not perform ops on this tx state proxy,assert true ( tx mgr . get lock ( tx @$ txid ) ) ;,lock get,success,pre
if this list does not contain our class name @$ then its not <PLACE_HOLDER> our class this is a cheezy test ... but it errs on the side of less false hits .,if ( meth . get reference names ( ) . is empty ( ) && ! meth . is super ( ) ) { list < string > pack class = meth . get qualifier names ( ) ; if ( ! pack class . is empty ( ) ) { for ( string name : pack class ) { if ( name . equals ( class name ) ) { found = true ; break ; } } } else { found = true ; } },its matching,fail,pre
the user <PLACE_HOLDER> the service to toggle touch exploration .,m enable touch exploration dialog = new alert dialog . builder ( m context ) . set icon attribute ( android . r . attr . alert dialog icon ) . set positive button ( android . r . string . ok @$ new on click listener ( ) { @ override public void on click ( dialog interface dialog @$ int which ) { user state . m touch exploration granted services . add ( service . m component name ) ; persist component names to setting locked ( settings . secure . touch_exploration_granted_accessibility_services @$ user state . m touch exploration granted services @$ user state . m user id ) ; user state . m is touch exploration enabled = true ; final long identity =,user provides,fail,pre
local job tracker <PLACE_HOLDER> the only task per wave @$ but text input format replaces it with the calculated value based on input split size option .,if ( __str__ . equals ( cfg . get ( __str__ @$ __str__ ) ) ) { file input format . set min input split size ( job cfg @$ __num__ * __num__ * __num__ ) ; file input format . set max input split size ( job cfg @$ long . max_value ) ; },tracker creates,fail,pre
if maps @$ recursively <PLACE_HOLDER> the key and value types,if ( c1 . equals ( category . map ) ) { map object inspector mapoi1 = ( map object inspector ) o1 ; map object inspector mapoi2 = ( map object inspector ) o2 ; object inspector child key1 = mapoi1 . get map key object inspector ( ) ; object inspector child key2 = mapoi2 . get map key object inspector ( ) ; if ( compare types ( child key1 @$ child key2 ) ) { object inspector child val1 = mapoi1 . get map value object inspector ( ) ; object inspector child val2 = mapoi2 . get map value object inspector ( ) ; if ( compare types ( child val1 @$ child val2 ) ) { return true ; } } return,maps compare,success,pre
json does n't <PLACE_HOLDER> null keys @$ so just write the null key value,if ( input == null ) { map . put ( null key value @$ null ) ; } else { map . put ( input @$ input ) ; },json support,success,pre
do not animate the clock when waking up from a pulse . the height callback will <PLACE_HOLDER> care of pushing the clock to the right position .,if ( ! m pulsing && ! m dozing ) { m animate next position update = false ; } m notification stack scroller . set pulsing ( pulsing @$ animate pulse ) ; m keyguard status view . set pulsing ( pulsing ) ;,callback take,success,pre
list of need <PLACE_HOLDER> next contains all tables that we have joined data in their candidate storage @$ and we need to clear candidate storage and promote their next group storage to candidate storage and <PLACE_HOLDER> data until we reach a new group .,if ( ( list of need fetch next . size ( ) > __num__ ) && clear ) { for ( byte b : list of need fetch next ) { try { fetch next group ( b ) ; } catch ( exception e ) { throw new hive exception ( e ) ; } } },list fetch,success,pre
we inserted a keyframe let 's <PLACE_HOLDER> the counter .,j ++ ;,'s increment,fail,pre
test that generated policy file has <PLACE_HOLDER> both policies,assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( class loader perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ; assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( socket perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ;,file generated,fail,pre
metered change should <PLACE_HOLDER> ifaces,m cell network agent . add capability ( network capabilities . net_capability_not_metered ) ; wait for idle ( ) ; verify ( m stats service @$ at least once ( ) ) . force update ifaces ( eq ( only cell ) @$ any ( network state [ ] . class ) @$ eq ( mobile_ifname ) @$ eq ( new vpn info [ __num__ ] ) ) ; reset ( m stats service ) ; m cell network agent . remove capability ( network capabilities . net_capability_not_metered ) ; wait for idle ( ) ; verify ( m stats service @$ at least once ( ) ) . force update ifaces ( eq ( only cell ) @$ any ( network state [ ] . class ),change update,success,pre
show a unified format which <PLACE_HOLDER> scheme @$ host and port .,string src service name = src fs . get canonical service name ( ) ; string des service name = des fs . get canonical service name ( ) ; if ( src service name == null || des service name == null ) { return false ; } if ( src service name . equals ( des service name ) ) { return true ; } if ( src service name . starts with ( __str__ ) && des service name . starts with ( __str__ ) ) { collection < string > internal name services = conf . get trimmed string collection ( __str__ ) ; if ( ! internal name services . is empty ( ) ) { if ( internal name services . contains,which includes,fail,pre
make cookie secure when <PLACE_HOLDER> ssl,if ( use secure ) { root context . get session handler ( ) . get session cookie config ( ) . set secure ( use secure ) ; },cookie using,success,pre
components commonly have conditional handlers assigned . using the click handler matcher we can assert whether or not a given component <PLACE_HOLDER> a handler attached to them . noinspection unchecked,assert that ( c @$ component ) . extracting sub component at ( __num__ ) . has ( sub component with ( c @$ test footer component . matcher ( c ) . click handler ( is null . < event handler < click event > > null value ( null ) ) . build ( ) ) ) ;,component has,success,pre
wsdl does n't <PLACE_HOLDER> any schema fragments,return new source [ __num__ ] ;,wsdl support,fail,pre
memory map does <PLACE_HOLDER> suppression,return - __num__ ;,map allow,fail,pre
jar files have only <PLACE_HOLDER> content @$ not the source toc file,return null ;,files contained,fail,pre
skip bundles that have n't reached <PLACE_HOLDER> state ; skip fragments .,if ( b . get state ( ) <= bundle . resolved || b . get headers ( ) . get ( constants . fragment_host ) != null ) continue ; try { cls = b . load class ( name ) ; break ; } catch ( class not found exception ignored ) { },bundles resolved,success,pre
check if instrument <PLACE_HOLDER> general midi 2 default banks .,int bank = get patch ( ) . get bank ( ) ; if ( bank > > __num__ == __num__ || bank > > __num__ == __num__ ) { boolean [ ] ch = new boolean [ __num__ ] ; for ( int i = __num__ ; i < ch . length ; i ++ ) ch [ i ] = true ; return ch ; } boolean [ ] ch = new boolean [ __num__ ] ; for ( int i = __num__ ; i < ch . length ; i ++ ) ch [ i ] = true ; ch [ __num__ ] = false ; return ch ;,instrument has,fail,pre
return new byte <PLACE_HOLDER> every time to prevent potential side effects,if ( clazz . equals ( byte buffer . class ) ) { byte buffer buff = byte buffer . allocate ( __num__ ) ; rand . next bytes ( buff . array ( ) ) ; return buff ; } else if ( type . equals ( placement constraint . class ) ) { placement constraint . abstract constraint s constraint expr = target in ( node @$ allocation tag ( __str__ ) ) ; ret = placement constraints . build ( s constraint expr ) ; },byte buffer,success,pre
completing all the tasks <PLACE_HOLDER> the case instance,for ( task t : cmmn task service . create task query ( ) . case instance id ( case instance . get id ( ) ) . list ( ) ) { cmmn task service . complete ( t . get id ( ) ) ; } assert case instance ended ( case instance ) ;,tasks completes,fail,pre
check new server <PLACE_HOLDER> listener for reconnected client .,try ( ignite new srv = start grid ( server count ( ) + __num__ ) ) { await partition map exchange ( ) ; lsnr . latch = new count down latch ( __num__ ) ; ignite cache < object @$ object > new srv cache = new srv . cache ( default_cache_name ) ; for ( integer key : primary keys ( new srv cache @$ __num__ ) ) new srv cache . put ( key @$ key ) ; assert true ( lsnr . latch . await ( __num__ @$ milliseconds ) ) ; } cur . close ( ) ;,server register,fail,pre
check if list <PLACE_HOLDER> some members that can be rebalanced,while ( iterator . has next ( ) ) { if ( iterator . next ( ) . ds member list . size ( ) > __num__ ) { flag to continue with rebalance = true ; break ; } } if ( ! flag to continue with rebalance ) { rebalance result . set status message ( cli strings . rebalance__msg__no_rebalancing_regions_on_ds ) ; rebalance result . set success ( true ) ; return rebalance result ; } list < rebalance region result > rebalance region results = new array list < > ( ) ; for ( memberpr info memberpr : list member region ) { try { if ( memberpr . ds member list . size ( ) > __num__ ) { for ( int i,list contains,fail,pre
protected mode should have <PLACE_HOLDER> the node,assert . assert equals ( children . size ( ) @$ __num__ @$ children . to string ( ) ) ;,mode encountered,fail,pre
prefer no default domain @$ as wild fly does not <PLACE_HOLDER> an mbean server with a default domain,if ( null != m bean server . get default domain ( ) ) { for ( int i = __num__ ; i < m bean server list . size ( ) ; i ++ ) { m bean server anm bean server = m bean server list . get ( i ) ; if ( null == anm bean server . get default domain ( ) ) { m bean server = anm bean server ; break ; } } },fly support,fail,pre
executors <PLACE_HOLDER> no offer @$ so make our own .,int queued = queue size . get and increment ( ) ; if ( max queue length > __num__ && queued >= max queue length ) { queue size . decrement and get ( ) ; return false ; } executor . execute ( new fifo call runner ( task ) { @ override public void run ( ) { task . set status ( rpc server . get status ( ) ) ; task . run ( ) ; queue size . decrement and get ( ) ; } } ) ; return true ;,executors have,fail,pre
does the program class already <PLACE_HOLDER> a new name ?,string new class name = new class name ( program class ) ; if ( new class name != null ) { class names to avoid . add ( mixed case class name ( new class name ) ) ; if ( repackage classes == null || ! allow access modification ) { string class name = program class . get name ( ) ; map package name ( class name @$ new class name @$ repackage classes == null && flatten package hierarchy == null ) ; } },class have,success,pre
alice can <PLACE_HOLDER> upstream @$ so downstream gets built @$ but the upstream build can not <PLACE_HOLDER> downstream :,auth . grant ( item . read ) . on items ( upstream ) . to ( __str__ @$ __str__ ) ; map < string @$ authentication > qia config = new hash map < string @$ authentication > ( ) ; qia config . put ( upstream name @$ user . get ( __str__ ) . impersonate ( ) ) ; qia config . put ( downstream name @$ user . get ( __str__ ) . impersonate ( ) ) ; queue item authenticator configuration . get ( ) . get authenticators ( ) . replace ( new mock queue item authenticator ( qia config ) ) ; b = r . build and assert success ( upstream ) ; r . assert log not contains (,upstream access,fail,pre
output proto is activity manager service <PLACE_HOLDER> activities proto,if ( __str__ . equals ( cmd ) || __str__ . equals ( cmd ) ) { m atm internal . write activities to proto ( proto ) ; } else if ( __str__ . equals ( cmd ) || __str__ . equals ( cmd ) ) { synchronized ( this ) { write broadcasts to proto locked ( proto ) ; } } else if ( __str__ . equals ( cmd ) ) { string [ ] new args ; string name ; if ( opti >= args . length ) { name = null ; new args = empty_string_array ; } else { name = args [ opti ] ; opti ++ ; new args = new string [ args . length - opti ] ;,service broadcasts,fail,pre
imply strictfp if owner <PLACE_HOLDER> strictfp set .,implicit |= sym . owner . flags_field & strictfp ; break ; default : throw new assertion error ( ) ;,owner has,success,pre
if our constituents have post exported linker flags @$ our dependents should <PLACE_HOLDER> them .,for ( native linkable linkable : constituents . get linkables ( ) ) { args builder . add all ( linkable . get exported post linker flags ( graph builder ) ) ; } return native linkable input . of ( args builder . build ( ) @$ immutable list . of ( ) @$ immutable list . of ( ) ) ;,dependents use,success,pre
update the screen to see changes <PLACE_HOLDER> effect,if ( m map view != null ) { this . invalidate compass ( ) ; },changes take,success,pre
response account features <PLACE_HOLDER> package name,m ams . has features ( m mock account manager response @$ account manager service test fixtures . account_success @$ null @$ __str__ ) ;,features op,success,pre
nm 5 <PLACE_HOLDER> no moo allocated containers .,for ( container moo container : allocated containers ) { assert . assert false ( moo container . get node id ( ) . equals ( nm5 . get node id ( ) ) ) ; },nm has,success,pre
recompute system <PLACE_HOLDER> visibility .,if ( ( m window attributes changes flag & window manager . layout params . translucent_flags_changed ) != __num__ ) { m attach info . m recompute global attributes = true ; },system handle,fail,pre
tccl ca n't <PLACE_HOLDER> the class,is context loader = false ;,tccl find,fail,pre
first @$ reduce to account asset balance . else ca n't <PLACE_HOLDER> this test case .,account capsule to account = db manager . get account store ( ) . get ( byte array . from hex string ( to_address ) ) ; long id = db manager . get dynamic properties store ( ) . get token id num ( ) ; to account . reduce asset amountv2 ( byte string . copy from utf8 ( string . value of ( id ) ) . to byte array ( ) @$ total_supply - __num__ @$ db manager . get dynamic properties store ( ) @$ db manager . get asset issue store ( ) ) ; db manager . get account store ( ) . put ( to account . get address ( ) . to byte array ( ) @$ to account,first complete,success,pre
for local queries returning pdx objects wrap the resultset with results collection pdx deserializer wrapper which <PLACE_HOLDER> these pdx objects .,if ( needspdx deserialization wrapper ( false ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ needs copy on read wrapper ) ; } else if ( ! is remote query ( ) && this . cache . get copy on read ( ) && result instanceof select results ) { if ( needs copy on read wrapper ) { result = new results collection copy on read wrapper ( ( select results ) result ) ; } },which wraps,fail,pre
call <PLACE_HOLDER> cutout with empty bounds to remove the cutout .,set cutout ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ;,call set,success,pre
assert that the current value in store <PLACE_HOLDER> all messages being processed,assert that ( new active store . get ( key ) @$ is ( equal to ( total num messages - __num__ ) ) ) ;,value reflects,success,pre
all <PLACE_HOLDER> return default,return tc . get random data node ( ) ;,all fail,success,pre
verify that scan <PLACE_HOLDER> correct exception,scan scan = new scan ( ) ; try { result scanner scanner = ht . get scanner ( scan ) ; result res = null ; do { res = scanner . next ( ) ; } while ( res != null ) ; } catch ( table not enabled exception e ) { ok = true ; } assert true ( ok ) ; admin . enable table ( table ) ; assert true ( __str__ @$ test_util . geth base cluster ( ) . get master ( ) . get table state manager ( ) . is table state ( ht . get name ( ) @$ table state . state . enabled ) ) ; assert equals ( table state . state . enabled,scan encounters,success,pre
the editing stopped event is how the client can <PLACE_HOLDER> the enter key to close the widget,assert equals ( __str__ + __str__ @$ listener . stopped count @$ __num__ ) ;,client trigger,fail,pre
master <PLACE_HOLDER> the meta @$ so restart the master .,if ( server . equals ( cluster status . get master name ( ) ) ) { restart master ( server @$ sleep time ) ; } else { restart rs ( server @$ sleep time ) ; },master changed,fail,pre
then the produced thread dump should <PLACE_HOLDER> that expected method at least,assert true ( file contains ( threaddump file @$ __str__ @$ dumpable process . class . get name ( ) ) ) ;,dump contain,success,pre
if string <PLACE_HOLDER> matrix @$ create aux vec,map < string @$ integer > map = m . get row label bindings ( ) ; if ( map != null ) { labels = make empty str vec ( frame . any vec ( ) ) ; vec . writer writer = labels . open ( ) ; map < integer @$ string > rmap = reverse map ( map ) ; for ( int r = __num__ ; r < m . row size ( ) ; r ++ ) { writer . set ( r @$ rmap . get ( r ) ) ; } writer . close ( closer ) ; },string contains,fail,pre
be sure to let our parent <PLACE_HOLDER> any initialization needed,j label renderer = ( j label ) super . get table cell renderer component ( data ) ; object value = data . get value ( ) ; j table table = data . get table ( ) ; boolean is selected = data . is selected ( ) ; set text ( __str__ ) ; set horizontal alignment ( center ) ; vt match match = ( vt match ) value ; vt association association = match . get association ( ) ; vt association status association status = association . get status ( ) ; if ( ! is selected ) { renderer . set background ( match table renderer . get background color ( association @$ table @$ renderer . get background ( ),parent perform,success,pre
the value specified in xml <PLACE_HOLDER> precedence over the environment variable !,if ( config . get object timeout minutes ( ) > __num__ ) { object timeout = config . get object timeout minutes ( ) ; } else if ( ! utils . is empty ( system timeout ) ) { object timeout = const . to int ( system timeout @$ __num__ ) ; } else { object timeout = __num__ * __num__ ; },value takes,success,pre
pulling this common case out of ` is timer enabled slow ` <PLACE_HOLDER> c 1 a better chance to inline this method .,if ( ! metrics enabled ) { return false ; },slow gives,success,pre
default config should not <PLACE_HOLDER> the identities,identity transformer identity transformer = get transformer with default identity config ( config ) ; identity transformer . transform acl entries for set request ( acl entries ) ; check acl entries list ( acl entries to be transformed @$ acl entries ) ; reset identity config ( config ) ;,config change,success,pre
one <PLACE_HOLDER> group,list < scanner report . duplication > duplication groups = result . duplications for ( input file ) ; assert that ( duplication groups ) . has size ( __num__ ) ; scanner report . duplication clone group = duplication groups . get ( __num__ ) ; assert that ( clone group . get origin position ( ) . get start line ( ) ) . is equal to ( __num__ ) ; assert that ( clone group . get origin position ( ) . get end line ( ) ) . is equal to ( __num__ ) ; assert that ( clone group . get duplicate list ( ) ) . has size ( __num__ ) ; assert that ( clone group . get duplicate ( __num__,one rebuilt,fail,pre
the <PLACE_HOLDER> should <PLACE_HOLDER> out all rows @$ but we still expect to see every row .,filter filter = new row filter ( compare operator . equal @$ new binary comparator ( bytes . to bytes ( __str__ ) ) ) ; scan = new scan ( base scan ) ; scan . set filter ( filter ) ; test metric ( scan @$ server side scan metrics . count_of_rows_scanned_key_metric_name @$ rows . length ) ;,filter filter,success,pre
lease will not be granted if the time taken so far plus lease time <PLACE_HOLDER> the max allowabale .,while ( segment completion mgr . _seconds + lease time sec <= start time + segment completion manager . get max commit time for all segments seconds ( ) ) { params = new request . params ( ) . with instance id ( s2 ) . with offset ( s2 offset ) . with segment name ( segment name str ) . with extra time sec ( lease time sec ) ; response = segment completion mgr . extend build time ( params ) ; assert . assert equals ( response . get status ( ) @$ controller response status . processed ) ; assert . assert true ( ( fsm map . contains key ( segment name str ) ) ) ; segment completion mgr .,time exceeds,success,pre
test <PLACE_HOLDER> last buffer byte across input data bytes .,test array . set position ( __num__ ) ; result [ __num__ ] = __num__ ; test array . read bits ( result @$ __num__ @$ __num__ ) ; assert that ( result [ __num__ ] ) . is equal to ( ( byte ) __num__ ) ;,test equals,fail,pre
checks the query buffer for cte queries or simple select statements <PLACE_HOLDER> the index where the injection should start .,final int offset = get statement index ( sb ) ; if ( ! limit helper . has first row ( selection ) ) { add top expression ( sb @$ offset ) ; } else { final string select clause = fill alias in select clause ( sb @$ offset ) ; if ( shallow index of pattern ( sb @$ order_by_pattern @$ offset ) > __num__ ) { add top expression ( sb @$ offset ) ; } enclose with outer query ( sb @$ offset ) ; sb . insert ( offset @$ ! iscte ? __str__ : __str__ ) ; sb . append ( __str__ ) . append ( select clause ) . append ( __str__ ) ; sb . append ( __str__ ),checks give,fail,pre
to test that the function correctly <PLACE_HOLDER> them in the resulting template .,string key uri = __str__ ; key template template = aead key templates . create kms aead key template ( key uri ) ; assert equals ( new kms aead key manager ( ) . get key type ( ) @$ template . get type url ( ) ) ; assert equals ( output prefix type . tink @$ template . get output prefix type ( ) ) ; kms aead key format format = kms aead key format . parse from ( template . get value ( ) @$ extension registry lite . get empty registry ( ) ) ; assert equals ( key uri @$ format . get key uri ( ) ) ;,function puts,success,pre
either node <PLACE_HOLDER> constraint or source allocation tag alone,if ( splitted . length == __num__ ) { node constraint parser np = new node constraint parser ( spec str ) ; optional < abstract constraint > constraint optional = optional . of nullable ( np . try parse ( ) ) ; if ( constraint optional . is present ( ) ) { st = source tags . empty source tags ( ) ; constraint = constraint optional . get ( ) . build ( ) ; } else { st = source tags . parse from ( spec str ) ; constraint = null ; } } else { throw new placement constraint parse exception ( __str__ + spec str ) ; },node contains,fail,pre
since we ca n't see the request object on the remote side @$ we ca n't check whether the remote side actually <PLACE_HOLDER> an authorization check here @$ so always set this to true for the proxy servlet . if the remote node failed to perform an authorization check @$ pre response authorization check filter will log that on the remote node .,client request . set attribute ( auth config . druid_authorization_checked @$ true ) ;,side does,fail,pre
revoke table <PLACE_HOLDER> permission to test grant revoke .,try { revoke from table using access control client ( test_util @$ system user connection @$ test grant revoke . get short name ( ) @$ test_table @$ null @$ null @$ permission . action . read ) ; } catch ( throwable e ) { log . error ( __str__ @$ e ) ; },table read,success,pre
provide an empty key store since trust manager impl does n't <PLACE_HOLDER> null key stores . trust manager impl will use cert store to lookup certificates .,key store store = key store . get instance ( key store . get default type ( ) ) ; store . load ( null ) ; m delegate = new trust manager impl ( store @$ null @$ cert store ) ;,impl support,success,pre
make the user agent tester <PLACE_HOLDER> its states and make sure we are notified,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = supported status set2 . get ( jabber status enum . free_for_chat ) ;,tester change,success,pre
now make sure declarative cache ca n't <PLACE_HOLDER> the same pool,try { cache creation cache = new cache creation ( ) ; cache . create pool factory ( ) . add locator ( alias2 @$ __num__ ) . create ( __str__ ) ; ignored exception expected exception = add ignored exception ( string . format ( __str__ @$ __str__ ) ) ; try { test xml ( cache ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } finally { expected exception . remove ( ) ; } } finally { pool manager . close ( ) ; },cache create,success,pre
excepted error will be something like : javax.ejb.ejb exception : wflyjpa 0030 : found extended persistence context in sfsb invocation call stack but that can not be used because the transaction already <PLACE_HOLDER> a transactional context associated with it ...,try { top level bean . reference two distinct extended persistence contexts in sametx_fail ( ) ; } catch ( ejb exception caught ) { error = caught ; } assert not null ( __str__ + __str__ @$ error ) ;,transaction has,success,pre
these should only <PLACE_HOLDER> the given table,session s = open session ( ) ; transaction t = s . begin transaction ( ) ; int count = s . create query ( __str__ ) . set string ( __str__ @$ __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; count = s . create query ( __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; t . commit ( ) ; s . close ( ) ; data . cleanup ( ) ;,these reference,fail,pre
some point the proxy client class should <PLACE_HOLDER> the connection aliveness monitoring and no longer rely on the custom tabs to ping the connections .,try { getm bean server connection ( ) . get default domain ( ) ; } catch ( io exception ex ) { vm panel . get proxy client ( ) . mark as dead ( ) ; },class remove,fail,pre
only static endpoints @$ <PLACE_HOLDER> an optimized endpoint group .,return new static endpoint group ( static endpoints ) ;,endpoints return,success,pre
wait 2 election timeouts so that this master and other masters <PLACE_HOLDER> time to realize they are not leader .,common utils . sleep ms ( __num__ * m conf . get election timeout ms ( ) ) ; if ( state machine . get last applied sequence number ( ) != last appliedsn || state machine . get last primary start sequence number ( ) != gain primacysn ) { continue ; },master have,success,pre
find all terminal nodes <PLACE_HOLDER> no incoming dependers .,set < object > all dependents = new hash set < > ( ) ; for ( dependency dep : edges ) { if ( ( dep . depends on instanceof local thread ) ) { if ( dep . depender instanceof message key ) { all dependents . add ( dep . depends on ) ; } } else { all dependents . add ( dep . depends on ) ; } } list < dependency graph > result = new linked list < > ( ) ; for ( object depender : depends on obj ) { if ( ! all dependents . contains ( depender ) ) { result . add ( get sub graph ( depender ) ) ; } } return result ;,nodes having,success,pre
this must be done to assure the correct vfs file system drivers will <PLACE_HOLDER> the parameters,string scheme = extract scheme ( full parameter name ) ; try { delegating file system options builder delegatefs options builder = new delegating file system options builder ( kettlevfs . get instance ( ) . get file system manager ( ) ) ; if ( scheme != null ) { delegatefs options builder . set config string ( opts @$ scheme @$ name @$ value ) ; } else { log . log minimal ( __str__ + vfs url ) ; } } catch ( file system exception e ) { if ( ( e . get code ( ) != null ) && ( e . get code ( ) . equals ignore case ( __str__ ) ) ) { log . log minimal ( __str__,drivers store,fail,pre
view <PLACE_HOLDER> high padding area :,if ( view max > client size + padding min ) { if ( m focus scroll strategy == base grid view . focus_scroll_page ) { first view = view ; do { circular int array positions = m grid . get item positions in rows ( pos @$ m grid . get last visible index ( ) ) [ row ] ; last view = find view by position ( positions . get ( positions . size ( ) - __num__ ) ) ; if ( get view max ( last view ) - view min > client size ) { last view = null ; break ; } } while ( append one column visible items ( ) ) ; if ( last view != null,view has,fail,pre
if the text wrapper does n't <PLACE_HOLDER> a start index and the new character is a starting one,if ( squigglies . get ( i ) . start index == - __num__ && ! is closing squiggly ( squiggly ) ) { if ( squiggly of interest == - __num__ ) { squiggly of interest = i ; } else { if ( squigglies . get ( i ) . end index < squigglies . get ( squiggly of interest ) . end index ) { squiggly of interest = i ; } } },wrapper have,success,pre
if the original test <PLACE_HOLDER> off sanity check @$ make sure our synthesized code passes it .,if ( ! validity check && ! compiler . has errors ( ) ) { new var check ( compiler @$ true ) . process ( externs @$ root ) ; },test turned,success,pre
exception should <PLACE_HOLDER> a report,system . err . println ( ) ;,exception provide,fail,pre
this test may not <PLACE_HOLDER> eviction each time @$ repeat it 20 times .,for ( int i = __num__ ; i < __num__ ; i ++ ) { delete during eviction ( i ) ; common utils . sleep ms ( __num__ * heartbeat_interval_ms ) ; },test perform,fail,pre
since on route unselected is triggered before on route selected when transferring to another route @$ pending update if m is <PLACE_HOLDER> route is true to prevent dialog from being dismissed in the process of <PLACE_HOLDER> route .,if ( m route for volume updating by user != null || m is selecting route || m is animating volume slider layout ) { return true ; },m selecting,success,pre
the value of call count can exceed 1 only if the callback thread <PLACE_HOLDER> the exception thrown by the first callback .,assert true ( call count . get ( ) > __num__ ) ; if ( watcher != null ) { watcher . stop ( ) ; watcher . wait for state ( file change watcher . state . stopped ) ; },thread throws,fail,pre
a media period may <PLACE_HOLDER> a discontinuity at the current playback position to ensure the renderers are flushed . only <PLACE_HOLDER> the discontinuity externally if the position changed .,if ( period position us != playback info . position us ) { playback info = playback info . copy with new position ( playback info . period id @$ period position us @$ playback info . content position us @$ get total buffered duration us ( ) ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( ) ; period position us = playing period holder . to period time ( renderer position us ) ; maybe trigger pending messages ( playback info . position us @$ period position us ) ; playback info . position us = period position us ;,period report,success,pre
the following check <PLACE_HOLDER> portions of the cutover year before the cutover itself happens .,if ( is gregorian != ( jd >= cutover julian day ) ) { invert gregorian = true ; jd = super . handle compute julian day ( best field ) ; } return jd ;,check covers,fail,pre
host name should be valid @$ but most probably not existing if its not enough @$ then should probably <PLACE_HOLDER> 'list ' command first to be sure ...,final string not existent fake host name = __str__ ; string credentials not found msg = null ; try { run credential program ( not existent fake host name @$ credential helper name ) ; log . warn ( __str__ @$ credential helper name ) ; } catch ( exception e ) { if ( e instanceof invalid result exception ) { credentials not found msg = extract credential provider error message ( ( invalid result exception ) e ) ; } if ( is blank ( credentials not found msg ) ) { log . warn ( __str__ @$ credential helper name @$ e . get message ( ) ) ; } else { log . debug ( __str__ @$ credentials not found msg ) ; },then have,fail,pre
register an ejb <PLACE_HOLDER> open listener,open listener channel open listener = remoteejb service . get open listener ( ) ; try { registration = endpoint . register service ( ejb_channel_name @$ channel open listener @$ this . channel creation options ) ; } catch ( service registration exception e ) { throw new start exception ( e ) ; },ejb stream,fail,pre
decide whether to perform a <PLACE_HOLDER> or put operation,while ( benchmark complete . get ( ) == false ) { if ( rand . next double ( ) < config . getputratio ) { try { client response response = client . call procedure ( __str__ @$ processor . generate random key for retrieval ( ) ) ; final volt table pair data = response . get results ( ) [ __num__ ] ; if ( pair data . get row count ( ) == __num__ ) missed gets . increment and get ( ) ; else { final payload processor . pair pair = processor . retrieve from store ( pair data . fetch row ( __num__ ) . get string ( __num__ ) @$ pair data . fetch row ( __num__ ) . get,a get,success,pre
sets <PLACE_HOLDER> options .,if ( handshaker options . get rpc protocol versions ( ) != null ) { start client req . set rpc versions ( handshaker options . get rpc protocol versions ( ) ) ; } if ( handshaker options instanceof alts client options ) { alts client options client options = ( alts client options ) handshaker options ; if ( ! strings . is null or empty ( client options . get target name ( ) ) ) { start client req . set target name ( client options . get target name ( ) ) ; } for ( string service account : client options . get target service accounts ( ) ) { start client req . add target identities builder ( ) . set,sets rendraced,fail,pre
time may be negative which <PLACE_HOLDER> time milli seconds before 00:00:00 this maybe a bug in calcite avatica,while ( time < __num__ ) { time += millis_per_day ; } int h = time / __num__ ; int time2 = time % __num__ ; int m = time2 / __num__ ; int time3 = time2 % __num__ ; int s = time3 / __num__ ; int ms = time3 % __num__ ; int2 ( buf @$ h ) ; buf . append ( __str__ ) ; int2 ( buf @$ m ) ; buf . append ( __str__ ) ; int2 ( buf @$ s ) ; if ( precision > __num__ ) { buf . append ( __str__ ) ; while ( precision > __num__ ) { buf . append ( ( char ) ( __str__ + ( ms / __num__ ) ) ) ;,which rememits,fail,pre
if not @$ let 's see which factory method to <PLACE_HOLDER> :,if ( type . is enum type ( ) ) { return factory . create enum deserializer ( ctxt @$ type @$ bean desc ) ; } if ( type . is container type ( ) ) { if ( type . is array type ( ) ) { return factory . create array deserializer ( ctxt @$ ( array type ) type @$ bean desc ) ; } if ( type . is map like type ( ) ) { json format . value format = bean desc . find expected format ( type . get raw class ( ) ) ; if ( format . get shape ( ) != json format . shape . pojo ) { map like type mlt = ( map like,method call,fail,pre
dynamic ser de always <PLACE_HOLDER> out bytes writable,bytes writable bw = ( bytes writable ) r ; out stream . write ( bw . get ( ) @$ __num__ @$ bw . get size ( ) ) ; out stream . write ( final row separator ) ;,ser write,fail,pre
excess processors will notice that they are not needed right now @$ and will park until they are . the most important thing here is that future units will <PLACE_HOLDER> a lower number of processor as expected max .,return lock ;,units have,success,pre
schema and relation id pools are tapped @$ schema id pool twice because the renew is triggered . each id acquisition <PLACE_HOLDER> 1 mutations and 2 reads,verify store metrics ( get config ( ) . get ( ids_store_name ) @$ system_metrics @$ immutable map . of ( m_mutate @$ __num__ @$ m_get_slice @$ __num__ ) ) ;,acquisition triggers,fail,pre
round screen @$ check boxed @$ do n't <PLACE_HOLDER> margins on boxed,if ( m is round ) { if ( ( lp . boxed edges & layout params . box_left ) == __num__ ) { margin left = lp . left margin ; } if ( ( lp . boxed edges & layout params . box_right ) == __num__ ) { margin right = lp . right margin ; } if ( ( lp . boxed edges & layout params . box_top ) == __num__ ) { margin top = lp . top margin ; } if ( ( lp . boxed edges & layout params . box_bottom ) == __num__ ) { margin bottom = lp . bottom margin ; } } else { margin left = lp . left margin ; margin top = lp . top,screen draw,fail,pre
unexpected in bazel logic @$ but skyframe <PLACE_HOLDER> no guarantees that this package and configuration are actually present .,if ( env . values missing ( ) ) { return null ; },unexpected makes,success,pre
we update m last frame always rather than in the conditional with the last inset variables @$ because m frame size changed only <PLACE_HOLDER> the width and height changing .,m window frames . m last frame . set ( m window frames . m frame ) ; if ( did frame insets change || win animator . m surface resized || config changed || drag resizing changed || m report orientation changed ) { if ( debug_resize || debug_orientation ) { slog . v ( tag_wm @$ __str__ + this + __str__ + __str__ + m window frames . get insets changed info ( ) + __str__ + win animator . m surface resized + __str__ + config changed + __str__ + drag resizing changed + __str__ + m report orientation changed ) ; } if ( m app token != null && m app died ) { m app token . remove dead windows ( ),size causes,fail,pre
the token was valid @$ but the impersonation failed . this token is clearly not his real password @$ so there 's no point in continuing the request processing . <PLACE_HOLDER> this error and abort .,logger . log ( warning @$ __str__ + username + __str__ @$ x ) ; throw new servlet exception ( x ) ; throw new servlet exception ( x ) ;,processing log,fail,pre
set bucket encryption <PLACE_HOLDER> no output shape,return new set bucket encryption result ( ) ;,encryption has,success,pre
marks both theme configs as changed so main activity <PLACE_HOLDER> itself on return,find preference ( __str__ ) . set on preference change listener ( new preference . on preference change listener ( ) { @ override public boolean on preference change ( preference preference @$ object new value ) { config . mark changed ( get activity ( ) @$ __str__ ) ; config . mark changed ( get activity ( ) @$ __str__ ) ; get activity ( ) . recreate ( ) ; return true ; } } ) ;,activity recreate,fail,pre
new streams will <PLACE_HOLDER> the last picker,delayed stream wfr5 = ( delayed stream ) delayed transport . new stream ( method @$ headers @$ wait for ready call options ) ; assert null ( wfr5 . get real stream ( ) ) ; in order . verify ( picker ) . pick subchannel ( new pick subchannel args impl ( method @$ headers @$ wait for ready call options ) ) ; in order . verify no more interactions ( ) ; assert equals ( __num__ @$ delayed transport . get pending streams count ( ) ) ;,streams use,success,pre
if records have been read @$ check the <PLACE_HOLDER> next value @$ if not then get the next file to process,if ( this . current file != null && this . current file itr != null ) { this . has next = this . current file itr . has next ( ) ; if ( ! this . has next ) { get next file to read ( ) ; } } else { get next file to read ( ) ; },the has,success,pre
first client that connects <PLACE_HOLDER> this future .,if ( ! connected client . set ( multiplexer ) ) { additional multiplexers . offer ( multiplexer ) ; } try { return connected client . get ( ) . get inbound observer ( ) ; } catch ( interrupted exception | execution exception e ) { throw new runtime exception ( e ) ; },client completes,success,pre
balancer should <PLACE_HOLDER> the region,assert true ( __str__ @$ admin . balance ( ) ) ; test_util . wait until no regions in transition ( ) ; admin . assign ( region . get encoded name as bytes ( ) ) ; test_util . wait until no regions in transition ( __num__ ) ; currentfn = fnm . get favored nodes ( region ) ; assert not null ( currentfn ) ; assert equals ( __str__ @$ favored node assignment helper . favored_nodes_num @$ currentfn . size ( ) ) ; assert true ( __str__ @$ admin . balance ( ) ) ; test_util . wait until no regions in transition ( __num__ ) ; check favored node assignments ( table name @$ fnm @$ region states ) ;,balancer reserve,fail,pre
the second request should fail with unprocessed request exception which <PLACE_HOLDER> a cause of go away received exception .,assert that thrown by ( future2 :: join ) . is instance of ( completion exception . class ) . has cause instance of ( unprocessed request exception . class ) . has root cause instance of ( go away received exception . class ) ;,which has,success,pre
<PLACE_HOLDER>ting the session timeout should not <PLACE_HOLDER> the poll timeout,time . sleep ( max poll interval ms + __num__ ) ; heartbeat . reset session timeout ( ) ; assert true ( heartbeat . poll timeout expired ( time . milliseconds ( ) ) ) ;,timeout reset,success,pre
register the result in the session to ensure retries <PLACE_HOLDER> the same output for the command .,session . register result ( sequence @$ result ) ;,retries see,fail,pre
the max and min distance is the total height of the recycler view minus the height of the last child . this ensures that each <PLACE_HOLDER> will never <PLACE_HOLDER> more than a single page on the recycler view . that is @$ the max <PLACE_HOLDER> will make the last child the first child and vice versa when <PLACE_HOLDER>ing the opposite way .,if ( percentage visible > __num__ ) { max distance -= layout manager . get decorated measured height ( last child ) ; },scroll scroll,success,pre
those modes never <PLACE_HOLDER> rules @$ there is no need to track un<PLACE_HOLDER>ped rules .,if ( build mode == build type . deep || build mode == build type . populate_from_remote_cache ) { return optional . empty ( ) ; },modes generate,fail,pre
right paren <PLACE_HOLDER> matched start of expression node @$ or end of expression matched with a left paren node .,if ( n . f precedence != p ) { error ( rbbi rule builder . u_brk_mismatched_paren ) ; },paren found,fail,pre
the above method will perform the quit as long as the user does not <PLACE_HOLDER> the request,td . display user save ( tool dialog . quit ) ;,user cancel,success,pre
pipeline should <PLACE_HOLDER> ssl handler and server tls handler,iterator < map . entry < string @$ channel handler > > iterator = pipeline . iterator ( ) ; assert that ( iterator . next ( ) . get value ( ) ) . is instance of ( ssl handler . class ) ;,pipeline have,success,pre
if the patch generation does n't <PLACE_HOLDER> port generation @$ return error here in case of mismatch between audio ports and audio patches .,if ( patch generation [ __num__ ] != port generation [ __num__ ] ) { return error ; } for ( int i = __num__ ; i < new patches . size ( ) ; i ++ ) { for ( int j = __num__ ; j < new patches . get ( i ) . sources ( ) . length ; j ++ ) { audio port config port cfg = update port config ( new patches . get ( i ) . sources ( ) [ j ] @$ new ports ) ; new patches . get ( i ) . sources ( ) [ j ] = port cfg ; } for ( int j = __num__ ; j < new patches . get (,generation match,fail,pre
chunk <PLACE_HOLDER> infos from glmmme run,glm weights fun [ ] glmfun rand = null ; long chk start row idx = chunks [ __num__ ] . start ( ) ;,chunk contains,success,pre
bits consumed range : 1 to 8 @$ where 8 <PLACE_HOLDER> last byte fully consumed,bits consumed = __num__ - last bitfield dt . get bit offset ( ) ;,8 represents,fail,pre
we rebuild the the external meeting using the <PLACE_HOLDER> of the parent meeting @$ the shared timestamp and the sequence number,string time stamp = string utils . substring after ( internal meeting id @$ __str__ ) ; string external hash = digest utils . sha1 hex ( ( parent meeting id + __str__ + time stamp + __str__ + params . get ( api params . sequence ) ) ) ; external meeting id = external hash + __str__ + time stamp ;,the see,fail,pre
m fake dragging is true when a fake drag is interrupted by an a 11 y command set it to false so end fake drag wo n't <PLACE_HOLDER> the recycler view,m fake dragging = false ; boolean has new target = m target != target ; m target = target ; dispatch state changed ( scroll_state_settling ) ; if ( has new target ) { dispatch selected ( target ) ; },drag hide,fail,pre
the while loop will <PLACE_HOLDER> a rotating cycle,int current index = i ; int proper doc id ; while ( actual doc id != ( proper doc id = sorted doc ids [ current index ] ) ) { int current doc id = current index + start doc id offset ; _data buffer . copy to ( proper doc id * _doc size long @$ _data buffer @$ current doc id * _doc size long @$ _doc size long ) ; sorted doc ids [ current index ] = current doc id ; current index = proper doc id - start doc id offset ; },loop run,fail,pre
this condition <PLACE_HOLDER> the head processor but has some inputs those are created by notification .,return inputs == null || ( ( collection ) inputs ) . is empty ( ) || ( ( collection < map < string @$ object > > ) inputs ) . stream ( ) . any match ( m -> ! __str__ . equals ( m . get ( __str__ ) ) ) ;,condition passes,fail,pre
set max <PLACE_HOLDER> size to 3 so that 2 calls from the test wo n't trigger back off because the <PLACE_HOLDER> is full .,rpc . builder builder = new server builder ( conf ) . set queue size per handler ( queue size per handler ) . set num handlers ( __num__ ) . set verbose ( true ) ; return setup test server ( builder ) ;,max queue,success,pre
we enqueued a pending frame @$ let 's <PLACE_HOLDER> something else next .,break ;,let try,success,pre
atomically <PLACE_HOLDER> this function on a clone of the bits from the existing key and install the result as the new value . this function may <PLACE_HOLDER> multiple times if there are collisions .,atomic q = new atomic2 ( ) ; q . invoke ( key ) ;,function execute,fail,pre
always <PLACE_HOLDER> a left child .,node left node = nodes [ left index ] ; float left value = left node . value ;,always have,fail,pre
fail rarely ... but allow failure as this one <PLACE_HOLDER> no fallback,if ( math . random ( ) > __num__ ) { throw new runtime exception ( __str__ ) ; },one provides,fail,pre
all elements of each list must <PLACE_HOLDER> equal size so we find the size using the first element .,strict bit vector res = new strict bit vector ( alist . get ( __num__ ) . get size ( ) + blist . get ( __num__ ) . get size ( ) ) ; list < strict bit vector > products = int stream . range ( __num__ @$ alist . size ( ) ) . parallel ( ) . map to obj ( i -> multiply without reduction ( alist . get ( i ) @$ blist . get ( i ) ) ) . collect ( collectors . to list ( ) ) ; products . stream ( ) . reduce ( res @$ ( a @$ b ) -> { a . xor ( b ) ; return a ; } ) ; return,elements have,success,pre
mssql bukl insert can only <PLACE_HOLDER> local files @$ so that 's what we limit ourselves to .,if ( ! ( file object instanceof local file ) ) { throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ vfs filename ) ) ; },insert use,success,pre
ensure that rtr is <PLACE_HOLDER> ok and still making progress,tasks [ __num__ ] = test tasks . unending ( __str__ ) ; results [ __num__ ] = remote task runner . run ( tasks [ __num__ ] ) ; wait for one worker to have unacked tasks ( ) ; if ( rtr test utils . task announced ( __str__ @$ tasks [ __num__ ] . get id ( ) ) ) { rtr test utils . mock worker running task ( __str__ @$ tasks [ __num__ ] ) ; rtr test utils . mock worker complete successful task ( __str__ @$ tasks [ __num__ ] ) ; } else { rtr test utils . mock worker running task ( __str__ @$ tasks [ __num__ ] ) ; rtr test utils . mock worker complete successful task,rtr running,fail,pre
this prevents the case where a program <PLACE_HOLDER> the entire address space in which everything looks like a pointer,if ( program is entire memory space ( program ) ) { return null ; },program covers,fail,pre
<PLACE_HOLDER> all classes with package visible members . <PLACE_HOLDER> all exception catches of methods . count all method invocations . <PLACE_HOLDER> super invocations and other access of methods .,stack size computer stack size computer = new stack size computer ( ) ; program class pool . classes accept ( new multi class visitor ( new class visitor [ ] { new package visible member containing class marker ( ) @$ new all constant visitor ( new package visible member invoking class marker ( ) ) @$ new all method visitor ( new optimization info member filter ( new all attribute visitor ( new multi attribute visitor ( new attribute visitor [ ] { stack size computer @$ new catch exception marker ( ) @$ new all instruction visitor ( new multi instruction visitor ( new instruction visitor [ ] { new super invocation marker ( ) @$ new dynamic invocation marker ( ) @$ new backward,mark see,fail,pre
explicitly setting service rpc <PLACE_HOLDER> datanode . this because dfs util.get nn service rpc addresses <PLACE_HOLDER> cluster looks up client facing port and service port at the same time @$ and if no setting <PLACE_HOLDER> service rpc @$ it would return client port @$ in this case @$ it will be the auxiliary port <PLACE_HOLDER> data node . which is not what auxiliary is,cluster conf . set ( dfs_namenode_service_rpc_address_key @$ __str__ ) ; cluster conf . set ( common configuration keys . hadoop_security_sasl_props_resolver_class @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set boolean ( dfs_namenode_send_qop_enabled @$ true ) ;,auxiliary is,fail,pre
domain attribute must <PLACE_HOLDER> at least one embedded dot @$ or the value must be equal to .local .,int dot index = cookie domain . index of ( __str__ @$ __num__ ) ; if ( ( ( dot index < __num__ ) || ( dot index == cookie domain . length ( ) - __num__ ) ) && ( ! cookie domain . equals ( __str__ ) ) ) { throw new malformed cookie exception ( __str__ + cookie . get domain ( ) + __str__ + __str__ ) ; },attribute have,fail,pre
start the truncate procedure & & <PLACE_HOLDER> the executor,long proc id = proc exec . submit procedure ( new truncate table procedure ( proc exec . get environment ( ) @$ table name @$ preserve splits ) ) ; test recovery and double execution ( util @$ proc id @$ step ) ; procedure testing utility . set kill and toggle before store update ( proc exec @$ false ) ; util . wait until all regions assigned ( table name ) ;,procedure kill,success,pre
no container to cleanup . cleanup <PLACE_HOLDER> level resources .,if ( app . containers . is empty ( ) ) { app . handle app finish with containers cleanedup ( ) ; return application state . application_resources_cleaningup ; },container managed,fail,pre
try all delimiters and choose the delimiter which <PLACE_HOLDER> the shortest frame .,int min frame length = integer . max_value ; channel buffer min delim = null ; for ( channel buffer delim : delimiters ) { int frame length = index of ( buffer @$ delim ) ; if ( frame length >= __num__ && frame length < min frame length ) { min frame length = frame length ; min delim = delim ; } } if ( min delim != null ) { int min delim length = min delim . capacity ( ) ; channel buffer frame ; if ( discarding too long frame ) { discarding too long frame = false ; buffer . skip bytes ( min frame length + min delim length ) ; int too long frame length = this . too,which yields,success,pre
path is n't supported @$ <PLACE_HOLDER> resolver behavior .,if ( remote locations . is empty ( ) ) { return null ; },path retry,fail,pre
let system <PLACE_HOLDER> blame for time tick events .,final work source work source = null ;,system have,fail,pre
so that future open gl revisions wo n't <PLACE_HOLDER> jme 3 fall through intentional,case __num__ : caps . add ( caps . glsl450 ) ; case __num__ : caps . add ( caps . glsl440 ) ; case __num__ : caps . add ( caps . glsl430 ) ; case __num__ : caps . add ( caps . glsl420 ) ; case __num__ : caps . add ( caps . glsl410 ) ; case __num__ : caps . add ( caps . glsl400 ) ; case __num__ : caps . add ( caps . glsl330 ) ; case __num__ : caps . add ( caps . glsl150 ) ; case __num__ : caps . add ( caps . glsl140 ) ; case __num__ : caps . add ( caps . glsl130 ) ; case __num__ : caps . add ( caps,revisions allow,fail,pre
note : the original implementation <PLACE_HOLDER> both methods calling the 3 arg method . this is preserved but perhaps it should pass the args array instead of null .,for ( int i = __num__ ; i < ifcs . length ; i ++ ) { method = internal find method ( ifcs [ i ] @$ method name @$ arg count @$ null ) ; if ( method != null ) { break ; } },implementation expects,fail,pre
both the collapsed and full qs panels <PLACE_HOLDER> this callback @$ this check determines which one should handle showing the detail .,if ( should show detail ( ) ) { qs panel . this . show detail ( show @$ r ) ; } if ( m detail record == r ) { fire toggle state changed ( state ) ; } r . scan state = state ; if ( m detail record == r ) { fire scan state changed ( r . scan state ) ; } if ( announcement != null ) { m handler . obtain message ( h . announce_for_accessibility @$ announcement ) . send to target ( ) ; },collapsed trigger,fail,pre
font shapes define no fillstyles per se @$ but do reference fillstyle index 1 @$ which <PLACE_HOLDER> the font color . we just report null in this case .,begin fill ( null ) ;,which represents,success,pre
we are interested in this fetch only if the beginning offset <PLACE_HOLDER> the current consumed position,if ( error == errors . none ) { subscription state . fetch position position = subscriptions . position ( tp ) ; if ( position == null || position . offset != fetch offset ) { log . debug ( __str__ + __str__ @$ tp @$ fetch offset @$ position ) ; return null ; } log . trace ( __str__ @$ partition . records . size in bytes ( ) @$ tp @$ position ) ; iterator < ? extends record batch > batches = partition . records . batches ( ) . iterator ( ) ; completed fetch = next completed fetch ; if ( ! batches . has next ( ) && partition . records . size in bytes ( ) > __num__ ),beginning matches,success,pre
the first round of <PLACE_HOLDER> uses the m iterator left from last eviction . then <PLACE_HOLDER> the pool from a new iterator for at most two round : first round to mark candidate.m is accessed as false @$ second round to remove the candidate from the pool .,int round to scan = __num__ ; while ( num to evict > __num__ && round to scan > __num__ ) { if ( ! m iterator . has next ( ) ) { m iterator = m pool . entry set ( ) . iterator ( ) ; round to scan -- ; } map . entry < k @$ resource > candidate map entry = m iterator . next ( ) ; resource candidate = candidate map entry . get value ( ) ; if ( candidate . m is accessed ) { candidate . m is accessed = false ; } else { if ( candidate . m ref count . compare and set ( __num__ @$ integer . min_value ) ) { m iterator,iterator scan,success,pre
negative cols <PLACE_HOLDER> number of cols added,if ( cols [ __num__ ] < __num__ ) for ( int i = __num__ ; i < cols . length ; i ++ ) cols [ i ] += fr . num cols ( ) ;,cols equals,fail,pre
intensity steps <PLACE_HOLDER> distance calories,return sample ;,steps hr,success,pre
add properties from <PLACE_HOLDER>er configuration @$ after the injected <PLACE_HOLDER> variables .,final variable resolver < string > resolver = new union < > ( new by map < > ( env ) @$ vr ) ; args . add key value pairs from property string ( __str__ @$ this . properties @$ resolver @$ sensitive vars ) ; if ( uses private repository ( ) ) args . add ( __str__ + build . get workspace ( ) . child ( __str__ ) ) ; args . add tokenized ( normalized target ) ; wrap up arguments ( args @$ normalized target @$ build @$ launcher @$ listener ) ; build env vars ( env @$ mi ) ; if ( ! launcher . is unix ( ) ) { args = args . to windows command ( ),injected include,fail,pre
inclusive range of i <PLACE_HOLDER> 6 addresses,handler . include ( __str__ ) ;,range pv,success,pre
we need some way to tell who the real author <PLACE_HOLDER> . right now i 'm just going to arbitrarily decide that th<PLACE_HOLDER> <PLACE_HOLDER> via the document title @$ as i ca n't really think of an easy way to do it otherw<PLACE_HOLDER>e ; no matter what we decide to use @$ the test set will need to be prepped before hand regardless .,for ( string test doc : results . key set ( ) ) { document curr test doc = null ; for ( document d : test docs ) { if ( d . get title ( ) . equals ( test doc ) ) { curr test doc = d ; break ; } } string selected author = __str__ ; double max = __num__ ; for ( string potential author : results . get ( curr test doc . get title ( ) ) . key set ( ) ) { if ( results . get ( curr test doc . get title ( ) ) . get ( potential author ) . double value ( ) > max ) { max = results . get,author is,success,pre
null params <PLACE_HOLDER> external subject login . no login context will be attached .,return do subject login ( subject @$ null ) ;,params means,fail,pre
v 0 <PLACE_HOLDER> the smallest overhead @$ stricter checking is done later,if ( record size < legacy record . record_overhead_v0 ) throw new corrupt record exception ( string . format ( __str__ @$ record size @$ legacy record . record_overhead_v0 ) ) ; if ( record size > max message size ) throw new corrupt record exception ( string . format ( __str__ @$ record size @$ max message size ) ) ; if ( remaining < header_size_up_to_magic ) return null ; byte magic = buffer . get ( buffer . position ( ) + magic_offset ) ; if ( magic < __num__ || magic > record batch . current_magic_value ) throw new corrupt record exception ( __str__ + magic ) ; return record size + log_overhead ;,v has,success,pre
use a condition that folds after floating guards and before guard lowering . after disabling pea and read elimination @$ the following condition <PLACE_HOLDER> the trick .,if ( static value == static value ) { value = a . x ; result = foo ( a ) ; },condition does,success,pre
clone the call to <PLACE_HOLDER> a race with another thread stomping on the response while being sent . the original call is effectively discarded since the wait count wo n't hit zero,call = new rpc call ( this ) ; setup response ( call @$ status @$ rpc error code proto . error_rpc_server @$ null @$ t . get class ( ) . get name ( ) @$ string utils . stringify exception ( t ) ) ; setup response ( call @$ call . response params . return status @$ call . response params . detailed err @$ call . rv @$ call . response params . error class @$ call . response params . error ) ;,call avoid,fail,pre
close server 2 and pause so server <PLACE_HOLDER> a chance to close,close server ( server2 ) ; wait . pause ( __num__ * __num__ ) ; wait for cqs disconnected ( client @$ __str__ @$ __num__ ) ;,server has,success,pre
hand <PLACE_HOLDER> the big table columns .,for ( int column : big table retain column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } for ( int column : non outer small table key column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } if ( outer small table key column map != null ) { for ( int column : outer small table key column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } },hand retain,fail,pre
the view <PLACE_HOLDER> layout,vertical layout view layout = new vertical layout ( fields ) ; view layout . set size full ( ) ; view layout . set component alignment ( fields @$ alignment . middle_center ) ; view layout . set style name ( reindeer . layout_blue ) ; set composition root ( view layout ) ;,view views,fail,pre
send two task start and two task <PLACE_HOLDER> events for tasks 0 and 1,mockito . when ( reader . get next event ( ) ) . then answer ( new answer < history event > ( ) { public history event answer ( invocation on mock invocation ) throws io exception { int event id = num events read . get and increment ( ) ; taskid tid = tids [ event id & __num__ ] ; if ( event id < __num__ ) { return new task started event ( tid @$ __num__ @$ task type @$ __str__ ) ; } if ( event id < __num__ ) { task failed event tfe = new task failed event ( tid @$ __num__ @$ task type @$ __str__ @$ __str__ @$ null @$ new counters ( ) ) ; tfe .,start started,fail,pre
constrain the maximum state version this backup agent can <PLACE_HOLDER> in case a newer or corrupt backup set existed,if ( state version > state_version ) { state version = state_version ; },agent set,fail,pre
only privileged apps and updated privileged apps can <PLACE_HOLDER> child packages .,if ( pkg . child packages != null && ! pkg . child packages . is empty ( ) ) { if ( ( scan flags & scan_as_privileged ) == __num__ ) { throw new package manager exception ( __str__ + __str__ + pkg . package name ) ; } final int child count = pkg . child packages . size ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { package parser . package child pkg = pkg . child packages . get ( i ) ; if ( m settings . has other disabled system pkg with childl pr ( pkg . package name @$ child pkg . package name ) ) { throw new package manager,apps declare,fail,pre
need to extend this test to validate that different users can <PLACE_HOLDER> apps of exact same name . so far only <PLACE_HOLDER> followed by build is tested . need to test <PLACE_HOLDER> followed by <PLACE_HOLDER> .,client . action destroy ( same app name ) ;,users destroy,fail,pre
was this condition here because it can be <PLACE_HOLDER> by <PLACE_HOLDER> camera ? <PLACE_HOLDER> camera will not trigger events for camera listeners ? or other cause ? actually we can say if we are starting fling here if reason is <PLACE_HOLDER> @$ was it an original condition intention here ?,if ( m camera change tracker . is empty ( ) ) { m camera change tracker . set reason ( reason ) ; log . d ( __str__ @$ __str__ + reason + __str__ + m camera change tracker . is user interaction ( ) + __str__ + m camera change tracker . is animated ( ) ) ; handle map changed event ( event types . region_will_change ) ; } else { log . d ( __str__ @$ __str__ ) ; },reason change,fail,pre
check that ranks <PLACE_HOLDER> a valid topological order,for ( int v = __num__ ; v < g . v ( ) ; v ++ ) { for ( int w : g . adj ( v ) ) { if ( rank ( v ) > rank ( w ) ) { system . err . printf ( __str__ @$ v @$ w @$ v @$ rank ( v ) @$ w @$ rank ( w ) ) ; return false ; } } },ranks provide,success,pre
set up for the schema factory and then set a global one which <PLACE_HOLDER> the others .,conf . set ( committer_factory_class @$ __str__ ) ; create committer factory ( simple committer factory . class @$ http_path @$ conf ) ;,which overrides,success,pre
wait a bit to make sure compiler <PLACE_HOLDER> a cache to do it 's stuff :,try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { } start time = system . nano time ( ) ; record loop with expected interval ( histogram @$ timing loop count @$ expected interval ) ; end time = system . nano time ( ) ; delta usec = ( end time - start time ) / __num__ ; rate = __num__ * timing loop count / delta usec ; system . out . println ( label + __str__ ) ; system . out . println ( label + timing loop count + __str__ + delta usec + __str__ + rate + __str__ ) ; rate = __num__ * histogram . get total count ( ) / delta usec ;,compiler had,success,pre
private method call leaving the invokespecial alone will <PLACE_HOLDER> a verify error,if ( owner . equals ( classname ) ) { string descriptor = utils . insert extra parameter ( owner @$ desc ) ; super . visit method insn ( invokestatic @$ utils . get executor name ( classname @$ suffix ) @$ name @$ descriptor @$ false ) ; return ; } else { type descriptor supertype descriptor = get type ( owner ) ; method member target = supertype descriptor . get by name and descriptor ( name + desc ) ; if ( target != null && target . is protected ( ) ) { super . visit method insn ( invokespecial @$ classname @$ name + method suffix super dispatcher @$ desc @$ false ) ; } else { super . visit method insn,call cause,success,pre
use <PLACE_HOLDER> hash map for predictable iteration order .,this . method to node = new linked hash map < > ( ) ;,use linked,success,pre
we are not interested in defining a new <PLACE_HOLDER> transition . instead we change default transition duration,get window ( ) . get enter transition ( ) . set duration ( get resources ( ) . get integer ( r . integer . anim_duration_long ) ) ;,new enter,success,pre
this is all jealously ; should be refactored to ask tibm if it wants to be filtered for rejoin and eliminate this horrible introspection . this implementation <PLACE_HOLDER> the original live rejoin code for execution site ... multi part ad hoc does not need to be checked because its an alias and runs procedure as planned .,if ( ! msg . is sys proc task ( ) ) { return true ; },implementation retains,fail,pre
sleep to ensure session registry impl will <PLACE_HOLDER> time,thread . sleep ( __num__ ) ;,session make,fail,pre
computation : unconfigured <PLACE_HOLDER> target to raw target node computation,default unconfigured target node factory raw target node factory = new default unconfigured target node factory ( params . get known rule types provider ( ) @$ new built target verifier ( ) @$ cell . get cell path resolver ( ) @$ new selector list factory ( new selector factory ( params . get unconfigured build target factory ( ) ) ) ) ; build target to unconfigured target node computation build target to unconfigured target node computation = build target to unconfigured target node computation . of ( raw target node factory @$ cell ) ;,unconfigured build,success,pre
looks like we 're going to need the buffers ... we know the new string will be shorter . using the old length may <PLACE_HOLDER> a bit @$ but it will save us from resizing the buffer .,if ( decoded == null ) { decoded = new string buffer ( old length ) ; out = new byte array output stream ( __num__ ) ; } else { out . reset ( ) ; },looks change,fail,pre
during this time @$ one of the threads should <PLACE_HOLDER> the partition state @$ and then all of the threads should get the new ring .,assert equals ( count done ( results ) @$ results . size ( ) ) ; executor . shutdown now ( ) ;,one list,fail,pre
workers <PLACE_HOLDER> some time to complete their in progress tasks,long until = system . current time millis ( ) + shutdown timeout in ms ; log . debug ( __str__ @$ shutdown timeout in ms ) ; while ( system . current time millis ( ) < until && ce worker controller . has at least one processing worker ( ) ) { thread . sleep ( __num__ ) ; },workers need,fail,pre
missing response <PLACE_HOLDER> spread around,for ( int row = __num__ ; row < y [ __num__ ] . _len ; ++ row ) { if ( y [ __num__ ] . isna ( row ) ) { if ( ( start + row ) % nfolds == test fold ) y [ __num__ ] . set ( row @$ test fold ) ; } else { if ( y [ __num__ ] . at8 ( row ) == ( classes == null ? class label : classes [ class label ] ) ) { if ( test fold == get fold id ( start + row @$ seeds [ class label ] ) ) y [ __num__ ] . set ( row @$ test fold ) ; } } },response gets,success,pre
remove all <PLACE_HOLDER> headers and find out which modules were used while at it .,cc compilation context . headers and modules headers and modules = cc compilation context . compute declared headers and used modules ( use pic @$ undeclared headers @$ header info ) ; used modules = immutable list . copy of ( headers and modules . modules ) ; undeclared headers . remove all ( headers and modules . headers ) ;,all declared,success,pre
weird one : does not <PLACE_HOLDER> one @$ because it scans for consistent case only,assert false ( r . contains ignore case ( __str__ ) ) ;,not have,fail,pre
it will be checked inside the put metadata that poor metadata does not <PLACE_HOLDER> rich metadata,segment . fulltext ( ) . put metadata ( entry ) ;,metadata contain,fail,pre
... then <PLACE_HOLDER> the uvs for the specified uv set name ...,list < vector2f > uvs for name = uvs for material . get ( uv set name ) ; if ( uvs for name == null ) { uvs for name = new array list < vector2f > ( ) ; uvs for material . put ( uv set name @$ uvs for name ) ; },then keep,fail,pre
blk can <PLACE_HOLDER> loop back,epsilon ( blk end @$ loop ) ;,blk throw,fail,pre
set stopped if no more <PLACE_HOLDER> requests tp meta tables since last time we went around the loop . any open meta regions will be closed on our way out .,if ( all user regions offline ) { if ( old request count == get write request count ( ) ) { stop ( __str__ ) ; break ; } old request count = get write request count ( ) ; } else { close user regions ( this . abort requested ) ; },tables write,success,pre
note : these lines <PLACE_HOLDER> the about dialog in the current window .,point p = main frame . get location on screen ( ) ; dimension d1 = main frame . get size ( ) ; dimension d2 = dialog . get size ( ) ; dialog . set location ( p . x + ( d1 . width - d2 . width ) / __num__ @$ p . y + ( d1 . height - d2 . height ) / __num__ ) ; dialog . set visible ( true ) ;,lines show,fail,pre
push view and context <PLACE_HOLDER> information from android to flutter .,send user settings to flutter ( ) ; send locales to flutter ( get resources ( ) . get configuration ( ) ) ; send viewport metrics to flutter ( ) ;,view config,fail,pre
check if the range is <PLACE_HOLDER> the quote or after it .,boolean is before = range . second . equals ( quote begin token index - __num__ ) ;,range following,fail,pre
revisit : the following should also <PLACE_HOLDER> id table,attr . set node value ( attr value ) ; index = f element . set xerces attribute node ( attr ) ; f augmentations . insert element at ( new augmentations impl ( ) @$ index ) ; attr . set specified ( false ) ;,the update,success,pre
update video <PLACE_HOLDER> count @$ audio & subtitle <PLACE_HOLDER> lists .,m video track count = __num__ ; m audio tracks = new array list < > ( ) ; m subtitle tracks = new array list < > ( ) ; m selected audio track index = __num__ ;,audio track,success,pre
checks for entries that failed to import . get failures <PLACE_HOLDER> up to 100 of the first failed entries for the job @$ if any exist .,list < string > failed endpoints = get import job result . get import job response ( ) . get failures ( ) ; if ( failed endpoints != null ) { system . out . println ( __str__ ) ; for ( string failed endpoint : failed endpoints ) { system . out . println ( failed endpoint ) ; } },failures provides,success,pre
assign playback info immediately such that all getters <PLACE_HOLDER> the right values .,playback info previous playback info = this . playback info ; this . playback info = playback info ; boolean is playing = is playing ( ) ; notify listeners ( new playback info update ( playback info @$ previous playback info @$ listeners @$ track selector @$ position discontinuity @$ position discontinuity reason @$ timeline change reason @$ seek processed @$ play when ready @$ previous is playing != is playing ) ) ;,getters return,success,pre
durable client <PLACE_HOLDER> durable cq on server,register interest ( durable clientvm @$ region name @$ true @$ interest result policy . keys_values ) ;,client registers,success,pre
initialize <PLACE_HOLDER> gesture .,previous touch point px . set ( e . getx ( ) @$ e . gety ( ) ) ; return true ;,initialize consume,fail,pre
lazy definition of schema : do not <PLACE_HOLDER> empty fields,final boolean solrlazy = get config bool ( switchboard constants . federated_service_solr_indexing_lazy @$ true ) ;,definition add,fail,pre
make sure the user now <PLACE_HOLDER> defaultproc permissions,try { user client . call procedure ( __str__ @$ __num__ @$ __num__ ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; } threw = false ; try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { assert true ( pce . get message ( ) . contains ( __str__ ) ) ; threw = true ; } assert true ( __str__ @$ threw ) ; threw = false ; try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { assert true ( pce . get message ( ),user has,success,pre
check <PLACE_HOLDER> permissions and member access before cracking .,try { check access ( ref kind @$ defc @$ member ) ; check security manager ( defc @$ member ) ; } catch ( illegal access exception ex ) { throw new illegal argument exception ( ex ) ; } if ( allowed modes != trusted && member . is caller sensitive ( ) ) { class < ? > caller class = target . internal caller class ( ) ; if ( ! has private access ( ) || caller class != lookup class ( ) ) throw new illegal argument exception ( __str__ + caller class ) ; },check write,fail,pre
now the block <PLACE_HOLDER> 10 internal blocks .,assert equals ( __num__ @$ new dn locs . length ) ;,block has,success,pre
now let 's try to do an upsert where the outcome <PLACE_HOLDER> both on doing an update @$ doing an insert and also triggering a delete .,client . call procedure ( __str__ @$ __num__ @$ __num__ @$ __num__ ) ; client . call procedure ( __str__ @$ __num__ @$ __num__ @$ __num__ ) ;,outcome worked,fail,pre
each follower may have just sent a leader check @$ which <PLACE_HOLDER> no response,cluster . stabilise ( math . max ( default millis ( leader_check_timeout_setting ) + default millis ( leader_check_interval_setting ) + default_delay_variability + default_election_delay @$ default millis ( follower_check_timeout_setting ) + default millis ( follower_check_interval_setting ) + default_delay_variability ) + default_cluster_state_update_delay + default_cluster_state_update_delay ) ;,which receives,success,pre
this object <PLACE_HOLDER> its data . we want to write a send channel that the other side can use to retrieve that data .,if ( m have data ) { if ( m send channel == null ) { m send channel = new send channel ( this ) ; } out . write strong binder ( m send channel ) ; } else { out . write strong binder ( m receive channel ) ; },object has,fail,pre
check that slave <PLACE_HOLDER> min requirements .,if ( cont . get resource ( ) . get virtual cores ( ) < props . cpus per node ( ) || cont . get resource ( ) . get memory ( ) < props . total memory per node ( ) ) { log . log ( level . fine @$ __str__ @$ new object [ ] { cont . get node id ( ) . get host ( ) @$ cont . get resource ( ) . get virtual cores ( ) @$ cont . get resource ( ) . get memory ( ) } ) ; return false ; } return true ;,slave has,fail,pre
verify login user <PLACE_HOLDER> a new ticket .,login user . relogin from keytab ( ) ; kerberos ticket new login ticket = check ticket and keytab ( login user @$ principal1 @$ true ) ; assert . assert not equals ( login ticket . get auth time ( ) @$ new login ticket . get auth time ( ) ) ;,user creates,fail,pre
doesnt matter what node.id and voldemort.home values are for this <PLACE_HOLDER>,props . set property ( __str__ @$ __str__ ) ; props . set property ( __str__ @$ __str__ ) ; voldemort config = new voldemort config ( props ) ; if ( this . prefix partition id ) { voldemort config . set rocksdb prefix keys with partition id ( true ) ; } this . rocks db config = new rocks db storage configuration ( voldemort config ) ; this . rocks db store = ( rocks db storage engine ) rocks db config . get store ( test utils . make store definition ( __str__ ) @$ test utils . make single node routing strategy ( ) ) ; random = new random ( ) ;,values layout,fail,pre
instantiate new headless <PLACE_HOLDER> and parse options .,headless analyzer analyzer = headless analyzer . get loggable instance ( log file @$ script log file @$ true ) ; headless options options = analyzer . get options ( ) ; parse options ( options @$ args @$ option start index @$ ghidraurl @$ files to import ) ;,headless analyze,fail,pre
check if any parameter is referenced . if so @$ user must have <PLACE_HOLDER> policy on the parameter context,for ( final string proposed property value : proposed properties . values ( ) ) { parameter token list token list = parameter parser . parse tokens ( proposed property value ) ; if ( ! token list . to reference list ( ) . is empty ( ) ) { references parameter = true ; break ; } },user read,success,pre
in case we have a pending repin @$ repin now . <PLACE_HOLDER> m pending repin for more information .,synchronized ( this ) { key = m pending repin . get or default ( uid @$ - __num__ ) ; if ( key == - __num__ ) { return ; } m pending repin . remove ( uid ) ; },repin see,success,pre
test trace signature uniqueness <PLACE_HOLDER> the trace file,fs . delete ( test trace file @$ false ) ;,uniqueness delete,fail,pre
fair scheduler does n't <PLACE_HOLDER> this test @$ set capacity scheduler as the scheduler for this test .,conf . set ( yarn configuration . rm_scheduler @$ capacity scheduler . class . get name ( ) ) ; m clock = mock ( clock . class ) ; mcs = mock ( capacity scheduler . class ) ; when ( mcs . get resource calculator ( ) ) . then return ( rc ) ; lm = mock ( rm node labels manager . class ) ; try { when ( lm . is exclusive node label ( any string ( ) ) ) . then return ( true ) ; } catch ( io exception e ) { },scheduler support,success,pre
offset the start so keys do n't <PLACE_HOLDER> and cause constraint violations,m_rangemin = m_rows * topicnum ;,keys determine,fail,pre
close the fd via the parent stream 's close method . the parent will reinvoke our close method @$ which is defined in the superclass abstract interruptible channel @$ but the is open logic in that method will <PLACE_HOLDER> this method from being reinvoked .,if ( parent != null ) { ( ( java . io . closeable ) parent ) . close ( ) ; } else { nd . close ( fd ) ; },method prevent,success,pre
let 's define the schema of the data that we want to import the order in which columns are defined here should <PLACE_HOLDER> the order in which they appear in the input data,schema input data schema = new schema . builder ( ) . add column string ( __str__ ) . add column categorical ( __str__ @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ) . add column string ( __str__ ) . build ( ) ;,order match,success,pre
units can not be new because wdtk <PLACE_HOLDER> them as strings already,return null ;,wdtk treats,fail,pre
cassandra connector currently does not <PLACE_HOLDER> comment on table,assert query fails ( __str__ @$ __str__ ) ;,connector support,success,pre
striped executor <PLACE_HOLDER> a custom adapter .,if ( striped exec svc != null ) { monitor striped pool ( striped exec svc ) ; },executor has,fail,pre
server <PLACE_HOLDER> the first challenge,if ( sasl auth type . has challenge ( ) ) { challenge token = sasl auth type . get challenge ( ) . to byte array ( ) ; sasl auth type = sasl auth . new builder ( sasl auth type ) . clear challenge ( ) . build ( ) ; } else if ( sasl client . has initial response ( ) ) { challenge token = new byte [ __num__ ] ; },server sends,fail,pre
types <PLACE_HOLDER> not <PLACE_HOLDER> types since shows up in intelli j next to types,vec [ ] vecs = vecs ( ) ; string s [ ] = new string [ vecs . length ] ; for ( int i = __num__ ; i < vecs . length ; ++ i ) s [ i ] = vecs [ i ] . get_type_str ( ) ; return s ;,types have,fail,pre
remove the register shutdown hook which <PLACE_HOLDER> the agent from the distributed system upon jvm shutdown,remove shutdown hook ( ) ; logger . info ( __str__ ) ; logging session . shutdown ( ) ;,which removes,fail,pre
now we know that both implementation <PLACE_HOLDER> same status,if ( got status == - __num__ ) { if ( ! got . to string ( ) . equals ( exp . to string ( ) ) ) { errln ( __str__ + refidna name + __str__ + uidna name + __str__ + exp + __str__ + got + __str__ + prettify ( src ) + __str__ + options ) ; } } else { logln ( __str__ + refidna name + __str__ + uidna name + __str__ + prettify ( src ) + __str__ + options ) ; },implementation return,fail,pre
nm 1 <PLACE_HOLDER> 50 heartbeats,donm heartbeat ( rm1 @$ nm1 . get node id ( ) @$ __num__ ) ; check num of containers in an app on given node ( __num__ @$ nm1 . get node id ( ) @$ cs . get application attempt ( am2 . get application attempt id ( ) ) ) ; report nm1 = rm1 . get resource scheduler ( ) . get node report ( nm1 . get node id ( ) ) ; assert . assert equals ( __num__ * gb @$ report nm1 . get used resource ( ) . get memory size ( ) ) ; assert . assert equals ( __num__ * gb @$ report nm1 . get available resource ( ) . get memory size ( ) ) ;,nm do,success,pre
need to set nodelay or else batches larger than mtu can <PLACE_HOLDER> 40 ms nagling delays .,conf copy . set boolean ( common configuration keys public . ipc_client_tcpnodelay_key @$ true ) ; rpc . set protocol engine ( conf copy @$ q journal protocolpb . class @$ protobuf rpc engine . class ) ; return security util . do as login user ( new privileged exception action < q journal protocol > ( ) { @ override public q journal protocol run ( ) throws io exception { rpc . set protocol engine ( conf copy @$ q journal protocolpb . class @$ protobuf rpc engine . class ) ; q journal protocolpb pbproxy = rpc . get proxy ( q journal protocolpb . class @$ rpc . get protocol version ( q journal protocolpb . class ) @$ addr @$ conf copy ),need have,fail,pre
google analytics <PLACE_HOLDER> no grouping symbol,conversion meta . set grouping symbol ( null ) ;,analytics has,fail,pre
server did not <PLACE_HOLDER> the invalidation @$ so do n't leave an invalid entry here,if ( owner . get concurrency checks enabled ( ) && event . no version received from server ( ) ) { if ( is debug enabled ) { logger . debug ( __str__ @$ event ) ; } return false ; },server perform,success,pre
check that null pd array <PLACE_HOLDER> npe,try { new access control context ( null ) ; throw new exception ( __str__ ) ; } catch ( exception e ) { if ( ! ( e instanceof null pointer exception ) ) { throw new exception ( __str__ ) ; } },array throws,success,pre
the user <PLACE_HOLDER> a preferred handler set and it just became available @$ thus setting it as active,if ( ( configured handler != null ) && configured handler . equals ( handler . get class ( ) . get name ( ) ) ) { set active popup message handler ( handler ) ; },user specified,fail,pre
test fact <PLACE_HOLDER> collections,assert true ( fact handle == connected fact handle ) ; assert true ( ! ( fact handle == disconnected fact handle ) ) ;,fact handle,success,pre
the container <PLACE_HOLDER> its parent so we can use it orientation if it has one specified ; otherwise we prefer to use the orientation of its topmost child that has one specified and fall back on this container 's unset or unspecified value as a candidate if none of the children have a better candidate for the orientation .,if ( m orientation != screen_orientation_unset && m orientation != screen_orientation_unspecified ) { return m orientation ; } for ( int i = m children . size ( ) - __num__ ; i >= __num__ ; -- i ) { final window container wc = m children . get ( i ) ; final int orientation = wc . get orientation ( candidate == screen_orientation_behind ? screen_orientation_behind : screen_orientation_unset ) ; if ( orientation == screen_orientation_behind ) { candidate = orientation ; continue ; } if ( orientation == screen_orientation_unset ) { continue ; } if ( wc . fills parent ( ) || orientation != screen_orientation_unspecified ) { return orientation ; } },container has,fail,pre
the mediacodec process has changed @$ <PLACE_HOLDER> up the old pid and client before we boost the new process @$ so that the state is left <PLACE_HOLDER> if things go wrong .,m boosted pid = - __num__ ; if ( m client != null ) { try { m client . unlink to death ( m death recipient @$ __num__ ) ; } catch ( exception e ) { } finally { m client = null ; } } try { client . link to death ( m death recipient @$ __num__ ) ; log . i ( tag @$ __str__ + pid + __str__ + process . thread_group_top_app ) ; process . set process group ( pid @$ process . thread_group_top_app ) ; m boosted pid = pid ; m client = client ; return package manager . permission_granted ; } catch ( exception e ) { log . e ( tag @$ __str__ + e ) ;,state clean,success,pre
loose signatures <PLACE_HOLDER> a return type of object ...,return impl method . return type . equals ( __str__ ) || ( impl method . return type . equals ( __str__ ) && sdk method . return type . ends with ( __str__ ) ) ;,signatures require,fail,pre
simple counter does n't <PLACE_HOLDER> the increment after simple is removed from the composite,assert that ( simple counter . count ( ) ) . is equal to ( __num__ ) ; composite . add ( simple ) ; composite counter . increment ( ) ;,counter decrement,fail,pre
does offset <PLACE_HOLDER> buffer limit or negative ?,if ( offset > data . limit ( ) || offset < __num__ ) { throw new assertion error ( ) ; },offset exceed,success,pre
history guru constructor <PLACE_HOLDER> environment properties so no locking is done here .,history guru hist guru = history guru . get instance ( ) ;,constructor sets,fail,pre
the context could <PLACE_HOLDER> a reference to a wgl surface data @$ which in turn has a reference back to this wgl graphics config @$ so in order for this instance to be disposed we need to break the connection,ogl render queue rq = ogl render queue . get instance ( ) ; rq . lock ( ) ; try { ogl context . invalidate current context ( ) ; } finally { rq . unlock ( ) ; },context hold,success,pre
someone else wants to subscribe to our presence . <PLACE_HOLDER> user for approval,swing utilities . invoke later ( ( ) -> { try { subscription request ( presence . get from ( ) . as bare jid ( ) ) ; } catch ( smack exception . not connected exception | interrupted exception e ) { log . warning ( __str__ + presence . get from ( ) @$ e ) ; } } ) ; break ; case unsubscribe :,someone consume,fail,pre
do null byte <PLACE_HOLDER> test case,if ( ! null byte buffer test ( the mac ) ) { system . out . println ( __str__ ) ; return false ; } return true ;,byte buffer,success,pre
only play this sound if the feet <PLACE_HOLDER> the water,if ( event . get character relative position ( ) . y == __num__ && character sounds . last sound time + min_time < time . get game time in ms ( ) ) { boolean old block is liquid = event . get old block ( ) . is liquid ( ) ; boolean new block is liquid = event . get new block ( ) . is liquid ( ) ; static sound sound = null ; if ( ! old block is liquid && new block is liquid ) { sound = random . next item ( character sounds . enter water sounds ) ; } else if ( old block is liquid && ! new block is liquid ) { sound = random .,feet eliminate,fail,pre
use the derived <PLACE_HOLDER> total score as a side input,p collection < kv < string @$ integer > > filtered = sum scores . apply ( __str__ @$ par do . of ( new do fn < kv < string @$ integer > @$ kv < string @$ integer > > ( ) { private final counter num spammer users = metrics . counter ( __str__ @$ __str__ ) ; @ process element public void process element ( process context c ) { integer score = c . element ( ) . get value ( ) ; double gmc = c . side input ( global mean score ) ; if ( score > ( gmc * score_weight ) ) { log . info ( __str__ + c . element ( ) . get key ( ),derived dimensionless,fail,pre
begin pulse . note that it 's very important that the pulse finished callback be invoked when we 're done so that the caller can <PLACE_HOLDER> the pulse wakelock .,m pulse callback = callback ; m pulse reason = reason ;,caller drop,success,pre
check if listener @$ defined by classname @$ <PLACE_HOLDER> all events,list < activiti event > events = static test activiti event listener . get events received ( ) ; assert false ( events . is empty ( ) ) ; boolean insert found = false ; boolean delete found = false ; for ( activiti event e : events ) { if ( activiti event type . entity_created == e . get type ( ) ) { insert found = true ; } else if ( activiti event type . entity_deleted == e . get type ( ) ) { delete found = true ; } } assert true ( insert found ) ; assert true ( delete found ) ;,listener received,success,pre
exception should <PLACE_HOLDER> a report,system . err . println ( ) ;,exception provide,fail,pre
which otherwise can <PLACE_HOLDER> problems with adding multiple entries to an ordered linked hash map,return new server web exchange matcher ( ) { @ override public mono < match result > matches ( server web exchange exchange ) { return server web exchange matcher . match result . match ( ) ; } } ;,which cause,success,pre
one observer is the use case group . the other observer <PLACE_HOLDER> the use case upon the lifecycle 's destruction .,assert that ( m lifecycle . get observer count ( ) ) . is equal to ( __num__ ) ;,observer removes,success,pre
start playback and wait until playback <PLACE_HOLDER> second window .,action schedule action schedule = new action schedule . builder ( __str__ ) . pause ( ) . seek ( __num__ ) . wait for seek processed ( ) . seek ( __num__ ) . seek ( __num__ ) . wait for playback state ( player . state_ready ) . seek ( __num__ ) . play until start of window ( __num__ ) . seek ( __num__ ) . seek ( __num__ ) . play ( ) . build ( ) ;,playback leaves,fail,pre
it makes some sense to return true @$ as no filter implies all shall <PLACE_HOLDER> the filter @$ but if this returns true @$ then any other filters can be used as the source of the data to filter @$ which does n't make sense if this is meant to only be used by itself .,return false ;,all pass,success,pre
unknown method @$ let the delegate <PLACE_HOLDER> a usual error .,if ( method == null ) { return delegate ( ) . serve ( ctx @$ req ) ; },delegate report,fail,pre
the owning rule 's divisor <PLACE_HOLDER> the behavior of this substitution : rather than keeping a backpointer to the rule @$ we keep a copy of the divisor,this . divisor = rule . get divisor ( ) ; if ( divisor == __num__ ) { throw new illegal state exception ( __str__ + divisor + __str__ + description . substring ( __num__ @$ pos ) + __str__ + description . substring ( pos ) ) ; },divisor determines,fail,pre
if mode is add then check if the repository name does not <PLACE_HOLDER> in the repository list then close this dialog if mode is edit then check if the repository name is the same as before if not check if the new name does not <PLACE_HOLDER> in the repository . otherwise return true to this method @$ which will mean that repository already <PLACE_HOLDER>,if ( meta . get name ( ) != null ) { if ( mode == mode . add ) { if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ; } else { display repository already exist message ( meta . get name ( ) ) ; } } else { if ( master repository name . equals ( meta . get name ( ) ) ) { repository meta = meta ; hide ( ) ; } else if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ;,repository exist,success,pre
this is the first thread which <PLACE_HOLDER> up the addresses this host or the cache entry for this host has been expired so this thread should do the lookup .,try { for ( name service name service : name services ) { try { addresses = name service . lookup all host addr ( host ) ; success = true ; break ; } catch ( unknown host exception uhe ) { if ( host . equals ignore case ( __str__ ) ) { inet address [ ] local = new inet address [ ] { impl . loopback address ( ) } ; addresses = local ; success = true ; break ; } else { addresses = unknown_array ; success = false ; ex = uhe ; } } } if ( req addr != null && addresses . length > __num__ && ! addresses [ __num__ ] . equals ( req addr ) ),which picks,fail,pre
j label <PLACE_HOLDER> html text,if ( at != null ) { return at . get before index ( part @$ index ) ; },label contains,success,pre
assert that only elements in the specified bucket <PLACE_HOLDER> up @$ and each element <PLACE_HOLDER> up 3 times .,int bucket count = __num__ ; set < long > expected ids = long stream . range ( __num__ @$ row count ) . filter ( x -> bucket ids . contains ( to int exact ( x % bucket count ) ) ) . boxed ( ) . collect ( to immutable set ( ) ) ;,elements shows,success,pre
we can inherit parent key serde if user do not <PLACE_HOLDER> specific overrides,return new k table impl < k @$ change < vr > @$ vr > ( k tablek table join node . node name ( ) @$ k tablek table join node . key serde ( ) @$ k tablek table join node . value serde ( ) @$ all source nodes @$ k tablek table join node . queryable store name ( ) @$ k tablek table join node . join merger ( ) @$ k tablek table join node @$ builder ) ;,user provide,success,pre
it is strange that an operation set presence does not <PLACE_HOLDER> a presence status so it may be safer to not mess with it .,forget presence status ( pps ) ; presence . remove provider presence status listener ( presence status listener ) ;,presence have,success,pre
confirm that peer with state a will <PLACE_HOLDER> replication request .,verify replication request rejection ( util1 @$ true ) ; verify replication request rejection ( util2 @$ false ) ; util1 . get admin ( ) . disable replication peer ( peer_id ) ; write ( util1 @$ __num__ @$ __num__ ) ; thread . sleep ( __num__ ) ;,peer reject,success,pre
top <PLACE_HOLDER> bottom <PLACE_HOLDER> bottom right top right,top card . set card vertices ( new float [ ] { __num__ @$ bitmap . get height ( ) @$ __num__ @$ __num__ @$ bitmap . get height ( ) / __num__ @$ __num__ @$ bitmap . get width ( ) @$ bitmap . get height ( ) / __num__ @$ __num__ @$ bitmap . get width ( ) @$ bitmap . get height ( ) @$ __num__ } ) ;,bottom left,success,pre
the current system does not <PLACE_HOLDER> our needs to disconnect and call recursively to get a new system .,if ( need new system ) { get log writer ( ) . info ( __str__ ) ; disconnect fromds ( ) ; get system ( props ) ; },system have,fail,pre
to prevent concurrent remove <PLACE_HOLDER> bug 41646,synchronized ( this . overflow map ) { overflow oplog oplog = get child ( ( int ) oplog id ) ; if ( oplog != null ) { oplog . remove ( dr @$ entry ) ; } },remove see,success,pre
test pascal 255 <PLACE_HOLDER> the fourth row,select rows ( table @$ addr ( __num__ ) ) ; selected row = table . get selected row ( ) ; perform action ( make string action @$ model ) ;,255 select,success,pre
throws a class not <PLACE_HOLDER> exception if not <PLACE_HOLDER> .,class < ? > cls = class . for name ( ci class ) ;,throws found,success,pre
ensure that the rvv has <PLACE_HOLDER> the event,distributed region r = ( distributed region ) get cache ( ) . get region ( region name ) ; if ( ! r . get version vector ( ) . contains ( xid @$ __num__ ) ) { get log writer ( ) . info ( __str__ + r . get version vector ( ) . full to string ( ) ) ; ( ( local region ) r ) . dump backing map ( ) ; } assert true ( r . contains key ( __str__ ) ) ;,rvv received,fail,pre
make a third request to be sure the proxy does not <PLACE_HOLDER> cookies,content response response3 = client . new request ( __str__ @$ server connector . get local port ( ) ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( __num__ @$ response3 . get status ( ) ) ; assert true ( response3 . get headers ( ) . contains key ( proxied_header ) ) ; client2 . stop ( ) ;,proxy have,fail,pre
media controller.set playlist does not <PLACE_HOLDER> the equality of the items .,for ( int i = __num__ ; i < list . size ( ) ; i ++ ) { assert equals ( list . get ( i ) @$ m player . m playlist . get ( i ) . get media id ( ) ) ; },media ensure,success,pre
filter <PLACE_HOLDER> @$ injection <PLACE_HOLDER> @$ exception not expected,run test ( true @$ true ) ;,injection enabled,success,pre
set asynchronous mode @$ since async caller <PLACE_HOLDER> thread,client . set asynchronous mode ( true ) ;,caller hits,fail,pre
in case of location shared sd <PLACE_HOLDER> the base location and partition <PLACE_HOLDER> relative location,for ( partition orig part : orig partitions ) { assert . assert equals ( __str__ @$ orig part . get sd ( ) . get location ( ) @$ sharedsd . get location ( ) + partition withoutsds . get ( i ) . get relative path ( ) ) ; assert . assert null ( __str__ @$ partition withoutsds . get ( i ) . get values ( ) ) ; assert . assert null ( __str__ @$ partition withoutsds . get ( i ) . get parameters ( ) ) ; i ++ ; },location has,success,pre
verify quorum entry <PLACE_HOLDER> active membership,membership state quorum entry = get namenode registration ( record . get nameservice id ( ) @$ record . get namenode id ( ) ) ; assert not null ( quorum entry ) ; assert equals ( routers [ __num__ ] @$ quorum entry . get router id ( ) ) ;,entry has,fail,pre
behind the scenes the server is <PLACE_HOLDER> data in a file @$ we read it directly here,file file = new file ( __str__ + id + __str__ ) ; return file utils . read file to string ( file @$ __str__ ) ;,server writing,fail,pre
if locality is enabled @$ then <PLACE_HOLDER> tablet location,for ( range range : split ranges ) { if ( fetch tablet locations ) { tablet splits . add ( new tablet split metadata ( get tablet location ( table name @$ range . get start key ( ) ) @$ immutable list . of ( range ) ) ) ; } else { tablet splits . add ( new tablet split metadata ( optional . empty ( ) @$ immutable list . of ( range ) ) ) ; } },then fetch,success,pre
create a binder that will let the activity ui <PLACE_HOLDER> commands to the service,mqtt service binder = new mqtt service binder ( this ) ;,ui handle,fail,pre
we are creating filter here so should not be <PLACE_HOLDER> null . not sure why calcite return null,rex node b = builder . literal ( true ) ; switch ( logic ) { case true_false_unknown : b = e . rel . get cluster ( ) . get rex builder ( ) . make null literal ( sql type name . boolean ) ; case unknown_as_true : operands . add ( builder . call ( sql std operator table . less_than @$ builder . field ( __str__ @$ __str__ ) @$ builder . field ( __str__ @$ __str__ ) ) @$ b ) ; break ; },filter using,fail,pre
r and s each <PLACE_HOLDER> half the array,byte [ ] res = new byte [ k << __num__ ] ; system . arraycopy ( br @$ __num__ @$ res @$ k - br . length @$ br . length ) ; system . arraycopy ( bs @$ __num__ @$ res @$ res . length - bs . length @$ bs . length ) ; return res ;,each occupy,success,pre
the new reference should <PLACE_HOLDER> the cloned region name as parent @$ if it is a clone .,string cloned region name = bytes . to string ( regions map . get ( bytes . to bytes ( snapshot region name ) ) ) ; if ( cloned region name == null ) cloned region name = snapshot region name ;,reference have,success,pre
return this instance of local service so clients can <PLACE_HOLDER> public methods .,return local service . this ;,clients call,success,pre
this call could <PLACE_HOLDER> node @$ we should keep the message tight,return volume state . node_ready ;,call remove,fail,pre
ensure sub visible to any new dest <PLACE_HOLDER> subscriptions for destination,subscriptions . put ( info . get consumer id ( ) @$ sub ) ; destinations lock . read lock ( ) . unlock ( ) ;,sub create,fail,pre
now try filling with black again @$ but it will come up as white because this fill rect wo n't <PLACE_HOLDER> the color properly,g . set color ( color . black ) ; g . fill rect ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; buffered image bi = vi . get snapshot ( ) ; if ( bi . getrgb ( __num__ @$ __num__ ) != color . black . getrgb ( ) ) { throw new runtime exception ( __str__ + integer . to hex string ( bi . getrgb ( __num__ @$ __num__ ) ) + __str__ + integer . to hex string ( color . black . getrgb ( ) ) ) ; } system . out . println ( __str__ ) ;,rect change,fail,pre
verify that the database connection being set to null <PLACE_HOLDER> a kettle exception with the following message .,try { ora bulk loader . verify database connection ( ) ; fail ( __str__ ) ; } catch ( kettle exception a kettle exception ) { assert that ( a kettle exception . get message ( ) @$ contains string ( __str__ ) ) ; },connection throws,success,pre
low ack should <PLACE_HOLDER> no effect .,s . remote ack ( __num__ ) ; sz = s . size in bytes ( ) ; assert equals ( __num__ @$ sz ) ; listing = get sorted directory listing segments ( ) ; assert equals ( listing . size ( ) @$ __num__ ) ; s . become leader ( ) ; wait for master ( s ) ;,ack have,success,pre
runtime exceptions <PLACE_HOLDER> here bubble up and are handled in replication source,pool . submit ( create replicator ( entries @$ i @$ replicate context . get timeout ( ) ) ) ; futures ++ ;,exceptions thrown,fail,pre
nm 2 <PLACE_HOLDER> couple of heartbeats,rm node rm node2 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm2 . get node id ( ) ) ; scheduler node scheduler node2 = cs . get scheduler node ( nm2 . get node id ( ) ) ; cs . handle ( new node update scheduler event ( rm node2 ) ) ;,nm do,success,pre
when test <PLACE_HOLDER> unfinished verification,verify ( mock ) ;,test finishes,fail,pre
set the uid range for this request to the single uid of the requester @$ or to an empty set of ui ds if the caller <PLACE_HOLDER> the appropriate permission and ui ds have not been set . this will overwrite any allowed ui ds in the requested capabilities . though there are no visible methods to set the ui ds @$ an app,restrict request uids for caller ( network capabilities ) ; if ( timeout ms < __num__ ) { throw new illegal argument exception ( __str__ ) ; } ensure valid ( network capabilities ) ; network request network request = new network request ( network capabilities @$ legacy type @$ next network request id ( ) @$ type ) ; network request info nri = new network request info ( messenger @$ network request @$ binder ) ; if ( dbg ) log ( __str__ + nri ) ; m handler . send message ( m handler . obtain message ( event_register_network_request @$ nri ) ) ; if ( timeout ms > __num__ ) { m handler . send message delayed ( m handler . obtain message (,caller has,success,pre
this is used primarily for gtk l & f @$ which <PLACE_HOLDER> the thumb to fit the track when it would otherwise be hidden .,if ( ui manager . get boolean ( __str__ ) ) { set thumb bounds ( itemx @$ itracky @$ itemw @$ itrackh ) ; } else { set thumb bounds ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; },which expands,success,pre
special case @$ a collection with only a <PLACE_HOLDER> method we assume we can just add to the connection,if ( collection . class . is assignable from ( i . get property type ( ) ) ) { handled properties . add ( i . get name ( ) ) ; collection property value = ( collection ) i . read ( param ) ; if ( ! property value . is empty ( ) ) { list < deferred parameter > params = new array list < > ( ) ; for ( object c : property value ) { deferred parameter to add = load object instance ( c @$ existing @$ object . class ) ; params . add ( to add ) ; } setup steps . add ( new serialzation step ( ) { @ override public void handle ( method,collection read,success,pre
if there is duplicate key @$ rollback previous <PLACE_HOLDER> operation,if ( old value != null ) { map . put ( key @$ old value ) ; throw duplicate key exception ( key @$ old value @$ value ) ; },rollback write,fail,pre
if the source does not <PLACE_HOLDER> a checkpoint mark skip updating the state .,@ suppress warnings ( __str__ ) final checkpoint markt finished read checkpoint mark = ( checkpoint markt ) microbatch reader . get checkpoint mark ( ) ; byte [ ] coded checkpoint = coder helpers . to byte array ( finished read checkpoint mark @$ checkpoint coder ) ;,source contain,fail,pre
there is no presence op set . let 's <PLACE_HOLDER> the connected cusax provider if available,if ( presence == null ) { operation set cusax utils cusax op set = provider . get operation set ( operation set cusax utils . class ) ; if ( cusax op set != null ) { protocol provider service linked cusax provider = cusax op set . get linked cusax provider ( ) ; if ( linked cusax provider != null ) { presence = linked cusax provider . get operation set ( operation set presence . class ) ; } } },let use,fail,pre
the new bounds should <PLACE_HOLDER> the new type variables in place of the old,list < type > new bounds = new bounds buf . to list ( ) ; from = tvars ; to = new tvars . to list ( ) ; for ( ; ! new bounds . is empty ( ) ; new bounds = new bounds . tail ) { new bounds . head = subst ( new bounds . head @$ from @$ to ) ; } new bounds = new bounds buf . to list ( ) ;,bounds use,success,pre
the current implementation <PLACE_HOLDER> the component 's scoped context .,layout = layout state . create layout ( next . get scoped context ( ) @$ next @$ false ) ;,implementation accesses,fail,pre
check snapshot copy of meta change <PLACE_HOLDER> 1,i node file meta change file1s copy = child . as file ( ) ; assert true ( meta change file1s copy . is with snapshot ( ) ) ; assert false ( meta change file1s copy . is under construction ( ) ) ; assert equals ( replication_1 @$ meta change file1s copy . get file replication ( snapshot . current_state_id ) ) ; assert equals ( replication_1 @$ meta change file1s copy . get file replication ( snapshot1 . get id ( ) ) ) ; assert equals ( replication @$ meta change file1s copy . get file replication ( snapshot0 . get id ( ) ) ) ;,copy file,success,pre
and <PLACE_HOLDER> the future for examination :,return create constraint transaction ;,and accept,fail,pre
error will <PLACE_HOLDER> exception other than security exception,system . out . println ( __str__ ) ;,error throw,success,pre
second frame <PLACE_HOLDER> the next chunk of data,msg = zmq . recv ( stream @$ __num__ ) ; assert that ( msg @$ not null value ( ) ) ; bytes read += msg . size ( ) ; read . put ( msg . buf ( ) ) ;,frame contains,success,pre
base invalidate operation is treated as destroy . when the invalidate comes through @$ the entry will no longer <PLACE_HOLDER> the query and will need to be deleted .,if ( b_cq results_old value ) { cq event = message_type_local_destroy ; c query . mark as destroyed in cq result keys ( event key ) ; },entry have,fail,pre
special case for bare arrays @$ <PLACE_HOLDER> the name of the param to the name of the complex type .,if ( op . is array ( ) && _parameter style == wsdl operation . soap parameter style . bare ) { op . set name ( schema element . get attribute ( wsdl utils . name_attr ) @$ _wsdl types ) ; } resolved params . add ( op ) ;,case change,success,pre
first call should <PLACE_HOLDER> empty @$ second call after network should <PLACE_HOLDER> the network value,when ( persister . get record state ( bar code ) ) . then return ( record state . missing ) ; when ( persister . write ( bar code @$ network1 ) ) . then return ( single . just ( true ) ) ; store . get ( bar code ) . test ( ) . assert error ( sorry ) ; in order in order = in order ( fetcher @$ persister ) ; in order . verify ( persister @$ times ( __num__ ) ) . read ( bar code ) ; in order . verify ( fetcher @$ times ( __num__ ) ) . fetch ( bar code ) ; in order . verify ( persister @$ times ( __num__ ) ) .,call return,success,pre
return this instance of my service so clients can <PLACE_HOLDER> public methods,return media player service . this ;,clients call,success,pre
we can know the size of the iterable . <PLACE_HOLDER> an encoding with a leading size field @$ followed by that many elements .,if ( iterable instanceof collection ) { collection < t > collection = ( collection < t > ) iterable ; observer . update ( __num__ ) ; for ( t elem : collection ) { element coder . register byte size observer ( elem @$ observer ) ; } } else { observer . update ( __num__ ) ; long count = __num__ ; for ( t elem : iterable ) { count += __num__ ; element coder . register byte size observer ( elem @$ observer ) ; } if ( count > __num__ ) { observer . update ( var int . get length ( count ) ) ; } observer . update ( __num__ ) ; },size use,success,pre
alternate 2 : rename across mountpoints with same target . i.e . rename across alias mountpoints . note we compare the ur is . the ur is <PLACE_HOLDER> the link targets . hence we allow renames across mount links as long as the mount links point to the same target .,if ( ! src uri . equals ( dst uri ) ) { throw new io exception ( __str__ ) ; } break ; case same_mountpoint :,ur equals,fail,pre
if <PLACE_HOLDER> function invoke before start function @$ then update topic <PLACE_HOLDER> info after initialization .,if ( subscription type == subscription type . subscribe ) { update topic subscribe info when subscription changed ( ) ; },topic subscribe,success,pre
postgre sql does not <PLACE_HOLDER> zone @$ only the point in time,return new timestamp ( millis utc ) ;,sql store,success,pre
we used to record use of unsupported class loaders @$ but we no longer do . <PLACE_HOLDER> such entries ; they will be deleted when we next write the file .,if ( unsupported_class_loader_context . equals ( class loader context ) ) { continue ; },use remove,fail,pre
first call should <PLACE_HOLDER> the wire,try { groups . get groups ( __str__ ) ; fail ( __str__ ) ; } catch ( io exception e ) { },call reach,fail,pre
we only generate merkle tree once here @$ so it 's important to make sure the root hash <PLACE_HOLDER> the signed one in the apk .,if ( ! arrays . equals ( expected root hash @$ generated root hash ) ) { throw new security exception ( __str__ + bytes to string ( generated root hash ) + __str__ + bytes to string ( expected root hash ) ) ; } int content size = shm buffer factory . get buffer limit ( ) ; shared memory shm = shm buffer factory . release shared memory ( ) ; if ( shm == null ) { throw new illegal state exception ( __str__ ) ; } if ( ! shm . set protect ( os constants . prot_read ) ) { throw new security exception ( __str__ ) ; } return pair . create ( shm @$ content size ) ;,hash matches,success,pre
client <PLACE_HOLDER> connection via echo,confirm connection ( client socket @$ client connect future ) ; try { client socket . get session ( ) . get remote ( ) . send string ( __str__ ) ; end point endp = client socket . get end point ( ) ; endp . shutdown output ( ) ; final string orig close reason = __str__ ; client socket . get session ( ) . close ( status code . normal @$ orig close reason ) ; assert that ( __str__ @$ client socket . error latch . await ( __num__ @$ seconds ) @$ is ( true ) ) ; assert that ( __str__ @$ client socket . error . get ( ) @$ instance of ( eof exception . class ) ) ;,client confirms,success,pre
default payment api will <PLACE_HOLDER> the default configured ones to this list,return payment control plugin names ;,api add,success,pre
check if the table file <PLACE_HOLDER> header to skip .,if ( this . io cxt ref . get current block start ( ) == __num__ ) { footer buffer = null ; path file path = this . io cxt ref . get input path ( ) ; partition desc part = null ; try { if ( path to partition info == null ) { path to partition info = utilities . get map work ( job conf ) . get path to partition info ( ) ; } part = hive file format utils . get from path recursively ( path to partition info @$ file path @$ io prepare cache . get ( ) . get partition desc map ( ) ) ; } catch ( assertion error ae ) { log . info,file contains,fail,pre
global map of all <PLACE_HOLDER> distinct words : thus enable reuse of the same word string instance appearing multiple times in different synonyms sets,final map < string @$ string > distinct words = new hash map < > ( ) ; for ( final string f : files ) { file ff = new file ( path @$ f ) ; string line ; try { blocking queue < string > list = files . concurent line reader ( ff ) ; while ( ( line = list . take ( ) ) != files . poison_line ) { line = line . trim ( ) ; if ( line . length ( ) == __num__ || line . char at ( __num__ ) == __str__ ) continue ; if ( line . char at ( line . length ( ) - __num__ ) == __str__ ) line = line .,map mapped,fail,pre
java <PLACE_HOLDER> off manual token file,g . add edge ( __str__ @$ __str__ ) ;,java k,fail,pre
endpoints do not directly forward <PLACE_HOLDER> chunks to next stage in the filter chain .,if ( new chunk != null ) { zuul req . buffer body contents ( new chunk ) ; if ( new chunk != chunk ) { chunk . release ( ) ; } if ( is filter awaiting body ( zuul req ) && zuul req . has complete body ( ) && ! ( endpoint instanceof proxy endpoint ) ) { invoke next stage ( filter ( endpoint @$ zuul req ) ) ; } },endpoints buffered,fail,pre
reset <PLACE_HOLDER> result .,standalone test strategy . posted result = null ; when ( spawn action context . begin execution ( any ( ) @$ any ( ) ) ) . then ( ( invocation ) -> { file system utils . touch file ( actionb . resolve ( get exec root ( ) ) . get xml output path ( ) ) ; return spawn continuation . failed with exec exception ( new spawn exec exception ( __str__ @$ expected spawn result @$ false ) ) ; } ) ;,reset posted,success,pre
this event <PLACE_HOLDER> a response @$ so add that expected response to the maps of pending events .,if ( request response pairs . contains key ( event . event id ) ) { for ( event manager . timed event pair p : request response pairs . get ( event . event id ) ) { pending responses . put ( p . m response @$ new pending response ( event . event id @$ event . time @$ p . m timeout millis @$ p . m name ) ) ; } },event includes,fail,pre
should use <PLACE_HOLDER> entity transformer by default,hql executor hql executor unaliased = new hql executor ( ) { protected query get query ( session s ) { return s . create query ( __str__ ) ; } } ;,use root,success,pre
the table metadata <PLACE_HOLDER> less or more columns than the event @$ which means the table structure <PLACE_HOLDER> changed @$ so we need to trigger a refresh ...,if ( msg has missing columns || msg has additional columns ) { logger . info ( __str__ @$ replication column count @$ table column count ) ; return true ; },metadata has,success,pre
finishing the tasks should also <PLACE_HOLDER> the end time,tasks = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . list ( ) ; assert equals ( __num__ @$ tasks . size ( ) ) ; for ( task task : tasks ) { task service . complete ( task . get id ( ) ) ; } historic activity instances = history service . create historic activity instance query ( ) . activity id ( __str__ ) . list ( ) ; assert equals ( __num__ @$ historic activity instances . size ( ) ) ; for ( historic activity instance historic activity instance : historic activity instances ) { assert not null ( historic activity instance . get end time ( ),tasks set,success,pre
make sure that container is in <PLACE_HOLDER> state before sending increase request,nm1 . node heartbeat ( container id . get application attempt id ( ) @$ container id . get container id ( ) @$ container state . running ) ; rm1 . drain events ( ) ; am client . request container update ( container @$ update container request . new instance ( container . get version ( ) @$ container . get id ( ) @$ container update type . increase_resource @$ resource . new instance ( __num__ @$ __num__ ) @$ null ) ) ; it . remove ( ) ; allocate response = am client . allocate ( __num__ ) ; rm1 . drain events ( ) ; assert . assert equals ( __str__ @$ __num__ @$ allocate response . get allocated containers ( ),container running,success,pre
custom <PLACE_HOLDER> target task ids will be null if this stream is not custom stream <PLACE_HOLDER>,if ( ! helper . is custom grouping empty ( ) ) { custom grouping target task ids = helper . choose tasks for custom stream grouping ( stream id @$ tuple ) ; },stream aggregated,fail,pre
cui does n't <PLACE_HOLDER> mini progress bars right now,painter . state . set ( size variant = size == size . mini ? size . small : size variant ) ;,cui support,success,pre
0 x 1002346 : p 1 repeatable comment <PLACE_HOLDER> p 2 repeatable comment .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ;,comment contains,success,pre
specialized entry point for zero argument <PLACE_HOLDER> calls,if ( arguments . length == __num__ ) { impl . execute void ( receiver ) ; } else { impl . execute void ( receiver @$ arguments ) ; },point execute,success,pre
if the caller does not <PLACE_HOLDER> permission to load the driver then skip it .,for ( driver info a driver : registered drivers ) { if ( is driver allowed ( a driver . driver @$ caller class loader ) ) { try { if ( a driver . driver . acceptsurl ( url ) ) { println ( __str__ + a driver . driver . get class ( ) . get name ( ) ) ; return ( a driver . driver ) ; } } catch ( sql exception sqe ) { } } else { println ( __str__ + a driver . driver . get class ( ) . get name ( ) ) ; } },caller have,success,pre
app 4 ca n't allocate its am container on node 1 because app 3 already <PLACE_HOLDER> its container on node 1 .,scheduler . handle ( updatee1 ) ; assert equals ( __str__ @$ __num__ @$ app4 . getam resource ( ) . get memory size ( ) ) ; assert equals ( __str__ @$ __num__ @$ app4 . get live containers ( ) . size ( ) ) ; assert equals ( __str__ @$ __num__ @$ queue1 . get am resource usage ( ) . get memory size ( ) ) ; scheduler . update ( ) ;,app reserved,success,pre
initialize state trackers for all <PLACE_HOLDER> tasks .,this . partial map progress = new float [ num maps ] ; this . map counters = new counters [ num maps ] ; for ( int i = __num__ ; i < num maps ; i ++ ) { this . map counters [ i ] = new counters ( ) ; } this . partial reduce progress = new float [ num reduces ] ; this . reduce counters = new counters [ num reduces ] ; for ( int i = __num__ ; i < num reduces ; i ++ ) { this . reduce counters [ i ] = new counters ( ) ; } this . num map tasks = num maps ; this . num reduce tasks = num reduces ;,trackers reduce,fail,pre
since filenames <PLACE_HOLDER> valuable information @$ split the string right before the filename and truncate both halves .,final int last comp = filename . last index of ( sep ) ; final int split len = maxlen - ( len - last comp ) ; final int split pos = ( split len / __num__ ) - ( ell len / __num__ ) ; if ( split pos > __num__ ) { result . append ( filename . substring ( __num__ @$ ( split pos - ( ell len % __num__ ) ) + ( split len % __num__ ) ) ) ; result . append ( ellipsis ) ; result . append ( filename . substring ( ( last comp - split pos ) + ( ell len % __num__ ) ) ) ; } else { result . append ( filename . substring,filenames contain,success,pre
period <PLACE_HOLDER> special semantics,if ( s instanceof period ) { return new comparable period ( ( period ) s ) ; } else { throw new illegal argument exception ( __str__ + s + __str__ ) ; },period has,success,pre
loop while other threads control the lock grantor future result <PLACE_HOLDER> loop if other thread has already made us lock grantor <PLACE_HOLDER> loop if this thread gets control of lock grantor future result,while ( ! own lock grantor future result ) { assert . assert holds lock ( this . destroy lock @$ false ) ; synchronized ( this . lock grantor id lock ) { if ( is currently or is making lock grantor ( ) ) { return ; } else if ( this . lock grantor future result != null ) { lock grantor future result ref = this . lock grantor future result ; } else { own lock grantor future result = true ; lock grantor future result ref = new future result ( this . dm . get cancel criterion ( ) ) ; if ( is debug enabled_dls ) { logger . trace ( log marker . dls_verbose @$ __str__ ) ; },grantor terminate,success,pre
let 's <PLACE_HOLDER> charles a promotion @$ this time using method references,charles . set oca ( __num__ ) ; charles . set title ( __str__ ) ; check dirty tracking ( charles @$ __str__ @$ __str__ ) ;,'s give,success,pre
serialize and deserialize unconfigured <PLACE_HOLDER> target as string,simple module build target module = new simple module ( __str__ ) ; build target module . add serializer ( unconfigured build target . class @$ new to string serializer ( ) ) ; build target module . add deserializer ( unconfigured build target . class @$ new from string deserializer < unconfigured build target > ( unconfigured build target . class ) { @ override protected unconfigured build target _deserialize ( string value @$ deserialization context ctxt ) { return unconfigured build target parser . parse ( value @$ intern ) ; } } ) ; mapper . register module ( build target module ) ; mapper . register module ( forward relative path module ( ) ) ; return mapper ;,unconfigured build,success,pre
we also need to let the framework know what type of event happened . accessibility services may <PLACE_HOLDER> this event to provide appropriate feedback to the <PLACE_HOLDER>r .,m touch helper . send event for virtual view ( index @$ accessibility event . type_view_clicked ) ; return true ;,services use,success,pre
only the good ones will be run and the latest does n't <PLACE_HOLDER> hung because of the second,queue task future [ ] task future = new queue task future [ __num__ ] ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ;,latest have,fail,pre
client 1 is attached to bridge server data store 3 client 1 does not <PLACE_HOLDER> interest but <PLACE_HOLDER>s cq,client1 . invoke ( ( ) -> create client cache ( port3 @$ false @$ false @$ true ) ) ;,1 have,fail,pre
boundary symbol <PLACE_HOLDER> one tagging,rules with word [ boundary word id ] . add ( new int tagged word ( boundary word id @$ boundary tag id ) ) ;,symbol has,success,pre
a short string referenced once should not <PLACE_HOLDER> a temp .,test needed temps ( __str__ @$ __str__ @$ empty_string_set ) ;,referenced have,success,pre
transfer send some asset issue to default account @$ to test if this transaction <PLACE_HOLDER> the creator net .,assert . assert true ( public methed . transfer asset ( to address @$ asset account id . to byte array ( ) @$ __num__ @$ transfer asset address @$ transfer asset create key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ; asset creator net = public methed . get account net ( asset012 address @$ blocking stub full ) ; asset transfer net = public methed . get account net ( transfer asset address @$ blocking stub full ) ; long creator after net used = asset creator net . get net used ( ) ; long transfer after free net used = asset transfer net . get free net used ( ) ; logger,transaction create,fail,pre
set the input to the script of its output . bitcoin core does this but the step has no obvious purpose as the signature covers the hash of the prevout transaction which obviously <PLACE_HOLDER> the output script already . perhaps it felt safer to him in some way @$ or is another leftover from how the code was written .,transaction input input = tx . inputs . get ( input index ) ; input . set script bytes ( connected script ) ; if ( ( sig hash type & __num__ ) == sig hash . none . value ) { tx . outputs = new array list < > ( __num__ ) ; for ( int i = __num__ ; i < tx . inputs . size ( ) ; i ++ ) if ( i != input index ) tx . inputs . get ( i ) . set sequence number ( __num__ ) ; } else if ( ( sig hash type & __num__ ) == sig hash . single . value ) { if ( input index >= tx . outputs . size,which contains,fail,pre
evaluate if the value <PLACE_HOLDER> the search criteria,if ( string utils . contains ignore case ( value @$ search str ) ) { matches . add ( __str__ + descriptor . get name ( ) + __str__ + value ) ; },value matches,success,pre
manually specified value <PLACE_HOLDER> precedence over settings .,set enabled ( system properties . get boolean ( debug_sys_looper_stats_enabled @$ parser . get boolean ( settings_enabled_key @$ default_enabled ) ) ) ;,value takes,success,pre
cleanup all <PLACE_HOLDER> resources .,for ( int i = __num__ ; i < num_workers ; i ++ ) { workers [ i ] . close ( ) ; } master session . close ( ) ; connection . close ( ) ;,cleanup lists,fail,pre
here we expected that id 1 and id 2 will be reusable @$ even if they were n't marked as such in the previous session making changes to the tree entry where they live will <PLACE_HOLDER> the generation and all of a sudden the reusable bits in that entry will matter when we want to allocate . this is why we now want to,mark deleted ( freelist @$ id3 ) ; final immutable long set reused = long sets . immutable . of ( freelist . next id ( ) @$ freelist . next id ( ) ) ; assert equals ( long sets . immutable . of ( id1 @$ id2 ) @$ reused @$ __str__ ) ;,bits fail,fail,pre
the final orientation of this activity will change after moving to full screen . <PLACE_HOLDER> freezing screen here to prevent showing a temporary full screen window .,if ( top != null && ! top . is configuration compatible ( parent config ) ) { top . start freezing screen locked ( top . app @$ config_screen_layout ) ; m service . move tasks to fullscreen stack ( m stack id @$ true ) ; return ; },orientation start,success,pre
setup an injection dependency to inject the default access timeout service in the singleton bean component <PLACE_HOLDER> service,configuration . get create dependencies ( ) . add ( new dependency configurator < singleton component create service > ( ) { @ override public void configure dependency ( service builder < ? > service builder @$ singleton component create service component create service ) throws deployment unit processing exception { service builder . add dependency ( default access timeout service . singleton_service_name @$ default access timeout service . class @$ component create service . get default access timeout injector ( ) ) ; } } ) ; return new singleton component create service ( configuration @$ this . ejb jar configuration @$ this . init on startup @$ depends on ) ;,service create,success,pre
enter a synthetic class that is used to provide profile info for classes in ct.sym . this class does not <PLACE_HOLDER> a class file .,profile type = enter synthetic annotation ( __str__ ) ; method symbol m = new method symbol ( public | abstract @$ names . value @$ int type @$ profile type . tsym ) ; profile type . tsym . members ( ) . enter ( m ) ;,class reference,fail,pre
see if our node <PLACE_HOLDER> the given key declaration according to the match attribute on xsl : key .,x path match expr = kd . get match ( ) ; double score = match expr . get match score ( xctxt @$ test node ) ; if ( score == kd . get match ( ) . match_score_none ) continue ; return dtm iterator . filter_accept ;,node matches,success,pre
device does n't <PLACE_HOLDER> nap @$ so remove pan profile on disconnect,if ( profile instanceof pan profile && ( ( pan profile ) profile ) . is local role nap ( m device ) ) { m local nap role connected = true ; },device support,success,pre
if this mime type <PLACE_HOLDER> an extension @$ customize the label,final char sequence label ; final string ext = mime map . get default ( ) . guess extension from mime type ( mime type ) ; if ( ! text utils . is empty ( ext ) && ext label id != - __num__ ) { label = res . get string ( ext label id @$ ext . to upper case ( locale . us ) ) ; } else { label = res . get string ( label id ) ; } return new mime type info ( icon . create with resource ( res @$ icon id ) @$ label @$ label ) ;,type has,success,pre
note on catch : dom level 1 does not specify this method and the code will <PLACE_HOLDER> a no such method error,if ( doctype != null ) { try { return doctype . get system id ( ) ; } catch ( error except ) { } },code throw,success,pre
but the write entity is complete in ddl operations @$ instead ddl <PLACE_HOLDER> the write type @$ so we use it to determine its lock mode @$ and first we check if the write type was set,write entity . write type write type = we . get write type ( ) ; if ( write type == null ) { return lock mode ; } switch ( write type ) { case ddl_exclusive : return hive lock mode . exclusive ; case ddl_shared : return hive lock mode . shared ; case ddl_no_lock : return null ; default : return lock mode ; },ddl sets,success,pre
does the word <PLACE_HOLDER> a capitalized letter ?,boolean is in cap = false ; for ( int i = __num__ ; i < word . length ( ) ; i ++ ) { if ( character . is upper case ( word . char at ( i ) ) ) { is in cap = true ; break ; } } if ( is in cap ) return case_incap ;,word contain,success,pre
of the candidates @$ <PLACE_HOLDER> the peers that meet the minimum protocol version we want to target . we could select the highest version we 've seen on the assumption that newer versions are always better but we do n't want to zap peers if they upgrade early . if we ca n't <PLACE_HOLDER> any peers that have our preferred protocol version or better,int highest version = __num__ @$ preferred version = __num__ ;,peers find,success,pre
check to see if the existence of this region <PLACE_HOLDER> the region in meta,for ( region info r : regions ) { hbck region info hbi = hbck . get or create info ( r . get encoded name ( ) ) ; hbi . add server ( r @$ rsinfo ) ; },existence matches,success,pre
if we have prepared the insert @$ we do n't do it again . we asume that all the step insert statements <PLACE_HOLDER> one after the other .,if ( ps job attributes insert == null ) { string sql = database . get insert statement ( kettle database repository . table_r_job_attribute @$ table . get row meta ( ) ) ; ps job attributes insert = database . preparesql ( sql ) ; } database . set values ( table @$ ps job attributes insert ) ; database . insert row ( ps job attributes insert @$ use batch processing ) ; if ( log . is debug ( ) ) { log . log debug ( __str__ + code + __str__ ) ; } return id ;,statements happen,fail,pre
then user <PLACE_HOLDER> all tables .,if ( tables to use == null ) { } else if ( tables to use . length != table map . length ) { throw new io exception ( __str__ ) ; },user specified,fail,pre
note that there is no grouping here @$ the bolt should not receive any tuple from spout i <PLACE_HOLDER> the parallelism of the bolt to check if it is correct when we have a parallelism greater than 1 .,topology builder . set bolt ( __str__ @$ new tick tuple test bolt ( ) @$ tick_tuple_bolt_parallelism ) . add configuration ( config . topology_tick_tuple_freq_secs @$ tick_tuple_cycle ) ; set < string > user define metrics = new hash set < string > ( ) ; user define metrics . add ( __str__ ) ; user define metrics . add ( __str__ ) ; j storm unit test validator validator = new j storm unit test metric validator ( user define metrics ) { @ override public boolean validate metrics ( map < string @$ double > metrics ) { double cycle = __num__ / ( metrics . get ( __str__ ) / tick_tuple_bolt_parallelism ) ; log . info ( __str__ + metrics . get ( __str__ ),tuple use,fail,pre
here @$ we verify that the op size <PLACE_HOLDER> sense and that the data matches its checksum before attempting to construct an op . this is important because otherwise we may encounter an out of memory exception which could bring down the name node or journal node when reading garbage data .,int op length = in . read int ( ) + op_id_length + checksum_length ; if ( op length > max op size ) { throw new io exception ( __str__ + ( int ) op code byte + __str__ + op length + __str__ + max op size ) ; } else if ( op length < min_op_length ) { throw new io exception ( __str__ + ( int ) op code byte + __str__ + op length + __str__ + min_op_length ) ; } long txid = in . read long ( ) ;,size makes,success,pre
our own copy of content <PLACE_HOLDER> types .,final string [ ] mime types ;,copy mime,success,pre
needed for settings changes which <PLACE_HOLDER> header labels,get table header ( ) . repaint ( ) ;,which remove,fail,pre
round this be<PLACE_HOLDER> having this floating may <PLACE_HOLDER> bad measurements,m bar gap = math . round ( resources . get dimension ( r . dimen . dad_gap_between_bars ) ) ; m spin = resources . get boolean ( r . bool . dad_spin_bars ) ; m middle arrow size = resources . get dimension ( r . dimen . dad_middle_bar_arrow_size ) ; m paint . set style ( paint . style . stroke ) ; m paint . set stroke join ( paint . join . miter ) ; m paint . set stroke cap ( paint . cap . butt ) ; m paint . set stroke width ( m bar thickness ) ; m max cut for bar size = ( float ) ( m bar thickness / __num__ * math . cos ( arrow_head_angle,floating cause,success,pre
check that the write table write operation latency does not <PLACE_HOLDER> the configured timeout .,if ( actual write latency > this . configured write table timeout ) { log . error ( __str__ @$ write table string name ) ; },latency exceed,success,pre
bind and start to <PLACE_HOLDER> incoming connections .,_bootstrap . bind ( new inet socket address ( _port ) ) ;,bind accept,success,pre
null should not <PLACE_HOLDER> result,current min = double min kudaf . aggregate ( null @$ current min ) ; assert that ( new big decimal ( __num__ @$ new math context ( __num__ ) ) @$ equal to ( current min ) ) ;,null impact,success,pre
writing will always <PLACE_HOLDER> full date string,string time zone = new simple date format ( __str__ ) . format ( date ) ; assert equals ( __str__ + time zone @$ formatted ) ; assert equals ( date @$ result ) ;,writing return,fail,pre
every other n <PLACE_HOLDER> n times . the sum is therefore n x n .,assert equals ( a1 . int value ( ) * a1 . int value ( ) @$ sum . int value ( ) ) ;,n succeeds,fail,pre
sync sending @$ will <PLACE_HOLDER> a send result,try { producer . send ( prepare message ( input ) ) ; collector . ack ( input ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; collector . report error ( e ) ; collector . fail ( input ) ; },sync need,fail,pre
during callback @$ the callback url is by definition not needed @$ but the saml 2 settings does never <PLACE_HOLDER> this setting to be empty ...,saml data . put ( __str__ @$ callback url != null ? callback url : any_url ) ; settings builder builder = new settings builder ( ) ; return builder . from values ( saml data ) . build ( ) ;,settings allow,success,pre
null session cache never <PLACE_HOLDER> the session that was removed from the cache because it was never in the cache !,assert null ( session ) ;,cache returns,success,pre
project name and enabled values should <PLACE_HOLDER> disabled notification .,this . project name test . assert values ( enabled notification . project ( ) . name ( ) @$ disabled notification . project ( ) . name ( ) ) ; this . enabled switch test . assert values ( true @$ false ) ;,name match,success,pre
stop emitting at a certain point @$ because log rolling <PLACE_HOLDER> the tests .,if ( num emitted >= testable topology . max_spout_emits ) { return ; },rolling breaks,success,pre
this happens on the driver since the call below <PLACE_HOLDER> jackson @$ and we have a version conflict with spark . or did .,return targets by tree andid . map values ( numeric targets -> stream support . stream ( numeric targets . spliterator ( ) @$ false ) . collect ( collectors . summarizing double ( f -> ( ( numeric feature ) f ) . get value ( ) ) ) ) . collect ( ) . stream ( ) . map ( p -> { integer treeid = p . _1 ( ) . get first ( ) ; string nodeid = p . _1 ( ) . get second ( ) ; double summary statistics stats = p . _2 ( ) ; return text utils . joinjson ( arrays . as list ( treeid @$ nodeid @$ stats . get average ( ) @$ stats .,call uses,success,pre
for now @$ start <PLACE_HOLDER> the data section right after the header but if this store evolves @$ we could decide to start the data section somewhere else .,_data section start off set = header_length ; write ( byte arrays ) ;,start read,fail,pre
oracle did <PLACE_HOLDER> support for ansi case statements in 9 i,return new ansi case fragment ( ) ;,oracle support,fail,pre
ensure the cloud provider does n't <PLACE_HOLDER> enterprise org folder,mbp = mock mbp ( create github enterprise credential ( ) @$ user @$ github enterprise scm . domain_name ) ; assert false ( __str__ @$ provider . support ( mbp ) ) ;,provider support,success,pre
this enlists the transaction <PLACE_HOLDER> em into the transaction,assert . assert true ( bike race . all motor bikes ( ) . is empty ( ) ) ; motorbike bike = bike race . create new bike ( __num__ @$ __str__ ) ; assert . assert false ( bike race . contains ( bike ) ) ;,transaction rolls,fail,pre
check mapping an element <PLACE_HOLDER> the expected result .,bounded window input window = new interval window ( instant . now ( ) @$ duration . standard minutes ( __num__ ) ) ; assert equals ( global window . instance @$ window mapping fn . get side input window ( input window ) ) ; assert equals ( input window @$ ( ( kv ) test sdk harness . get input values ( ) . get ( __num__ ) . get value ( ) ) . get value ( ) ) ;,check gives,fail,pre
validate the window <PLACE_HOLDER> function .,if ( windowing strategy . get window fn ( ) instanceof invalid windows ) { string cause = ( ( invalid windows < ? > ) windowing strategy . get window fn ( ) ) . get cause ( ) ; throw new illegal state exception ( __str__ + __str__ + cause ) ; },window show,fail,pre
create a handler and handle each <PLACE_HOLDER> pairs,s3 guard fsck violation handler handler = new s3 guard fsck violation handler ( rawfs @$ metadata store ) ; compare pairs . for each ( handler :: handle ) ; stopwatch . stop ( ) ; log . info ( __str__ @$ stopwatch . now ( time unit . seconds ) ) ; log . info ( __str__ @$ ddb tree . content map . size ( ) ) ; return compare pairs ;,each violated,success,pre
direct comparison with undefined will always <PLACE_HOLDER> empty set,object key = index info . evaluate index key ( context ) ; create empty set = ( key != null && key . equals ( query service . undefined ) ) ; if ( result type instanceof struct type ) { index fields size = ( ( struct type impl ) result type ) . get field names ( ) . length ; } else { index fields size = __num__ ; },comparison return,success,pre
note : the following line is necessary for garbage collection because the menus <PLACE_HOLDER> references to the graph panel and this leads to circular object references .,m_menu bar . remove all ( ) ; m_tool bar . remove all ( ) ; m_menu bar = null ; m_tool bar = null ; m_right panel . dispose ( ) ; m_left panel . dispose ( ) ; m_dialogs . dispose ( ) ; remove all ( ) ;,menus have,fail,pre
filter all message stanzas <PLACE_HOLDER> a data stanza extension @$ matching session id and recipient,return new and filter ( new stanza type filter ( message . class ) @$ new ibb data packet filter ( ) ) ;,stanzas containing,success,pre
wait a minute and you should <PLACE_HOLDER> other 6 requests executed,wait minute quota ( ) ;,minute get,success,pre
if the text wrapper does n't <PLACE_HOLDER> a start index and the new character is a starting one,if ( parenthesis . get ( i ) . start index == - __num__ && ! is closing parenthesis ( paren ) ) { if ( parenthesis of interest == - __num__ ) { parenthesis of interest = i ; } else { if ( parenthesis . get ( i ) . end index < parenthesis . get ( parenthesis of interest ) . end index ) { parenthesis of interest = i ; } } },wrapper have,success,pre
official java api does not <PLACE_HOLDER> memory aligned strides ...,assert equals ( cvmat2 . cols ( ) * cvmat2 . channels ( ) @$ frame3 . image stride ) ; frame3 . image stride = frame2 . image stride ; u byte indexer frame1 idx = frame1 . create indexer ( ) ; u byte indexer frame2 idx = frame2 . create indexer ( ) ; u byte indexer frame3 idx = frame3 . create indexer ( ) ; for ( int i = __num__ ; i < frame idx . rows ( ) ; i ++ ) { for ( int j = __num__ ; j < frame idx . cols ( ) ; j ++ ) { for ( int k = __num__ ; k < frame idx . channels ( ) ; k ++,api support,success,pre
no existing match found . <PLACE_HOLDER> a new one .,final list < virtual column > virtual columns = new array list < > ( ) ; if ( input . is direct column access ( ) ) { aggregator factory = new t digest sketch aggregator factory ( agg name @$ input . get direct column ( ) @$ compression ) ; } else { virtual column virtual column = virtual column registry . get or create virtual column for expression ( planner context @$ input @$ sql type name . float ) ; virtual columns . add ( virtual column ) ; aggregator factory = new t digest sketch aggregator factory ( agg name @$ virtual column . get output name ( ) @$ compression ) ; } return aggregation . create ( virtual columns @$,match create,success,pre
anonymous loggers can always <PLACE_HOLDER> handlers,if ( this . is named ) { log manager . get log manager ( ) . check access ( ) ; } this . handlers . add ( handler ) ; update dalvik log handler ( ) ;,loggers declare,fail,pre
the partition must <PLACE_HOLDER> at least two consumers,if ( partition2 all potential consumers . get ( partition ) . size ( ) <= __num__ ) log . error ( __str__ @$ partition ) ;,partition have,success,pre
no more directories to watch @$ something <PLACE_HOLDER> the root directory being watched .,if ( watch key to dir bi map . is empty ( ) ) { throw new io exception ( __str__ + watch root path + __str__ ) ; },directories happened,success,pre
user may have <PLACE_HOLDER> an order by ... limit clause,if ( m_parsed delete . order by columns ( ) . size ( ) > __num__ && ! is single partition plan && ! target table . get isreplicated ( ) ) { throw new planning error exception ( __str__ + __str__ + __str__ + __str__ ) ; } boolean needs order by node = is order by node required ( m_parsed delete @$ sub select root ) ; abstract expression address expr = new tuple address expression ( ) ; node schema proj_schema = new node schema ( ) ;,user given,fail,pre
host name should be valid @$ but most probably not existing if its not enough @$ then should probably <PLACE_HOLDER> 'list ' command first to be sure ...,final string not existent fake host name = __str__ ; string credentials not found msg = null ; try { run credential program ( not existent fake host name @$ credential helper name ) ; log . warn ( __str__ @$ credential helper name ) ; } catch ( exception e ) { if ( e instanceof invalid result exception ) { credentials not found msg = extract credential provider error message ( ( invalid result exception ) e ) ; } if ( is blank ( credentials not found msg ) ) { log . warn ( __str__ @$ credential helper name @$ e . get message ( ) ) ; } else { log . debug ( __str__ @$ credentials not found msg ) ; },its have,fail,pre
if only one memory space <PLACE_HOLDER> a valid value @$ use it,if ( containing mem space cnt == __num__ && containing addr != null ) { return containing addr . get address space ( ) . get unique spaceid ( ) ; } if ( symbol target cnt == __num__ && symbol target != null ) { return symbol target . get address space ( ) . get unique spaceid ( ) ; },space has,fail,pre
prints <PLACE_HOLDER> information .,log . v ( tag @$ file name + __str__ + exif interface . get altitude ( __num__ ) ) ; float [ ] lat long = new float [ __num__ ] ; if ( exif interface . get lat long ( lat long ) ) { log . v ( tag @$ file name + __str__ + lat long [ __num__ ] ) ; log . v ( tag @$ file name + __str__ + lat long [ __num__ ] ) ; } else { log . v ( tag @$ file name + __str__ ) ; },prints gps,success,pre
if the value type <PLACE_HOLDER> no data members @$ a null return is expected,if ( v . state ( ) == null ) return ; for ( int i = __num__ ; i < v . state ( ) . size ( ) ; i ++ ) { interface state member = ( interface state ) v . state ( ) . element at ( i ) ; symtab entry entry = ( symtab entry ) member . entry ; util . fill info ( entry ) ; if ( entry . comment ( ) != null ) entry . comment ( ) . generate ( __str__ @$ stream ) ; string modifier = __str__ ; if ( member . modifier == interface state . public ) modifier = __str__ ; util . write initializer ( modifier @$ entry . name,type contains,success,pre
to produce the final output select the tuples directed to the 'output ' channel then <PLACE_HOLDER> the input pairs that have the greatest iteration counter on a 1 second sliding window,data stream < tuple2 < tuple2 < integer @$ integer > @$ integer > > numbers = step . select ( __str__ ) . map ( new output map ( ) ) ;,pairs select,fail,pre
if its an external table @$ even though the temp table <PLACE_HOLDER> flag is on @$ we create the table since we need the hdfs path to temp table lineage .,return table != null && skip temp tables && table . is temporary ( ) && ! external_table . equals ( table . get table type ( ) ) ;,table builds,fail,pre
the is being initialize @$ <PLACE_HOLDER> the new value .,if ( n . has children ( ) ) { parent . remove child ( n ) ; node value = n . get first child ( ) ; n . remove child ( value ) ; node replacement = ir . assign ( n @$ value ) ; replacement . setjs doc info ( parent . getjs doc info ( ) ) ; replacement . use source info if missing from ( parent ) ; node statement = node util . new expr ( replacement ) ; grandparent . replace child ( parent @$ statement ) ; report code change ( __str__ @$ statement ) ; } else { if ( node util . is statement block ( grandparent ) ) { grandparent . remove child (,the copy,fail,pre
verify sync <PLACE_HOLDER> or not,if ( expect sync || expect sync from log syncer ) { test_util . wait for ( timeout @$ new waiter . predicate < exception > ( ) { @ override public boolean evaluate ( ) throws exception { try { if ( expect sync ) { verify ( wal @$ times ( __num__ ) ) . sync ( any long ( ) ) ; } else if ( expect sync from log syncer ) { verify ( wal @$ times ( __num__ ) ) . sync ( ) ; } } catch ( throwable ignore ) { } return true ; } } ) ; } else { verify ( wal @$ never ( ) ) . sync ( ) ; },sync occured,fail,pre
m launch transition end runnable might <PLACE_HOLDER> show keyguard @$ which would execute it again @$ which would lead to infinite recursion . protect against it .,m launch transition end runnable = null ; r . run ( ) ;,end cancel,fail,pre
the computation <PLACE_HOLDER> an exception @$ so it did not complete successfully .,return false ;,computation threw,success,pre
set content <PLACE_HOLDER> processors for various content types .,if ( completion processor == null ) { this . completion processor = new sql completion processor ( editor ) ; } assistant . add content assist processor ( completion processor @$ i document . default_content_type ) ; assistant . add content assist processor ( completion processor @$ sql parser partitions . content_type_sql_quoted ) ;,content open,fail,pre
<PLACE_HOLDER> a mapping between the step name and the injection ... <PLACE_HOLDER> new injection info,data . step injection metas map = new hash map < string @$ step meta interface > ( ) ; for ( step meta step meta : data . trans meta . get used steps ( ) ) { step meta interface meta = step meta . get step meta interface ( ) ; if ( bean injection info . is injection supported ( meta . get class ( ) ) ) { data . step injection metas map . put ( step meta . get name ( ) @$ meta ) ; } },mapping add,fail,pre
it 's the engine context factory the one who <PLACE_HOLDER> the responsibility of creating the specific implementation of the engine context needed .,final i engine context factory engine context factory = configuration . get engine context factory ( ) ; return engine context factory . create engine context ( configuration @$ template data @$ template resolution attributes @$ context ) ;,who has,success,pre
field choice will <PLACE_HOLDER> the entire tree of a field,return parse unknown field ( start element @$ false @$ null ) ;,choice consume,fail,pre
master does n't <PLACE_HOLDER> any more the <PLACE_HOLDER>ed master should not call retain assignment @$ as it is not a clean startup .,assert false ( __str__ @$ mock load balancer . retain assign called ) ;,master offer,fail,pre
multi <PLACE_HOLDER> field separator,wl multi valued separator = new label ( w settings @$ swt . right ) ; wl multi valued separator . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl multi valued separator ) ; fdl multi valued separator = new form data ( ) ; fdl multi valued separator . left = new form attachment ( __num__ @$ __num__ ) ; fdl multi valued separator . top = new form attachment ( w operation @$ margin ) ; fdl multi valued separator . right = new form attachment ( middle @$ - margin ) ; wl multi valued separator . set layout data ( fdl multi valued separator ) ; w multi valued separator =,multi valued,success,pre
make sure the client recognizes the underlying exception otherwise @$ throw a do not <PLACE_HOLDER> io exception .,if ( version info util . has minimum version ( connection header . get version info ( ) @$ request too big exception . major_version @$ request too big exception . minor_version ) ) { req too big . set response ( null @$ null @$ simple rpc server . request_too_big_exception @$ msg ) ; } else { req too big . set response ( null @$ null @$ new do not retryio exception ( ) @$ msg ) ; },a retry,success,pre
push twice will <PLACE_HOLDER> and temp dir cleaned,file out dir = new file ( string utils . format ( __str__ @$ config . get storage directory ( ) @$ segment path ) ) ; out dir . set read only ( ) ; try { pusher . push ( segment dir @$ segments [ i ] @$ false ) ; } catch ( io exception e ) { assert . fail ( __str__ ) ; },push fail,success,pre
simple insert of simple keys @$ with no reprobing on insert until the table gets full exactly . then <PLACE_HOLDER> a 'get ' on the totally full table .,non blocking hash map < integer @$ object > map = new non blocking hash map < > ( __num__ ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { map . put ( i @$ new object ( ) ) ; } map . get ( __num__ ) ;,insert do,success,pre
localized string <PLACE_HOLDER> name,args . filtered resources provider = new resources filter ( aapt target . with flavors ( internal flavor . of ( __str__ ) ) @$ filesystem @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ graph builder @$ immutable list . of ( resource1 . get res ( ) @$ resource2 . get res ( ) ) @$ immutable set . of ( ) @$ immutable set . of ( ) @$ null @$ resources filter . resource compression mode . disabled @$ filter resources steps . resource filter . empty_filter @$ optional . empty ( ) ) ;,string file,success,pre
zap : <PLACE_HOLDER> the type argument .,vector < object > v = get list ( uri @$ key ) ; if ( v == null || v . size ( ) == __num__ ) { return null ; } return v . get ( __num__ ) ;,zap added,success,pre
report all volumes as unmounted until we 've recorded that user 0 <PLACE_HOLDER> unlocked . there are no guarantees that callers will see a consistent view of the volume before that point,final boolean system user unlocked = is system unlocked ( user handle . user_system ) ; final boolean user key unlocked ; final boolean storage permission ; final long token = binder . clear calling identity ( ) ; try { user key unlocked = is user key unlocked ( user id ) ; storage permission = m storage manager internal . has external storage ( uid @$ package name ) ; } finally { binder . restore calling identity ( token ) ; } boolean found primary = false ; final array list < storage volume > res = new array list < > ( ) ; synchronized ( m lock ) { for ( int i = __num__ ; i < m volumes . size (,user got,fail,pre
setup the configurator to inject the pool config in the message driven component <PLACE_HOLDER> service,final message driven component description mdb component description = ( message driven component description ) mdb component configuration . get component description ( ) ; mdb component configuration . get create dependencies ( ) . add ( new pool injecting configurator ( mdb component description ) ) ;,component create,success,pre
the following code is for mapjoin <PLACE_HOLDER> all the dummy ops,log . info ( __str__ ) ; list < operator < ? extends operator desc > > dummy ops = local work . get dummy parent op ( ) ; for ( operator < ? extends operator desc > dummy op : dummy ops ) { dummy op . set exec context ( exec context ) ; dummy op . initialize ( jc @$ null ) ; },mapjoin initialize,success,pre
local variable types must be resolved from the point of view of the annotated class . so <PLACE_HOLDER> the resolution early @$ because users of the local variable table only have access to the original .,if ( annotated . get local variable table ( ) != null ) { local [ ] orig locals = annotated . get local variable table ( ) . get locals ( ) ; local [ ] new locals = new local [ orig locals . length ] ; resolved java type accessing class = annotated . get declaring class ( ) ; for ( int i = __num__ ; i < new locals . length ; i ++ ) { local orig local = orig locals [ i ] ; new locals [ i ] = new local ( orig local . get name ( ) @$ orig local . get type ( ) . resolve ( accessing class ) @$ orig local . get startbci (,types do,success,pre
base the data ref on the request parameter but ensure the scheme is based off the incoming request ... this is necessary for scenario 's where the ni fi instance is behind a proxy <PLACE_HOLDER> a different scheme,ref uri builder . scheme ( request . get scheme ( ) ) ;,instance ering,fail,pre
read a property <PLACE_HOLDER> and convert them into configuration lines,file input stream in stream = null ; try { final file f = new file ( args [ __num__ ] ) ; final properties p = new properties ( ) ; in stream = new file input stream ( f ) ; p . load ( in stream ) ; final string builder sb = new string builder ( ) ; sb . append ( __str__ ) ; for ( int i = __num__ ; i <= __num__ ; i ++ ) { sb . append ( __str__ ) ; final string s = p . get property ( __str__ + i ) ; final string [ ] l = common pattern . comma . split ( s ) ; for ( final string element : l,property files,fail,pre
the source character was not <PLACE_HOLDER> . skip this case ; the first step in obtaining a skeleton is to <PLACE_HOLDER> the input @$ so the mapping in this line of confusables.txt will never be applied .,if ( ! normalizer . is normalized ( from ) ) { continue ; },character translate,fail,pre
it 's an array field type @$ <PLACE_HOLDER> check the inner type,inner field type . check compatibility ( ( ( array field type ) other ) . inner field type @$ conflicts @$ strict ) ;,type lets,success,pre
if this rule does n't <PLACE_HOLDER> local resources @$ no resource processing was done @$ so it does n't produce data binding output .,if ( ! android resources . defines android resources ( rule context . attributes ( ) ) ) { return immutable list . of ( ) ; },rule define,success,pre
<PLACE_HOLDER> this buffer buffer <PLACE_HOLDER> this buffer buffer,byte [ ] [ ] cols = new byte [ ] [ ] { bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) } ;,buffer hit,fail,pre
use this setting to improve performance if you know that <PLACE_HOLDER>s in content do not <PLACE_HOLDER> the layout size of the recycler view,m recycler view . set has fixed size ( true ) ; recycler view . layout manager layout manager = new linear layout manager ( get activity ( ) ) ; m recycler view . set layout manager ( layout manager ) ; m recycler view . set adapter ( adapter ) ; m recycler view . add item decoration ( new spaces item decoration ( quick return utils . dp2px ( get activity ( ) @$ __num__ ) ) ) ; array list < view > header views = new array list < > ( ) ; header views . add ( get action bar view ( ) ) ; array list < view > footer views = new array list < > ( ) ; m,changes change,success,pre
the server initially <PLACE_HOLDER> the connection flow control window to 0 .,run in channel ( server connected channel @$ new http2 runnable ( ) { @ override public void run ( ) throws http2 exception { http2 server . encoder ( ) . write settings ( server ctx ( ) @$ new http2 settings ( ) . copy from ( http2 server . decoder ( ) . local settings ( ) ) . initial window size ( __num__ ) @$ server new promise ( ) ) ; http2 server . flush ( server ctx ( ) ) ; } } ) ; assert true ( server settings ack latch1 . await ( default_await_timeout_seconds @$ seconds ) ) ;,server closes,fail,pre
the removed admin might have <PLACE_HOLDER> camera @$ so update user restrictions .,if ( removed admin ) { push user restrictions ( user handle ) ; },admin disabled,success,pre
even after the security realm <PLACE_HOLDER> the user @$ they can still connect @$ until session invalidation,assert user connected ( wc @$ alice ) ; request renew seed for user ( alice ) ; assert user not connected ( wc @$ alice ) ; assert user connected ( wc @$ __str__ ) ; try { wc . login ( alice ) ; fail ( __str__ ) ; } catch ( failing http status code exception e ) { assert equals ( __num__ @$ e . get status code ( ) ) ; },realm drops,fail,pre
the button <PLACE_HOLDER> precedence over the summary text and the chevron .,int edit button state = get edit button state ( ) ; if ( edit button state == edit_button_gone ) { m edit button view . set visibility ( gone ) ; m chevron view . set visibility ( m display mode == display_mode_expandable ? visible : gone ) ; boolean show summary = m is summary allowed && ! text utils . is empty ( m summary left text view . get text ( ) ) ; m summary layout . set visibility ( show summary ? visible : gone ) ; } else { boolean is button allowed = m display mode == display_mode_expandable || m display mode == display_mode_normal ; m summary layout . set visibility ( gone ) ; m chevron view . set,button takes,success,pre
there is a possibility that another thread may have <PLACE_HOLDER> the same class in the meantime,try { the class = cl . load class ( stub class name ) ; } catch ( class not found exception e1 ) { ejb logger . root_logger . dynamic stub creation failed ( stub class name @$ ex ) ; throw ex ; },thread loaded,fail,pre
filter and reverse <PLACE_HOLDER> the parsed forms .,tree set < string > parsed set = new tree set < string > ( string . case_insensitive_order ) ; parsed set . add ( text ) ; parsed set . add ( final text ) ; if ( variants != null ) { for ( int i = variants . length ; -- i >= __num__ ; ) { parsed set . add ( variants [ i ] ) ; } } array list < string > parsed list = new array list < string > ( parsed set ) ; collections . reverse ( parsed list ) ; i parsed forms = parsed list . to array ( new string [ parsed list . size ( ) ] ) ;,filter attached,fail,pre
iterate through all source paths to make sure we are <PLACE_HOLDER> a complete set of source folders for the source paths .,set < string > src folders = new hash set < > ( ) ; loop through source path : for ( source path java src path : java srcs ) { if ( rule finder . get rule ( java src path ) . is present ( ) ) { continue ; } path java src relative path = rule finder . get source path resolver ( ) . get relative path ( java src path ) ; for ( string src folder : src folders ) { if ( java src relative path . starts with ( src folder ) ) { continue loop through source path ; } } immutable sorted set < string > paths from root = default java package finder . get,paths returning,fail,pre
now the tricky one . 'before a ' <PLACE_HOLDER> to 'call activity a ' @$ which calls subprocess 02 which terminates,process instance = runtime service . start process instance by key ( __str__ ) ; tasks = assert task names ( process instance @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; task task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ; assert historic process,a leads,success,pre
if current <PLACE_HOLDER> position is already set @$ update the next <PLACE_HOLDER> position .,m next seek position = new position ;,current seek,success,pre
note : use a message object to facilite script message <PLACE_HOLDER> filtering,msg . info ( ghidra script . class @$ new script message ( decorated message ) ) ; if ( is running headless ( ) ) { return ; } plugin tool tool = state . get tool ( ) ; if ( tool == null ) { return ; } console service console = tool . get service ( console service . class ) ; if ( console == null ) { return ; } try { console . add message ( get script name ( ) @$ message ) ; } catch ( exception e ) { msg . error ( this @$ __str__ + message @$ e ) ; },message perform,fail,pre
let the data store know about the real bucket at this point so that other v ms which <PLACE_HOLDER> the real bucket via a profile exchange can send messages to the data store and safely use the bucket .,if ( buk reg != null ) { observer . before assign bucket ( this . partitioned region @$ possibly free bucket id ) ; assign bucket region ( buk reg . get id ( ) @$ buk reg ) ; buk . set hosting ( true ) ; buk reg . invoke partition listener after bucket created ( ) ; } else { if ( buk . get partitioned region ( ) . get colocated with ( ) == null ) { buk . get bucket advisor ( ) . set shadow bucket destroyed ( true ) ; clear all temp queue for shadowpr ( buk . get bucket id ( ) ) ; } },which get,fail,pre
if the pom <PLACE_HOLDER> no file @$ we cached a missing artifact @$ only return the cached data if no update forced,if ( cached != null && ( ! request . is force update ( ) || has file ( cached . get pom artifact ( ) ) ) ) { return cached ; },pom has,success,pre
this one <PLACE_HOLDER> a primitive type as much as possible,if ( prop . is unboxable ( ) ) return frf . get required unboxed ( ) ; else return frf . get single ( ) ;,one requires,fail,pre
test that it works when the shutdown <PLACE_HOLDER> the outstanding request ...,test shutdown request outstanding ( __num__ @$ __num__ @$ remote invocation exception . class @$ timeout exception . class ) ;,shutdown kills,success,pre
the spans that do not cross the network boundary should <PLACE_HOLDER> the same id .,assert that ( client bar span . id ( ) ) . is equal to ( service bar span . id ( ) ) ; assert that ( client qux span . id ( ) ) . is equal to ( service qux span . id ( ) ) ;,spans have,success,pre
old implementations might <PLACE_HOLDER> reset for initialization @$ so ensure it is initialized here as well . initialization has no side effects if already done .,if ( is enabled ( ) ) { initialize ( ) ; clear events list ( ) ; },implementations call,success,pre
to do not <PLACE_HOLDER> the length of the separator @$ let us point to the last symbol of it .,if ( ! escaped && space_byte != after next or space && tab_byte != after next or space && linefeed_byte == current ) { return i + __num__ ; },to change,fail,pre
after resume second client join @$ transaction should succesfully <PLACE_HOLDER> new affinity and commit .,tx fut . get ( ) ;,transaction assign,fail,pre
we do n't know how the results have changed after being filtered . must <PLACE_HOLDER> progress according to contents of results now .,if ( scanner context . get keep progress ( ) ) { scanner context . set progress ( initial batch progress @$ initial size progress @$ initial heap size progress ) ; } else { scanner context . clear progress ( ) ; } scanner context . increment batch progress ( results . size ( ) ) ; for ( cell cell : results ) { scanner context . increment size progress ( private cell util . estimated serialized size of ( cell ) @$ cell . heap size ( ) ) ; },now set,success,pre
<PLACE_HOLDER> null so that server will <PLACE_HOLDER> 550 file not found .,return null ;,server return,success,pre
increment the generation first @$ so observers always <PLACE_HOLDER> the new value,final int key = make key ( settings_type_secure @$ user id ) ; m generation registry . increment generation ( key ) ; final uri uri = get notification uri for ( key @$ secure . location_mode ) ; m handler . obtain message ( my handler . msg_notify_uri_changed @$ user id @$ __num__ @$ uri ) . send to target ( ) ;,observers see,success,pre
check if the dataset version <PLACE_HOLDER> the correct partition,assert . assert true ( dataset version . get partition ( ) . get complete name ( ) . equals ignore case ( partition_name ) ) ;,version has,fail,pre
if these targets were treated as distinct targets @$ the rule will <PLACE_HOLDER> duplicate symbols .,assume that ( platform . detect ( ) @$ is ( not ( windows ) ) ) ; pair < project workspace @$ project workspace > cells = prepare ( __str__ @$ __str__ ) ; project workspace primary = cells . get first ( ) ; project workspace secondary = cells . get second ( ) ; register cell ( primary @$ __str__ @$ primary ) ; register cell ( secondary @$ __str__ @$ primary ) ; path output = primary . build and return output ( __str__ ) ; assert equals ( __str__ @$ __num__ @$ primary . run command ( output . to string ( ) ) . get exit code ( ) ) ;,rule generate,fail,pre
the private key section <PLACE_HOLDER> both the public key and the private key,string key type = key buffer . read string ( ) ;,section contains,success,pre
make sure users with huge dict do n't <PLACE_HOLDER> up the cache,if ( dict . size ( ) <= __num__ ) { user dict cache . put ( user id @$ dict ) ; } else { logger . info ( __str__ + dict . size ( ) + __str__ + user id + __str__ ) ; },users mess,fail,pre
no values provided . <PLACE_HOLDER> all sectors .,if ( no from ) { m sector range . set text ( get string ( r . string . text_sector_range_all ) ) ; if ( save as default . is checked ( ) ) { save mapping range ( __str__ @$ __str__ ) ; } return ; },values preserve,fail,pre
not a small battery device @$ so plugged in status should not <PLACE_HOLDER> forced app standby,m is small battery device = false ; final app state tracker testable instance = new instance ( ) ; call start ( instance ) ; assert false ( instance . is force all apps standby enabled ( ) ) ; m power save mode = true ; m power save observer . accept ( get power save state ( ) ) ; assert true ( instance . is force all apps standby enabled ( ) ) ;,device trigger,fail,pre
we need a real server and client in this test @$ because netty 's embedded channel is not <PLACE_HOLDER> the channel promise of failed writes .,netty server and client server and client = init server and client ( protocol @$ create config ( ) ) ; channel ch = connect ( server and client ) ; network client handler handler = get client handler ( ch ) ;,channel giving,fail,pre
if it is a node try to parse with the node parser to find out whether we should may use the generated <PLACE_HOLDER> and get uncached methods .,if ( node code generator . is specialized node ( parameter . get type ( ) ) ) { node parser parser = node parser . create default parser ( ) ; parser . node only = true ; type element element = element utils . cast type element ( parameter . get type ( ) ) ; if ( ! node only ) { node data parsed node = parser . parse ( element ) ; if ( parsed node != null ) { list < code executable element > executables = node factory factory . create factory methods ( parsed node @$ element filter . constructors in ( element . get enclosed elements ( ) ) ) ; type element type = element utils . cast,generated function,fail,pre
if the scope <PLACE_HOLDER> no listeners try to remove it,if ( ! ( ( basic scope ) broadcast scope ) . has event listeners ( ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ ) ; } scope . remove child scope ( broadcast scope ) ; } log . debug ( __str__ @$ name ) ;,scope has,success,pre
insert rows on different streams but since they all go thru one export <PLACE_HOLDER> them thru the check stream,for ( int i = __num__ ; i < loop_count ; i ++ ) { for ( int j = __num__ ; j < m_stream count ; j ++ ) { string stream name = string . format ( stream_template @$ j ) ; for ( int k = row count ; k < row count + row_batch ; k ++ ) { data [ __num__ ] = k ; m_verifier . add row ( m_client @$ check stream @$ k @$ data ) ; m_client . call procedure ( __str__ @$ __str__ + stream name + __str__ + k + __str__ ) ; } row count += row_batch ; } },export find,fail,pre
for now we just propagate statistics for source symbols . handling semi join output symbols <PLACE_HOLDER> support for correlation for boolean columns .,return optional . of ( source stats ) ;,symbols requires,success,pre
this method always <PLACE_HOLDER> activation exception,user service . activate user ( __str__ ) ;,method throws,success,pre
rather than risk a mysterious class <PLACE_HOLDER> exception during unit tests @$ throw an explanatory exception .,throw new unsupported operation exception ( __str__ + __str__ + __str__ ) ;,class found,fail,pre
and <PLACE_HOLDER> a new file to that dir :,path uf = ro path . resolve ( system utils . file name ( __str__ ) ) ; files . write ( uf @$ arrays . as list ( __str__ @$ __str__ ) ) ; files . set posix file permissions ( uf @$ posix file permissions . from string ( __str__ ) ) ; files . set posix file permissions ( ro path @$ posix file permissions . from string ( __str__ ) ) ;,and add,success,pre
super translation should <PLACE_HOLDER> the generic private method declaration .,assert translation ( super translation @$ __str__ ) ;,translation have,fail,pre
json serializer will always <PLACE_HOLDER> the yielder,try { json writer . write value ( os @$ yielder ) ; os . flush ( ) ; os . close ( ) ; } catch ( exception ex ) { e = ex ; log . no stack trace ( ) . error ( ex @$ __str__ ) ; throw new runtime exception ( ex ) ; } finally { thread . current thread ( ) . set name ( curr thread name ) ; query lifecycle . emit logs and metrics ( e @$ req . get remote addr ( ) @$ os . get count ( ) ) ; if ( e == null ) { successful query count . increment and get ( ) ; } else { failed query count . increment,serializer use,fail,pre
we perform the analogous test on the <PLACE_HOLDER> methods .,subclass . set field ( type @$ subclass @$ boolean field @$ illegal argument exception class @$ value ) ;,test get,success,pre
update queues @$ all queue can <PLACE_HOLDER> this node,for ( queue q : queue collections . values ( ) ) { resources . subtract from ( q . resource @$ oldnm . resource ) ; },queue access,success,pre
been returned from the discovery agent . up to that point the reconnect delay is used which <PLACE_HOLDER> a default value of 10,long initial reconnect delay = __num__ ; long startt = system . current time millis ( ) ; string group id = __str__ + startt ; try { string url str = __str__ + group id + __str__ + initial reconnect delay ; activemq connection factory factory = new activemq connection factory ( url str ) ; log . info ( __str__ ) ; connection connection = factory . create connection ( ) ; connection . set clientid ( __str__ ) ; fail ( __str__ ) ; } catch ( jms exception expected ) { assert true ( __str__ + expected . get cause ( ) @$ expected . get cause ( ) instanceof java . io . io exception ) ; long duration = system . current,which has,success,pre
the list <PLACE_HOLDER> exactly the same number of elements as the list of element matchers :,for ( int i = __num__ ; i < pattern token matchers . size ( ) ; i ++ ) { p tokens matched . add ( boolean . false ) ; } int i = __num__ ; int min occur correction = get min occurrence correction ( ) ; while ( i < limit + min occur correction && ! ( rule . is sent start ( ) && i > __num__ ) ) { int skip shift total = __num__ ; boolean all elements match = false ; unified tokens = null ; int matching tokens = __num__ ; int first match token = - __num__ ; int last match token = - __num__ ; int first marker match token = - __num__ ; int last,list contains,fail,pre
step 1 : ensure that user has write permissions to the process group . if not @$ then immediately fail . step 2 : <PLACE_HOLDER> flow from flow registry,if ( version control info != null && request process group entity . get versioned flow snapshot ( ) == null ) { final versioned flow snapshot flow snapshot = get flow from registry ( version control info ) ; service facade . discover compatible bundles ( flow snapshot . get flow contents ( ) ) ; service facade . resolve inherited controller services ( flow snapshot @$ group id @$ ni fi user utils . get ni fi user ( ) ) ; request process group entity . set versioned flow snapshot ( flow snapshot ) ; },not get,fail,pre
2 partitions <PLACE_HOLDER> segment,for ( int partition id = __num__ ; partition id < __num__ ; partition id ++ ) { string segment name = new llc segment name ( raw_table_name @$ partition id @$ __num__ @$ current_time_ms ) . get segment name ( ) ; committing segment descriptor committing segment descriptor = new committing segment descriptor ( segment name @$ partition_offset + num_docs @$ __num__ ) ; committing segment descriptor . set segment metadata ( mock segment metadata ( ) ) ; segment manager . commit segment metadata ( realtime_table_name @$ committing segment descriptor ) ; } test set up new partitions ( segment manager @$ false ) ;,partitions commit,success,pre
aws <PLACE_HOLDER> all the exceptions but illegal argument exception @$ throw runtime exception .,rate limited log ( level . error @$ e @$ __str__ @$ m_config . get resourceid ( ) ) ; m_worker . shutdown ( ) ;,aws logs,fail,pre
the face timeout message is not very actionable @$ let 's <PLACE_HOLDER> the user to manually retry .,if ( msg id == face manager . face_error_timeout ) { show swipe up to unlock ( ) ; } else if ( m status bar keyguard view manager . is bouncer showing ( ) ) { m status bar keyguard view manager . show bouncer message ( err string @$ m initial text color state ) ; } else if ( update monitor . is screen on ( ) ) { show transient indication ( err string ) ; hide transient indication delayed ( hide_delay_ms ) ; } else { m message to show on screen on = err string ; },let allow,fail,pre
setup the <PLACE_HOLDER> params we 'll pass to description when generating the <PLACE_HOLDER> rules .,build target target = build target factory . new instance ( __str__ ) ; cxx source rule factory cxx source rule factorypdc = cxx source rule factory helper . of ( filesystem . get root path ( ) @$ target @$ cxx platform @$ pic type . pdc ) ; cxx library builder cxx library builder = new cxx library builder ( target ) . set exported headers ( immutable sorted map . of ( gen header name @$ default build target source path . of ( gen header target ) ) ) . set srcs ( immutable sorted set . of ( source with flags . of ( fake source path . of ( source name ) ) @$ source with flags . of ( default build,the build,success,pre
first verify that our function does not already <PLACE_HOLDER> the tags we 're going to add .,collection < ? extends function tag > tags = get all tags ( ) ; assert true ( ! is tag name in list ( tag name1 @$ tags ) ) ; assert true ( ! is tag name in list ( tag name2 @$ tags ) ) ; assert true ( ! is tag name in list ( tag name3 @$ tags ) ) ;,function contain,success,pre
let the ejb jar metadata tell us what the version <PLACE_HOLDER>,return ejb jar meta data . is version greater than or equal ( ejb jar version . ejb_3_2 ) ;,version is,success,pre
this worker <PLACE_HOLDER> this block @$ so it is no longer lost .,m lost blocks . remove ( block id ) ;,worker lost,fail,pre
check that new dynamic config includes the updated client port . check that server <PLACE_HOLDER> server id erased client port from static config . check that other servers still have client port in static config .,for ( int i = __num__ ; i < server_count ; i ++ ) { reconfig test . test server has config ( zk [ i ] @$ new servers @$ null ) ; properties static cfg = read properties from file ( mt [ i ] . conf file ) ; if ( i == changed server id ) { assert false ( static cfg . contains key ( __str__ ) ) ; } else { assert true ( static cfg . contains key ( __str__ ) ) ; } } for ( int i = __num__ ; i < server_count ; i ++ ) { mt [ i ] . shutdown ( ) ; zk [ i ] . close ( ) ; zk admin [,server has,fail,pre
this panel <PLACE_HOLDER> the results panel and guarantees the scrolling behaviour,j panel wrapper = new j panel ( new border layout ( ) ) ; wrapper . set background ( color scheme . dark_gray_color ) ; wrapper . add ( search items panel @$ border layout . north ) ;,panel wraps,success,pre
optimization : default serializer just <PLACE_HOLDER> string @$ so we can avoid a call :,if ( is default serializer ( ser ) ) { if ( unwrap single == _unwrap single ) { return this ; } return _with resolved ( property @$ unwrap single ) ; },serializer writes,success,pre
call create edits <PLACE_HOLDER> and move the resulting edits to the name dir .,create edits log . main ( new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ test_dir . get absolute path ( ) } ) ; path edits wildcard = new path ( test_dir . get absolute path ( ) @$ __str__ ) ; file context local fc = file context . get localfs file context ( ) ; for ( file status edits : local fc . util ( ) . glob status ( edits wildcard ) ) { path src = edits . get path ( ) ; path dst = new path ( new file ( name dir @$ __str__ ) . get absolute path ( ) @$ src . get name ( ) ) ; local fc . rename,edits log,success,pre
short codes can not <PLACE_HOLDER> padding,if ( separator position < separator_position ) { return false ; },codes start,fail,pre
if a metric is counter @$ the value sent via rpc should be the <PLACE_HOLDER>al value ; i.e . the amount the value has changed since the last rpc . the master should equivalently <PLACE_HOLDER> its value based on the received metric rather than replacing it .,if ( ! metric set . add ( metric ) ) { metric old metric = metric set . get first by field ( full_name_index @$ metric . get full metric name ( ) ) ; if ( metric . get metric type ( ) == metric type . counter ) { if ( metric . get value ( ) != __num__ ) { old metric . add value ( metric . get value ( ) ) ; } } else { old metric . set value ( metric . get value ( ) ) ; } },master set,fail,pre
if system audio control feature is enabled @$ <PLACE_HOLDER> on system audio mode when new avr is detected . otherwise @$ <PLACE_HOLDER> off system audio mode .,boolean target system audio mode = tv ( ) . is system audio control feature enabled ( ) ; if ( current system audio mode != target system audio mode ) { add and start action ( new system audio action from tv ( tv ( ) @$ m avr address @$ target system audio mode @$ null ) ) ; } else { tv ( ) . set system audio mode ( target system audio mode ) ; },mode turn,success,pre
delivery timed out @$ and the timeout handling already <PLACE_HOLDER> care of updating our tracking here @$ so we need n't do anything further .,if ( debug_listener_callback ) { slog . i ( tag @$ __str__ + who ) ; },handling took,success,pre
decide whether to perform a <PLACE_HOLDER> or put operation,boolean response ; if ( rand . next double ( ) < config . getputratio ) { mp rand = rand . next double ( ) ; if ( mp rand < config . multisingleratio ) { if ( total connections . get ( ) > __num__ && config . poolsize > __num__ ) { slow = true ; debug = true ; } else debug = false ; response = client . call procedure ( new get callback ( mp rand ) @$ __str__ @$ processor . generate random key for retrieval ( ) ) ; } else { response = client . call procedure ( new get callback ( mp rand ) @$ __str__ @$ processor . generate random key for retrieval ( ) ) ;,a get,success,pre
delete <PLACE_HOLDER> odex for android o @$ directory change . fortunately @$ we do n't need to support android o <PLACE_HOLDER> mode any more,log . i ( tag @$ __str__ ) ; share patch file util . delete dir ( patch version directory + __str__ + share constants . interpret_dex_optimize_path ) ;,o java,fail,pre
test missing proxy class class not <PLACE_HOLDER> exception,oin = new proxy blind input stream ( new byte array input stream ( bout . to byte array ( ) ) ) ; try { oin . read object ( ) ; throw new error ( ) ; } catch ( class not found exception ex ) { },class found,success,pre
one <PLACE_HOLDER> timeout,timeout millis . set ( persists3 . _bucket cache @$ __num__ ) ;,one persist,fail,pre
maps must <PLACE_HOLDER> 2 type parameters @$ not just one,try { tf . construct parametric type ( map . class @$ strc ) ; } catch ( illegal argument exception e ) { verify exception ( e @$ __str__ ) ; },maps have,fail,pre
this version does not <PLACE_HOLDER> fractional seconds,int nano of second = __num__ ;,version support,success,pre
do not allow to reset password when current user <PLACE_HOLDER> a managed profile,if ( ! is managed profile ( user handle ) ) { for ( user info user info : m user manager . get profiles ( user handle ) ) { if ( user info . is managed profile ( ) ) { if ( ! pren ) { throw new illegal state exception ( __str__ ) ; } else { slog . e ( log_tag @$ __str__ ) ; return false ; } } } },user has,success,pre
note : the lookup is enforcing security across users by making sure the caller can only <PLACE_HOLDER> widgets it hosts or provides .,widget widget = lookup widget locked ( app widget id @$ binder . get calling uid ( ) @$ calling package ) ; if ( widget != null ) { update app widget instance locked ( widget @$ views @$ partially ) ; },caller access,success,pre
dump the last row which <PLACE_HOLDER> less than 16 bytes .,if ( remainder != __num__ ) { final int row start index = ( full rows << __num__ ) + start index ; append hex dump row prefix ( dump @$ full rows @$ row start index ) ; final int row end index = row start index + remainder ; for ( int j = row start index ; j < row end index ; j ++ ) { dump . append ( byte2hex [ get unsigned byte ( buffer @$ j ) ] ) ; } dump . append ( hex_padding [ remainder ] ) ; dump . append ( __str__ ) ; for ( int j = row start index ; j < row end index ; j ++ ) { dump . append (,which has,success,pre
the first read should <PLACE_HOLDER> one shared memory segment and slot .,dfs test util . read file buffer ( fs @$ test_path1 ) ;,read yield,fail,pre
any app can <PLACE_HOLDER> new static shared libraries,if ( scan result . static shared library info != null ) { return collections . singleton list ( scan result . static shared library info ) ; } final boolean has dynamic libraries = ( pkg . application info . flags & application info . flag_system ) != __num__ && scan result . dynamic shared library infos != null ; if ( ! has dynamic libraries ) { return null ; } final boolean is updated system app = pkg . is updated system app ( ) ;,app register,fail,pre
need to dig out actual destination map <PLACE_HOLDER> and use map property descriptor to set the value on that target <PLACE_HOLDER> ... .,if ( get dest field map get method ( ) != null || mapping utils . is supported map ( determine actual property type ( get dest field name ( ) @$ is dest field indexed ( ) @$ get dest field index ( ) @$ dest obj @$ true ) ) ) { prepare target object result result = prepare target object ( dest obj ) ; target object = result . target object ; prop descriptor = result . prop descriptor ; } else { prop descriptor = super . get dest property descriptor ( dest obj . get class ( ) ) ; },need object,success,pre
perform placeholder substitution @$ this is useful when the library is <PLACE_HOLDER> a placeholder in a key element @$ we however do not need to record these substitutions so feed it with a fake merging report .,merging report . builder builder = new merging report . builder ( merging report builder . get logger ( ) ) ; builder . get action recorder ( ) . record default node action ( library document . get root node ( ) ) ; perform place holder substitution ( manifest info @$ library document @$ builder ) ; if ( builder . has errors ( ) ) { builder . build ( ) . log ( m logger ) ; },library reusing,fail,pre
this interface @$ a default implementation is supplied which <PLACE_HOLDER> the old method . all new implementations must override this interface and should not use the other add response time method .,int queue time ms = ( int ) details . get ( processing details . timing . queue @$ time unit . milliseconds ) ; int processing time ms = ( int ) details . get ( processing details . timing . processing @$ time unit . milliseconds ) ; add response time ( call name @$ schedulable . get priority level ( ) @$ queue time ms @$ processing time ms ) ;,which calls,fail,pre
does unaltered block still <PLACE_HOLDER> ref block ?,bos . reset ( ) ; bs ref . serialize ( b ref @$ bos ) ; ser deser ( bs @$ b1 @$ bos . to byte array ( ) @$ null @$ null ) ;,block contain,fail,pre
complete task a which should <PLACE_HOLDER> another instance,cmmn runtime service . trigger plan item instance ( get plan item instance id by name and state ( plan item instances @$ __str__ @$ active ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active @$ waiting_for_repetition ) ; assert plan item instance state ( plan item instances @$ __str__ @$ available ) ; assert plan item instance state ( plan item instances @$ __str__ @$ available ) ;,which start,success,pre
shutdown the client factory which will <PLACE_HOLDER> the channel pool manager factory sharing connecion shutdown,future callback < none > shutdown callback = new future callback < > ( ) ; client factory . shutdown ( shutdown callback ) ; shutdown callback . get ( __num__ @$ time unit . seconds ) ;,which trigger,success,pre
no previous session or invalid @$ <PLACE_HOLDER> this one,if ( session == null || ! is valid ( session ) ) { requested session id = id ; session = s ; } else { if ( s != null && is valid ( s ) ) throw new bad message exception ( __str__ + requested session id + __str__ + id ) ; },session prefer,fail,pre
but if we are deserializing an exception too many received versions <PLACE_HOLDER> a bitset anyway .,long delta = next version - previous version ; if ( use tree sets for testing || ( delta > rvv_max_bitset_span && initial exception count * __num__ < delta ) ) { return new rvv exceptiont ( previous version @$ next version ) ; } return new rvv exceptionb ( previous version @$ next version ) ;,versions provide,fail,pre
set up duplicate counter . expect exactly the responses corresponding to needs repair . these may @$ or may not @$ <PLACE_HOLDER> the local site .,list < long > expectedhs ids = new array list < long > ( needs repair ) ; duplicate counter counter = new duplicate counter ( host messenger . valhalla @$ message . get txn id ( ) @$ expectedhs ids @$ message @$ m_mailbox . geths id ( ) ) ; final duplicate counter key dc key = new duplicate counter key ( message . get txn id ( ) @$ message . get sp handle ( ) ) ; update or add duplicate counter ( dc key @$ counter ) ; m_unique id generator . update most recently generated unique id ( message . get unique id ( ) ) ;,these include,success,pre
internal state updated @$ now <PLACE_HOLDER> the view hierarchy .,m view . dispatch moved to display ( m display @$ config ) ;,updated move,fail,pre
check for loops in the chain . if there are repeated certs @$ the set of certs in the chain will <PLACE_HOLDER> fewer certs than the chain,set < certificate > set = new hash set < > ( arrays . as list ( cert chain ) ) ; return set . size ( ) == cert chain . length ;,certs contain,success,pre
stores all <PLACE_HOLDER> style maps from the container,if ( container . get style map ( ) != null ) { super . assign style map ( container . get style map ( ) @$ get styles renderer ( ) ) ; },stores found,success,pre
this check <PLACE_HOLDER> node lengths to work correctly . if we 're not in ide mode @$ we do n't have that information @$ so just skip the check .,if ( length == __num__ ) { return ; },check forces,fail,pre
refresh store files <PLACE_HOLDER> compaction @$ this should not open already compacted files,hr1 . refresh store files ( true ) ; int num regions before split = admin . get regions ( test_table ) . size ( ) ;,files post,success,pre
app engine does n't <PLACE_HOLDER> thread renaming @$ so do n't even try,if ( is app engine ( ) ) { return executor ; },engine support,success,pre
the query delegate does n't <PLACE_HOLDER> sorting .,query delegate . set order by ( sorters ) ;,delegate maintain,fail,pre
move out one step @$ if the current instance <PLACE_HOLDER> an outer link,member definition outer member = thisc . find outer member ( ) ; if ( outer member == null ) { thise = null ; continue ; },instance has,success,pre
change the reference to new topology . so everyone who <PLACE_HOLDER> old version will see a consistent snapshot .,nodes = updated top ;,who captured,success,pre
if we have still have disabling services then we may have a stuck service so <PLACE_HOLDER> an exception,if ( service states . get disabling ( ) > __num__ ) { if ( should print ( properties ) ) { print services still disabling ( flow client @$ pg id ) ; } throw new command exception ( __str__ ) ; },service throw,success,pre
we do not yet know what these numbers <PLACE_HOLDER> @$ so giving them meaningless names for now .,md signed encoded number a @$ b @$ c @$ name num ; switch ( code ) { case __str__ : { code = dmang . get and increment ( ) ; switch ( code ) { case __str__ : name num = new md signed encoded number ( dmang ) ; name num . parse ( ) ; name = name num . to string ( ) ; break ; case __str__ : { dmang . push modifier context ( ) ; md object object = new md objectcpp ( dmang ) ; object . parse ( ) ; dmang . pop context ( ) ; string builder builder = new string builder ( ) ; object . insert ( builder ) ; dmang . insert string,numbers are,success,pre
check that the name <PLACE_HOLDER> the found ones,for ( object name found : found names ) { if ( name . apply ( found ) ) { result . add ( found ) ; } },name matches,success,pre
if the last marker instruction encountered was an outlineable chunk end @$ it means that the current instruction <PLACE_HOLDER> the end of a chunk that contained child chunks . those children might need to be examined below in case they are better candidates for outlining than the current chunk .,if ( ! open chunk at curr level ) { nested sub chunks = curr level chunks ; curr level chunks = ( array list ) sub chunk stack . pop ( ) ; },instruction marks,success,pre
the limit and offset <PLACE_HOLDER> the first and last groups @$ leaving 2 groups of 1 and 1 group of 2 .,validate table of longs ( client @$ __str__ + tb + __str__ @$ new long [ ] [ ] { { __num__ @$ __num__ } @$ { __num__ @$ __num__ } } ) ;,limit compare,fail,pre
attempting to create transform a second time with the same sp is should <PLACE_HOLDER> an error ...,try { m ip sec service . create transform ( ip sec config @$ new binder ( ) @$ __str__ ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { },time throw,success,pre
not enough space to <PLACE_HOLDER> the whole content @$ we will try again later .,if ( offset == limits . iov_max || ! iov array . add ( buf @$ index @$ len ) ) { return false ; },space hold,success,pre
no exception <PLACE_HOLDER> verification passed,return true ;,exception changed,fail,pre
if limit specified and is smaller than reasonable large batch size then we <PLACE_HOLDER> batch size to be the same as limit @$ but negative @$ this <PLACE_HOLDER> cursor to close right after result is sent,if ( limit <= large_batch_size ) { cursor . batch size ( - limit ) ; },size set,fail,pre
set <PLACE_HOLDER> 0 default <PLACE_HOLDER>port .,bgfx_set_view_rect ( __num__ @$ __num__ @$ __num__ @$ width @$ height ) ;,set apply,fail,pre
ensure every key <PLACE_HOLDER> a value and substitute keys for values,if ( ivs . size ( ) > __num__ ) { int template key count = __num__ ; for ( interpolation variable variable : ivs . key set ( ) ) { string value = row data provider . get template key value ( variable . get var name ( ) ) ; if ( ! __str__ . equals ( value ) ) { template key count ++ ; } } if ( action instanceof free form line ) { add action = template key count == ivs . size ( ) ; } else if ( template key count > __num__ ) { add action = true ; } },key has,success,pre
finally @$ the consumer operation <PLACE_HOLDER> formatted log output,log . info ( string . format ( __str__ @$ kw . get key ( ) . get key ( ) @$ kw . get value ( ) ) ) ;,operation logs,fail,pre
<PLACE_HOLDER> unsafe access impl.unsafe <PLACE_HOLDER> int intrinsified,registration r = new registration ( invocation plugins @$ my unsafe access . class ) ; truffle graph builder plugins . register unsafe load store plugins ( r @$ false @$ null @$ java kind . int ) ; super . register invocation plugins ( invocation plugins ) ;,impl.unsafe test,fail,pre
if the user <PLACE_HOLDER> a custom view for the tab indicators @$ then do not draw the bottom strips .,if ( ! m draw bottom strips ) { return ; },user specified,success,pre
check the rowcount value @$ if it is equal to 0 it means the call did not <PLACE_HOLDER> the application from federation state store,if ( cstmt . get int ( __num__ ) == __num__ ) { string err msg = __str__ + request . get application id ( ) + __str__ ; federation state store utils . log and throw store exception ( log @$ err msg ) ; },call reload,fail,pre
if it 's a virtual service then <PLACE_HOLDER> the data from parent service,if ( object . get virtual ( ) == __num__ ) { rec data = get data from service ( object . get parent ( ) ) ; } else { rec data = get data from service ( service ) ; },service get,fail,pre
copy is necessary since the instance info builder just <PLACE_HOLDER> the original reference @$ and we do n't want to corrupt the global eureka copy of the object which may be used by other clients in our system,instance info copy = new instance info ( ii ) ; if ( is secure ) { ii = new instance info . builder ( copy ) . set secure port ( override port ) . build ( ) ; } else { ii = new instance info . builder ( copy ) . set port ( override port ) . build ( ) ; },builder holds,fail,pre
should not <PLACE_HOLDER> exception,handler . on throwable ( e ) ;,not throw,success,pre
<PLACE_HOLDER> scores should not <PLACE_HOLDER> any caches since they are both unregistered,m network score service . update scores ( new scored network [ ] { scored_network } ) ;,scores update,success,pre
do n't use delayed clipboard rendering for the transferable 's data . if we did that @$ we would call transferable.<PLACE_HOLDER> transfer data on the toolkit thread @$ which is a security hole . <PLACE_HOLDER> all of the tar<PLACE_HOLDER> formats into which the transferable can be translated . then @$ for each format @$ translate the data and post it to the clipboard .,data transferer data transferer = data transferer . get instance ( ) ; long [ ] format array = data transferer . get formats for transferable as array ( contents @$ flavor map ) ; declare types ( format array @$ this ) ; map < long @$ data flavor > format map = data transferer . get formats for transferable ( contents @$ flavor map ) ; for ( map . entry < long @$ data flavor > entry : format map . entry set ( ) ) { long format = entry . get key ( ) ; data flavor flavor = entry . get value ( ) ; try { byte [ ] bytes = data transferer . get instance ( ) . translate transferable,formats read,fail,pre
reachable only if the file naming is incorrect by current standard . thus we <PLACE_HOLDER> an error instead of recording failure to uma .,if ( tries >= max_tries_allowed || tries < __num__ ) { log . e ( tag @$ __str__ + minidump file name + __str__ ) ; return ; },reachable log,success,pre
set the scrollbars value . if the thumb has reached the end of the scrollbar @$ then just set the value to its maximum . otherwise <PLACE_HOLDER> the value as accurately as possible .,if ( thumb pos == thumb max ) { if ( scrollbar . get orientation ( ) == j scroll bar . vertical || scrollbar . get component orientation ( ) . is left to right ( ) ) { scrollbar . set value ( model . get maximum ( ) - model . get extent ( ) ) ; } else { scrollbar . set value ( model . get minimum ( ) ) ; } } else { float value max = model . get maximum ( ) - model . get extent ( ) ; float value range = value max - model . get minimum ( ) ; float thumb value = thumb pos - thumb min ; float thumb range = thumb max,set convert,fail,pre
menu currently always enabled @$ but <PLACE_HOLDER>nt <PLACE_HOLDER> anything unless a message is being held,if ( breakpoint management interface . is hold message ( null ) ) { breakpoint management interface . cont ( ) ; },dont do,success,pre
and the outer struct <PLACE_HOLDER> fields that are defined to be within the footprint of the embedded struct 's trailing padding .,debug info entry intdie = add int ( cu ) ;,struct has,success,pre
set output collector for any <PLACE_HOLDER> sink operators in the pipeline .,list < operator < ? > > children = new array list < > ( ) ; children . add ( reducer ) ; children . add all ( dummy ops ) ; create output map ( ) ; operator utils . set children collector ( children @$ out map ) ; check abort condition ( ) ; reducer . set reporter ( reporter ) ; mapred context . get ( ) . set reporter ( reporter ) ;,collector reduce,success,pre
let 's <PLACE_HOLDER> this thread and wait for a primary !,synchronized ( this ) { stop watch timer = new stop watch ( true ) ; long warn time = get distribution manager ( ) . get config ( ) . get ack wait threshold ( ) * __num__ ; boolean logged warning = false ; try { for ( ; ; ) { get advisee ( ) . get cancel criterion ( ) . check cancel in progress ( null ) ; final internal cache cache = get bucket ( ) . get cache ( ) ; if ( cache != null && cache . is cache at shutdown all ( ) ) { throw cache . get cache closed exception ( __str__ ) ; } if ( get bucket redundancy ( ) == - __num__ ),'s park,success,pre
we expect that both nn 1 and nn 2 will <PLACE_HOLDER> some number of deletions queued up for the d ns .,banner ( __str__ ) ; block manager test util . compute invalidation work ( nn1 . get namesystem ( ) . get block manager ( ) ) ; banner ( __str__ ) ; block manager test util . compute invalidation work ( nn2 . get namesystem ( ) . get block manager ( ) ) ;,nn have,success,pre
user 1 <PLACE_HOLDER> an empty room configuration form which indicates that we want an instant room,form form = new form ( form . type_submit ) ; form field field = new form field ( __str__ ) ; field . set type ( __str__ ) ; form . add field ( field ) ; form . set answer ( __str__ @$ arrays . as list ( __str__ ) ) ; muc . send configuration form ( form ) ;,user sends,success,pre
set up the 2 d array used to hold the names . the first column <PLACE_HOLDER> the olson ids .,string [ ] [ ] result = new string [ available time zone ids . length ] [ __num__ ] ; for ( int i = __num__ ; i < available time zone ids . length ; ++ i ) { result [ i ] [ __num__ ] = available time zone ids [ i ] ; } long native start = system . current time millis ( ) ; fill zone strings ( locale . to string ( ) @$ result ) ; long native end = system . current time millis ( ) ; intern strings ( result ) ;,column contains,success,pre
update vertex positions for ' <PLACE_HOLDER> samples ' parallel circles .,float z rate = ( radius * __num__ ) / ( float ) ( z samples ) ; float z height = - radius + ( z rate / __num__ ) ; float rb = __num__ / z samples ; float b = rb / __num__ ; for ( int k = __num__ ; k < z samples ; k ++ ) { angle = __num__ ; float scale = __num__ * fast math . sqrt ( b - b * b ) ; for ( int i = __num__ ; i < samples ; i ++ ) { float x = radius * fast math . cos ( angle ) ; float y = radius * fast math . sin ( angle ) ; pb . put,' g,fail,pre
this situation is possible only if a callee <PLACE_HOLDER> an exception which type extends throwable directly,if ( cause instanceof command action execution exception ) { command action execution exception command action execution exception = ( command action execution exception ) cause ; cause = command action execution exception . get cause ( ) ; },callee throws,success,pre
one file may <PLACE_HOLDER> several names,final list < b object > l = patho . get list ( ) ;,file have,success,pre
if the app <PLACE_HOLDER> everything right @$ return without logging .,if ( app op mode == app ops manager . mode_allowed ) { return location permission result . allowed ; } else { log . i ( tag @$ query . calling package + __str__ + location type for log + __str__ + __str__ ) ; return app ops mode to permission result ( app op mode ) ; },app does,fail,pre
j rockit <PLACE_HOLDER> this exception instead of returning null as the javadocs say it should . see bug 36348,this . set collection usage unsupported ( mp ) ;,rockit throws,success,pre
confirm that the result represents a video . otherwise @$ the item will not <PLACE_HOLDER> a video id .,if ( r id . get kind ( ) . equals ( __str__ ) ) { thumbnail thumbnail = single video . get snippet ( ) . get thumbnails ( ) . get default ( ) ; system . out . println ( __str__ + r id . get video id ( ) ) ; system . out . println ( __str__ + single video . get snippet ( ) . get title ( ) ) ; system . out . println ( __str__ + thumbnail . get url ( ) ) ; system . out . println ( __str__ ) ; },item have,fail,pre
timeout . we do n't need to call ` terminate ` here @$ because wait for automatically <PLACE_HOLDER> the process in case of a timeout .,return wait result . timeout ; default :,wait kills,fail,pre
if the autocomplete whitelist does n't <PLACE_HOLDER> the key @$ skip storing its value,continue ;,whitelist contain,success,pre
if we have an image src and the target is <PLACE_HOLDER> mark the get initial image so that it will wait until the target region is fully initialized before responding to the get image request . otherwise @$ the source may respond with no data because it is still initializing @$ e.g . loading a snapshot .,cache observer holder . get instance ( ) . after markinggii started ( ) ;,target running,fail,pre
genrule <PLACE_HOLDER> legacy format,genrule output path = build target paths . get gen path ( filesystem @$ build target factory . new instance ( __str__ ) @$ __str__ ) . resolve ( __str__ ) . to string ( ) ;,genrule needs,fail,pre
we wo n't race another upgrade attempt because only one thread will <PLACE_HOLDER> the timeout from the map,integer timeout = local sessions with timeouts . remove ( session id ) ; if ( timeout != null ) { log . info ( __str__ @$ long . to hex string ( session id ) ) ; track session ( session id @$ timeout ) ; upgrading sessions . put ( session id @$ timeout ) ; local session tracker . remove session ( session id ) ; return timeout ; },thread get,success,pre
all in the same call stack @$ the upgrade codec should <PLACE_HOLDER> the message @$ written the upgrade response @$ and upgraded the pipeline .,assert true ( write upgrade message ) ; assert false ( write flushed ) ; assert null ( ctx . pipeline ( ) . get ( http server codec . class ) ) ; assert not null ( ctx . pipeline ( ) . get ( __str__ ) ) ; in read call = false ;,codec output,fail,pre
if an exception here occurs then there is the danger that urls which had been in the crawler are <PLACE_HOLDER> a second time to prevent that @$ we reject urls in these events,concurrent log . log exception ( e ) ; return __str__ + e . get message ( ) ;,urls reverted,fail,pre
copy dex <PLACE_HOLDER> reorder directly on dex,steps . add ( copy step . for file ( filesystem @$ input primary dex path @$ output primary dex path ) ) ; steps . add ( new default shell step ( filesystem . get root path ( ) @$ immutable list . of ( reorder tool . to string ( ) @$ reorder data file . to string ( ) @$ output primary dex path . to string ( ) ) ) ) ;,dex file,fail,pre
get all cookies that domain <PLACE_HOLDER> the uri,for ( map . entry < uri @$ list < http cookie > > entry : map . entry set ( ) ) { if ( uri . equals ( entry . get key ( ) ) ) { continue ; } list < http cookie > entry cookies = entry . get value ( ) ; for ( iterator < http cookie > i = entry cookies . iterator ( ) ; i . has next ( ) ; ) { http cookie cookie = i . next ( ) ; if ( ! http cookie . domain matches ( cookie . get domain ( ) @$ uri . get host ( ) ) ) { continue ; } if ( cookie . has expired ( ),domain matches,success,pre
update last <PLACE_HOLDER> tx id even in case of error @$ since some ops may have been successfully <PLACE_HOLDER> before the error .,last applied tx id = loader . get last applied tx id ( ) ;,update received,fail,pre
the second observer should only <PLACE_HOLDER> the newest value and any later values .,live data . observe ( m lifecycle owner @$ new observer < string > ( ) { @ override public void on changed ( @ nullable string s ) { output2 . add ( s ) ; } } ) ; live data . remove observer ( m observer ) ; processor . on next ( __str__ ) ; assert that ( m live data output @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ; assert that ( output2 @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ;,observer get,success,pre
group set position <PLACE_HOLDER> all the positions,final immutable bit set group set = immutable bit set . of ( group set positions ) ; list < aggregate call > aggregate calls = lists . new array list ( ) ; rel data type agg fn ret type = type converter . convert ( type info factory . long type info @$ cluster . get type factory ( ) ) ; aggregate call aggregate call = hive calcite util . create single arg agg call ( __str__ @$ cluster @$ type info factory . long type info @$ input . get row type ( ) . get field list ( ) . size ( ) @$ agg fn ret type ) ; aggregate calls . add ( aggregate call ) ; return new hive aggregate,position includes,success,pre
verify the signature .. shows the response was generated by someone who <PLACE_HOLDER> the associated private key,signature sig = signature . get instance ( sig alg . get object id ( ) . get id ( ) @$ __str__ ) ; sig . init verify ( pubkey ) ; sig . update ( content . get bytes ( ) ) ; return sig . verify ( sig bits ) ;,who has,fail,pre
indent:11 exp:12 <PLACE_HOLDER> indent:8 exp:8,return __str__ ;,exp:12 warn,success,pre
if this buffer is finished @$ notify the client of this @$ so the client will <PLACE_HOLDER> this buffer,if ( pages . is empty ( ) && no more pages ) { return empty results ( task instance id @$ current sequence id . get ( ) @$ true ) ; },client resume,fail,pre
skip block if block error <PLACE_HOLDER> no instruction address,continue ;,error has,success,pre
special case because xp <PLACE_HOLDER> no skin for menus,if ( part == part . menu ) { if ( flat menus ) { return new xp fill border ( ui manager . get color ( __str__ ) @$ __num__ ) ; } else { return null ; } },xp has,success,pre
execute process does n't wait for finishing to drain error stream if it 's configure not to redirect stream . this causes test failure when draining the error stream did n't finish fast enough before the thread of this test case method <PLACE_HOLDER> the warn msg count . so @$ this loop wait for a while until the log msg count becomes expected number,final int expected warning messages = __num__ ; final int max retry = __num__ ; for ( int i = __num__ ; i < max retry && ( runner . get logger ( ) . get warn messages ( ) . size ( ) < expected warning messages ) ; i ++ ) { try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { } } final list < log message > warn messages = runner . get logger ( ) . get warn messages ( ) ; assert equals ( __str__ + __str__ @$ expected warning messages @$ warn messages . size ( ) ) ; final list < mock flow file > succeeded = runner . get flow files,thread reports,fail,pre
3 rd parameter <PLACE_HOLDER> no affect for ownerless,test ( false @$ true @$ false ) ;,parameter has,success,pre
base condition 2 : check if we are already at the target . if so @$ <PLACE_HOLDER> an empty path .,if ( ( src node . equals ( dest node ) ) && dest dataset descriptor . contains ( src dataset descriptor ) ) { return new array list < > ( ) ; } linked list < flow edge context > edge queue = new linked list < > ( ) ; edge queue . add all ( get next edges ( src node @$ src dataset descriptor @$ dest dataset descriptor ) ) ; for ( flow edge context flow edge context : edge queue ) { this . path map . put ( flow edge context @$ flow edge context ) ; },condition return,success,pre
updates the port when the user <PLACE_HOLDER> the security type . this allows us to show a reasonable default which the user can change .,m security type view . set on item selected listener ( new adapter view . on item selected listener ( ) { @ override public void on item selected ( adapter view < ? > parent @$ view view @$ int position @$ long id ) { if ( m current security type view position != position ) { update port from security type ( ) ; validate fields ( ) ; } } @ override public void on nothing selected ( adapter view < ? > parent ) { } } ) ;,user changes,success,pre
send event notifications saying that all our buddies are offline . the protocol does not <PLACE_HOLDER> top level buddies nor subgroups for top level groups so a simple nested loop would be enough .,if ( new status . equals ( offline status ) ) { iterator < contact group > groups iter = get server stored contact list root ( ) . subgroups ( ) ; while ( groups iter . has next ( ) ) { contact group group = groups iter . next ( ) ; iterator < contact > contacts iter = group . contacts ( ) ; while ( contacts iter . has next ( ) ) { contact jabber impl contact = ( contact jabber impl ) contacts iter . next ( ) ; update contact status ( contact @$ offline status ) ; } } iterator < contact > contacts iter = get server stored contact list root ( ) . contacts ( ) ;,protocol implement,success,pre
the kings hand should not <PLACE_HOLDER> any events before he received one,verify zero interactions ( observer ) ;,hand receive,fail,pre
if the java library does n't <PLACE_HOLDER> any output @$ it does n't contribute a gwt module,if ( java library . get source path to output ( ) == null ) { return rule deps ; } build rule gwt module = graph builder . compute if absent ( java library . get build target ( ) . assert unflavored ( ) . with flavors ( java library . gwt_module_flavor ) @$ gwt module target -> { immutable sorted set < source path > files for gwt module = immutable sorted set . < source path > natural order ( ) . add all ( java library . get sources ( ) ) . add all ( java library . get resources ( ) ) . build ( ) ; immutable sorted set < build rule > deps = immutable sorted set . copy,library produce,fail,pre
portion of the skip region is verified and portion is not <PLACE_HOLDER> the verified portion,if ( verified position > get file pointer ( ) ) { input . seek ( verified position ) ; skip bytes ( pos - verified position ) ; } else { skip bytes ( pos - get file pointer ( ) ) ; },portion matcher,fail,pre
get the superclass type def . <PLACE_HOLDER> the class the class scope belongs to,try { java type definition super class = get super class type definition ( ( ( class scope ) scope ) . get class declaration ( ) . get node ( ) @$ null ) ; java type definition found type def = get field type ( super class @$ image @$ accessing class ) ; if ( found type def != null ) { return found type def ; } } catch ( class cast exception ignored ) { },def find,fail,pre
metadata files <PLACE_HOLDER> no entropy,assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( entropy_marker ) ) ) ; assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( resolved_marker ) ) ) ;,files have,success,pre
start new transaction does n't <PLACE_HOLDER> cache operations on failed channel .,try ( client transaction tx1 = client . transactions ( ) . tx start ( ) ) { fail ( ) ; } catch ( client exception expected ) { },transaction recover,success,pre
if that is first collection we split single composite key on several keys @$ each of those composite keys <PLACE_HOLDER> single item from collection,if ( ! contains collection ) for ( int i = __num__ ; i < collection size ; i ++ ) { final o composite key composite key = new o composite key ( first key . get keys ( ) ) ; composite keys . add ( composite key ) ; } else throw new o index exception ( __str__ ) ;,each contains,fail,pre
simple equals can cause troubles here because of how equals <PLACE_HOLDER> e.g . between lists and sets .,return collection utils . is equal collection ( state objects @$ that . state objects ) ;,equals handles,fail,pre
the finally clause will <PLACE_HOLDER> an error .,remove decoder ( imgd ) ; if ( thread . current thread ( ) . is interrupted ( ) || ! thread . current thread ( ) . is alive ( ) ) { error all consumers ( imgd . queue @$ true ) ; } else { error all consumers ( imgd . queue @$ false ) ; },clause throw,fail,pre
assumption : the client has already <PLACE_HOLDER> references to the given address,if ( addr . equals ( code unit address ) ) { continue ; },client made,fail,pre
media player 2 <PLACE_HOLDER> text instead of application mime types .,if ( mime types . application_cea608 . equals ( mime type ) ) { media format . set string ( media format . key_mime @$ mimetype_text_cea_608 ) ; } else if ( mime types . application_cea708 . equals ( mime type ) ) { media format . set string ( media format . key_mime @$ mimetype_text_cea_708 ) ; },media uses,success,pre
now let 's <PLACE_HOLDER> the contents of the class,for ( property node pn : get properties ( ) ) { visitor . visit property ( pn ) ; } for ( field node fn : get fields ( ) ) { visitor . visit field ( fn ) ; } for ( constructor node cn : get declared constructors ( ) ) { visitor . visit constructor ( cn ) ; } visit methods ( visitor ) ;,'s visit,success,pre
when data file <PLACE_HOLDER> more columns than header file,int extra columns = __num__ ; run import ( __str__ @$ bad . get absolute path ( ) @$ __str__ @$ integer . to string ( node ids . size ( ) * extra columns ) @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ string . value of ( config . array delimiter ( ) ) @$ __str__ + node header ( config ) . get absolute path ( ) + __str__ + node data ( false @$ config @$ node ids @$ true @$ charset . default charset ( ) @$ extra columns ) . get absolute path ( ) @$ __str__ @$ relationship header ( config ) . get absolute path ( ) + __str__ + relationship data ( false @$ config @$ node ids,file contains,success,pre
fail fast in case this structure <PLACE_HOLDER> no associated processors .,if ( this . template boundaries processors . length == __num__ ) { this . next . handle template end ( itemplate end ) ; return ; },structure has,success,pre
if the local is just being created @$ mark the store as the start of its live range . note that it might have been created by initialize variables already @$ which would have <PLACE_HOLDER> the start of the live range already .,if ( create local ) { _local . set start ( store inst ) ; } string signature = _type . to signature ( ) ;,which caused,fail,pre
only recreate the context if the sampler <PLACE_HOLDER> a decision,if ( sampled == null && ( sampled = sampler . try sample ( request ) ) != null ) { extracted = extracted . sampled ( sampled . boolean value ( ) ) ; } return extracted . context ( ) != null ? tracer . join span ( extracted . context ( ) ) : tracer . next span ( extracted ) ;,sampler made,success,pre
generate the racks <PLACE_HOLDER> rack 1 for data node,string rack1 = get rack ( __num__ @$ level ) ;,racks form,fail,pre
name has n't changed ; construct old binding <PLACE_HOLDER> name from new binding,binding old bd = new binding ( new bd . get name ( ) @$ null @$ new bd . is relative ( ) ) ; naming event e = new naming event ( event src @$ naming event . object_changed @$ new bd @$ old bd @$ new long ( changeid ) ) ; support . queue event ( e @$ naming listeners ) ;,construct extract,fail,pre
need shut down background thread gracefully @$ driver.close will <PLACE_HOLDER> background thread a cancel request is sent .,if ( should run async ( ) && state != operation state . canceled && state != operation state . timedout ) { future < ? > background handle = get background handle ( ) ; if ( background handle != null ) { boolean success = background handle . cancel ( true ) ; string query id = query state . get query id ( ) ; if ( success ) { log . info ( __str__ + query id ) ; } else if ( state == operation state . canceled ) { log . info ( __str__ + query id ) ; } } } if ( driver != null ) { driver . close ( ) ; driver . destroy ( ) ; },driver.close make,fail,pre
the decimal column vector set method will quickly <PLACE_HOLDER> the deserialized decimal writable fields .,( ( decimal column vector ) col vector ) . set ( batch index @$ deserialize read . current hive decimal writable ) ;,method copy,success,pre
since a is in the using list @$ lr.a and rr.a are the same . this is not ambiguous . the two aliases <PLACE_HOLDER> the same column .,query = __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . seqscan ) ; query = __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . projection @$ plan node type . orderby @$ plan node type . nestloop @$ plan node type . seqscan @$ plan node type . seqscan ) ; query = __str__ + __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . projection @$ plan node type . orderby @$ plan node type . seqscan ) ; query = __str__ ; compile to top down tree,aliases denote,fail,pre
non existing offset will be <PLACE_HOLDER> last offset,new input split = new kafka input split ( tp . topic ( ) @$ tp . partition ( ) @$ existing input split . get end offset ( ) @$ existing input split . get end offset ( ) @$ existing input split . get path ( ) ) ;,non inserting,fail,pre
check the v 8 <PLACE_HOLDER> no co location hints,execution vertex [ ] v8s = eg . get job vertex ( v8 . getid ( ) ) . get task vertices ( ) ; for ( int i = __num__ ; i < v8 . get parallelism ( ) ; i ++ ) { assert null ( v8s [ i ] . get location constraint ( ) ) ; } e . print stack trace ( ) ; fail ( e . get message ( ) ) ;,8 has,success,pre
wait for monitoring interval time and verify server is still in <PLACE_HOLDER> mode,resume and wait ( monitoring_interval + __num__ ) ; assert equals ( operation mode . running @$ voltdb . instance ( ) . get mode ( ) ) ;,server running,success,pre
change offset to 1 @$ preview should <PLACE_HOLDER> string at offset 1,set offset field value ( __num__ ) ; assert equals ( __str__ @$ preview text field . get text ( ) ) ;,preview show,success,pre
if requested id <PLACE_HOLDER> invalid characters @$ then sso can not exist and would otherwise cause sso lookup to fail,try { base64 . get url decoder ( ) . decode ( id ) ; } catch ( illegal argument exception e ) { return null ; } batcher < batch > batcher = this . manager . get batcher ( ) ;,id contains,success,pre
now make all calls to the fake group mapper <PLACE_HOLDER> exceptions,fake group mapping . set throw exception ( true ) ;,calls throw,success,pre
prevents warning when script <PLACE_HOLDER> something that will get compiled,options . add ( __str__ ) ;,script contains,fail,pre
a volt db extension to <PLACE_HOLDER> the assume unique attribute,index = index . set assume unique ( c . assume unique ) ;,extension support,success,pre
event bus goes here . this method is only called if one of the binding <PLACE_HOLDER>rs <PLACE_HOLDER> a binding for the given 'item name ' .,if ( command instanceof on off type ) { final on off type switch command = ( on off type ) command ; final open sprinkler binding provider binding provider = find first matching binding provider ( item name @$ command ) ; final int station = binding provider . get station number ( item name ) ; if ( station < __num__ || station >= number of stations ) { logger . warn ( __str__ + station + __str__ + __num__ + __str__ + number of stations + __str__ ) ; return ; } switch ( switch command ) { case on : open sprinkler . open station ( station ) ; break ; case off : open sprinkler . close station ( station ) ; break,one provide,success,pre
we expect only 1 request @$ but we ask for 2 requests here so that if a misbehaving client <PLACE_HOLDER> more than 1 requests @$ server call will catch it . note that disabling auto inbound flow control has no effect on unary calls .,call . request ( __num__ ) ; return new unary server call listener ( response observer @$ call ) ;,client sends,success,pre
the maximum size <PLACE_HOLDER> the default max header size of 10 1024,return arrays . as list ( new object [ ] [ ] { { test type . frame_max_greater_than_header_max @$ __num__ * __num__ } @$ { test type . frame_max_less_than_header_max @$ __num__ * __num__ } @$ { test type . frame_max_less_than_action_max @$ __num__ } } ) ;,size exceeds,success,pre
create threads and make them <PLACE_HOLDER> workload concurrently .,try { workload = new workload [ num threads ] ; for ( int i = __num__ ; i < num threads ; i ++ ) { workload [ i ] = new workload ( append test util . next long ( ) @$ fs @$ i @$ number of files @$ replication @$ __num__ ) ; workload [ i ] . start ( ) ; } mod thread = new modify ( conf @$ cluster ) ; mod thread . start ( ) ; for ( int i = __num__ ; i < num threads ; i ++ ) { try { system . out . println ( __str__ + i + __str__ ) ; workload [ i ] . join ( ) ; if ( i,them run,success,pre
just read it @$ the checked input stream will <PLACE_HOLDER> the checksum on it 's own,try ( checked input stream stream = new checked input stream ( new buffered input stream ( files . new input stream ( source file . to path ( ) ) ) @$ new adler32 ( ) ) ) { io utils . skip fully ( stream @$ source file . length ( ) ) ; return stream . get checksum ( ) . get value ( ) ; } catch ( final io exception ignored ) { },stream update,success,pre
if still null then the implementation does n't <PLACE_HOLDER> a presence operation set which is unacceptable for gibberish .,if ( op set pers presence2 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ; contact group root group1 = op set pers presence1 . get server stored contact list root ( ) ;,implementation offer,success,pre
3 function executions <PLACE_HOLDER> place,function service stats function service stats = ids . get function stats manager ( ) . get function service stats ( ) ; wait no functions running ( function service stats ) ; no of execution calls_ aggregate += __num__ ; no of executions completed_ aggregate += __num__ ; assert equals ( no of execution calls_ aggregate @$ function service stats . get function execution calls ( ) ) ; assert equals ( no of executions completed_ aggregate @$ function service stats . get function executions completed ( ) ) ; function stats function stats = function stats manager . get function stats ( test function . test_function2 @$ ids ) ;,executions take,fail,pre
if media <PLACE_HOLDER> upload failed @$ let 's stop here and prompt the user,if ( m is media error ) { return upload post task result . error ; } if ( m post . get category id list ( ) . size ( ) > __num__ ) { m has category = true ; },media file,success,pre
: a blank line <PLACE_HOLDER> no files @$ so it can serve as a separator for readability .,if ( line . length ( ) == __num__ ) continue ;,line means,fail,pre
if this method did not <PLACE_HOLDER> remote exception as required @$ generate the error but continue @$ so that multiple such errors can be reported .,if ( ! has remote exception ) { env . error ( __num__ @$ __str__ @$ interface def . get name ( ) @$ member . to string ( ) ) ; errors = true ; continue next member ; },method set,fail,pre
checks whether the dashboard configuration <PLACE_HOLDER> all expected fields,get dashboard configuration ( base url ) ; hs . stop ( ) ;,configuration contains,success,pre
<PLACE_HOLDER> annotations <PLACE_HOLDER> binary annotations,assert that ( bytes ) . contains sequence ( type_list @$ __num__ @$ __num__ @$ type_struct @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) . contains sequence ( type_list @$ __num__ @$ __num__ @$ type_struct @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ;,annotations visit,fail,pre
bean may have <PLACE_HOLDER> new weak affinity,target = result . get node ( ) ; assert . assert equals ( count ++ @$ result . get value ( ) . int value ( ) ) ;,bean added,fail,pre
player <PLACE_HOLDER> all files in arbitrary directory structure and creates a map task for each file . we use ' ; ' as separator because wal file names contains ' @$ ',string dirs = string utils . join ( dir paths @$ __str__ ) ; string jobname = __str__ + backup id ; path bulk output path = get bulk output dir ( ) ; conf . set ( wal player . bulk_output_conf_key @$ bulk output path . to string ( ) ) ; conf . set ( wal player . input_files_separator_key @$ __str__ ) ; conf . set boolean ( wal player . multi_tables_support @$ true ) ; conf . set ( job_name_conf_key @$ jobname ) ; string [ ] player args = { dirs @$ string utils . join ( table list @$ __str__ ) } ; try { player . set conf ( conf ) ; int result = player . run ( player args ),player reads,success,pre
param type will <PLACE_HOLDER> null if there is no parameter,if ( param type == null ) { param type = setter . get parameter type ( __num__ ) ; },type return,fail,pre
copy all rocks <PLACE_HOLDER> sst files to local disk,string sst file list = rocks db path + __str__ + sst_file_list ; file file = new file ( sst file list ) ; list < string > sst files = file utils . read lines ( file ) ; log . debug ( __str__ @$ sst files ) ; for ( string sst file : sst files ) { dfs . copy to local ( remote db backup dir + __str__ + sst file @$ rocks db path ) ; } file utils . delete quietly ( file ) ;,rocks db,success,pre
for compatibility @$ first do the lookup and then verify the provider . this makes the difference between a nsae and a security exception if the provider does not <PLACE_HOLDER> the algorithm .,if ( provider checked == false ) { exception ve = jce security . get verification result ( provider ) ; if ( ve != null ) { string msg = __str__ + provider . get name ( ) ; throw new security exception ( msg @$ ve ) ; } provider checked = true ; },provider support,success,pre
start the java <PLACE_HOLDER> context,try ( print writer report writer = checker utils . init report file ( ) ) { spark conf conf = new spark conf ( ) . set app name ( spark integration checker . class . get name ( ) ) ; java spark context sc = new java spark context ( conf ) ; checker . print config info ( conf @$ report writer ) ; status result status = checker . run ( sc @$ report writer @$ alluxio conf ) ; checker . print result info ( result status @$ report writer ) ; report writer . flush ( ) ; system . exit ( result status . equals ( status . success ) ? __num__ : __num__ ) ; },java spark,success,pre
by design @$ the buffer must always <PLACE_HOLDER> enough space for one packet,if ( build config . debug && len > buffer . remaining ( ) ) { log . e ( tag @$ len + __str__ + buffer . remaining ( ) ) ; log . e ( tag @$ buffer . to string ( ) ) ; throw new assertion error ( __str__ ) ; } buffer . put ( b @$ off @$ len ) ; buffer . flip ( ) ; sink ( ) ; buffer . compact ( ) ;,buffer have,success,pre
the target did n't <PLACE_HOLDER> the coordinates for the reply .,handle drag reply ( action @$ x root @$ y root ) ;,target specify,success,pre
the cache should not <PLACE_HOLDER> any paths now .,assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ;,cache have,fail,pre
recurring plans will always <PLACE_HOLDER> an active cycle,if ( plan . get cycle rule ( ) . is recurring ( ) ) { return plan ; } else { final range < zoned date time > cycle = plan . cycle iterator ( ) . next ( ) ; if ( cycle . contains ( zoned date time . now ( m clock ) ) ) { return plan ; } },plans have,success,pre
and that local files always <PLACE_HOLDER> preference over remote files .,action lookup key action lookup key = new action lookup key ( ) { @ override public sky function name function name ( ) { return sky function name . for_testing ; } } ; sky key action key = action lookup data . create ( action lookup key @$ __num__ ) ; special artifact tree artifact = create tree artifact ( __str__ ) ; tree artifact . get path ( ) . create directory and parents ( ) ; map < path fragment @$ remote file artifact value > tree artifact metadata = new hash map < > ( ) ; tree artifact metadata . put ( path fragment . create ( __str__ ) @$ create remote file artifact value ( __str__ ) ) ; tree artifact,files take,fail,pre
normal case : one size <PLACE_HOLDER> both width and height greater,dimension size1 = new dimension ( __num__ @$ __num__ ) ; dimension size2 = new dimension ( __num__ @$ __num__ ) ; assert . assert equals ( __num__ @$ icon entry . get distance ( size1 @$ size2 ) @$ __num__ ) ;,size has,success,pre
ooohml <PLACE_HOLDER> the hook for disconnecting and closing cache on out of off heap memory exception,out of off heap memory listener ooohml = new disconnecting out of off heap memory listener ( ( internal distributed system ) system ) ; return basic create off heap storage ( sf @$ off heap memory size @$ ooohml ) ;,ooohml has,fail,pre
let 's <PLACE_HOLDER> sanity check ; easier to spot buggy handlers,if ( instantiator == null ) { ctxt . report bad type definition ( bean desc @$ __str__ @$ insts . get class ( ) . get name ( ) ) ; },'s do,success,pre
make sure <PLACE_HOLDER> the son before <PLACE_HOLDER> the parent .,for ( int i = pid list . size ( ) - __num__ ; i >= __num__ ; i -- ) { string ppid = pid list . get ( i ) ; if ( ! is pid running ( ppid ) ) { continue ; } if ( force ) { exe whole cmd ( __str__ + ppid ) ; } else { exe whole cmd ( __str__ + ppid ) ; } },son start,fail,pre
this statement <PLACE_HOLDER> a return value on the stack .,if ( save != null ) { class definition def = ctx . field . get class definition ( ) ; if ( ! have non local finally ) { local member lf = ctx . get local field ( id finally return value ) ; num = new integer ( lf . number ) ; asm . add ( where @$ opc_istore + save . get type code offset ( ) @$ num ) ; } else { switch ( ctx . field . get type ( ) . get return type ( ) . get type code ( ) ) { case tc_void : break ; case tc_double : case tc_long : asm . add ( where @$ opc_pop2 ) ; break ; default : asm,statement puts,fail,pre
kerberos if delegation token is passed from the client side @$ do not <PLACE_HOLDER> the principal,if ( matcher . group ( __num__ ) . equals ignore case ( __str__ ) && ! ( conn params . get session vars ( ) . contains key ( jdbc connection params . auth_type ) && conn params . get session vars ( ) . get ( jdbc connection params . auth_type ) . equals ignore case ( jdbc connection params . auth_token ) ) && ! ( conn params . get session vars ( ) . contains key ( jdbc connection params . auth_principal ) ) ) { conn params . get session vars ( ) . put ( jdbc connection params . auth_principal @$ matcher . group ( __num__ ) ) ; },kerberos encode,fail,pre
android just <PLACE_HOLDER> the right thing .,charset cs = charset . for name ( __str__ ) ; charset encoder e = cs . new encoder ( ) ; e . on malformed input ( cea ) ; e . on unmappable character ( cea ) ; byte buffer bb = byte buffer . allocate ( __num__ ) ; coder result cr = e . encode ( char buffer . wrap ( new char [ ] { __str__ } ) @$ bb @$ false ) ; assert equals ( coder result . underflow @$ cr ) ; assert equals ( __num__ @$ bb . position ( ) ) ; cr = e . encode ( char buffer . wrap ( new char [ ] { __str__ } ) @$ bb @$ false ) ; assert,android does,success,pre
note end of write . this does not <PLACE_HOLDER> the state of the remote fs .,write operation helper . write successful ( bytes ) ;,end change,success,pre
removing this line will <PLACE_HOLDER> the error,max log summand = math . max ( max log summand @$ log summand ) ;,line give,fail,pre
create e service which should <PLACE_HOLDER> the submitted jobs .,final runnable srv runnable = new service executor ( msg queue ) ;,which handle,fail,pre
validate our name <PLACE_HOLDER> an existing image,file topic = get help topic ( ) ; file image file = get image file ( topic @$ name ) ; finished ( topic @$ image file . get name ( ) ) ;,name represents,fail,pre
if we cant parse the listen address we have no information on how to proceed with the migration the config will <PLACE_HOLDER> the error later,default values . put ( advertised address @$ new advertised . to string ( ) ) ;,config flag,fail,pre
since the api request <PLACE_HOLDER> a unique channel section id @$ the api response should return exactly one channel section . if the response does not contain a channel section @$ then the <PLACE_HOLDER> channel section id was not found .,list < channel section > channel section list = channel section list response . get items ( ) ; if ( channel section list . is empty ( ) ) { system . out . println ( __str__ + channel section id ) ; return ; } channel section channel section = channel section list . get ( __num__ ) ;,request specified,success,pre
if the user <PLACE_HOLDER> a word highlighted and we 're updating the list @$ we want to keep the word highlighted if it 's in the updated list,string prev selected element = __str__ ; if ( main . elements to add pane . get selected value ( ) != null ) prev selected element = main . elements to add pane . get selected value ( ) ; if ( main . elements to remove table . get selected row ( ) != - __num__ ) prev selected element = ( string ) main . elements to remove table . get model ( ) . get value at ( main . elements to remove table . get selected row ( ) @$ __num__ ) ; if ( main . elements to remove table . get row count ( ) > __num__ ) main . elements to remove table . remove all elements ( ) ;,user selected,fail,pre
if an user <PLACE_HOLDER> customizations for projects he perhaps just <PLACE_HOLDER> the key value for project without a name but the code expects a name for the project . therefore we fill the name according to the project key which is the same .,for ( entry < string @$ project > entry : cfg . get projects ( ) . entry set ( ) ) { if ( entry . get value ( ) . get name ( ) == null ) { entry . get value ( ) . set name ( entry . get key ( ) ) ; } },user change,fail,pre
recursively search given subpackages . if any packages are found @$ <PLACE_HOLDER> them to the list .,if ( ! doc classes ) { map < string @$ list < java file object > > package files = search sub packages ( sub packages @$ names @$ excluded packages ) ; for ( list < string > packs = names . to list ( ) ; packs . non empty ( ) ; packs = packs . tail ) { string package name = packs . head ; parse package classes ( package name @$ package files . get ( package name ) @$ pack trees @$ excluded packages ) ; } if ( messager . nerrors ( ) != __num__ ) return null ; docenv . notice ( __str__ ) ; javadoc enter . main ( class trees . to list ( ) .,search add,success,pre
rs and datanode may <PLACE_HOLDER> different host in local machine test,if ( ! top hosts . contains ( server . get server name ( ) . get hostname ( ) ) ) { continue ; } for ( int j = __num__ ; j < server num ; j ++ ) { server name server name = cluster . get region server ( j ) . get server name ( ) ; assert true ( servers . contains ( server name ) ) ; },rs have,success,pre
we put 2 photos in album 2 ; <PLACE_HOLDER> them,assert . assert equals ( _entry res . purge ( long . value of ( __num__ ) @$ null ) @$ __num__ ) ;,photos clear,fail,pre
try to move focus to the next visible stack with a running activity if this stack is not <PLACE_HOLDER> the entire screen or is on a secondary display with no home stack .,if ( next focused stack != null ) { return m root activity container . resume focused stacks top activities ( next focused stack @$ prev @$ null ) ; },stack showing,fail,pre
below we create a ldap server which will <PLACE_HOLDER> a client request @$ authenticate it successfully ; but it will never reply to the following query request . client of this ldap server is expected to get a read timeout .,final thread ldap server = new thread ( new runnable ( ) { @ override public void run ( ) { try { try ( socket client sock = server sock . accept ( ) ) { io utils . skip fully ( client sock . get input stream ( ) @$ __num__ ) ; client sock . get output stream ( ) . write ( authenticate_success_msg ) ; fin latch . await ( ) ; } } catch ( exception e ) { e . print stack trace ( ) ; } } } ) ; ldap server . start ( ) ; final ldap groups mapping mapping = new ldap groups mapping ( ) ; string ldap url = __str__ + server sock . get local,which send,fail,pre
volt db <PLACE_HOLDER> aggregates with only one parameter,preconditions . check state ( aggr expr indexes . size ( ) < __num__ ) ; abstract expression aggr expr = null ; if ( ! aggr expr indexes . is empty ( ) ) { rel data type field field = fields . get ( aggr expr indexes . get ( __num__ ) ) ; aggr expr = rex converter . convert data type field ( field ) ; } else if ( expression type . aggregate_count == aggr type ) { aggr type = expression type . aggregate_count_star ; } assert ( aggr field idx < aggr row type . get field count ( ) ) ; apn . add aggregate ( aggr type @$ aggr call . is distinct ( ) @$ aggr field idx,db allocates,fail,pre
the service may not <PLACE_HOLDER> on first app boot . choose sane defaults,if ( m aimsicd service == null ) { toggle attack detection menu item . set checked ( false ) ; toggle cell tracking menu item . set checked ( false ) ; } else { toggle attack detection menu item . set checked ( m aimsicd service . is monitoring cell ( ) ) ; toggle cell tracking menu item . set checked ( m aimsicd service . is tracking cell ( ) ) ; } return true ;,service happen,fail,pre
management server <PLACE_HOLDER> back an eds response containing cluster load assignments for some cluster not requested .,list < any > cluster load assignments = immutable list . of ( any . pack ( build cluster load assignment ( __str__ @$ immutable list . of ( build locality lb endpoints ( __str__ @$ __str__ @$ __str__ @$ immutable list . of ( build lb endpoint ( __str__ @$ __num__ @$ health status . healthy @$ __num__ ) ) @$ __num__ @$ __num__ ) ) @$ immutable list . < policy . drop overload > of ( ) ) ) ) ; discovery response response = build discovery response ( __str__ @$ cluster load assignments @$ xds client impl . ads_type_url_eds @$ __str__ ) ; response observer . on next ( response ) ;,server sends,success,pre
running a combiner over the result . the combiner 's accumulator would be the state we use below . however @$ combiners can not <PLACE_HOLDER> intermediate results @$ thus we need to wait for the pending reduce fn api .,person existing person = person state . read ( ) ; if ( existing person != null ) { for ( auction new auction : c . element ( ) . get value ( ) . get all ( nexmark query util . auction_tag ) ) { new auction counter . inc ( ) ; new old output counter . inc ( ) ; c . output ( kv . of ( new auction @$ existing person ) ) ; } return ; },combiners produce,fail,pre
child left tuple is now null @$ so the next check will <PLACE_HOLDER> matches for new bucket,child left tuple = next child ;,check attempt,success,pre
request a container for am 2 @$ will <PLACE_HOLDER> a container on nm 1,am2 . allocate ( __str__ @$ __num__ @$ __num__ @$ new array list < > ( ) ) ; cs . handle ( new node update scheduler event ( rm . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ) ) ; response = r . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . accept ( media type . application_json ) . get ( client response . class ) ; assert equals ( media type . application_json_type + __str__ + jetty utils . utf_8 @$ response . get type ( ) . to string ( ) ) ; json = response . get entity,container reserve,success,pre
triggering the task should <PLACE_HOLDER> the child case instance,cmmn runtime service . trigger plan item instance ( plan item instance . get id ( ) ) ; assert equals ( __num__ @$ cmmn runtime service . create case instance query ( ) . count ( ) ) ; assert equals ( __str__ @$ cmmn rule . get cmmn history service ( ) . create historic variable instance query ( ) . case instance id ( case instance . get id ( ) ) . variable name ( __str__ ) . single result ( ) . get value ( ) ) ; remove all deployments ( ) ;,task start,success,pre
else only the latest could <PLACE_HOLDER> changed param info & we <PLACE_HOLDER> those changes .,if ( latest changed param sig ) { if ( my changed param sig || my changed param info || my changed return ) { save function detail conflict ( functions @$ func_signature ) ; } } else if ( my changed param sig ) { if ( latest changed param info || latest changed return ) { save function detail conflict ( functions @$ func_signature ) ; } else { get merge my ( ) . replace function parameters ( entry @$ monitor ) ; } },only have,success,pre
ok @$ pop jar open & <PLACE_HOLDER> md 5,input stream is = null ; try { is = new file input stream ( jarpath ) ; message digest md5 = message digest . get instance ( __str__ ) ; byte [ ] buf = new byte [ __num__ ] ; int pos ; while ( ( pos = is . read ( buf ) ) > __num__ ) md5 . update ( buf @$ __num__ @$ pos ) ; return md5 . digest ( ) ; } catch ( io exception | no such algorithm exception e ) { log . err ( e ) ; } finally { try { if ( is != null ) is . close ( ) ; } catch ( io exception ignore ) { } },& load,fail,pre
you may not call the <PLACE_HOLDER> method in a sub<PLACE_HOLDER>er .,throw new doclet abort exception ( __str__ ) ;,the get,fail,pre
searches started from a folder list activity will <PLACE_HOLDER> an account @$ but no folder,if ( app data . get string ( extra_search_folder ) != null ) { search . add allowed folder ( app data . get string ( extra_search_folder ) ) ; },searches have,fail,pre
otherwise @$ throw file already <PLACE_HOLDER> exception @$ which means the file owner is dead,removable = true ;,file found,fail,pre
the touch was not on the end button so the touch highlight should <PLACE_HOLDER> everything except the end button .,return m overlay panel . get content view width px ( ) - m end button width - get divider line width ( ) ;,highlight cover,success,pre
all key input columns are repeating . <PLACE_HOLDER> key once . lookup once . since the key is repeated @$ we must use entry 0 regardless of selected in use .,if ( all key input columns repeating ) { join util . join result join result ; if ( ! join col vector . no nulls && join col vector . is null [ __num__ ] ) { join result = join util . join result . nomatch ; } else { byte [ ] key bytes = vector [ __num__ ] ; int key start = start [ __num__ ] ; int key length = length [ __num__ ] ; join result = hash multi set . contains ( key bytes @$ key start @$ key length @$ hash multi set results [ __num__ ] ) ; } if ( log . is debug enabled ( ) ) { log . debug ( class_name + __str__ +,input generate,success,pre
the child case instance <PLACE_HOLDER> the plan item instance id as callback id stored . when the child case instance is finished @$ the plan item of the parent case needs to be triggered .,if ( case instance state . terminated . equals ( callback data . get new state ( ) ) || case instance state . completed . equals ( callback data . get new state ( ) ) ) { command context command context = command context util . get command context ( ) ; plan item instance entity plan item instance entity = command context util . get plan item instance entity manager ( command context ) . find by id ( callback data . get callback id ( ) ) ; if ( plan item instance entity != null ) { command context util . get agenda ( command context ) . plan trigger plan item instance operation ( plan item instance entity ) ; } },instance has,success,pre
second <PLACE_HOLDER> the first sub packet . get the value of the piggyback associated with the erased location,if ( is direct ) { int idx to write = __num__ ; do decode by piggy back ( inputs [ __num__ ] @$ tmp outputs [ __num__ ] [ idx to write ] @$ piggy back @$ erased location to fix ) ; } else { byte buffer buffer ; byte [ ] [ ] [ ] new inputs = new byte [ get sub packet size ( ) ] [ inputs [ __num__ ] . length ] [ ] ; int [ ] [ ] input offsets = new int [ get sub packet size ( ) ] [ inputs [ __num__ ] . length ] ; byte [ ] [ ] [ ] new outputs = new byte [ get sub packet size ( ),second process,fail,pre
noinspection <PLACE_HOLDER> allocation in loop,bucket = new osb tree bucketv1 < > ( cache entry ) ; if ( item index == osb tree bucketv1 . max_page_size_bytes + __num__ ) { item index = bucket . size ( ) - __num__ ; },noinspection object,success,pre
make sure we do n't accidently overwrite this transformation so we 'll remove the filename and object id <PLACE_HOLDER> the name so the users sees it 's a result,if ( step meta . get step meta interface ( ) instanceof meta inject meta ) { trans meta . set filename ( null ) ; trans meta . set object id ( null ) ; string append name = __str__ + base messages . get string ( pkg @$ __str__ ) + __str__ ; if ( ! trans meta . get name ( ) . ends with ( append name ) ) { trans meta . set name ( trans meta . get name ( ) + append name ) ; } },id replace,fail,pre
as last resort <PLACE_HOLDER> the retriever @$ cache it and return it,internal vertex retrieve vertex = retriever . get ( vertex id ) ; cache . put ( vertex id @$ retrieve vertex ) ; return retrieve vertex ;,resort pull,fail,pre
register before completion callback the before causes this thread to wait until the reaper thread <PLACE_HOLDER> our transaction,final session implementor session = entity manager . unwrap ( session implementor . class ) ; session . get action queue ( ) . register process ( new before callback completion handler ( ) ) ; testing jta platform impl . transaction manager ( ) . commit ( ) ;,thread commit,fail,pre
make sure platform can <PLACE_HOLDER> colon,if ( replaced value . contains ( __str__ ) ) { try { file tmp = file . create temp file ( __str__ @$ __str__ ) ; tmp . delete ( ) ; } catch ( io exception e ) { throw new invalid builds dir ( new builds dir value + __str__ ) ; } },platform handle,success,pre
window has been removed or hidden ; no draw will now <PLACE_HOLDER> @$ so stop waiting .,if ( win . m removed || ! win . m has surface || ! win . is visible by policy ( ) ) { if ( debug_screen_on ) slog . w ( tag_wm @$ __str__ + win ) ; m waiting for drawn . remove ( win ) ; } else if ( win . has drawn lw ( ) ) { if ( debug_screen_on ) slog . d ( tag_wm @$ __str__ + win ) ; m waiting for drawn . remove ( win ) ; },draw trigger,fail,pre
convert multimap to a map @$ because every key should <PLACE_HOLDER> only one value .,immutable map . builder < string @$ path > builder = immutable map . builder ( ) ; for ( map . entry < string @$ path > entry : multimap . entries ( ) ) { builder . put ( entry ) ; } return builder . build ( ) ;,key have,success,pre
disable the native input method so that the other input method could <PLACE_HOLDER> the input focus .,disable input method ( ) ; if ( need resetxic ) { resetxic ( ) ; need resetxic client . clear ( ) ; need resetxic = false ; },method capture,fail,pre
a locally generated change should always <PLACE_HOLDER> a later timestamp than one received from a wan gateway @$ so fake a timestamp if necessary,if ( time <= stamp . get version time stamp ( ) && dsid != tag . get distributed system id ( ) ) { time = stamp . get version time stamp ( ) + __num__ ; } tag . set version time stamp ( time ) ; tag . set distributed system id ( dsid ) ;,change have,success,pre
audit sql dao for example wo n't <PLACE_HOLDER> entity sql dao,if ( ! ( type instanceof java . lang . reflect . parameterized type ) ) { return null ; },dao support,fail,pre
when reader <PLACE_HOLDER> multiple stmts @$ set it to indicate we are in batch mode .,boolean is batch = false ;,reader contains,success,pre
undocumented feature alert ! microsoft compilers will sometimes <PLACE_HOLDER> field sig references in this table @$ too @$ despite that not fitting the iso standard they wrote themselves .,if ( cli sig field . is field sig ( sig blob ) ) { sig = new cli sig field ( sig blob ) ; } else { sig = new cli sig stand alone method ( sig blob ) ; },compilers put,success,pre
must ensure that all names <PLACE_HOLDER> not conflict and that variable types are resolved to this program so that they have the proper sizes,list < variable > cloned params = new array list < > ( ) ; for ( int i = __num__ ; i < new params . size ( ) ; i ++ ) { variable p = new params . get ( i ) ; if ( ! use custom storage && ( p instanceof auto parameter impl ) ) { continue ; } if ( p . is unique variable ( ) ) { throw new illegal argument exception ( __str__ ) ; } check for parameter name conflict ( p @$ new params @$ non param names ) ; cloned params . add ( get resolved variable ( p @$ false @$ ! use custom storage ) ) ; } new params = cloned params,names do,success,pre
api version 8 has package info @$ 10 has <PLACE_HOLDER> apk . 9 @$ i do n't know .,class < ? > loaded apk class ; try { loaded apk class = class . for name ( __str__ ) ; } catch ( class not found exception e ) { loaded apk class = class . for name ( __str__ ) ; } res dir = find field ( loaded apk class @$ __str__ ) ; packages filed = find field ( activity thread @$ __str__ ) ; if ( build . version . sdk_int < __num__ ) { resource packages filed = find field ( activity thread @$ __str__ ) ; },10 loaded,success,pre
json payload did not <PLACE_HOLDER> expected format,try ( response response = client . new call ( json request ) . execute ( ) ) { assert equals ( http response status . bad_request . code ( ) @$ response . code ( ) ) ; },payload match,success,pre
verify that the subprocess <PLACE_HOLDER> no system.out or system.err output .,string out string = out . to string ( ) ; string err string = err . to string ( ) ; system . err . println ( __str__ ) ; system . err . print ( out ) ; system . err . println ( __str__ ) ; system . err . print ( err ) ; system . err . println ( __str__ ) ; if ( out string . length ( ) > __num__ || err string . length ( ) > __num__ ) { throw new error ( __str__ ) ; } system . err . println ( __str__ ) ;,subprocess has,fail,pre
has to be a concurrent hash map because tests might <PLACE_HOLDER> this service concurrently via contains job,job leader services = new concurrent hash map < > ( __num__ ) ; state = job leader service . state . created ; owner address = null ; rpc service = null ; high availability services = null ; job leader listener = null ;,tests remove,fail,pre
the file had bad characters in the name that the filesystem does n't like . <PLACE_HOLDER> thru with null value @$ caller will get original file back as result and they can deal with io exception errors when they try to use it .,string case sensitive name = case sensitive file . get name ( ) ; return ( canonical name != null ) && canonical name . equals ignore case ( case sensitive name ) && ! canonical name . equals ( case sensitive name ) ? null : case sensitive file ;,file pass,fail,pre
the method should still be present @$ just not public @$ let 's try <PLACE_HOLDER> methods,actual method = actual test instance . get class ( ) . get declared method ( get method ( ) . get name ( ) @$ convert totccl ( get method ( ) . get parameter types ( ) ) ) ; actual method . set accessible ( true ) ;,try allow,fail,pre
test parallelism in detached mode @$ should <PLACE_HOLDER> parallelism default,string [ ] parameters = { __str__ @$ __str__ @$ get test jar path ( ) } ; verify cli frontend ( get cli ( configuration ) @$ parameters @$ __num__ @$ true ) ;,parallelism use,success,pre
need to close before setting the flag since the close function itself may <PLACE_HOLDER> rebalance callback that needs the consumer to be open still,if ( ! closed ) { close ( timeout . to millis ( ) @$ false ) ; },itself trigger,success,pre
for backward compatibility with old coprocessor service impl which do n't <PLACE_HOLDER> region coprocessor .,if ( coprocessor service . class . is assignable from ( impl class ) ) { coprocessor service cs ; cs = impl class . as subclass ( coprocessor service . class ) . get declared constructor ( ) . new instance ( ) ; return new coprocessor service backward compatiblity . region coprocessor service ( cs ) ; } else { log . error ( __str__ @$ impl class . get name ( ) @$ coprocessor host . region_coprocessor_conf_key ) ; return null ; },which extend,success,pre
pdx @$ alias @$ <PLACE_HOLDER> queries,return new object [ ] { new object [ ] { __str__ @$ true } @$ new object [ ] { __str__ @$ false } @$ new object [ ] { __str__ @$ true } @$ new object [ ] { __str__ @$ false } } ;,pdx nested,success,pre
key which <PLACE_HOLDER> the start of a record,string record key = null ;,which indicates,success,pre
sometime @$ the dataset <PLACE_HOLDER> duplicate samples . if the distances are same @$ we sort by the sample index .,return d == __num__ ? index - o . index : d ;,dataset contains,success,pre
transient status bar is not allowed if status bar is on lockscreen or status bar is <PLACE_HOLDER> the navigation keys from the user .,if ( ( m force status bar from keyguard || status bar forces showing navigation ) && m status bar controller . is transient showing ( ) ) { m status bar controller . update visibility lw ( false @$ m last system ui flags @$ m last system ui flags ) ; },bar handling,fail,pre
to avoid changing the output path of binaries built without a flavor @$ we 'll default to no flavor @$ which implicitly <PLACE_HOLDER> the default platform .,return immutable sorted set . of ( ) ;,which uses,fail,pre
apply the changes the apply will <PLACE_HOLDER> the undefined at 0 xd @$ and param size will become 0 x 5 .,invoke ( apply action ) ; assert equals ( __str__ @$ model . get status ( ) ) ; stack = function . get stack frame ( ) ; assert equals ( __num__ @$ stack . get frame size ( ) ) ; assert equals ( __num__ @$ stack . get parameter size ( ) ) ; assert equals ( __num__ @$ stack model . get frame size ( ) ) ; assert equals ( __num__ @$ stack model . get parameter size ( ) ) ; close editor ( ) ;,apply put,fail,pre
should not <PLACE_HOLDER> : groups on custom type,group ds . max by ( __num__ ) ;,not work,success,pre
trigger source info refresh for lazy source and check that the timeline now <PLACE_HOLDER> all information for all windows .,test runner . run on playback thread ( ( ) -> lazy sources [ __num__ ] . set new source info ( create fake timeline ( __num__ ) @$ null ) ) ; timeline = test runner . assert timeline change blocking ( ) ; timeline asserts . assert period counts ( timeline @$ __num__ @$ __num__ ) ; timeline asserts . assert window tags ( timeline @$ __num__ @$ __num__ ) ; timeline asserts . assert window is dynamic ( timeline @$ false @$ false ) ; test runner . assert prepare and release all periods ( ) ; test runner . assert completed manifest loads ( __num__ @$ __num__ ) ; assert completed all media period loads ( timeline ) ;,timeline contains,success,pre
there must be 2 keys 42 @$ 43 registered for the watermark <PLACE_HOLDER> all the seen elements must be in the priority queues but no nfa yet .,assert equals ( __num__ @$ harness . num event time timers ( ) ) ; assert equals ( __num__ @$ operator . getpq size ( __num__ ) ) ; assert equals ( __num__ @$ operator . getpq size ( __num__ ) ) ; assert true ( ! operator . has non empty shared buffer ( __num__ ) ) ; assert true ( ! operator . has non empty shared buffer ( __num__ ) ) ; harness . process watermark ( new watermark ( __num__ ) ) ; verify watermark ( harness . get output ( ) . poll ( ) @$ long . min_value ) ; verify watermark ( harness . get output ( ) . poll ( ) @$ __num__ ) ;,keys file,fail,pre
the area of the html content is absolute inside of the entire document . however @$ the user is <PLACE_HOLDER> the document inside of a scroll pane . so @$ we want the offset of the element within the viewer @$ not the absolute position .,area . y -= view position . y ;,user touching,fail,pre
basic internal frame ui creates an action with the same name @$ we override it as motif internal frame title pane <PLACE_HOLDER> a title pane ivar that shadows the title pane ivar in basic internal frame ui @$ making supers action throw an npe for us .,if ( map != null ) { map . put ( __str__ @$ new abstract action ( ) { public void action performed ( action event e ) { title pane . show system menu ( ) ; } public boolean is enabled ( ) { return is key binding active ( ) ; } } ) ; },pane has,success,pre
set the choice to 0 x 01 if the sender should <PLACE_HOLDER> the 0 and 1 messages,if ( choice bit ^ choices . get bit ( offset @$ false ) == true ) { switch bit [ __num__ ] = __num__ ; } network . send ( resources . get other id ( ) @$ switch bit ) ;,sender send,fail,pre
if the bounds are currently frozen @$ it means that the layout size that the app sees and the bounds we <PLACE_HOLDER> this window to might be different . in order to avoid holes @$ we simulate that we are still resizing so the app fills the hole with the resizing background .,return ( get display content ( ) . m divider controller locked . is resizing ( ) || m app token != null && ! m app token . m frozen bounds . is empty ( ) ) && ! task . in freeform windowing mode ( ) && ! is gone for layout lw ( ) ;,size expect,fail,pre
the number of unenqueued bytes that the decoder thread <PLACE_HOLDER> track of .,int unqueued bytes = stream . get unqueued buffer bytes ( ) ;,thread keeps,success,pre
1 means asc @$ could really <PLACE_HOLDER> enum here in the thrift if,sort order = collections . singleton list ( __num__ ) ;,means use,success,pre
is no inheritance on constructors @$ so reloaded superclasses can <PLACE_HOLDER> method lookup in the same way .,class < ? > clazz = c . get declaring class ( ) ; reloadable type rtype = get reloadable type if has been reloaded ( clazz ) ; if ( rtype == null ) { c = as accessible constructor ( c @$ true ) ; return c . new instance ( params ) ; } else { boolean ctor changed = rtype . get live version ( ) . has constructor changed ( utils . to constructor descriptor ( c . get parameter types ( ) ) ) ; if ( ! ctor changed ) { c = as accessible constructor ( c @$ true ) ; return c . new instance ( params ) ; } as accessible constructor ( c @$ false ) ;,superclasses handle,fail,pre
null default <PLACE_HOLDER> messages .,assert null ( dpm . get long support message ( admin1 ) ) ; assert null ( dpm . get short support message ( admin1 ) ) ; m context . binder . calling uid = dpm mock context . system_uid ; assert null ( dpm . get short support message for user ( admin1 @$ dpm mock context . caller_user_handle ) ) ; assert null ( dpm . get long support message for user ( admin1 @$ dpm mock context . caller_user_handle ) ) ; m mock context . binder . calling uid = dpm mock context . caller_uid ;,default protected,fail,pre
rpc lit can not <PLACE_HOLDER> repeated element in wrapper,if ( parent . get binding ( ) . is rpc lit ( ) || wrapper == null ) return null ;,lit contain,fail,pre
the application can <PLACE_HOLDER> its event dispatching mechanism .,reactor = new nio reactor ( dispatcher ) ;,application implement,fail,pre
json does not <PLACE_HOLDER> a list of all columns for replica identity default,object [ ] values = new object [ columns without toasted . size ( ) < schema columns . size ( ) ? schema columns . size ( ) : columns without toasted . size ( ) ] ; final set < string > undelivered toastable columns = new hash set < > ( schema . get toastable columns for table id ( table . id ( ) ) ) ; for ( replication message . column column : columns ) { final string column name = strings . unquote identifier part ( column . get name ( ) ) ; undelivered toastable columns . remove ( column name ) ; int position = get position ( column name @$ table @$ values ) ; if ( position,json provide,fail,pre
druid itself does n't explictly handle options requests @$ no resource handler will authorize such requests . so this filter <PLACE_HOLDER> all options requests and authorizes them .,if ( http method . options . equals ( http req . get method ( ) ) ) { if ( http req . get attribute ( auth config . druid_authentication_result ) == null ) { if ( allow unauthenticated http options ) { http req . set attribute ( auth config . druid_authentication_result @$ new authentication result ( auth config . allow_all_name @$ auth config . allow_all_name @$ null @$ null ) ) ; } else { ( ( http servlet response ) response ) . send error ( http servlet response . sc_unauthorized ) ; } } http req . set attribute ( auth config . druid_authorization_checked @$ true ) ; },filter handles,fail,pre
small retry number can <PLACE_HOLDER> up the failed tests .,util . get configuration ( ) . set int ( h constants . hbase_client_retries_number @$ __num__ ) ; util . start mini cluster ( ) ;,number mess,fail,pre
deletes <PLACE_HOLDER> so key 2 works . this should work as a <PLACE_HOLDER> should n't be cached if initialization failed .,assert true ( realm . delete realm ( configa ) ) ; realm = realm . get instance ( configb ) ; realm . close ( ) ;,deletes config,fail,pre
ar link commands can also <PLACE_HOLDER> huge command lines .,if ( link target type . linker or archiver ( ) == linker or archiver . archiver ) { list < string > param file args = new array list < > ( ) ; list < string > commandline args = new array list < > ( ) ; extract arguments for static link param file ( args @$ commandline args @$ param file args ) ; return pair . of ( commandline args @$ param file args ) ; } else { list < string > param file args = new array list < > ( ) ; list < string > commandline args = new array list < > ( ) ; extract arguments for dynamic link param file ( args @$ commandline args @$,commands produce,fail,pre
reconnect internet after testing network health <PLACE_HOLDER> rollbacks,get device ( ) . execute shell command ( __str__ ) ; get device ( ) . execute shell command ( __str__ ) ;,health add,fail,pre
this test is specifically <PLACE_HOLDER> loose property check behavior .,disable strict missing property checks ( ) ; test types ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,test checking,success,pre
java does not <PLACE_HOLDER> \a or \v @$ apparently .,switch ( b ) { case __num__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __num__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; default : if,java allow,fail,pre
if liveness <PLACE_HOLDER> zero @$ queue is considered disconnected,liveness = heartbeat_liveness ; heartbeat at = system . current time millis ( ) + heartbeat ;,liveness hits,success,pre
config <PLACE_HOLDER> 4 j with customized files check whether hive conf initialize <PLACE_HOLDER> 4 j correctly,config log ( hive log4j property @$ hive exec log4j property ) ;,config log,success,pre
target float value hard <PLACE_HOLDER> version,string [ ] [ ] rounding test cases = { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__,value sized,fail,pre
reset the schedule and reload the latest card off the top of the stack if required . the card could <PLACE_HOLDER> been rescheduled @$ the deck could <PLACE_HOLDER> changed @$ or a change of note type could <PLACE_HOLDER> lead to the card being deleted,if ( data != null && data . has extra ( __str__ ) ) { get col ( ) . get sched ( ) . reset ( ) ; deck task . launch deck task ( deck task . task_type_answer_card @$ m answer card handler @$ new deck task . task data ( null @$ __num__ ) ) ; } if ( request code == edit_current_card ) { if ( result code == result_ok ) { timber . i ( __str__ ) ; deck task . launch deck task ( deck task . task_type_update_fact @$ m update card handler @$ new deck task . task data ( m current card @$ true ) ) ; } else if ( result code == result_canceled && ! ( data !=,change have,success,pre
increment job execution time . this counter <PLACE_HOLDER> reset once this job will be accounted for in metrics .,long exec time = worker . get execute time ( ) ; finished jobs time . add ( exec time ) ; total execution time metric . add ( exec time ) ; max finished jobs time . set if greater ( exec time ) ; if ( job always activate ) { if ( metrics update freq > - __num__ ) update job metrics ( ) ; if ( ! remove from active ( worker ) ) cancelled jobs . remove ( worker . get job id ( ) @$ worker ) ; held jobs . remove ( worker . get job id ( ) ) ; } else { if ( ! rw lock . try read lock ( ) ) { if ( log .,counter gets,success,pre
specified scheme does not <PLACE_HOLDER> the existing scheme for the pipeline . returns channel back to the pool and throws exception to the caller .,if ( ! scheme . equals ignore case ( _scheme ) ) { ( ( request with callback ) msg ) . handle ( ) . release ( ) ; throw new illegal state exception ( string . format ( __str__ @$ _scheme @$ scheme @$ ctx . channel ( ) . remote address ( ) ) ) ; },scheme match,success,pre
the probe <PLACE_HOLDER> everything,probe = join probe factory . create join probe ( page ) ; for ( int join position = __num__ ; probe . advance next position ( ) ; join position ++ ) { lookup join page builder . append row ( probe @$ lookup source @$ join position ) ; } output = lookup join page builder . build ( probe ) ; assert equals ( output . get channel count ( ) @$ __num__ ) ; assert false ( output . get block ( __num__ ) instanceof dictionary block ) ; assert equals ( output . get position count ( ) @$ entries ) ; for ( int i = __num__ ; i < entries ; i ++ ) { assert equals ( output . get,probe covers,success,pre
in general a filter can not be pushed below a windowing calculation . applying the filter before the aggregation function <PLACE_HOLDER> the results of the windowing invocation . when the filter is on the partition by expression of the over clause it can be pushed down . for now we do n't support this .,if ( rex over . contains over ( project . get projects ( ) @$ null ) ) { return ; },function updates,fail,pre
only input 2 side <PLACE_HOLDER> nulls,if ( input col vector1 . no nulls && ! input col vector2 . no nulls ) { if ( ( input col vector1 . is repeating ) && ( input col vector2 . is repeating ) ) { outv . is repeating = true ; output vector [ __num__ ] = vector1 [ __num__ ] | vector2 [ __num__ ] ; output is null [ __num__ ] = ( vector1 [ __num__ ] == __num__ ) && input col vector2 . is null [ __num__ ] ; } else if ( input col vector1 . is repeating && ! input col vector2 . is repeating ) { if ( batch . selected in use ) { for ( int j = __num__ ; j != n ;,side has,success,pre
test fs 2 <PLACE_HOLDER> fs 1 .,fs1 . clear ( ) ; fs2 . clear ( ) ; fs result . clear ( ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs result = new field selection ( fs1 ) ; fs1 . intersect ( fs2 ) ; assert equals ( fs result @$ fs1 ) ;,fs contains,success,pre
type procname client <PLACE_HOLDER> extension count,int size = __num__ + __num__ + get proc name bytes ( ) . length + __num__ + __num__ + batch extension size + all partition extension size + partition destination size ;,client handle,success,pre
return the fragment noinspection <PLACE_HOLDER> conditional expression,return new sentence fragment ( fragment tree @$ fragment . has assumed truth ( ) ? fragment . get assumed truth ( ) : true @$ false ) . change score ( fragment . has score ( ) ? fragment . get score ( ) : __num__ ) ;,noinspection object,fail,pre
database will result in a log.wtf call that will crash this process on eng builds . to allow the test to run through to completion <PLACE_HOLDER> this test on eng builds .,if ( build . is_eng ) { return ; } account account = new account ( __str__ @$ __str__ ) ; long id = m accounts db . insert ce account ( account @$ __str__ ) ; assert equals ( __str__ @$ - __num__ @$ id ) ;,builds skip,success,pre
tm ca n't <PLACE_HOLDER> this application exception @$ so it needs cancel .,uba . cancel ( ) ;,tm handle,fail,pre
if parent <PLACE_HOLDER> a hint @$ do n't measure unlimited .,m layout state . m infinite = resolve is infinite ( ) ; m layout state . m extra = get extra layout space ( state ) ; m layout state . m layout direction = layout direction ; int scrolling offset ; if ( layout direction == layout state . layout_end ) { m layout state . m extra += m orientation helper . get end padding ( ) ; final view child = get child closest to end ( ) ; m layout state . m item direction = m should reverse layout ? layout state . item_direction_head : layout state . item_direction_tail ; m layout state . m current position = get position ( child ) + m layout state . m item direction ;,parent provides,success,pre
2 nd tx should <PLACE_HOLDER> document back into existence,check result ( new index query ( def store @$ predicate condition . of ( text @$ text . contains @$ __str__ ) ) @$ def doc ) ;,tx assign,fail,pre
if file has <PLACE_HOLDER> max user defined size . close current file and open a new file .,if ( ! meta . is file name in field ( ) && ( get lines output ( ) > __num__ ) && ( data . split every > __num__ ) && ( ( get lines output ( ) + meta . get footer shift ( ) ) % data . split every ) == __num__ ) { if ( meta . is footer enabled ( ) ) { write header ( ) ; } close file ( filename ) ; data . splitnr ++ ; data . fos = null ; data . out = null ; data . writer = null ; filename = get output file name ( null ) ; is write header = is write header ( filename ) ; init file stream,file reached,success,pre
if the tests are run on windows @$ the expected paths need to be adjusted . on platforms that use the unix convention @$ the following does not actually <PLACE_HOLDER> the test data .,for ( int i = __num__ ; i < data . length ; ++ i ) { if ( data [ i ] != null ) { file file = new file ( data [ i ] ) ; if ( data [ i ] . starts with ( __str__ ) ) { file = file . get absolute file ( ) ; } data [ i ] = file . get path ( ) ; } } for ( int i = __num__ ; i < data . length ; i += __num__ ) { build image configuration config = new build image configuration . builder ( ) . context dir ( data [ i ] ) . docker file ( data [ i + __num__ ],following copy,fail,pre
the process will <PLACE_HOLDER> an error event @$ which is caught and escalated by a user org.flowable.task.service.task,assert equals ( __str__ @$ __num__ @$ task service . create task query ( ) . task definition key ( __str__ ) . count ( ) ) ;,process throw,success,pre
stop the timers because they 're implicitly globally referenced and thus do n't let them <PLACE_HOLDER> this instance .,typing timer . stop ( ) ; typing timer . remove action listener ( this ) ; stopped typing timer . stop ( ) ; stopped typing timer . remove action listener ( this ) ; if ( typing state != operation set typing notifications . state_stopped ) stop typing timer ( ) ; if ( outdated resource timer != null ) { outdated resource timer . cancel ( ) ; outdated resource timer . purge ( ) ; outdated resource timer = null ; } editor pane . remove key listener ( this ) ; menu listeners . clear ( ) ; if ( right button menu != null ) { right button menu . dispose ( ) ; right button menu = null ; } scroll,them touch,fail,pre
asynchronous <PLACE_HOLDER> the queue @$ but still is single thread,if ( _buffer . get cursor ( ) > _consumer . get ( ) ) consume batch when available ( handler ) ;,asynchronous release,success,pre
req.get remote host returns <PLACE_HOLDER> address @$ try to resolve hostname to be consistent with raw protocol .,try { final inet address client address = inet address . get by name ( client host name ) ; client host name = client address . get host name ( ) ; } catch ( unknown host exception e ) { logger . info ( __str__ @$ client host name @$ e . get message ( ) ) ; },returns socket,fail,pre
set sign data and set keep mapping data must before <PLACE_HOLDER> xml config or it will <PLACE_HOLDER>,read xml config ( config ) ; this . m7zip path = sevenzip path ; this . m zipalign path = zip align path ;,data read,success,pre
options <PLACE_HOLDER> activity launch,wait for latch ( latch ) ; verify ( m mock account manager response ) . on result ( m bundle captor . capture ( ) ) ; bundle result = m bundle captor . get value ( ) ;,options expect,success,pre
on error @$ report failure to container and signal abort <PLACE_HOLDER> resource of failed localization,container id c id = context . get container id ( ) ; dispatcher . get event handler ( ) . handle ( new container resource failed event ( c id @$ null @$ exception . get message ( ) ) ) ;,abort host,fail,pre
create threads and make them <PLACE_HOLDER> transactions concurrently .,thread thread id [ ] = new thread [ num_threads ] ; for ( int i = __num__ ; i < num_threads ; i ++ ) { transactions trans = new transactions ( namesystem @$ num_transactions @$ i * num_transactions ) ; thread id [ i ] = new thread ( trans @$ __str__ + i ) ; thread id [ i ] . start ( ) ; },them run,success,pre
the wait is necessary to have the poll function <PLACE_HOLDER> and propagate the changes from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one three = database one text node . get comments ( ) ; final list < i comment > two three = database two text node . get comments ( ) ; assert equals ( one two size - __num__ @$ one three . size ( ) ) ; assert equals ( two two size - __num__ @$ two three . size ( ) ) ; assert equals ( one three @$ two three ) ;,function complete,success,pre
note that concurrent linked queue actually supports doing this the iterator does not <PLACE_HOLDER> a concurrent modification exception,new references . remove all ( collected new references ) ; processed references . remove all ( collected processed references ) ;,iterator throw,success,pre
for posterity : the moment this user <PLACE_HOLDER> the easter egg,try { settings . system . put long ( cr @$ __str__ @$ system . current time millis ( ) ) ; } catch ( runtime exception e ) { log . e ( __str__ @$ __str__ @$ e ) ; },user use,fail,pre
and associate teams with players via the special one v one methods . clear the team reference to players @$ which should <PLACE_HOLDER> the teams . <PLACE_HOLDER>ing the team should delete the team .,session s = open session ( ) ; transaction tx = s . begin transaction ( ) ; soccer team team = new soccer team ( ) ; team . set name ( __str__ ) ; player player1 = new player ( ) ; player1 . set name ( __str__ ) ; team . set one vone player ( player1 ) ; player1 . set one vone team ( team ) ; s . persist ( team ) ; soccer team team2 = new soccer team ( ) ; team2 . set name ( __str__ ) ; player player2 = new player ( ) ; player2 . set name ( __str__ ) ; team2 . set one vone player ( player2 ) ; player2 . set one vone,which delete,fail,pre
foo 's runtime bind release service should now <PLACE_HOLDER> a reference to the new bind,assert true ( foo du binding references . contains ( service name ) ) ;,service have,success,pre
interface jars artifacts <PLACE_HOLDER> proper owner labels,return new java compilation artifacts . builder ( ) . add runtime jars ( jars ) . add full compile time jars ( jars ) . add interface jars ( interface jars ) . build ( ) ;,artifacts provide,fail,pre
make the last field <PLACE_HOLDER> the remaining space,last field . set width ( last width ) ;,field take,fail,pre
check that they are referentially equal . since getting a group for a users that does n't exist <PLACE_HOLDER> a new string array the only way that they should be referentially equal is if the cache worked and made sure we did n't go to hadoop 's script twice .,assert true ( u one . get group names ( ) == u two . get group names ( ) ) ; assert equals ( __num__ @$ ugi one . get group names ( ) . length ) ;,check returns,fail,pre
this asserts that all threads have wait wail the testing thread notifies all . we have to do this to guarantee that the thread pool <PLACE_HOLDER> 10 live threads before we check the 'created ' meter .,try { all tasks are counted . await ( ) ; } catch ( interrupted exception e ) { interrupted . increment and get ( ) ; thread . current thread ( ) . interrupt ( ) ; },pool has,success,pre
not null constraint should <PLACE_HOLDER> a single column,m constraint muk = new m constraint ( constraint name @$ constraint type @$ __num__ @$ null @$ null @$ enable validate rely @$ parent table @$ null @$ parentcd @$ null @$ null @$ parent integer index @$ constraint value ) ;,constraint support,fail,pre
static shared libs can not <PLACE_HOLDER> permission groups,if ( ! pkg . permission groups . is empty ( ) ) { throw new package manager exception ( __str__ ) ; },libs declare,success,pre
read the block from io engine based on the bucket entry 's offset and length @$ notice : the block will <PLACE_HOLDER> the ref cnt of bucket entry @$ which means if two h file block mapping to the same bucket entry @$ then all of the three will share the same ref cnt .,if ( bucket entry . equals ( backing map . get ( key ) ) ) { cacheable cached block = io engine . read ( bucket entry ) ; if ( io engine . uses shared memory ( ) ) { cached block . retain ( ) ; } if ( update cache metrics ) { cache stats . hit ( caching @$ key . is primary ( ) @$ key . get block type ( ) ) ; cache stats . io hit ( system . nano time ( ) - start ) ; } bucket entry . access ( access count . increment and get ( ) ) ; if ( this . io error start time > __num__ ) { io error start time,block share,fail,pre
load balancer will normally <PLACE_HOLDER> all subchannels,subchannel1 . shutdown ( ) ; subchannel2 . shutdown ( ) ;,balancer shutdown,success,pre
if it is a dynamic queue mapping @$ we can safely assume leaf queue name does not have ' . ' in it validate if parent queue is specified @$ then parent queue exists and an instance of auto <PLACE_HOLDER> enabled parent queue,queue mapping new mapping = validate and get auto created queue mapping ( queue manager @$ mapping @$ queue path ) ; if ( new mapping != null ) { new mappings . add ( new mapping ) ; } else { new mappings . add ( mapping ) ; },instance created,fail,pre
since each partition may <PLACE_HOLDER> stats collected for different set of columns @$ we request them separately .,if ( get col stats ) { for ( partition part : ret ) { string part name = warehouse . make part name ( table . get partition keys ( ) @$ part . get values ( ) ) ; list < column statistics > part col stats list = getms ( ) . get partition column statistics ( parsed cat name @$ parsed db name @$ tbl name @$ collections . singleton list ( part name ) @$ stats setup const . get columns having stats ( part . get parameters ( ) ) @$ engine ) ; if ( part col stats list != null && ! part col stats list . is empty ( ) ) { column statistics part col stats = part,partition have,success,pre
start new instance of secondary and verify that a new roll <PLACE_HOLDER> log succedes in spite of the fact that we had a partially failed checkpoint previously .,secondary = start secondary name node ( conf ) ; secondary . do checkpoint ( ) ;,roll roll,fail,pre
ensure the specification <PLACE_HOLDER> all services .,return specification . services ( ) . stream ( ) . collect ( to immutable map ( service info :: name @$ function . identity ( ) ) ) ;,specification contains,success,pre
no context class loader @$ <PLACE_HOLDER> the current class loader,in = ss . get resource as stream ( cl @$ service ) ;,loader use,fail,pre
instance is cached by realm @$ so no need to <PLACE_HOLDER> strong reference,return flowable . create ( new flowable on subscribe < realm > ( ) { @ override public void subscribe ( final flowable emitter < realm > emitter ) throws exception { final realm observable realm = realm . get instance ( realm config ) ; final realm change listener < realm > listener = new realm change listener < realm > ( ) { @ override public void on change ( realm realm ) { if ( ! emitter . is cancelled ( ) ) { emitter . on next ( realm ) ; } } } ; observable realm . add change listener ( listener ) ; emitter . set disposable ( disposables . from runnable ( new runnable ( ) { @ override public void,need keep,success,pre
false signifies that a marker message has not already been processed . <PLACE_HOLDER> and send one .,if ( proxy != null ) { proxy . start or resume message dispatcher ( false ) ; },signifies start,fail,pre
the real value will be sent back as long as the field is marked as dirty and diffstate contains what the client <PLACE_HOLDER>,assert equals ( __str__ @$ get diff state string ( rta @$ __str__ ) ) ; assert true ( __str__ @$ is dirty ( rta ) ) ;,client decides,fail,pre
dynamic ser de always <PLACE_HOLDER> out bytes writable,bytes writable bw = ( bytes writable ) r ; out stream . write ( bw . get ( ) @$ __num__ @$ bw . get size ( ) ) ;,ser write,fail,pre
make sure the component are the same if the input activity <PLACE_HOLDER> the same real activity as the one in the task because either one of them could be the alias activity .,if ( objects . equals ( real activity @$ r . m activity component ) && this . intent != null ) { intent . set component ( this . intent . get component ( ) ) ; } return intent . filter equals ( this . intent ) ;,activity shares,fail,pre
this tells the tar<PLACE_HOLDER> entry to not set the size header field @$ to ensure that no tar client accidentally extracts only a portion of the file data . if the client ca n't read the correct size from the pif data @$ we want the client to report that so the user can <PLACE_HOLDER> a better tar client !,pax sized = true ;,user form,fail,pre
this is somewhat unsafe @$ but it 's better than outright throwing an exception here . returning null will just <PLACE_HOLDER> an exception down the pipeline .,return ( jc expression ) in ;,here cause,success,pre
open a new region which <PLACE_HOLDER> this wal,table descriptor htd = table descriptor builder . new builder ( table name . value of ( this . name . get method name ( ) ) ) . set column family ( column family descriptor builder . of ( b ) ) . build ( ) ; region info hri = region info builder . new builder ( htd . get table name ( ) ) . build ( ) ; chunk creator . initialize ( mem storelab impl . chunk_size_default @$ false @$ __num__ @$ __num__ @$ __num__ @$ null ) ; final h region region = test_util . create localh region ( hri @$ htd @$ log ) ; executor service exec = executors . new fixed thread pool ( __num__ ) ;,which owns,fail,pre
if this node <PLACE_HOLDER> the manufacturer specific command class @$ we use it to get manufacturer info .,if ( manufacturer specific != null ) { logger . debug ( __str__ @$ node . get node id ( ) ) ; add to queue ( manufacturer specific . get manufacturer specific message ( ) ) ; },node has,fail,pre
read repair get operation <PLACE_HOLDER> different versions for same entries loaded via read through feature .,if ( context ( ) . read through ( ) ) { throw new unsupported operation exception ( __str__ ) ; },operation allows,fail,pre
it 's important to use the page transition from the suggestion or we might end up saving generated ur ls as typed ur ls @$ which would then <PLACE_HOLDER> the subsequent omnibox results . there is one special case where the suggestion text was pasted @$ where we want the transition type to be link .,int transition = suggestion match . get type ( ) == omnibox suggestion type . url_what_you_typed && m url bar . is pasted text ( ) ? page transition . link : suggestion match . get transition ( ) ; load url from omnibox match ( suggestion match url @$ transition @$ suggestion match position @$ suggestion match . get type ( ) ) ;,which cache,fail,pre
to avoid rejected execution exception in basic directory model <PLACE_HOLDER> a second,try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; },exception take,fail,pre
these are all of the events listed in the javadoc for xml event . the spec only really <PLACE_HOLDER> 11 of them .,while ( true ) { switch ( event ) { case xml stream constants . start_element : handle start element ( ) ; depth ++ ; break ; case xml stream constants . end_element : depth -- ; handle end element ( ) ; if ( depth == __num__ ) break outer ; break ; case xml stream constants . characters : case xml stream constants . cdata : case xml stream constants . space : handle characters ( ) ; break ; } event = stax stream reader . next ( ) ; },spec describes,success,pre
the subscriber will <PLACE_HOLDER> control before an entire term buffer . so @$ send until ca n't send no 'more . then start up subscriber to drain .,final int term buffer length = __num__ * __num__ ; final int num messages per term = __num__ ; final int message length = ( term buffer length / num messages per term ) - header_length ; final int max fails = __num__ ; int messages sent = __num__ ; context . publication term buffer length ( term buffer length ) ; launch ( channel ) ; for ( int i = __num__ ; i < num messages per term ; i ++ ) { int offer fails = __num__ ; while ( publication . offer ( buffer @$ __num__ @$ message length ) < __num__ ) { if ( ++ offer fails > max fails ) { break ; } system test . check interrupted status (,subscriber take,fail,pre
check template literal <PLACE_HOLDER> type string,test types ( __str__ @$ lines ( __str__ @$ __str__ @$ __str__ ) ) ;,literal target,fail,pre
check whether next token <PLACE_HOLDER> the specified pattern,if ( matcher . matches ( ) ) { cached next index = find start index ; match successful = true ; has next = true ; } recover previous status ( ) ; return has next ;,token matches,success,pre
if event <PLACE_HOLDER> new value @$ then it may mean that the delta bytes should not be applied . this is possible if the event originated locally .,if ( this . delta bytes != null && this . new value == null && this . new value bytes == null ) { process delta bytes ( old value for delta ) ; } if ( owner != null ) { owner . generate and set version tag ( this @$ reentry ) ; } else { get region ( ) . generate and set version tag ( this @$ reentry ) ; } generate new value from bytes if needed ( ) ; object v = this . new value ; if ( v == null ) { v = is local invalid ( ) ? token . local_invalid : token . invalid ; } else { get region ( ) . set region invalid,event has,fail,pre
upload service.get pending media for post will be populated only when the user <PLACE_HOLDER> the editor but if the user does n't exit the editor and sends the app to the background @$ a reattachment for the media within this post is needed as soon as the app comes back to foreground @$ so we get the list of progressing media for this post,if ( m editor fragment instanceof aztec editor fragment && m editor media upload listener != null ) { set < media model > uploading media in post = m edit post repository . get pending media for post ( ) ; list < media model > all uploading media in post = new array list < > ( uploading media in post ) ; for ( media model media1 : m edit post repository . get pending or in progress media uploads for post ( ) ) { boolean found = false ; for ( media model media2 : uploading media in post ) { if ( media1 . get id ( ) == media2 . get id ( ) ) { found = true ; break,user enters,fail,pre
use annotation <PLACE_HOLDER> configurator,container . add endpoint ( server endpoint config . builder . create ( pong socket . class @$ __str__ ) . build ( ) ) ;,annotation generated,fail,pre
create a bunch of test files with random replication factors . <PLACE_HOLDER> them into a linked list .,try { for ( int i = __num__ ; i < number of files ; i ++ ) { final int replication = append test util . next int ( num datanodes - __num__ ) + __num__ ; path test file = new path ( __str__ + i + __str__ ) ; fs data output stream stm = append test util . create file ( fs @$ test file @$ replication ) ; stm . close ( ) ; test files . add ( test file ) ; } workload = new workload [ num threads ] ; for ( int i = __num__ ; i < num threads ; i ++ ) { workload [ i ] = new workload ( cluster @$ i @$ append to,bunch merge,fail,pre
expect an error dialog <PLACE_HOLDER> 3 versions of the program,do create versions ( ) ; final g tree node node = get node ( program_a ) ; select node ( node ) ; final docking action if history action = get action ( __str__ ) ; run swing ( ( ) -> history action . action performed ( get domain file action context ( node ) ) ) ; version history dialog dialog = wait for dialog component ( version history dialog . class ) ; docking action if delete action = get delete action ( dialog ) ; g table table = find component ( dialog @$ g table . class ) ; run swing ( ( ) -> table . select row ( __num__ ) ) ; perform action ( delete action @$ false ),dialog create,success,pre
wrapping the cleanup logic in an auto closable automatically <PLACE_HOLDER> additional exceptions,try ( final cleanup yarn application ignored = new cleanup yarn application ( ) ) { test . run ( ) ; },logic throws,fail,pre
we collapse property definitions after collapsing property references because this step can <PLACE_HOLDER> the parse tree above property references @$ invalidating the node ancestry stored with each reference .,for ( name name : global names ) { collapse declaration of name and descendants ( name @$ name . get base name ( ) @$ escaped ) ; },step walk,fail,pre
open enough file descriptors that we will crash something if we leak f <PLACE_HOLDER> or address space,for ( int i = __num__ ; i < __num__ ; i ++ ) { uri uri = uri . parse ( __str__ ) ; input stream in = resolver . open input stream ( uri ) ; assert not null ( __str__ + i @$ in ) ; byte [ ] buf = new byte [ memory file provider . test_blob . length ] ; int count = in . read ( buf ) ; assert equals ( buf . length @$ count ) ; assert true ( arrays . equals ( memory file provider . test_blob @$ buf ) ) ; in . close ( ) ; },f reference,fail,pre
we do n't need to retry this operation in the case of failure as zk will <PLACE_HOLDER> ephemeral files and we do n't wan na hang this process when closing if we can not reconnect to zk,if ( ! is closed ( ) && id != null ) { try { zoo keeper operation zopdel = ( ) -> { zookeeper . delete ( id @$ - __num__ ) ; return boolean . true ; } ; zopdel . execute ( ) ; } catch ( interrupted exception e ) { log . warn ( __str__ @$ e ) ; thread . current thread ( ) . interrupt ( ) ; } catch ( keeper exception . no node exception e ) { } catch ( keeper exception e ) { log . warn ( __str__ @$ e ) ; throw new runtime exception ( e . get message ( ) @$ e ) ; } finally { lock listener lock listener = get,zk contain,fail,pre
make sure no partition <PLACE_HOLDER> more than active stream,volt table stats = client . call procedure ( __str__ @$ __str__ @$ __num__ ) . get results ( ) [ __num__ ] ; map < string @$ integer > master counters = maps . new hash map ( ) ; while ( stats . advance row ( ) ) { string target = stats . get string ( __str__ ) ; string ttable = stats . get string ( __str__ ) ; string is master = stats . get string ( __str__ ) ; long pid = stats . get long ( __str__ ) ; string key = __str__ + target + __str__ + ttable + __str__ + pid + __str__ + is master ; integer count = master counters . get ( key ) ; if,partition has,success,pre
some services automatically <PLACE_HOLDER> contacts from an addressbook to our roster and this contacts are with subscription none . if such already exist @$ remove it . this is typically our own contact,if ( ! server stored contact list jabber impl . is entry displayable ( item ) ) { if ( contact != null ) { remove contact ( contact ) ; sscl callback . fire contact removed ( this @$ contact ) ; } continue ; } if ( contact != null ) { contact . set resolved ( item ) ; sscl callback . fire contact resolved ( this @$ contact ) ; } else { contact jabber impl new contact = new contact jabber impl ( item @$ sscl callback @$ true @$ true ) ; add contact ( new contact ) ; sscl callback . fire contact added ( this @$ new contact ) ; },services add,fail,pre
minimum scale reached so do n't pan . <PLACE_HOLDER> start settings so any expand will zoom in .,if ( scale <= min scale ( ) ) { v dist start = v dist end ; scale start = min scale ( ) ; v center start . set ( v center endx @$ v center endy ) ; v translate start . set ( v translate ) ; } else if ( pan enabled ) { float v left start = v center start . x - v translate start . x ; float v top start = v center start . y - v translate start . y ; float v left now = v left start * ( scale / scale start ) ; float v top now = v top start * ( scale / scale start ) ; v translate .,pan reset,fail,pre
execute client <PLACE_HOLDER> all from multithread client,assert that ( region . size ( ) ) . is zero ( ) ;,client receives,fail,pre
draw text is always relative to text view 's origin @$ this translation <PLACE_HOLDER> this range of text back to the top left corner of the viewport,try { recording canvas . translate ( - left @$ - top ) ; layout . draw text ( recording canvas @$ block begin line @$ block end line ) ; m text render nodes [ block index ] . is dirty = false ; } finally { block display list . end recording ( ) ; block display list . set clip to bounds ( false ) ; },translation collapses,fail,pre
since lambda form caches are based on soft references @$ gc can <PLACE_HOLDER> element eviction .,if ( lambda form0 != lambda form1 ) { if ( nogc happened ( ) ) { system . err . println ( __str__ ) ; system . err . println ( lambda form0 ) ; system . err . println ( __str__ ) ; system . err . println ( lambda form1 ) ; throw new assertion error ( __str__ + __str__ ) ; } else { system . err . println ( __str__ ) ; } },gc do,fail,pre
doesnt matter what node.id and voldemort.home values are for this <PLACE_HOLDER>,props . set property ( __str__ @$ __str__ ) ; props . set property ( __str__ @$ __str__ ) ; voldemort config = new voldemort config ( props ) ; voldemort config . set rocksdb prefix keys with partition id ( this . prefix partition id ) ; this . rocks db config = new rocks db storage configuration ( voldemort config ) ; this . rocks db store = ( rocks db storage engine ) rocks db config . get store ( test utils . make store definition ( __str__ ) @$ test utils . make single node routing strategy ( ) ) ; random = new random ( ) ;,values and,fail,pre
then evaluation is successful and the nodes <PLACE_HOLDER> the expected values .,assert that evaluation result ( result ) . has entry that ( a key ) . is equal to ( new string value ( __str__ ) ) ; assert that evaluation result ( result ) . has entry that ( b key ) . is equal to ( new string value ( __str__ ) ) ;,nodes have,success,pre
options processed by abstract java <PLACE_HOLDER> server codegen,additional properties . put ( codegen constants . impl_folder @$ __str__ ) ; additional properties . put ( bean validation features . use_beanvalidation @$ __str__ ) ; additional properties . put ( abstract javajaxrs server codegen . server_port @$ __str__ ) ;,options jaxrs,success,pre
the amount with which to adjust the user <PLACE_HOLDER> content padding to account for stroke and shape corners .,int content padding offset = ( int ) ( ( include corner padding ? calculate actual corner padding ( ) : __num__ ) - get parent card view calculated corner padding ( ) ) ; material card view . set ancestor content padding ( user content padding . left + content padding offset @$ user content padding . top + content padding offset @$ user content padding . right + content padding offset @$ user content padding . bottom + content padding offset ) ;,amount viewed,fail,pre
returns <PLACE_HOLDER> list from document loader .,return get document loader ( parent identifier ) . query child documents ( projection @$ parent identifier ) ;,returns object,success,pre
lots of messages may be lost when deserialize queue has n't <PLACE_HOLDER> init operation,while ( ! bstart rec ) { log . info ( __str__ ) ; boolean is finish init = true ; for ( integer task : worker tasks ) { if ( deserialize queues . get ( task ) == null ) { is finish init = false ; j storm utils . sleep ms ( __num__ ) ; break ; } } if ( is finish init ) { bstart rec = is finish init ; } } short type = message . get_type ( ) ; if ( type == task message . normal_message ) { int task = message . task ( ) ; disruptor queue queue = deserialize queues . get ( task ) ; if ( queue == null ) { log .,queue finish,fail,pre
if the value <PLACE_HOLDER> a decimal or grouping symbol or some sort @$ it 's not an integer,if ( ( c == __str__ && cmm . get conversion meta ( ) . is integer ( ) ) || ( c == __str__ && cmm . get conversion meta ( ) . is integer ( ) ) ) { evaluation results . remove ( cmm ) ; stop = true ; break ; } if ( c == __str__ ) { nr dots ++ ; } if ( c == __str__ ) { nr commas ++ ; } pos ++ ;,value has,fail,pre
fire column <PLACE_HOLDER> events for all columns but the source of the <PLACE_HOLDER> action @$ since an event will fire separately for this .,list < header cell > columns = new array list < header cell > ( available cells . values ( ) ) ; columns . remove ( source ) ; send column width updates ( columns ) ; force realign column headers ( ) ;,column update,fail,pre
best effort to <PLACE_HOLDER> a rich error message,message proxy creation error = iterables . get only element ( e . get errors ( ) ) ; message cycle dependencies message = create cycle dependencies message ( locks cycle @$ proxy creation error ) ;,effort give,fail,pre
wait for first job <PLACE_HOLDER> execution .,job executed latch . await ( ) ; start grid ( __num__ ) ; for ( ignite g : g . all grids ( ) ) info ( __str__ + g . cluster ( ) . local node ( ) . id ( ) + __str__ + g . cluster ( ) . local node ( ) . metrics ( ) + __str__ ) ; future . get ( ) ; assert null ( __str__ @$ fail . get ( ) ) ;,wait completed,fail,pre
updates <PLACE_HOLDER> layer with the blob information .,path layer file = cache storage files . get layer file ( written layer . layer digest @$ written layer . layer diff id ) ; return cached layer . builder ( ) . set layer digest ( written layer . layer digest ) . set layer diff id ( written layer . layer diff id ) . set layer size ( written layer . layer size ) . set layer blob ( blobs . from ( layer file ) ) . build ( ) ;,updates cached,success,pre
need to close before emiting file to the subscriber @$ because when subscriber <PLACE_HOLDER> data in the same thread the file may be truncated,sink . close ( ) ; sink = null ; subscriber . on next ( output ) ; subscriber . on completed ( ) ; if ( sink != null ) { try { sink . close ( ) ; } catch ( io exception e ) { subscriber . on error ( e ) ; } },subscriber writes,fail,pre
force transparence of this layer only . if no buffering we would <PLACE_HOLDER> layers below,if ( buffering ) { comp = g2 . get composite ( ) ; g2 . set composite ( alpha composite . clear ) ; } g2 . set color ( new color ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; g2 . fill rect ( __num__ @$ __num__ @$ bounds . width @$ bounds . height ) ; g2 . set color ( col ) ; if ( comp != null ) { g2 . set composite ( comp ) ; },buffering remove,fail,pre
user jack can not <PLACE_HOLDER> master from journal @$ in which the root is owned by alluxio .,master test utils . create leader file system master from journal ( new test user state ( user @$ server configuration . global ( ) ) ) . close ( ) ;,jack recover,success,pre
older versions did not <PLACE_HOLDER> a resource field @$ they stuffed it into the description .,if ( origin type == origin type . resource && resource or null == null ) { resource or null = description ; } return new simple config origin ( description @$ line number != null ? line number : - __num__ @$ end line number != null ? end line number : - __num__ @$ origin type @$ url or null @$ resource or null @$ comments or null ) ;,versions add,fail,pre
0 progress <PLACE_HOLDER>es n't <PLACE_HOLDER> anything .,assert . assert equals ( collections . n copies ( __num__ @$ false ) @$ multithreaded executor . invoke all ( collections . n copies ( __num__ @$ ( ) -> allocation completion tracker . update progress ( allocation tree . child1 @$ __num__ ) ) ) ) ; assert . assert equals ( arrays . as list ( allocation tree . root @$ allocation tree . child1 @$ allocation tree . child1 child @$ allocation tree . child2 ) @$ allocation completion tracker . get unfinished allocations ( ) ) ;,progress do,success,pre
for local queries returning <PLACE_HOLDER> objects wrap the resultset with results collection <PLACE_HOLDER> deserializer wrapper which deserializes these <PLACE_HOLDER> objects .,if ( needspdx deserialization wrapper ( true ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ false ) ; },collection pdx,success,pre
check:3 <PLACE_HOLDER> the key types,for ( int i = __num__ ; i < prev count ; i ++ ) { value meta interface pre value = null ; for ( int j = __num__ ; j < rows . length ; j ++ ) { value meta interface v = rows [ j ] . search value meta ( key list . get ( j ) [ i ] ) ; if ( v == null ) { return false ; } if ( j != __num__ && v . get type ( ) != pre value . get type ( ) ) { log error ( __str__ ) ; return false ; } else { pre value = v ; } } },check:3 check,fail,pre
map the spring to the feedback bar position so that its <PLACE_HOLDER> off screen and bounces in on tap .,float bar position = ( float ) spring util . map value from range to range ( value @$ __num__ @$ __num__ @$ m feedback bar . get height ( ) @$ __num__ ) ; m feedback bar . set translationy ( bar position ) ;,its sends,fail,pre
spawn a new thread <PLACE_HOLDER> and cust id one in another tx so that outer thread fails,class tx thread extends thread { @ override public void run ( ) { cache transaction manager mgr = get gemfire cache ( ) . get tx manager ( ) ; mgr . set distributed ( true ) ; mgr . begin ( ) ; cust id cust id one = new cust id ( __num__ ) ; customer customer one = new customer ( __str__ @$ __str__ ) ; region < cust id @$ customer > cust region = get cache ( ) . get region ( region name ) ; cust region . put ( cust id one @$ customer one ) ; mgr . commit ( ) ; } } tx thread tx thread = new tx thread ( ) ; tx thread . start,thread put,fail,pre
the jitter <PLACE_HOLDER> the clients to get the data at different times @$ and avoids thundering herd,if ( ! ( boolean ) topology conf . get ( config . topology_disable_loadaware_messaging ) ) { worker state . refresh load timer . schedule recurring with jitter ( __num__ @$ __num__ @$ __num__ @$ worker . this :: do refresh load ) ; } worker state . refresh connections timer . schedule recurring ( __num__ @$ ( integer ) conf . get ( config . task_refresh_poll_secs ) @$ worker state :: refresh connections ) ; worker state . reset log levels timer . schedule recurring ( __num__ @$ ( integer ) conf . get ( config . worker_log_level_reset_poll_secs ) @$ log config manager :: reset log levels ) ; worker state . refresh active timer . schedule recurring ( __num__ @$ ( integer ) conf . get,jitter allows,success,pre
listener for changes to the destination pattern <PLACE_HOLDER> itself and not a permission <PLACE_HOLDER> .,event context . add naming listener ( topic search base @$ __str__ @$ new search controls ( ) @$ this . new cachedldap authorization map namespace change listener ( destination type . topic @$ null ) ) ;,listener map,fail,pre
only a ui automation can <PLACE_HOLDER> this flag and now that it is finished we make sure it is re<PLACE_HOLDER> to its default .,m user is monkey = false ;,automation set,success,pre
first check length of journal to make sure it makes sense to even try . if there is only one journal file with acks in it we do n't need to move it since it wo n't be chained to any later logs . if the logs have n't grown since the last time then we need to compact otherwise there seems to still,if ( ++ check point cycles with nogc >= get compact acks after nogc ( ) ) { if ( metadata . ack message file map . size ( ) > __num__ && ( journal log on last compaction check == journal . get current data file id ( ) || is compact acks ignores store growth ( ) ) ) { log . trace ( __str__ ) ; try { scheduler . execute ( new ack compaction runner ( ) ) ; } catch ( exception ex ) { log . warn ( __str__ @$ ex ) ; } } else { log . trace ( __str__ ) ; } check point cycles with nogc = __num__ ; } else { log . trace ( __str__ @$,compaction ' d,fail,pre
if this dialog has n't <PLACE_HOLDER> an hwnd or has been already closed @$ do n't send notification,if ( hwnd != __num__ ) { window . modal enable ( ( dialog ) target ) ; },dialog had,fail,pre
count the constant method <PLACE_HOLDER> values .,if ( method propagation returnvalue ) { program class pool . classes accept ( new all method visitor ( new constant member filter ( method propagation returnvalue counter ) ) ) ; },method return,success,pre
'next ' <PLACE_HOLDER> the resulting events after transformation in the current iteration . it should not contain null values .,final linked list < event object > next = new linked list < event object > ( ) ; for ( map . entry < integer @$ vector < transform layer > > entry : op set message transform . transform layers . entry set ( ) ) { for ( transform layer transform layer : entry . get value ( ) ) { next . clear ( ) ; while ( ! current . is empty ( ) ) { final event object event = current . remove ( ) ; switch ( event type ) { case message delivered : message delivered event transformed delivered = transform layer . message delivered ( ( message delivered event ) event ) ; if ( transformed delivered != null,'next contains,success,pre
build map of all hash symbols note : full outer join does n't <PLACE_HOLDER> hash symbols,map < hash computation @$ symbol > all hash symbols = new hash map < > ( ) ; if ( node . get type ( ) == inner || node . get type ( ) == left ) { all hash symbols . put all ( left . get hash symbols ( ) ) ; } if ( node . get type ( ) == inner || node . get type ( ) == right ) { all hash symbols . put all ( right . get hash symbols ( ) ) ; } return build join node with preferred hashes ( node @$ left @$ right @$ all hash symbols @$ parent preference @$ optional . of ( left hash symbol ) @$ optional . of,join use,success,pre
debug mode overrides all <PLACE_HOLDER> state so no setup needed,set acra config ( __str__ @$ anki droid app . get shared prefs ( instrumentation registry . get instrumentation ( ) . get target context ( ) ) ) ; assert array equals ( __str__ @$ app . get acra core config builder ( ) . build ( ) . logcat arguments ( ) . to array ( ) @$ new immutable list < > ( debug logcat arguments ) . to array ( ) ) ; verify debugacra preferences ( ) ;,all supported,fail,pre
also @$ equality should <PLACE_HOLDER> ok,assert equals ( result @$ double node . value of ( value ) ) ;,equality work,success,pre
show that hql <PLACE_HOLDER> work,do in hibernate ( this :: session factory @$ session -> { query query = session . create query ( __str__ @$ foo . class ) ; list < foo > list = query . get result list ( ) ; assert equals ( __num__ @$ list . size ( ) ) ; } ) ;,hql does,success,pre
completing task will <PLACE_HOLDER> completion condition,task service . complete ( new task . get id ( ) ) ; assert equals ( __num__ @$ task service . create task query ( ) . count ( ) ) ; assert process ended ( proc id ) ;,task satisfy,fail,pre
search backwards from the end of the zip file @$ searching for the eocd signature @$ which <PLACE_HOLDER> the start of the eocd .,int eocd offset = ( int ) zip size - zip entry . endhdr ; while ( map . get int ( eocd offset ) != zip entry . endsig ) { eocd offset -- ; } long cd entries = short . to unsigned long ( map . get short ( eocd offset + zip entry . endtot ) ) ; if ( ( cd entries & __num__ ) == __num__ ) { int zip64eocd offset = eocd offset ; while ( map . get int ( zip64eocd offset ) != zip64_endsig ) { zip64eocd offset -- ; } cd entries = map . get long ( zip64eocd offset + __num__ ) ; },which indicates,fail,pre
this timezone did n't <PLACE_HOLDER> any daylight savings prior to 1917 and this date is sometime in 1901 .,assert false ( tz . in daylight time ( new date ( - __num__ ) ) ) ; assert equals ( - __num__ @$ tz . get offset ( - __num__ ) ) ;,timezone have,success,pre
note : the python version uses slices which <PLACE_HOLDER> an empty list when indexed beyond what the list contains . since we ca n't slice out an empty sublist in java @$ we must check if we 've reached the end and clear the fnames list manually .,if ( cnt == fnames . size ( ) ) { fnames . clear ( ) ; } else { fnames = fnames . sub list ( cnt @$ fnames . size ( ) ) ; } m con . publish progress ( string . format ( anki droid app . get app resources ( ) . get string ( r . string . sync_media_downloaded_count ) @$ m download count ) ) ;,which return,success,pre
the sub directory should also <PLACE_HOLDER> the old acl,acl entry [ ] child dir expected acl = new acl entry [ ] { acl entry ( access @$ user @$ __str__ @$ all ) @$ acl entry ( access @$ group @$ read_execute ) @$ acl entry ( default @$ user @$ all ) @$ acl entry ( default @$ user @$ __str__ @$ all ) @$ acl entry ( default @$ group @$ read_execute ) @$ acl entry ( default @$ mask @$ all ) @$ acl entry ( default @$ other @$ read_execute ) } ; acl status child dir acl = hdfs . get acl status ( child dir ) ; assert array equals ( child dir expected acl @$ child dir acl . get entries ( ) . to array ( ),directory have,success,pre
the user may have <PLACE_HOLDER> tabs .,if ( tab id == m tab model selector . get current tab id ( ) ) { request reader panel show ( state change reason . unknown ) ; },user clicked,fail,pre
told server do not <PLACE_HOLDER> me up if new initializing cache data added in,if ( is initializing cache list ) { headers . add ( __str__ ) ; headers . add ( __str__ ) ; } if ( string utils . is blank ( probe update string ) ) { return collections . empty list ( ) ; } try { long read timeout ms = timeout + ( long ) math . round ( timeout > > __num__ ) ; http result result = agent . http post ( constants . config_controller_path + __str__ @$ headers @$ params @$ agent . get encode ( ) @$ read timeout ms ) ; if ( httpurl connection . http_ok == result . code ) { set health server ( true ) ; return parse update data id response ( result . content,server wake,fail,pre
most hebrew labels should <PLACE_HOLDER> the second to last character,if ( locale . get language ( ) . equals ( __str__ ) || locale . get language ( ) . equals ( __str__ ) ) { if ( m day label calendar . get ( calendar . day_of_week ) != calendar . saturday ) { int len = day name . length ( ) ; day label = day name . substring ( len - __num__ @$ len - __num__ ) ; } else { day label = day name . to upper case ( locale ) . substring ( __num__ @$ __num__ ) ; } },labels follow,fail,pre
done @$ no one is <PLACE_HOLDER> anything from us,if ( operation < __num__ ) { return ; },one doing,fail,pre
legacy level does n't <PLACE_HOLDER> min frame duration,if ( m static info . is hardware level legacy ( ) ) { if ( candidate size . get width ( ) <= video sz . get width ( ) && candidate size . get height ( ) <= video sz . get height ( ) ) { video snapshot sz = candidate size ; } } else { long jpeg frame duration = min frame duration map . get ( candidate size ) ; assert true ( __str__ + candidate size @$ jpeg frame duration != null ) ; if ( candidate size . get width ( ) <= video sz . get width ( ) && candidate size . get height ( ) <= video sz . get height ( ) && jpeg frame duration,level apply,fail,pre
this should never happen with warnings as errors . the plus set should always <PLACE_HOLDER> at least the warnings in warnings as errors .,if ( xlint plus . is empty ( ) ) { normalized . add ( __str__ ) ; },set have,fail,pre
truncate did not <PLACE_HOLDER> immediately @$ we must wait for the operation to <PLACE_HOLDER> and release the lease .,if ( ! truncated ) { wait until lease is revoked ( file system @$ path ) ; },truncate complete,success,pre
if the list accepts the key events and the key event was a click @$ the text view <PLACE_HOLDER> the selected item from the drop down as its content,if ( consumed && key event . is confirm key ( key code ) ) { dismiss ( ) ; },view gets,success,pre
something else <PLACE_HOLDER> the exception @$ throw it ...,if ( ! ( surface data . is null ( dst data ) || surface data . is null ( src data ) ) ) { throw e ; },something caused,success,pre
creating a size and time policy does not <PLACE_HOLDER> a separate triggering policy set on the appender because this policy registers the trigger policy,final size and time based rolling policy < e > size and time based rolling policy = new size and time based rolling policy < > ( ) ; size and time based rolling policy . set max file size ( new file size ( max file size . to bytes ( ) ) ) ; rolling policy = size and time based rolling policy ;,policy create,fail,pre
check if the user <PLACE_HOLDER> this permission defined in the context,for ( role role : this . user name to user . get ( user . name ) . roles ) { if ( role == null ) continue ; for ( permission permitted : role . permissions ) { if ( permitted . implies ( context ) ) { return true ; } } } return false ;,user has,success,pre
it depends ... for example @$ tasks and stacks are only visible if there children are visible but @$ window state are not visible if there parent are not visible . maybe have the container specify which direction to <PLACE_HOLDER> for visibility ?,for ( int i = m children . size ( ) - __num__ ; i >= __num__ ; -- i ) { final window container wc = m children . get ( i ) ; if ( wc . is visible ( ) ) { return true ; } } return false ;,direction take,fail,pre
element <PLACE_HOLDER> no namespace,if ( elem . get local name ( ) == null ) { if ( fdom error handler != null ) { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { elem . get node name ( ) } ) ; modifydom error ( msg @$ dom error . severity_error @$ null @$ elem ) ; boolean continue process = fdom error handler . handle error ( fdom error ) ; if ( ! continue process ) { throw new runtime exception ( dom message formatter . format message ( dom message formatter . serializer_domain @$ __str__ @$ null ) ) ; } } } else { uri = fns binder . geturi (,element has,success,pre
setup server certificates and private keys . clients will <PLACE_HOLDER> a call back to request certificates .,if ( ! client ) { set < string > key types = new hash set < string > ( ) ; for ( string enabled cipher suite : enabled cipher suites ) { if ( enabled cipher suite . equals ( native crypto . tls_empty_renegotiation_info_scsv ) ) { continue ; } string key type = cipher suite . get by name ( enabled cipher suite ) . get server key type ( ) ; if ( key type != null ) { key types . add ( key type ) ; } } for ( string key type : key types ) { try { set certificate ( ssl parameters . get key manager ( ) . choose server alias ( key type @$ null @$ this,clients send,fail,pre
check if any of the filters <PLACE_HOLDER> this uri,for ( fetch filter f : fetch filters ) { fetch status s = f . check filter ( uriv ) ; if ( s != fetch status . valid ) { log . debug ( __str__ + uriv + __str__ + s ) ; spider . notify listeners founduri ( uri @$ http request header . get @$ s ) ; return ; } },any disallows,success,pre
go through the set of partition columns @$ and find their <PLACE_HOLDER>atives in the values these <PLACE_HOLDER> the bucketed columns,list < bucket col > bucket cols = extract bucket cols ( rop @$ output values ) ;,these represent,success,pre
as the generate bucket name function <PLACE_HOLDER> a uuid @$ this should pretty much never happen,if ( storage . get ( bucket ) != null ) { fail ( __str__ + bucket + __str__ ) ; },function creates,fail,pre
if the cookie does n't <PLACE_HOLDER> a path @$ set one . if it does @$ validate it .,if ( cookie . get path ( ) == null ) { cookie . set path ( path to cookie path ( uri . get path ( ) ) ) ; } else if ( ! http cookie . path matches ( cookie @$ uri ) ) { continue ; },cookie have,success,pre
<PLACE_HOLDER>d failed so lets do some checks <PLACE_HOLDER> the backup back to the original file new file doesnt exist,if ( ! rename result ) { if ( ! new file . exists ( ) ) { logger . warning ( error message . general_write_failed_new_file_doesnt_exist . get msg ( new file . get absolute path ( ) ) ) ; } if ( ! original file backup . rename to ( af . get file ( ) ) ) { logger . warning ( error message . general_write_failed_to_rename_original_backup_to_original . get msg ( original file backup . get absolute path ( ) @$ af . get file ( ) . get name ( ) ) ) ; } logger . warning ( error message . general_write_failed_to_rename_to_original_file . get msg ( af . get file ( ) . get absolute path ( ) @$ new file . get name,checks rename,success,pre
set text may <PLACE_HOLDER> the given text with spanned string . check the contents by casting to string .,try { tv . get text ( ) ; fail ( ) ; } catch ( illegal argument exception e ) { },text wrap,success,pre
if we are using the bean item container or another container which <PLACE_HOLDER> the objects as ids then just return the instances,if ( id instanceof calendar event ) { event = ( calendar event ) id ; } else { basic event basic event = new container calendar event ( index ) ; if ( caption property != null && item . get item property ids ( ) . contains ( caption property ) ) { basic event . set caption ( string . value of ( item . get item property ( caption property ) . get value ( ) ) ) ; } if ( description property != null && item . get item property ids ( ) . contains ( description property ) ) { basic event . set description ( string . value of ( item . get item property ( description property ) .,which provided,fail,pre
if we have processed all connected vertices and there are edges remaining @$ graph <PLACE_HOLDER> multiple connected components .,if ( edges . size ( ) > __num__ ) return null ; return sorted ;,graph contains,fail,pre
if there is a channel @$ then <PLACE_HOLDER> the socket timeout should not matter . if there is not a channel @$ it will take effect .,s . set so timeout ( __num__ ) ; if ( with channel ) { assert read timeout ( stm @$ __num__ ) ; } else { assert read timeout ( stm @$ __num__ ) ; } io utils . close stream ( stm ) ; io utils . close socket ( s ) ; ss . close ( ) ;,matter using,fail,pre
memoized should always <PLACE_HOLDER> the same object,assert same ( class nodes @$ supplier . get ( ) ) ;,memoized produce,fail,pre
vertical layout and a child <PLACE_HOLDER> height,return ol instanceof vertical layout && has non relative width component ( ol ) ;,layout set,fail,pre
verify the async event listener has <PLACE_HOLDER> the substituted values,my async event listener listener = ( my async event listener ) queue . get async event listener ( ) ; final map events map = listener . get events map ( ) ; assert not null ( events map ) ; assert equals ( expected num invocations @$ events map . size ( ) ) ; for ( iterator i = events map . entry set ( ) . iterator ( ) ; i . has next ( ) ; ) { map . entry < integer @$ string > entry = ( map . entry < integer @$ string > ) i . next ( ) ; assert equals ( my gateway event substitution filter . substitution_prefix + entry . get key ( ) @$ entry,listener received,success,pre
remove any array models which <PLACE_HOLDER> validation and enums,if ( cm . is array model && ! model property . is enum && ! model property . has validation ) { model schemas to remove . put ( cm . name @$ model schema ) ; },which have,fail,pre
poor man 's <PLACE_HOLDER> injection through the jersey application scope .,m block master = ( ( alluxio master process ) context . get attribute ( master web server . alluxio_master_servlet_resource_key ) ) . get master ( block master . class ) ;,man dependency,success,pre
because an unresolved address always <PLACE_HOLDER> a host name .,name resolver . resolve ( unresolved address . get host name ( ) ) . add listener ( new future listener < inet address > ( ) { @ override public void operation complete ( future < inet address > future ) throws exception { if ( future . is success ( ) ) { promise . set success ( new inet socket address ( future . get now ( ) @$ unresolved address . get port ( ) ) ) ; } else { promise . set failure ( future . cause ( ) ) ; } } } ) ;,address has,success,pre
another bookmark @$ then <PLACE_HOLDER> the old definition .,if ( shortcut != __num__ ) { cr . delete ( content_uri @$ s shortcut selection @$ new string [ ] { string . value of ( ( int ) shortcut ) } ) ; } content values values = new content values ( ) ; if ( title != null ) values . put ( title @$ title ) ; if ( folder != null ) values . put ( folder @$ folder ) ; values . put ( intent @$ intent . to uri ( __num__ ) ) ; if ( shortcut != __num__ ) values . put ( shortcut @$ ( int ) shortcut ) ; values . put ( ordering @$ ordering ) ; return cr . insert ( content_uri @$ values ) ;,bookmark remove,success,pre
log changes in upstream network <PLACE_HOLDER> strength @$ if available .,if ( network . equals ( m tethering upstream network ) && new nc . has signal strength ( ) ) { final int new signal = new nc . get signal strength ( ) ; final string prev signal = get signal strength ( prev . network capabilities ) ; m log . logf ( __str__ @$ prev signal @$ new signal ) ; } m network map . put ( network @$ new network state ( null @$ prev . link properties @$ new nc @$ network @$ null @$ null ) ) ;,changes receive,fail,pre
let the session <PLACE_HOLDER> the message,new thread ( ) { @ override public void run ( ) { session . run ( ) ; log . debug ( __str__ ) ; synchronized ( pool ) { try { log . debug ( __str__ ) ; session . commit ( ) ; log . debug ( __str__ ) ; } catch ( jms exception e ) { log . error ( __str__ @$ e ) ; } pool . server session in use = false ; } } } . start ( ) ;,session write,fail,pre
set transient state to true to simulate the view is <PLACE_HOLDER> custom view property animation .,m recycler view . get child at ( __num__ ) . set has transient state ( true ) ;,state computing,fail,pre
javac can <PLACE_HOLDER> these in whatever order it desires . hopper does it one way and mantis another .,test2 ( method @$ variables @$ __str__ @$ __str__ @$ __str__ ) ; test ( method @$ byname @$ __str__ @$ __str__ ) ; test ( method @$ arguments @$ __str__ @$ __str__ ) ;,javac return,fail,pre
to simplify @$ <PLACE_HOLDER> messages first <PLACE_HOLDER> all the necessary fragment responses to satisfy deps just be lazy and perturb the buddy response here,plan . generated responses . get ( plan . generated responses . size ( ) - __num__ ) . set status ( fragment response message . unexpected_error @$ new ee exception ( __num__ ) ) ; for ( fragment response message msg : plan . generated responses ) { system . out . println ( __str__ + msg ) ; dut . offer received fragment response ( msg ) ; },messages offer,success,pre
clear interrupt flag if execute <PLACE_HOLDER> exit .,interrupted ( ) ;,flag does,fail,pre
always use uncustomized version for editing . it helps caching and customized lambda forms reuse <PLACE_HOLDER> cache field to keep a link to uncustomized version .,return new lambda form editor ( lambda form . uncustomize ( ) ) ;,forms keep,fail,pre
endpoint should <PLACE_HOLDER> three different messages in the end order of the messages is not important,endpoint . expected message count ( __num__ ) ; endpoint . assert is satisfied ( ) ;,endpoint have,success,pre
if we received invalid end <PLACE_HOLDER> values @$ we clear the known <PLACE_HOLDER> to refetch the last committed <PLACE_HOLDER> from metadata . if any end <PLACE_HOLDER> values are invalid @$ we treat the entire set as invalid as a safety measure .,boolean end offsets are invalid = false ; for ( entry < partition id type @$ sequence offset type > entry : publishing task end offsets . entry set ( ) ) { partition id type partition = entry . get key ( ) ; sequence offset type sequence = entry . get value ( ) ; if ( sequence . equals ( get end of partition marker ( ) ) ) { log . info ( __str__ @$ task id @$ partition ) ; end offsets are invalid = true ; partition offsets . put ( partition @$ get not set marker ( ) ) ; } } if ( ! end offsets are invalid ) { for ( entry < partition id type @$ sequence offset,committed offset,success,pre
s <PLACE_HOLDER> space,if ( c == __str__ ) { result . append ( __str__ ) ; } else { result . append ( c ) ; },s |,fail,pre
assert that the input script only <PLACE_HOLDER> 3 chunks,assert true ( input script . get chunks ( ) . size ( ) == __num__ ) ;,script contains,success,pre
component 4 is <PLACE_HOLDER> block map pointer or displacement .,return eh data type utilities . get component address ( get data type ( ) @$ try_block_map_ordinal @$ get mem buffer ( ) ) ;,component try,success,pre
otherwise tests of deprecated code generate nuisance warnings . do n't <PLACE_HOLDER> deprecation if the current target is also deprecated @$ or if the current context is evaluating an aspect @$ as the base target would have already printed the deprecation warnings .,return ( ! for aspect && prerequisite deprecation != null && ! is same logical package ( this package @$ prerequisite package ) && this deprecation == null ) ;,aspect consider,fail,pre
filter will decide if the requester <PLACE_HOLDER> branches or pull requests,collection all jobs matchin filter = container filter . filter ( pipeline . mbp . get all jobs ( ) ) ;,requester submit,fail,pre
the method should have <PLACE_HOLDER> the error and reported it to the logger .,final list < log message > log messages = runner . get logger ( ) . get error messages ( ) ; assert false ( log messages . is empty ( ) ) ; assert equals ( __num__ @$ log messages . size ( ) ) ;,method logged,fail,pre
load <PLACE_HOLDER> the encoding factory from the output stream,return block encoding . read block ( this @$ input ) ;,load picks,fail,pre
check to ensure mesh <PLACE_HOLDER> texcoords and normals before generating,if ( mesh . get buffer ( type . tex coord ) != null && mesh . get buffer ( type . normal ) != null ) { generate ( geom . get mesh ( ) @$ true @$ split mirrored ) ; },mesh has,success,pre
use local mode for embedded <PLACE_HOLDER> mode when <PLACE_HOLDER>.master is not found,conf . set if missing ( __str__ @$ __str__ ) ; this . inner interpreter = load spark scala interpreter ( conf ) ; this . inner interpreter . open ( ) ; sc = this . inner interpreter . get spark context ( ) ; jsc = java spark context . from spark context ( sc ) ; spark version = spark version . from version string ( sc . version ( ) ) ; if ( enable supported version check && spark version . is unsupported version ( ) ) { throw new exception ( __str__ + spark version + __str__ + __str__ ) ; } sql context = this . inner interpreter . get sql context ( ) ; spark session = this . inner,mode spark,success,pre
output <PLACE_HOLDER> reference,if ( instr . get flow type ( ) . is call ( ) ) { address call addr = get call address ( instr ) ; if ( call addr != null ) { s = symbol table . get primary symbol ( call addr ) ; if ( s != null ) { buf . append ( __str__ ) ; buf . append ( s . get name ( ) ) ; buf . append ( __str__ ) ; } } },output call,success,pre
this should probably be a java.text.parse exception @$ but illegal argument exception is what android 's idn <PLACE_HOLDER> .,if ( m - n > ( integer . max_value - delta ) / ( h + __num__ ) ) { throw new illegal argument exception ( __str__ ) ; },idn does,fail,pre
there is no support for a native boolean type that accepts values of true @$ false or unknown . using the tinyint type <PLACE_HOLDER> substitions of true and false .,get default properties ( ) . set property ( environment . query_substitutions @$ __str__ ) ;,type allows,fail,pre
test that submit does n't <PLACE_HOLDER> np es,executor service . submit ( new test event handler ( mocked server @$ event type . m_server_shutdown @$ lock @$ counter ) ) ;,submit trigger,fail,pre
need to catch this @$ as windows does not <PLACE_HOLDER> the posix call . this is not an error @$ however @$ and should just silently fail .,files . set posix file permissions ( file . to path ( ) @$ perms ) ;,windows handle,fail,pre
both listeners should <PLACE_HOLDER> and not cause the other not to <PLACE_HOLDER> .,assert . assert equals ( __num__ @$ io exception on online listener . on online count ) ; assert . assert equals ( __num__ @$ runtime exception on online listener . on online count ) ;,listeners change,fail,pre
the following should <PLACE_HOLDER> the same result as the previous @$ since it has the same restrictions applied in reverse order .,s = open session ( ) ; s . get transaction ( ) . begin ( ) ; root criteria = s . create criteria ( order . class @$ __str__ ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ;,following have,success,pre
the list of relational values should <PLACE_HOLDER> 2 or more values : the first represents the discriminator the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ plural attribute source . get attribute role ( ) . get full path ( ) ) @$ mapping document . get origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source discriminator type source = new hibernate type source impl ( jaxb many to any mapping . get meta type ( ) ) ; private final relational value source discriminator relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > discriminator value mapping = new hash,list contain,success,pre
return the first call to a function which <PLACE_HOLDER> vn as an argument,for ( reference ref : collection utils . as iterable ( ref iter ) ) { if ( ! func . get body ( ) . contains ( ref . get from address ( ) ) ) { continue ; } if ( is valid call reference ( ref ) ) { function called func = current program . get function manager ( ) . get function at ( ref . get to address ( ) ) ; parameter [ ] params = called func . get parameters ( ) ; for ( parameter param : params ) { address addr = param . get min address ( ) ; if ( addr != null && addr . equals ( variable addr ) ) { return ref .,which has,fail,pre
binding <PLACE_HOLDER> parts to <PLACE_HOLDER> holder,view holder . price = cell . find view by id ( r . id . title_price ) ; view holder . time = cell . find view by id ( r . id . title_time_label ) ; view holder . date = cell . find view by id ( r . id . title_date_label ) ; view holder . from address = cell . find view by id ( r . id . title_from_address ) ; view holder . to address = cell . find view by id ( r . id . title_to_address ) ; view holder . requests count = cell . find view by id ( r . id . title_requests_count ) ; view holder . pledge price = cell . find view by,parts view,success,pre
serialized output should still <PLACE_HOLDER> the v.2 fields,byte [ ] v1 bytes = v1 adapter . encode ( v1 ) ;,output contain,success,pre
use the selected versions field to embed the original <PLACE_HOLDER> target name . we 'll use this to verify the correct node was recovered from the target graph .,target node builder . set selected versions ( immutable map . of ( build target @$ version . of ( __str__ ) ) ) ; for ( target node < ? > dep : deps ) { target node builder . add dep ( dep . get build target ( ) ) ; } return target node builder . build ( ) ;,original build,success,pre
store the <PLACE_HOLDER> target back into the intent @$ because now that we have it we never want to do this again . for example @$ if the user navigates back to this point in the history @$ we should always restart the exact same activity .,if ( a info != null ) { intent . set component ( new component name ( a info . application info . package name @$ a info . name ) ) ; if ( ! a info . process name . equals ( __str__ ) ) { if ( ( start flags & ( start_flag_debug | start_flag_native_debugging | start_flag_track_allocation ) ) != __num__ || profiler info != null ) { synchronized ( m service . m global lock ) { final message msg = pooled lambda . obtain message ( activity manager internal :: set debug flags for starting activity @$ m service . m am internal @$ a info @$ start flags @$ profiler info @$ m service . m global lock ) ; m service,the build,fail,pre
client <PLACE_HOLDER> an ack rds request .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_rds @$ __str__ ) ) ) ; verify ( config watcher @$ never ( ) ) . on config changed ( any ( config update . class ) ) ; verify ( config watcher @$ never ( ) ) . on error ( any ( status . class ) ) ;,client sent,fail,pre
trailers always <PLACE_HOLDER> the stream even if not explicitly set .,end of stream = true ; log builder . request trailers ( trailers ) ; log builder . increase request length ( ( http data ) o ) ;,trailers end,success,pre
enum class <PLACE_HOLDER> imports for jackson 's json creator,if ( additional properties . contains key ( serialization_library_jackson ) ) { model . imports . add ( __str__ ) ; model . imports . add ( __str__ ) ; },class needed,success,pre
0 x 1002 cfc : p 1 and p 2 <PLACE_HOLDER> mem refs @$ but p 1 has offset .,program builder1 . create offset mem reference ( __str__ @$ __str__ @$ __num__ @$ ref type . read @$ source type . user_defined @$ __num__ ) ; program builder2 . create memory reference ( __str__ @$ __str__ @$ ref type . read @$ source type . user_defined @$ __num__ ) ; program diff = new program diff ( p1 @$ p2 ) ; address set as = new address set ( addr ( __num__ ) @$ addr ( __num__ ) ) ; program diff . set filter ( new program diff filter ( program diff filter . reference_diffs ) ) ; assert equals ( as @$ program diff . get differences ( program diff . get filter ( ) @$ null ) ) ;,0 have,success,pre
this is a user who already <PLACE_HOLDER> the option,if ( get polling thread count ( ) != threads_default ) { return true ; },who has,fail,pre
it is not guaranteed that the source record will <PLACE_HOLDER> a task associated with it . for @$ example @$ if this method is being called for processing a pending activity launch @$ it is possible that the activity has been removed from the task after the launch was enqueued .,final task record source task = m source record . get task record ( ) ; m new task intent = source task != null ? source task . intent : null ;,record have,success,pre
the execution attempt id should <PLACE_HOLDER> no difference in this case,final function < intermediate result partitionid @$ result partitionid > partitionid mapper = intermediate result partitionid -> new result partitionid ( intermediate result partitionid @$ new execution attemptid ( ) ) ; final result partition availability checker result partition availability checker = new execution graph result partition availability checker ( partitionid mapper @$ partition tracker ) ; for ( intermediate result partitionid irpid : expected availability . key set ( ) ) { assert equals ( expected availability . get ( irpid ) @$ result partition availability checker . is available ( irpid ) ) ; },id make,success,pre
a volt <PLACE_HOLDER> extension to avoid using exceptions for flow control .,if ( parse list . length == __num__ ) { return new expression or exception ( function ) ; },volt db,success,pre
if we cant parse the advertised address we act as if it has no port specified if invalid hostname config will <PLACE_HOLDER> the error,if ( ! advertised already has port ) { try { int port = socket_address . parse ( listen value ) . get port ( ) ; if ( port >= __num__ ) { socket address new advertised = new socket address ( advertised value @$ port ) ; log . warn ( __str__ @$ port @$ listen address @$ advertised address ) ; default values . put ( advertised address @$ new advertised . to string ( ) ) ; } } catch ( runtime exception e ) { } },config throw,fail,pre
the server does n't <PLACE_HOLDER> the response in the first place @$ so the client can read the response .,client . send request ( find request @$ request context ) . get response ( ) ;,server send,fail,pre
rather than fight it @$ let root <PLACE_HOLDER> an alias,nodes . put ( __str__ @$ root ) ; nodes . put without digest ( root zookeeper @$ root ) ; root . add child ( proc child zookeeper ) ; nodes . put ( proc zookeeper @$ proc data node ) ; proc data node . add child ( quota child zookeeper ) ; nodes . put ( quota zookeeper @$ quota data node ) ; add config node ( ) ; node data size . set ( approximate data size ( ) ) ; try { data watches = watch manager factory . create watch manager ( ) ; child watches = watch manager factory . create watch manager ( ) ; } catch ( exception e ) { log . error ( __str__ @$ e,root have,success,pre
no local class for this descriptor @$ skip over the data for this class . like default <PLACE_HOLDER> object with a null current object . the code will <PLACE_HOLDER> the values but discard them .,object stream field [ ] fields = current class desc . get fields no copy ( ) ; if ( fields . length > __num__ ) { input class fields ( null @$ current class @$ fields @$ sender ) ; },default read,success,pre
streams should <PLACE_HOLDER> no effect .,path src = get test root path ( f sys @$ __str__ ) ; fs data output stream out = f sys . create ( src ) ; out . write char ( __str__ ) ;,streams have,success,pre
lookup does n't <PLACE_HOLDER> client,when ( security context . get user principal ( ) ) . then return ( simple principal . of ( __str__ ) ) ; when ( clientdao . get client ( __str__ ) ) . then return ( optional . empty ( ) ) ;,lookup reference,fail,pre
at this point we 've entered the threshold zone so more keys wo n't immediately <PLACE_HOLDER> more generations .,assert equals ( __num__ @$ listener keys . size ( ) ) ;,keys add,fail,pre
after opening the link <PLACE_HOLDER> the current href to avoid clicking on the window to gain focus to open the link again,this . current href = __str__ ;,link remove,success,pre
these two objects should not <PLACE_HOLDER> each other,if ( o1 . equals ( o2 ) ) { throw new assertion error ( string . format ( __str__ @$ o1 @$ o2 ) ) ; } if ( o2 . equals ( o1 ) ) { throw new assertion error ( string . format ( __str__ @$ o2 @$ o1 ) ) ; } verify hash code consistency ( o1 @$ o2 ) ;,objects equal,success,pre
we do n't have the last output <PLACE_HOLDER> a shallow copy,if ( it . has next ( ) ) { stream record < out > shallow copy = record . copy ( record . get value ( ) ) ; out . collect ( shallow copy ) ; } else { out . collect ( record ) ; break ; },output generate,fail,pre
client worker <PLACE_HOLDER> verified messages .,msg . verify ( get local node id ( ) ) ;,worker sends,fail,pre
determine which <PLACE_HOLDER> columns we will read,list < hive column handle > read columns = columns . stream ( ) . filter ( column -> column . get column type ( ) == regular ) . collect ( to immutable list ( ) ) ; list < integer > read hive column indexes = read columns . stream ( ) . map ( hive column handle :: get hive column index ) . collect ( to immutable list ( ) ) ;,which payload,fail,pre
error on ri ri <PLACE_HOLDER> array index out of bounds exception,assert equals ( __str__ @$ f . to string ( ) ) ;,error read,fail,pre
modifying the ac ls of root directory of the snapshot should <PLACE_HOLDER> new acl feature . and old acl feature should be <PLACE_HOLDER>enced by snapshot,hdfs . modify acl entries ( path @$ acl spec ) ; path snapshot path = snapshot test helper . create snapshot ( hdfs @$ path @$ snapshot name ) ; acl feature snapshot acl = fs acl base test . get acl feature ( snapshot path @$ cluster ) ; acl feature = fs acl base test . get acl feature ( path @$ cluster ) ; assert equals ( __str__ @$ __num__ @$ acl feature . get ref count ( ) ) ; list < acl entry > new acl = lists . new array list ( acl entry ( access @$ user @$ __str__ @$ all ) ) ; hdfs . modify acl entries ( path @$ new acl ) ; acl feature = fs,ls refer,success,pre
construct a new model with the intended architecture and print summary note : this architecture is constructed with the primary intent of demonstrating use of the transfer learning api @$ secondary to what might <PLACE_HOLDER> better results,computation graph vgg16 transfer = new transfer learning . graph builder ( vgg16 ) . fine tune configuration ( fine tune conf ) . set feature extractor ( feature extraction layer ) . n out replace ( __str__ @$ __num__ @$ weight init . xavier ) . remove vertex and connections ( __str__ ) . add layer ( __str__ @$ new dense layer . builder ( ) . activation ( activation . tanh ) . n in ( __num__ ) . n out ( __num__ ) . build ( ) @$ __str__ ) . add layer ( __str__ @$ new output layer . builder ( loss functions . loss function . negativeloglikelihood ) . activation ( activation . softmax ) . n in ( __num__ ) . n,secondary give,success,pre
if the proxied object <PLACE_HOLDER> the wrapper interface @$ then return the result of it 's is wrapper for method .,if ( wrapper . class . is assignable from ( delegate . get class ( ) ) ) { return ( ( wrapper ) unwrapp6 spy proxy ( ) ) . is wrapper for ( iface ) ; },object implements,success,pre
what we really want to do here is <PLACE_HOLDER> a versioned tool @$ however @$ this will suffice for now .,if ( default_version . equals ( get version ( ) ) ) { return get short name ( ) ; } else { return get version ( ) . to string ( ) ; },here offer,fail,pre
attribute change <PLACE_HOLDER> capability registration,if ( ! registered && should register ) { context . register capability ( capability . resolve ( address ) ) ; } else if ( registered && ! should register ) { context . deregister capability ( capability . resolve ( address ) . get name ( ) ) ; },change triggered,fail,pre
test that inverting a transformed matrix <PLACE_HOLDER> the original matrix .,region placement maintainer . randomized matrix rm = new region placement maintainer . randomized matrix ( rows @$ cols ) ; float [ ] [ ] transformed = rm . transform ( matrix ) ; float [ ] [ ] inverted transformed = rm . invert ( transformed ) ; for ( int i = __num__ ; i < rows ; i ++ ) { for ( int j = __num__ ; j < cols ; j ++ ) { if ( matrix [ i ] [ j ] != inverted transformed [ i ] [ j ] ) { throw new runtime exception ( ) ; } } },test changes,fail,pre
wsaw wsdl binding case will <PLACE_HOLDER> some value set on wbo,return wbo != null ? wbo . get anonymous ( ) : wsdl bound operation . anonymous . optional ;,case have,success,pre
configure the cluster and commit the configuration as we know the successful response <PLACE_HOLDER> commitment .,configure ( configuration ) . commit ( ) ; future . complete ( null ) ;,response performs,fail,pre
without the metadata the status and health check ur ls will not be set and the status page and health check url paths will not <PLACE_HOLDER> the context path so set them here,if ( string utils . has text ( management context path ) ) { instance . set health check url path ( management context path + instance . get health check url path ( ) ) ; instance . set status page url path ( management context path + instance . get status page url path ( ) ) ; },page reference,fail,pre
shorter value to <PLACE_HOLDER> test faster,viewer . set popup delay ( __num__ ) ;,value run,fail,pre
response <PLACE_HOLDER> type,m ams . get account by type and features ( m mock account manager response @$ null @$ account manager service test fixtures . account_features @$ __str__ ) ;,response account,success,pre
authentication ok : ' r ' | <PLACE_HOLDER> 32 len | <PLACE_HOLDER> 32 code,assert that ( response bytes @$ is ( new byte [ ] { __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ) ) ;,| read,fail,pre
inject our empty wal into the replication queue @$ and then roll the original wal @$ which <PLACE_HOLDER> a new wal behind our empty wal . we must roll the wal here as now we use the wal to determine if the file being replicated currently is still opened for write @$ so just inject a new wal to the replication queue does not,for ( int i = __num__ ; i < num rs ; i ++ ) { h region server hrs = util1 . geth base cluster ( ) . get region server ( i ) ; replication replication service = ( replication ) hrs . get replication source service ( ) ; replication service . get replication manager ( ) . pre log roll ( empty wal paths . get ( i ) ) ; replication service . get replication manager ( ) . post log roll ( empty wal paths . get ( i ) ) ; region info region info = util1 . geth base cluster ( ) . get regions ( htable1 . get name ( ) ) . get ( __num__ ) . get,which puts,fail,pre
signatures which jca does not <PLACE_HOLDER> via signature . we thus have to support this .,set keymaster purpose override ( keymaster defs . km_purpose_sign ) ; return true ;,jca support,fail,pre
check failure for partitioned table where where clause can not <PLACE_HOLDER> partitioning,verify stmt fails ( client @$ __str__ @$ __str__ + __str__ + __str__ + __str__ ) ;,clause influence,fail,pre
expr index <PLACE_HOLDER> missing value with <PLACE_HOLDER> missing value,byte [ ] expected cache key = new byte [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ } ;,index indented,fail,pre
it is an error if document has <PLACE_HOLDER> l 1 nodes .,if ( attr . get local name ( ) == null ) { if ( f namespace validation ) { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { attr . get node name ( ) } ) ; reportdom error ( f error handler @$ f error @$ f locator @$ msg @$ dom error . severity_fatal_error @$ __str__ ) ; } else { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { attr . get node name ( ) } ) ; reportdom error ( f error handler @$ f error @$ f locator @$ msg @$ dom,document node,fail,pre
print <PLACE_HOLDER> depth @$ unless verbose was selected,if ( verbose ) { zkdu . print size map ( ) ; } else { zkdu . print size map depth ( depth ) ; } zkdu . close ( ) ;,print map,fail,pre
now <PLACE_HOLDER> the partitioning schema for the slave step ... for the slave step @$ we only should those partition i ds that are interesting for the current slave ...,step partitioning meta = target step partitioning meta . clone ( ) ; partition schema = step partitioning meta . get partition schema ( ) ; partition schema . set name ( create slave partition schema name ( partition schema . get name ( ) ) ) ; if ( partition schema . is dynamically defined ( ) ) { partition schema . expand partitions dynamically ( cluster schema . find nr slaves ( ) @$ original transformation ) ; },now set,success,pre
when an unrelated notification <PLACE_HOLDER> a new app op,m controller . update notifications for app op ( app ops manager . op_camera @$ __num__ @$ __str__ @$ true ) ;,notification installs,fail,pre
default to number class in exception details @$ else <PLACE_HOLDER> the specified number subtype .,return cast to number ( object @$ number . class ) ;,default accept,fail,pre
o <PLACE_HOLDER> the target to return our mock protocol,try { mockito . do return ( mock protocol ) . when ( spy ) . get proxy ( mockito . < configuration > any ( ) @$ mockito . any int ( ) ) ; mockito . do return ( mock zkfc protocol ) . when ( spy ) . getzkfc proxy ( mockito . < configuration > any ( ) @$ mockito . any int ( ) ) ; } catch ( io exception e ) { throw new assertion error ( e ) ; },o require,fail,pre
bg layout state future finished @$ let the ui thread complete layout . countdown happens here if test is successful . otherwise it will happen when the bg thread <PLACE_HOLDER> layout @$ which means ui thread still gets unblocked but assertion will fail .,unlockui thread layout . count down ( ) ;,thread finishes,fail,pre
no need to write anything on the client side @$ it will just <PLACE_HOLDER> the output .,client out = byte buffer . wrap ( __str__ . get bytes ( ) ) ;,need write,fail,pre
this is the response for the first page . <PLACE_HOLDER> the future in the cache so subsequent requests for the first page can return immediately .,if ( state tag . request position == null ) { should remove = false ; } else { should remove = true ; },page save,fail,pre
now remove the bytes we just processed . if @$ for some reason @$ the bytes array does not contain at least as many bytes as the instruction @$ then there 's a problem with the input . just <PLACE_HOLDER> a message to the user and exit .,if ( all bytes . size ( ) < instruction . get length ( ) ) { msg panel . set message text ( __str__ @$ color . red ) ; return ; } all bytes . sub list ( __num__ @$ instruction . get length ( ) ) . clear ( ) ;,now show,fail,pre
not seen yet ; must add an entry @$ return it . for that @$ we <PLACE_HOLDER> generator,object id generator < ? > generator = null ; if ( _object id generators == null ) { _object id generators = new array list < object id generator < ? > > ( __num__ ) ; } else { for ( int i = __num__ @$ len = _object id generators . size ( ) ; i < len ; ++ i ) { object id generator < ? > gen = _object id generators . get ( i ) ; if ( gen . can use for ( generator type ) ) { generator = gen ; break ; } } } if ( generator == null ) { generator = generator type . new for serialization ( this ) ; _object id generators .,not determine,fail,pre
reverse the parameter order so that the queue <PLACE_HOLDER> the oldest items,return b . last accessed copy < a . last accessed copy ;,queue has,fail,pre
the host <PLACE_HOLDER> a domain ; use its key,hash alias = alias ; key alias = alias ;,host specified,fail,pre
make sure user who do n't have <PLACE_HOLDER> access to file ca n't access raw xattr .,try { dfs . getx attr ( path @$ raw1 ) ; fail ( __str__ ) ; } catch ( access control exception ace ) { },who read,success,pre
try to verify whether the previous <PLACE_HOLDER> data success or not,if ( is retry ) { try { stat stat = new stat ( ) ; byte [ ] rev data = check zk ( ) . get data ( path @$ false @$ stat ) ; if ( bytes . compare to ( rev data @$ new data ) == __num__ ) { return stat ; } } catch ( keeper exception keeper exception ) { throw keeper exception ; } },previous get,fail,pre
color any interfering variables with different colors and any variables that can be safely coalesced <PLACE_HOLDER> the same color .,graph coloring < var @$ void > coloring = new greedy graph coloring < > ( interference graph @$ coloring tie breaker ) ; coloring . color ( ) ; colorings . push ( coloring ) ;,color share,fail,pre
this fragment <PLACE_HOLDER> each execution node to create the files that will be written to during the snapshot,byte [ ] hashinator bytes = ( hashinator data != null ? hashinator data . m_ser data : null ) ; long hashinator version = ( hashinator data != null ? hashinator data . m_version : __num__ ) ; return create and execute sys proc plan ( sys proc fragment id . pf_create snapshot targets @$ sys proc fragment id . pf_create snapshot targets results @$ file path @$ file nonce @$ per partition txn ids @$ block @$ format . name ( ) @$ data @$ hashinator bytes @$ hashinator version @$ system . current time millis ( ) @$ path type ) ;,fragment uses,fail,pre
no output stream . just <PLACE_HOLDER> the length of the serialized trie @$ in bytes .,if ( os == null ) { return length ; },output return,success,pre
initialize a complete map <PLACE_HOLDER> configuration,try { expr node desc expr1 = new expr node column desc ( type info factory . string type info @$ f1 @$ __str__ @$ false ) ; expr node desc expr2 = new expr node column desc ( type info factory . string type info @$ f2 @$ __str__ @$ false ) ; expr node desc filter expr = type check proc factory . default expr processor . get func expr node desc ( __str__ @$ expr1 @$ expr2 ) ; filter desc filter ctx = new filter desc ( filter expr @$ false ) ; operator < filter desc > op = operator factory . get ( new compilation op context ( ) @$ filter desc . class ) ; op . set conf ( filter,map reduce,success,pre
.split main body of client we use a lazy pirate strategy in the client . if there 's no reply within our timeout @$ we close the socket and try again . in binary star @$ it 's the client vote that decides which server is <PLACE_HOLDER> ; the client must therefore try to connect to each server in turn :,if ( poller . pollin ( __num__ ) ) { string reply = client . recv str ( ) ; if ( integer . parse int ( reply ) == sequence ) { system . out . printf ( __str__ @$ reply ) ; expect reply = false ; thread . sleep ( __num__ ) ; } else system . out . printf ( __str__ @$ reply ) ; } else { system . out . printf ( __str__ ) ; poller . unregister ( client ) ; ctx . destroy socket ( client ) ; server nbr = ( server nbr + __num__ ) % __num__ ; thread . sleep ( settle_delay ) ; system . out . printf ( __str__ @$ server [ server nbr ],server running,fail,pre
show only what the user required but make sure we show something and the spinners <PLACE_HOLDER> higher priority,if ( ! spinners shown && ! calendar view shown ) { set spinners shown ( true ) ; } else { set spinners shown ( spinners shown ) ; set calendar view shown ( calendar view shown ) ; },something have,success,pre
if all maps are assigned @$ then ramp up all <PLACE_HOLDER> irrespective of the headroom,if ( scheduled maps == __num__ && num pending reduces > __num__ ) { log . info ( __str__ + __str__ + num pending reduces ) ; schedule all reduces ( ) ; return ; } float completed map percent = __num__ ; if ( total maps != __num__ ) { completed map percent = ( float ) completed maps / total maps ; } else { completed map percent = __num__ ; },ramp reduces,success,pre
lock acquired @$ current thread <PLACE_HOLDER> this instance initialization,try { switch ( state ) { case ready : return instance ; case injecting : return instance ; case validated : state = injectable reference state . injecting ; break ; case new : throw new illegal state exception ( __str__ ) ; default : throw new illegal state exception ( __str__ + state ) ; } try { members injector . inject and notify ( instance @$ key @$ provision callback @$ source @$ injector . options . stage == stage . tool ) ; } catch ( internal provision exception ipe ) { throw ipe . add source ( source ) ; } state = injectable reference state . ready ; return instance ; } finally { lock . unlock ( ) ; },thread performed,fail,pre
implementation could have <PLACE_HOLDER> measurement when measurement was enabled @$ in which case size can be 0,if ( size < __num__ ) { throw new runtime exception ( __str__ + size ) ; } doit ( ) ;,implementation called,fail,pre
search <PLACE_HOLDER> position for first view,while ( left view offset > left border && left view position >= __num__ ) { if ( m is collapsed ) { left view offset -= ( m settings . get view width px ( ) + math . abs ( overlap distance ) ) ; } else { left view offset -= ( m settings . get view width px ( ) - math . abs ( overlap distance ) ) ; } left view position -- ; },search left,success,pre
for root elements <PLACE_HOLDER> serializer version,if ( ! is nested ) container . offset ++ ; if ( serializer . get current serializer ( ) . is serializing class name by default ( ) || is nested ) { read string ( container ) ; } int len = __num__ ; while ( len != __num__ ) { len = o var int serializer . read as integer ( container ) ; if ( len > __num__ ) { container . offset += len ; int pointer = read integer ( container ) ; pointer -= step size ; o integer serializer . instance . serialize literal ( pointer @$ container . bytes @$ container . offset - o integer serializer . int_size ) ; container . offset ++ ; } else if,elements check,fail,pre
rare averaging <PLACE_HOLDER> performance @$ but might reduce model accuracy,parallel wrapper wrapper = new parallel wrapper . builder ( net ) . prefetch buffer ( __num__ ) . workers ( __num__ ) . averaging frequency ( __num__ ) . report score after averaging ( true ) . build ( ) ;,averaging improves,success,pre
there are no regions @$ or no samples available . just <PLACE_HOLDER> the entire range .,if ( sample row keys . is empty ( ) ) { log . info ( __str__ @$ this ) ; return collections . singleton list ( this ) ; } log . info ( __str__ @$ desired bundle size bytes @$ sample row keys . size ( ) @$ sample row keys . get ( __num__ ) ) ; immutable list . builder < bigtable source > splits = immutable list . builder ( ) ; for ( byte key range range : ranges ) { splits . add all ( split range based on samples ( desired bundle size bytes @$ sample row keys @$ range ) ) ; } return splits . build ( ) ;,samples return,fail,pre
assuming side fields are <PLACE_HOLDER> its order,for ( string field : origin fields ) { if ( side fields . contains ( field ) ) { ret . add ( side values . get ( side idx ++ ) ) ; } else { ret . add ( state . group . get ( join fields . field index ( field ) ) ) ; } },fields ascending,fail,pre
internal or empty <PLACE_HOLDER> internal storage @$ neither or them <PLACE_HOLDER> sdcard storage,if ( ! __str__ . equals ignore case ( lite pal attr . get storage ( ) ) && ! text utils . is empty ( lite pal attr . get storage ( ) ) ) { string db path = environment . get external storage directory ( ) . get path ( ) + __str__ + lite pal attr . get storage ( ) ; db path = db path . replace ( __str__ @$ __str__ ) ; if ( base utility . is class and method exist ( __str__ @$ __str__ ) && context compat . check self permission ( lite pal application . get context ( ) @$ manifest . permission . write_external_storage ) != package manager . permission_granted ) { throw new database generate,means support,fail,pre
then the listeners <PLACE_HOLDER> separate instances of the bubble clock plugin .,argument captor < clock plugin > captor1 = argument captor . for class ( clock plugin . class ) ; argument captor < clock plugin > captor2 = argument captor . for class ( clock plugin . class ) ; verify ( m mock listener1 ) . on clock changed ( captor1 . capture ( ) ) ; verify ( m mock listener2 ) . on clock changed ( captor2 . capture ( ) ) ; assert that ( captor1 . get value ( ) ) . is instance of ( bubble_clock_class ) ; assert that ( captor2 . get value ( ) ) . is instance of ( bubble_clock_class ) ; assert that ( captor1 . get value ( ) ) . is not same as (,listeners generate,fail,pre
rightmost branch <PLACE_HOLDER> the order .,return immutable list . of ( __str__ @$ __str__ ) ;,branch determines,success,pre
the job should <PLACE_HOLDER> more than one time,if ( cur priority > executable . get default priority ( ) + __num__ ) { add to job pool ( executable @$ cur priority ) ; } else { left job priorities . put ( executable . get id ( ) @$ cur priority + __num__ ) ; },job take,fail,pre
no node <PLACE_HOLDER> node ids @$ so return the strict resource request node ids,return nodes for reqs . size ( ) ;,node check,fail,pre
no input consumed @$ better <PLACE_HOLDER> an error node,if ( _input . index ( ) == i ) { if ( e instanceof input mismatch exception ) { input mismatch exception ime = ( input mismatch exception ) e ; token tok = e . get offending token ( ) ; int expected token type = token . invalid_type ; if ( ! ime . get expected tokens ( ) . is nil ( ) ) { expected token type = ime . get expected tokens ( ) . get min element ( ) ; } token err token = get token factory ( ) . create ( new pair < token source @$ char stream > ( tok . get token source ( ) @$ tok . get token source ( ) . get input,input generate,fail,pre
when view <PLACE_HOLDER> no edges,get elements = new get elements . builder ( ) . input ( new entity seed ( __str__ ) @$ new entity seed ( __str__ ) ) . view ( new view . builder ( ) . entity ( test groups . entity ) . build ( ) ) . build ( ) ; results = graph . execute ( get elements @$ new user ( ) ) ;,view has,success,pre
when one line is larger than the other @$ it contains extra vertical p<PLACE_HOLDER>ing . this produces more apparent whitespace above or below the text lines . <PLACE_HOLDER> a small offset to compensate .,if ( line1 height != line2 height ) { vertical offset += ( line2 height - line1 height ) / __num__ ; },vertical add,success,pre
copy process groups <PLACE_HOLDER> the process groups @$ renaming as necessary,final set < process groupdto > groups = new hash set < > ( ) ; if ( snippet contents . get process groups ( ) != null ) { for ( final process groupdto groupdto : snippet contents . get process groups ( ) ) { final process groupdto cp = dto factory . copy ( groupdto @$ false ) ; cp . set id ( generate id ( groupdto . get id ( ) @$ id generation seed @$ is copy ) ) ; cp . set parent group id ( group id ) ; final flow snippetdto contents copy = copy contents for group ( groupdto . get contents ( ) @$ cp . get id ( ) @$ connectable map @$ service id map,groups happen,fail,pre
no such method should <PLACE_HOLDER> the caught exception . so if we get here @$ there was such a method .,if ( ! modifier . is static ( is method . get modifiers ( ) ) && is method . get annotation ( transient . class ) == null ) { check get and is variants ( container class @$ property name @$ get method @$ is method ) ; },method throw,success,pre
note : the hls spec <PLACE_HOLDER> initialization segments for packed audio .,if ( init load completed || init data spec == null ) { return ; },spec defines,fail,pre
specified attributes should already <PLACE_HOLDER> a normalized form since those were added by validator,if ( ! attr . get specified ( ) ) { return value ; },attributes have,success,pre
verifies <PLACE_HOLDER> access to the source and destination,return with write lock ( service facade @$ request connection entity @$ request revision @$ lookup -> { final authorizable authorizable = lookup . get connection ( id ) . get authorizable ( ) ; authorizable . authorize ( authorizer @$ request action . write @$ ni fi user utils . get ni fi user ( ) ) ; authorizable . get parent authorizable ( ) . authorize ( authorizer @$ request action . write @$ ni fi user utils . get ni fi user ( ) ) ; } @$ ( ) -> service facade . verify delete connection ( id ) @$ ( revision @$ connection entity ) -> { final connection entity entity = service facade . delete connection ( revision @$ connection entity .,verifies write,success,pre
calculate ifd group data area sizes . ifd group data area is assigned to save the entry value which <PLACE_HOLDER> a bigger size than 4 bytes .,for ( int i = __num__ ; i < exif_tags . length ; ++ i ) { int sum = __num__ ; for ( map . entry entry : ( set < map . entry > ) m attributes [ i ] . entry set ( ) ) { final exif attribute exif attribute = ( exif attribute ) entry . get value ( ) ; final int size = exif attribute . size ( ) ; if ( size > __num__ ) { sum += size ; } } ifd data sizes [ i ] += sum ; },which has,success,pre
since we <PLACE_HOLDER> extra nodes @$ there may be extra <PLACE_HOLDER> and cancel events @$ so we check only the difference between <PLACE_HOLDER> and cancel and not <PLACE_HOLDER> and cancel events individually .,assert equals ( name @$ total instances @$ dummy service . started ( name ) - dummy service . cancelled ( name ) ) ; check count ( name @$ g @$ total instances ) ; stop extra nodes ( extra nodes ) ;,events start,success,pre
we could move this three statements below registration @$ but then we should change the logic of the subscriber : if you register before any value is in @$ you 'll get a null value before in on initialize and subsequently values in on add . therefore moving those <PLACE_HOLDER> a refactor of the subscriber,zk persistent connection zk persistent connection = zk store test only util . get zk persistent connection ( port ) ; zoo keeper ephemeral store < string > writer store = zk store test only util . get zoo keeper ephemeral store ( port ) ; writer store . put ( test_zk_prop_name @$ __str__ ) ; property event bus . register ( collections . singleton ( test_zk_prop_name ) @$ new property event subscriber < string > ( ) { @ override public void on initialize ( string property name @$ string property value ) { if ( property name != null ) { initialized latch . count down ( ) ; } } @ override public void on add ( string property name @$ string property value ),those creates,fail,pre
drop the request silently if memory aware thread pool has <PLACE_HOLDER> the flag .,if ( read suspended ) { e . get future ( ) . set success ( ) ; return true ; },pool set,success,pre
see if user is <PLACE_HOLDER> a new policy file,if ( filename == null ) { modified = false ; return ; },user creating,fail,pre
make sure that the scanner does n't <PLACE_HOLDER> an exception after the connection cache timeout,for ( int i = __num__ ; i < num trials ; i ++ ) { list < t result > results = handler . get scanner rows ( scan id @$ __num__ ) ; assert array equals ( bytes . to bytes ( __str__ + i ) @$ results . get ( __num__ ) . get row ( ) ) ; thread . sleep ( trial pause ) ; },scanner throw,success,pre
drwho is not authorized to access test protocol 1 because it uses the default <PLACE_HOLDER> acl .,try { service authorization manager . authorize ( drwho @$ test protocol1 . class @$ conf @$ inet address . get by name ( address ) ) ; fail ( ) ; } catch ( authorization exception e ) { },default read,fail,pre
set hll <PLACE_HOLDER> 2 m .,_hll log2m = segment metadata properties configuration . get int ( segment_hll_log2m @$ hll constants . default_log2m ) ;,hll log,success,pre
let the emulator view <PLACE_HOLDER> taps if mouse tracking is active,if ( view . is mouse tracking active ( ) ) return false ;,view handle,success,pre
in this case the top activity on the task is the same as the one being launched @$ so we take that as a request to <PLACE_HOLDER> the task to the foreground . if the top activity in the task is the root activity @$ deliver this new intent to it if it desires .,if ( m start activity . m activity component . equals ( intent activity . get task record ( ) . real activity ) ) { if ( ( ( m launch flags & flag_activity_single_top ) != __num__ || launch_single_top == m launch mode ) && intent activity . m activity component . equals ( m start activity . m activity component ) ) { if ( intent activity . front of task ) { intent activity . get task record ( ) . set intent ( m start activity ) ; } deliver new intent ( intent activity ) ; } else if ( ! intent activity . get task record ( ) . is same intent filter ( m start activity ) ) { m adding,request draw,fail,pre
user <PLACE_HOLDER> default package there wo n't be any package to annotate @$ so disable them automatically as a usability feature,if ( default package . length ( ) == __num__ ) { package level annotations = false ; },user specified,success,pre
this tests also <PLACE_HOLDER> type node concurrency,final string drl = __str__ + bean . class . get canonical name ( ) + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; test concurrent insertions ( drl @$ __num__ @$ __num__ @$ true @$ true ) ;,tests repeated,fail,pre
the time this app will <PLACE_HOLDER> quota again .,long in quota time elapsed = stats . in quota time elapsed ; if ( ! is under job count quota && stats . bg job count in window < stats . job count limit ) { in quota time elapsed = math . max ( in quota time elapsed @$ stats . job rate limit expiration time elapsed ) ; },app have,success,pre
unlike pql @$ sql <PLACE_HOLDER> the group columns in select statements .,string group by columns = string utils . join ( _group columns @$ __str__ ) ;,sql contains,fail,pre
the current path must <PLACE_HOLDER> potential to be better than the best match,if ( ! best match . is empty ( ) && path info . distance ( current path ) >= path info . distance ( best match ) ) return false ;,path have,success,pre
if crlf just <PLACE_HOLDER> lf,if ( c == __str__ ) { c = read ( reader ) ; if ( c != __str__ ) { last read = c ; use last read = true ; c = __str__ ; } },crlf return,fail,pre
lhs shares and rhs shares must necessarily have the same length @$ because everyone <PLACE_HOLDER> the same master resource list .,for ( int i = lhs shares . length - __num__ ; i >= __num__ ; i -- ) { if ( lhs shares [ i ] == float . positive_infinity || rhs shares [ i ] == float . positive_infinity ) { continue ; } diff = lhs shares [ i ] - rhs shares [ i ] ; if ( diff != __num__ ) { break ; } } return diff ;,everyone holds,fail,pre
a single checkout does not <PLACE_HOLDER> a dialog,return ;,checkout create,fail,pre
let the optional watcher <PLACE_HOLDER> a peak,if ( m_watcher != null ) { m_watcher . handle line ( data ) ; },watcher generate,fail,pre
we 'll skip words which are punctuation . retrieve tags <PLACE_HOLDER> punctuation in this treebank .,set < string > punctuation tags = get punctuation tags ( ) ; if ( trees . size ( ) != gold trees . size ( ) ) { log . error ( __str__ ) ; return null ; } int correct arcs = __num__ ; int correct arcs no punc = __num__ ; int correct heads = __num__ ; int correct heads no punc = __num__ ; int correct trees = __num__ ; int correct trees no punc = __num__ ; int correct root = __num__ ; int sum arcs = __num__ ; int sum arcs no punc = __num__ ; for ( int i = __num__ ; i < trees . size ( ) ; ++ i ) { list < core label > tokens =,tags handling,fail,pre
the default entry point <PLACE_HOLDER> the user interface .,start application ( null ) ;,point interrupts,fail,pre
these credentials must <PLACE_HOLDER> those in the default password file,string [ ] credentials = new string [ ] { __str__ @$ __str__ } ; cli_env . put ( __str__ @$ credentials ) ; jmxc = jmx connector factory . connect ( url @$ cli_env ) ; m bean server connection mbsc = jmxc . getm bean server connection ( ) ;,credentials match,success,pre
there exists one independent iterator in the current condition which is also a part of the intermediate resultset <PLACE_HOLDER> the final list which will depend upon the complete expansion flag <PLACE_HOLDER> the iterators to be expanded to @$ which will also depend upon complete expansion flag..,if ( no of indexes to use == __num__ ) { list total exp list = new array list ( single usableich . expansion list ) ; if ( complete expansion needed ) { support . assert ( expn itrs to ignore != null @$ __str__ ) ; expn itrs to ignore . add all ( single usableich . final list ) ; int size = final list . size ( ) ; for ( int i = __num__ ; i < size ; ++ i ) { runtime iterator curr itr = ( runtime iterator ) final list . get ( i ) ; if ( ! expn itrs to ignore . contains ( curr itr ) ) { total exp list . add ( curr itr,list identify,success,pre
these classes should <PLACE_HOLDER> no line numbers @$ except for one in the implicit constructor .,reference type rt = find reference type ( __str__ ) ; if ( rt == null ) { throw new exception ( __str__ ) ; } method method = find method ( rt @$ __str__ @$ __str__ ) ; if ( method == null ) { throw new exception ( __str__ ) ; } test ( method @$ variables @$ __str__ @$ __str__ ) ; test ( method @$ byname @$ __str__ @$ __str__ ) ; test ( method @$ arguments @$ __str__ @$ __str__ ) ; method = find method ( rt @$ __str__ @$ __str__ ) ; if ( method == null ) { throw new exception ( __str__ ) ; } test ( method @$ variables @$ __str__ @$ __str__ ) ; test ( method,classes have,success,pre
if the pattern <PLACE_HOLDER> an accumulate as source it wo n't be relevant for calculation of property reactivity masks,if ( ! ( pattern . get source ( ) instanceof accumulate ) ) { context . set last built pattern ( pattern ) ; },pattern has,success,pre
do n't do anything here . the user did n't <PLACE_HOLDER> the producer .,if ( producer edited == false ) { } else { try { tag . set field ( field key . producer @$ producer edit text . get text ( ) . to string ( ) ) ; } catch ( key not found exception e ) { e . print stack trace ( ) ; } catch ( field data invalid exception e ) { e . print stack trace ( ) ; } catch ( no such element exception e ) { e . print stack trace ( ) ; } },user change,success,pre
since this function may not <PLACE_HOLDER> the underlying inner store @$ we need to validate if store is open outside as well .,validate store open ( ) ; final key value iterator < windowed < bytes > @$ byte [ ] > underlying iterator = wrapped ( ) . fetch ( from @$ to @$ time from @$ time to ) ; if ( cache == null ) { return underlying iterator ; } final peeking key value iterator < bytes @$ lru cache entry > cache iterator = wrapped ( ) . persistent ( ) ? new cache iterator wrapper ( from @$ to @$ time from @$ time to ) : cache . range ( name @$ cache function . cache key ( key schema . lower range ( from @$ time from ) ) @$ cache function . cache key ( key schema . upper range (,function modify,fail,pre
windows secure container executor will <PLACE_HOLDER> permissions to allow nm to read the file,line ( __str__ ) ; line with len check ( string . format ( __str__ @$ src . to string ( ) @$ dest . to string ( ) ) ) ;,executor grant,fail,pre
data source should <PLACE_HOLDER> polling only after command log replay on a recover,source . set ready for polling ( m_start polling ) ; runner . run ( ) ;,source start,success,pre
ensure broker <PLACE_HOLDER> a chance to send on the new connection,time unit . seconds . sleep ( __num__ ) ; log . info ( __str__ + socket ) ; socket . close ( ) ;,broker gets,success,pre
backslash and <PLACE_HOLDER> quote need <PLACE_HOLDER> the escapement for both java and haskell,special char replacements . remove ( __str__ ) ; special char replacements . remove ( __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ;,backslash take,fail,pre
check that the byte array <PLACE_HOLDER> the complete packet,if ( len > b . length ) { throw new illegal argument exception ( __str__ ) ; } try { synchronized ( send lock ) { send packet0 ( id @$ b ) ; } } catch ( io exception ioe ) { if ( ! is open ( ) ) { throw new closed connection exception ( __str__ ) ; } else { throw ioe ; } },array contains,success,pre
we ca n't do anr checks after we cease to exist ! reset any <PLACE_HOLDER> behavior changes we might have made .,if ( m closed . compare and set ( false @$ true ) ) { set detect not responding ( __num__ ) ; if ( m stable ) { return m content resolver . release provider ( m content provider ) ; } else { return m content resolver . release unstable provider ( m content provider ) ; } } else { return false ; },any posted,fail,pre
create three task generator threads . each of them will <PLACE_HOLDER> different number of jobs .,final runnable task runnable1 = new task generator ( msg queue @$ __num__ ) ; final runnable task runnable2 = new task generator ( msg queue @$ __num__ ) ; final runnable task runnable3 = new task generator ( msg queue @$ __num__ ) ;,threads submit,success,pre
check again with hashtable just in case another thread <PLACE_HOLDER> a handler since we last checked,handler2 = handlers . get ( protocol ) ; if ( handler2 != null ) { return handler2 ; },thread created,success,pre
accessible j label implements accessible text if the j label <PLACE_HOLDER> html text,if ( note label != null ) { return note label . get accessible context ( ) . get accessible text ( ) ; },label contains,success,pre
if no entry <PLACE_HOLDER> skipping rows until we come to the end @$ or find one that is populated,while ( this . entry == null && this . row < this . length ) { this . entry = this . table [ this . row ] ; this . row ++ ; } return this . entry ;,entry found,fail,pre
only global stats <PLACE_HOLDER> sense,_total stats . register buffer metrics ( r @$ s @$ since @$ free space ) ;,stats make,success,pre
each source lines below should <PLACE_HOLDER> 32 bytes @$ until the next comment .,return new byte array input stream ( ( __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ) . get bytes ( ) ) ;,lines represent,success,pre
does the class <PLACE_HOLDER> optimization info ?,if ( class optimization info . get class optimization info ( library class ) != null ) { class visitor . visit library class ( library class ) ; },class have,success,pre
this check is somewhat unnecessary as all partitioned regions should have the same scope due to the fact that partitioned regions <PLACE_HOLDER> no support scope .,if ( profile . scope . is distributed ( ) && other scope . is distributed ( ) ) { if ( profile . scope != other scope ) { result = string . format ( __str__ @$ this . region path @$ profile . scope @$ my id @$ other scope ) ; } },regions have,fail,pre
when streaming @$ we only <PLACE_HOLDER> whole paragraphs thus some updates are only done on paragraph boundaries,if ( ( reordering options & option_streaming ) != __num__ ) { length = i ; this . control count = control count ; },streaming show,fail,pre
reordering the input from the previous test does not <PLACE_HOLDER> the resulting order,list < spawn exec > l = test stable sort ( immutable list . of ( f @$ e @$ d @$ c @$ b @$ a ) ) ; assert that ( l ) . contains exactly ( a @$ c @$ d @$ b @$ e @$ f ) . in order ( ) ;,input change,success,pre
j progress bar.vertical <PLACE_HOLDER> each tile vertically,for ( int i = y - offset ; i < height + tile width ; i += tile width ) { context . get painter ( ) . paint progress bar foreground ( context @$ g @$ x @$ i @$ width @$ tile width @$ p bar . get orientation ( ) ) ; },bar.vertical paints,fail,pre
do everything as bytes ; do n't <PLACE_HOLDER> any conversion,@ suppress warnings ( __str__ ) buffered input stream istr = new buffered input stream ( new file input stream ( args [ __num__ ] ) ) ; byte array output stream ostr = new byte array output stream ( ) ; byte [ ] bytes = new byte [ __num__ ] ; boolean more = true ; while ( more ) { int len = istr . read ( bytes ) ; if ( len < bytes . length ) { more = false ; if ( len > __num__ ) { ostr . write ( bytes @$ __num__ @$ len ) ; } } else { ostr . write ( bytes ) ; } } byte [ ] data = ostr . to byte array (,everything force,fail,pre
add the mapper scanner as a bean definition rather than explicitly setting a post processor on the context so initialization <PLACE_HOLDER> the same code path as reading from an xml config file,generic bean definition definition = new generic bean definition ( ) ; definition . set bean class ( mapper scanner configurer . class ) ; definition . get property values ( ) . add ( __str__ @$ __str__ ) ; application context . register bean definition ( __str__ @$ definition ) ; setup sql session factory ( __str__ ) ;,initialization follows,success,pre
this rule <PLACE_HOLDER> an error .,verifier rule rule1 = verifier component mock factory . create rule1 ( ) ; pattern pattern1 = verifier component mock factory . create pattern1 ( ) ; sub pattern pp1 = new sub pattern ( pattern1 @$ __num__ ) ; sub pattern pp2 = new sub pattern ( pattern1 @$ __num__ ) ; incompatibility i1 = new incompatibility ( pp1 @$ pp2 ) ; sub rule rp1 = new sub rule ( rule1 @$ __num__ ) ; rp1 . add ( pp1 ) ; rp1 . add ( pp2 ) ; sub pattern pp3 = new sub pattern ( pattern1 @$ __num__ ) ; sub pattern pp4 = new sub pattern ( pattern1 @$ __num__ ) ; incompatibility i2 = new incompatibility ( pp1 @$ pp2 ) ;,rule has,success,pre
pkcs 11 test.main will <PLACE_HOLDER> this provider if needed,providers . set at ( p @$ __num__ ) ; random = new secure random ( ) ; factory = certificate factory . get instance ( __str__ ) ; try { factory . generate certificate ( null ) ; } catch ( certificate exception e ) { },test.main remove,success,pre
tests all expect to create a new ds <PLACE_HOLDER> the test object numinstance for the next test .,test object . num instance = __num__ ; portfolio pdx . debug = false ;,ds reset,success,pre
check that deleting the ip address <PLACE_HOLDER> the keepalive .,link properties bogus lp = new link properties ( lp ) ; try ( socket keepalive ka = m cm . create socket keepalive ( my net @$ test socket @$ myi pv4 @$ dsti pv4 @$ executor @$ callback ) ) { ka . start ( valid ka interval ) ; callback . expect started ( ) ; bogus lp . remove link address ( new link address ( myi pv4 @$ __num__ ) ) ; bogus lp . add link address ( new link address ( not myi pv4 @$ __num__ ) ) ; m wi fi network agent . send link properties ( bogus lp ) ; callback . expect error ( socket keepalive . error_invalid_ip_address ) ; m wi fi network agent . send,check stops,success,pre
unknown <PLACE_HOLDER> environment @$ assuming native .,log . ignore ( t ) ;,unknown build,fail,pre
just for overriding the findbugs <PLACE_HOLDER> warnings @$ as the parameter is marked as nullable in the guava predicate .,return iterables . filter ( files @$ new predicate < file status > ( ) { @ override public boolean apply ( file status file ) { if ( file == null ) { return false ; } string wal = file . get path ( ) . get name ( ) ; boolean log in replication queue = wals . contains ( wal ) ; if ( log in replication queue ) { log . debug ( __str__ @$ wal ) ; } return ! log in replication queue && ( file . get modification time ( ) < readzk timestamp ) ; } } ) ;,findbugs np,success,pre
the act of persisting does n't <PLACE_HOLDER> anything @$ so our id is still 1 .,assert equals ( integer . value of ( __num__ ) @$ author . get id ( ) ) ;,act flush,success,pre
if this is a .com or jetpack blog @$ tapping the title <PLACE_HOLDER> the associated post in the reader,if ( can request post ) { if ( ! post exists ) { app log . d ( t . comments @$ __str__ ) ; reader post actions . request blog post ( site . get site id ( ) @$ post id @$ new reader actions . on request listener ( ) { @ override public void on success ( ) { if ( ! is added ( ) ) { return ; } if ( ! has title ) { string post title = reader post table . get post title ( site . get site id ( ) @$ post id ) ; if ( ! text utils . is empty ( post title ) ) { set post title ( txt post title,title set,fail,pre
! ! ! would n't preflighting be simpler ? this looks like it is effectively be doing that . it seems that for every true error the code will <PLACE_HOLDER> errln @$ which will throw the error @$ which this will catch @$ which this will then rethrow the error . just seems cumbersome .,if ( e . get message ( ) . index of ( __str__ ) >= __num__ ) { warnln ( __str__ ) ; } else { errln ( e . get message ( ) ) ; },code call,success,pre
this is admittedly a bit simple @$ stats object converter seems to allow old stats attributes to be kept if the new values do not <PLACE_HOLDER> them .,for ( column statistics obj col stat : new col list ) { old stats . put ( col stat . get col name ( ) . to lower case ( ) @$ col stat ) ; },values have,fail,pre
the enum bit set ; classes for specialized enum constants <PLACE_HOLDER> n't <PLACE_HOLDER> the former .,return ( this . get modifiers ( ) & enum ) != __num__ && this . get superclass ( ) == java . lang . enum . class ;,classes do,success,pre
update button deselects bundles @$ revert buttons to <PLACE_HOLDER> state,default button state ( ) ;,buttons restore,fail,pre
if the user <PLACE_HOLDER> a message id @$ use it .,final string message id = one way feature . get message id ( ) ; if ( ! is message id added && message id != null ) { headers . add ( new string header ( av . messageid tag @$ message id ) ) ; },user specified,fail,pre
need also <PLACE_HOLDER> the case if surface width is smaller than scissor width .,if ( surface width < scissor width ) { surface percentageh *= ( float ) surface width / scissor width ; },need consider,success,pre
some services automatically <PLACE_HOLDER> contacts from their <PLACE_HOLDER>ressbook to the roster and those contacts are with subscription none . if such already exist @$ remove them . this is typically our own contact,if ( ! is entry displayable ( item ) ) { if ( contact != null ) { contact group parent = contact . get parent contact group ( ) ; if ( parent instanceof root contact group jabber impl ) ( ( root contact group jabber impl ) parent ) . remove contact ( contact ) ; else ( ( contact group jabber impl ) parent ) . remove contact ( contact ) ; fire contact removed ( parent @$ contact ) ; } continue ; } if ( contact == null ) { contact = new contact jabber impl ( item @$ this @$ true @$ true ) ; root group . add contact ( contact ) ; fire contact added ( root group @$ contact,services add,success,pre
if parent statistics is null then that branch of the tree is not walked yet . do n't <PLACE_HOLDER> the stats until all branches are walked,if ( stats == null && op . get parent operators ( ) != null ) { if ( is all parents contain statistics ( op ) ) { for ( operator < ? extends operator desc > parent : op . get parent operators ( ) ) { statistics parent stats = parent . get statistics ( ) ; if ( stats == null ) { stats = parent stats . clone ( ) ; } else { stats . add basic stats ( parent stats ) ; } stats . update column stats state ( parent stats . get column stats state ( ) ) ; list < col statistics > col stats = stats utils . get col statistics from expr map ( hconf @$,branch store,fail,pre
indicates that the cell <PLACE_HOLDER> a the tag which was modified in the src replication cluster,tag tag = pair . get second ( ) ; if ( cell visibility == null && tag != null ) { cell visibility = new cell visibility ( tag . get value as string ( tag ) ) ; modified tag found = true ; },cell had,fail,pre
call set system ui visibility with flags which will <PLACE_HOLDER> window insets to be dispatched,final int flags = view . system_ui_flag_layout_stable | view . system_ui_flag_layout_fullscreen ; on view ( with id ( r . id . test_content ) ) . perform ( set system ui visibility ( flags ) ) ;,which cause,success,pre
shallow equals is a middle ground between using default equals @$ which might <PLACE_HOLDER> nested sets with the same elements @$ and deep equality checking @$ which would be expensive . all three choices are sound @$ since shallow equals and default equals are more conservative than deep equals . using shallow equals means that we may unnecessarily consider some values unequal that are,return objects . equals ( this . value @$ that . value ) && objects . equals ( this . error info @$ that . error info ) && transitive events . shallow equals ( that . transitive events ) && transitive postables . shallow equals ( that . transitive postables ) ;,which miss,success,pre
let authenticator <PLACE_HOLDER> the identity of the caller,login options . put int ( account manager . key_caller_uid @$ caller uid ) ; login options . put int ( account manager . key_caller_pid @$ binder . get calling pid ( ) ) ; if ( notify on auth failure ) { login options . put boolean ( account manager . key_notify_on_failure @$ true ) ; } long identity token = clear calling identity ( ) ; try { final byte [ ] caller pkg sig digest = calculate package signature digest ( caller pkg ) ; if ( ! custom tokens && permission granted ) { string auth token = read auth token internal ( accounts @$ account @$ auth token type ) ; if ( auth token != null ) { bundle result = new,authenticator change,fail,pre
default to the global scope because externs may <PLACE_HOLDER> qualified names with undeclared roots .,if ( root var == null ) { return this . get parent ( ) . get global scope ( ) . get own slot ( name ) ; } else { return root var . get scope ( ) . get own slot ( name ) ; },externs have,success,pre
only new user state <PLACE_HOLDER> array defined ; different,final package user state test user state01 = new package user state ( ) ; test user state01 . disabled components = new array set < > ( ) ; assert that ( test user state01 . equals ( old user state ) @$ is ( false ) ) ;,state has,success,pre
clear calling identity as initialization <PLACE_HOLDER> the system identity but we can be coming from shell .,if ( get user manager ( ) . is user unlocked ( user id ) ) { long old id = binder . clear calling identity ( ) ; try { start service for user ( user id ) ; } finally { binder . restore calling identity ( old id ) ; } },identity reuses,fail,pre
null should not <PLACE_HOLDER> count,current count = double count kudaf . aggregate ( null @$ current count ) ; assert that ( __num__ @$ equal to ( current count ) ) ;,null impact,fail,pre
get the first person <PLACE_HOLDER> point and rotate it away from the camera,character held item component character held item component = local player . get character entity ( ) . get component ( character held item component . class ) ; first person held item mount point component mount point component = local player . get camera entity ( ) . get component ( first person held item mount point component . class ) ; if ( character held item component == null || mount point component == null ) { return ; } location component location component = mount point component . mount point entity . get component ( location component . class ) ; if ( location component == null ) { return ; } long time elapsed since last used = time . get game time in,person mount,success,pre
no node for this letter yet . <PLACE_HOLDER> one .,if ( ! found ) { char node = new trie node ( ) ; m trie [ herep ] = node ; m trie [ m trie [ herep ] + trie_c ] = c ; m trie [ m trie [ herep ] + trie_off ] = trie_null ; m trie [ m trie [ herep ] + trie_next ] = trie_null ; m trie [ m trie [ herep ] + trie_child ] = trie_null ; if ( i == slen - __num__ ) { m trie [ m trie [ herep ] + trie_off ] = off ; return ; } herep = m trie [ herep ] + trie_child ; },node create,fail,pre
order does n't <PLACE_HOLDER> size,output assembler temp = assembly buffer . create output assembler ( byte order . native order ( ) ) ; write ( temp ) ; return temp . pos ( ) ;,order affect,success,pre
package 3 <PLACE_HOLDER> a rule working on person instances . as we added person instance in advance @$ rule should fire now,working memory . fire all rules ( ) ; assert equals ( __str__ @$ __str__ @$ bob . get status ( ) ) ; assert equals ( __num__ @$ list . size ( ) ) ; assert equals ( bob @$ list . get ( __num__ ) ) ; kpkgs = serialization helper . serialize object ( load knowledge packages ( __str__ ) ) ; kbase . add packages ( kpkgs ) ; working memory . fire all rules ( ) ; kbase = serialization helper . serialize object ( kbase ) ; assert equals ( __str__ @$ __str__ @$ bob . get status ( ) ) ; assert equals ( __num__ @$ list . size ( ) ) ; assert equals ( bob @$ list . get,package creates,fail,pre
if the referred document <PLACE_HOLDER> a target namespace differing from the caller @$ it 's an error,if ( callertns != curr schema info . f target namespace ) { report schema error ( ns_error_codes [ refer type ] [ second idx ] @$ new object [ ] { callertns @$ curr schema info . f target namespace } @$ schema root ) ; return null ; },document has,success,pre
check whether first tile <PLACE_HOLDER> the beginning of the tile vertically,int first top = traps . get top ( trap list . get int ( __num__ ) ) ; int first bottom = traps . get bottom ( trap list . get int ( __num__ ) ) ; if ( first top > tile starty || first bottom < tile starty ) { return false ; },tile hits,fail,pre
no reducers . just write straight to table . <PLACE_HOLDER> init table reducer job because it sets up the table output format .,job . set mapper class ( importer . class ) ; table map reduce util . init table reducer job ( table name . get name as string ( ) @$ null @$ job ) ; job . set num reduce tasks ( __num__ ) ;,reducers call,success,pre
'not found ' can happen if import <PLACE_HOLDER> more than one subnetwork,throw new runtime exception ( __str__ + __str__ + from + __str__ + to + __str__ + req @$ ex ) ;,import has,fail,pre
java 's <PLACE_HOLDER> function <PLACE_HOLDER> every character in origin with <PLACE_HOLDER>ment @$ while origin value should not be changed is expected in sql .,if ( target . length ( ) == __num__ ) { return origin ; } return origin . replace ( target @$ replacement ) ;,function replace,success,pre
if the size of bloom filter is smaller than 1 mb @$ use default <PLACE_HOLDER> false positive probability,if ( num bits required <= max num bits ) { return default max false pos probability ; },default max,success,pre
assume approximately half of values will <PLACE_HOLDER> a filter,return cost_node_unspecific_predicate ;,half pass,fail,pre
sometimes providers do not <PLACE_HOLDER> keys the same way . e.g . bouncy castle may use long form encoding @$ where jdk uses a short encoding with named curves . this checks the encodings and logs them if they differ .,string hex reencoded key = hex . encode ( pub key . get encoded ( ) ) ; if ( ! hex pub key . equals ( hex reencoded key ) ) { system . out . println ( __str__ + hex pub key ) ; system . out . println ( __str__ + hex reencoded key ) ; },providers resolve,fail,pre
this call will <PLACE_HOLDER> all the basic info into the element,element element = super . toxml ( ) ;,call put,success,pre
moving the timer 6 minutes should <PLACE_HOLDER> the timer,for ( int i = __num__ ; i < __num__ ; i ++ ) { current time = new date ( current time . get time ( ) + ( __num__ * __num__ * __num__ ) ) ; set clock to ( current time ) ; job job = cmmn management service . create timer job query ( ) . case instance id ( case instance . get id ( ) ) . single result ( ) ; assert true ( job . get duedate ( ) . get time ( ) - current time . get time ( ) <= ( __num__ * __num__ * __num__ ) ) ; job = cmmn management service . move timer to executable job ( job . get id ( ),timer move,fail,pre
this checks for null data type manager below since bad data type wo n't <PLACE_HOLDER> one .,source archive source archive = data type . get source archive ( ) ; boolean local source = ( source archive == null ) || ( ( data type manager != null ) && system utilities . is equal ( data type manager . get universalid ( ) @$ source archive . get source archiveid ( ) ) ) ; if ( local source ) { source archive = originaldtm . get source archive ( originaldtm . get universalid ( ) ) ; } data type found data type = originaldtm . get data type ( data type . get data type path ( ) ) ; string display name = __str__ ; if ( found data type != null && ( data type manager != null ),checks have,success,pre
at boot time @$ we know that the screen is on and the electron beam animation is not playing . we do n't know the screen 's brightness though @$ so prepare to set it to a known state when the state is next applied . although we set the brightness to full on here @$ the display power controller will reset the brightness,m screen state = display . state_on ; m screen brightness = power manager . brightness_on ; schedule screen update ( ) ; m color fade prepared = false ; m color fade level = __num__ ; m color fade ready = true ;,changes recover,fail,pre
<PLACE_HOLDER> <PLACE_HOLDER> a height of 1 @$ and then wack the matrix each time we actually <PLACE_HOLDER> it .,shader = new linear gradient ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ shader . tile mode . clamp ) ; paint . set shader ( shader ) ; paint . set xfermode ( new porter duff xfermode ( porter duff . mode . dst_out ) ) ; this . host = host ;,use have,fail,pre
check if the current segment <PLACE_HOLDER> the given index first in order to prevent an unnecessary map lookup .,if ( current segment != null && index > current segment . index ( ) ) { return current segment ; },segment has,fail,pre
we will approve the request @$ which will <PLACE_HOLDER> the entity,variables = new hash map < > ( ) ; variables . put ( __str__ @$ boolean . true ) ; task task = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) @$ variables ) ;,which update,success,pre
try to find columns in left table which <PLACE_HOLDER> unique key in right table,sql = generate table join by columns ( monitor @$ left table @$ left alias @$ right table @$ right alias ) ; if ( sql != null ) return sql ;,which have,fail,pre
trigger should <PLACE_HOLDER> visibility .,assert true ( m accounts db . delete de account ( acc id ) ) ;,trigger undo,fail,pre
this overwrites whatever <PLACE_HOLDER> the user configured in the properties,adjust auto commit config ( properties @$ offset commit mode ) ; return new kafka fetcher < > ( source context @$ assigned partitions with initial offsets @$ watermarks periodic @$ watermarks punctuated @$ runtime context . get processing time service ( ) @$ runtime context . get execution config ( ) . get auto watermark interval ( ) @$ runtime context . get user code class loader ( ) @$ runtime context . get task name with subtasks ( ) @$ deserializer @$ properties @$ poll timeout @$ runtime context . get metric group ( ) @$ consumer metric group @$ use metrics ) ;,whatever setting,success,pre
if s does n't <PLACE_HOLDER> the current line to cross the limit @$ buffer it and return . we 'll decide whether or not we have to wrap it later .,if ( next newline == - __num__ && column + s . length ( ) <= column limit ) { buffer . append ( s ) ; column += s . length ( ) ; return ; },s cause,success,pre
drop should succeed since no query is <PLACE_HOLDER> the table,final optional < command status > drop table command status2 = statement executor . get status ( drop table command id2 ) ; assert . assert true ( drop table command status2 . is present ( ) ) ; assert that ( drop table command status2 . get ( ) . get status ( ) @$ equal to ( command status . status . success ) ) ;,query using,success,pre
there are too many configuration processors so we do n't know which one to <PLACE_HOLDER> so report the error .,if ( user supplied configuration processor count > __num__ ) { string builder sb = new string builder ( string . format ( __str__ @$ user supplied configuration processor count ) ) ; for ( entry < string @$ configuration processor > entry : configuration processors . entry set ( ) ) { string hint = entry . get key ( ) ; if ( ! hint . equals ( settings xml configuration processor . hint ) ) { configuration processor configuration processor = entry . get value ( ) ; sb . append ( string . format ( __str__ @$ configuration processor . get class ( ) . get name ( ) ) ) ; } } sb . append ( __str__ ) ; throw new exception,one use,fail,pre
we need to go through a list of appenders and locate the async ones @$ as those could <PLACE_HOLDER> messages left to write . since there is no flushing mechanism built into logback @$ we wait for a short period of time before giving up that the appender will be completely flushed .,try { final logger logger = logger context . get logger ( org . slf4j . logger . root_logger_name ) ; final list < appender < i logging event > > appenders = lists . of ( logger . iterator for appenders ( ) ) ; for ( appender < i logging event > appender : appenders ) { if ( appender instanceof async appender base ) { flush appender ( ( async appender base < ? > ) appender ) ; } else if ( appender instanceof async appender base proxy ) { flush appender ( ( ( async appender base proxy < ? > ) appender ) . get appender ( ) ) ; } } } catch ( interrupted exception ignored ) { thread .,those have,success,pre
make sure master is fully up before progressing . could <PLACE_HOLDER> a while if regions being reassigned .,while ( ! master . get master ( ) . is initialized ( ) ) { threads . sleep ( __num__ ) ; },progressing take,success,pre
cache was probably closed which <PLACE_HOLDER> this lock service note : <PLACE_HOLDER> lock services release all held locks,cache . get cancel criterion ( ) . check cancel in progress ( null ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . lock service @$ e ) ; },which closed,fail,pre
now <PLACE_HOLDER> the same test @$ but make all the requests for one group first @$ then the second group .,for ( int i = __num__ ; i < __num__ ; i ++ ) { server location l = sn . get server for connection ( __str__ @$ collections . empty_set ) ; assert true ( l1 . equals ( l ) || l2 . equals ( l ) ) ; } expected = new hash map ( ) ; expected . put ( l1 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; expected . put ( l2 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; expected . put ( l3 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; assert equals ( expected @$ sn . get,now do,success,pre
request decommission for data <PLACE_HOLDER> 1 and 2 .,list < datanode info > decom data nodes = take node outof service ( __num__ @$ lists . new array list ( get cluster ( ) . get data nodes ( ) . get ( __num__ ) . get datanode uuid ( ) @$ get cluster ( ) . get data nodes ( ) . get ( __num__ ) . get datanode uuid ( ) ) @$ long . max_value @$ null @$ null @$ admin states . decommissioned ) ; generic test utils . wait for ( new supplier < boolean > ( ) { @ override public boolean get ( ) { try { string err msg = check file ( file sys @$ file @$ repl @$ decom data nodes . get ( __num__ ),decommission nodes,success,pre
scan will <PLACE_HOLDER> blocks to be added in block cache,scan all regions forrs ( rs1 ) ; assert equals ( block cache1 . get block count ( ) - initial block count1 @$ htu . get numh files forrs ( rs1 @$ table_name @$ family ) ) ; scan all regions forrs ( rs2 ) ; assert equals ( block cache2 . get block count ( ) - initial block count2 @$ htu . get numh files forrs ( rs2 @$ table_name @$ family ) ) ; cache eviction stats stats = admin . clear block cache ( table_name ) . get ( ) ; assert equals ( stats . get evicted blocks ( ) @$ htu . get numh files forrs ( rs1 @$ table_name @$ family ) + htu . get numh files forrs (,scan cause,success,pre
expression has n't <PLACE_HOLDER> field,if ( tail == null ) { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( __str__ @$ offset @$ result ) ; } else { result . add ( new flat field descriptor ( offset @$ field type ) ) ; } } else { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( tail @$ offset @$ result ) ; } else { throw new invalid field reference exception ( __str__ + tail + __str__ + field type + __str__ ) ; } },expression set,fail,pre
make sure both packages are <PLACE_HOLDER> the same package .,if ( ! item to move . get target package name ( ) . equals ( m items . get ( parent idx ) . get target package name ( ) ) ) { return false ; } m items . remove ( move idx ) ; final int new parent idx = select ( new parent package name @$ user id ) + __num__ ; m items . add ( new parent idx @$ item to move ) ; return move idx != new parent idx ;,packages requesting,fail,pre
only the stream which was just added will <PLACE_HOLDER> parents . so we only need an array of size 1 .,list < parent changed event > events = new array list < parent changed event > ( __num__ ) ; connection state . take child ( state @$ false @$ events ) ; notify parent changed ( events ) ; state only removal queue . remove typed ( state ) ; state . stream = stream ;,stream notify,fail,pre
even the looser regime <PLACE_HOLDER> the toplevel constructor to be list or dict .,if ( x instanceof starlark list || x instanceof dict ) { throw new eval exception ( null @$ string . format ( __str__ @$ eval utils . get data type name ( x ) ) ) ; },regime requires,fail,pre
if non <PLACE_HOLDER> basic block,if ( end >= start ) { for ( int i = start ; i < end ; ++ i ) { code . data [ i ] = opcodes . nop ; } code . data [ end ] = ( byte ) opcodes . athrow ; start frame ( start @$ __num__ @$ __num__ ) ; frame [ frame index ++ ] = frame . object | cw . add type ( __str__ ) ; end frame ( ) ; },non empty,success,pre
the encryption key factory only <PLACE_HOLDER> a key if encryption is enabled .,data encryption key encryption key = ! trusted channel resolver . is trusted ( ) ? encryption key factory . new data encryption key ( ) : null ; io stream pair ios = send ( socket . get inet address ( ) @$ underlying out @$ underlying in @$ encryption key @$ access token @$ datanode id @$ null ) ; return ios != null ? ios : new io stream pair ( underlying in @$ underlying out ) ;,factory returns,success,pre
this would mean we 're explicitly queued or we have no running nodes but do have a cause of blockage @$ which <PLACE_HOLDER> out the same..,if ( ! is running && ( is queued || cause of blockage != null ) ) { return blue run state . queued ; },which stays,fail,pre
this operation does not <PLACE_HOLDER> the current offset of the file,if ( n > __num__ ) { decrypt ( position @$ buffer @$ offset @$ n ) ; },operation change,success,pre
if any event in the set <PLACE_HOLDER> a thread associated with it @$ they all will @$ so just grab the first one .,if ( set . size ( ) > __num__ ) { event event = set . iterator ( ) . next ( ) ; thread = event thread ( event ) ; } else { thread = null ; },event has,success,pre
search in descending order @$ so that the first <PLACE_HOLDER> value is the result .,for ( ninja scope included scope : sub map . descending map ( ) . values ( ) ) { t included value = included scope . find by name and offset recursively ( integer . max_value @$ name @$ map supplier ) ; if ( included value != null ) { return included value ; } } if ( current scope value != null ) { return current scope value . get second ( ) ; } if ( parent scope != null ) { preconditions . check not null ( include point ) ; return parent scope . find by name and offset recursively ( include point - __num__ @$ name @$ map supplier ) ; },first included,fail,pre
load <PLACE_HOLDER> the encoding factory from the output stream,return block encoding . read block ( this @$ input ) ;,load picks,fail,pre
test 1 : does not <PLACE_HOLDER> padding,text view = m grid view . find view holder for adapter position ( __num__ ) . item view . find view by id ( r . id . t1 ) ; rect . set ( __num__ @$ __num__ @$ text view . get width ( ) @$ text view . get height ( ) ) ; m grid view . offset descendant rect to my coords ( text view @$ rect ) ; assert equals ( window align center @$ rect . top @$ delta ) ;,not include,success,pre
do n't import nested classes because some of them <PLACE_HOLDER> the same name .,annotation mirror am = get current annotation ( ) ; annotation value av = helpers . get annotation type mirror value ( am @$ __str__ ) ; annotation value cv = helpers . get annotation type mirror value ( am @$ __str__ ) ; annotation value min sdk val = helpers . get annotation type mirror value ( am @$ __str__ ) ; int min sdk = min sdk val == null ? - __num__ : helpers . get annotation int value ( min sdk val ) ; annotation value max sdk val = helpers . get annotation type mirror value ( am @$ __str__ ) ; int max sdk = max sdk val == null ? - __num__ : helpers . get annotation int value ( max,some have,success,pre
next @$ look for errors from the unsuccessfully <PLACE_HOLDER> transitive traversal skyfunctions .,iterable < sky key > unsuccessful keys = iterables . filter ( transitive traversal keys @$ predicates . not ( predicates . in ( successful keys ) ) ) ; set < map . entry < sky key @$ exception > > error entries = graph . get missing and exceptions ( unsuccessful keys ) . entry set ( ) ; set < sky key > missing keys = new hash set < > ( ) ; for ( map . entry < sky key @$ exception > entry : error entries ) { if ( entry . get value ( ) == null ) { missing keys . add ( ( sky key ) entry . get key ( ) . argument ( ) ) ; },look invalidated,fail,pre
get the next instruction . the loop will <PLACE_HOLDER> one extra iteration after it reaches the end of the instruction list @$ with current handle set to null .,do { current handle = instructions . has next ( ) ? ( instruction handle ) instructions . next ( ) : null ; instruction inst = ( current handle != null ) ? current handle . get instruction ( ) : null ; if ( first instruction ) { open chunk at curr level = true ; curr level chunks . add ( current handle ) ; first instruction = false ; } if ( inst instanceof outlineable chunk start ) { if ( open chunk at curr level ) { sub chunk stack . push ( curr level chunks ) ; curr level chunks = new array list ( ) ; } open chunk at curr level = true ; curr level chunks . add (,loop do,fail,pre
note : lobounds should <PLACE_HOLDER> at least one element,type owntype = lobounds . tail . tail == null ? lobounds . head : infer . types . lub ( lobounds ) ; if ( owntype . is primitive ( ) || owntype . has tag ( error ) ) { throw infer . inference exception . set message ( __str__ @$ uv . qtype @$ lobounds ) ; } else { return owntype ; },lobounds have,success,pre
check that the api <PLACE_HOLDER> the array,final string multipart array api = files . get ( __str__ ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ;,api handles,success,pre
testing if an extremely large pattern will <PLACE_HOLDER> the initialization,pattern = __str__ ; for ( int count = __num__ ; count < __num__ ; count ++ ) { pattern += temp ; } try { result = new string search ( pattern @$ new string character iterator ( text ) @$ m_en_us_ @$ null ) ; logln ( __str__ + result . get pattern ( ) ) ; } catch ( exception e ) { errln ( __str__ ) ; return ; },pattern fail,success,pre
this method does not <PLACE_HOLDER> the results,return new hash set < > ( get matching beans ( new resolvable ( required type @$ qualifiers ) ) ) ;,method cache,success,pre
driver <PLACE_HOLDER> clause auxiliary <PLACE_HOLDER> clause,double inner join row count = left_rows_count * right_rows_count / left_join_column_ndv * left_join_column_non_nulls * right_join_column_non_nulls * unknown_filter_coefficient ;,auxiliary join,success,pre
the very first notification is likely to come in slower than all the others . because that test is n't <PLACE_HOLDER> the speed notifications are delivered with @$ we prefer to secure it .,if ( i == __num__ && notif == null ) { system . out . println ( __str__ + time for notification in seconds + __str__ + __str__ ) ; notif = notif list . poll ( time for notification in seconds @$ time unit . seconds ) ; } if ( notif == null ) { error count ++ ; system . out . println ( __str__ + __str__ + time for notification in seconds + __str__ ) ; } else { error count += check notification ( notif @$ ( string ) send notif param [ __num__ ] @$ basic . notification_message @$ obj name ) ; },test specified,fail,pre
if a target <PLACE_HOLDER> more than one source path @$ the latter one will take effect .,bind mounts . put ( mount target @$ mount source ) ;,target has,success,pre
this probably wo n't actually be uploaded @$ as android will probably <PLACE_HOLDER> all processes & data before it gets sent to the network .,if ( m is native initialized ) { record histogram . record enumerated histogram ( __str__ @$ option_clear_app_data @$ option_max ) ; },android clear,fail,pre
nobody <PLACE_HOLDER> entry,assert equals ( __num__ @$ calling . size ( ) ) ;,nobody occurs,fail,pre
..and most applications are not currently <PLACE_HOLDER> an sql ... it needs to activate some monitoring flags that are usually off..,return null ;,applications using,fail,pre
if we do n't have an mx record @$ <PLACE_HOLDER> the machine itself,if ( ( attr == null ) || ( attr . size ( ) == __num__ ) ) { attrs = ictx . get attributes ( host name @$ new string [ ] { __str__ } ) ; attr = attrs . get ( __str__ ) ; if ( attr == null ) { throw new naming exception ( base messages . get string ( pkg @$ __str__ @$ host name ) ) ; } },itself assign,fail,pre
toggling check box <PLACE_HOLDER> layout,assert counts ( __num__ @$ __num__ ) ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { e . print stack trace ( ) ; },box implies,fail,pre
negative test that assert glob does <PLACE_HOLDER> an error when asserting against the wrong values .,illegal argument exception e = assert throws ( illegal argument exception . class @$ ( ) -> assert glob matches ( immutable list . of ( __str__ @$ __str__ ) @$ immutable list . of ( __str__ @$ __str__ ) @$ immutable list . < string > of ( ) @$ true ) ) ;,test throw,success,pre
fetch the viterbi <PLACE_HOLDER> sequence,int [ ] computed = hmm algorithms . viterbi algorithm ( get model ( ) @$ get sequence ( ) @$ false ) ;,viterbi generated,success,pre
create two files to ensure each storage <PLACE_HOLDER> a block,dfs test util . create file ( cluster . get file system ( ) @$ new path ( __str__ ) @$ __num__ @$ __num__ @$ __num__ @$ ( short ) __num__ @$ __num__ ) ; dfs test util . create file ( cluster . get file system ( ) @$ new path ( __str__ ) @$ __num__ @$ __num__ @$ __num__ @$ ( short ) __num__ @$ __num__ ) ;,storage has,success,pre
nosonar <PLACE_HOLDER> internal representation,return decorations ;,nosonar stores,fail,pre
lookup for a non java jndi resource @$ inject factory which <PLACE_HOLDER> a true jndi lookup,managed reference factory = new managed reference factory ( ) { @ override public managed reference get reference ( ) { try { return new immediate managed reference ( new initial context ( ) . lookup ( lookup name ) ) ; } catch ( naming exception e ) { ee logger . root_logger . tracef ( e @$ __str__ @$ lookup name ) ; return null ; } } } ;,which performs,fail,pre
configuration 's equality does n't consider seq so if only seq number <PLACE_HOLDER>s in resolved override configuration . therefore configuration container does n't <PLACE_HOLDER> merged override configuration @$ but it 's used to push configuration <PLACE_HOLDER>s so explicitly update that .,if ( get merged override configuration ( ) . seq != get resolved override configuration ( ) . seq ) { on merged override configuration changed ( ) ; },container range,fail,pre
now asynchronously <PLACE_HOLDER> a checkpoint that initial fails .,when ( work executor . request checkpoint ( ) ) . then return ( null ) ; progress updater . request checkpoint ( ) ;,now request,success,pre
if added only one computed flow reference @$ then <PLACE_HOLDER> a good thunk .,return found count . get ( ) == __num__ ;,then assume,fail,pre
if the parent <PLACE_HOLDER> a default acl @$ copy that default acl an ded with the umask as the new file 's access acl . if it is a metadata load operation @$ do not consider the umask .,default access control list d acl = current inode directory . get defaultacl ( ) ; short mode = context . is metadata load ( ) ? mode . create full access ( ) . to short ( ) : new file . get mode ( ) ; if ( ! d acl . is empty ( ) ) { access control list acl = d acl . generate child fileacl ( mode ) ; new file . set internal acl ( acl ) ; } if ( file context . is cacheable ( ) ) { new file . set cacheable ( true ) ; } if ( file context . get write type ( ) == write type . async_through ) { new file . set,parent has,success,pre
zip entry does n't <PLACE_HOLDER> equals,assert equals ( entry . get name ( ) @$ entries . next element ( ) . get name ( ) ) ;,entry support,fail,pre
dispatch message <PLACE_HOLDER> event,if ( context . get process engine configuration ( ) . get event dispatcher ( ) . is enabled ( ) ) { delegate execution execution = process instance . get executions ( ) . get ( __num__ ) ; activiti event dispatcher event dispatcher = context . get process engine configuration ( ) . get event dispatcher ( ) ; event dispatcher . dispatch event ( activiti event builder . create message received event ( execution @$ message name @$ null @$ variables ) ) ; },message received,success,pre
conditions to <PLACE_HOLDER> the loop count,if ( break loop || end of arguments ( ) || loop count >= get end index ( ) ) { reset break loop ( ) ; reset loop count ( ) ; return null ; },conditions reset,success,pre
do not reset the partial flag for local tracker @$ as here the local tracker only <PLACE_HOLDER> the procedures which are modified in this file .,procedure map . merge ( local procedure map ) ;,tracker tracks,fail,pre
node <PLACE_HOLDER> both left and right children @$ find the minimum of the two .,t left child value = _values . get ( left child index ) ; t right child value = _values . get ( right child index ) ; if ( compare ( left child value @$ right child value ) <= __num__ ) { min index = left child index ; } else { min index = right child index ; },node has,success,pre
temp array <PLACE_HOLDER> those elements we know we want to keep,if ( len != __num__ ) { int newlen = __num__ ; object [ ] temp = new object [ len ] ; for ( int i = __num__ ; i < len ; ++ i ) { object element = elements [ i ] ; if ( c . contains ( element ) ) temp [ newlen ++ ] = element ; } if ( newlen != len ) { set array ( arrays . copy of ( temp @$ newlen ) ) ; return true ; } },array holds,success,pre
the leader <PLACE_HOLDER> the following block @$ which essentially shuts down the peer if it is not the last round .,if ( v . get id ( ) == i ) { log . info ( __str__ @$ i ) ; if ( lc < this . total rounds ) { log . info ( __str__ @$ i ) ; fast leader election election = ( fast leader election ) peer . get election alg ( ) ; election . shutdown ( ) ; assert equals ( - __num__ @$ election . get vote ( ) . get id ( ) ) ; log . info ( __str__ @$ i ) ; break ; } },leader executes,success,pre
the second update should n't <PLACE_HOLDER> a item change,model = new model with click listener_ ( ) ; model . click listener ( model click listener ) ; controller . set model ( model ) ; lifecycle helper . build models and bind ( controller ) ; model = new model with click listener_ ( ) ; model . click listener ( view click listener ) ; controller . set model ( model ) ; lifecycle helper . build models and bind ( controller ) ; verify ( observer mock @$ times ( __num__ ) ) . on item range changed ( eq ( __num__ ) @$ eq ( __num__ ) @$ any ( ) ) ; verify no more interactions ( observer mock ) ;,update trigger,fail,pre
assert that the table created still <PLACE_HOLDER> no hcat instrumentation,table table2 = client . get table ( __str__ @$ __str__ ) ; assert . assert true ( table2 . get sd ( ) . get input format ( ) . equals ( h cat constants . hive_rcfile_if_class ) ) ; driver . run ( __str__ ) ;,table has,success,pre
by jmf . if the directory does not exist or it does not contain a jmf.properties file . or if the jmf.properties file <PLACE_HOLDER> 0 length then this is the first time we 're running and should continue to with jmf init,string home dir = system . get property ( __str__ ) ; file jmf dir = new file ( home dir @$ __str__ ) ; string classpath = system . get property ( __str__ ) ; classpath += system . get property ( __str__ ) + jmf dir . get absolute path ( ) ; system . set property ( __str__ @$ classpath ) ; if ( ! jmf dir . exists ( ) ) jmf dir . mkdir ( ) ; file jmf properties = new file ( jmf dir @$ __str__ ) ; if ( ! jmf properties . exists ( ) ) { try { jmf properties . create new file ( ) ; } catch ( io exception ex ) { system . out,file has,success,pre
as part of the process of launching @$ activity thread also <PLACE_HOLDER> a resume .,if ( and resume && ready to resume ( ) ) { stack . minimal resume activity locked ( r ) ; } else { if ( debug_states ) slog . v ( tag_states @$ __str__ + r + __str__ ) ; r . set state ( paused @$ __str__ ) ; },thread performs,success,pre
else : the command itself @$ such as a shutdown task @$ might have <PLACE_HOLDER> all the scheduled tasks already .,future = fake clock . get scheduled executor service ( ) . schedule ( wrap @$ delay @$ time unit ) ;,itself processed,fail,pre
if the child <PLACE_HOLDER> an id and a behavior @$ let it save some state ...,if ( child id != no_id && b != null ) { parcelable state = b . on save instance state ( this @$ child ) ; if ( state != null ) { behavior states . append ( child id @$ state ) ; } },child has,success,pre
if another thread has <PLACE_HOLDER> the version @$ no need to recompute again .,synchronized ( this ) { if ( m should update . get ( ) ) { m version = compute ( ) ; m should update . set ( false ) ; } },thread updated,fail,pre
insert into its base table get the generated id <PLACE_HOLDER> indexes,try { return run task ( new query task < long > ( ) { @ override public long handle ( connection connection ) throws exception { pojo info pojo info = pojo info map . get ( pojo . get class ( ) ) ; abstract json entity generic json entity = new generic json entity ( ) ; generic json entity . set create time ( new timestamp ( system . current time millis ( ) ) ) ; generic json entity . set update time ( new timestamp ( system . current time millis ( ) ) ) ; generic json entity . set version ( __num__ ) ; generic json entity . set bean class ( pojo . get class ( ) . get,id create,fail,pre
frame can <PLACE_HOLDER> the entire display . use exact values .,if ( frame width >= display width && frame height >= display height ) { return new recording info ( display width @$ display height @$ camera frame rate @$ display density ) ; },frame fill,fail,pre
note : the legacy jni code used to do a query right after a load success to synchronize the service cache . instead <PLACE_HOLDER> the binary that was requested to load to update the cache later without doing a query .,if ( result == context hub transaction . result_success ) { m nano app state manager . add nano app instance ( context hub id @$ nano app binary . get nano app id ( ) @$ nano app binary . get nano app version ( ) ) ; },note use,fail,pre
because there is no need to expand the visual range @$ no row or spacer contents <PLACE_HOLDER> updated . all rows @$ spacers @$ and scroll position simply need to be shifted down accordingly and the spacer indexes need updating .,spacer container . update spacer indexes for row and after ( index @$ old top row logical index + visual row order . size ( ) @$ number of rows ) ;,contents get,success,pre
the first and second streams in a reprocessable combination <PLACE_HOLDER> the same size and format . the first is the input and the second is the output used for generating the subsequent input buffers .,if ( is reprocessable ) { array list < size > input size = new array list < size > ( ) ; int format ; if ( comb template . m reprocess type == reprocess type . private ) { input size . add ( max private input size ) ; format = image format . private ; } else { input size . add ( maxyuv input size ) ; format = image format . yuv_420_888 ; } streams info . add ( new mandatory stream information ( input size @$ format @$ true ) ) ; streams info . add ( new mandatory stream information ( input size @$ format ) ) ; },streams have,success,pre
make sure null set of anchors <PLACE_HOLDER> null pointer exception,try { pkix builder parameters p = new pkix builder parameters ( ( set ) null @$ null ) ; throw new exception ( __str__ ) ; } catch ( null pointer exception npe ) { },set throws,success,pre
load default <PLACE_HOLDER> size .,default font size = config service . get string ( __str__ ) ;,default font,success,pre
prime the connection to <PLACE_HOLDER> client and server prefaces to be exchanged .,content response response = client . new request ( __str__ @$ connector . get local port ( ) ) . path ( __str__ ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( http status . ok_200 @$ response . get status ( ) ) ; org . eclipse . jetty . client . api . request request = client . new request ( __str__ @$ connector . get local port ( ) ) . method ( http method . head ) . path ( __str__ ) ; request . send ( result -> { if ( result . is failed ( ) ) latch . count down ( ) ; } ) ; assert true ( stream latch,connection allow,success,pre
if all of the options come from the same package @$ show the application 's label and icon instead of the generic resolver 's . some calls like intent.resolve activity info query the resolve info from here and then throw away the resolve info itself @$ meaning that the caller loses the resolve package name . therefore the activity info.label res above <PLACE_HOLDER> a,final string intent package = intent . get package ( ) ; if ( ! text utils . is empty ( intent package ) && all have package ( query @$ intent package ) ) { final application info appi = query . get ( __num__ ) . activity info . application info ; ri . resolve package name = intent package ; if ( user needs badging ( user id ) ) { ri . no resource id = true ; } else { ri . icon = appi . icon ; } ri . icon resource id = appi . icon ; ri . label res = appi . label res ; } ri . activity info . application info = new application info ( ri,res adds,fail,pre
now <PLACE_HOLDER> a test that ensures stuff works when we go over block boundary @$ especially that we return good length on file .,final byte [ ] value = new byte [ __num__ * __num__ ] ;,now make,fail,pre
this color state list does not <PLACE_HOLDER> an activated state so this should not yield a change,boolean changed = result . on state changed ( new int [ ] { android . r . attr . state_activated } ) ; assert false ( changed ) ; assert equals ( result . get color ( ) @$ csl . get default color ( ) ) ;,list contain,fail,pre
if the method <PLACE_HOLDER> parameters @$ skip it,try { if ( method . is empty ( ) || method . get parameter types ( ) . length != __num__ ) { continue ; } } catch ( not found exception e ) { continue ; },method has,success,pre
uri parsing <PLACE_HOLDER> limitation .,try { uri oracle rac = uri . create ( __str__ + __str__ + __str__ + __str__ ) ; logger . debug ( oracle rac . to string ( ) ) ; logger . debug ( oracle rac . get scheme ( ) ) ; assert . fail ( ) ; } catch ( exception e ) { },parsing has,success,pre
new collection <PLACE_HOLDER> one less element,int new row location = get celly ( __num__ ) ; assert close to ( __str__ @$ new row location @$ row location ) ;,collection has,success,pre
start a container that listens on a poke port @$ and once poked <PLACE_HOLDER> a web server,final job job = job . new builder ( ) . set name ( test job name ) . set version ( test job version ) . set image ( nginx ) . set command ( as list ( __str__ @$ __str__ @$ __str__ ) ) . add port ( __str__ @$ port mapping . of ( __num__ ) ) . add port ( __str__ @$ port mapping . of ( __num__ ) ) . add registration ( service endpoint . of ( __str__ @$ __str__ ) @$ service ports . of ( __str__ ) ) . set health check ( health check ) . build ( ) ; assert container registers after poke ( client @$ job ) ;,poked add,fail,pre
could n't <PLACE_HOLDER> a named 'event bus ' event bus object . try to <PLACE_HOLDER> the first typed value we can :,for ( object v : objects . values ( ) ) { if ( v instanceof event bus ) { return ( event bus ) v ; } } return null ;,bus get,fail,pre
parameters for cert store is collection type @$ using collection with crl <PLACE_HOLDER> parameters,collection cert store parameters params = new collection cert store parameters ( crl collection ) ;,collection create,success,pre
nn should have <PLACE_HOLDER> a new image,long nn image after = nn . getfs image ( ) . get storage ( ) . get most recent checkpoint tx id ( ) ; assert true ( __str__ + nn image before + __str__ + nn image after @$ nn image after > nn image before ) ;,nn produced,fail,pre
if we could n't find the region because the cache is <PLACE_HOLDER> @$ throw a cache <PLACE_HOLDER> exception,if ( rgn == null ) { if ( cache . is closed ( ) ) { throw new cache closed exception ( ) ; } throw new region not found exception ( string . format ( __str__ @$ this . region path ) ) ; },cache closed,success,pre
one could <PLACE_HOLDER> an empty one otherwise,if ( length < __num__ ) { throw new illegal argument exception ( ) ; },one return,fail,pre
root states should only <PLACE_HOLDER> 1,for ( string root : start symbols ) { if ( new state split counts . get count ( root ) > __num__ ) { new state split counts . set count ( root @$ __num__ ) ; } } if ( new state split counts . get count ( lexicon . boundary_tag ) > __num__ ) { new state split counts . set count ( lexicon . boundary_tag @$ __num__ ) ; } state split counts = new state split counts ;,states have,success,pre
if reference <PLACE_HOLDER> a factory @$ use exclusively,if ( f != null ) { factory = get object factory from reference ( ref @$ f ) ; if ( factory != null ) { return factory . get object instance ( ref @$ name @$ name ctx @$ environment ) ; } return ref info ; } else { answer = processurl addrs ( ref @$ name @$ name ctx @$ environment ) ; if ( answer != null ) { return answer ; } },reference identifies,success,pre
ttl tags <PLACE_HOLDER> ts in milliseconds,region . put ( new put ( row ) . add ( new key value ( row @$ fam1 @$ q3 @$ now + __num__ - __num__ @$ h constants . empty_byte_array @$ new array backed tag [ ] { new array backed tag ( tag type . ttl_tag_type @$ bytes . to bytes ( __num__ ) ) } ) ) ) ;,tags specify,success,pre
cut the connection @$ so the server will <PLACE_HOLDER> close session as part of expiring the session .,client . dont reconnect ( ) ; client . disconnect ( ) ; watcher . reset ( ) ;,server send,fail,pre
recurring alarms may have <PLACE_HOLDER> several alarm intervals while the alarm was kept pending . send the appropriate trigger count .,if ( alarm . repeat interval > __num__ ) { alarm . count += ( nowelapsed - alarm . expected when elapsed ) / alarm . repeat interval ; final long delta = alarm . count * alarm . repeat interval ; final long next elapsed = alarm . expected when elapsed + delta ; set impl locked ( alarm . type @$ alarm . when + delta @$ next elapsed @$ alarm . window length @$ max trigger time ( nowelapsed @$ next elapsed @$ alarm . repeat interval ) @$ alarm . repeat interval @$ alarm . operation @$ null @$ null @$ alarm . flags @$ true @$ alarm . work source @$ alarm . alarm clock @$ alarm . uid @$ alarm . package,alarms passed,success,pre
count down every time a thread <PLACE_HOLDER> work @$ so that we can wait for work to complete,final count down latch work finished latch = new count down latch ( num samples ) ;,thread does,fail,pre
assertion that accreditations collection table got cleaned up if they did n't @$ the delete should have <PLACE_HOLDER> a constraint error @$ but just to be sure ...,session s = open session ( ) ; s . begin transaction ( ) ; s . do work ( new work ( ) { @ override public void execute ( connection connection ) throws sql exception { final statement statement = connection . create statement ( ) ; final result set result set = statement . execute query ( __str__ ) ; assert true ( result set . next ( ) ) ; final int count = result set . get int ( __num__ ) ; assert equals ( __num__ @$ count ) ; } } ) ; s . get transaction ( ) . commit ( ) ; s . close ( ) ;,delete thrown,fail,pre
the above transaction now contain the changes we want to expose to the index updater as updates . this will happen when we commit the transaction . the transaction now also holds the schema read lock @$ so we ca n't begin creating our constraint just yet . we first have to unlock the schema @$ and then block just before we send off,lock lock blocking data change transaction = get lock service ( ) . acquire node lock ( block data change transaction on lock on id @$ lock type . write_lock ) ;,thread contains,fail,pre
some ipp printers like lexc 710 do not <PLACE_HOLDER> list of supported media but cups can get the media from ppd @$ so we still report as supported category .,if ( is cups printer ) { if ( ! cat list . contains ( media . class ) ) { cat list . add ( media . class ) ; } cat list . add ( media printable area . class ) ; cat list . add ( destination . class ) ; if ( ! print service lookup provider . is linux ( ) ) { cat list . add ( sheet collate . class ) ; } },printers have,success,pre
render the world <PLACE_HOLDER> the debug renderer,renderer . render ( world @$ camera . combined ) ; float render time = ( time utils . nano time ( ) - start time ) / __num__ ; batch . begin ( ) ; font . draw ( batch @$ __str__ + gdx . graphics . get frames per second ( ) + __str__ + update time + __str__ + render time @$ __num__ @$ __num__ ) ; batch . end ( ) ;,world using,success,pre
many tools @$ just <PLACE_HOLDER> the largest value,analysis shared thread pool size = math . max ( analysis shared thread pool size @$ current tool size ) ;,tools take,fail,pre
get the currency pattern for this locale . we have to fish it out of the resource bundle directly @$ since decimal format.to pattern will <PLACE_HOLDER> the localized symbol @$ not \00 a 4,string [ ] num patterns = ( string [ ] ) rb . get object ( __str__ ) ; string pattern = num patterns [ __num__ ] ; if ( pattern . index of ( __str__ ) == - __num__ ) { errln ( __str__ + locales [ i ] + __str__ + pattern ) ; },pattern return,success,pre
one shortcut when contracting <PLACE_HOLDER> 3,add shortcut ( nodea @$ nodeb @$ e3toa . get edge ( ) @$ e3tob . get edge ( ) @$ e3toa . get edge ( ) @$ e3tob . get edge ( ) @$ __num__ ) ; set level equal to node id for all nodes ( ) ;,contracting e,fail,pre
first comment line consists of exactly one character . so only line break or line feed can be left . <PLACE_HOLDER> line .,if ( m_editable object . get length ( ) == __num__ ) { if ( y > __num__ ) { m_carety = y - __num__ ; final zy line content prev line content = get line content ( m_carety ) ; m_caretx = prev line content . get text ( ) . length ( ) ; } changed text = get multiline comment ( y @$ changed text ) ; } else { m_caretx = get caret end posx ( ) ; m_carety = get caret mouse releasedy ( ) ; return ; },line trim,fail,pre
if both threads finished already @$ assert that both threads <PLACE_HOLDER> exception,assert that ( ai1 . get exception ( ) instanceof unsupported operation exception ) . is true ( ) ; assert that ( ai2 . get exception ( ) instanceof unsupported operation exception ) . is true ( ) ;,threads throw,success,pre
node might 've not have <PLACE_HOLDER> the license cluster state,if ( current license == null ) { return true ; },might changed,fail,pre
arbitrary size <PLACE_HOLDER> bitmap factory .,byte [ ] storage = new byte [ __num__ * __num__ ] ; image decoder decoder = null ; try { decoder = n create ( is @$ storage @$ source ) ; } finally { if ( decoder == null ) { if ( close input stream ) { io utils . close quietly ( is ) ; } } else { decoder . m input stream = is ; decoder . m owns input stream = close input stream ; decoder . m temp storage = storage ; } } return decoder ;,size based,fail,pre
fetcher has <PLACE_HOLDER> the input field when object reuse is enabled,fetcher . flat map ( in @$ get fetcher collector ( ) ) ; if ( is left outer join && ! collector . is collected ( ) ) { out row . replace ( in @$ null row ) ; out row . set header ( in . get header ( ) ) ; out . collect ( out row ) ; },fetcher copied,success,pre
how may patterns <PLACE_HOLDER> this bit,numfixed = get num fixed ( sbit @$ __num__ @$ context ) ;,patterns handle,fail,pre
verify false constant <PLACE_HOLDER> the conditional with the else expression .,assert translation ( translation @$ __str__ ) ;,verify replaces,success,pre
count <PLACE_HOLDER> keepalives for the same uid across networks .,int unprivileged count same uid = __num__ ; for ( final hash map < integer @$ keepalive info > ka for network : m keepalives . values ( ) ) { for ( final keepalive info ki : ka for network . values ( ) ) { if ( ki . m uid == m uid ) { unprivileged count same uid ++ ; } } } if ( unprivileged count same uid > m allowed unprivileged slots for uid ) { return error_insufficient_resources ; } return success ;,count unlocked,fail,pre
without a copy @$ but <PLACE_HOLDER> this instance its own position and limit .,this . content = content . as read only buffer ( ) ;,but hold,fail,pre
the client has departed . <PLACE_HOLDER> this last connection and unregister it .,boolean needs unregister = false ; synchronized ( chm lock ) { if ( chm registered ) { needs unregister = true ; chm registered = false ; } } if ( unregister client ) { clean client auths ( ) ; },departed remove,success,pre
we know root <PLACE_HOLDER> 2 fragments because we create 2 memory blocks in the set up .,f1 = ( program fragment ) groups [ __num__ ] ; f2 = ( program fragment ) groups [ __num__ ] ;,root has,success,pre
this thread has <PLACE_HOLDER> the blocked state release the lock and a short wait before continue,try { lock . wait ( __num__ ) ; } catch ( interrupted exception e ) { interrupted . increment and get ( ) ; },thread changed,fail,pre
port check should <PLACE_HOLDER> default port of that url,test implies ( thisurl @$ thaturl @$ true ) ; system . out . println ( __str__ ) ;,check consider,fail,pre
yolo <PLACE_HOLDER> the first parent !,return this ;,yolo has,fail,pre
if the genrule rewrites the source map @$ we have to create the parent dir @$ and record the <PLACE_HOLDER> artifact,if ( rewrite sourcemap ) { source path source path to source map = get source path to source map ( ) ; buildable context . record artifact ( source path resolver adapter . get relative path ( source path to source map ) ) ; builder . add ( mkdir step . of ( build cell relative path . from cell relative path ( context . get build cell root path ( ) @$ get project filesystem ( ) @$ source path resolver adapter . get relative path ( source path to source map ) . get parent ( ) ) ) ) ; },dir build,success,pre
arizona does not <PLACE_HOLDER> dst @$ so even phoenix and denver have the same raw offest @$ they have different rules .,a = time zone . get time zone ( __str__ ) ; b = time zone . get time zone ( __str__ ) ; assert equals ( a . get raw offset ( ) @$ b . get raw offset ( ) ) ; assert false ( a . has same rules ( b ) ) ;,arizona create,fail,pre
only process if curi <PLACE_HOLDER> evidence of fetch attempt,return uri . contains data key ( a_fetch_began_time ) && uri . get recorder ( ) != null && uri . get recorder ( ) . get response content length ( ) >= get lower bound ( ) && uri . get recorder ( ) . get response content length ( ) <= get upper bound ( ) ;,curi contains,success,pre
for read lock @$ if the caller has <PLACE_HOLDER> the same row previously @$ it will not try to acquire the same read lock . it simply returns the previous row lock .,if ( read lock ) { row lock impl prev row lock impl = ( row lock impl ) prev row lock ; if ( ( prev row lock impl != null ) && ( prev row lock impl . get lock ( ) == row lock context . read write lock . read lock ( ) ) ) { success = true ; return prev row lock ; } result = row lock context . new read lock ( ) ; } else { result = row lock context . new write lock ( ) ; },caller locked,success,pre
consume native <PLACE_HOLDER> event if we generate one,if ( id == mouse event . mouse_entered ) { e . consume ( ) ; },native enter,success,pre
add in reverse order so we can tell that sorting actually <PLACE_HOLDER> something,points . add ( second ) ; points . add ( first ) ; collections . sort ( points ) ; assert with message ( string . format ( __str__ + __str__ @$ points . index of ( second ) @$ points . index of ( first ) ) ) . that ( points . index of ( second ) ) . is greater than ( points . index of ( first ) ) ;,sorting hides,fail,pre
add snowplow emitter to context @$ contributors can <PLACE_HOLDER> this emitter to emit other event,emitter = telemetry emitter . builder ( ) . build ( ) ; root context . put ( emitter . class @$ emitter ) ;,contributors use,success,pre
check that the api <PLACE_HOLDER> the single file,final string multipart single api = files . get ( __str__ ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ;,api handles,success,pre
if so use sentences to <PLACE_HOLDER> so context from sentences,if ( sentences != null && options . coreference context size > __num__ ) { list < core label > tokens = sentences . get ( mention . sent num - __num__ ) . get ( core annotations . tokens annotation . class ) ; int context start = math . max ( mention . start index - __num__ - __num__ @$ __num__ ) ; int context end = math . min ( mention . end index - __num__ + __num__ @$ tokens . size ( ) ) ; string left context = string utils . join words ( tokens @$ __str__ @$ context start @$ mention . start index - __num__ ) ; string right context = string utils . join words ( tokens @$ __str__ @$,sentences admit,fail,pre
we only cache <PLACE_HOLDER> and head requests,if ( ! exchange . get request method ( ) . equals ( get ) && ! exchange . get request method ( ) . equals ( head ) ) { return false ; } if ( entry == null ) { this . response cachable = mark cacheable ; return false ; },cache get,success,pre
the file filter will tell us which <PLACE_HOLDER> match and which do n't .,try { final file filter filter = config . build file filter ( ) ; while ( ! list . is empty ( ) && state == running ) { final file current = list . poll ( ) ; task . scanned ++ ; if ( current . is file ( ) ) { task . matching ++ ; act on ( current ) ; } if ( current . is directory ( ) ) { final file [ ] content = current . list files ( filter ) ; if ( content == null ) continue ; list . add all ( __num__ @$ arrays . as list ( content ) ) ; } } if ( list . is empty ( ) ) { task,which does,fail,pre
if the option <PLACE_HOLDER> values and a non blank argname,if ( option . accept value ( ) && ( option . get arg name ( ) == null || option . get arg name ( ) . length ( ) != __num__ ) ) { buff . append ( is null or empty ( option . get short name ( ) ) ? get long option separator ( ) : __str__ ) ; buff . append ( __str__ ) . append ( option . get arg name ( ) != null ? option . get arg name ( ) : get arg name ( ) ) . append ( __str__ ) ; },option accepts,success,pre
intent filter verification is only for filters that specify a host . so do n't <PLACE_HOLDER> those that handle all web uris .,if ( ri target user . handle all web datauri ) { continue ; } string package name = ri target user . activity info . package name ; package setting ps = m settings . m packages . get ( package name ) ; if ( ps == null ) { continue ; } long verification state = get domain verification statusl pr ( ps @$ parent user id ) ; int status = ( int ) ( verification state > > __num__ ) ; if ( result == null ) { result = new cross profile domain info ( ) ; result . resolve info = create forwarding resolve info unchecked ( new intent filter ( ) @$ source user id @$ parent user id ),verification skip,fail,pre
wraps sticky or default . then has next and next just <PLACE_HOLDER> the closure .,if ( is addr disposition retry ) { return true ; } boolean result ; if ( primary to contact info != null ) { result = primary to contact info . has next ( primary contact info @$ previous contact info @$ list of contact infos ) ; } else { result = effective targetior iterator . has next ( ) ; } return result ;,sticky calls,fail,pre
we now should only have zero since the disk store <PLACE_HOLDER> no regions remaining in it .,file count = __num__ ; for ( file disk dir : disk dirs ) { file [ ] files = disk dir . list files ( ) ; file count += files . length ; } assert that ( file count ) . is equal to ( __num__ ) ;,store has,fail,pre
custom deserializer always <PLACE_HOLDER> instance like this :,assert equals ( __str__ @$ bean . a ) ; assert equals ( __str__ @$ bean . b ) ;,deserializer returns,fail,pre
since task resource has been closed @$ broker should <PLACE_HOLDER> a new instance of the object,test factory . shared resource task resource2 = task broker . get shared resource at scope ( new test factory < gobblin scope types > ( ) @$ new test resource key ( __str__ ) @$ gobblin scope types . task ) ; assert . assert not equals ( task resource @$ task resource2 ) ; top broker . close ( ) ; assert . assert true ( job resource . is closed ( ) ) ; assert . assert true ( task resource . is closed ( ) ) ;,broker create,fail,pre
create a walker which <PLACE_HOLDER> the tree in a dfs manner while maintaining the operator stack . the dispatcher generates the plan from the operator tree,set multimap < integer @$ node processor > ast node to processor = hash multimap . create ( ) ; ast node to processor . put ( hive parser . tok_null @$ tf . get null expr processor ( ) ) ; ast node to processor . put ( hive parser . number @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . integral literal @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . number literal @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . identifier @$ tf . get str expr processor,which walks,success,pre
class <PLACE_HOLDER> or implements clause,case class_extends : print ( __str__ ) ; print ( pos . type_index ) ; break ;,class extends,success,pre
use previously <PLACE_HOLDER> job entry info !,if ( je . get nr ( ) > __num__ ) { je . set entry ( prev . get entry ( ) ) ; prev = find job entry ( je . get name ( ) @$ je . get nr ( ) @$ true ) ; if ( prev != null ) { int idx = index of job entry ( prev ) ; remove job entry ( idx ) ; } },use cached,fail,pre
since behavior files might <PLACE_HOLDER> language features that are n't present in the class file @$ we might need to update the feature set .,if ( cls . features != null ) { node script node = node util . get enclosing script ( parent ) ; feature set old features = ( feature set ) script node . get prop ( node . feature_set ) ; feature set new features = old features . union ( cls . features ) ; if ( ! new features . equals ( old features ) ) { script node . put prop ( node . feature_set @$ new features ) ; compiler . report change to change scope ( script node ) ; } } if ( node util . is name declaration ( expr root ) ) { node assign expr = var to assign ( expr root ) ; parent . replace,files contain,success,pre
we do n't use do unchecked here as there is a chance the below method can <PLACE_HOLDER> user supplied code,if ( wild fly security manager . is checking ( ) ) { access controller . do privileged ( ( privileged action < object > ) ( ) -> { session destroyed impl ( se ) ; return null ; } ) ; } else { session destroyed impl ( se ) ; },method invoke,fail,pre
these tests do n't <PLACE_HOLDER> exchanges from local parallel,session . session builder session builder = test session builder ( ) . set catalog ( catalog_id ) . set schema ( __str__ ) . set system property ( __str__ @$ __str__ ) ;,tests handle,success,pre
noinspection <PLACE_HOLDER> allocation in loop,if ( insertion index >= key bucket . size ( ) ) { entry = key bucket . get entry ( insertion index - __num__ @$ key serializer @$ value serializer ) ; } else { entry = key bucket . get entry ( insertion index @$ key serializer @$ value serializer ) ; },noinspection object,success,pre
log factory can not be <PLACE_HOLDER> by the classloader which <PLACE_HOLDER> the custom factory implementation . the custom implementation is not viable until this is corrected . ensure that the jcl jar and the custom class are available from the same classloader . running with diagnostics on should give information about the classloaders used to load the custom factory .,log diagnostic ( __str__ + __str__ ) ;,which provides,fail,pre
the tree <PLACE_HOLDER> a jmx file,data flavor [ ] flavors = support . get data flavors ( ) ; for ( data flavor flavor : flavors ) { if ( flavor . is flavor java file list type ( ) ) { return true ; } },tree contains,fail,pre
to take appropriate action when the activity <PLACE_HOLDER> focus,super . on pause ( ) ; mgl surface view . on pause ( ) ;,activity looses,success,pre
one for event id <PLACE_HOLDER> message type,message . set message type ( message type . register_instantiators ) ; for ( int i = __num__ ; i < instantiators length ; i = i + __num__ ) { message . add bytes part ( this . serialized instantiators [ i ] ) ; message . add bytes part ( this . serialized instantiators [ i + __num__ ] ) ; message . add bytes part ( this . serialized instantiators [ i + __num__ ] ) ; } message . set transaction id ( __num__ ) ; message . add obj part ( this . get event id ( ) ) ; return message ;,one override,fail,pre
this time @$ we <PLACE_HOLDER> seven display names rad dude 's surfer gal 'replaces ' later 's surfer gal,map dids = get display names ( service ) ; iterator iter = dids . entry set ( ) . iterator ( ) ; int count = __num__ ; while ( iter . has next ( ) ) { ++ count ; entry e = ( entry ) iter . next ( ) ; logln ( __str__ + e . get key ( ) + __str__ + e . get value ( ) ) ; },time have,success,pre
store any <PLACE_HOLDER> images in drive in a new folder .,if ( as object . first image ( ) != null ) { drive drive interface = get or create drive service ( auth data ) ; string folder id = idempotent executor . execute or throw exception ( __str__ @$ __str__ @$ ( ) -> create album folder ( drive interface ) ) ; for ( link value image : as object . image ( ) ) { try { string new img src = idempotent executor . execute and swallowio exceptions ( image . to string ( ) @$ __str__ @$ ( ) -> upload image ( ( as object ) image @$ drive interface @$ folder id ) ) ; content += __str__ + new img src + __str__ ; } catch ( runtime exception,any storeed,fail,pre
do n't let the region <PLACE_HOLDER> creation escape to the user,attributes factory factory = new attributes factory ( creation ) ; region attributes attrs = factory . create region attributes ( ) ; cache . set region attributes ( id @$ attrs ) ;,region attributes,success,pre
websphere servlet which <PLACE_HOLDER> websocket endpoints @$ dynamically added,if ( servlet class name . equals ( __str__ ) ) { return false ; },which provides,fail,pre
cup <PLACE_HOLDER> position .,enter string ( __str__ + ( i + __num__ ) + __str__ + ( j + __num__ ) + __str__ ) ;,cup press,fail,pre
verify that the append <PLACE_HOLDER> backoff due to timeotu,assert . assert equals ( satus @$ status . backoff ) ;,append caused,fail,pre
less common case : there are no valid omnibox suggestions . this can happen if the user <PLACE_HOLDER> the url bar to dismiss the suggestions @$ then pressed enter . this can also happen if the user presses enter before any suggestions have been received from the autocomplete controller .,suggestion match = m autocomplete . classify ( url text ) ; suggestion match position = __num__ ;,user clicks,fail,pre
since null could be a valid key @$ explicitly check whether map <PLACE_HOLDER> the key,try { if ( ! serialized to original . contains key ( raw entry . get key ( ) ) ) { log . error ( __str__ + __str__ @$ raw entry . get key ( ) ) ; continue ; } map < string @$ t > orig key = serialized to original . get ( raw entry . get key ( ) ) ; schema and value deserialized schema and value = value converter . to connect data ( namespace @$ raw entry . get value ( ) != null ? raw entry . get value ( ) . array ( ) : null ) ; object deserialized value = deserialized schema and value . value ( ) ; offset utils . validate format ( deserialized,map contains,success,pre
ok @$ this action <PLACE_HOLDER> a reference to the file containing the definition . such actions do n't have to have component alias defined .,if ( action . get component ( ) == null && ( action . get reference ( ) != null || action . get reference content ( ) != null ) ) { continue ; },action has,fail,pre
extend is in left @$ top @$ right @$ bottom ; which is very uncommon @$ photon <PLACE_HOLDER> the same,if ( extent . length == __num__ ) { this . extent = new b box ( extent [ __num__ ] @$ extent [ __num__ ] @$ extent [ __num__ ] @$ extent [ __num__ ] ) ; } else { throw new runtime exception ( __str__ + extent . length ) ; },photon does,fail,pre
this is the second field on row 3 @$ drag the first field past the middle of this field which <PLACE_HOLDER> this field to move to the left edge,int startx = bf . get startx ( ) ; int width = bf . get width ( ) ; int middle first fieldx = startx / __num__ ; int past middle second fieldx = startx + width / __num__ + __num__ ; int row = __num__ ; drag field ( row @$ middle first fieldx @$ row @$ past middle second fieldx ) ; wait for swing ( ) ; bf = cb . get current field ( ) ; assert equals ( __num__ @$ bf . get startx ( ) ) ;,which causes,success,pre
meta key key listener does n't <PLACE_HOLDER> control key state . need to check the key event instead .,boolean is ctrl active = ( ( event . get meta state ( ) & key event . meta_ctrl_on ) != __num__ ) ; boolean is shift active = ( get meta state ( content @$ meta_shift_on @$ event ) == __num__ ) ; boolean is alt active = ( get meta state ( content @$ meta_alt_on @$ event ) == __num__ ) ; if ( is ctrl active ) { if ( is alt active || is shift active ) { return false ; } return delete until word boundary ( view @$ content @$ is forward delete ) ; },listener control,fail,pre
chinese date format <PLACE_HOLDER> pattern letter ' l ' for leap month marker in addition to regular date format,cal . clear ( ) ; cal . set ( __num__ @$ calendar . june @$ __num__ ) ;,format uses,fail,pre
if this server has not yet applied entries up to the client 's session id @$ forward the query to the leader . this ensures that a follower does not <PLACE_HOLDER> the client its session does n't exist if the follower has n't had a chance to see the session 's registration entry .,if ( raft . get last applied ( ) < request . session ( ) ) { return completable future . completed future ( log response ( query response . builder ( ) . with status ( raft response . status . error ) . with error ( raft error . type . unknown_session @$ __str__ ) . build ( ) ) ) ; },follower tell,success,pre
this method automatically <PLACE_HOLDER> the stream with the given registry .,@ suppress warnings ( __str__ ) closingfs data input stream pis = closingfs data input stream . wrap safe ( test stream @$ ( safety net closeable registry ) registry @$ debug ) ;,method closes,fail,pre
redis async commands <PLACE_HOLDER> redis cluster async commands,if ( redis cluster async commands . class . is assignable from ( clazz ) ) { key += __str__ ; } else if ( redis cluster reactive commands . class . is assignable from ( clazz ) ) { key += __str__ ; } else { throw new illegal argument exception ( clazz . get name ( ) ) ; },commands extends,success,pre
flush the output @$ this is necessary for f <PLACE_HOLDER> buffered output .,_printer . flush ( ) ;,f processing,fail,pre
check row <PLACE_HOLDER> comparator to check we are writing in order .,if ( ! check row ( cell ) ) { if ( start offset < __num__ ) { start offset = out . size ( ) ; } rows offsetbaos . write int ( out . size ( ) - start offset ) ; } last cell = cell ; return encoder . write ( cell ) ;,row uses,success,pre
nightly testing <PLACE_HOLDER> some intermittent failure . check here to get diagnostic information if some strange behavior occurs .,check thread count ( expected count @$ current @$ __num__ ) ;,testing contains,fail,pre
publish topic name must not <PLACE_HOLDER> any wildcard,for ( char c : topic_wildcards ) { if ( topic name . index of ( c ) >= __num__ ) { return false ; } } return true ;,name contain,success,pre
step light idle state locked does n't <PLACE_HOLDER> the active case @$ so the state should stay as active .,verify light state conditions ( light_state_active ) ;,state detect,fail,pre
we do n't need to add the source <PLACE_HOLDER> itself as it is a mandatory input .,if ( artifact != null ) { if ( ! artifact . equals ( source file ) ) { inputs . add ( artifact ) ; } continue ; },source file,success,pre
if global <PLACE_HOLDER>s excludes @$ individual modules can only <PLACE_HOLDER> additional excludes .,if ( ! root config . exclude . is empty ( ) && ! entry . get value ( ) . include . is empty ( ) ) { throw new illegal state exception ( string . format ( __str__ @$ entry . get key ( ) ) ) ; },modules have,fail,pre
find installed font name always <PLACE_HOLDER> an available font name,return font tag . find installed font name ( get font name ( ) ) ;,name returns,success,pre
test non <PLACE_HOLDER> iterator .,iterator < element > it = vector . non zeroes ( ) . iterator ( ) ; int i = __num__ ; while ( it . has next ( ) ) { it . next ( ) ; ++ i ; },non zero,success,pre
negative exit values are <PLACE_HOLDER> 256 :,for ( int exit : new int [ ] { - __num__ @$ - __num__ @$ - __num__ } ) { int expected = __num__ + exit ; string [ ] args = { __str__ @$ __str__ @$ __str__ + exit } ; bad exit status exception e = assert throws ( __str__ + expected @$ bad exit status exception . class @$ ( ) -> new command ( args ) . execute ( ) ) ; assert that ( e ) . has message that ( ) . is equal to ( __str__ + expected ) ; check command elements ( e @$ __str__ @$ __str__ @$ __str__ + exit ) ; termination status status = e . get result ( ) . get termination status (,values scans,fail,pre
1 st <PLACE_HOLDER> registration,if ( parent == null ) { long next = ( ( long ) phase << phase_shift ) | adj ; if ( state . compare and set ( this @$ s @$ next ) ) break ; } else { main lock . lock ( ) ; try { if ( state == s ) { parent . do register ( __num__ ) ; do { phase = ( int ) ( root . state > > > phase_shift ) ; } while ( ! state . compare and set ( this @$ state @$ ( ( long ) phase << phase_shift ) | adj ) ) ; break ; } } finally { main lock . unlock ( ) ; } },st root,success,pre
next try the current class loader which <PLACE_HOLDER> p 6 spy,if ( result == null ) { result = p6 util . class . get class loader ( ) . get resource ( filename ) ; },which supports,fail,pre
add a new routes that will handle endpoints form wire <PLACE_HOLDER> route class .,camel context . add routes ( new route builder ( ) { @ override public void configure ( ) throws exception { from ( __str__ ) . log ( __str__ ) ; from ( __str__ ) . log ( __str__ ) ; } } ) ;,wire api,fail,pre
shrink data <PLACE_HOLDER> id table,int [ ] new data buffer id table = new int [ new index count ] ; system . arraycopy ( data buffer id table @$ __num__ @$ new data buffer id table @$ __num__ @$ new data buffer id table . length ) ; data buffer id table = new data buffer id table ;,data buffer,success,pre
check to see if the cipher even <PLACE_HOLDER> the attributes before trying to instantiate it .,try { if ( ! match attribute ( service @$ attribute_modes @$ tokenized transformation [ __num__ ] ) || ! match attribute ( service @$ attribute_paddings @$ tokenized transformation [ __num__ ] ) ) { return null ; } cipher spi and provider sap = new cipher spi and provider ( ( cipher spi ) service . new instance ( null ) @$ service . get provider ( ) ) ; if ( sap . cipher spi == null || sap . provider == null ) { return null ; } cipher spi spi = sap . cipher spi ; if ( ( ( type == need to set . mode ) || ( type == need to set . both ) ) && ( tokenized transformation [,cipher supports,success,pre
the rest api <PLACE_HOLDER> versions in decreasing order @$ but we want them in increasing order,collections . sort ( src versions ) ; for ( final integer src version : src versions ) { final versioned flow snapshot src flow snapshot = src client . get flow snapshot client ( ) . get ( src flow id @$ src version ) ; src flow snapshot . set flow ( null ) ; src flow snapshot . set bucket ( null ) ; final versioned flow snapshot metadata dest metadata = new versioned flow snapshot metadata ( ) ; dest metadata . set bucket identifier ( dest flow . get bucket identifier ( ) ) ; dest metadata . set flow identifier ( dest flow id ) ; dest metadata . set version ( src version ) ; dest metadata . set comments (,api returns,success,pre
this is a delayed chat room message @$ a history message for the room coming from server . lets check have we already <PLACE_HOLDER> this message and if this is the case skip it otherwise save it as last seen delayed message,if ( last seen delayed message == null ) { string timestamp = configuration utils . get chat room property ( provider @$ get identifier ( ) @$ last_seen_delayed_message_prop ) ; try { last seen delayed message = new date ( long . parse long ( timestamp ) ) ; } catch ( throwable t ) { } },check seen,fail,pre
a 'direct ' read actually has three phases . the first drains any remaining bytes from the slow read buffer . after this the read is guaranteed to be on a checksum chunk boundary . if there are still bytes to read @$ the fast direct path is used for as many remaining bytes as possible @$ up to a multiple of the checksum,if ( verify checksum ) { if ( slow read buff . has remaining ( ) ) { int from slow read buff = math . min ( buf . remaining ( ) @$ slow read buff . remaining ( ) ) ; write slice ( slow read buff @$ buf @$ from slow read buff ) ; n read += from slow read buff ; } if ( buf . remaining ( ) >= bytes per checksum && offset from chunk boundary == __num__ ) { int len = buf . remaining ( ) - ( buf . remaining ( ) % bytes per checksum ) ; len = math . min ( len @$ slow read buff . capacity ( ) ) ; int oldlimit =,which contains,fail,pre
weighted multi work unit weighted queue @$ the job will <PLACE_HOLDER> new work units to the queue along with a weight for each work unit . the queue will take care of balancing the work units amongst a set number of multi work units,multi work unit weighted queue multi work unit weighted queue = new multi work unit weighted queue ( this . max work units per job ) ;,queue add,success,pre
check to see if the resulting graph <PLACE_HOLDER> the expected node,structured graph replacement = get replacements ( ) . get substitution ( real method @$ - __num__ @$ false @$ null @$ graph . get options ( ) ) ; if ( replacement == null ) { assert in graph ( graph @$ expected node ) ; } option values options ; boolean need check node = true ; if ( java version util . java_spec <= __num__ ) { need check node = false ; } else { list < string > vm args = graal services . get input arguments ( ) ; assume . assume true ( vm args != null ) ; for ( string vm arg : vm args ) { if ( vm arg . equals ( disable_compactstrings_flag ) ) { need,graph contains,success,pre
not a scheme <PLACE_HOLDER> char .,return - __num__ ;,scheme drop,fail,pre
very special case to ensure that an indexed property descriptor does n't <PLACE_HOLDER> less information than the enclosed property descriptor . if it does @$ then recreate as a property descriptor . see 4168833,if ( pd instanceof indexed property descriptor ) { ipd = ( indexed property descriptor ) pd ; if ( ipd . get indexed read method ( ) == null && ipd . get indexed write method ( ) == null ) { pd = new property descriptor ( ipd ) ; } },descriptor lose,fail,pre
we deliberately do not create the call node during parsing @$ because the call target is only created after the code entry is parsed . the code entry might not be yet parsed when we encounter this call . furthermore @$ if the call target is imported from another module @$ then that other module might not have been parsed yet . therefore @$,call nodes . add ( new wasm call stub node ( function ) ) ; break ;,wasm call,fail,pre
playback will <PLACE_HOLDER> active source . otherwise audio system will .,if ( device type == hdmi device info . device_playback ) { hdmi cec local device playback playback = playback ( ) ; playback . set is active source ( true ) ; playback . wake up if active source ( ) ; playback . may send active source ( source ) ; set active source ( playback . m address @$ physical address ) ; } if ( device type == hdmi device info . device_audio_system ) { hdmi cec local device audio system audio system = audio system ( ) ; if ( playback ( ) != null ) { audio system . set is active source ( false ) ; } else { audio system . set is active source ( true ) ; audio,playback claim,success,pre
this should n't ever be possible as annotation processor should <PLACE_HOLDER> empty constructor,throw new runtime exception ( e ) ;,processor consider,fail,pre
this checks for the <PLACE_HOLDER> file names in the correct precedence order .,for ( root package path entry : pkg locator . get path entries ( ) ) { for ( build file name build file name : build files by priority ) { package lookup value result = get package lookup value ( env @$ package path entry @$ package key @$ build file name ) ; if ( result == null ) { return null ; } if ( result != package lookup value . no_build_file_value ) { return result ; } } },checks build,success,pre
when entity expansion limit is set by the application @$ we need to check for the entity expansion limit set by the parser @$ if number of entity expansions <PLACE_HOLDER> the entity expansion limit @$ parser will throw fatal error . note that this represents the nesting level of open entities .,f entity expansion count ++ ; if ( f limit analyzer != null ) { f limit analyzer . add value ( entity expansion index @$ name @$ __num__ ) ; } if ( f security manager != null && f security manager . is over limit ( entity expansion index @$ f limit analyzer ) ) { f security manager . debug print ( f limit analyzer ) ; f error reporter . report error ( xml message formatter . xml_domain @$ __str__ @$ new object [ ] { f security manager . get limit value by index ( entity expansion index ) } @$ xml error reporter . severity_fatal_error ) ; f entity expansion count = __num__ ; },number exceeds,success,pre
first check to see if this plain old field map <PLACE_HOLDER> hints to the actual type .,if ( result == null ) { if ( field map . get dest hint container ( ) != null ) { class < ? > dest hint type = field map . get dest hint type ( src field value . get class ( ) ) ; if ( dest hint type != null ) { dest field type = dest hint type ; } } string map id = field map . get map id ( ) ; class < ? > target class ; if ( field map . get dest hint container ( ) != null && field map . get dest hint container ( ) . get hint ( ) != null ) { target class = field map . get dest hint,map has,success,pre
indent:8 exp:12 <PLACE_HOLDER> indent:8 exp:8,try { } catch ( throwable t ) { system . identity hash code ( __str__ ) ; } finally { },exp:12 warn,success,pre
first call <PLACE_HOLDER> all @$ better use activate ? if you have providers,logger . debug ( __str__ @$ arrays . to string ( provider . get item names ( ) . to array ( ) ) ) ; for ( string item name : provider . get item names ( ) ) { bind gpio pin ( ( mcp3424 binding provider ) provider @$ item name ) ; },call contains,success,pre
the type flow of a field load is the type flow of the field itself . it <PLACE_HOLDER> all types ever stored to the field .,if ( n instanceof load field node ) { load field node node = ( load field node ) n ; if ( is object ( node ) ) { register flow ( node @$ results . lookup field ( node . field ( ) ) ) ; } } else if ( n instanceof store field node ) { store field node node = ( store field node ) n ; if ( is object ( node . value ( ) ) ) { type flow field flow = results . lookup field ( node . field ( ) ) ; lookup flow ( node . value ( ) ) . add use ( field flow ) ; } } else if ( n instanceof return node,flow handles,fail,pre
run streaming k <PLACE_HOLDER> step in parallel by spawning 1 thread per input path to process .,executor service pool = executors . new cached thread pool ( ) ; list < future < iterable < centroid > > > intermediate centroid futures = new array list < > ( ) ; for ( file status status : hadoop util . list status ( file system . get ( conf ) @$ input @$ path filters . logscrc filter ( ) ) ) { intermediate centroid futures . add ( pool . submit ( new streamingk means thread ( status . get path ( ) @$ conf ) ) ) ; } log . info ( __str__ ) ;,k means,success,pre
dispose the input channel after removing the window so the window manager does n't <PLACE_HOLDER> the input channel being closed as an abnormal termination .,if ( m input channel != null ) { m input channel . dispose ( ) ; m input channel = null ; } m display manager . unregister display listener ( m display listener ) ; unschedule traversals ( ) ;,manager interpret,success,pre
m vpns <PLACE_HOLDER> needs to be hold here to ensure that the active vpn can not be changed between these two calls .,synchronized ( m vpns ) { old blocked = is uid networking with vpn blocked ( nri . m uid @$ uid rules @$ old metered @$ old restrict background ) ; new blocked = is uid networking with vpn blocked ( nri . m uid @$ uid rules @$ new metered @$ new restrict background ) ; } if ( old blocked != new blocked ) { call callback for request ( nri @$ nai @$ connectivity manager . callback_blk_changed @$ encode bool ( new blocked ) ) ; },vpns list,fail,pre
sometimes most recent focus owner may be null @$ but focus owner is not e.g . we reset most recent focus owner if user <PLACE_HOLDER> focus owner,if ( focus owner == null ) { focus owner = keyboard focus manager . get current keyboard focus manager ( ) . get focus owner ( ) ; if ( focus owner != null && focus owner . get containing window ( ) != window ) { focus owner = null ; } },user changes,fail,pre
if the caller does not <PLACE_HOLDER> permission to load the driver then skip it .,for ( driver info a driver : registered drivers ) { if ( is driver allowed ( a driver . driver @$ caller class loader ) ) { result . add element ( a driver . driver ) ; } else { println ( __str__ + a driver . get class ( ) . get name ( ) ) ; } },caller have,success,pre
some exceptions rudely do n't <PLACE_HOLDER> cause initialization,return new throwable ;,rudely throw,fail,pre
the other thread will now try to close the connection map but it will block because this thread has <PLACE_HOLDER> one of the connections,await ( ) . until ( ( ) -> connection map . closing ) ; try { connection manager . borrow connection ( __num__ ) ; fail ( __str__ ) ; } catch ( cache closed exception e ) { },thread closed,fail,pre
let the os <PLACE_HOLDER> the start address of the region in memory,mmap ptr = os . mmap ( __num__ @$ mmap region size @$ os constants . prot_read @$ os constants . map_shared | os constants . map_populate @$ m fd @$ mmap file position ) ;,os store,fail,pre
test method <PLACE_HOLDER> output stream.constructor,try { file output stream out file = new file output stream ( file . create temp file ( __str__ @$ __str__ ) ) ; checked output stream chk out = new checked output stream ( out file @$ new crc32 ( ) ) ; assert equals ( __str__ @$ __num__ @$ chk out . get checksum ( ) . get value ( ) ) ; out file . close ( ) ; } catch ( io exception e ) { fail ( __str__ ) ; } catch ( security exception e ) { fail ( __str__ ) ; },method has,fail,pre
let the remote peer know that we support rtcp xr in general and vo ip metrics <PLACE_HOLDER> block in particular .,string rtcpxr = md . get attribute ( rtcp extended report . sdp_attribute ) ; if ( rtcpxr == null ) { md . set attribute ( rtcp extended report . sdp_attribute @$ rtcp extended report . voip metrics report block . sdp_parameter ) ; } int ptime setting = sip activator . get configuration service ( ) . get int ( __str__ + __str__ @$ __num__ ) ;,xr report,success,pre
we do n't use preconditions.check argument because it requires boxing field id @$ which <PLACE_HOLDER> inner loop performance,if ( ! types [ field id ] . get java type ( ) . equals ( type ) ) { throw new illegal argument exception ( format ( __str__ @$ type @$ types [ field id ] @$ field id ) ) ; },which improves,fail,pre
verify that both the shutdown command and the application <PLACE_HOLDER> ok .,output . should have exit value ( __num__ ) ; process thread . join and throw ( ) ; process thread . get output ( ) . should have exit value ( __num__ ) ;,command finished,success,pre
when traversing dependencies for resources @$ we get resources attached to libraries that are statically linked @$ and resources attached to the initial bundle . this heuristic is based on the idea that the bundle holding the compiled code of a library also <PLACE_HOLDER> the resources .,case copying_include_shared_resources :,bundle contains,fail,pre
note : the lookup is enforcing security across users by making sure the caller can only <PLACE_HOLDER> widgets it hosts or provides .,widget widget = lookup widget locked ( app widget id @$ binder . get calling uid ( ) @$ calling package ) ; if ( widget == null ) { throw new illegal argument exception ( __str__ ) ; },caller access,success,pre
no luck with this constructor @$ let 's <PLACE_HOLDER> another one,throw new illegal state exception ( __str__ ) ;,'s try,success,pre
fake client as client.default <PLACE_HOLDER> redirects .,test interface api = feign . builder ( ) . client ( ( request @$ options ) -> response ) . target ( test interface . class @$ __str__ + server . get port ( ) ) ; assert equals ( api . response ( ) . headers ( ) . get ( __str__ ) @$ collections . singleton list ( __str__ ) ) ;,client follows,success,pre
first round @$ only calculate those <PLACE_HOLDER> a pa entry,for ( int i = __num__ ; i < e types . length ; i ++ ) { pa data . salt and params snp = pa data . get salt and params ( e types [ i ] @$ pa list ) ; if ( snp != null ) { if ( e types [ i ] != encrypted data . etype_arcfour_hmac && snp . salt != null ) { salt = snp . salt ; } result [ i ] = encryption key . acquire secret key ( cname @$ password @$ e types [ i ] @$ snp ) ; } },round using,fail,pre
make sure the response <PLACE_HOLDER> at least 1 result with a valid dependency id,if ( response . get table count ( ) == __num__ ) { response . add dependency ( new dependency pair . buffer dependency pair ( m_fragment msg . get output dep id ( __num__ ) @$ m_raw dummy result @$ __num__ @$ m_raw dummy result . length ) ) ; },response has,success,pre
page views . do this once we <PLACE_HOLDER> the right padding offsets from above .,for ( int i = __num__ ; i < count ; i ++ ) { final view child = get child at ( i ) ; if ( child . get visibility ( ) == gone ) { continue ; } final layout params lp = ( layout params ) child . get layout params ( ) ; if ( lp . is decor ) { continue ; } final item info ii = info for child ( child ) ; if ( ii == null ) { continue ; } if ( lp . needs measure ) { lp . needs measure = false ; final int width spec = measure spec . make measure spec ( ( int ) ( child width * lp . width,views have,success,pre
key size | <PLACE_HOLDER> bytes,should put and get key size ( __num__ @$ __num__ ) ; should put and get key size ( key_one_byte_max @$ __num__ ) ; should put and get key size ( key_two_byte_min @$ __num__ ) ; should put and get key size ( key_two_byte_max @$ __num__ ) ; should put and get key size ( key_two_byte_no_offload_max @$ __num__ ) ;,| count,fail,pre
predicate may <PLACE_HOLDER> any column .,for ( string col name : serde . column names ) { analyzer . allow column name ( col name ) ; } list < index search condition > search conditions = new linked list < index search condition > ( ) ; expr node desc residual = analyzer . analyze predicate ( predicate @$ search conditions ) ; decomposed predicate decomposed = new decomposed predicate ( ) ; decomposed . pushed predicate = analyzer . translate search conditions ( search conditions ) ; decomposed . residual predicate = ( expr node generic func desc ) residual ; return decomposed ;,predicate contain,success,pre
auto expand only support when each cell <PLACE_HOLDER> one span .,if ( ! laying out in primary direction && remaining span == __num__ && ( count == consumed span count ) && range style . m is auto expand ) { if ( layout in vertical ) { range style . m size per span = ( m total size - ( count - __num__ ) * range style . mh gap ) / count ; } else { range style . m size per span = ( m total size - ( count - __num__ ) * range style . mv gap ) / count ; } },cell occupy,success,pre
restart and gii @$ it should be full gii because number of unfinished operations <PLACE_HOLDER> limit,change unfinished operation limit ( r @$ __num__ ) ; create distributed region ( r ) ;,number exceeds,success,pre
empty constructor should <PLACE_HOLDER> a new instance of adapter delegates manager,list delegation adapter < list < object > > adapter = new list delegation adapter < list < object > > ( ) { @ override public int get item count ( ) { assert . assert not null ( this . delegates manager ) ; return __num__ ; } } ;,constructor produce,success,pre
case 9 : true if an attribute named integer att name of type integer <PLACE_HOLDER> the value integer value we cover javax.management.binary rel query exp with a rel op equal to eq and javax.management.numeric value exp,queries . add ( query . eq ( query . attr ( integer att name ) @$ query . value ( integer value ) ) ) ;,name has,success,pre
we expect this to throw an io exception since we 're faking socket <PLACE_HOLDER> errors every time,learner . connect to leader ( new multiple addresses ( addr ) @$ __str__ ) ;,time closed,fail,pre
sets payload to empty as frame <PLACE_HOLDER> no data,frame builder . payload ( new byte [ ] { } ) ; curr bytes . reset ( ) ; curr state = beats state . complete ; window size = frame builder . data size ; break ;,frame contains,success,pre
load apdex <PLACE_HOLDER> threshold,final long apdex satisfied threshold = get required property ( props @$ report_generator_key_apdex_satisfied_threshold @$ report_generator_key_apdex_satisfied_threshold_default @$ long . class ) ; configuration . set apdex satisfied threshold ( apdex satisfied threshold ) ;,apdex pass,fail,pre
5 minutes in milliseconds <PLACE_HOLDER> a configuration from the arguments,config config = new config ( ) ; config . parse ( async benchmark . class . get name ( ) @$ args ) ; system . out . print ( horizontal_rule ) ; log . info ( __str__ ) ; log . info ( horizontal_rule ) ; log . info ( config . get config dump string ( ) ) ; if ( config . latencyreport ) { log . info ( __str__ ) ; },minutes reload,fail,pre
... but the three requests that follow requests <PLACE_HOLDER> an authorization header,for ( int i = __num__ ; i < __num__ ; i ++ ) { request = server . take request ( ) ; assert equals ( __str__ @$ request . get request line ( ) ) ; assert contains ( request . get headers ( ) @$ __str__ + simple authenticator . base_64_credentials ) ; },requests have,fail,pre
using factory <PLACE_HOLDER> an instance of document builder,try { document builder db = dbf . new document builder ( ) ; document doc = db . parse ( is ) ; element doc ele = doc . get document element ( ) ; node list nl = doc ele . get elements by tag name ( __str__ ) ; if ( nl != null && nl . get length ( ) > __num__ ) { element el = ( element ) nl . item ( __num__ ) ; string reason = get text value from attribute ( el @$ __str__ ) ; throw new logout notification exception ( __str__ + reason ) ; } nl = doc ele . get elements by tag name ( __str__ ) ; if ( nl != null && nl,factory get,success,pre
user <PLACE_HOLDER> a to transfer token to b,string param = __str__ + base58 . encode58 check ( transfer token contract address ) + __str__ + asset account id . to string utf8 ( ) + __str__ ; final string trigger txid = public methed . trigger contract ( transfer token contract address @$ __str__ @$ param @$ false @$ __num__ @$ __num__ @$ __str__ @$ __num__ @$ user001 address @$ user001 key @$ blocking stub full ) ; public methed . wait produce next block ( blocking stub full ) ; account infoafter = public methed . query account ( user001 address @$ blocking stub full ) ; account resource message resource infoafter = public methed . get account resource ( user001 address @$ blocking stub full ) ; long after balance = infoafter .,user trigger,success,pre
test that all subtasks are taken into the account for the summary . the correctness of the actual results is checked in the test of the min <PLACE_HOLDER> avg stats .,task state stats . task state stats summary summary = task stats . get summary stats ( ) ; assert equals ( subtasks . length @$ summary . get state size stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get ack timestamp stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get sync checkpoint duration stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get async checkpoint duration stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get alignment buffered stats ( ) . get count (,test reduce,fail,pre
if user <PLACE_HOLDER> address or port @$ we have to find the row with the previous value,if ( is edit mode ) { stun server = get stun server ( previous server @$ previous port ) ; } else { stun server = get stun server ( address @$ port ) ; },user specified,fail,pre
determine container and cache names <PLACE_HOLDER> legacy logic,string replication config cache name = ( config != null ) ? config . get cache name ( ) : null ; service name replication config service name = service name factory . parse service name ( ( replication config cache name != null ) ? replication config cache name : __str__ ) ; service name base replication config service name = service name . jboss . append ( __str__ ) ; if ( ! base replication config service name . is parent of ( replication config service name ) ) { replication config service name = base replication config service name . append ( replication config service name ) ; } string container name = ( ( replication config service name . length ( ) > __num__,names using,success,pre
even though we 've <PLACE_HOLDER> a field @$ the original list should have an empty row <PLACE_HOLDER> to make it the same size,assert equals ( field list . size ( ) @$ other field list . size ( ) ) ; for ( int i = __num__ ; i < field list . size ( ) ; i ++ ) { data type line data type line = ( data type line ) field list . get ( i ) ; data type line other data type line = ( data type line ) other field list . get ( i ) ; if ( i == ( insert indexb + __num__ ) ) { assert true ( data type line instanceof empty data type line ) ; assert not null ( other data type line . get name color ( ) ) ; assert not null ( other data,size added,success,pre
get the lease renewer now so we can verify it later without calling get lease renewer @$ which will automatically <PLACE_HOLDER> the client into it .,final lease renewer lease renewer = dfs . get client ( ) . get lease renewer ( ) ; stream . write ( __str__ . get bytes ( ) ) ; try { stream . hflush ( ) ; fail ( __str__ ) ; } catch ( ds quota exceeded exception expected ) { },which write,fail,pre
<PLACE_HOLDER> the default max thread number to 100 to limit the number of concurrent requests so that rest server does n't oom easily . jetty <PLACE_HOLDER> the default max thread number to 250 @$ if we do n't <PLACE_HOLDER> it . our default min thread number 2 is the same as that used by jetty .,int max threads = servlet . get configuration ( ) . get int ( rest_thread_pool_threads_max @$ __num__ ) ; int min threads = servlet . get configuration ( ) . get int ( rest_thread_pool_threads_min @$ __num__ ) ;,jetty set,success,pre
map the <PLACE_HOLDER> position to a position in the corresponding timeline .,pair < object @$ long > period position ; try { period position = seek timeline . get period position ( window @$ period @$ seek position . window index @$ seek position . window position us ) ; } catch ( index out of bounds exception e ) { return null ; },the seek,success,pre
the order in which the partition by keys are listed should not radically <PLACE_HOLDER> the plan structure .,windowed query = __str__ ; validate windowed function plan ( windowed query @$ __num__ @$ __num__ @$ __num__ @$ expression type . aggregate_windowed_rank ) ;,order change,success,pre
ensure that per rfc 6265 @$ cookie.value <PLACE_HOLDER> syntax rules,syntax . require validrfc6265 cookie value ( _value ) ;,cookie.value follows,success,pre
see which urls the pws did n't <PLACE_HOLDER> us a response for .,set < string > missed = new hash set < > ( broadcast urls ) ; missed . remove all ( found urls ) ; for ( string url : missed ) { pws result callback . on pws result absent ( url ) ; } record response ( ) ; pws result callback . on pws result error ( broadcast urls @$ response code @$ e ) ;,pws give,success,pre
yay ! let 's <PLACE_HOLDER> the port number,string s = m . group ( __num__ ) ; port = integer . parse int ( s ) ; inet address add = server . get inet address ( ) ; if ( add != null ) { dest = new inet socket address ( add @$ port ) ; } else { dest = inet socket address . create unresolved ( server addr . get host name ( ) @$ port ) ; },'s get,fail,pre
if cipher is null @$ <PLACE_HOLDER> random bytes failed @$ which means encryption is meaningless . therefore @$ do not save anything . this will cause users to lose incognito state in certain cases . that is annoying @$ but is better than failing to provide the guarantee of incognito mode .,return ;,failed generating,fail,pre
calls on monitors from child should <PLACE_HOLDER> both listeners,my monitor child monitor = child . new monitor ( my monitor . class ) ; child monitor . a void ( ) ; mockito . verify ( parent listener @$ mockito . times ( __num__ ) ) . a void ( ) ; mockito . verify ( child listener ) . a void ( ) ;,calls trigger,fail,pre
let 's <PLACE_HOLDER> folder,data . sftpclient . create folder ( spool directory ) ; if ( is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ spool directory ) ) ; },'s create,success,pre
the wire protocol <PLACE_HOLDER> an array of keys .,char sequence [ ] sequence = { key to release } ; executor . execute ( driver command . send_keys_to_active_element @$ immutable map . of ( __str__ @$ sequence ) ) ;,protocol requires,success,pre
be aware that probe index builder will not <PLACE_HOLDER> its capacity,probe index builder . clear ( ) ; build page builder . reset ( ) ; estimated probe block bytes = __num__ ; is sequential probe indices = true ;,builder increase,fail,pre
process entries in vx <PLACE_HOLDER> symbol table,address vx sym tbl = vx num sym entries addr . subtract ( vx num sym entries * sym_entry_size ) ; for ( int i = __num__ ; i < vx num sym entries ; i ++ ) { if ( monitor . is cancelled ( ) ) { return ; } println ( __str__ + i ) ; address sym entry = vx sym tbl . add ( i * sym_entry_size ) ; address sym name addr = to addr ( mem . get int ( sym entry . add ( sym_name_off ) ) ) ; address sym loc addr = to addr ( mem . get int ( sym entry . add ( sym_loc_off ) ) ) ; byte sym type = mem . get byte,entries works,success,pre
count variational numbers . every next one either <PLACE_HOLDER> the same value or previous one plus probability of loss .,for ( int i = __num__ ; i < losses . length ; i ++ ) if ( i == __num__ ) loss probs [ i ] = get loss probability ( losses @$ __num__ ) ; else if ( losses [ i ] != losses [ i - __num__ ] ) loss probs [ i ] = get loss probability ( losses @$ i ) + loss probs [ i - __num__ ] ; else loss probs [ i ] = loss probs [ i - __num__ ] ;,one has,success,pre
close req 1 and make sure req 2 does not <PLACE_HOLDER> num active requests .,actual res1 . close ( ) ; await ( ) . until asserted ( ( ) -> assert that ( client . num active requests ( ) ) . is zero ( ) ) ;,req exist,fail,pre
okay @$ ` former method ' and ` method ' both <PLACE_HOLDER> the same signature . see if they are compatible .,all methods . replace ( method ) ;,method have,success,pre
the following trick <PLACE_HOLDER> the instantiation of a record writer via the job thus supporting arbitrary output formats .,job job = job . get instance ( context . get configuration ( ) ) ; job . set output format class ( get named output format class ( context @$ name output ) ) ; job . set output key class ( get named output key class ( context @$ name output ) ) ; job . set output value class ( get named output value class ( context @$ name output ) ) ; task context = new task attempt context impl ( job . get configuration ( ) @$ context . get task attemptid ( ) @$ new wrapped status reporter ( context ) ) ; task contexts . put ( name output @$ task context ) ; return task context ;,trick leverages,success,pre
took header does not <PLACE_HOLDER> sense as we stream,return response . ok ( out ) . build ( ) ;,header make,success,pre
a long does not <PLACE_HOLDER> empty string,assert filter matches skip vectorize ( edf ( __str__ ) @$ immutable list . of ( ) ) ;,long match,success,pre
if this was the last listener that was registered with us then no long need to have a delegator registered with the call <PLACE_HOLDER> media handlers . we therefore remove it so that audio level calculations would be ceased .,if ( ( local user audio level listeners == null ) || local user audio level listeners . is empty ( ) ) { iterator < t > call peer iter = get call peers ( ) ; while ( call peer iter . has next ( ) ) { call peer iter . next ( ) . get media handler ( ) . set local user audio level listener ( null ) ; } },handlers peer,success,pre
if write id list is not null @$ that means stats are requested within a txn context . so set stats compliant to false @$ if are txn stats supported is false or the write id which has <PLACE_HOLDER> the stats in not compatible with write id list . this is done within table lock as the number of partitions may be more than,list < column statistics > column statistics = shared cache . get partition col stats list from cache ( cat name @$ db name @$ tbl name @$ part names @$ col names @$ write id list @$ are txn stats supported ) ; if ( column statistics == null ) { return raw store . get partition column statistics ( cat name @$ db name @$ tbl name @$ part names @$ col names @$ engine @$ write id list ) ; } return column statistics ;,which contributed,fail,pre
ticket set to anonymous for anonymous user . <PLACE_HOLDER> testing .,subject current user = org . apache . shiro . security utils . get subject ( ) ; if ( current user . is authenticated ( ) ) { current user . logout ( ) ; } log . debug ( __str__ + current user ) ; if ( ! current user . is authenticated ( ) ) { username password token token = new username password token ( user name @$ password ) ; response = proceed to login ( current user @$ token ) ; } if ( response == null ) { response = new json response ( response . status . forbidden @$ __str__ @$ __str__ ) ; } log . warn ( response . to string ( ) ) ; return response .,ticket simplify,success,pre
each instance should <PLACE_HOLDER> 18 segments assigned,num segments assigned per instance = segment assignment utils . get num segments assigned per instance ( new assignment @$ new instances ) ; expected num segments assigned per instance = new int [ new num instances ] ; new num segments per instance = num segments * num_replicas / new num instances ; arrays . fill ( expected num segments assigned per instance @$ new num segments per instance ) ; assert equals ( num segments assigned per instance @$ expected num segments assigned per instance ) ;,instance have,success,pre
repository objects take priority so let 's <PLACE_HOLDER> them ...,read databases ( trans meta @$ true ) ; read partition schemas ( trans meta @$ true ) ; read slaves ( trans meta @$ true ) ; read clusters ( trans meta @$ true ) ; return trans meta . get shared objects ( ) ;,'s overwrite,success,pre
key a was added first and key b has <PLACE_HOLDER> 2,assert equals ( __num__ @$ key set utils . get key set ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get pub key ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get key set ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get pub key ref count ( m ksms @$ __num__ ) ) ; assert equals ( keyb @$ key set utils . get pub key ( m ksms @$ __num__ ) ) ; mapping = ks mapping . get ( __num__ ) ; assert equals ( __num__ @$ mapping . size ( ),key db,fail,pre
a newly created one from the source should <PLACE_HOLDER> a different value .,recoverable js ast ast2 = new recoverable js ast ( real ast @$ true ) ; check compile ( real ast @$ make defensive copy ( ast2 ) @$ __str__ ) ;,one have,success,pre
verify that the class does not also <PLACE_HOLDER> a method with the same stem name with 'is ',try { final method get method = container class . get declared method ( __str__ + stem name ) ; if ( ! modifier . is static ( get method . get modifiers ( ) ) && get method . get annotation ( transient . class ) == null ) { check get and is variants ( container class @$ property name @$ get method @$ is method ) ; } } catch ( no such method exception ignore ) { },class define,success,pre
children only <PLACE_HOLDER> configuration details of the parent @$ and are not independent entities,return false ;,children affect,fail,pre
make the icon change between media icon and switch field icon depending on whether editing <PLACE_HOLDER> type,if ( edit model mode && allow field remapping ( ) ) { media button . set background resource ( icons [ __num__ ] ) ; set remap button listener ( media button @$ i ) ; } else if ( edit model mode && ! allow field remapping ( ) ) { media button . set background resource ( __num__ ) ; } else { media button . set background resource ( icons [ __num__ ] ) ; setmm button listener ( media button @$ i ) ; },editing changed,fail,pre
and now pretend that the remote contact has <PLACE_HOLDER> us authorization,return new authorization response ( authorization response . accept @$ __str__ ) ;,contact given,fail,pre
note : this is crazy . <PLACE_HOLDER> rounds up @$ but time format down we maintain this behavior for backward compatibility .,long start units = ( long ) math . ceil ( start . get millis ( ) ) / __num__ ; long end units = ( long ) math . ceil ( end exclusive . get millis ( ) ) / __num__ ; if ( objects . equals ( start units @$ end units ) ) { return string . format ( __str__ @$ get to unix time clause ( time format @$ time field @$ source name ) @$ start units ) ; } return string . format ( __str__ @$ get to unix time clause ( time format @$ time field @$ source name ) @$ start units @$ end units ) ;,note offset,fail,pre
test that the module map content <PLACE_HOLDER> the individual headers inside the header tree artifact .,assert that ( module map content ) . contains ( __str__ ) ; assert that ( umbrella header content ) . contains ( headers . get exec path string ( ) + __str__ ) ; assert that ( umbrella header content ) . contains ( headers . get exec path string ( ) + __str__ ) ;,content contains,success,pre
emit a copy <PLACE_HOLDER> phase for each destination .,for ( copy file phase destination spec destination spec : rule by destination spec . key set ( ) ) { iterable < target node < ? > > target nodes = rule by destination spec . get ( destination spec ) ; phases . add ( get single copy files build phase ( destination spec @$ target nodes ) ) ; } return phases . build ( ) ;,copy build,fail,pre
now test that using different translates with lbm is ok this test does n't prove a lot since showing a leak really <PLACE_HOLDER> a basher test that can run for a long time .,for ( int x = __num__ ; x < __num__ ; x ++ ) { for ( int y = __num__ ; y < __num__ ; y ++ ) { attributed character iterator aci = van gogh . get iterator ( ) ; affine transform tx = affine transform . get translate instance ( x @$ y ) ; font render context frc = new font render context ( tx @$ false @$ false ) ; line break measurer lbm = new line break measurer ( aci @$ frc ) ; lbm . set position ( aci . get begin index ( ) ) ; while ( lbm . get position ( ) < aci . get end index ( ) ) { lbm . next layout (,test causes,fail,pre
now if we found a region load <PLACE_HOLDER> the type of cost that was requested .,if ( region load list != null ) { cost = ( long ) ( cost + get region load cost ( region load list ) ) ; },load add,fail,pre
if no resources in the holder @$ or if the holder <PLACE_HOLDER> different resources loaded @$ then load the configuration and set the new resources in the holder,if ( resources == null || ! config resources . equals ( resources . get config resources ( ) ) ) { get logger ( ) . debug ( __str__ ) ; final configuration config = new extended configuration ( get logger ( ) ) ; config . set class loader ( thread . current thread ( ) . get context class loader ( ) ) ; resources = new validation resources ( config resources @$ get configuration from resources ( config @$ config resources ) ) ; validation resource holder . set ( resources ) ; } final configuration conf = resources . get configuration ( ) ; results . add all ( kerberos properties . validate principal and keytab ( this . get class ( ),holder has,success,pre
needed because this class overrides <PLACE_HOLDER> next event @$ move down .,move down ( ) ;,overrides send,fail,pre
file . we 're going to check the consistency of the resource file while building this mapping @$ and throw errors if the file does not <PLACE_HOLDER> our assumptions .,map < string @$ collection < string > > lines = new hash map < > ( ) ; final check charset mapping mapping = new check charset mapping ( ) ; for ( string key : props . string property names ( ) ) { collection < string > values = get values ( props . get property ( key ) ) ; lines . put ( key @$ values ) ; mapping . add mapping ( key @$ values ) ; },file match,fail,pre
big query <PLACE_HOLDER> seconds so we have to divide here,row . put ( __str__ @$ now in millis / __num__ ) ; client . insert row ( row @$ schema @$ table name ) ;,query expects,fail,pre
copy the variables of the transformation to the step ... do n't share . each copy of the step <PLACE_HOLDER> its own variables .,step . initialize variables from ( this ) ; step . set using thread priority managment ( trans meta . is using thread priority managment ( ) ) ;,copy has,success,pre
create a table with a primary key named 'name ' @$ which <PLACE_HOLDER> a string,try { create table ( ) ; describe table ( ) ; put item ( new item ( __str__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) ; put item ( new item ( __str__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) ; get item ( __str__ ) ; get item ( __str__ ) ; map < string @$ condition > scan filter = new hash map < string @$ condition > ( ) ; condition condition = new condition ( ) . with comparison operator ( comparison operator . gt . to string ( ) ) . with attribute value list ( new attribute value ( ) . withn ( __str__ ) ) ; scan filter . put ( __str__ @$ condition,which holds,success,pre
phone 1 should always <PLACE_HOLDER> phone 2 @$ all other things being equal .,assert equals ( expected phone1 scored suggestion @$ m time zone detector strategy . find best phone suggestion for tests ( ) ) ;,phone beat,success,pre
when process is creating a lot of connections this can take some time so <PLACE_HOLDER> the timeout,if ( holder == null ) { holder = new connection holder ( this @$ the serverid @$ options ) ; connection holder prev holder = connections . put if absent ( the serverid @$ holder ) ; if ( prev holder != null ) { holder = prev holder ; } else { holder . connect ( ) ; } },time use,fail,pre
this call <PLACE_HOLDER> j mockit to load our spy,new spy go to helper ( ) ; tool . get options ( __str__ ) . set enum ( __str__ @$ navigation options . external navigation enum . navigate to external program ) ;,call triggers,success,pre
snapshot <PLACE_HOLDER> no data after flush,int number of mem scanners after flush = input cells after snapshot . is empty ( ) ? __num__ : __num__ ; boolean more ; int cell count = __num__ ; do { list < cell > cells = new array list < > ( ) ; more = s . next ( cells ) ; cell count += cells . size ( ) ; assert equals ( more ? number of mem scanners after flush : __num__ @$ count mem store scanner ( s ) ) ; } while ( more ) ; assert equals ( __str__ + input cells before snapshot . size ( ) + __str__ + input cells after snapshot . size ( ) @$ input cells before snapshot . size ( ) +,snapshot contains,fail,pre
merge similar styles . doing so will <PLACE_HOLDER> performance .,last style = styles . get ( styles . size ( ) - __num__ ) ; if ( last style . similar to ( style ) && ( last style . start + last style . length == style . start ) ) { last style . length += style . length ; } else { styles . add ( style ) ; },styles improve,success,pre
if it is not a global @$ it might be accessing a local of the outer scope . if that 's the case the functions between the variable 's <PLACE_HOLDER> scope and the variable reference scope can not be moved .,if ( var . get scope ( ) != t . get scope ( ) ) { for ( name context context : symbol stack ) { if ( context . scope == var . get scope ( ) ) { break ; } context . name . read closure variables = true ; } },functions grabbed,fail,pre
the server side will <PLACE_HOLDER> something @$ and in order to proceed with the initial tls handshake we need to start reading before waiting for the callback .,byte [ ] buffer = new byte [ __num__ ] ; int len = client . get input stream ( ) . read ( buffer ) ; assert equals ( __str__ @$ new string ( buffer @$ __num__ @$ len @$ standard charsets . utf_8 ) ) ; assert null ( _write callback . get ( __num__ @$ time unit . seconds ) ) ;,side write,success,pre
if instruction <PLACE_HOLDER> fall thru,address fall thru = null ; if ( instr . has fallthrough ( ) ) { if ( check non returning ( program @$ flow type @$ instr ) ) { target = get next target ( body @$ untried target list ) ; repeat instruction byte tracker . reset ( ) ; continue ; } new target = instr . get fall through ( ) ; fall thru = new target ; } else { address next addr = max addr . next ( ) ; if ( target list . contains ( next addr ) ) { new target = next addr ; } else if ( flow type . is jump ( ) ) { address flows [ ] = instr . get flows (,instruction has,success,pre
native library <PLACE_HOLDER> code generator,android binary graph enhancer graph enhancer = new android binary graph enhancer ( toolchain provider @$ context . get cell path resolver ( ) @$ build target @$ context . get project filesystem ( ) @$ android platform target @$ params @$ graph builder @$ args . get aapt mode ( ) @$ immutable list . of ( ) @$ resource compression mode . disabled @$ filter resources steps . resource filter . empty_filter @$ enum set . none of ( r type . class ) @$ optional . empty ( ) @$ optional . empty ( ) @$ immutable set . of ( ) @$ null @$ args . get manifest ( ) @$ args . get manifest skeleton ( ) @$ optional . empty ( ),library merge,success,pre
0 x 100248 f : p 1 and p 2 <PLACE_HOLDER> same function comment .,program builder1 . create function comment ( __str__ @$ __str__ ) ; program builder2 . create function comment ( __str__ @$ __str__ ) ; check no comment difference ( ) ;,1 have,success,pre
without the name @$ the suppression node <PLACE_HOLDER> the topology index,assert that ( anonymous node topology @$ is ( anonymous_intermediate_topology ) ) ;,node increments,success,pre
do n't need to clone these @$ since the trigger context does n't <PLACE_HOLDER> modification,for ( map . entry < w @$ value state < bit set > > entry : state . access in each merging window ( finished_bits_tag ) . entry set ( ) ) { builder . put ( entry . get key ( ) @$ read finished bits ( entry . get value ( ) ) ) ; clear finished bits ( entry . get value ( ) ) ; },context perform,fail,pre
add the tail string which <PLACE_HOLDER> no variables and return the result .,sbuf . append ( message pattern . substring ( i @$ message pattern . length ( ) ) ) ; return sbuf . to string ( ) ;,which contains,success,pre
' : ' is an <PLACE_HOLDER> of extensible rules,if ( filter [ i ] == __str__ && ftype == ldap_filter_ext ) { if ( filter [ i - __num__ ] == __str__ ) { throw new invalid search filter exception ( __str__ ) ; } extensible start = i ; break ; },' indicator,success,pre
replace spaces because importer ca n't <PLACE_HOLDER> attributes titles in quotes,writer . append ( __str__ ) ; edge iterable edge iterable = graph . get edges ( ) ; for ( edge edge : edge iterable ) { print edge data ( edge @$ edge . get source ( ) @$ edge . get target ( ) @$ graph ) ; if ( ! edge . is directed ( ) && ! edge . is self loop ( ) ) { print edge data ( edge @$ edge . get target ( ) @$ edge . get source ( ) @$ graph ) ; } progress . progress ( progress ticket ) ; if ( cancel ) { edge iterable . do break ( ) ; return ; } },importer give,fail,pre
pmd can not <PLACE_HOLDER> array length type @$ but only the,if ( expressions . size ( ) == number constants . integer_size_or_length_2 ) { ast primary expression left = expressions . get ( __num__ ) ; ast primary expression right = expressions . get ( __num__ ) ; boolean both array length = is array length ( left ) && is array length ( right ) ; boolean both wrapper type = node utils . is wrapper type ( left ) && node utils . is wrapper type ( right ) ; if ( ! both array length && both wrapper type ) { add violation with message ( data @$ node @$ __str__ ) ; } },pmd define,fail,pre
if true @$ returns <PLACE_HOLDER> responses instead of correct protobufs .,boolean poisoned = false ; if ( worker options . poison after > __num__ && work unit counter > worker options . poison after ) { poisoned = true ; } if ( poisoned && worker options . hard poison ) { system . err . println ( __str__ ) ; system . exit ( __num__ ) ; } else { int exit code = __num__ ; try { options parser parser = parser helper ( request . get arguments list ( ) ) ; example work multiplexer options options = parser . get options ( example work multiplexer options . class ) ; if ( options . write counter ) { counter output = work unit counter ++ ; } results . add ( executor service .,returns empty,fail,pre
if no application can <PLACE_HOLDER> the url @$ assume that the web view can <PLACE_HOLDER> it .,return overriding url loading ;,application handle,success,pre
let 's use simple baseline value @$ arbitrary date in gmt @$ <PLACE_HOLDER> the standard notation,string input str = __str__ ; date input date = mapper . read value ( __str__ + input str + __str__ @$ java . util . date . class ) ; calendar c = calendar . get instance ( time zone . get time zone ( __str__ ) ) ; c . set time ( input date ) ; assert equals ( __num__ @$ c . get ( calendar . year ) ) ; assert equals ( calendar . december @$ c . get ( calendar . month ) ) ; assert equals ( __num__ @$ c . get ( calendar . day_of_month ) ) ;,value using,success,pre
v <PLACE_HOLDER> 00 f 2 <PLACE_HOLDER> 00 cd z <PLACE_HOLDER> 00 b 8 <PLACE_HOLDER> 0002 <PLACE_HOLDER> 0000 <PLACE_HOLDER> 0004 j <PLACE_HOLDER> 0000,byte [ ] template = { ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__,j \ u,fail,pre
check if current <PLACE_HOLDER> time exceeds the max <PLACE_HOLDER> time,long current wait time ; site tasker next task = m_tasks . peek ( ) ; if ( next task == null ) { current wait time = __num__ ; } else { current wait time = current time - next task . get queue offer time ( ) ; },max wait,success,pre
both should <PLACE_HOLDER> the same number of servers .,assert equals ( first balancer . get no of servers ( ) @$ second balancer . get no of servers ( ) ) ;,both have,success,pre
this constructor delays <PLACE_HOLDER> lookup to detect changes,channel builder = netty channel builder . for address ( inet server address . get host name ( ) @$ inet server address . get port ( ) ) ; channel builder = netty channel builder . for address ( address ) ;,delays idle,fail,pre
perfect match @$ just <PLACE_HOLDER> the replacement text in .,if ( text1 . equals ( text2 ) ) { text = text . substring ( __num__ @$ start_loc ) + diff_text2 ( a patch . diffs ) + text . substring ( start_loc + text1 . length ( ) ) ; } else { linked list < diff > diffs = diff_main ( text1 @$ text2 @$ false ) ; if ( text1 . length ( ) > this . match_ max bits && diff_levenshtein ( diffs ) / ( float ) text1 . length ( ) > this . patch_ delete threshold ) { results [ x ] = false ; } else { diff_cleanup semantic lossless ( diffs ) ; int index1 = __num__ ; for ( diff a diff : a patch . diffs,match put,fail,pre
if the post does n't <PLACE_HOLDER> location set @$ show the picker directly,if ( ! get edit post repository ( ) . has location ( ) ) { show location picker ( ) ; return ; },post have,success,pre
the suite made here will all be <PLACE_HOLDER> the tests from this class,multi config suite builder builder = new multi config suite builder ( testjdbc connection fail . class ) ;,suite using,success,pre
check that the starting event occurs before the <PLACE_HOLDER> event,assert true ( listener . starting time <= listener . started time @$ __str__ ) ;,the started,success,pre
as tomcat can not <PLACE_HOLDER> the provider object in the configuration . it 'll go into this path,if ( secret provider == null ) { string config prefix = filter config . get init parameter ( config_prefix ) ; config prefix = ( config prefix != null ) ? config prefix + __str__ : __str__ ; try { secret provider = authentication filter . construct secret provider ( filter config . get servlet context ( ) @$ super . get configuration ( config prefix @$ filter config ) @$ false ) ; this . is initialized by tomcat = true ; } catch ( exception ex ) { throw new servlet exception ( ex ) ; } },tomcat specify,success,pre
together @$ these two <PLACE_HOLDER> care that multi exceptions thrown from route resource come out as json or gpx @$ depending on the media type,environment . jersey ( ) . register ( new multi exception mapper ( ) ) ; environment . jersey ( ) . register ( new multi exceptiongpx message body writer ( ) ) ; environment . jersey ( ) . register ( new illegal argument exception mapper ( ) ) ; environment . jersey ( ) . register ( new gh point converter provider ( ) ) ; final graph hopper managed graph hopper managed = new graph hopper managed ( configuration . get graph hopper configuration ( ) @$ environment . get object mapper ( ) ) ; environment . lifecycle ( ) . manage ( graph hopper managed ) ; environment . jersey ( ) . register ( new abstract binder ( ) { @ override,two take,success,pre
what is current sequenceid ? we read the current sequenceid from the current file . after we read it @$ another thread could come in and compete with us writing out next version of file . the below retries should help in this case some but its hard to <PLACE_HOLDER> guarantees in face of concurrent schema edits .,int current sequence id = current descriptor file == null ? __num__ : get table info sequence id ( current descriptor file . get path ( ) ) ; int new sequence id = current sequence id ;,some give,fail,pre
last element <PLACE_HOLDER> repeated : what 's the name ?,data . item element = meta . get input position ( ) [ meta . get input position ( ) . length - __num__ ] ; data . item count = xml handler . count nodes ( data . section @$ data . item element ) ; data . item position = meta . get nr rows to skip ( ) ;,element provided,fail,pre
second renderer <PLACE_HOLDER> german .,map < string @$ integer > second renderer mapped capabilities = new hash map < > ( ) ; second renderer mapped capabilities . put ( english . id @$ format_unsupported_subtype ) ; second renderer mapped capabilities . put ( german . id @$ format_handled ) ; renderer capabilities second renderer capabilities = new fake mapped renderer capabilities ( c . track_type_audio @$ second renderer mapped capabilities ) ; renderer capabilities [ ] renderer capabilities = new renderer capabilities [ ] { first renderer capabilities @$ second renderer capabilities } ;,renderer handles,success,pre
number match <PLACE_HOLDER> at least the two chars being checked,if ( num end idx > pos ) { if ( num end idx > next pos ) { next pos = num end idx ; pos = num end idx ; do { pos = move index32 ( f text @$ pos @$ - __num__ ) ; this char = utf16 . char at ( f text @$ pos ) ; } while ( fcm . contains ( this char ) ) ; } continue ; },match has,fail,pre
start a new session and attempt to access jenkins @$ which should <PLACE_HOLDER> auto login failures,wc = j . create web client ( ) ; wc . get cookie manager ( ) . add cookie ( c ) ;,which avoid,fail,pre
the same . this tests that the resource processor <PLACE_HOLDER> the r classes correctly to avoid collisions between the two identical layout values .,java file object model = java file objects . for resource ( __str__ ) ; java file object model with differentr class = java file objects . for resource ( __str__ ) ; java file object generated model = java file objects . for resource ( __str__ ) ; java file object generated model with differentr class = java file objects . for resource ( __str__ ) ; assert_ ( ) . about ( java sources ( ) ) . that ( arrays . as list ( model @$ model with differentr class @$ r @$ r_from_different_package_with_same_value ) ) . processed with ( new epoxy processor ( ) ) . compiles without error ( ) . and ( ) . generates sources ( generated model @$ generated model,processor represents,fail,pre
if this item <PLACE_HOLDER> a uri value @$ try using that .,uri uri = get uri ( ) ; if ( uri != null ) { final content resolver resolver = context . get content resolver ( ) ; asset file descriptor descr = null ; file input stream stream = null ; input stream reader reader = null ; try { try { descr = resolver . open typed asset file descriptor ( uri @$ __str__ @$ null ) ; } catch ( security exception e ) { log . w ( __str__ @$ __str__ @$ e ) ; } catch ( file not found exception | runtime exception e ) { } if ( descr != null ) { try { stream = descr . create input stream ( ) ; reader = new input stream reader,item has,success,pre
we need to mark this basic block as changed so that this monitorexit will be visited again . we need to do this to ensure that we have accounted for the possibility that this bytecode will <PLACE_HOLDER> an exception .,basic block bb = get basic block containing ( bci ) ; bb . set changed ( true ) ; bb . _monitor_top = bad_monitors ; if ( trace monitor mismatch ) { report monitor mismatch ( __str__ ) ; },bytecode throw,success,pre
valve <PLACE_HOLDER> the context to log messages,when ( delta session manager . get the context ( ) ) . then return ( mock ( context . class ) ) ; when ( delta session manager . get the context ( ) . get logger ( ) ) . then return ( mock ( log . class ) ) ;,valve uses,fail,pre
setting heartbeat interval to 1 hour to prevent bp service actor <PLACE_HOLDER> heartbeat periodically to nn during running test case @$ and bp service actor only <PLACE_HOLDER> heartbeat once after startup,conf . set time duration ( dfs_heartbeat_interval_key @$ __num__ @$ time unit . hours ) ; minidfs cluster cluster = new minidfs cluster . builder ( conf ) . build ( ) ; cluster . wait active ( ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; metrics record builder rb = get metrics ( dn . get metrics ( ) . name ( ) ) ; assert counter ( __str__ @$ __num__ @$ rb ) ;,actor sends,success,pre
make sure the ping <PLACE_HOLDER> more than 1 s,thread . sleep ( __num__ ) ; assert false ( volt . handler . got ping ) ;,ping takes,success,pre
get the targets from the main app class . only one will be used @$ depending on whether the user is <PLACE_HOLDER> old rows by date or by row count .,try { timestamp type date target = app . get target date ( ) ; long row target = app . get target rows per partition ( ) ; client response with partition key [ ] responses ; if ( app . config . historyseconds > __num__ ) { responses = app . client . call all partition procedure ( __str__ @$ date target @$ app . config . deletechunksize ) ; } else { responses = app . client . call all partition procedure ( __str__ @$ row target @$ app . config . deletechunksize ) ; } app . update partition count ( responses . length ) ; for ( client response with partition key resp : responses ) { if ( resp . response .,user setting,fail,pre
the amount with which to <PLACE_HOLDER> the user provided content padding to account for stroke and shape corners .,int content padding offset = ( int ) ( ( include corner padding ? calculate actual corner padding ( ) : __num__ ) - get parent card view calculated corner padding ( ) ) ; material card view . set ancestor content padding ( user content padding . left + content padding offset @$ user content padding . top + content padding offset @$ user content padding . right + content padding offset @$ user content padding . bottom + content padding offset ) ;,which change,fail,pre
let the caller <PLACE_HOLDER> sense of it @$ then .,type = type . t package ; return vset ;,caller make,success,pre
check for test <PLACE_HOLDER> instances . it should be 0,vm1 . invoke ( new cache serializable runnable ( __str__ ) { @ override public void run2 ( ) throws cache exception { assert equals ( __num__ @$ test object . num instance ) ; } } ) ; this . close client ( vm2 ) ; this . close client ( vm3 ) ; this . close client ( vm1 ) ; this . close client ( vm0 ) ;,test object,success,pre
make the user agent tester <PLACE_HOLDER> its states and make sure we do n't get notifications as we 're now unsubscribed .,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = get sample status1 ( ) ;,tester change,success,pre
close the socket @$ which <PLACE_HOLDER> connection if it has created any .,if ( socket != null ) { try { socket . close ( ) ; } catch ( exception e ) { } },which closes,fail,pre
generate entities that have multiple errors the string a field <PLACE_HOLDER> a value over the length limitation and miss string b fields,if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__ ; i ++ ) { validation demo . union field with inline record union = new validation demo . union field with inline record ( ) ; union . set my enum ( my enum . foofoo ) ; validation demos . add ( new validation demo ( ) . set stringa ( __str__ ) . set inta ( current criteria . get inta ( ) ) . set union field with inline record ( union ) ) ; } } else if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__,string has,success,pre
init assembler . each plan assembler <PLACE_HOLDER> a new instance of the plan selector to keep track of the best plan,plan assembler assembler = new plan assembler ( m_db @$ m_partitioning @$ ( plan selector ) m_plan selector . clone ( ) @$ m_is large query ) ;,assembler needs,fail,pre
if we have throttle threads @$ make sure the user also <PLACE_HOLDER> size,preconditions . check argument ( large threads > __num__ && small threads > __num__ ) ; final string n = thread . current thread ( ) . get name ( ) ; steal job queue < runnable > steal job queue = new steal job queue < runnable > ( comparator ) ; this . long compactions = new thread pool executor ( large threads @$ large threads @$ __num__ @$ time unit . seconds @$ steal job queue @$ new thread factory builder ( ) . set name format ( n + __str__ ) . set daemon ( true ) . build ( ) ) ; this . long compactions . set rejected execution handler ( new rejection ( ) ) ; this . long compactions .,user sets,fail,pre
validate the <PLACE_HOLDER> clause if any,validate having clause ( root node ) ; broker request broker request = new broker request ( ) ; root node . update broker request ( broker request ) ; if ( enable_pinot_query ) { try { pinot query pinot query = new pinot query ( ) ; root node . update pinot query ( pinot query ) ; if ( validate_converter ) { pinot query2 broker request converter converter = new pinot query2 broker request converter ( ) ; broker request temp broker request = converter . convert ( pinot query ) ; boolean result = broker request comparison utils . validate ( broker request @$ temp broker request ) ; if ( ! result ) { logger . error ( __str__ @$ expression ) ; if,the has,fail,pre
orc does n't currently <PLACE_HOLDER> timestamplocaltz,case timestamp : timestamp ts = data type utils . to timestamp ( field value @$ ( ) -> data type utils . get date format ( field data type . get format ( ) ) @$ field name ) ;,orc support,fail,pre
this is called only if the peer <PLACE_HOLDER> junior status,this . seeddb . add potential ( peer ) ;,peer has,success,pre
let 's <PLACE_HOLDER> an identical permission to the child @$ but it 'll appear after the current permission @$ so has no impact,child . insert ace ( __num__ @$ base permission . delete @$ new principal sid ( auth ) @$ true ) ;,'s add,success,pre
this can contain user code . wrap it in case it <PLACE_HOLDER> an exception .,try { invoker . invoke start bundle ( new do fn start bundle context ( ) ) ; } catch ( throwable t ) { throw wrap user code exception ( t ) ; },code throws,success,pre
create the mapping from index source <PLACE_HOLDER> up symbol to probe key input,immutable set multimap . builder < symbol @$ integer > builder = immutable set multimap . builder ( ) ; for ( map . entry < symbol @$ symbol > entry : index key trace . entry set ( ) ) { symbol index join symbol = entry . get key ( ) ; symbol index lookup symbol = entry . get value ( ) ; builder . put all ( index lookup symbol @$ index to probe key input . get ( index join symbol ) ) ; } return builder . build ( ) ;,mapping look,success,pre
null indicates a static method which may still <PLACE_HOLDER> generics correction,generics type [ ] generics types = helper method . get generics types ( ) ; if ( generics types != null ) { map < string @$ class node > method spec = generics utils . add method generics ( helper method @$ collections . empty map ( ) ) ; generics type [ ] new gt = generics utils . apply generics context to place holders ( method spec @$ helper method . get generics types ( ) ) ; forwarder . set generics types ( new gt ) ; },which need,success,pre
the remaining <PLACE_HOLDER> reference rows that do n't exist in db . they must be deleted from index .,remaining . values ( ) . for each ( item -> bulk indexer . add deletion ( type_active_rule @$ item . get doc id ( ) @$ item . get doc routing ( ) ) ) ; return bulk indexer . stop ( ) ;,the uuids,fail,pre
set the new mark so that next time we <PLACE_HOLDER> new data since this point .,if ( scan time since mark ms > __num__ ) { u . m bluetooth scan timer . set mark ( elapsed realtime ms ) ; long scan time rx since mark ms = scan time since mark ms ; long scan time tx since mark ms = scan time since mark ms ; if ( normalize scan rx time ) { scan time rx since mark ms = ( rx time ms * scan time rx since mark ms ) / total scan time ms ; } if ( normalize scan tx time ) { scan time tx since mark ms = ( tx time ms * scan time tx since mark ms ) / total scan time ms ; } final controller activity counter impl counter,time get,success,pre
on t vs @$ if the app does n't <PLACE_HOLDER> search @$ we want to launch assist .,if ( ! result && ( get context ( ) . get resources ( ) . get configuration ( ) . ui mode & configuration . ui_mode_type_mask ) == configuration . ui_mode_type_television ) { bundle args = new bundle ( ) ; args . put int ( intent . extra_assist_input_device_id @$ event . get device id ( ) ) ; return ( ( search manager ) get context ( ) . get system service ( context . search_service ) ) . launch legacy assist ( null @$ get context ( ) . get user id ( ) @$ args ) ; },app support,fail,pre
result of this conversion will <PLACE_HOLDER> the original length and cause a newline to be inserted,string data [ ] = { __str__ @$ __str__ @$ __str__ @$ __str__ } ;,result change,fail,pre
original capacity <PLACE_HOLDER> 20 chars per headers,string builder sb = new string builder ( simple name . length ( ) + __num__ + size * __num__ ) . append ( simple name ) . append ( __str__ ) ; while ( headers it . has next ( ) ) { entry < ? @$ ? > header = headers it . next ( ) ; sb . append ( header . get key ( ) ) . append ( __str__ ) . append ( header . get value ( ) ) . append ( __str__ ) ; } sb . set length ( sb . length ( ) - __num__ ) ; return sb . append ( __str__ ) . to string ( ) ;,capacity has,fail,pre
if the existing monitor has already been cancelled @$ then do not <PLACE_HOLDER> the state,if ( delegate . is cancelled ( ) ) { new delegate . cancel ( ) ; return ; } for ( cancelled listener l : listeners ) { new delegate . add cancelled listener ( l ) ; delegate . remove cancelled listener ( l ) ; } new delegate . set maximum ( delegate . get maximum ( ) ) ; new delegate . set progress ( delegate . get progress ( ) ) ; new delegate . set message ( delegate . get message ( ) ) ; new delegate . set indeterminate ( delegate . is indeterminate ( ) ) ; new delegate . set cancel enabled ( delegate . is cancel enabled ( ) ) ; this . delegate = new delegate ;,then reset,fail,pre
make sure the d ns do n't <PLACE_HOLDER> a heartbeat for a while @$ so the blocks wo n't actually get completed during lease recovery .,for ( data node dn : cluster . get data nodes ( ) ) { data node test utils . set heartbeats disabled for tests ( dn @$ true ) ; },ns receive,fail,pre
create directories one by one with explicit permissions to ensure no umask is applied @$ using mkdirs will <PLACE_HOLDER> the permission only to the last directory,stack < path > dirs to make = new stack < > ( ) ; dirs to make . push ( hdfs path ) ; path parent = hdfs path . get parent ( ) ; while ( ! hdfs . exists ( parent ) ) { dirs to make . push ( parent ) ; parent = parent . get parent ( ) ; } while ( ! dirs to make . empty ( ) ) { path dir to make = dirs to make . pop ( ) ; if ( ! file system . mkdirs ( hdfs @$ dir to make @$ new fs permission ( options . get mode ( ) . to short ( ) ) ) ) { return false ; },mkdirs write,fail,pre
the stdouterr file <PLACE_HOLDER> all the system.out and system.err writes to disk .,string stdouterrfile = util . extract string option ( __str__ @$ settings ) ;,file wrote,fail,pre
notify the listeners . do that from the end of the list so that if a listener <PLACE_HOLDER> itself as the result of being called @$ it wo n't mess up with our iteration,if ( m listeners != null ) { int listener count = m listeners . size ( ) ; for ( int i = listener count - __num__ ; i >= __num__ ; i -- ) { m listeners . get ( i ) . on drawer closed ( drawer view ) ; } },listener removes,success,pre
stomp out any bad characters since this is from a circular buffer a corruption is seen sometimes that results in the vm crashing this should <PLACE_HOLDER> crashes and the line will probably fail to parse,for ( int j = start index ; j < end index ; j ++ ) { if ( ( wl buffer [ j ] & __num__ ) != __num__ ) wl buffer [ j ] = ( byte ) __str__ ; } boolean parsed = process . parse proc line ( wl buffer @$ start index @$ end index @$ wakeup_sources ? wakeup_sources_format : proc_wakelocks_format @$ name string array @$ wl data @$ null ) ; name = name string array [ __num__ ] . trim ( ) ; count = ( int ) wl data [ __num__ ] ; if ( wakeup_sources ) { total time = wl data [ __num__ ] * __num__ ; } else { total time = ( wl data [ __num__,results cause,fail,pre
a fourth rebalance should not <PLACE_HOLDER> assignments,apply assignments ( returned assignments ) ; member configs = member configs ( leader @$ offset @$ assignments ) ; assignor . perform task assignment ( leader @$ offset @$ member configs @$ coordinator @$ protocol version ) ; ++ rebalance num ; returned assignments = assignments capture . get value ( ) ; assert delay ( __num__ @$ returned assignments ) ; expected member configs = member configs ( leader @$ offset @$ returned assignments ) ; assert no reassignments ( member configs @$ expected member configs ) ; assert assignment ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __str__ ) ; verify ( coordinator @$ times ( rebalance num ) ) . config snapshot ( ) ; verify ( coordinator @$ times (,rebalance change,success,pre
button text can <PLACE_HOLDER> a different color,if ( m ok color == null ) m ok color = m accent color ; m ok button . set text color ( m ok color ) ; if ( m cancel color == null ) m cancel color = m accent color ; m cancel button . set text color ( m cancel color ) ; if ( get dialog ( ) == null ) { view . find view by id ( r . id . mdtp_done_background ) . set visibility ( view . gone ) ; } int circle background = context compat . get color ( context @$ r . color . mdtp_circle_background ) ; int background color = context compat . get color ( context @$ r . color . mdtp_background_color ),text have,success,pre
now <PLACE_HOLDER> splitting and it should work .,admin . split region async ( hri . get region name ( ) ) . get ( __num__ @$ time unit . minutes ) ;,now do,fail,pre
find the next index @$ in case the assumed next index is not unique . for instance @$ if there is no p @$ then request for p 's position actually <PLACE_HOLDER> q 's . so we need to look ahead to make sure that there is really a q at q 's position . if not @$ move further down ...,int next next section = next section + __num__ ; while ( next next section < section count && m section indexer . get position for section ( next next section ) == next index ) { next next section ++ ; next section ++ ; },request matches,fail,pre
should use <PLACE_HOLDER> entity transformer by default,criteria executor criteria executor = new criteria executor ( ) { protected criteria get criteria ( session s ) { return s . create criteria ( enrolment . class @$ __str__ ) . create alias ( __str__ @$ __str__ @$ criteria . left_join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set projection ( projections . projection list ( ) . add ( projections . property ( __str__ ) ) . add ( projections . property ( __str__ ) ) ) . add order ( order . asc ( __str__ ) ) ; } } ;,use root,success,pre
completing the user event listener should <PLACE_HOLDER> the case,user event listener instance user event listener instance = cmmn runtime service . create user event listener instance query ( ) . case instance id ( case instance . get id ( ) ) . single result ( ) ; assert that ( user event listener instance . get id ( ) ) . is equal to ( user event listener plan item instance . get id ( ) ) ; cmmn runtime service . complete user event listener instance ( user event listener instance . get id ( ) ) ; assert case instance ended ( case instance ) ;,listener terminate,success,pre
we deduced thoughts from the inferences in the intents . the keynote also <PLACE_HOLDER> these actions,this . get actions clone ( ) . for each ( action -> keynote . add action ( action ) ) ;,keynote visits,fail,pre
out stream <PLACE_HOLDER> problems .,throw new illegal state exception ( ioex ) ;,stream has,success,pre
we can not determine which method is the most <PLACE_HOLDER> because one parameter of the first candidate was more <PLACE_HOLDER> and another parameter of the second candidate was more <PLACE_HOLDER> .,if ( best match != null && ! potential match . equals ( best match ) ) { return null ; } else { best match = potential match ; },parameter specific,success,pre
default to number class in exception details @$ else <PLACE_HOLDER> the specified number subtype .,return cast to number ( object @$ number . class ) ;,default accept,fail,pre
sql <PLACE_HOLDER> type list @$,finish initialization ( query return type list ) ;,sql return,fail,pre
use configuration that is same as what the transition will <PLACE_HOLDER> .,use configuration ( immutable map . of ( __str__ @$ __str__ ) ) ; configured target test = get configured target ( __str__ ) ; @ suppress warnings ( __str__ ) configured target dep = iterables . get only element ( ( list < configured target > ) get my info from target ( test ) . get value ( __str__ ) ) ; assert that ( get core options ( test ) . transition directory name fragment ) . is null ( ) ; assert that ( get core options ( dep ) . transition directory name fragment ) . is not null ( ) ;,transition write,fail,pre
seasonal does n't <PLACE_HOLDER> boss hiscores,if ( index < skills . size ( ) ) { hiscore result . set abyssal sire ( skills . get ( index ++ ) ) ; hiscore result . set alchemical hydra ( skills . get ( index ++ ) ) ; hiscore result . set barrows chests ( skills . get ( index ++ ) ) ; hiscore result . set bryophyta ( skills . get ( index ++ ) ) ; hiscore result . set chambers of xeric ( skills . get ( index ++ ) ) ; hiscore result . set chambers of xeric challenge mode ( skills . get ( index ++ ) ) ; hiscore result . set chaos elemental ( skills . get ( index ++ ) ) ; hiscore result,seasonal have,success,pre
<PLACE_HOLDER>back <PLACE_HOLDER> sound button .,if ( feature notify and playback devices ) { cnstrnts . gridy = __num__ ; playback play sound button = new j button ( new image icon ( neomedia activator . get resources ( ) . get image in bytes ( __str__ ) ) ) ; playback play sound button . set minimum size ( new dimension ( __num__ @$ __num__ ) ) ; playback play sound button . set preferred size ( new dimension ( __num__ @$ __num__ ) ) ; if ( ( ( device configuration combo box model . capture device ) playback combo . get selected item ( ) ) . info == null ) { playback play sound button . set enabled ( false ) ; } playback play sound button . set,playback play,success,pre
no security exception @$ <PLACE_HOLDER> the grant .,int source user id = content provider . get user id from uri ( uri @$ m user ) ; uri = content provider . get uri without user id ( uri ) ; uri grants manager . get service ( ) . grant uri permission from owner ( m permission owner @$ src uid @$ dest pkg @$ uri @$ flag_grant_read_uri_permission @$ source user id @$ m user ) ; binder . restore calling identity ( ident ) ;,exception grant,fail,pre
moving a day the notification q <PLACE_HOLDER> the commit invoice . now payment is expected,bus handler . push expected events ( next event . invoice @$ next event . payment @$ next event . invoice_payment ) ; clock . add days ( __num__ ) ; assert listener status ( ) ; parent invoice = invoice user api . get invoice ( parent invoice . get id ( ) @$ call context ) ; assert equals ( parent invoice . get status ( ) @$ invoice status . committed ) ; assert equals ( parent invoice . get balance ( ) . compare to ( big decimal . zero ) @$ __num__ ) ;,q calls,success,pre
simulate a 2 nn <PLACE_HOLDER> a checkpoint @$ but not finishing . this will cause name 1 to be restored .,cluster . get name node rpc ( ) . roll edit log ( ) ; print storages ( fs image ) ;,nn committing,fail,pre
xml parser only <PLACE_HOLDER> string types,return get field value ( ) ;,parser supports,fail,pre
tests whether the width of the picture changes trough all qualities and every step <PLACE_HOLDER> a few pictures .,try { int n = __num__ ; int m = __num__ ; int q = __num__ ; int [ ] qualities = { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ; while ( ! end requested ) { frame source = fg . grab frame ( ) ; n ++ ; m ++ ; if ( source . image width != qualities [ q ] ) { q ++ ; assert equals ( source . image width @$ qualities [ q ] ) ; assert true ( m > __num__ ) ; assert true ( m <= __num__ ) ; m = __num__ ; } fr . record ( source ) ; } assert equals ( q @$ qualities,tests processes,fail,pre
update controllers <PLACE_HOLDER> information,environment . getv rinput ( ) . update controller states ( ) ;,controllers region,fail,pre
the region state node may <PLACE_HOLDER> no procedure in a test scenario ; allow for this .,update user region location ( region state node . get region info ( ) @$ region state node . get state ( ) @$ region state node . get region location ( ) @$ open seq num @$ region state node . get procedure ( ) != null ? region state node . get procedure ( ) . get proc id ( ) : procedure . no_proc_id ) ;,node have,success,pre
parsing should go all the way to the end of the string . we want the longest match @$ and we do n't care if the plural form of the unit <PLACE_HOLDER> the plural form of the number .,assert equals ( __str__ @$ parse string . length ( ) @$ ppos . get index ( ) ) ;,form matches,success,pre
closing the entity manager factory should <PLACE_HOLDER> the em,em . get entity manager factory ( ) . close ( ) ;,factory close,success,pre
no fragment may be present on a toolchain rule in retroactive <PLACE_HOLDER> mode . this is because <PLACE_HOLDER> expects that platform and toolchain resolution uses only the platform configuration . in theory @$ this means toolchains could use platforms @$ but the current expectation is that toolchains should not use anything at all @$ so better to go with the stricter expectation for now,if ( configuration . trim configurations retroactively ( ) && ! target . get configuration key ( ) . get fragments ( ) . is empty ( ) ) { string extra fragment description = target . get configuration key ( ) . get fragments ( ) . stream ( ) . map ( cl -> cl . get simple name ( ) ) . collect ( joining ( __str__ ) ) ; throw new registered toolchains function exception ( new invalid toolchain label exception ( toolchain label @$ __str__ + __str__ + __str__ + extra fragment description + __str__ ) @$ transience . persistent ) ; },configuration build,fail,pre
the data will <PLACE_HOLDER> corrected samples :,assert equals ( __num__ @$ histogram . get count at value ( __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get total count ( ) ) ;,data include,success,pre
the copying thread wo n't generally <PLACE_HOLDER> eof,swin . interrupt ( ) ;,thread get,fail,pre
no tables <PLACE_HOLDER> an empty catalog,try ( connection connection = create connection ( ) ) { try ( result set rs = connection . get meta data ( ) . get tables ( __str__ @$ null @$ null @$ null ) ) { assert table metadata ( rs ) ; assert equals ( read rows ( rs ) . size ( ) @$ __num__ ) ; } } try ( connection connection = create connection ( ) ) { try ( result set rs = connection . get meta data ( ) . get tables ( test_catalog @$ __str__ @$ null @$ null ) ) { assert table metadata ( rs ) ; set < list < object > > rows = immutable set . copy of ( read rows ( rs ),tables has,fail,pre
native methods may <PLACE_HOLDER> null filenames,if ( e . get file name ( ) != null ) { assert that ( json ) . contains ( __str__ + e . get file name ( ) + __str__ ) ; },methods return,fail,pre
note that view.on <PLACE_HOLDER> accessibility node info was introduced in ics and we would like to tweak a bit the text that is reported to accessibility services via the accessibility node info .,info . set text ( get context ( ) . get string ( r . string . accessibility_delegate_custom_text_added ) ) ;,view.on add,fail,pre
can only apply if both sides <PLACE_HOLDER> functions .,return ;,sides have,success,pre
action has no configuration @$ we just <PLACE_HOLDER> this method for the future,if ( config != null ) { },configuration override,fail,pre
make sure that an app with coarse permissions ca n't <PLACE_HOLDER> frequent location updates by calling location manager.<PLACE_HOLDER> last known location repeatedly .,if ( allowed resolution level < resolution_level_fine ) { location = m last location coarse interval . get ( name ) ; } else { location = m last location . get ( name ) ; },app get,success,pre
just use the first record in the batch here to satisfy the record cursor . the truth is that we 'll be using the read method which <PLACE_HOLDER> an external record anyway so it does n't matter .,try ( page cursor cursor = store . open page cursor for reading ( id ) ) { boolean has next = true ; while ( has next ) { if ( assembler . append ( store @$ cursor @$ batch @$ id @$ i ) ) { i ++ ; } if ( has next = id range . has next ( ) ) { id = id range . next ( ) ; } } } sender . send ( assembler . cut off at ( batch @$ i ) ) ;,which returns,fail,pre
always consume the drag so that java does not <PLACE_HOLDER> the selection,e . consume ( ) ;,java change,success,pre
otherwise register a dummy provider which would <PLACE_HOLDER> the issue of bug 8139436,boolean provider prepended = false ; string testprovider = system . get property ( __str__ ) ; if ( testprovider != null && ! testprovider . is empty ( ) ) { try { system . out . println ( __str__ + testprovider ) ; class < ? > providerclass = class . for name ( testprovider ) ; object provider = providerclass . new instance ( ) ; security . insert provider at ( ( provider ) provider @$ __num__ ) ; } catch ( exception e ) { system . out . println ( __str__ + testprovider + __str__ ) ; e . print stack trace ( system . out ) ; } provider prepended = true ; system . out . println ( __str__ +,which fail,fail,pre
we catch throwable here because netty <PLACE_HOLDER> clever tricks to have method signatures that look like they do not throw checked exceptions @$ but they actually do . the compiler wo n't let us catch them explicitly because in theory they should n't be possible @$ so we have to catch throwable and do our own checks to grab them,throw new port bind exception ( initializer . address ( ) @$ e ) ;,netty has,fail,pre
system apps <PLACE_HOLDER> control over where their default storage context is pointed @$ so we 're always explicit when building paths .,try { final context ce context = create credential protected storage context ( ) ; root dir = ce context . get data dir ( ) . get canonical path ( ) ; files dir = ce context . get files dir ( ) . get canonical path ( ) ; nb files dir = ce context . get no backup files dir ( ) . get canonical path ( ) ; db dir = ce context . get database path ( __str__ ) . get parent file ( ) . get canonical path ( ) ; sp dir = ce context . get shared preferences path ( __str__ ) . get parent file ( ) . get canonical path ( ) ; cache dir = ce context,apps have,success,pre
check if the crypto permissions granted to the app contain a crypto permission for the requested algorithm that does not require any exemption mechanism to be enforced . <PLACE_HOLDER> that permission @$ if present .,permission collection app pc = app perms . get permission collection ( alg ) ; if ( app pc == null ) { return default perm ; } enumeration < permission > enum_ = app pc . elements ( ) ; while ( enum_ . has more elements ( ) ) { crypto permission cp = ( crypto permission ) enum_ . next element ( ) ; if ( cp . get exemption mechanism ( ) == null ) { return cp ; } },check use,fail,pre
generate the strongest key using the shared secret assuming the key sizes in aes constants class are in <PLACE_HOLDER> order,while ( skey == null && idx >= __num__ ) { if ( keysize >= aes constants . aes_keysizes [ idx ] ) { keysize = aes constants . aes_keysizes [ idx ] ; skey = new secret key spec ( secret @$ __num__ @$ keysize @$ __str__ ) ; } idx -- ; },class ascending,success,pre
all surrogate pairs with this lead surrogate <PLACE_HOLDER> only irrelevant data,if ( ( norm32 & mask ) == __num__ ) { return __num__ ; } else { return get norm32 from surrogate pair ( norm32 @$ args . c ) ; },pairs have,success,pre
make sure that the pattern <PLACE_HOLDER> a subtraction by one .,if ( ! b . is java constant ( ) ) { return null ; } java constant b cst = b . as java constant ( ) ; long b value ; if ( b cst . get java kind ( ) == java kind . int ) { b value = b cst . as int ( ) ; } else if ( b cst . get java kind ( ) == java kind . long ) { b value = b cst . as long ( ) ; } else { return null ; } if ( b value == - __num__ ) { return builder -> get arithmeticlir generator ( ) . emit get mask up to lowest set bit ( operand ( a,pattern matches,success,pre
otherwise can only <PLACE_HOLDER> if enough room .,return true ;,otherwise work,fail,pre
create calendar which <PLACE_HOLDER> time,for ( action . handler action handler : action handlers ) { gregorian calendar cal = new gregorian calendar ( get time zone ( ) @$ get locale ( ) ) ; cal . clear ( ) ; cal . set ( current calendar . get ( java . util . calendar . year ) @$ current calendar . get ( java . util . calendar . month ) @$ current calendar . get ( java . util . calendar . date ) ) ; date start = cal . get time ( ) ; cal . add ( java . util . calendar . date @$ __num__ ) ; cal . add ( java . util . calendar . second @$ - __num__ ) ; date end,which contains,fail,pre
assume root always <PLACE_HOLDER> a parent,if ( is root ( path ) ) { return true ; } string parent key = get parent path ( path ) ; return parent key != null && is directory ( parent key ) ;,root has,success,pre
proprietary class format @$ can not <PLACE_HOLDER> or check logical channel for now @$ just return,if ( cla < __num__ ) { return ; },format adopt,fail,pre
check each new char to make sure it matches what the group <PLACE_HOLDER> matched last time around,int x = i ; for ( int index = __num__ ; index < group size ; index ++ ) { int c1 = character . code point at ( seq @$ x ) ; int c2 = character . code point at ( seq @$ j ) ; if ( c1 != c2 ) { if ( do unicode case ) { int cc1 = character . to upper case ( c1 ) ; int cc2 = character . to upper case ( c2 ) ; if ( cc1 != cc2 && character . to lower case ( cc1 ) != character . to lower case ( cc2 ) ) return false ; } else { if ( ascii . to lower ( c1 ) != ascii,group referenced,success,pre
since some records do n't <PLACE_HOLDER> all possible attributes @$ we initialize average coordinates equal to current centroid coordinates,map < string @$ double > average = centroid . get coordinates ( ) ;,records have,success,pre
wild card <PLACE_HOLDER> 6 single broadcast <PLACE_HOLDER> 6 address : fe 80 : xx : xx ... loopback <PLACE_HOLDER> 6 address site local <PLACE_HOLDER> 6 address : fec 0 : xx : xx ...,if ( inet addr . is any local address ( ) || inet addr . is link local address ( ) || inet addr . is loopback address ( ) || inet addr . is site local address ( ) ) { return true ; },card pv,fail,pre
count keys as equal if they <PLACE_HOLDER> the same key values .,return o instanceof g infopc && arrays . equals ( _gs @$ ( ( g infopc ) o ) . _gs ) ;,keys have,success,pre
renewing token and adding it to timer calls are separated purposefully if user <PLACE_HOLDER> incorrect token then it should not be added for renewal .,if ( ! token list . is empty ( ) ) { for ( delegation token to renew dtr : token list ) { delegation token to renew current dtr = all tokens . put if absent ( dtr . token @$ dtr ) ; if ( current dtr != null ) { current dtr . referring app ids . add ( application id ) ; app tokens . get ( application id ) . add ( current dtr ) ; } else { app tokens . get ( application id ) . add ( dtr ) ; set timer for token renewal ( dtr ) ; } } },user gave,fail,pre
at this point we are done processing the input . <PLACE_HOLDER> the record processor,if ( ! is closed ) { close record processor ( ) ; is closed = true ; },input close,success,pre
interactive bugreports <PLACE_HOLDER> progress updates .,assert that ( callback . has received progress ( ) ) . is true ( ) ; assert that ( callback . get error code ( ) ) . is equal to ( bugreport callback . bugreport_error_user_consent_timed_out ) ; assert fds are closed ( m bugreport fd ) ;,bugreports receive,fail,pre
if namespace parents are implicitly <PLACE_HOLDER>d @$ they wo n't have ac ls . so @$ let 's explicitly <PLACE_HOLDER> them .,curator framework null ns fw = zk client . using namespace ( null ) ; ensure path ensure ns = null ns fw . new namespace aware ensure path ( __str__ + zk client . get namespace ( ) ) ; try { ensure ns . ensure ( null ns fw . get zookeeper client ( ) ) ; } catch ( exception e ) { throw new io exception ( __str__ @$ e ) ; },let enlist,fail,pre
no rows to <PLACE_HOLDER> but update the spacer indexes,spacer container . update spacer indexes for row and after ( index @$ index + number of rows - added row count @$ number of rows - added row count ) ; double new rows height = number of rows * default row height ; if ( added row count > __num__ ) { move viewport and content ( null @$ __num__ @$ __num__ @$ new rows height ) ; row visibility changed = true ; } else { move viewport and content ( index @$ new rows height @$ new rows height @$ new rows height ) ; },rows add,fail,pre
the offset is just to get the right <PLACE_HOLDER> slots highlighted in the output,if ( interpreter frame method != null ) { int offset = __num__ ; anno panel . add annotation ( new annotation ( cur frame . address of interpreter frame local ( offset ) @$ cur frame . address of interpreter frame local ( ( int ) interpreter frame method . get max locals ( ) + offset ) @$ __str__ + cur frame . getsp ( ) ) ) ; },right stack,success,pre
normalization can not <PLACE_HOLDER> leading uplevel references @$ so this will be true,assert that ( create ( __str__ ) . contains uplevel references ( ) ) . is true ( ) ;,normalization remove,success,pre
the directory entries typically have different permissions to the files @$ e.g . <PLACE_HOLDER> permission or ca n't cd into it,files . walk file tree ( target @$ new simple file visitor < path > ( ) { public file visit result pre visit directory ( path dir @$ basic file attributes attrs ) throws io exception { files . set posix file permissions ( dir @$ dir permissions ) ; return file visit result . continue ; } public file visit result visit file ( path file @$ basic file attributes attrs ) throws io exception { files . set posix file permissions ( file @$ permissions ) ; return file visit result . continue ; } } ) ;,directory add,fail,pre
decrement <PLACE_HOLDER> simultaneous segment builds .,_server metrics . add value to global gauge ( server gauge . llc_simultaneous_segment_builds @$ - __num__ ) ;,decrement llc,success,pre
prints <PLACE_HOLDER> 1 1 1 1 ... .,no inline ( __str__ ) ;,prints 0,success,pre
make sure gl does not <PLACE_HOLDER> our objects before we start computing,if ( syncc ltogl || should init buffers ) { gl finish ( ) ; } if ( should init buffers ) { initgl objects ( ) ; set kernel constants ( ) ; } if ( rebuild ) { build program ( ) ; set kernel constants ( ) ; } computecl ( double precision ) ; rendergl ( ) ;,gl initialize,fail,pre
the log level would actually <PLACE_HOLDER> a warning to be logged .,if ( log warnings ) { try { walk warnings ( statement . get warnings ( ) @$ handler ) ; } catch ( sql exception sql exception ) { log . debug ( __str__ @$ sql exception ) ; } },level cause,fail,pre
assume something generous if we <PLACE_HOLDER> no information,m interarrival time = __num__ ;,something have,success,pre
load the texture again . but this time @$ <PLACE_HOLDER> the gwt implementation of pixmap to move to a canvas representation of the image,pixmap pixmap = new pixmap ( gdx . files . internal ( __str__ ) ) ; pixmap . get pixel ( __num__ @$ __num__ ) ; file texture data data1 = new file texture data ( null @$ pixmap @$ null @$ false ) ; bad texture = new texture ( data1 ) ;,texture use,fail,pre
add nodes to cluster @$ so cluster <PLACE_HOLDER> 20 gb and 20 vcores,resource new resource = resource . new instance ( __num__ * gb @$ __num__ ) ; rm node node = mock nodes . new node info ( __num__ @$ new resource @$ __num__ @$ __str__ ) ; cs . handle ( new node added scheduler event ( node ) ) ; resource new resource2 = resource . new instance ( __num__ * gb @$ __num__ ) ; rm node node2 = mock nodes . new node info ( __num__ @$ new resource2 @$ __num__ @$ __str__ ) ; cs . handle ( new node added scheduler event ( node2 ) ) ; fi ca scheduler app fi ca app1 = cs . get scheduler applications ( ) . get ( app . get application id ( ) ),cluster has,fail,pre
chromium 's info bar container may <PLACE_HOLDER> an info bar immediately during this initialization call @$ so make sure everything in the info bar container is completely ready beforehand .,m native info bar container = native init ( ) ;,container add,success,pre
items to <PLACE_HOLDER> messages,item = new privacy item ( privacy item . type . group . name ( ) @$ false @$ i ) ; item . set value ( group name ) ; item . set filter message ( true ) ; original privacy items [ i ] = item ; i = i + __num__ ;,items filter,fail,pre
user 4 <PLACE_HOLDER> another global permission via a group,group dto administrator group3 = db . users ( ) . insert group ( organization2 ) ; db . users ( ) . insert permission on group ( administrator group3 @$ quality_profile_admin ) ; user dto user4 = db . users ( ) . insert user ( with email ( __str__ ) ) ; db . users ( ) . insert member ( administrator group3 @$ user4 ) ; component dto project = db . components ( ) . insert private project ( ) ;,user has,success,pre
hive does not <PLACE_HOLDER> result set meta data on prepared statement @$ and hive describe does not <PLACE_HOLDER> queries @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,hive support,success,pre
check if method <PLACE_HOLDER> calls that need profiling,if ( graph . get nodes ( ) . filter ( invoke node . class ) . filter ( ( n ) -> ( ( invoke node ) n ) . get invoke kind ( ) . is indirect ( ) ) . count ( ) > options . simple method indirect calls . get default value ( ) ) { return false ; } return true ;,method contains,fail,pre
let 's <PLACE_HOLDER> the link text as default title when no mapping is defined .,if ( link element . has text ( ) && ! this . html mapping . contains key ( __str__ ) ) { doc . set field ( collection schema . title . get solr field name ( ) @$ link element . text ( ) ) ; },let use,success,pre
check <PLACE_HOLDER> audio control just in case !,if ( ! has audio control ( ) ) { if ( my debug . log ) log . e ( tag @$ __str__ ) ; return ; } this . close popup ( ) ; shared preferences shared preferences = get default shared preferences ( this ) ; string audio_control = shared preferences . get string ( preference keys . get audio control preference key ( ) @$ __str__ ) ; if ( audio_control . equals ( __str__ ) && speech recognizer != null ) { if ( speech recognizer is started ) { speech recognizer . stop listening ( ) ; speech recognizer stopped ( ) ; } else { preview . show toast ( audio_control_toast @$ r . string . speech_recognizer_started ) ; intent intent,check has,success,pre
append will fail when the file size <PLACE_HOLDER> the checksum chunk boundary @$ if append was called with a stale file stat .,do small appends ( file @$ fs @$ __num__ ) ;,size exceeds,fail,pre
return the snapshot from the update request as another thread may have already <PLACE_HOLDER> the request,return my update request . get finished index snapshot ( ) ;,thread started,fail,pre
noop as lzma output stream <PLACE_HOLDER> an exception in flush,return new flush shield filter output stream ( new lzma output stream ( out @$ get options ( opts ) @$ false ) ) ;,stream throws,success,pre
the class cast exception might <PLACE_HOLDER> outside of the completion stage which might be confusing,return apply ( publisher @$ ( class < r > ) type ) . then apply ( type :: cast ) ;,exception occur,success,pre
first @$ with debug @$ which should not <PLACE_HOLDER> index writer output :,try { parsed document doc = test parsed document ( __str__ @$ null @$ test document with text field ( ) @$ b_1 @$ null ) ; engine . index ( index for doc ( doc ) ) ; engine . flush ( ) ; assert false ( mock appender . saw index writer message ) ; loggers . set level ( root logger @$ level . trace ) ; engine . index ( index for doc ( doc ) ) ; engine . flush ( ) ; assert true ( mock appender . saw index writer message ) ; } finally { loggers . remove appender ( root logger @$ mock appender ) ; mock appender . stop ( ) ; loggers . set level ( root,which log,success,pre
if the process wrapper is enabled @$ we use its timeout feature @$ which first interrupts the subprocess and only kills it after a grace period so that the subprocess can <PLACE_HOLDER> a stack trace @$ test log or similar @$ which is incredibly helpful for debugging .,if ( use process wrapper ) { process wrapper util . command line builder command line builder = process wrapper util . command line builder ( process wrapper . get path string ( ) @$ spawn . get arguments ( ) ) . set timeout ( context . get timeout ( ) ) . set kill delay ( duration . of seconds ( local execution options . local sigkill grace seconds ) ) ; if ( local execution options . collect local execution statistics ) { statistics path = tmp dir . get relative ( __str__ ) ; command line builder . set statistics path ( statistics path ) ; } args = command line builder . build ( ) ; } else { subprocess builder . set,subprocess contain,fail,pre
if keys do not <PLACE_HOLDER> : plus a new entry,set ( val @$ keys ) ;,keys exist,success,pre
first time just <PLACE_HOLDER> suggestions .,tables info = load hdfs region infos ( ) ;,time give,fail,pre
scanning the entire table should <PLACE_HOLDER> us three rows,meta table accessor . scan meta for table regions ( connection @$ visitor @$ table name ) ; verify ( visitor @$ times ( __num__ ) ) . visit ( ( result ) any object ( ) ) ;,table give,success,pre
do n't cache these @$ bumps do n't <PLACE_HOLDER> updated otherwise,return new vertical slider thumb icon ( ) ;,bumps get,success,pre
delayed transport has already terminated . terminating the transport terminates the subchannel @$ which in turn <PLACE_HOLDER> the oob channel @$ which terminates the channel .,assert false ( oob1 . is terminated ( ) ) ; verify ( balancer rpc executor pool ) . return object ( balancer rpc executor . get scheduled executor service ( ) ) ; transport info . listener . transport terminated ( ) ; assert true ( oob1 . is terminated ( ) ) ; assert true ( channel . is terminated ( ) ) ; verify ( balancer rpc executor pool @$ times ( __num__ ) ) . return object ( balancer rpc executor . get scheduled executor service ( ) ) ;,which terminates,fail,pre
array precedence <PLACE_HOLDER> 0,if ( array . get precedence ( ) > get precedence ( ) ) { res . enclose ( __str__ @$ __str__ ) ; },precedence has,fail,pre
the file failed to write @$ try 5 times @$ calling gc and sleep between each iteration sometimes windows <PLACE_HOLDER> a while to release a lock on a file,while ( ! are byte arrays equal ( data @$ read bytes ( f ) ) && count < __num__ ) { system . gc ( ) ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { throw new runtime exception ( __str__ ) ; } write bytes ( f @$ data ) ; count ++ ; },windows takes,success,pre
if the user only wants to <PLACE_HOLDER> the selection @$ <PLACE_HOLDER> only the selection,if ( pod . get selection ( ) ) { print selection ( monitor @$ start date @$ job @$ book @$ lm @$ scale amount @$ scaled height ) ; if ( monitor . is cancelled ( ) ) { return ; } } else if ( pod . get visible ( ) ) { print visible content ( monitor @$ start date @$ job @$ book @$ lm @$ scale amount @$ scaled height ) ; if ( monitor . is cancelled ( ) ) { return ; } } else { int page count = get printable page count ( monitor @$ lm @$ scaled height @$ big integer . zero @$ null ) ; print view ( monitor @$ start date @$ job @$,selection show,fail,pre
for nested result maps @$ partial <PLACE_HOLDER> the same as none,sql session factory . get configuration ( ) . set auto mapping behavior ( auto mapping behavior . partial ) ; try ( sql session sql session = sql session factory . open session ( ) ) { mapper mapper = sql session . get mapper ( mapper . class ) ; user user = mapper . get user with pets_ external ( __num__ ) ; assertions . assert equals ( integer . value of ( __num__ ) @$ user . get id ( ) ) ; assertions . assert equals ( __str__ @$ user . get name ( ) ) ; assertions . assert null ( user . get pets ( ) . get ( __num__ ) . get pet name ( ) @$ __str__ ) ;,partial works,success,pre
each entry <PLACE_HOLDER> the value of the row number,b = new array list < > ( arrays . as list ( big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) ) ) ; f = new array list < > ( arrays . as list ( big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) ) ) ; input tableau ( builder ) ; builder . seq ( seq -> { secret tableau . debug info ( seq @$ ps ) ; return null ; } ) ;,entry has,success,pre
because the cluster <PLACE_HOLDER> only 3 nodes @$ and 2 of which are decomm'ed @$ the last block file will remain under replicated .,init exclude hosts ( nodes ) ; refresh nodes ( __num__ ) ;,cluster has,success,pre
the property ca n't <PLACE_HOLDER> any value,return ;,property have,success,pre
worker might <PLACE_HOLDER> properties that the master does n't yet know about @$ e.g . ufs specific properties @$ or properties from a different version of alluxio .,return new property key . builder ( name ) . set is built in ( false ) . build unregistered ( ) ;,worker require,fail,pre
so that everyone <PLACE_HOLDER> the change .,query contact status ( parent provider . get aim connection ( ) . get screenname ( ) . get formatted ( ) ) ;,everyone has,fail,pre
we speculate on the first compilation . the global value numbering will <PLACE_HOLDER> the two floating integer exact operation nodes .,installed code code = get code ( method ) ; code . execute varargs ( x @$ y ) ; if ( ! code . is valid ( ) ) { code = get code ( method ) ; code . execute varargs ( x @$ y ) ; assert true ( code . is valid ( ) ) ; },numbering join,fail,pre
every selectable node in the tree is either of type function tree <PLACE_HOLDER>er node or of type function tree function node . only nodes of the first type <PLACE_HOLDER> assembler code that is displayed .,if ( selected node instanceof function tree block node ) { final function tree block node block node = ( function tree block node ) selected node ; final basic block basic block = block node . get basic block ( ) ; final string builder text = new string builder ( __str__ ) ; for ( final instruction instruction : basic block . get instructions ( ) ) { text . append ( instruction . to string ( ) + __str__ ) ; } m_field . set text ( text . to string ( ) ) ; m_field . set caret position ( __num__ ) ; } else { m_field . set text ( __str__ ) ; },nodes serialize,fail,pre
we are careful here to avoid putting the value id into file metadata if we already have a digest . arbitrary filesystems may <PLACE_HOLDER> weird things with the value id ; a digest is more robust .,return new regular file state value ( stat . get size ( ) @$ digest @$ null ) ;,digest do,success,pre
simply return do n't <PLACE_HOLDER> auth,return ;,return need,fail,pre
since the work <PLACE_HOLDER> the immediate scheduler @$ it is unaffected by the main looper being pa<PLACE_HOLDER> .,assert equals ( __num__ @$ x . get ( ) ) ; observable . just ( __num__ ) . compose ( observe forui ( ) ) . subscribe ( x :: set ) ;,work has,fail,pre
make sure vm 3 can <PLACE_HOLDER> the value,deserialization future = vm3 . invoke async ( ( ) -> { final region r = cache . get region ( region . separator + get test method name ( ) + __str__ ) ; pdx value result = ( pdx value ) r . get ( key_0 ) ; assert equals ( result @$ new pdx value ( __num__ ) ) ; } ) ; try { deserialization future . await ( __num__ @$ time unit . seconds ) ; fail ( __str__ ) ; } catch ( timeout exception e ) { },vm deserialize,success,pre
existing tables and partitions may <PLACE_HOLDER> columns in a different order than the writer is providing @$ so build an index to rearrange columns in the proper order,list < string > file column names = get column names ( schema ) ; list < type > file column types = get column types ( schema ) . stream ( ) . map ( hive type -> hive type . get type ( type manager ) ) . collect ( to list ( ) ) ; int [ ] file input column indexes = file column names . stream ( ) . map to int ( input column names :: index of ) . to array ( ) ; try { file system file system = hdfs environment . get file system ( session . get user ( ) @$ path @$ configuration ) ; output stream output stream = file system . create ( path,tables have,success,pre
change <PLACE_HOLDER> snapshot,final remote animation adapter adapter = new remote animation adapter ( runner @$ __num__ @$ __num__ @$ true ) ;,change needs,success,pre
properties @$ <PLACE_HOLDER> an array of strings representing its groups .,string [ ] test groups = new string [ ] { __str__ @$ __str__ @$ __str__ } ; do test get groups ( arrays . as list ( test groups ) ) ;,properties return,success,pre
if the context <PLACE_HOLDER> the effective deadline @$ we do n't need to schedule an extra task .,if ( effective deadline != null && ! effective deadline . equals ( context . get deadline ( ) ) && deadline cancellation executor != null ) { deadline cancellation future = start deadline timer ( effective deadline ) ; },context has,success,pre
this locator will <PLACE_HOLDER> itself up with the first membership to be created,l = internal locator . start locator ( port @$ new file ( __str__ ) @$ null @$ null @$ local host @$ false @$ new properties ( ) @$ null @$ temporary folder . get root ( ) . to path ( ) ) ;,locator hook,success,pre
but not again after we denied ourselves <PLACE_HOLDER> permission with an acl update,verify denied ( incrementq2 @$ user_other @$ group_user ) ;,ourselves read,fail,pre
update should not <PLACE_HOLDER> new rows !,assert that ( cursor . get count ( ) ) . is equal to ( users for update . size ( ) ) ;,update add,success,pre
check if contact <PLACE_HOLDER> additional phone numbers @$ if yes show the call button,meta contact phone util contact phone util = null ; details response listener details listener = null ;,contact has,success,pre
derive <PLACE_HOLDER> internal memory from explicitly configure task heap memory size and managed memory size,final memory size task heap memory size = get task heap memory size ( config ) ; final memory size managed memory size = get managed memory size ( config ) ; final memory size framework heap memory size = get framework heap memory size ( config ) ; final memory size framework off heap memory size = get framework off heap memory size ( config ) ; final memory size task off heap memory size = get task off heap memory size ( config ) ; final memory size shuffle memory size ; final memory size total flink exclude shuffle memory size = framework heap memory size . add ( framework off heap memory size ) . add ( task heap memory size ) . add (,derive requested,fail,pre
thread which will <PLACE_HOLDER> around indexes .,final query index idx = index ( idx_name_1 @$ field ( field_name_1 ) ) ; ignite internal future idx fut = multithreaded async ( new callable < void > ( ) { @ override public void call ( ) throws exception { boolean exists = false ; while ( ! stopped . get ( ) ) { ignite node = grid ( thread local random . current ( ) . next int ( __num__ @$ __num__ ) ) ; ignite internal future fut ; if ( exists ) { fut = query processor ( node ) . dynamic index drop ( cache_name @$ cache_name @$ idx_name_1 @$ true ) ; exists = false ; } else { fut = query processor ( node ) . dynamic index create,which mess,success,pre
first transition <PLACE_HOLDER> the callback,state . goto state ( connecting ) ; assert equals ( __num__ @$ executor . run due tasks ( ) ) ; assert equals ( __num__ @$ sink . size ( ) ) ; assert equals ( connecting @$ sink . poll ( ) ) ; assert equals ( __num__ @$ executor . num pending tasks ( ) ) ;,transition triggers,success,pre
sequenced event is a package private class @$ which can not be instantiated by importing . so use reflection to <PLACE_HOLDER> an instance .,try { class < ? extends awt event > seq class = ( class < ? extends awt event > ) class . for name ( __str__ ) ; constructor < ? extends awt event > seq const = seq class . get constructor ( awt event . class ) ; seq const . set accessible ( true ) ; return seq const . new instance ( wrap me ) ; } catch ( throwable err ) { throw new runtime exception ( __str__ @$ err ) ; },reflection get,fail,pre
table <PLACE_HOLDER> instruction,if ( opcode == opcodes . switch ) { return false ; },table change,fail,pre
now @$ we have to make sure the finalizer <PLACE_HOLDER>s run . we will keep allocating more and more memory with the idea that eventually @$ the memory occupied by the big object will <PLACE_HOLDER> reclaimed and the finalizer will be run .,list hold alot = new array list ( ) ; for ( int chunk = __num__ ; chunk > __num__ ; chunk = chunk / __num__ ) { if ( finalizer run ) { return ; } try { while ( true ) { hold alot . add ( new byte [ chunk ] ) ; system . err . println ( __str__ + chunk ) ; } } catch ( throwable thrown ) { system . gc ( ) ; } system . run finalization ( ) ; },memory get,success,pre
app : <PLACE_HOLDER> api 10 dev : <PLACE_HOLDER> api 20,verify compute target sdk version ( older_version @$ released @$ true @$ older_version ) ;,app released,success,pre
if the key is the left operand @$ then <PLACE_HOLDER> the operator before the index lookup,int op = reflect on operator ( index info . _key ( ) ) ;,then call,fail,pre
if its a <PLACE_HOLDER> parent queue @$ it might not have any children,if ( ( q . children == null || q . children . is empty ( ) ) && ! ( q . parent queue instanceof managed parent queue ) ) { return immutable set . of ( q . queue name ) ; } set < string > leaf queue names = new hash set < > ( ) ; for ( temp queue per partition child : q . children ) { leaf queue names . add all ( get leaf queue names ( child ) ) ; } return leaf queue names ;,a managed,success,pre
it is fully initialized . the previous implementation of this rule did not <PLACE_HOLDER> subclasses of ha region queue and caused the bug .,return this . owning queue . is queue initialized ( ) ? this . owning queue : null ;,implementation install,fail,pre
and now fire the message <PLACE_HOLDER> event .,fire message received ( message @$ from ) ;,message received,success,pre
check memory <PLACE_HOLDER> the new process .,process tree . update process tree ( ) ; assert . assert equals ( __str__ @$ __num__ @$ process tree . get virtual memory size ( ) ) ; if ( ! smap enabled ) { long cumu rss mem = procfs based process tree . page_size > __num__ ? __num__ * procfs based process tree . page_size : resource calculator process tree . unavailable ; assert . assert equals ( __str__ @$ cumu rss mem @$ process tree . get rss memory size ( ) ) ; } else { assert . assert equals ( __str__ @$ __num__ * kb_to_bytes * __num__ @$ process tree . get rss memory size ( ) ) ; },memory accounts,fail,pre
if the jvm is not <PLACE_HOLDER> url caching @$ then the jar files will not be cached either @$ and so they are safe to close,if ( ! get use caches ( ) ) { if ( _jar file != null ) { try { if ( log . is debug enabled ( ) ) log . debug ( __str__ + _jar file . get name ( ) ) ; _jar file . close ( ) ; } catch ( io exception ioe ) { log . ignore ( ioe ) ; } } } _jar file = null ; super . close ( ) ;,jvm using,fail,pre
expected only metadata <PLACE_HOLDER> proposed message for <PLACE_HOLDER> schema .,assert custom messages ( __num__ ) ; assert communication messages ( ) ;,metadata update,success,pre
delete again should not <PLACE_HOLDER> any exception,try { dlm . delete ( ) ; } catch ( io exception ioe ) { fail ( __str__ ) ; },delete throw,success,pre
this index already <PLACE_HOLDER> @$ verify meta data aligns with expectations,mutable boolean paged file open = new mutable boolean ( true ) ; boolean success = false ; try { meta meta = read meta ( null @$ paged file ) ; paged file = map with correct page size ( page cache @$ index file @$ paged file @$ meta . get page size ( ) @$ paged file open @$ open options ) ; success = true ; return paged file ; } catch ( illegal state exception e ) { throw new metadata mismatch exception ( __str__ @$ e ) ; } finally { if ( ! success && paged file open . boolean value ( ) ) { paged file . close ( ) ; } },index exists,success,pre
the hadoop factory <PLACE_HOLDER> various schemes,return __str__ ;,factory supports,fail,pre
get spans should <PLACE_HOLDER> the span,object [ ] spans = spannable . get spans ( __num__ @$ spannable . length ( ) @$ m class ) ; assert not null ( spans ) ; assert that ( spans @$ array with size ( __num__ ) ) ; assert same ( m watcher @$ spans [ __num__ ] ) ;,spans return,success,pre
assigned partitions for task will either <PLACE_HOLDER> all partitions for the task or be empty @$ so just add all,assigned partitions . add all ( assigned partitions for task ) ;,partitions have,fail,pre
if the entity has <PLACE_HOLDER> state @$ then update that as well,if ( entry . get deleted state ( ) != null ) { entry . get deleted state ( ) [ lazy property numbers [ j ] ] = lazy property types [ j ] . deep copy ( prop value @$ factory ) ; } return field name . equals ( lazy property names [ j ] ) ;,entity deleted,success,pre
modify the grammar to make checksum comparison <PLACE_HOLDER> a change,try ( change change = change . of ( base grammar @$ __str__ ) ) { byte [ ] test lexer sum = checksum ( gen test lexer ) ; byte [ ] test parser sum = checksum ( gen test parser ) ; byte [ ] hello sum = checksum ( gen hello ) ; maven . execute mojo ( session @$ project @$ exec ) ; assert false ( arrays . equals ( test lexer sum @$ checksum ( gen test lexer ) ) ) ; assert false ( arrays . equals ( test parser sum @$ checksum ( gen test parser ) ) ) ; assert true ( arrays . equals ( hello sum @$ checksum ( gen hello ) ) ) ; },comparison detect,success,pre
method name can not <PLACE_HOLDER> reserved keyword @$ e.g . return,if ( is reserved word ( operation id ) ) { logger . warn ( operation id + __str__ + camelize ( sanitize name ( __str__ + operation id ) @$ true ) ) ; operation id = __str__ + operation id ; } return camelize ( sanitize name ( operation id ) @$ true ) ;,name use,success,pre
process instance <PLACE_HOLDER> event,flowable engine entity event event = ( flowable engine entity event ) listener . get events received ( ) . get ( __num__ ) ; assert equals ( flowable engine event type . entity_created @$ event . get type ( ) ) ; assert equals ( process instance . get id ( ) @$ ( ( process instance ) event . get entity ( ) ) . get id ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get process instance id ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get execution id ( ) ) ; assert equals ( process instance . get process definition id ( ) @$ event,instance created,fail,pre
allow all <PLACE_HOLDER> taggings @$ unless very common,int tagged word iw = new int tagged word ( word @$ null tag ) ; if ( seen counter . get count ( iw ) > smooth in unknowns threshold ) { return rules with word [ word ] . iterator ( ) ; } else { word taggings = new array list < > ( __num__ ) ; for ( int tagged word itw2 : tags ) { int tagged word itw = new int tagged word ( word @$ itw2 . tag ) ; if ( score ( itw @$ loc @$ word index . get ( word ) @$ null ) > float . negative_infinity ) { word taggings . add ( itw ) ; } } },all known,fail,pre
destination creation will <PLACE_HOLDER> the advisory since the virtual topic exists,final destination statistics destination statistics = local broker . get destination ( new activemq queue ( test queue name ) ) . get destination statistics ( ) ; final destination statistics remote dest statistics = remote broker . get destination ( new activemq queue ( __str__ ) ) . get destination statistics ( ) ; thread . sleep ( __num__ ) ; assert advisory broker counts ( __num__ @$ __num__ @$ __num__ ) ;,creation do,fail,pre
if both old and new table does not <PLACE_HOLDER> the policy @$ then do n't dump the event .,log . info ( __str__ ) ; return false ;,table satisfy,fail,pre
all the containers @$ the container monitor and all the container . the container scheduler may <PLACE_HOLDER> its own dispatcher .,return new container scheduler ( cntxt @$ dispatcher @$ metrics ) ;,containers implement,fail,pre
if the query <PLACE_HOLDER> more than one join,if ( query properties . get join count ( ) > __num__ ) { profilescbo . add ( extendedcbo profile . join_reordering ) ; },query has,fail,pre
app is <PLACE_HOLDER> upgrade key sets ; make sure all are valid,long [ ] upgrade key sets = old ps . key set data . get upgrade key sets ( ) ; for ( int i = __num__ ; i < upgrade key sets . length ; i ++ ) { if ( ! is id valid key set id ( upgrade key sets [ i ] ) ) { slog . wtf ( tag @$ __str__ + ( old ps . name != null ? old ps . name : __str__ ) + __str__ + upgrade key sets [ i ] + __str__ ) ; return false ; } } return true ;,app producing,fail,pre
no direct instances present @$ however another two role entity <PLACE_HOLDER> another single role entity and has instances,entity type another single role entity = tx . get entity type ( __str__ ) ; assert false ( test tx . rule cache ( ) . absent types ( collections . singleton ( another single role entity ) ) ) ;,present extends,fail,pre
converts <PLACE_HOLDER> vector back to polar .,double lat = atan2 ( z @$ sqrt ( x * x + y * y ) ) ; double lng = atan2 ( y @$ x ) ; return new lat lng ( to degrees ( lat ) @$ to degrees ( lng ) ) ;,converts interpolated,success,pre
new entries should not <PLACE_HOLDER> any copies,assert . assert equals ( record . get record metadata ( __str__ ) @$ __str__ ) ; assert . assert null ( derived1 . get record metadata ( __str__ ) ) ; assert . assert null ( derived2 . get record metadata ( __str__ ) ) ; assert . assert null ( derived3 . get record metadata ( __str__ ) ) ; assert . assert equals ( derived1 . get record metadata ( __str__ ) @$ __str__ ) ; assert . assert null ( record . get record metadata ( __str__ ) ) ; assert . assert null ( derived2 . get record metadata ( __str__ ) ) ; assert . assert null ( derived3 . get record metadata ( __str__ ) ) ; assert . assert equals,entries affect,success,pre
associate <PLACE_HOLDER> component virtual file with mocked <PLACE_HOLDER> component cls,psi class mocked generated component cls = mock ( psi class . class ) ; psi file mocked generated component file = mock ( psi file . class ) ; when ( mocked generated component cls . get containing file ( ) ) . then return ( mocked generated component file ) ; virtual file generated component virtual file = create present in scope virtual file ( ) ; when ( mocked generated component file . get virtual file ( ) ) . then return ( generated component virtual file ) ; virtual file present in scope virtual file = create present in scope virtual file ( ) ;,associate generated,success,pre
call the method from the parent class which <PLACE_HOLDER> the debugger .,super . init debugger ( ) ;,which initializes,success,pre
only 1 process can <PLACE_HOLDER> this process id,break ;,process use,fail,pre
attach the original predicate to the table <PLACE_HOLDER> operator for index optimizations that require the pushed predicate before pcr & later optimizations are applied,if ( hive conf . get bool var ( hive conf @$ hive conf . conf vars . hiveoptindexfilter ) ) { table scan desc . set filter expr ( original predicate ) ; },predicate scan,success,pre
chris added this next test in 2017 ; a bit weird @$ but kbp setup does n't <PLACE_HOLDER> newline in sentence boundary patterns @$ just in to discard,if ( abstract tokenizer . newline_token . equals ( word ) ) { last token was newline = true ; } if ( debug ) { log . info ( __str__ + word + __str__ + debug text ) ; },setup use,fail,pre
when slow <PLACE_HOLDER> the start of the loop with k step @$ fast already goes for 2 k steps @$ so it is k steps ahead of the start of the loop .,return null ;,slow reaches,fail,pre
collator.get keyword values <PLACE_HOLDER> the same contents for both commonly used true and false .,string [ ] all = collator . get keyword values for locale ( __str__ @$ loc @$ false ) ; boolean match all = false ; if ( pref . length == all . length ) { match all = true ; for ( int j = __num__ ; j < pref . length ; j ++ ) { boolean found match = false ; for ( int k = __num__ ; k < all . length ; k ++ ) { if ( pref [ j ] . equals ( all [ k ] ) ) { found match = true ; break ; } } if ( ! found match ) { match all = false ; break ; } } } if ( ! match,values returns,fail,pre
stub <PLACE_HOLDER> shared cache manager to return true,do return ( true ) . when ( spied ) . notify shared cache manager ( isa ( string . class ) @$ isa ( string . class ) ) ; assert true ( spied . call ( ) ) ;,stub notify,success,pre
ensure that snapshot directory will always <PLACE_HOLDER> the local file system instead of the default file system,configuration configuration = new configuration ( ) ; configuration . set string ( core options . default_filesystem_scheme @$ __str__ ) ; file system . initialize ( configuration ) ; final file folder root = temporary folder . get root ( ) ; try { file folderb = new file ( folder root @$ string . value of ( uuid . randomuuid ( ) ) ) ; snapshot directory snapshot directoryb = snapshot directory . temporary ( folderb ) ; assert . assert equals ( snapshot directoryb . get file system ( ) @$ file system . get local file system ( ) ) ; } finally { file system . initialize ( new configuration ( ) ) ; },directory use,success,pre
handle @$ mark as processed @$ unless the exception handler <PLACE_HOLDER> an exception,exception handler . handle event exception ( ex @$ next sequence @$ event ) ; processed sequence = true ;,handler throws,fail,pre
warning the return in the finally clause will <PLACE_HOLDER> any return before,if ( failed ) return failure ;,return override,fail,pre
event with no message <PLACE_HOLDER> severity level info,assert false ( filter . accept ( ev ) @$ __str__ ) ; final severity level error level = severity level . error ; final localized message error message = new localized message ( __num__ @$ __num__ @$ __str__ @$ __str__ @$ null @$ error level @$ null @$ get class ( ) @$ null ) ; final audit event ev2 = new audit event ( this @$ __str__ @$ error message ) ; assert true ( filter . accept ( ev2 ) @$ __str__ + error level ) ; final severity level info level = severity level . info ; final localized message info message = new localized message ( __num__ @$ __num__ @$ __str__ @$ __str__ @$ null @$ info level @$ null @$ get class,event has,success,pre
if the new candidate <PLACE_HOLDER> more duplicates than previously tracked mc vs we insert it and bubble others down,int j ; for ( j = num tracked - __num__ ; j > __num__ ; j -- ) { if ( duplicates <= most common value candidates [ j - __num__ ] . count ) { break ; } most common value candidates [ j ] . count = most common value candidates [ j - __num__ ] . count ; most common value candidates [ j ] . first = most common value candidates [ j - __num__ ] . first ; } most common value candidates [ j ] . count = duplicates ; most common value candidates [ j ] . first = i - duplicates ;,candidate has,success,pre
we allow one <PLACE_HOLDER> comma,if ( flavor != config syntax . json && t == tokens . close_square ) { put back ( t ) ; } else { throw parse error ( __str__ + t + __str__ + t + __str__ ) ; },one escaped,fail,pre
bob can <PLACE_HOLDER> project,wc = j . create web client ( ) . login ( __str__ ) ; wc . go to ( __str__ ) ;,bob see,fail,pre
check that the jar does not <PLACE_HOLDER> an entry for the removed class,try ( jar file abi jar = new jar file ( abi jar path . to file ( ) ) ) { assert that ( abi jar . stream ( ) . map ( jar entry :: get name ) . collect ( collectors . to set ( ) ) @$ matchers . contains in any order ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; manifest manifest = abi jar . get manifest ( ) ; assert null ( manifest . get attributes ( __str__ ) ) ; },jar have,success,pre
sensless tagging : josm does <PLACE_HOLDER> a warning here . we follow the highway tag :,way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . clear tags ( ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . can skip ( ) ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . clear tags ( ) ; way . set tag ( __str__,josm emit,fail,pre
inherit <PLACE_HOLDER> timeout from server pool configuration .,for ( ldap server server : servers ) { if ( server . get connect timeout ( ) == __num__ && timeout != __num__ ) { server . set connect timeout ( timeout ) ; } } return this ;,inherit connect,success,pre
one in 4 tests generate a sync and volt bulk <PLACE_HOLDER> internal state verification,if ( ! abort1 && rnd . next int ( ) % __num__ == __num__ ) { bulk loader1 . drain ( ) ; assert ( bulk loader1 . get outstanding row count ( ) == __num__ ) ; assert ( bulk loader1 . get completed row count ( ) == row cnt1 ) ; },bulk loader,success,pre
url <PLACE_HOLDER> port number,if ( - __num__ != colon index ) { if ( baseurl . starts with ( __str__ ) && baseurl . ends with ( __str__ ) ) { baseurl = baseurl . substring ( __num__ @$ colon index ) ; } else if ( baseurl . starts with ( __str__ ) && baseurl . ends with ( __str__ ) ) { baseurl = baseurl . substring ( __num__ @$ colon index ) ; } },url contains,success,pre
then : '' collected must <PLACE_HOLDER> duplicates '',assert that ( tap . block ( ) ) . contains exactly ( __num__ @$ __num__ @$ __num__ ) ;,collected remove,success,pre
checking if the current continuous frame <PLACE_HOLDER> a correct payload with the other frames combined,if ( curop == opcode . continuous && current continuous frame != null ) { add to buffer list ( frame . get payload data ( ) ) ; },frame has,fail,pre
required features <PLACE_HOLDER> activity launch,wait for latch ( latch ) ; verify ( m mock account manager response ) . on error ( account manager . error_code_invalid_response @$ account manager service test fixtures . error_message ) ; verify ( m mock account manager response @$ never ( ) ) . on result ( any ( bundle . class ) ) ;,features expect,success,pre
identify the x position at which to <PLACE_HOLDER> the final line : note : the left position of the line is needed for the case of rtl text .,final float ellipsis target = layout width - bounds . width ( ) + new layout . get line left ( ellipsized line number ) ;,which put,fail,pre
create the parts <PLACE_HOLDER> any parameters,for ( j meter property j meter property : get arguments ( ) ) { http argument arg = ( http argument ) j meter property . get object value ( ) ; string parameter name = arg . get name ( ) ; if ( arg . is skippable ( parameter name ) ) { continue ; } string body string body = new string body ( arg . get value ( ) @$ content type . create ( arg . get content type ( ) @$ charset ) ) ; form body part form part = form body part builder . create ( parameter name @$ string body ) . build ( ) ; multipart entity builder . add part ( form part ) ; },parts containing,fail,pre
adjust start index to pickup first range which <PLACE_HOLDER> start,try { record rec = range table . get record at or before ( start ) ; if ( rec != null && rec . get long value ( range_to_col ) >= start ) { start = rec . get key ( ) ; } map rec iter = map table . index iterator ( map_range_key_col @$ new long field ( start ) @$ new long field ( end ) @$ true ) ; } catch ( io exception e ) { err handler . db error ( e ) ; },which includes,fail,pre
battery stats <PLACE_HOLDER> cpu time locked,m battery stats impl . set power profile ( null ) ; m battery stats impl . update time bases locked ( unplugged @$ screen state @$ up time @$ real time ) ; m battery stats impl . set power profile ( m power profile ) ;,stats has,fail,pre
we only look for a partial <PLACE_HOLDER> which does n't <PLACE_HOLDER> on subid because we can crossfade views that <PLACE_HOLDER> everything except for subid .,for ( pair < view @$ string > p : transition view pairs ) { if ( tn . partial equals ( transition name . parse ( view compat . get transition name ( p . first ) ) ) ) { it . remove ( ) ; break ; } },which match,success,pre
succeed if user <PLACE_HOLDER> permission for all regions and all keys for the given operation,if ( allowed permissions . contains ( new resource permission ( resource permission . resource . data @$ permission . get operation ( ) ) ) ) { return true ; },user has,success,pre
check that the alluxio path we 're creating does n't <PLACE_HOLDER> a path in the parent ufs,mount table . resolution resolution = m mount table . resolve ( alluxio path ) ; try ( closeable resource < under file system > ufs resource = resolution . acquire ufs resource ( ) ) { string ufs resolved path = resolution . get uri ( ) . get path ( ) ; if ( ufs resource . get ( ) . exists ( ufs resolved path ) ) { throw new io exception ( exception message . mount_path_shadows_parent_ufs . get message ( alluxio path @$ ufs resolved path ) ) ; } },path contain,fail,pre
create a walker which <PLACE_HOLDER> the tree in a dfs manner while maintaining the operator stack .,map < rule @$ node processor > op rules = new linked hash map < rule @$ node processor > ( ) ; op rules . put ( new rule reg exp ( __str__ @$ reduce sink operator . get operator name ( ) + __str__ ) @$ new set reducer parallelism ( ) ) ; op rules . put ( new rule reg exp ( __str__ @$ join operator . get operator name ( ) + __str__ ) @$ new convert join map join ( ) ) ; if ( proc ctx . conf . get bool var ( conf vars . hivemapaggrhashminreductionstatsadjust ) ) { op rules . put ( new rule reg exp ( __str__ @$ group by operator . get operator name ( ),which walks,success,pre
if the score of this request is higher or equal to that of this factory and some other factory is responsible for it @$ then this factory should not <PLACE_HOLDER> the request because it has no hope of satisfying it .,return ! n . requested && ( n . score < m score || n . factory serial number == m serial number ) && n . request . network capabilities . satisfied by network capabilities ( m capability filter ) && accept request ( n . request @$ n . score ) ;,factory accept,fail,pre
using just the package name is n't great @$ since it disallows having multiple engines in the same package @$ but that 's what the existing api <PLACE_HOLDER> .,engine . name = service . package name ; char sequence label = service . load label ( pm ) ; engine . label = text utils . is empty ( label ) ? engine . name : label . to string ( ) ; engine . icon = service . get icon resource ( ) ; engine . priority = resolve . priority ; engine . system = is system engine ( service ) ; return engine ;,api says,fail,pre
now the tricky one . 'before a ' leads to 'call activity a ' @$ which <PLACE_HOLDER> subprocess 02 which terminates,process instance = runtime service . start process instance by key ( __str__ ) ; tasks = assert task names ( process instance @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; task task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ; assert historic process,which calls,success,pre
forbid <PLACE_HOLDER> flags .,if ( python opts . incompatible use python toolchains ) { if ( opts . python2 path != null ) { reporter . handle ( event . error ( __str__ + __str__ ) ) ; } if ( opts . python3 path != null ) { reporter . handle ( event . error ( __str__ + __str__ ) ) ; } if ( opts . python top != null ) { reporter . handle ( event . error ( __str__ + __str__ + __str__ + __str__ + __str__ ) ) ; } },forbid raced,fail,pre
since the expression must <PLACE_HOLDER> a single corresponding node nullable is our default assumption,if ( expr nodes . size ( ) != __num__ ) { return nullness . nullable ; },expression have,success,pre
nodes to start <PLACE_HOLDER> the list of nodes to be started immediately . we do n't want to start the animations in the loop directly because we first need to set up dependencies on all of the nodes . for example @$ we do n't want to start an animation when some other animation also wants to start when the first animation begins .,final array list < node > nodes to start = new array list < node > ( ) ; for ( int i = __num__ ; i < num sorted nodes ; ++ i ) { node node = m sorted nodes . get ( i ) ; if ( m set listener == null ) { m set listener = new animator set listener ( this ) ; } if ( node . dependencies == null || node . dependencies . size ( ) == __num__ ) { nodes to start . add ( node ) ; } else { int num dependencies = node . dependencies . size ( ) ; for ( int j = __num__ ; j < num dependencies ; ++ j ),nodes holds,success,pre
signal for factory to <PLACE_HOLDER> ; sets <PLACE_HOLDER> time,start flag . count down ( ) ;,sets start,success,pre
make volume keys <PLACE_HOLDER> multimedia volume even if music is not playing now,set volume control stream ( audio manager . stream_music ) ; try { play services available = google api availability . get instance ( ) . is google play services available ( this ) == connection result . success ; } catch ( exception ignored ) { } if ( play services available ) init cast ( ) ;,keys change,success,pre
second call should <PLACE_HOLDER> exception,assert null ( shard expression . value ( ) ) ;,call throw,success,pre
ok @$ lets <PLACE_HOLDER> this fragment lexicographically,subversion comparision result = v comps1 [ i ] . compare to ( v comps2 [ i ] ) ;,lets evaluate,fail,pre
type parameter does n't have <PLACE_HOLDER> upper bounds,if ( ! node . has type bound ( ) ) { node . set type definition ( java type definition . for class ( upper_bound @$ object . class ) ) ; } else { super . visit ( node @$ data ) ; rollup type unary ( node ) ; },parameter set,fail,pre
gcm mode <PLACE_HOLDER> additional handling,if ( cipher mode == gcm_mode ) { if ( tag len == - __num__ ) { tag len = galois counter mode . default_tag_len ; } if ( decrypting ) { min bytes = tag len ; } else { require reinit = arrays . equals ( iv bytes @$ last enc iv ) && message digest . is equal ( key bytes @$ last enc key ) ; if ( require reinit ) { throw new invalid algorithm parameter exception ( __str__ ) ; } last enc iv = iv bytes ; last enc key = key bytes ; } ( ( galois counter mode ) cipher ) . init ( decrypting @$ algorithm @$ key bytes @$ iv bytes @$ tag len ) ; },mode requires,fail,pre
some np <PLACE_HOLDER> drop items onto multiple tiles,final list < item stack > all items = new array list < > ( ) ; for ( int i = __num__ ; i < size ; ++ i ) { for ( int j = __num__ ; j < size ; ++ j ) { final int packed = ( x + i ) << __num__ | ( y + j ) ; final collection < item stack > items = item spawns . get ( packed ) ; all items . add all ( items ) ; } } if ( all items . is empty ( ) ) { return ; } kill points . add ( location ) ; event bus . post ( new npc loot received ( npc @$ all items,np pushed,fail,pre
it 's possible the default tag wo n't exist if the user just <PLACE_HOLDER> the app 's language @$ in which case default to the first tag in the table,if ( ! reader tag table . tag exists ( tag ) ) { tag = reader tag table . get first tag ( ) ; } set current tag ( tag ) ; if ( build config . information_architecture_available && m is top level ) { if ( tag . is followed sites ( ) ) { m view model . set default subfilter ( ) ; } },user changed,success,pre
win<PLACE_HOLDER>w <PLACE_HOLDER> fn operator <PLACE_HOLDER>es not use a <PLACE_HOLDER> fn,this . requires stable input = do fn != null && do fn signatures . get signature ( do fn . get class ( ) ) . process element ( ) . requires stable input ( ) ;,a do,success,pre
finishing the tasks should also <PLACE_HOLDER> the end time,tasks = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . list ( ) ; assert equals ( __num__ @$ tasks . size ( ) ) ; for ( org . flowable . task . api . task task : tasks ) { task service . complete ( task . get id ( ) ) ; } wait for history job executor to process all jobs ( __num__ @$ __num__ ) ; historic activity instances = history service . create historic activity instance query ( ) . activity id ( __str__ ) . list ( ) ; assert equals ( __num__ @$ historic activity instances . size ( ) ) ; for ( historic activity,tasks set,success,pre
the pending intent to launch our activity if the user <PLACE_HOLDER> this notification,pending intent content intent = pending intent . get activity ( this @$ __num__ @$ new intent ( this @$ main activity . class ) @$ __num__ ) ;,user selects,success,pre
now test that compare to does what we expect . the exact ordering here does n't <PLACE_HOLDER> much .,collections . shuffle ( paths ) ; collections . sort ( paths ) ; list < path fragment > expected order = to paths ( immutable list . of ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; assert that ( paths ) . is equal to ( expected order ) ;,test matter,success,pre
user has <PLACE_HOLDER> initial context factory ; try to get it,if ( my props != null && my props . get ( context . initial_context_factory ) != null ) { get default init ctx ( ) ; },user specified,success,pre
indicate the the client <PLACE_HOLDER> muc,join presence . add extension ( new muc initial presence ( password @$ max chars @$ max stanzas @$ seconds @$ since ) ) ;,client joins,fail,pre
the file we should <PLACE_HOLDER> new content to .,path output file ; if ( old file != null && old file . exists ( ) && ! is renaming ) { output file = old file ; } else { output file = new file ; },file write,success,pre
0 <PLACE_HOLDER> an unrepresentable character,return __num__ ;,0 indicates,success,pre
if memory does n't <PLACE_HOLDER> the target address,if ( ! reference . is memory reference ( ) ) { continue ; } if ( ! memory . contains ( target ) ) { continue ; } if ( ignore new pointers . contains ( target ) ) { continue ; },memory contain,success,pre
if syntax error occurs then message is printed by error listener and parser <PLACE_HOLDER> this runtime exception to stop parsing . just stop processing current javadoc comment .,if ( parse error message == null ) { parse error message = error listener . get error message ( ) ; },parser consumes,fail,pre
flow files those are already committed will not <PLACE_HOLDER> rollback .,session . rollback ( ) ;,files need,fail,pre
we need to filter the index result if the property is not allowed on some label since the nodes in the index might <PLACE_HOLDER> both an allowed and a disallowed label for the property,if ( ! access mode . allows traverse all nodes with label ( label ) || access mode . disallows read property for some label ( prop ) || ! access mode . allows read node property ( ( ) -> labels . from ( label ) @$ prop ) ) { return new node label security filter ( property ids @$ cursor @$ cursors . allocate node cursor ( ) @$ this @$ access mode ) ; },nodes have,success,pre
keystore contains a hostname which does n't <PLACE_HOLDER> localhost,client ssl context factory . set key store path ( __str__ ) ; client ssl context factory . set key store password ( __str__ ) ; queued thread pool client threads = new queued thread pool ( ) ; client threads . set name ( __str__ ) ; client = new http client ( client ssl context factory ) ; client . set executor ( client threads ) ; client . start ( ) ;,which represent,fail,pre
inspect the source ce 32 s. just <PLACE_HOLDER> them if none are modified . otherwise <PLACE_HOLDER> to modified c es @$ with modifications .,boolean is modified = false ; for ( int i = __num__ ; i < length ; ++ i ) { ce32 = srcce32s [ src index + i ] ; long ce ; if ( collation . is specialce32 ( ce32 ) || ( ce = modifier . modifyce32 ( ce32 ) ) == collation . no_ce ) { if ( is modified ) { modifiedc es [ i ] = collation . ce fromce32 ( ce32 ) ; } } else { if ( ! is modified ) { for ( int j = __num__ ; j < i ; ++ j ) { modifiedc es [ j ] = collation . ce fromce32 ( srcce32s [ src index + j ] ) ; } is,s. add,fail,pre
zap : <PLACE_HOLDER> the statement .,int current idx = __num__ ; if ( is exist status code ) { ps insert . set int ( current idx @$ status code ) ; ++ current idx ; },zap added,success,pre
oss capabilities <PLACE_HOLDER> a chrome webdriver,map < string @$ object > payload = immutable map . of ( __str__ @$ immutable map . of ( __str__ @$ __str__ ) @$ __str__ @$ immutable map . of ( __str__ @$ immutable map . of ( __str__ @$ __str__ ) @$ __str__ @$ immutable list . of ( immutable map . of ( __str__ @$ __str__ ) @$ immutable map . of ( __str__ @$ __str__ ) ) ) ) ;,capabilities do,fail,pre
two 'union with aliases ' fields under different records but with the same field name . the generated record representation for these two unions should <PLACE_HOLDER> the parent record 's name to avoid any name conflicts .,return new object [ ] [ ] { { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ translate default @$ __str__ @$ empty foo schema @$ empty foo value @$ null },'union use,fail,pre
the address should <PLACE_HOLDER> logging.properties,deque < property > result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; model node handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; string file name = null ;,address have,success,pre
inline the wsdl as extensibility element <PLACE_HOLDER> mex : metadata wrapper,if ( wsdl address != null ) { writer . write start element ( member submission addressing constants . mex_metadata . get prefix ( ) @$ member submission addressing constants . mex_metadata . get local part ( ) @$ member submission addressing constants . mex_metadata . get namespaceuri ( ) ) ; writer . write start element ( member submission addressing constants . mex_metadata_section . get prefix ( ) @$ member submission addressing constants . mex_metadata_section . get local part ( ) @$ member submission addressing constants . mex_metadata_section . get namespaceuri ( ) ) ; writer . write attribute ( member submission addressing constants . mex_metadata_dialect_attribute @$ member submission addressing constants . mex_metadata_dialect_value ) ; write wsdl ( writer @$ service @$ wsdl address ) ; writer,element addressing,fail,pre
these ms <PLACE_HOLDER> all presume utc,double [ ] [ ] exp = new double [ ] [ ] { d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d (,ms counts,success,pre
clear <PLACE_HOLDER> extracted ' flag .,this . link extractor finished = false ; extra info = null ; out links = null ; this . revisit profile = null ;,clear ged,fail,pre
the component <PLACE_HOLDER> timeout methods @$ it needs a 'real ' timer service,if ( component description . is timer service required ( ) ) { final string deployment name ; if ( module description . get distinct name ( ) == null || module description . get distinct name ( ) . length ( ) == __num__ ) { deployment name = module description . get application name ( ) + __str__ + module description . get module name ( ) ; } else { deployment name = module description . get application name ( ) + __str__ + module description . get module name ( ) + __str__ + module description . get distinct name ( ) ; } root_logger . debugf ( __str__ @$ component description . get component name ( ) ) ; component description . get,component has,success,pre
now we must read and convert this file to the target format with the target <PLACE_HOLDER> 1024 x 1024,try { file new png file = new file ( png file . get absolute path ( ) + __str__ ) ; png file . rename to ( new png file ) ; final image img = image parser . parse ( png file . get absolute path ( ) @$ file utils . read ( new png file ) ) ; if ( img == null ) { return false ; } final image scaled = img . get scaled instance ( width @$ height @$ image . scale_area_averaging ) ; final media tracker media tracker = new media tracker ( new container ( ) ) ; media tracker . add image ( scaled @$ __num__ ) ; try { media tracker . wait forid ( __num__,1024 size,success,pre
modal bottom sheets <PLACE_HOLDER> auto peek height by default .,assert that ( behavior . get peek height ( ) @$ is ( bottom sheet behavior . peek_height_auto ) ) ;,sheets use,fail,pre
we do n't <PLACE_HOLDER> permission to read boolean creds s ince the creds <PLACE_HOLDER> no boolean creds we should get an empty set,try { set priv cred set1 = s . get private credentials ( boolean . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { e . print stack trace ( ) ; throw new runtime exception ( __str__ ) ; } system . out . println ( __str__ ) ;,creds have,success,pre
second @$ check if this work has multiple reduce sinks . if so @$ <PLACE_HOLDER> split .,split base work ( spark work @$ work @$ child works ) ;,second do,success,pre
for each aggregation function in the reference <PLACE_HOLDER> @$ create a corresponding aggregation function that points to the null row . map the symbols from the aggregations in reference aggregation to the symbols in these new aggregations .,immutable map . builder < symbol @$ symbol > aggregations symbol mapping builder = immutable map . builder ( ) ; immutable map . builder < symbol @$ aggregation node . aggregation > aggregations over null builder = immutable map . builder ( ) ; for ( map . entry < symbol @$ aggregation node . aggregation > entry : reference aggregation . get aggregations ( ) . entry set ( ) ) { symbol aggregation symbol = entry . get key ( ) ; aggregation node . aggregation aggregation = entry . get value ( ) ; if ( ! is using symbols ( aggregation @$ sources symbol mapping . key set ( ) ) ) { return optional . empty ( ) ; } aggregation over,function agg,fail,pre
xlsx <PLACE_HOLDER> full reevaluation,if ( data . wb instanceof xssf workbook ) { formula evaluator evaluator = data . wb . get creation helper ( ) . create formula evaluator ( ) ; for ( int sheet num = __num__ ; sheet num < data . wb . get number of sheets ( ) ; sheet num ++ ) { sheet sheet = data . wb . get sheet at ( sheet num ) ; for ( row r : sheet ) { for ( cell c : r ) { if ( c . get cell type ( ) == cell . cell_type_formula ) { evaluator . evaluate formula cell ( c ) ; } } } } } else if ( data . wb instanceof hssf workbook ) {,xlsx requires,fail,pre
this streamer <PLACE_HOLDER> internal error before getting updated block,if ( ! streamer . is healthy ( ) && coordinator . get new blocks ( ) . peek ( streamer . get index ( ) ) != null ) { failed . add ( streamer ) ; },streamer has,fail,pre
field field <PLACE_HOLDER> name line,wl field filename = new label ( w file name @$ swt . right ) ; wl field filename . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl field filename ) ; fdl field filename = new form data ( ) ; fdl field filename . left = new form attachment ( __num__ @$ __num__ ) ; fdl field filename . top = new form attachment ( w file name in field @$ margin ) ; fdl field filename . right = new form attachment ( middle @$ - margin ) ; wl field filename . set layout data ( fdl field filename ) ; w field filename = new c combo ( w file,field file,success,pre
test increase container api and make sure requests can <PLACE_HOLDER> nm,test increase container resource ( container ) ; test restart container ( container . get id ( ) ) ; test get container status ( container @$ i @$ container state . running @$ __str__ @$ exit code ) ; wait for container transition count ( container @$ org . apache . hadoop . yarn . server . nodemanager . containermanager . container . container state . running @$ __num__ ) ; if ( i % __num__ == __num__ ) { test re initialize container ( container . get id ( ) @$ clc @$ false ) ; test get container status ( container @$ i @$ container state . running @$ __str__ @$ exit code ) ; wait for container transition count ( container @$ org .,requests steal,fail,pre
we do our own adjustment as calendar can not <PLACE_HOLDER> a long .,time -= value ; value to use = __num__ ;,calendar handle,success,pre
check that one of them <PLACE_HOLDER> the correct match action,boolean has match = false ; for ( match action match : patterns . get post patterns ( ) . get ( __num__ ) . get match actions ( ) ) { if ( ! ( match instanceof function start analyzer . context action ) ) { continue ; } has match = true ; assert equals ( __str__ @$ ( ( function start analyzer . context action ) match ) . get name ( ) ) ; assert equals ( new big integer ( __str__ ) @$ ( ( function start analyzer . context action ) match ) . get value ( ) ) ; } assert true ( has match ) ;,one has,success,pre
build period <PLACE_HOLDER> criteria,offset criteria = new offset criteria . offset criteria builder ( ) . with offset as period ( __str__ ) ; assert . assert false ( offset criteria . is smallest ( ) ) ; assert . assert false ( offset criteria . is largest ( ) ) ; assert . assert true ( offset criteria . is period ( ) ) ; assert . assert false ( offset criteria . is custom ( ) ) ; assert . assert equals ( offset criteria . get offset string ( ) @$ __str__ ) ;,period offset,success,pre
each zoom level <PLACE_HOLDER> viewable size by 2,for ( int i = default_min_zoom ; i < default_max_zoom ; i ++ ) { max intensity array [ i ] = get max value ( m data @$ m bounds @$ radius @$ ( int ) ( screen_size * math . pow ( __num__ @$ i - __num__ ) ) ) ; if ( i == default_min_zoom ) { for ( int j = __num__ ; j < i ; j ++ ) max intensity array [ j ] = max intensity array [ i ] ; } },level increases,fail,pre
if the entity does n't <PLACE_HOLDER> an id @$ it will be created instead of just being updated,rest comment mock mvc . perform ( put ( __str__ ) . content type ( test util . application_json_utf8 ) . content ( test util . convert object to json bytes ( comment ) ) ) . and expect ( status ( ) . is created ( ) ) ;,entity have,success,pre
ensure that all headed tests <PLACE_HOLDER> swing popups when displaying errors . setting this to false would force errors to only be written to the console .,set errorgui enabled ( true ) ;,tests apply,fail,pre
weak values allows transform executor services that are no longer in use to be reclaimed . executing transform executor services <PLACE_HOLDER> a strong reference to their transform executor service which stops the transform executor services from being prematurely garbage collected,serial executor services = cache builder . new builder ( ) . weak values ( ) . removal listener ( shutdown executor service listener ( ) ) . build ( serial transform executor service cache loader ( ) ) ; this . visible updates = new queue message receiver ( ) ; parallel executor service = transform executor services . parallel ( executor service ) ; executor factory = new direct transform executor . factory ( context @$ registry @$ transform enforcements ) ;,services keep,fail,pre
load <PLACE_HOLDER> lib for hook .,return load dexposed lib ( context ) ;,load dismissed,fail,pre
the classes bellow <PLACE_HOLDER> a static final secure random field . note that if the classes are not found as reachable by the analysis registering them form class initialization rerun does n't <PLACE_HOLDER> any effect .,image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( javax . net . ssl . ssl context . class @$ __str__ ) ;,rerun have,success,pre
check whether the callable <PLACE_HOLDER> an exception .,if ( future . is done ( ) ) { try { future . get ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; throw new runtime exception ( e ) ; } it . remove ( ) ; },callable threw,success,pre
add legacy subsystems the regular model will have the new attributes because they are in the xml @$ but the reverse controller model will not because transformation <PLACE_HOLDER> them,builder . create legacy kernel services builder ( null @$ controller version @$ model version ) . add maven resourceurl ( __str__ + controller version . get maven gav version ( ) ) . configure reverse controller check ( additional initialization . management @$ model node -> { for ( model node node : model node . get ( global_modules ) . as list ( ) ) { if ( __str__ . equals ( node . get ( name ) . as string ( ) ) ) { if ( ! node . has ( annotations ) ) { node . get ( annotations ) . set ( false ) ; } if ( ! node . has ( meta_inf ) ) { node . get ( meta_inf,model needs,fail,pre
bad value <PLACE_HOLDER> escape .,trigger action in cell editor ( key event . vk_escape ) ; assert not editing field ( ) ; assert equals ( __num__ @$ model . get num selected rows ( ) ) ; assert equals ( __num__ @$ model . get min index selected ( ) ) ; assert equals ( get data type ( __num__ ) @$ dt ) ;,value allows,success,pre
if we attempt to interact with the server too quickly @$ we will get a zoo keeper connection loss exception @$ which the provider <PLACE_HOLDER> in an io exception . we will wait 1 second in this case and try again . the test will timeout if this does not succeeed within 20 seconds .,thread . sleep ( __num__ ) ; e . print stack trace ( ) ; assert . fail ( __str__ + e . get class ( ) + __str__ @$ e ) ;,provider wraps,success,pre
this could happen when the time zone is not associated with a country @$ and its <PLACE_HOLDER> is not hierarchical @$ for example @$ cst 6 cdt . we use the canonical <PLACE_HOLDER> itself as the location for this case .,if ( location == null ) { location = tzid ; },canonical offset,fail,pre
create a simple rule which just <PLACE_HOLDER> something new to the output file .,build target target = build target factory . new instance ( __str__ ) ; build rule rule = new write file ( target @$ filesystem @$ __str__ @$ output @$ false ) ;,which writes,success,pre
calls <PLACE_HOLDER> image drawable internally,super . set image bitmap ( bitmap ) ;,calls set,success,pre
first dfs client has no files open so does n't <PLACE_HOLDER> leases .,final dfs client mock client1 = create mock client ( ) ; mockito . do return ( false ) . when ( mock client1 ) . renew lease ( ) ; assert same ( renewer @$ lease renewer . get instance ( fake_authority @$ fake_ugi_a @$ mock client1 ) ) ; long file id = __num__ ; renewer . put ( mock client1 ) ;,files renew,success,pre
make sure any exceptions caused by handlers do n't <PLACE_HOLDER> callers,return noop aware finished span handler . create ( defensive copy @$ noop ) ;,exceptions affect,fail,pre
let the application <PLACE_HOLDER> the key .,return __num__ ;,application handle,success,pre
apply the predicates and fetch the primary key ids <PLACE_HOLDER> up the id and convert them to bean,try { return run task ( new query task < list < e > > ( ) { @ override public list < e > handle ( connection connection ) throws exception { list < e > ret = new array list < > ( ) ; if ( ! ids to find . is empty ( ) ) { list < generic json entity > entities ; try ( prepared statement select statement = sql query builder . create find by id statement ( connection @$ generic json entity . class @$ ids to find ) ) { try ( result set result set = select statement . execute query ( ) ) { entities = generic result set mapper . map all ( result set @$,ids look,success,pre
close server 1 and pause so server <PLACE_HOLDER> chance to close,close server ( server1 ) ; wait . pause ( __num__ * __num__ ) ; wait for cqs disconnected ( client @$ __str__ @$ __num__ ) ;,server has,success,pre
promo snapshot collapses as the panel <PLACE_HOLDER> the maximized state .,update appearance ( __num__ - percentage ) ;,panel reaches,success,pre
if fifo size <PLACE_HOLDER> max size we evict the eldest entry .,if ( queue . size ( ) > max size ) { evict one ( ) ; } return result ;,size exceeds,fail,pre
log a warning if the resource <PLACE_HOLDER> an unnecessary service error definition annotation,if ( service error def annotation != null && ! resource model . is any service error list defined ( ) ) { log . warn ( string . format ( __str__ + __str__ + __str__ @$ resource class . get name ( ) @$ service error def . class . get simple name ( ) @$ service errors . class . get simple name ( ) @$ param error . class . get simple name ( ) ) ) ; },resource has,fail,pre
make sure the user can <PLACE_HOLDER> the file option to null @$ to allow the clearing of a value,options options = load search options ( ) ; help location help = null ; file default value = null ; string option name = __str__ ; options . register option ( option name @$ option type . file_type @$ default value @$ help @$ __str__ ) ; file option value = options . get file ( option name @$ null ) ; assert null ( option value ) ; file file = new file ( __str__ ) ; options . put object ( option name @$ file ) ; option value = options . get file ( option name @$ null ) ; assert equals ( file @$ option value ) ;,user set,success,pre
calculate the current radius at which to <PLACE_HOLDER> the selection circle .,final int sel radius = m selector radius ; final float sel length = m circle radius - math utils . lerp ( hours inset @$ minutes inset @$ m hours to minutes ) ; final double sel angle rad = math . to radians ( math utils . lerp deg ( hours angle deg @$ minutes angle deg @$ m hours to minutes ) ) ; final float sel centerx = mx center + sel length * ( float ) math . sin ( sel angle rad ) ; final float sel centery = my center - sel length * ( float ) math . cos ( sel angle rad ) ;,which place,success,pre
this newly opened jar file <PLACE_HOLDER> its own index @$ merge it into the parent 's index @$ taking into account the relative path .,jar index new index = new loader . get index ( ) ; if ( new index != null ) { int pos = jar name . last index of ( __str__ ) ; new index . merge ( this . index @$ ( pos == - __num__ ? null : jar name . substring ( __num__ @$ pos + __num__ ) ) ) ; },file has,success,pre
the application should not <PLACE_HOLDER> the cookie to connect,assert user connected ( wc @$ username ) ;,application use,success,pre
disabling the single plan item will <PLACE_HOLDER> the case,cmmn runtime service . disable plan item instance ( plan item instance . get id ( ) ) ; assert case instance ended ( case instance ) ;,item terminate,success,pre
if user <PLACE_HOLDER> a config file @$ use it as base configs .,if ( string utils . is none empty ( config file path ) ) { log . info ( __str__ + config file path ) ; final file config file = new file ( config file path ) ; final uri config uri = config file . touri ( ) ; final config factory config factory = options . get config factory ( ) . get declared constructor ( ) . new instance ( ) ; log . info ( __str__ + config factory . get class ( ) . get name ( ) ) ; if ( config factory instanceof properties config factory ) { check argument ( config file . exists ( ) @$ __str__ @$ config file path ) ; } config . put all,user specified,fail,pre
if target <PLACE_HOLDER> no remaining @$ need n't to check the is read only,target . flip ( ) ; assert equals ( __num__ @$ source . read ( target ) ) ;,target has,success,pre
make sure task 1 is <PLACE_HOLDER> and blocking the execution,assert that ( task1 running . await ( __num__ @$ time unit . seconds ) ) . is true ( ) ;,task running,success,pre
check if content <PLACE_HOLDER> minimum length requirement,if ( algorithm . equals ( tlsh ) && flow file . get size ( ) < __num__ ) { return false ; } else { return true ; },content matches,success,pre
say that a null label <PLACE_HOLDER> no positive pattern @$ but any negated patern,if ( lab == null ) { return negated pattern ; } else { if ( basic cat ) { lab = basic cat function . apply ( lab ) ; } matcher m = pattern . matcher ( lab ) ; return m . find ( ) != negated pattern ; },label matches,success,pre
if preflight timed out @$ m result will <PLACE_HOLDER> error code as int .,if ( total size < __num__ ) { return ( int ) total size ; } if ( more_debug ) { slog . v ( tag @$ __str__ + total size ) ; } i backup transport transport = m transport client . connect or throw ( __str__ ) ; result = transport . check full backup size ( total size ) ; if ( result == backup transport . transport_quota_exceeded ) { if ( more_debug ) { slog . d ( tag @$ __str__ + pkg . package name + __str__ + total size + __str__ + m quota ) ; } remote call . execute ( callback -> agent . do quota exceeded ( total size @$ m quota @$ callback ) @$ m agent timeout,result contain,success,pre
this property is n't <PLACE_HOLDER> anything this happens when you try to map an empty sequence to a property,if ( prop . ref ( ) . is empty ( ) ) return ;,property setting,fail,pre
the component has <PLACE_HOLDER> the focused range either if it is larger than half of the viewport and it occupies at least half of the viewport or if it is smaller than half of the viewport and it is fully visible .,return ( total component area >= half viewport area ) ? ( visible component area >= half viewport area ) : component bounds . equals ( component visible bounds ) ;,component entered,success,pre
calculate how many splits we need . as each task <PLACE_HOLDER> a separate split of data @$ so we want the number of splits equal to the number of tasks,int workergroup number = conf . get int ( angel conf . angel_workergroup_number @$ angel conf . default_angel_workergroup_number ) ; int task num in worker = conf . get int ( angel conf . angel_worker_task_number @$ angel conf . default_angel_worker_task_number ) ; int split num = workergroup number * task num in worker ; log . info ( __str__ + split num ) ; if ( ! use newapi ) { log . info ( __str__ ) ; org . apache . hadoop . mapred . input split [ ] split array = generate splits use oldapi ( conf @$ split num ) ; log . info ( __str__ + split array . length ) ; if ( log . is debug enabled ( ) ) { int,task has,fail,pre
usually happens when permission <PLACE_HOLDER> listing files in directory,log . e ( __str__ @$ __str__ + remote path @$ e ) ;,permission has,fail,pre
assume properties <PLACE_HOLDER> paths,if ( property value == null ) { if ( require property ) { throw new illegal argument exception ( __str__ + property key + __str__ + property key + __str__ ) ; } else { return null ; } } property value = get slashy path ( property value ) ; property value = correct double slash ( property value @$ property index end @$ str ) ; result += property value ; property index end ++ ; property index start = property index end ;,properties contain,success,pre
avoid output <PLACE_HOLDER> : cookie rejected,request config global config = request config . custom ( ) . set cookie spec ( cookie specs . ignore_cookies ) . build ( ) ; builder . set default request config ( global config ) ; closeable http client http client = builder . build ( ) ; return http client ;,output expect,fail,pre
merge input fields : data fields <PLACE_HOLDER> header fields,if ( fields from header != null ) { if ( ! ( fields from header instanceof point builder ) ) { throw new illegal state exception ( __str__ + fields from header ) ; } fields from data . merge with header ( ( point builder ) fields from header ) ; },fields override,success,pre
if the ancestor <PLACE_HOLDER> a forced preferred size @$ its layout manager may be able to give a good enough estimation .,if ( ancestor . is preferred size set ( ) ) { layout manager ancestor layout = ancestor . get layout ( ) ; if ( ancestor layout != null ) { dimension preferred layout size = ancestor layout . preferred layout size ( ancestor ) ; if ( preferred layout size != null ) { component = ancestor ; width = preferred layout size . width ; height = preferred layout size . height ; } } } else { dimension pref size = ancestor . get preferred size ( ) ; if ( pref size != null ) { component = ancestor ; width = pref size . width ; height = pref size . height ; } },ancestor has,success,pre
\u 00 a 5 and \uffe 5 are actually the same symbol @$ just different code points . but the ri <PLACE_HOLDER> the \uffe 5 and android <PLACE_HOLDER> those with \u 00 a 5,string [ ] yen = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; string [ ] dollar = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ;,ri breaks,fail,pre
add all udtf columns following sel will <PLACE_HOLDER> cp for columns from udtf @$ not adding sel in here,new cols . add all ( from column names ( output cols . sub list ( num sel columns @$ output cols . size ( ) ) ) ) ; op . get conf ( ) . set output internal col names ( to column names ( new cols ) ) ; prune operator ( ctx @$ op @$ new cols ) ; cpp ctx . get pruned col lists ( ) . put ( op @$ cols after replacement ) ; return null ;,sel change,fail,pre
for the best integration possible @$ we will ask ognl which property accessor it would use for this target object @$ and then depending on the result <PLACE_HOLDER> our equivalent or just default to ognl evaluation if it is a custom property accessor we do not implement .,final class < ? > target class = ognl runtime . get target class ( target ) ; final property accessor ognl property accessor = ognl runtime . get property accessor ( target class ) ;,accessor find,fail,pre
this can only happen if one of our entries <PLACE_HOLDER> an unsupported name .,throw wrapper . invalid ctb converter name ( ucne @$ codeset . get name ( ) ) ;,one has,success,pre
walk the args : the various drivers <PLACE_HOLDER> unpacked versions of the elements,object [ ] used args = new object [ args . length ] ; for ( int i = __num__ ; i < args . length ; i ++ ) { used args [ i ] = unpack wrapped element ( args [ i ] ) ; } return used args ;,drivers create,fail,pre
view table can not <PLACE_HOLDER> ttl columns,if ( m_in strict mat view diff mode ) { return __str__ ; },table have,success,pre
proxy servlet will receive an absolute uri from the client @$ and convert it to a relative uri . the connect handler wo n't modify what the client <PLACE_HOLDER> @$ which must be a relative uri .,assert that ( request . length ( ) @$ matchers . greater than ( __num__ ) ) ; if ( server ssl context factory == null ) assert false ( request . contains ( __str__ ) ) ; else assert false ( request . contains ( __str__ ) ) ; string response = __str__ + __str__ + __str__ ; get end point ( ) . write ( callback . noop @$ byte buffer . wrap ( response . get bytes ( standard charsets . utf_8 ) ) ) ;,client decides,fail,pre
create dummy get all elements operation as some of the methods in accumulo store test if the operation is a get all elements operation and if so set some options . we need those options if operation is <PLACE_HOLDER> all the elements .,if ( operation instanceof getrdd of all elements ) { derived operation = get get all elements ( operation ) ; } else { derived operation = operation ; },operation repeating,fail,pre
client 1 did not <PLACE_HOLDER> interest,await ( ) . until asserted ( ( ) -> { assert that ( client1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ ) ; assert that ( client2 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ ) ; assert that ( server1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ ) ; assert that ( server2 . invoke ( (,client register,success,pre
document can <PLACE_HOLDER> null value by key,if ( doc . contains key ( field ) ) { modify field with dot notation ( doc @$ field ) ; return ; } list < field name and value > new fields = null ; iterator < map . entry < string @$ object > > it = doc . entry set ( ) . iterator ( ) ; while ( it . has next ( ) ) { map . entry < string @$ object > entry = it . next ( ) ; string [ ] origin key nodes = dot . split ( entry . get key ( ) ) ; string [ ] key nodes = exclude numeric items ( origin key nodes ) ; if ( key nodes . length >,document contain,success,pre
search for issues of project 1 <PLACE_HOLDER> less than 14 days and project 2 <PLACE_HOLDER> less then 25 days,assert that search returns only ( issue query . builder ( ) . created after by project uuids ( immutable map . of ( project1 . uuid ( ) @$ new issue query . period start ( add days ( now @$ - __num__ ) @$ true ) @$ project2 . uuid ( ) @$ new issue query . period start ( add days ( now @$ - __num__ ) @$ true ) ) ) @$ project1 issue1 . key ( ) @$ project2 issue1 . key ( ) ) ;,days having,success,pre
verify that an invoke method <PLACE_HOLDER> ok after a pop frames in a thread suspended by an event .,main thread . pop frames ( frame for ( __str__ ) ) ; system . out . println ( __str__ ) ; system . out . println ( __str__ ) ; method invokeee method = ( method ) target class . methods by name ( __str__ ) . get ( __num__ ) ; try { target class . invoke method ( main thread @$ invokeee method @$ new array list ( ) @$ __num__ ) ; } catch ( exception ex ) { failure ( __str__ + ex ) ; ex . print stack trace ( ) ; } system . out . println ( __str__ ) ;,method comes,fail,pre
entry does not have <PLACE_HOLDER> note : ca certs can have no <PLACE_HOLDER> open ssl generates pkcs 12 with no attr for ca certs .,attr set = null ;,entry attribute,fail,pre
relative paths <PLACE_HOLDER> no bucket .,return new gcs path ( fs @$ __str__ @$ component ) ;,paths have,success,pre
test using default value for the string . individually specified env <PLACE_HOLDER>erties do not have defaults @$ so this should just get things from the default <PLACE_HOLDER> name string .,string bogus prop = prop name + __str__ ; apps . set env from input property ( env @$ bogus prop @$ default prop value @$ conf @$ file . path separator ) ;,things file,fail,pre
this configuration does not <PLACE_HOLDER> commands,return null ;,configuration support,fail,pre
more complicated . vh 2 <PLACE_HOLDER> a higher version @$ but <PLACE_HOLDER> some exceptions that vh 1 does not have .,region version holder vh1 = new region version holder ( member ) ; region version holder vh2 = new region version holder ( member ) ; bit set bs1 = new bit set ( ) ; bs1 . set ( __num__ @$ __num__ ) ; bs1 . set ( __num__ @$ __num__ ) ; record versions ( vh1 @$ bs1 ) ; bit set bs2 = new bit set ( ) ; bs2 . set ( __num__ @$ __num__ ) ; bs2 . set ( __num__ @$ __num__ ) ; record versions ( vh2 @$ bs2 ) ;,vh has,success,pre
confirm both iterables <PLACE_HOLDER> the same number of elements,assert . assert equals ( all points list . size ( ) @$ iterable with size . size ( ) ) ;,iterables return,fail,pre
the caller must <PLACE_HOLDER> lock to synchronize the update .,final ref count += n ; if ( final ref count > peak final ref count ) { peak final ref count = final ref count ; },caller have,fail,pre
this is to see if the server <PLACE_HOLDER> any listeners,try { orb port info [ ] serverorb and port list = entry . lookup ( iiop_clear_text . value ) ; } catch ( exception exc ) { return null ; },server has,success,pre
ok @$ so perhaps @$ we can <PLACE_HOLDER> the arguments from a previous execution ?,string [ ] saved = props . is initialized ( ) ? props . get instance ( ) . get last arguments ( ) : null ;,ok use,success,pre
no rows <PLACE_HOLDER> purge criteria,verify proc fails ( client @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __num__ ) ; client response cr = client . call procedure ( __str__ @$ __str__ + __str__ + __str__ ) ; assert equals ( client response . success @$ cr . get status ( ) ) ;,rows match,success,pre
2 range conditions are <PLACE_HOLDER>d on different columns @$ but not all sql databases properly optimize it . some databases can only <PLACE_HOLDER> an index on one of the columns . an additional condition provides explicit knowledge that 'start ' can not be greater than 'end ' .,list < data segment > matching segments = connector . in read only transaction ( new transaction callback < list < data segment > > ( ) { @ override public list < data segment > in transaction ( final handle handle @$ final transaction status status ) { return handle . create query ( string utils . format ( __str__ + __str__ @$ db tables . get segments table ( ) @$ connector . get quote string ( ) ) ) . set fetch size ( connector . get streaming fetch size ( ) ) . bind ( __str__ @$ data source ) . bind ( __str__ @$ interval . get start ( ) . to string ( ) ) . bind ( __str__ @$ interval .,databases allow,fail,pre
we treat calls from a profile as if made by its parent as profiles share the accessibility state of the parent . the call below <PLACE_HOLDER> the current profile parent resolution .,synchronized ( m lock ) { final int resolved user id = m security policy . resolve calling user id enforcing permissions locked ( user id ) ; user state user state = get user state locked ( resolved user id ) ; client client = new client ( callback @$ binder . get calling uid ( ) @$ user state ) ; if ( m security policy . is caller interacting across users ( user id ) ) { m global clients . register ( callback @$ client ) ; if ( debug ) { slog . i ( log_tag @$ __str__ + binder . get calling pid ( ) ) ; } return int pair . of ( user state . get client state ( ),call performs,success,pre
announce what cloud we think we are in . <PLACE_HOLDER> our health as well .,udp heartbeat . build_and_multicast ( cloud @$ hb ) ;,announce send,fail,pre
tests <PLACE_HOLDER> maximum version,stores to versions = get admin client ( ) . readonly ops . getro max version ( __num__ @$ lists . new array list ( __str__ @$ __str__ ) ) ; assert equals ( stores to versions . size ( ) @$ __num__ ) ; assert equals ( stores to versions . get ( __str__ ) . long value ( ) @$ __num__ ) ; assert equals ( stores to versions . get ( __str__ ) . long value ( ) @$ __num__ ) ;,tests get,success,pre
make sure a status update <PLACE_HOLDER> report progress,map task status mock status = new map task status ( attemptid @$ __num__ @$ __num__ @$ task status . state . running @$ __str__ @$ __str__ @$ __str__ @$ task status . phase . map @$ new counters ( ) ) ; feedback = listener . status update ( attemptid @$ mock status ) ; assert true ( feedback . get task found ( ) ) ; verify ( hb handler ) . progressing ( eq ( attempt id ) ) ;,update guarantees,fail,pre
if obj <PLACE_HOLDER> unicast remote object @$ set its ref .,if ( obj instanceof unicast remote object ) { ( ( unicast remote object ) obj ) . ref = sref ; } return sref . export object ( obj @$ null @$ false ) ;,obj implements,fail,pre
this simulates the completion of txnid : id txn <PLACE_HOLDER> 1,long write id = txn mgr2 . get table write id ( __str__ @$ __str__ ) ; add dynamic partitions adp = new add dynamic partitions ( txn mgr2 . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr2 . commit txn ( ) ;,txn update,success,pre
if this component already has focus @$ then activate the input method by dispatching a synthesized focus <PLACE_HOLDER> event .,if ( is focus owner ( ) ) { input context input context = get input context ( ) ; if ( input context != null ) { focus event focus gained event = new focus event ( this @$ focus event . focus_gained ) ; input context . dispatch event ( focus gained event ) ; } } event mask |= awt event . input_methods_enabled_mask ; if ( ( event mask & awt event . input_methods_enabled_mask ) != __num__ ) { input context input context = get input context ( ) ; if ( input context != null ) { input context . end composition ( ) ; input context . remove notify ( this ) ; } } event mask &= ~ awt event . input_methods_enabled_mask,focus gained,success,pre
skip notes that match csum but not key <PLACE_HOLDER> copy of note to every position in duplicates array corresponding to the current key,if ( key to indexes map . contains key ( note . get key ( ) ) ) { list < integer > output pos = key to indexes map . get ( note . get key ( ) ) ; for ( int i = __num__ ; i < output pos . size ( ) ; i ++ ) { add note to duplicates array ( i > __num__ ? new note info ( note ) : note @$ duplicates @$ output pos . get ( i ) ) ; } },notes add,success,pre
if this project has <PLACE_HOLDER> reference @$ create value generator and produce the <PLACE_HOLDER> variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel ) ; },project correlated,success,pre
case . it consists of two arrays of lines . the first array of lines is the test input @$ and the second one is the expected output . if the second array <PLACE_HOLDER> a single element starting with ! ! then it is expected that import orderer will throw a formatter exception with that message . if a line ends with \ then,string [ ] [ ] [ ] inputs outputs = { { { } @$ { } } @$ { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__,array has,success,pre
check that class does not <PLACE_HOLDER> the same parameterized interface with two different argument lists .,chk . check class bounds ( tree . pos ( ) @$ c . type ) ; tree . type = c . type ; for ( list < jc type parameter > l = tree . typarams ; l . non empty ( ) ; l = l . tail ) { assert . check non null ( env . info . scope . lookup ( l . head . name ) . scope ) ; },class have,fail,pre
copy local variables to event scope execution by value . this way @$ the event scope execution <PLACE_HOLDER> a 'snapshot ' of the local variables,new sub process variable snapshotter ( ) . set variables snapshots ( sub process execution @$ event scope execution ) ;,execution references,success,pre
normal template literals still <PLACE_HOLDER> error,parse error ( __str__ @$ __str__ ) ;,literals throw,success,pre
let 's <PLACE_HOLDER> the response packet,datagram packet resp packet = make response packet ( packet ) ;,'s make,fail,pre
for local queries returning pdx objects wrap the resultset with results collection pdx deserializer wrapper which <PLACE_HOLDER> these pdx objects .,if ( needspdx deserialization wrapper ( true ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ false ) ; },which wraps,fail,pre
figure out what type of glob path was given @$ will <PLACE_HOLDER> globbed paths to match the type to preserve relativity,path type glob type ; uri glob uri = glob path . to uri ( ) ; if ( glob uri . get scheme ( ) != null ) { glob type = path type . has_scheme ; } else if ( ! glob uri . get path ( ) . is empty ( ) && new path ( glob uri . get path ( ) ) . is absolute ( ) ) { glob type = path type . schemeless_absolute ; } else { glob type = path type . relative ; },figure prefer,fail,pre
special <PLACE_HOLDER> gpu,if ( key . equals ( __str__ ) ) { key = resource information . gpu_uri ; },special handle,success,pre
when the encoded graph <PLACE_HOLDER> methods inlining @$ we can already have a proper caller state . if not @$ we set the caller state here .,if ( outer state . outer frame state ( ) == null && method scope . caller != null ) { ensure outer state decoded ( method scope . caller ) ; outer state . set outer frame state ( method scope . caller . outer state ) ; } method scope . outer state = outer state ;,graph contains,fail,pre
assert the package tracker <PLACE_HOLDER> an update .,check update check triggered ( new package versions ) ; check token token2 = m fake intent helper . capture and reset last token ( ) ;,tracker did,fail,pre
each application <PLACE_HOLDER> 10 5 gb containers,am1 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; am2 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; am3 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; capacity scheduler cs = ( capacity scheduler ) rm1 . get resource scheduler ( ) ; rm node rm node1 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ; fi ca scheduler app scheduler app1 = cs . get application attempt ( am1 . get application attempt id ( ) ) ; fi ca scheduler app scheduler app2 = cs . get application attempt ( am2 . get,application asks,success,pre
test <PLACE_HOLDER> that cross checksum boundary,stm . seek ( __num__ ) ; assert equals ( stm . get pos ( ) @$ __num__ ) ; stm . read fully ( actual @$ __num__ @$ half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ half_chunk_size ) ; stm . read fully ( actual @$ half_chunk_size @$ block_size - half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ block_size ) ; stm . read fully ( actual @$ block_size @$ bytes_per_sum + half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ block_size + bytes_per_sum + half_chunk_size ) ; stm . read fully ( actual @$ __num__ * block_size - half_chunk_size @$ file_size - ( __num__ * block_size - half_chunk_size ) ) ; assert,test read,fail,pre
null value will always <PLACE_HOLDER> null quotes,if ( value == null ) { this . value quotes = null ; } else { if ( value quotes == null ) { this . value quotes = attribute value quotes . double ; } else if ( value quotes == attribute value quotes . none && value . length ( ) == __num__ ) { this . value quotes = attribute value quotes . double ; } else { this . value quotes = value quotes ; } },value emit,fail,pre
block until submissions <PLACE_HOLDER> further work,if ( m_clients pending notification . is empty ( ) ) { run submissions ( true ) ; m_limiter = rate limiter . create ( notification_rate @$ warmup_ms @$ time unit . milliseconds ) ; } else { run submissions ( false ) ; },submissions have,fail,pre
if map has a null comparator @$ the keys should <PLACE_HOLDER> a natural ordering @$ even though k does n't explicitly implement comparable .,if ( comparator == null ) { comparator = ( comparator < ? super k > ) natural_order ; },keys generate,fail,pre
when a runtime exception gets thrown out @$ this provider will get called again if the object is asked for again . this will have the same failed result @$ 'cause when it 's called no parameters will have actually changed . guice will then <PLACE_HOLDER> the same error multiple times @$ which is pretty annoying . cache a null supplier and return that,ret val = suppliers . of instance ( null ) ; throw e ;,times throw,fail,pre
buffer wo n't <PLACE_HOLDER> the uid if the caller does n't have permission .,return - __num__ ;,buffer contain,success,pre
always a good idea to <PLACE_HOLDER> up default uniforms ...,blur shader . use ( ) ; blur shader . set uniformf ( __str__ @$ __num__ @$ __num__ ) ;,idea set,success,pre
failure during parameter serialization ; call may have <PLACE_HOLDER> server @$ so call retry not possible .,throw e ;,call terminated,fail,pre
now remove first region in table t 2 to see if catalogjanitor <PLACE_HOLDER> notices .,list < region info > t2 ris = meta table accessor . get table regions ( test_util . get connection ( ) @$ t2 ) ; meta table accessor . delete region info ( test_util . get connection ( ) @$ t2 ris . get ( __num__ ) ) ; gc = janitor . scan ( ) ; report = janitor . get last report ( ) ; assert false ( report . is empty ( ) ) ; assert equals ( __num__ @$ report . get holes ( ) . size ( ) ) ; assert true ( report . get holes ( ) . get ( __num__ ) . get first ( ) . get table ( ) . equals ( t1 ) ) ; assert,catalogjanitor object,fail,pre
let 's <PLACE_HOLDER> child nodes,if ( cce . node count ( ) > __num__ ) { add loopx path ( cnode @$ monitor ) ; },'s add,fail,pre
each test will <PLACE_HOLDER> the graph as needed,test visual graph g = new test visual graph ( ) ; return g ;,test initialize,fail,pre
log if the installed pkg <PLACE_HOLDER> a higher version number .,if ( existing pkg info != null ) { if ( existing pkg info . get long version code ( ) == pkg . get long version code ( ) ) { if ( skip if same version ) { log . w ( tag @$ __str__ + pkg . get long version code ( ) + __str__ + package name + __str__ ) ; return ; } else { log . w ( tag @$ __str__ + pkg . get long version code ( ) + __str__ + package name ) ; } } else if ( existing pkg info . get long version code ( ) > pkg . get long version code ( ) ) { if ( skip if lower version ) { log,pkg has,success,pre
this map will <PLACE_HOLDER> pegasus options.generation mode to project property,project . get extensions ( ) . get extra properties ( ) . set ( __str__ @$ arrays . stream ( pegasus options . generation mode . values ( ) ) . collect ( collectors . to map ( pegasus options . generation mode :: name @$ function . identity ( ) ) ) ) ; synchronized ( static_project_evaluated_lock ) { if ( ! project . get root project ( ) . has property ( run_once ) || ! boolean . parse boolean ( string . value of ( project . get root project ( ) . property ( run_once ) ) ) ) { project . get gradle ( ) . projects evaluated ( gradle -> gradle . get root project ( ) . subprojects ( subproject,map apply,fail,pre
ok we have a real file <PLACE_HOLDER> the local filename,this . gpgexe = kettlevfs . get filename ( file ) ; try { if ( file != null ) { file . close ( ) ; } } catch ( exception e ) { },file use,fail,pre
user <PLACE_HOLDER> the memory for local mode hadoop run,console . print info ( __str__ + hadoop mem + __str__ ) ; variables . put ( hadoop_mem_key @$ string . value of ( hadoop mem ) ) ;,user specified,success,pre
test that offering another element preserves the priority <PLACE_HOLDER> semantics .,assert offer ( queue @$ e ) ; assert equals ( __num__ @$ queue . size ( ) ) ; assert same ( a @$ queue . peek ( ) ) ; assert same ( a @$ queue . poll ( ) ) ; assert equals ( __num__ @$ queue . size ( ) ) ;,priority change,fail,pre
this request <PLACE_HOLDER> a bad acl @$ so we are dismissing it early .,if ( err != keeper exception . code . ok . int value ( ) ) { dec in process ( ) ; reply header rh = new reply header ( request . cxid @$ __num__ @$ err ) ; try { request . cnxn . send response ( rh @$ null @$ null ) ; } catch ( io exception e ) { log . error ( __str__ @$ e ) ; } },request has,success,pre
user 2 <PLACE_HOLDER> the new room,try { multi user chat muc2 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc2 . join ( __str__ ) ; multi user chat muc3 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc3 . join ( __str__ ) ; muc . grant ownership ( get barejid ( __num__ ) ) ; muc . grant moderator ( __str__ ) ; collection < affiliate > affiliates = muc . get owners ( ) ; assert equals ( __str__ @$ __num__ @$ affiliates . size ( ) ) ; for ( affiliate affiliate1 : affiliates ) { if ( get barejid ( __num__ ) . equals ( affiliate1 . get jid ( ) ) ) {,user joins,success,pre
the file only <PLACE_HOLDER> one block,located block lblock = locatedblocks . get ( __num__ ) ; datanode info [ ] datanodeinfos = lblock . get locations ( ) ; extended block block = lblock . get block ( ) ;,file contains,fail,pre
dismiss the grouped notification if a user <PLACE_HOLDER> all notifications from a wear device,if ( ! mgcm message handler . has notifications ( ) ) { notification manager compat notification manager = notification manager compat . from ( m context ) ; notification manager . cancel ( group_notification_id ) ; },user chooses,fail,pre
verify owner <PLACE_HOLDER> active tx lock,d lock batch [ ] batches = grantor . get lock batches ( owner ) ; if ( batches . length == __num__ ) { logger . debug ( __str__ @$ owner ) ; return ; } send recovery msgs ( dlock . get distribution manager ( ) @$ batches @$ owner @$ grantor ) ;,owner has,success,pre
visibility @$ like all package groups @$ does n't <PLACE_HOLDER> a configuration,nested set < package group contents > visibility = convert visibility ( prerequisite map @$ analysis environment . get event handler ( ) @$ target @$ null ) ; if ( target instanceof output file ) { output file output file = ( output file ) target ; target context target context = new target context ( analysis environment @$ target @$ config @$ prerequisite map . get ( dependency resolver . output_file_rule_dependency ) @$ visibility ) ; if ( analysis environment . get skyframe env ( ) . values missing ( ) ) { return null ; } rule configured target rule = ( rule configured target ) target context . find direct prerequisite ( output file . get generating rule ( ) . get label (,visibility require,fail,pre
skip gc <PLACE_HOLDER> weak refs,for ( hash entry < k @$ v > p = e ; p != last run ; p = p . next ) { k key = p . key ref . get ( ) ; if ( key == null ) { reduce ++ ; continue ; } int k = p . hash & size mask ; hash entry < k @$ v > n = new table [ k ] ; new table [ k ] = new hash entry < k @$ v > ( key @$ p . hash @$ n @$ p . value @$ ref queue ) ; },gc ' d,fail,pre
puts @$ gets @$ <PLACE_HOLDER> @$ unused @$ unused .,@ suppress warnings ( __str__ ) atomic integer [ ] counts = { in . caches . put count @$ is ppd ? in . caches . get by expr count : in . caches . get count @$ is ppd ? in . caches . get hit by expr count : in . caches . get hit count @$ is ppd ? in . caches . get count : in . caches . get by expr count @$ is ppd ? in . caches . get hit count : in . caches . get hit by expr count } ; verify splits ( original hs @$ splits ) ; verify call counts ( counts @$ __num__ @$ __num__ @$ __num__ ) ; assert equals ( __num__ @$,puts includes,fail,pre
we need to overwrite the default <PLACE_HOLDER> factory @$ which does not expand views . the expanding <PLACE_HOLDER> factory uses the flink planner to translate a view into a rel tree @$ before applying any subsequent rules .,context chain = contexts . chain ( context @$ contexts . of ( rel factories . expanding scan factory ( create flink planner ( current catalog @$ current database ) @$ rel factories . default_table_scan_factory ) ) ) ;,default scan,success,pre
the url has been seen before . <PLACE_HOLDER> listeners with the new distance estimate .,if ( m nearby urls . contains ( url info . get url ( ) ) ) { if ( url info . get distance ( ) >= __num__ && m pws result map . contains key ( url info . get url ( ) ) ) { safe notify native listeners on distance changed ( url info . get url ( ) @$ url info . get distance ( ) ) ; } return ; },url notify,success,pre
it 's fine to just check departure time @$ as the above pass ensures that all stop times <PLACE_HOLDER> either both arrival and departure times @$ or neither,if ( stop times [ __num__ ] . departure_time == entity . int_missing || stop times [ stop times . length - __num__ ] . departure_time == entity . int_missing ) { throw new first and last stops do not have times ( ) ; },times contains,fail,pre
the linker <PLACE_HOLDER> keys to bindings by their type .,@ suppress warnings ( __str__ ) binding < t > binding = ( binding < t > ) get injectable type binding ( class loader @$ injectable type key @$ key ) ;,linker matches,success,pre
now let 's <PLACE_HOLDER> fords for foot,foot encoder . set block fords ( boolean . false ) ; node = new reader node ( __num__ @$ - __num__ @$ - __num__ ) ; node . set tag ( __str__ @$ __str__ ) ; assert true ( foot encoder . handle node tags ( node ) == __num__ ) ; node = new reader node ( __num__ @$ - __num__ @$ - __num__ ) ; node . set tag ( __str__ @$ __str__ ) ; assert true ( foot encoder . handle node tags ( node ) == __num__ ) ;,'s allow,success,pre
the semantic graph does n't explicitly <PLACE_HOLDER> the root node @$ so we print that out ourselves,for ( indexed word root : graph . get roots ( ) ) { string rel = grammatical relation . root . get long name ( ) ; rel = rel . replace all ( __str__ @$ __str__ ) ; int source = __num__ ; int target = root . index ( ) ; string source word = __str__ ; string target word = tokens . get ( target - __num__ ) . word ( ) ; final boolean is extra = false ; add dependency info ( dep info @$ rel @$ is extra @$ source @$ source word @$ null @$ target @$ target word @$ null @$ curns ) ; },graph show,fail,pre
the following code <PLACE_HOLDER> an aes cipher to encrypt the message .,final buffered block cipher cipher = new padded buffered block cipher ( new cbc block cipher ( new aes fast engine ( ) ) ) ; cipher . init ( true @$ key ) ; final byte [ ] encrypted bytes = new byte [ cipher . get output size ( plain text as bytes . length ) ] ; final int process len = cipher . process bytes ( plain text as bytes @$ __num__ @$ plain text as bytes . length @$ encrypted bytes @$ __num__ ) ; final int do final len = cipher . do final ( encrypted bytes @$ process len ) ;,code uses,success,pre
this creates the ephemeral sequential node with host id 0 which this node already <PLACE_HOLDER> for itself . just recording that fact .,final int selected host id = select new host id ( m_config . coordinator ip . to string ( ) ) ; if ( selected host id != __num__ ) { org . voltdb . voltdb . crash local voltdb ( __str__ + selected host id @$ false @$ null ) ; },node has,fail,pre
if baseline provider <PLACE_HOLDER> both current values and baseline values @$ using them as the result,if ( df baseline . contains ( col_current ) ) { df aligned = df baseline ; df aligned . rename series ( col_value @$ col_baseline ) ; } else { data frame df current = this . time series loader . load ( slice view current ) ; df aligned = df current . rename series ( col_value @$ col_current ) . join outer ( df baseline . rename series ( col_value @$ col_baseline ) ) ; },provider contains,fail,pre
sometimes the network connection notifier lags the actual connection type @$ especially when the gcm nm wakes us from doze state . if we are really connected @$ <PLACE_HOLDER> the connection type from android .,if ( connection type == connection type . connection_none ) { connectivity manager cm = ( connectivity manager ) context . get system service ( context . connectivity_service ) ; network info active network = cm . get active network info ( ) ; boolean is connected = active network != null && active network . is connected or connecting ( ) ; if ( is connected ) { connection type = convert android network type to connection type ( active network . get type ( ) ) ; } },type change,fail,pre
this test <PLACE_HOLDER> two task attempts @$ but uberization overrides max to 1,conf . set boolean ( mr job config . job_ubertask_enable @$ false ) ; job job = app . submit ( conf ) ; app . wait for state ( job @$ job state . succeeded ) ; map < task id @$ task > tasks = job . get tasks ( ) ; assert . assert equals ( __str__ @$ __num__ @$ tasks . size ( ) ) ; task task = tasks . values ( ) . iterator ( ) . next ( ) ; assert . assert equals ( __str__ @$ task state . succeeded @$ task . get report ( ) . get task state ( ) ) ; map < task attempt id @$ task attempt > attempts = tasks . values (,test does,fail,pre
complete the task on the same level should not <PLACE_HOLDER> the deletion of the user event listener,cmmn task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; assert that ( cmmn runtime service . create user event listener instance query ( ) . case instance id ( case instance . get id ( ) ) . count ( ) ) . is equal to ( __num__ ) ;,task trigger,success,pre
split does not <PLACE_HOLDER> consumed parallelism ; consumed parallelism is still 0 .,assert equals ( __num__ @$ consumed parallelism from progress ( iter . get progress ( ) ) @$ __num__ ) ;,split change,fail,pre
delegate already <PLACE_HOLDER> pk columns,if ( get factory ( ) . get session factory options ( ) . is comments enabled ( ) ) { insert . set comment ( __str__ + get entity name ( ) ) ; } return insert . to statement string ( ) ;,delegate handles,success,pre
when instance <PLACE_HOLDER> second time,instance id refresh id = registry . register ( registration . create ( __str__ @$ __str__ ) . build ( ) ) . block ( ) ; assert that ( refresh id ) . is equal to ( id ) ; step verifier . create ( registry . get instance ( id ) ) . assert next ( ( registered ) -> { assert that ( registered . get info ( ) ) . is equal to ( info ) ; assert that ( registered . get status info ( ) ) . is equal to ( status ) ; } ) . verify complete ( ) ;,instance occurs,fail,pre
start at 1 @$ since a miss in the open object int hash map <PLACE_HOLDER> a 0,int i = __num__ ;,map gives,fail,pre
class file to write @$ if directory then <PLACE_HOLDER> the name of the input,path output = paths . get ( args [ __num__ ] ) ; if ( files . is directory ( output ) ) output = output . resolve ( input . get file name ( ) ) ;,directory use,success,pre
if the requestable capabilities have n't <PLACE_HOLDER> @$ and the score has n't <PLACE_HOLDER> @$ then the change we 're processing ca n't affect any requests @$ it can only affect the listens on this network . we might have been called by rematch network and requests when a network <PLACE_HOLDER> foreground state .,if ( nai . get current score ( ) == old score && new nc . equal requestable capabilities ( prev nc ) ) { process listen requests ( nai ) ; } else { rematch all networks and requests ( ) ; notify network callbacks ( nai @$ connectivity manager . callback_cap_changed ) ; },network changed,success,pre
default <PLACE_HOLDER> namenode mbeans only,jmx . init ( ) ;,default accept,fail,pre
client <PLACE_HOLDER> response,assert equals ( __num__ @$ client . get input stream ( ) . read ( ) ) ;,client reads,success,pre
if the request <PLACE_HOLDER> a term that is greater than the current term then assign that term and leader to the current context .,boolean transition = update term and leader ( request . term ( ) @$ null ) ; completable future < vote response > future = completable future . completed future ( log response ( handle vote ( request ) ) ) ; if ( transition ) { raft . transition ( raft server . role . follower ) ; } return future ;,request indicates,success,pre
do not invoke class descriptor <PLACE_HOLDER> hook with old protocol,if ( protocol == protocol_version_1 ) { desc . write non proxy ( this ) ; } else { write class descriptor ( desc ) ; },descriptor write,success,pre
process <PLACE_HOLDER> requests .,for ( long file id : m persist requests . key set ( ) ) { if ( thread . interrupted ( ) ) { throw new interrupted exception ( __str__ ) ; } boolean remove = true ; alluxio . time . exponential timer timer = m persist requests . get ( file id ) ; if ( timer == null ) { continue ; } alluxio . time . exponential timer . result timer result = timer . tick ( ) ; if ( timer result == alluxio . time . exponential timer . result . not_ready ) { continue ; } alluxiouri uri = null ; try { try ( locked inode path inode path = m inode tree . lock full inode path (,process persist,success,pre
worker is still <PLACE_HOLDER> this task,task location task location = task runner work item . get location ( ) ; final url url = task runner utils . make task locationurl ( task location @$ __str__ @$ task id ) ; return optional . of ( new byte source ( ) { @ override public input stream open stream ( ) throws io exception { try { return http client . go ( new request ( http method . get @$ url ) @$ new input stream response handler ( ) ) . get ( ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; } catch ( execution exception e ) { throwables . propagate if possible ( e . get cause ( ),worker running,success,pre
check that both animators ' listeners have <PLACE_HOLDER> the animation callbacks .,assert true ( l1 . start called ) ; assert true ( l1 . end called ) ; assert false ( a1 . is started ( ) ) ; assert true ( l1 . end time >= l1 . start time ) ; assert true ( l2 . start called ) ; assert true ( l2 . end called ) ; assert false ( a2 . is started ( ) ) ; assert true ( l2 . end time >= l1 . start time ) ;,listeners handled,fail,pre
invalid port number <PLACE_HOLDER> an error .,check error state ( new ignite callable < void > ( ) { @ override public void call ( ) throws exception { driver manager . get connection ( __str__ ) ; return null ; } } @$ __str__ @$ __str__ ) ;,number yields,success,pre
name of the attribute <PLACE_HOLDER> the user roles . if not specified @$ this defaults to roles .,module options . put ( __str__ @$ __str__ ) ;,name provides,fail,pre
at this point the clone is complete . next step is <PLACE_HOLDER> the table .,string msg = __str__ + snapshot . get name ( ) + __str__ + table name + __str__ ; log . info ( msg ) ; monitor status . set status ( msg + __str__ ) ;,step setting,fail,pre
historic rules may <PLACE_HOLDER> null entries when original zoneinfo data includes non transition data .,if ( historic rules != null ) { for ( int i = __num__ ; i < historic rules . length ; i ++ ) { if ( historic rules [ i ] != null ) { size ++ ; } } },rules contain,success,pre
xshift and yshift <PLACE_HOLDER> the amount & direction to shift the tab in their respective axis .,for ( int j = start + __num__ ; j <= end ; j ++ ) { int xshift = __num__ ; int yshift = __num__ ; switch ( tab pane . get tab placement ( ) ) { case j tabbed pane . top : case j tabbed pane . bottom : xshift = ltr ? tab overlap : - tab overlap ; break ; case j tabbed pane . left : case j tabbed pane . right : yshift = tab overlap ; break ; default : } rects [ j ] . x += xshift ; rects [ j ] . y += yshift ; rects [ j ] . width += math . abs ( xshift ) ; rects [ j ] . height,xshift indicate,fail,pre
if we 're becoming visible @$ immediately change client visibility as well . there seem to be some edge cases where we change our visibility but client visibility never <PLACE_HOLDER> updated . if we 're becoming invisible @$ update the client visibility if we are not running an animation . otherwise @$ we 'll update client visibility in on animation finished .,if ( visible || ! is really animating ( ) ) { set client hidden ( ! visible ) ; } if ( ! get display content ( ) . m closing apps . contains ( this ) && ! get display content ( ) . m opening apps . contains ( this ) ) { get display content ( ) . get docked divider controller ( ) . notify app visibility changed ( ) ; m wm service . m task snapshot controller . notify app visibility changed ( this @$ visible ) ; },visibility gets,success,pre
inline editor is n't supported but panel viewer is <PLACE_HOLDER> panel,if ( array utils . contains ( supported edit types @$ i value controller . edit type . panel ) ) { controller . activate panel ( value viewer panel . panel_id @$ true @$ true ) ; return null ; },viewer showing,fail,pre
the number of <PLACE_HOLDER> bytes,int got = skipped + cis . read ( result @$ __num__ @$ __num__ ) ;,number got,success,pre
can this left constituent <PLACE_HOLDER> space for a right constituent ?,boolean i possiblel = ( narrowr < end ) ;,constituent leave,success,pre
check if any of the references <PLACE_HOLDER> the module boundaries .,if ( check modules && ref . module != null ) { if ( ref . module != fn module && ! module graph . depends on ( ref . module @$ fn module ) ) { is removable = false ; check modules = false ; } },any cross,success,pre
cleanup session <PLACE_HOLDER> directory .,cleanup session log dir ( ) ; hive history hive hist = session state . get hive history ( ) ; if ( null != hive hist ) { hive hist . close stream ( ) ; } try { session state . reset thread name ( ) ; session state . close ( ) ; } finally { session state = null ; } if ( session state != null ) { try { session state . reset thread name ( ) ; session state . close ( ) ; } catch ( throwable t ) { log . warn ( __str__ @$ t ) ; } session state = null ; } if ( session hive != null ) { try { session hive . close,session log,success,pre
let evaluation prints stats how often the right output <PLACE_HOLDER> the highest value,evaluation eval = new evaluation ( ) ; eval . eval ( ds . get labels ( ) @$ output ) ; system . out . println ( eval . stats ( ) ) ;,output has,fail,pre
if the scan <PLACE_HOLDER> all rows @$ we can throw away the scan nodes and use a truncate delete node .,if ( delete is truncate ( m_parsed delete @$ sub select root ) ) { delete node . set truncate ( true ) ; } else { if ( m_parsed delete . order by columns ( ) . size ( ) > __num__ && ! is single partition plan && ! target table . get isreplicated ( ) ) { throw new planning error exception ( __str__ + __str__ + __str__ + __str__ ) ; } boolean needs order by node = is order by node required ( m_parsed delete @$ sub select root ) ; abstract expression address expr = new tuple address expression ( ) ; node schema proj_schema = new node schema ( ) ; proj_schema . add column ( abstract parsed stmt . temp_table_name,scan contains,fail,pre
note : each event loop <PLACE_HOLDER> its own connection pool .,factory with pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( true ) . build ( ) ; factory without pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( false ) . build ( ) ;,loop has,success,pre
this is technically a recursive constraint for cast @$ but type registry.can cast <PLACE_HOLDER> explicit handling for row to row cast,super ( cast @$ immutable list . of ( new type variable constraint ( __str__ @$ false @$ false @$ __str__ @$ immutable set . of ( new type signature ( __str__ ) ) @$ immutable set . of ( ) ) @$ with variadic bound ( __str__ @$ __str__ ) ) @$ immutable list . of ( ) @$ new type signature ( __str__ ) @$ immutable list . of ( new type signature ( __str__ ) ) @$ false ) ;,cast has,success,pre
if this instruction <PLACE_HOLDER> a delay slot @$ adjust max addr accordingly,if ( instr . get prototype ( ) . has delay slots ( ) ) { max addr = instr . get min address ( ) . add ( instr . get default fall through offset ( ) - __num__ ) ; } v context . set current instruction ( instr ) ; v context . flow to address ( flow from addr @$ max addr ) ; if ( evaluator != null ) { if ( evaluator . evaluate context before ( v context @$ instr ) ) { body . add ( conflicts ) ; return body ; } },instruction has,success,pre
makes it more easy to see which <PLACE_HOLDER> blocks process in thread dump,thread . current thread ( ) . set name ( __str__ + uri . get host ( ) ) ;,which runs,fail,pre
we must sort i <PLACE_HOLDER> 4 and i <PLACE_HOLDER> 6 here,set < string > ipv6 = new hash set < > ( ) ; set < string > ipv4 = new hash set < > ( ) ; for ( string ip : ips ) if ( is properip ( ip ) ) { if ( ip . index of ( __str__ ) >= __num__ ) ipv6 . add ( ip ) ; else ipv4 . add ( ip ) ; } if ( ipv4 . size ( ) == __num__ ) { if ( ipv6 . size ( ) == __num__ ) { this . dna . put ( seed . ip @$ ipv6 . iterator ( ) . next ( ) ) ; this . dna . put ( seed . ip6 @$ __str__ ) ;,pv pv,success,pre
for a normal iterator @$ we can not ensure that the underlying compression stream is closed @$ so we decompress the full record set here . <PLACE_HOLDER> cases which call for a lower memory footprint can <PLACE_HOLDER> ` streaming iterator ` at the cost of additional complexity,try ( closeable iterator < record > iterator = compressed iterator ( buffer supplier . no_caching @$ false ) ) { list < record > records = new array list < > ( count ( ) ) ; while ( iterator . has next ( ) ) records . add ( iterator . next ( ) ) ; return records . iterator ( ) ; },footprint use,success,pre
if an entry already exists for a dataset @$ add it to the current state @$ else <PLACE_HOLDER> a new state,for ( string dataset : iterables . filter ( datasets @$ new dataset predicate ( dataset name element . get as string ( ) ) ) ) { if ( dataset specific config map . contains key ( dataset ) ) { dataset specific config map . get ( dataset ) . add all ( state utils . json object to state ( object @$ dataset ) ) ; } else { dataset specific config map . put ( dataset @$ state utils . json object to state ( object @$ dataset ) ) ; } },else create,success,pre
make sure processor <PLACE_HOLDER> work to do .,if ( ! is work to do ( ) ) { logger . debug ( __str__ @$ connectable ) ; return invocation result . yield ( __str__ ) ; } if ( num relationships > __num__ ) { final int required number of available relationships = connectable . is trigger when any destination available ( ) ? __num__ : num relationships ; if ( ! repository context . is relationship availability satisfied ( required number of available relationships ) ) { logger . debug ( __str__ @$ connectable ) ; return invocation result . yield ( __str__ ) ; } } logger . debug ( __str__ @$ connectable ) ; final long batch nanos = connectable . get run duration ( time unit . nanoseconds ) ; final,processor has,success,pre
use a small long here which will only <PLACE_HOLDER> one property block,my node . set property ( __str__ @$ small value ) ; tx . commit ( ) ;,which allocate,fail,pre
there are two special cases below @$ because 2 cliques <PLACE_HOLDER> 2 names,c . set domain ( domain ) ; if ( clique == cliquec ) { featuresc ( c info @$ loc @$ c ) ; } else if ( clique == clique cpc ) { features cpc ( c info @$ loc @$ c ) ; features cnc ( c info @$ loc - __num__ @$ c ) ; } else if ( clique == clique cp2c ) { features cp2c ( c info @$ loc @$ c ) ; } else if ( clique == clique cp3c ) { features cp3c ( c info @$ loc @$ c ) ; } else if ( clique == clique cp4c ) { features cp4c ( c info @$ loc @$ c ) ; } else if ( clique ==,cliques have,success,pre
if the current dimensions of the window do n't <PLACE_HOLDER> the desired dimensions @$ then adjust the min width and min height arrays according to the weights .,diffw = parent . width - r . width ; if ( diffw != __num__ ) { weight = __num__ ; for ( i = __num__ ; i < info . width ; i ++ ) weight += info . weightx [ i ] ; if ( weight > __num__ ) { for ( i = __num__ ; i < info . width ; i ++ ) { int dx = ( int ) ( ( ( ( double ) diffw ) * info . weightx [ i ] ) / weight ) ; info . min width [ i ] += dx ; r . width += dx ; if ( info . min width [ i ] < __num__ ) { r . width -= info,dimensions match,success,pre
key value should allow negative timestamps for backwards compat . otherwise @$ if the user already <PLACE_HOLDER> negative timestamps in cluster data @$ h base wo n't be able to handle that,try { new key value ( bytes . to bytes ( __num__ ) @$ bytes . to bytes ( __num__ ) @$ bytes . to bytes ( __num__ ) @$ - __num__ @$ bytes . to bytes ( __num__ ) ) ; } catch ( illegal argument exception ex ) { fail ( __str__ ) ; },user set,fail,pre
check non singleton beans for types do not eagerly <PLACE_HOLDER> factor beans when getting bean factory post processor beans,collection < bean factory post processor > factory post processors = bean factory . get beans of type ( bean factory post processor . class @$ true @$ false ) . values ( ) ; if ( factory post processors . is empty ( ) ) { factory post processors = collections . singleton ( new property placeholder configurer ( ) ) ; } for ( bean factory post processor factory post processor : factory post processors ) { factory post processor . post process bean factory ( bean factory ) ; } abstract engine configuration engine configuration = ( abstract engine configuration ) bean factory . get bean ( bean name ) ; engine configuration . set beans ( new spring bean factory proxy map ( bean,types initialize,success,pre
the ri <PLACE_HOLDER> datagram socket in broadcast mode .,assert true ( ds . get broadcast ( ) ) ;,ri opens,fail,pre
only log the message if it 's from a recipient we 're waiting for . it could have been multicast to all members @$ which would <PLACE_HOLDER> us one response per member,if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . processor id @$ is present @$ sender @$ this . key @$ serialized @$ version tag ) ; },which give,success,pre
if we are unable to write the current buffer to socket channel we should break @$ as we might have reached max socket <PLACE_HOLDER> buffer size .,break ;,socket write,fail,pre
clear defined indexes <PLACE_HOLDER> @$ create defined indexes <PLACE_HOLDER> @$ create index <PLACE_HOLDER> @$ define index <PLACE_HOLDER> @$ destroy index <PLACE_HOLDER> @$ list index <PLACE_HOLDER>,create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster read query ) ;,indexes register,fail,pre
we check that cookie manager <PLACE_HOLDER> the cookies for the main domain,url url main domain = new url ( __str__ ) ; cookies = cookie handler . get cookies for url ( man . get cookies ( ) @$ url main domain @$ cookie manager . allow_variable_cookies ) ; assert . assert equals ( __num__ @$ cookies . size ( ) ) ;,manager returns,success,pre
resort <PLACE_HOLDER> array @$ make i node id not sorted .,for ( int j = __num__ ; j < files . length / __num__ ; j ++ ) { path temp path = files [ j ] ; files [ j ] = files [ files . length - __num__ - j ] ; files [ files . length - __num__ - j ] = temp path ; byte [ ] temp bytes = bytes [ __num__ + j ] ; bytes [ __num__ + j ] = bytes [ files . length - __num__ - j + __num__ ] ; bytes [ files . length - __num__ - j + __num__ ] = temp bytes ; },resort file,success,pre
resetting max length should not <PLACE_HOLDER> maxlength attribute,tf . add focus listener ( event -> { tf . set max length ( __num__ ) ; } ) ;,length set,fail,pre
rather than create a validated network which <PLACE_HOLDER> things by registering it 's own network request during startup @$ just bump up the score to cancel out the unvalidated penalty .,test agent . adjust score ( __num__ ) ; cv = test factory . get network stoppedcv ( ) ;,which mutates,fail,pre
for an incoming message : shall remove and store the address envelope @$ including the delimiter . shall pass the remaining data frames to its calling application . shall wait for a single reply message from its calling application . shall <PLACE_HOLDER> the address envelope and delimiter . shall deliver this message back to the originating peer .,for ( string bind address : binds ) { envelope ( ctx @$ bind address @$ zmq . zmq_rep @$ zmq . zmq_dealer ) ; },envelope remove,fail,pre
take in two stringified lists @$ and return true if the first list <PLACE_HOLDER> elements that are not in the second list,bi function < string @$ string @$ boolean > has exclusive elements = ( string a @$ string b ) -> { if ( a == null || a . is empty ( ) ) { return false ; } else if ( b == null || b . is empty ( ) ) { return true ; } set < string > b set = stream . of ( b . split ( element sep ) ) . collect ( collectors . to set ( ) ) ; return ! stream . of ( a . split ( element sep ) ) . filter ( ( x ) -> ! b set . contains ( x ) ) . collect ( collectors . to set ( ),list contains,success,pre
also check what happens if a value xml file also <PLACE_HOLDER> the same id .,merged android data actual = unwritten merged android data . of ( source . resolve ( __str__ ) @$ direct @$ parsed android data builder . empty ( ) ) . write ( merged data writer ) ; assert about ( paths ) . that ( actual . get manifest ( ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve ( __str__ ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve ( __str__ ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve (,check contains,success,pre
do n't add trust anchors if not already present @$ the builder will <PLACE_HOLDER> the anchors from its parent @$ and that 's where the trust anchors should be added .,if ( ! builder . has certificates entry refs ( ) ) { return ; } builder . add certificates entry refs ( debug config builder . get certificates entry refs ( ) ) ;,builder read,fail,pre
case 6 : true if an attribute named primitive float att name of type float <PLACE_HOLDER> the value float value we cover javax.management.binary rel query exp with a rel op equal to eq and javax.management.numeric value exp,queries . add ( query . eq ( query . attr ( primitive float att name ) @$ query . value ( float value ) ) ) ;,name has,success,pre
use modified portions of do fast <PLACE_HOLDER> down code here since we do not want to allocate a temporary fast hive decimal object .,long compare0 ; long compare1 ; long compare2 ; int scale down ; if ( left scale < right scale ) { scale down = right scale - left scale ; if ( scale down < longword_decimal_digits ) { final long divide factor = power of ten table [ scale down ] ; final long multiply factor = power of ten table [ longword_decimal_digits - scale down ] ; compare0 = right fast0 / divide factor + ( ( right fast1 % divide factor ) * multiply factor ) ; compare1 = right fast1 / divide factor + ( ( right fast2 % divide factor ) * multiply factor ) ; compare2 = right fast2 / divide factor ; } else if ( scale down < two_x_longword_decimal_digits ),portions scale,success,pre
the ws loop <PLACE_HOLDER> whitespace from the beginning of each line .,while ( true ) { ws loop : while ( true ) { switch ( c ) { case __str__ : case __str__ : c = in . read ( ) ; break ; case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : default : break ws loop ; } } if ( c == __str__ ) { do { c = in . read ( ) ; } while (,loop consumes,success,pre
apply the offset to the <PLACE_HOLDER> time to find the absolute time .,stats . m last time used = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_active_attr ) ; try { stats . m last time visible = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_visible_attr ) ; } catch ( io exception e ) { log . i ( tag @$ __str__ ) ; } try { stats . m last time foreground service used = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_service_used_attr ) ; } catch ( io exception e ) { log . i ( tag @$ __str__ ) ; } stats . m total time in foreground = xml utils .,offset begin,success,pre
handle provisioning internally ; it 'll reset m <PLACE_HOLDER> drm in progress,int result = handle provisioninig ( uuid ) ;,m has,fail,pre
client <PLACE_HOLDER> response,buffer util . clear ( buffer ) ; len = c . client . fill ( buffer ) ; assert equals ( __num__ @$ len ) ; assert equals ( __str__ @$ buffer util . to string ( buffer ) ) ;,client reads,success,pre
as subsequent <PLACE_HOLDER> and set operations will fail .,cluster id before deserialize . compare and set ( no cluster id @$ cluster meta . get ( ) ) ; return data ;,subsequent compare,success,pre
fast path @$ if the stream can <PLACE_HOLDER> a buffer directly just write to it,if ( output stream instanceof buffer writable output stream ) { try { ( ( buffer writable output stream ) output stream ) . write ( buffers ) ; return true ; } catch ( io exception e ) { callback . on exception ( exchange @$ this @$ e ) ; return false ; } },stream write,fail,pre
issue 668 : do n't <PLACE_HOLDER> singleton getter methods calls as this confused class removing logic .,if ( convention . get singleton getter class name ( call node ) != null ) { return false ; },issue inline,success,pre
clear non <PLACE_HOLDER> up data from expected interval stats,m interval stats . active configuration = null ; m interval stats . configurations . clear ( ) ; m interval stats . events . clear ( ) ;,non cleaned,fail,pre
user 1 <PLACE_HOLDER> the message that contains the xhtml to user 2,try { chat1 . send message ( msg ) ; } catch ( exception e ) { fail ( __str__ ) ; } packet packet = chat2 . next result ( __num__ ) ; message message = ( message ) packet ; assert not null ( __str__ @$ message . get body ( ) ) ; try { xhtml extension = ( xhtml extension ) message . get extension ( __str__ @$ __str__ ) ; assert not null ( __str__ @$ xhtml extension ) ; assert true ( __str__ @$ xhtml extension . get bodies count ( ) > __num__ ) ; for ( iterator < string > it = xhtml extension . get bodies ( ) ; it . has next ( ) ; ) { string,user sends,success,pre
expected since only test role can <PLACE_HOLDER> that method,assert . fail ( __str__ ) ;,role call,success,pre
provoke an exception when the nl <PLACE_HOLDER> a row @$ must bubble up and nl must stop,expected exception . expect message ( __str__ ) ; execute ( __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ) ;,nl produces,fail,pre
check that the room 's owner can <PLACE_HOLDER> a simple participant,muc . ban user ( get barejid ( __num__ ) @$ __str__ ) ; thread . sleep ( __num__ ) ; assert null ( __str__ @$ muc . get occupant ( room + __str__ ) ) ; assert false ( __str__ @$ muc2 . is joined ( ) ) ;,owner ban,success,pre
launcher on a managed profile is <PLACE_HOLDER> ot user 0 !,run with caller ( launcher_1 @$ user_p0 @$ ( ) -> { m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_2 @$ list ( __str__ @$ __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_3 @$ list ( __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_p0 ) ; } ) ; run with caller ( launcher_1 @$ user_10 @$ ( ) -> { m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_10 ) ;,launcher showing,fail,pre
the shard is <PLACE_HOLDER> @$ the one it is <PLACE_HOLDER> to will be in initializing state @$ so we do n't count it,if ( shard routing . relocating ( ) ) { compute relocating shards ++ ; },shard allocating,fail,pre
test releasing mms network request does not <PLACE_HOLDER> main cellular network agent,m cm . unregister network callback ( network callback ) ; mms network agent . expect disconnected ( ) ; verify active network ( transport_cellular ) ;,request disconnect,success,pre
check <PLACE_HOLDER> operation .,boolean failed = false ; try { dflt cache . put ( __num__ @$ __num__ ) ; } catch ( cache exception e ) { failed = true ; check and wait ( e ) ; } assert true ( failed ) ; dflt cache . put ( __num__ @$ __num__ ) ; return true ; assert true ( ( boolean ) o ) ; assert equals ( __num__ @$ dflt cache . get ( __num__ ) ) ; return true ;,check put,success,pre
this point is only reached if the operation <PLACE_HOLDER> more than the allowed retry count,throw new io exception ( __str__ + m upload id + __str__ + m key @$ last exception ) ;,operation failed,success,pre
note that the only current caller of this method checks first to see if there is a path <PLACE_HOLDER>fore calling this method . it is not clear what the return value should <PLACE_HOLDER> here .,return null ;,value be,success,pre
last valid y coordinate which did n't <PLACE_HOLDER> a missing value,int last = - __num__ ;,which cause,fail,pre
let the <PLACE_HOLDER>or <PLACE_HOLDER> the classes referenced in the constant element value .,enum constant element value . referenced classes accept ( class visitor ) ;,visitor visit,success,pre
this layout <PLACE_HOLDER> red @$ blue @$ and green squares from the top .,enter scene ( r . layout . scene3 ) ;,layout collapses,fail,pre
when the animation completes @$ adjust the relative layout params of the recycler to make sure the bar does n't <PLACE_HOLDER> the bottom row when showing,long ms delay = ani utils . duration . short . to millis ( this ) ; m handler . post delayed ( new runnable ( ) { @ override public void run ( ) { if ( ! is finishing ( ) ) { relative layout . layout params params = ( relative layout . layout params ) m recycler . get layout params ( ) ; if ( show ) { params . add rule ( relative layout . above @$ r . id . container_selection_bar ) ; } else { params . add rule ( relative layout . above @$ __num__ ) ; } } } } @$ ms delay ) ;,bar exercise,fail,pre
retry with the old formatter which <PLACE_HOLDER> synchronization locks,try { parsed = generic formatter . short_day_formatter . parse ( date str @$ __num__ ) . get time ( ) ; } catch ( final parse exception pe ) { parsed = new date ( ) ; },which had,fail,pre
stop automatically reconnecting to this network in the future . automatically connecting to a network that provides no or limited connectivity is not <PLACE_HOLDER>ful @$ beca<PLACE_HOLDER> the <PLACE_HOLDER>r can not <PLACE_HOLDER> that network except through the notification shown by this method @$ and the notification is only shown if the network is explicitly selected by the <PLACE_HOLDER>r .,nai . async channel . send message ( network agent . cmd_prevent_automatic_reconnect ) ;,user use,success,pre
no param constructors do not <PLACE_HOLDER> js doc,test same ( __str__ ) ;,constructors inline,fail,pre
additionally @$ this condition <PLACE_HOLDER> computation time,if ( ! m answer sounds added ) { string answer sound source = remove front side audio ( answer ) ; m sound player . add sounds ( m base url @$ answer sound source @$ sound . sounds_answer ) ; m answer sounds added = true ; },condition reduces,success,pre
the suite made here will all be <PLACE_HOLDER> the tests from this class,multi config suite builder builder = new multi config suite builder ( test admin mode from command line . class ) ;,suite using,success,pre
this is what always should happen . a sort <PLACE_HOLDER> the items one by one @$ temporarily breaking the uniqueness requirement .,if ( dupl == __num__ ) { duplicates . remove ( e ) ; } else { duplicates . put ( e @$ dupl - __num__ ) ; },sort filters,fail,pre
this leads to missing the after command profiles of the other modules in the profile . since the bep currently shuts down at the <PLACE_HOLDER> complete event @$ we can not just move posting the <PLACE_HOLDER> tool logs to after command of this module .,if ( profile path != null ) { try { profiler . instance ( ) . stop ( ) ; event . get result ( ) . get build tool log collection ( ) . add local file ( profile path . get base name ( ) @$ profile path ) ; } catch ( io exception e ) { reporter . handle ( event . error ( __str__ + e . get message ( ) ) ) ; } },the build,success,pre
higher priority @$ so <PLACE_HOLDER> new work units,assert . assert true ( add files ( list @$ __str__ @$ __num__ ) ) ; assert . assert equals ( list . get work units ( ) . size ( ) @$ __num__ ) ;,priority add,fail,pre
block any external content resolving actions since we do n't need them and a report says these actions may <PLACE_HOLDER> security problems .,document builder . set entity resolver ( new entity resolver ( ) { @ override public input source resolve entity ( string public id @$ string system id ) throws sax exception @$ io exception { return new input source ( ) ; } } ) ;,actions cause,success,pre
if the query <PLACE_HOLDER> a having clause and both h 2 and pinot have no groups @$ that is expected @$ so we do n't need to compare the number of docs scanned,if ( pinot query . contains ( __str__ ) ) { return ; } if ( pinot num records selected != __num__ ) { string failure message = __str__ + pinot num records selected + __str__ ; failure ( pinot query @$ sql queries @$ failure message ) ; },query has,success,pre
... if the cache did not contain a color state list @$ try and <PLACE_HOLDER> one,if ( tint == null ) { tint = ( m hooks == null ) ? null : m hooks . get tint list for drawable res ( context @$ res id ) ; if ( tint != null ) { add tint list to cache ( context @$ res id @$ tint ) ; } },and add,fail,pre
empty control batch should not <PLACE_HOLDER> an exception,default record batch . write empty header ( buffer @$ record batch . magic_value_v2 @$ __num__ @$ ( short ) __num__ @$ - __num__ @$ __num__ @$ __num__ @$ record batch . no_partition_leader_epoch @$ timestamp type . create_time @$ time . milliseconds ( ) @$ true @$ true ) ; current offset += append transactional records ( buffer @$ __num__ @$ current offset @$ new simple record ( time . milliseconds ( ) @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) @$ new simple record ( time . milliseconds ( ) @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) ) ; commit transaction ( buffer @$ __num__ @$ current offset ) ; buffer .,batch throw,fail,pre
when location <PLACE_HOLDER> one particular dir,for ( storage dir dir : tier . get storage dirs ( ) ) { location = dir . to block store location ( ) ; assert equals ( m meta manager . get available bytes ( location ) @$ m metadata view . get available bytes ( location ) ) ; },location represents,success,pre
then : should get created interpreters which <PLACE_HOLDER> different dependencies,md1 = setting ;,which have,success,pre
t qk js n 2 q 2 s zv 9 h 2 d q 5 x 2 r sne k ny l <PLACE_HOLDER> vv,ec key ec key2 = new ec key ( utils . get random ( ) ) ; byte [ ] low bal address2 = ec key2 . get address ( ) ; ret1 = public methed . sendcoin2 ( low bal address2 @$ __num__ @$ from address @$ test key002 @$ blocking stub full ) ; assert . assert equals ( ret1 . get code ( ) @$ grpcapi . return . response_code . success ) ; witness list witnesslist = blocking stub full . list witnesses ( grpcapi . empty message . new builder ( ) . build ( ) ) ; optional < witness list > result = optional . of nullable ( witnesslist ) ; witness list witness list = result . get ( ),l kxs,fail,pre
pre grouped symbols reflect properties of the input . splitting the aggregation and pushing partial aggregation through the exchange may or may not <PLACE_HOLDER> these properties . hence @$ it is safest to drop pre grouped symbols here .,plan node partial = new aggregation node ( context . get id allocator ( ) . get next id ( ) @$ node . get source ( ) @$ intermediate aggregation @$ node . get grouping sets ( ) @$ immutable list . of ( ) @$ partial @$ node . get hash symbol ( ) @$ node . get group id symbol ( ) ) ;,properties preserve,success,pre
<PLACE_HOLDER> a connection from a new key with the 'always <PLACE_HOLDER> ' option set to false,run adb test ( test_key_1 @$ true @$ false @$ false ) ;,connection steal,fail,pre
do not set ms lab to null as scanners may still be <PLACE_HOLDER> the data here and need to decrease the counter when they finish,this . mem storelab . close ( ) ;,scanners controlling,fail,pre
init <PLACE_HOLDER> read commands,lenient ( ) . do return ( test_log_infos ) . when ( test workspace logs reader ) . get log infos ( ) ; lenient ( ) . do return ( test_read_first_log_command ) . when ( test workspace logs reader ) . get read logs command ( test_workspace_id @$ paths . get ( format ( __str__ @$ path_to_store_logs @$ test_workspace_id @$ first_log_info . get name ( ) ) ) @$ first_log_info . get location inside workspace ( ) ) ; lenient ( ) . do return ( test_read_second_log_command ) . when ( test workspace logs reader ) . get read logs command ( test_workspace_id @$ paths . get ( format ( __str__ @$ path_to_store_logs @$ test_workspace_id @$ second_log_info . get name ( ) ) ) @$ second_log_info .,init log,fail,pre
we are setting an empty value to these attributes @$ since now we have a new entity type called hive process execution which <PLACE_HOLDER> these values . we have to set empty values here because these attributes are mandatory attributes for hive process entity type .,ret . set attribute ( attribute_start_time @$ empty_attribute_value ) ; ret . set attribute ( attribute_end_time @$ empty_attribute_value ) ; ret . set attribute ( attribute_user_name @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_text @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_id @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_plan @$ __str__ ) ; ret . set attribute ( attribute_recent_queries @$ collections . singleton list ( query str ) ) ; return ret ;,which captures,success,pre
check existing schema <PLACE_HOLDER> all the type in to validate schema,if ( to validate . get types ( ) . size ( ) != expected . get types ( ) . size ( ) ) { return false ; } hash set < schema > types = new hash set < schema > ( expected . get types ( ) ) ; for ( schema to validate type : to validate . get types ( ) ) { schema equal schema = null ; for ( schema type : types ) { if ( compare ( type @$ to validate type ) ) { equal schema = type ; break ; } } if ( equal schema == null ) { return false ; } types . remove ( equal schema ) ; } return true ;,schema matches,fail,pre
local get can not be used with mvcc as local node can <PLACE_HOLDER> some visible version which is not latest .,boolean fast loc get = ! cctx . mvcc enabled ( ) && ( ! force primary || aff nodes . get ( __num__ ) . is local ( ) ) && cctx . reserve for fast local get ( part @$ top ver ) ; if ( fast loc get ) { try { if ( local get ( top ver @$ key @$ part @$ loc vals ) ) return true ; } finally { cctx . release for fast local get ( part @$ top ver ) ; } } return false ;,node contain,success,pre
find function name @$ pointed by function . long timestamp can shift if position so <PLACE_HOLDER> end of timestamp to find the function which is ended by ' : ' .,left index = right index + __num__ ; while ( character . is whitespace ( line . char at ( left index ) ) ) { ++ left index ; } right index = line . index of ( __str__ @$ left index ) ; final string function = line . substring ( left index @$ right index ) ; if ( ! function . equals ( function_tracing_mark_write ) ) { continue ; },position exceed,fail,pre
we read a further byte @$ which <PLACE_HOLDER> the zmtp version .,byte protocol = greeting recv . get ( revision pos ) ; if ( protocol == protocol . v1 . revision || protocol == protocol . v2 . revision ) { greeting send . limit ( v2_greeting_size ) ; greeting send . position ( signature_size + __num__ ) ; greeting send . put ( ( byte ) options . type ) ; outsize += __num__ ; } else { greeting send . limit ( v3_greeting_size ) ; greeting send . position ( signature_size + __num__ ) ; greeting send . put ( ( byte ) __num__ ) ; outsize += __num__ ; greeting send . mark ( ) ; greeting send . put ( new byte [ __num__ ] ) ; assert ( mechanism == mechanisms . null,which indicates,success,pre
while it does seem a bit silly @$ we need a check to make sure the document pane is enabled since this method will get called during the processing stage and we do n't want anything below to be fired during so . this checks to make sure the user has <PLACE_HOLDER> appropriate text for the combine sentences option to be enabled .,if ( e . is popup trigger ( ) && main . document pane . is enabled ( ) ) { if ( main . editor driver . new caret position [ __num__ ] > main . editor driver . new caret position [ __num__ ] ) { start = main . editor driver . new caret position [ __num__ ] ; stop = main . editor driver . new caret position [ __num__ ] ; } else { start = main . editor driver . new caret position [ __num__ ] ; stop = main . editor driver . new caret position [ __num__ ] ; } if ( start == stop ) { right click menu . enable combine sentences ( false ) ; right click,user specified,fail,pre
vm 1 <PLACE_HOLDER> and frees key 1,vm1 . invoke ( new serializable runnable ( ) { @ override public void run ( ) { logger . info ( __str__ ) ; connect distributed system ( ) ; d lock service dls = ( d lock service ) distributed lock service . create ( dls name @$ get system ( ) ) ; assert that ( dls . lock ( key1 @$ - __num__ @$ - __num__ ) ) . is true ( ) ; assert that ( dls . is lock grantor ( ) ) . is false ( ) ; assert that ( dls . get token ( key1 ) ) . is not null ( ) ; dls . unlock ( key1 ) ; assert that ( dls . get token (,vm locks,success,pre
flush enough files to get up to the threshold @$ does n't <PLACE_HOLDER> compactions,for ( int i = __num__ ; i < __num__ ; i ++ ) { put put = new put ( table name . to bytes ( ) ) . add column ( family @$ family @$ table name . to bytes ( ) ) ; region . put ( put ) ; fr = region . flush ( true ) ; assert true ( fr . is flush succeeded ( ) ) ; assert false ( fr . is compaction needed ( ) ) ; },files perform,fail,pre
this will change the source text view instead @$ which will <PLACE_HOLDER> the extract text view .,mime . on extracted delete text ( start @$ end ) ;,which update,success,pre
<PLACE_HOLDER> new items without calling notify cell data <PLACE_HOLDER> changed method of cell recycler view adapter,m row header recycler view adapter . set items ( new row header @$ ! m enable animation ) ; m cell recycler view adapter . set items ( new column items @$ ! m enable animation ) ; if ( m enable animation ) { final row header sort callback diff callback = new row header sort callback ( old row header @$ new row header ) ; final diff util . diff result diff result = diff util . calculate diff ( diff callback ) ; diff result . dispatch updates to ( m row header recycler view adapter ) ; diff result . dispatch updates to ( m cell recycler view adapter ) ; },data set,success,pre
the specific device class does not <PLACE_HOLDER> the generic device class .,if ( specific device class . generic device class != generic . not_known && specific device class . generic device class != this . generic device class ) { throw new illegal argument exception ( __str__ ) ; } this . specific device class = specific device class ;,class implement,fail,pre
completing the task should <PLACE_HOLDER> the event subprocess . this interupts the main flow .,task service . complete ( task . get id ( ) ) ; task sub one task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( sub one task ) ; task service . complete ( sub one task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ;,task end,fail,pre
check whether the caller can <PLACE_HOLDER> the core file ?,sm . check read ( corefile ) ;,caller read,success,pre
trigger a bucket creation in vm 1 @$ which should <PLACE_HOLDER> server 1 to close it 's cache .,assert that thrown by ( ( ) -> server1 . invoke ( ( ) -> put data ( __num__ @$ __num__ @$ __str__ ) ) ) . is instance of ( rmi exception . class ) . has cause instance of ( distributed system disconnected exception . class ) ; assert that ( server2 . invoke ( ( ) -> get bucket list ( ) ) ) . contains exactly ( __num__ ) ;,which cause,success,pre
reorder only down to the lowest odd level and reorder at an odd min level in a separate @$ simpler loop . <PLACE_HOLDER> comments above for why min level is always incremented .,++ min level ; runs = bidi . runs ; levels = bidi . levels ; run count = bidi . run count ;,reorder see,success,pre
run the trash emptier for 120 ms @$ it should run 2 times deletion as the interval is 50 ms. <PLACE_HOLDER> the checkpoints number when shutting down the emptier .,verify auditable trash emptier ( trash @$ __num__ @$ __num__ ) ;,ms. validate,fail,pre
let 's <PLACE_HOLDER> the first file with evict on <PLACE_HOLDER> turned on,conf . set boolean ( __str__ @$ true ) ; cache conf = new cache config ( conf @$ bc ) ; hsf = new h store file ( this . fs @$ path cow off @$ conf @$ cache conf @$ bloom type . none @$ true ) ; hsf . init reader ( ) ; reader = hsf . get reader ( ) ; reader . close ( cache conf . should evict on close ( ) ) ;,'s close,success,pre
making subcluster 0 slow to reply @$ should only <PLACE_HOLDER> d ns from nn 1,minidfs cluster dfs cluster = cluster . get cluster ( ) ; name node nn0 = dfs cluster . get name node ( __num__ ) ; simulate slow namenode ( nn0 @$ __num__ ) ; wait update live nodes ( json string2 @$ metrics ) ; final string json string3 = metrics . get live nodes ( ) ; assert equals ( __num__ @$ get num datanodes ( json string3 ) ) ;,1 read,fail,pre
nobody <PLACE_HOLDER> the correct role,test protected denied ( url @$ __str__ @$ __str__ ) ; test protected denied ( url @$ __str__ @$ __str__ ) ; test protected denied ( url @$ __str__ @$ __str__ ) ;,nobody has,success,pre
verify the server <PLACE_HOLDER> the request with right size,assert equals ( request size @$ result size ) ;,server received,fail,pre
a stubborn caller will <PLACE_HOLDER> string reader exceptions .,if ( m_bytes == null ) { m_string reader = new string reader ( __str__ ) ; throw new io exception ( string . format ( __str__ @$ m_path ) ) ; },caller throw,fail,pre
russian internal passports <PLACE_HOLDER> transliteration for the name and have a number one digit longer than fits into the standard mrz format,if ( __str__ . equals ( result . issuing country ) && mrz lines [ __num__ ] . char at ( __num__ ) == __str__ ) { result . type = result . type_internal_passport ; string [ ] names = result . first name . split ( __str__ ) ; result . first name = cyrillic to latin ( russian passport translit ( names [ __num__ ] ) ) ; if ( names . length > __num__ ) result . middle name = cyrillic to latin ( russian passport translit ( names [ __num__ ] ) ) ; result . last name = cyrillic to latin ( russian passport translit ( result . last name ) ) ; if ( result . number != null ) result .,passports allow,fail,pre
always uninstall before installing . we might be downgrading @$ which <PLACE_HOLDER> an uninstall @$ or we might just want a clean installation .,uninstall agent ( event bus @$ device ) ; agent info = optional . empty ( ) ;,which requires,success,pre
run two animators @$ one of which <PLACE_HOLDER> a start delay @$ after setting the duration scale to 0,a1 . set start delay ( __num__ ) ; final my listener l1 = new my listener ( ) ; final my listener l2 = new my listener ( ) ; a1 . add listener ( l1 ) ; a2 . add listener ( l2 ) ; m activity rule . run on ui thread ( new runnable ( ) { @ override public void run ( ) { assert false ( l1 . start called ) ; assert false ( l2 . start called ) ; assert false ( l1 . end called ) ; assert false ( l2 . end called ) ; a1 . start ( ) ; a2 . start ( ) ; assert equals ( a2_start_value @$ a2 . get animated value (,one specify,fail,pre
initial security properties should only <PLACE_HOLDER> initial set of values,this . server starter . start server ( props @$ this . ls rule . get member ( __num__ ) . get port ( ) ) ; distributed system ds = this . server starter . get cache ( ) . get distributed system ( ) ;,properties contain,success,pre
populate the cache and then reset the <PLACE_HOLDER> log,process result cache populating result = workspace . run buck command ( __str__ @$ target . get fully qualified name ( ) ) ; cache populating result . assert success ( ) ;,the add,fail,pre
only master stream can <PLACE_HOLDER> the data gap,assert ( m_coordinator . is master ( ) ) ;,stream change,fail,pre
if in zip 64 format or using strict entry numbers @$ use the parsed information as is to read the central directory <PLACE_HOLDER> headers .,if ( zip data . is zip64 ( ) || strict entries ) { read central directory file headers ( zip data . get expected entries ( ) @$ zip data . get central directory offset ( ) ) ; } else { long central directory offset = eocd location - zip data . get central directory size ( ) ; if ( ( int ) central directory offset == ( int ) zip data . get central directory offset ( ) ) { read central directory file headers ( central directory offset ) ; } else { read central directory file headers ( zip data . get expected entries ( ) @$ zip data . get central directory offset ( ) ) ; } },directory file,success,pre
use raw socket instead of telnet client here because telnet client sends an extra newline char after each write which <PLACE_HOLDER> the connection to become unresponsive .,socket = new socket ( ) ; socket . connect ( new inet socket address ( connection . get host ( ) @$ connection . get telnet port ( ) ) @$ timeout ) ; socket . set keep alive ( true ) ; socket . set so timeout ( timeout ) ; in = new buffered reader ( new input stream reader ( socket . get input stream ( ) ) ) ; out = new output stream writer ( socket . get output stream ( ) @$ __str__ ) ; connected = true ; callback . listener connected ( ) ;,which causes,success,pre
if multiple threads are <PLACE_HOLDER> the initialization at the same time it is not a problem because they will all get to the same result in the end .,if ( format == null ) { format = logging support . get simple format ( ) ; },threads doing,success,pre
if we have more than one stream @$ <PLACE_HOLDER> the audio be the master,if ( ! master stream set ) { if ( remote descriptions . size ( ) > __num__ ) { if ( media type . audio . equals ( media type ) ) { master stream = true ; master stream set = true ; } } else { master stream = true ; master stream set = true ; } } media stream stream = init stream ( connector @$ dev @$ fmt @$ target @$ direction @$ rtp extensions @$ master stream ) ;,master lets,success,pre
if job is already succeed and h base segment in ready state @$ remove the <PLACE_HOLDER> state,if ( executable state . succeed . equals ( job state ) ) { cube segment cube segment = cube instance . get segment ( segment state . get segment name ( ) @$ null ) ; if ( cube segment != null && segment status enum . ready == cube segment . get status ( ) ) { logger . info ( __str__ @$ job id @$ segment state . get segment name ( ) ) ; coordinator . get stream metadata store ( ) . remove segment build state ( cube name @$ segment state . get segment name ( ) ) ; } return false ; },the build,success,pre
rose and durant <PLACE_HOLDER> 5 rebounds per game but only rose does not play in okc,iterable < player > filtered players = filter ( players ) . with ( __str__ ) . equals to ( __num__ ) . and ( __str__ ) . not equals to ( __str__ ) . get ( ) ; assert that ( filtered players ) . contains only ( kobe ) ;,durant have,success,pre
relay state data may be included with a saml protocol message transmitted with this binding . the value must not <PLACE_HOLDER> 80 bytes in length and should be integrity protected by the entity creating the message independent of any other protections that may or may not exist during message transmission .,if ( relay state != null ) { if ( relay state . length ( ) > __num__ ) { throw new illegal argument exception ( __str__ + relay state . length ( ) ) ; } encoder . add param ( relay_state @$ relay state ) ; },value exceed,success,pre
inversion does not <PLACE_HOLDER> exceptions correctly .,test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; invert = true ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ;,inversion handle,success,pre
else release <PLACE_HOLDER> lock,pause reader ( ) ;,release write,fail,pre
by convention @$ full message type <PLACE_HOLDER> first message type .,if ( content . readable bytes ( ) > max content length - m . content ( ) . readable bytes ( ) ) { @ suppress warnings ( __str__ ) s s = ( s ) current message ; invoke handle oversized message ( ctx @$ s ) ; return ; },type gets,fail,pre
then inner comparison fails as the fields <PLACE_HOLDER> different types,comparison difference value difference = diff ( __str__ @$ witha . inner @$ withb . inner @$ __str__ ) ; verify should be equal by comparing field by field recursively call ( witha @$ withb @$ value difference ) ;,fields have,success,pre
we have a real error @$ so do what the appropriate action <PLACE_HOLDER> us what to do ...,coding error action action = result . is unmappable ( ) ? unmappable character action : malformed input action ; if ( action == coding error action . report ) { return result ; } else if ( action == coding error action . replace ) { if ( out . remaining ( ) < replacement bytes . length ) { return coder result . overflow ; } out . put ( replacement bytes ) ; } in . position ( in . position ( ) + result . length ( ) ) ;,action tells,success,pre
base <PLACE_HOLDER> but with label b and different aspects,new equals tester ( ) . add equality group ( dependency . with configuration and aspects ( a @$ host @$ two aspects ) @$ dependency . with configuration and aspects ( a explicit @$ host @$ two aspects ) @$ dependency . with configuration and aspects ( a @$ host @$ inverse aspects ) @$ dependency . with configuration and aspects ( a explicit @$ host @$ inverse aspects ) @$ dependency . with configured aspects ( a @$ host @$ two aspects @$ two aspects host map ) @$ dependency . with configured aspects ( a explicit @$ host @$ two aspects @$ two aspects host map ) ) . add equality group ( dependency . with configuration and aspects ( b @$ host @$ two,base config,fail,pre
delete notifications do n't <PLACE_HOLDER> all attributes . hence the special handling for delete operation,if ( instance converter != null ) { if ( operation == entity operation . delete ) { for ( atlas entity header entity header : entity headers ) { ret . add ( new referenceable ( entity header . get guid ( ) @$ entity header . get type name ( ) @$ entity header . get attributes ( ) ) ) ; } } else { for ( atlas entity header entity header : entity headers ) { ret . add ( to referenceable ( entity header . get guid ( ) ) ) ; } } },notifications need,success,pre
hadoop 's external sort <PLACE_HOLDER> the number of available memory bytes in an int @$ this prevents integer overflow,check argument ( memorymb < __num__ @$ __str__ ) ; this . memorymb = memorymb ; return this ;,sort reports,fail,pre
if the message <PLACE_HOLDER> no body @$ or the bodies list is empty,if ( old msg . get body ( ) == null && old msg . get bodies ( ) . size ( ) == __num__ ) { return old msg ; } message new msg = new message ( ) ; new msg . set stanza id ( packet . get stanza id ( ) ) ; new msg . set to ( packet . get to ( ) ) ; new msg . set from ( packet . get from ( ) ) ;,message has,success,pre
control all of the rows which <PLACE_HOLDER> same column position .,if ( should fit columns ( column position @$ my position ) ) { if ( m last dx < __num__ ) { log . e ( log_tag @$ __str__ + column position + __str__ + my position + __str__ + __str__ ) ; m cell layout manager . fit width size ( column position @$ true ) ; } else { m cell layout manager . fit width size ( column position @$ false ) ; log . e ( log_tag @$ __str__ + column position + __str__ + my position + __str__ + __str__ ) ; } m need fit for vertical scroll = false ; },which have,fail,pre
use the new provider api to better leverage the <PLACE_HOLDER> cache,_target . get jvm argument providers ( ) . add ( new data template jvm argument provider ( resolver path str @$ get project ( ) . get root dir ( ) ) ) ; _target . get argument providers ( ) . add ( new data template argument provider ( arrays . as list ( _target . get destination dir ( ) . get path ( ) @$ _target . get input dir ( ) . get path ( ) ) ) ) ;,the build,success,pre
user 1 <PLACE_HOLDER> note,note note public = notebook . create note ( __str__ @$ new authentication info ( __str__ ) ) ;,user created,fail,pre
check that the password is disguised and the toggle button <PLACE_HOLDER> the same state,assert not equals ( input_text @$ text input . get layout ( ) . get text ( ) . to string ( ) ) ; on view ( with id ( r . id . textinput_password ) ) . perform ( click icon ( true ) ) ;,button stays,fail,pre
we just wait for expected events to arrive . caller will <PLACE_HOLDER> validation and throw exception .,return true ;,caller perform,fail,pre
resolution of symlinks is funky on windows . python does n't <PLACE_HOLDER> symlinks @$ but java does so : do an extra resolution in java @$ and see what happens . yay .,path original pwd = pwd ; try { pwd = pwd . to real path ( ) . to absolute path ( ) ; } catch ( io exception e ) { },python have,fail,pre
number of state <PLACE_HOLDER> that match .,int match count = __num__ ; int maxo styles = style . states . length ; for ( int this counter = states . length - __num__ ; this counter >= __num__ ; this counter -- ) { int state = states [ this counter ] . get component state ( ) ; boolean found = false ; for ( int o counter = maxo styles - __num__ - match count ; o counter >= __num__ ; o counter -- ) { if ( state == style . states [ o counter ] . get component state ( ) ) { style . states [ o counter ] = states [ this counter ] . add to ( style . states [ o counter ] ) ; state,number matches,fail,pre
we need this delay so wm can not <PLACE_HOLDER> two clicks on title as double click,robot . delay ( __num__ ) ; util . click on title ( owner_frame @$ robot ) ; util . wait for idle ( robot ) ; system . out . println ( __str__ ) ;,wm handle,fail,pre
check if session handler <PLACE_HOLDER> a goaway frame when closing,session handler . write inbound ( close message ) ; assert go away ( session handler . read outbound ( ) @$ local stream id ) ; assert null ( session handler . read outbound ( ) ) ; local stream id += __num__ ;,handler sends,success,pre
this way we can <PLACE_HOLDER> the behavior of the runnable as well .,verify ( m network queue ) . put ( m request ) ; assert same ( entry @$ m request . get cache entry ( ) ) ; verify ( m delivery @$ never ( ) ) . post error ( any ( request . class ) @$ any ( volley error . class ) ) ;,way verify,success,pre
this method will be called by the printer job on a thread other that the application 's thread . we hold on to the graphics until we can rendevous with the application 's thread and hand over the graphics . the application then <PLACE_HOLDER> all the drawing . when the application is done drawing we rendevous again with the printer job thread and release,int result ;,application processes,fail,pre
when the trailer does not <PLACE_HOLDER> orca report @$ listener callback will not be invoked .,metadata trailer = new metadata ( ) ; tracer . inbound trailers ( trailer ) ; verify no more interactions ( orca listener1 ) ;,trailer contain,success,pre
now @$ send the second request so that the client <PLACE_HOLDER> the connection .,final http response res2 = client . get ( __str__ ) ;,client reopens,fail,pre
evaluate any karate expression even on lhs will <PLACE_HOLDER> exception if variable does not exist,actual = eval karate expression ( expression @$ context ) ; if ( actual . is json like ( ) ) { path = var_root ; },expression throw,success,pre
for every time when a user has not <PLACE_HOLDER> a function but a basic block this breaks . as it does throw a null pointer exception .,first function . load ( ) ; second function . load ( ) ; final creation thread creation thread = new creation thread ( module @$ source block @$ target block @$ first function @$ second function ) ; progress dialog . show ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ @$ creation thread ) ; if ( ( ! ( creation thread . threw exception ( ) ) ) && ( creation thread . get created view ( ) == null ) ) { message box . show information ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ ) ; } else { new thread ( ) { @ override public void,user loaded,fail,pre
application <PLACE_HOLDER> dir,if ( app id != null ) { app log dir present . set true ( ) ; if ( should clean app log dir ( child path @$ now @$ fs @$ retain millis ) ) { delete dir ( child path ) ; } } else { clean app log dir ( child path @$ retain millis @$ app log dir present ) ; },application log,success,pre
insert one row @$ so calls below <PLACE_HOLDER> just one row .,client . call procedure ( __str__ @$ __num__ @$ __num__ ) ;,calls produce,success,pre
management server <PLACE_HOLDER> back an rds response containing route configurations more than requested .,list < any > route configs = immutable list . of ( any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) @$ any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) @$ any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) ) ;,server sends,success,pre
the default cost provider <PLACE_HOLDER> a cost of 1 for all calls @$ ignoring the processing details @$ so an empty one is fine,processing details empty processing details = new processing details ( time unit . milliseconds ) ; scheduler . add response time ( __str__ @$ mock call @$ empty processing details ) ; return priority ;,provider issues,fail,pre
components with their own comp context <PLACE_HOLDER> their own binding,if ( naming mode == component naming mode . create && comp binding ) { continue ; },components do,success,pre
update <PLACE_HOLDER> reg model with new portion of data .,logistic regression model mdl2 = trainer . update ( mdl @$ ignite @$ data cache @$ split . get test filter ( ) @$ normalization preprocessor ) ; system . out . println ( __str__ + mdl ) ; double accuracy = evaluator . evaluate ( data cache @$ mdl2 @$ normalization preprocessor @$ metric name . accuracy ) ; system . out . println ( __str__ + accuracy ) ; system . out . println ( __str__ + ( __num__ - accuracy ) ) ; system . out . println ( __str__ ) ;,update log,success,pre
everyone can <PLACE_HOLDER> reservations on queue c .,verify submit reservation success ( queue_b_user @$ queuec ) ; verify submit reservation success ( queue_b_admin @$ queuec ) ; verify submit reservation success ( queue_a_user @$ queuec ) ; verify submit reservation success ( queue_a_admin @$ queuec ) ; verify submit reservation success ( common_user @$ queuec ) ;,everyone submit,success,pre
update the context chars and the unsafe backward set while copying @$ in case a character <PLACE_HOLDER> conditional mappings in the source builder and they were removed later .,modified |= src . modified ;,character had,success,pre
test <PLACE_HOLDER> method,echo request proto echo request = echo request proto . new builder ( ) . set message ( __str__ ) . build ( ) ; echo response proto echo response = stub . echo ( null @$ echo request ) ; assert equals ( __str__ @$ echo response . get message ( ) ) ; stub . error ( null @$ empty request ) ; fail ( __str__ ) ; rpc client . close ( ) ;,test echo,success,pre
array must not <PLACE_HOLDER> aliases,return new object [ ] [ ] { { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$,array contain,fail,pre
default nine should not <PLACE_HOLDER> @$ not checked checkbox ten should not <PLACE_HOLDER> @$ disabled eleven should not <PLACE_HOLDER> @$ button,assert equals ( __str__ @$ data . get ( __num__ ) . to string ( ) ) ;,checkbox appear,success,pre
get image cell layout which <PLACE_HOLDER> image view instead of text view .,layout = inflater . inflate ( r . layout . table_view_image_cell_layout @$ parent @$ false ) ; return new gender cell view holder ( layout ) ; default :,which has,success,pre
default method below so we they can <PLACE_HOLDER> the default behavior when needed .,return g tree . this . get tool tip text ( event ) ;,method override,fail,pre
the title is same @$ <PLACE_HOLDER> their paths,if ( result == __num__ ) { result = lhs [ __num__ ] . compare to ignore case ( rhs [ __num__ ] ) ; },title compare,success,pre
in hive one can <PLACE_HOLDER> conflicting values for the same property @$ in such case it looks like table properties are used,if ( serde property value != null && table property value != null && ! table property value . equals ( serde property value ) ) { throw new presto exception ( hive_invalid_metadata @$ format ( __str__ @$ key @$ serde property value @$ table property value ) ) ; },one have,fail,pre
producer is not <PLACE_HOLDER> any thing .,create client ( producer @$ the port @$ host0 ) ; final int size = __num__ ; create values ( producer @$ regions [ __num__ ] @$ size ) ; createcq ( client @$ pool name @$ __str__ @$ cqs [ __num__ ] ) ; executecq ( client @$ __str__ @$ true @$ null ) ; create values ( producer @$ regions [ __num__ ] @$ ( __num__ * size ) ) ; for ( int i = __num__ ; i <= size ; i ++ ) { if ( i % __num__ == __num__ ) wait for updated ( client @$ __str__ @$ key + i ) ; } for ( int i = ( size + __num__ ) ; i <= __num__ * size ; i,producer doing,success,pre
if the declared type can be assigned into the actual type @$ or the expected type @$ then the compiler already <PLACE_HOLDER> sufficient type info .,return false ;,compiler has,success,pre
key 1 <PLACE_HOLDER> 1 key 2 ...,node key = n . get first child ( ) ; while ( key != null ) { switch ( key . get token ( ) ) { case getter_def : case setter_def : case string_key : case member_function_def : if ( is strip name ( key . get string ( ) ) ) { node next = key . get next ( ) ; n . remove child ( key ) ; node util . mark functions deleted ( key @$ compiler ) ; key = next ; compiler . report change to enclosing scope ( n ) ; break ; } default : key = key . get next ( ) ; } },key has,fail,pre
if the user <PLACE_HOLDER> the network @$ then keep that,if ( from parms . _network == network . auto || from parms . _network == null ) { if ( from parms . _network_definition_file != null && ! from parms . _network_definition_file . equals ( __str__ ) ) { if ( ! from parms . _quiet_mode ) log . info ( __str__ ) ; to parms . _network = network . user ; } else { if ( to parms . _problem_type == problem type . image ) to parms . _network = network . inception_bn ; if ( to parms . _problem_type == problem type . text || to parms . _problem_type == problem type . dataset ) { to parms . _network = null ; if ( from parms . _hidden == null ) {,user set,fail,pre
test that ` source path ` coercion does n't <PLACE_HOLDER> the error from ` path ` cercion .,assert same message ( path coerce exception @$ get coerce exception ( source path . class @$ invalid path ) ) ;,path throw,fail,pre
create a temporary file @$ as jaudiotagger <PLACE_HOLDER> a file rather than an input stream,temp file = file . create temp file ( filename @$ __str__ + file ext ) ; long bytes copied = file utils . copy ( source @$ temp file @$ max bytes ) ; partially parsed = bytes copied == max bytes && source . read ( ) != - __num__ ; f = audio fileio . read ( temp file ) ;,jaudiotagger creates,fail,pre
click on the input icon <PLACE_HOLDER> : start to a new hop in this case @$ we set the end hop step ...,selected step = null ; start hop step = null ; end hop step = ( step meta ) area owner . get parent ( ) ; candidate hop type = null ; start error hop step = false ;,click choose,fail,pre
roll over the master key do allocate again . the am should <PLACE_HOLDER> the latest amrm token,rm . getrm context ( ) . getamrm token secret manager ( ) . roll master key ( ) ; response = am . allocate ( records . new record ( allocate request . class ) ) ; assert . assert not null ( response . getamrm token ( ) ) ; token < amrm token identifier > amrm token = converter utils . convert from yarn ( response . getamrm token ( ) @$ new text ( response . getamrm token ( ) . get service ( ) ) ) ; assert . assert equals ( amrm token . decode identifier ( ) . get key id ( ) @$ rm . getrm context ( ) . getamrm token secret manager ( ) . get master key,am have,fail,pre
remote exception <PLACE_HOLDER> useful information as against the java.lang.reflect exceptions .,if ( cause instanceof io exception ) { throw ( io exception ) cause ; } else if ( cause instanceof runtime exception ) { throw ( runtime exception ) cause ; } else { throw new io exception ( se ) ; },exception contains,success,pre
not testing string equality since some browsers <PLACE_HOLDER> the style with quotes around the url argument and some without quotes .,assert true ( background image + __str__ @$ background image . contains ( __str__ ) ) ;,equality invert,fail,pre
a media check on anki droid will also <PLACE_HOLDER> the media db,col . get media ( ) . find changes ( true ) ;,check update,success,pre
all rows <PLACE_HOLDER> this class,model = new object table model ( columns @$ calculator . class @$ new functor [ ] { new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) } @$ new functor [ ] { null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null } @$ new class [ ] { string . class @$ integer . class @$ long .,rows have,success,pre
check if we can use the fast path @$ resuming a session . we can do so iff we have a valid record for that session @$ and the cipher suite for that session was on the list which the client <PLACE_HOLDER> @$ and if we 're not forgetting any needed authentication on the part of the client .,if ( previous != null ) { resuming session = previous . is rejoinable ( ) ; if ( resuming session ) { protocol version old version = previous . get protocol version ( ) ; if ( old version != mesg . protocol version ) { resuming session = false ; } } if ( resuming session && use extended master secret ) { if ( requested to useems && ! previous . get use extended master secret ( ) ) { resuming session = false ; } else if ( ! requested to useems && previous . get use extended master secret ( ) ) { fatalse ( alerts . alert_handshake_failure @$ __str__ + __str__ ) ; } else if ( ! requested to useems &&,client sent,fail,pre
there are context registers to worry about @$ so we need a function <PLACE_HOLDER> action and a function <PLACE_HOLDER> analyzer.context action for each context register,map < string @$ big integer > regs to values = c reg filter . get value map ( ) ; match action [ ] actions = new match action [ __num__ + regs to values . size ( ) ] ; actions [ __num__ ] = func start analyzer . new function start action ( ) ; int match index = __num__ ; for ( string register : regs to values . key set ( ) ) { big integer value = regs to values . get ( register ) ; actions [ match index ] = func start analyzer . new context action ( register @$ value ) ; match index ++ ; } return actions ;,function start,success,pre
update the backup if the user id or any of the <PLACE_HOLDER> up android preferences change .,shared prefs . register on shared preference change listener ( new shared preferences . on shared preference change listener ( ) { @ override public void on shared preference changed ( shared preferences shared preferences @$ string key ) { if ( key . equals ( chrome signin controller . signed_in_account_key ) ) { on backup prefs changed ( ) ; return ; } for ( string pref : chrome backup agent . backup_android_bool_prefs ) { if ( key . equals ( pref ) ) { on backup prefs changed ( ) ; return ; } } } } ) ;,id mix,fail,pre
the first page load is used only to cause the web view to fetch the proxy settings . do n't <PLACE_HOLDER> the url bar @$ and do n't check if the captive portal is still there .,if ( m pages loaded == __num__ ) return ;,settings check,fail,pre
each sample <PLACE_HOLDER> up to three bytes of overhead for the start code that replaces its length . allow ten source samples per output sample @$ like the platform extractor .,int max input size = track sample table . maximum size + __num__ * __num__ ; format format = track . format . copy with max input size ( max input size ) ; if ( track . type == c . track_type_video && track duration us > __num__ && track sample table . sample count > __num__ ) { float frame rate = track sample table . sample count / ( track duration us / __num__ ) ; format = format . copy with frame rate ( frame rate ) ; } format = metadata util . get format with metadata ( track . type @$ format @$ udta metadata @$ mdta metadata @$ gapless info holder ) ; mp4 track . track output . format (,sample has,success,pre
try to update something that does n't exists should not <PLACE_HOLDER> existing record,repository . update all ( collections . singleton ( generator . get ( ) ) ) ;,something create,fail,pre
the task manager wo n't <PLACE_HOLDER> heartbeat requests to the resource manager,resource manager gateway . heartbeat from task manager ( resourceid @$ heartbeat payload ) ;,manager send,success,pre
if we 're only parsing integers @$ or if we already saw the decimal @$ then do n't <PLACE_HOLDER> this one .,if ( is parse integer only ( ) || saw decimal ) { break ; } digits . decimal at = digit count ;,then parse,success,pre
button.foreground @$ button.shadow @$ button.dark shadow @$ button.disabled forground @$ and button.disabled shadow are only <PLACE_HOLDER>d for windows classic . windows xp will <PLACE_HOLDER> colors from the current visual style .,object [ ] defaults = { __str__ @$ null @$ __str__ @$ boolean . value of ( use system font settings ) @$ __str__ @$ field input map @$ __str__ @$ password input map @$ __str__ @$ multiline input map @$ __str__ @$ multiline input map @$ __str__ @$ multiline input map @$ __str__ @$ control font @$ __str__ @$ control background color @$ __str__ @$ control text color @$ __str__ @$ control shadow color @$ __str__ @$ control dark shadow color @$ __str__ @$ control light color @$ __str__ @$ control highlight color @$ __str__ @$ inactive text color @$ __str__ @$ control highlight color @$ __str__ @$ button focus color @$ __str__ @$ new xp value ( integer . value of ( __num__ ) @$ integer,shadow use,success,pre
somebody <PLACE_HOLDER> away our unwanted ducks .,if ( delta <= unused guaranteed ) { int result = ( unused guaranteed -= delta ) ; if ( metrics != null ) { metrics . set wm unused guaranteed ( result ) ; } wm_log . info ( __str__ + result + __str__ + delta ) ; return ; } else { delta -= unused guaranteed ; unused guaranteed = __num__ ; to update = new array list < > ( ) ; int total updated = revoke guaranteed ( delta @$ null @$ to update ) ; if ( metrics != null ) { metrics . set wm unused guaranteed ( __num__ ) ; } wm_log . info ( __str__ + total updated + __str__ + delta ) ; if ( delta != total updated,somebody eats,fail,pre
for each dimension <PLACE_HOLDER> the length of each dimension followed by the values,try { for ( int i = __num__ ; i < size ; i ++ ) { int length = in . read int ( ) ; byte [ ] b = new byte [ length ] ; in . read fully ( b ) ; dimension values [ i ] = new string ( b @$ __str__ ) ; } } catch ( exception e ) { logger . info ( arrays . to string ( bytes ) @$ e ) ; throw new runtime exception ( e ) ; } return new dimension key ( dimension values ) ;,dimension read,success,pre
we need something that will measure the amount of time since our consumer has <PLACE_HOLDER> a record ...,time since time since last record = threads . time since ( clock . system ) ;,consumer received,fail,pre
platform.run later is necessary or profiles will be empty since check profiles <PLACE_HOLDER> 2 base profile later .,platform . run later ( ( ) -> { initialized = true ; selected profile . set ( profiles . stream ( ) . filter ( it -> it . get name ( ) . equals ( config ( ) . get selected profile ( ) ) ) . find first ( ) . or else ( profiles . get ( __num__ ) ) ) ; } ) ; event bus . event_bus . channel ( refreshed versions event . class ) . register weak ( event -> { run infx ( ( ) -> { profile profile = selected profile . get ( ) ; if ( profile != null && profile . get repository ( ) == event . get source ( ) ) { selected,profiles add,fail,pre
any activity on the power button <PLACE_HOLDER> the accessibility shortcut,cancel pending accessibility shortcut action ( ) ; result &= ~ action_pass_to_user ; is wake key = false ;,activity performs,fail,pre
check if the grantor <PLACE_HOLDER> current user,if ( grant info . get grantor ( ) != null && grant info . get grantor ( ) . equals ( user name ) && grant info . get grantor type ( ) == principal type . user ) { priv2priv obj . put ( grant info . get privilege ( ) @$ ms obj priv ) ; },grantor matches,success,pre
for annotation type @$ if annotation only <PLACE_HOLDER> single mandatory attribute which is called 'value ' it will be automatically turned into constructor parameter,if ( is annotation value attribute ( ) && there are no other mandatory attributes ( ) ) { return constructor_parameter_default_order ; },annotation has,fail,pre
update application catalog sometimes <PLACE_HOLDER> the password . the second procedure call will fail in that case @$ so do n't call it a second time .,if ( pre hash && ! proc name . equals ( __str__ ) ) { params . put ( __str__ @$ get hashed password forhttp var ( password @$ client auth scheme . hash_sha1 ) ) ; call proc overjson raw ( params @$ http port @$ expected code @$ session id ) ; },catalog changes,success,pre
dalvik vm <PLACE_HOLDER> classes in an apk that are already defined . framework classes take precedence over local classes .,string class name = smali file . get class name ( ) ; if ( is framework class ( class name ) && ! class name . starts with ( __str__ ) ) { log . warn ( __str__ @$ class name ) ; } else { smali files . add ( smali file ) ; },vm creates,fail,pre
local merge source must <PLACE_HOLDER> a single driver,context . set driver instance count ( __num__ ) ; plan node source node = get only element ( node . get sources ( ) ) ; local execution plan context sub context = context . create sub context ( ) ; physical operation source = source node . accept ( this @$ sub context ) ; int operators count = sub context . get driver instance count ( ) . or else ( __num__ ) ; list < type > types = get source operator types ( node @$ context . get types ( ) ) ; local exchange factory exchange factory = new local exchange factory ( node . get partitioning scheme ( ) . get partitioning ( ) . get handle ( ) @$ operators,source have,success,pre
second elector <PLACE_HOLDER> election @$ becomes standby .,electors [ __num__ ] . join election ( app datas [ __num__ ] ) ; mockito . verify ( cbs [ __num__ ] @$ mockito . timeout ( __num__ ) ) . become standby ( ) ; check fatals and reset ( ) ;,elector joins,success,pre
need to go through all elements @$ extract data @$ and check distance agains new centroids <PLACE_HOLDER> list of all centroids,int i ; int j ; int k ; int m ; double [ ] temp = new double [ __num__ ] ;,centroids contains,fail,pre
static clinits are generated in gen @$ so we need to use a fake one . attr <PLACE_HOLDER> a fake clinit method while attributing lambda expressions used as initializers of static fields @$ so let 's use that one .,if ( is static ) { method symbol clinit = attr . remove clinit ( csym ) ; if ( clinit != null ) { clinits . put ( csym @$ clinit ) ; return clinit ; } clinit = ( method symbol ) clinits . get ( csym ) ; if ( clinit == null ) { clinit = make private synthetic method ( static @$ names . clinit @$ new method type ( list . < type > nil ( ) @$ syms . void type @$ list . < type > nil ( ) @$ syms . method class ) @$ csym ) ; clinits . put ( csym @$ clinit ) ; } return clinit ; } else { for ( symbol s :,one has,fail,pre
when table limit feature is fully supported @$ there needs to be more <PLACE_HOLDER> cases . generalize this <PLACE_HOLDER> within a loop @$ maybe . <PLACE_HOLDER> max row 0,vt = client . call procedure ( __str__ @$ __str__ ) . get results ( ) [ __num__ ] ; validate table of scalar longs ( vt @$ new long [ ] { __num__ } ) ; verify proc fails ( client @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ ) ; vt = client . call procedure ( __str__ @$ __str__ ) . get results ( ) [ __num__ ] ; validate table of scalar longs ( vt @$ new long [ ] { __num__ } ) ;,cases test,success,pre
previously we cloned the artifact @$ but it is more efficient to just update the artifact scope if problems are later discovered that the original object <PLACE_HOLDER> its original artifact scope value @$ cloning may again be appropriate,nearest artifact . set scope ( farthest artifact . get scope ( ) ) ;,object had,fail,pre
this is okay . the function <PLACE_HOLDER> a name @$ but it is empty .,break ; case param_list :,function has,success,pre
two different properties can <PLACE_HOLDER> the routing field extraction,chained field extractor . no value handler routing response = chained field extractor . no value handler . skip ; list < field extractor > routings = new array list < field extractor > ( __num__ ) ; if ( settings . get mapping routing ( ) != null ) { settings . set property ( constant field extractor . property @$ settings . get mapping routing ( ) ) ; field extractor extractor = object utils . < field extractor > instantiate ( settings . get mapping routing extractor class name ( ) @$ settings ) ; routing response = chained field extractor . no value handler . not_found ; routings . add ( extractor ) ; },properties trim,fail,pre
global declaration must <PLACE_HOLDER> a name,if ( name attr == null ) { report schema error ( __str__ @$ new object [ ] { __str__ @$ __str__ } @$ elm node ) ; name attr = no_name ; } attr grp . f name = name attr ; attr grp . f target namespace = schema doc . f target namespace ;,declaration have,success,pre
step 2 <PLACE_HOLDER> the signal from step 1 to step 3,thread step2 = new step2 ( context ) ; step2 . start ( ) ;,step receives,fail,pre
wait a little bit to let the delete <PLACE_HOLDER> effect .,thread . sleep ( __num__ ) ;,delete take,success,pre
wait until the task <PLACE_HOLDER> the file,long deadline = system . current time millis ( ) + __num__ ; while ( ! temp test file . exists ( ) && system . current time millis ( ) < deadline ) { thread . sleep ( __num__ ) ; } assert true ( __str__ @$ temp test file . exists ( ) ) ;,task creates,fail,pre
modifier letter glottal stop..modifier letter <PLACE_HOLDER> glottal stop,if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ch == __num__ ) { return true ; } else if ( ch == __num__ ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else { return false ;,letter !,fail,pre
the top of the stack always <PLACE_HOLDER> the current filter . when back is pressed @$ the top is popped off and the new top indicates what filter to use . if there are no filters remaining @$ the activity itself is closed .,if ( m back stack . size ( ) > __num__ ) { m back stack . pop ( ) ; m download manager ui . update for url ( m back stack . peek ( ) ) ; } else { if ( ! m back stack . is empty ( ) ) m back stack . pop ( ) ; super . on back pressed ( ) ; },top contains,fail,pre
partitioned tables do n't <PLACE_HOLDER> table desc set on the fetch task . instead they <PLACE_HOLDER> a list of partition desc objects @$ each with a table desc . let 's try to fetch the desc for the first partition and use it 's deserializer .,if ( td == null && ft . get work ( ) != null && ft . get work ( ) . get part desc ( ) != null ) { if ( ft . get work ( ) . get part desc ( ) . size ( ) > __num__ ) { td = ft . get work ( ) . get part desc ( ) . get ( __num__ ) . get table desc ( ) ; } } if ( td == null ) { log . info ( __str__ ) ; } else { string table name = __str__ ; list < field schema > lst = null ; try { lst = hive meta store utils . get fields from deserializer ( table,tables have,success,pre
when clearing the selection after a text change @$ state is not reset correctly so hitting down again will cause it to start from the previous selection point . we still have to send the key down event to let the list view items <PLACE_HOLDER> focus @$ but then we select the first item explicitly .,if ( m suggestion list . get selected item position ( ) == list view . invalid_position ) { boolean result = m suggestion list . on key down ( key code @$ event ) ; m suggestion list . set selection ( __num__ ) ; return result ; } else { return m suggestion list . on key down ( key code @$ event ) ; },items receive,fail,pre
if the option <PLACE_HOLDER> a value and a non blank argname,if ( option . has arg ( ) && ( option . get arg name ( ) == null || option . get arg name ( ) . length ( ) != __num__ ) ) { buff . append ( option . get opt ( ) == null ? long opt separator : __str__ ) ; buff . append ( __str__ ) . append ( option . get arg name ( ) != null ? option . get arg name ( ) : get arg name ( ) ) . append ( __str__ ) ; },option has,success,pre
ensure the insertion logic can <PLACE_HOLDER> a comment appended to the front,do in hibernate ( this :: session factory @$ s -> { query < employee > query = s . create query ( __str__ @$ employee . class ) . set comment ( __str__ ) . add query hint ( __str__ ) . set parameter ( __str__ @$ __str__ ) ; list < employee > results = query . list ( ) ; assert equals ( results . size ( ) @$ __num__ ) ; } ) ; sql statement interceptor . assert executed count ( __num__ ) ; assert that ( sql statement interceptor . get sql queries ( ) . get ( __num__ ) @$ contains string ( __str__ ) ) ; sql statement interceptor . clear ( ) ;,logic handle,success,pre
flag <PLACE_HOLDER> notation for checking after the dtd is fully processed,if ( notations . get ( external id . notation ) == null ) notations . put ( external id . notation @$ boolean . true ) ;,flag enabled,fail,pre
check that slave <PLACE_HOLDER> min requirements .,if ( cpus < cluster props . min cpu per node ( ) || mem < cluster props . min memory per node ( ) ) { log . log ( level . fine @$ __str__ @$ offer . get resources list ( ) ) ; return null ; } double total cpus = __num__ ; double total mem = __num__ ; double total disk = __num__ ;,slave has,fail,pre
wake up the reader ... there <PLACE_HOLDER> stuff to do @$ data to read,has read ahead = false ; lock support . unpark ( this ) ;,stuff is,fail,pre
let 's <PLACE_HOLDER> the default path,lst . add ( __str__ ) ;,'s add,fail,pre
the ' : ' at pos p may be either a port divider or a part of an i <PLACE_HOLDER> 6 address,if ( p > target . last index of ( __str__ ) ) { target = target . substring ( __num__ @$ p ) ; },part pv,success,pre
when final state in last allocation is not default @$ it means last allocation has finished . <PLACE_HOLDER> a new allocation for this node @$ and add it to the allocation list . return this new allocation . when final state in last allocation is default @$ it means last allocation has not finished . just get last allocation .,if ( node allocation . get final allocation state ( ) != allocation state . default ) { node allocation = new node allocation ( nodeid ) ; node allocations . add ( node allocation ) ; },return create,success,pre
if the test <PLACE_HOLDER> a specific order @$ do not reorder .,return get description ( ) . get annotation ( fix method order . class ) != null ;,test requires,fail,pre
simple equals can <PLACE_HOLDER> troubles here be<PLACE_HOLDER> of how equals works e.g . between lists and sets .,return collection utils . is equal collection ( state objects @$ that . state objects ) ;,equals cause,success,pre
j 2 se does not <PLACE_HOLDER> xalan interpretive,do exit ( msg ) ;,se support,success,pre
then the write behind action <PLACE_HOLDER> 3 entries to write,awaitility . await ( ) . until true ( writer called ) ; assert . assert equals ( __num__ @$ number of entries . int value ( ) ) ;,write has,fail,pre
first <PLACE_HOLDER> existing files from the files table . because we 'll be deleting entries for missing files as we go @$ we need to query the database in small batches @$ to avoid problems with cursor window positioning .,if ( prescan files ) { long last id = long . min_value ; uri limit uri = m files uri . build upon ( ) . append query parameter ( media store . param_limit @$ __str__ ) . build ( ) ; while ( true ) { selection args [ __num__ ] = __str__ + last id ; if ( c != null ) { c . close ( ) ; c = null ; } c = m media provider . query ( limit uri @$ files_prescan_projection @$ where @$ selection args @$ media store . files . file columns . _id @$ null ) ; if ( c == null ) { break ; } int num = c . get count ( ) ;,first remove,fail,pre
this happens at least if the server is restarted without preserving the session . after restart the client reconnects @$ gets a session <PLACE_HOLDER> notification and then closes the connection and ends up here,get logger ( ) . log ( level . finer @$ __str__ @$ e ) ; return ;,session revalidated,fail,pre
and <PLACE_HOLDER> a header,select conf . set ( csv_input_header @$ csv_header_opt_use ) ; select conf . set boolean ( select_errors_include_sql @$ true ) ; input must ( select conf @$ csv_input_header @$ csv_header_opt_use ) ; input must ( select conf @$ select_input_format @$ select_format_csv ) ; input must ( select conf @$ select_output_format @$ select_format_csv ) ; input must ( select conf @$ select_input_compression @$ compression_opt_gzip ) ;,and have,fail,pre
robo vm note : seems like the gc can not <PLACE_HOLDER> the list after this method finishes . it 's probably still referenced by some register . clear it to make its elements <PLACE_HOLDER>able .,list . clear ( ) ; return true ;,gc clear,fail,pre
the anonymous class ca n't <PLACE_HOLDER> generics @$ but we may be binding generics from super classes,if ( clazz . is anonymous class ( ) ) { if ( clazz . get interfaces ( ) . length != __num__ ) { type parameters = clazz . get interfaces ( ) [ __num__ ] . get type parameters ( ) ; } else { type parameters = clazz . get superclass ( ) . get type parameters ( ) ; } } else { type parameters = clazz . get type parameters ( ) ; },class have,success,pre
the configuration distinguisher is only <PLACE_HOLDER> by apple crosstool transition and apple binary transition @$ both of which also <PLACE_HOLDER> the crosstool and the cpu to apple ones . so we are fine not doing anything .,if ( apple options . configuration distinguisher != configuration distinguisher . unknown ) { return build options ; },which shared,fail,pre
accessibility needs to be able to interact with the device . if these settings are already configured @$ we will not overwrite them . if they are already set @$ it means that the user has <PLACE_HOLDER> a global gesture to enable accessibility or set these settings in the accessibility portion of the setup wizard @$ and definitely needs these features working after the,switch ( name ) { case settings . secure . accessibility_enabled : case settings . secure . touch_exploration_enabled : case settings . secure . accessibility_display_daltonizer_enabled : case settings . secure . accessibility_display_magnification_enabled : case settings . secure . accessibility_display_magnification_navbar_enabled : return settings . secure . get int ( m context . get content resolver ( ) @$ name @$ __num__ ) != __num__ ; case settings . secure . touch_exploration_granted_accessibility_services : case settings . secure . enabled_accessibility_services : case settings . secure . accessibility_display_daltonizer : return ! text utils . is empty ( settings . secure . get string ( m context . get content resolver ( ) @$ name ) ) ; case settings . secure . accessibility_display_magnification_scale : float default scale = m context . get,user performed,success,pre
this is a 'patch configuration ' which <PLACE_HOLDER> 'solr ' as default collection,remote instance instance = new remote instance ( target baseurl + __str__ @$ null @$ null @$ __num__ @$ trust self signed on authenticated server @$ long . max_value @$ false ) ;,which considers,success,pre
tab beyond last line : <PLACE_HOLDER> a line to table !,if ( active table row >= maxrows ) { table item item = new table item ( table @$ swt . none @$ active table row ) ; item . set text ( __num__ @$ __str__ ) ; set row nums ( ) ; } if ( sel ) { edit ( active table row @$ active table column ) ; } table . set focus ( ) ;,tab add,success,pre
the rollback operation should have <PLACE_HOLDER> back the first nn 's local dirs @$ and the shared dir @$ but not the other nn 's dirs . those have to be done by bootstrapping the standby .,check nn previous dir existence ( cluster @$ __num__ @$ false ) ; check jn previous dir existence ( qj cluster @$ false ) ; if ( fs != null ) { fs . close ( ) ; } if ( qj cluster != null ) { qj cluster . shutdown ( ) ; },operation rolled,success,pre
test results of within using <PLACE_HOLDER> function,prefix = __str__ ; sql = __str__ + __str__ + __str__ ; vt1 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; sql = __str__ + __str__ + __str__ ; vt2 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; assert tables are equal ( prefix @$ vt2 @$ vt1 @$ geography_distance_epsilon ) ; sql = __str__ + __str__ + __str__ ; vt1 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; sql = __str__ + __str__ + __str__ ; vt2 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__,results equals,fail,pre
res stores the first <PLACE_HOLDER> abstract method,method res = null ; for ( method mi : methods ) { if ( ! modifier . is abstract ( mi . get modifiers ( ) ) ) continue ; if ( mi . get annotation ( traits . implemented . class ) != null ) continue ; try { object . class . get method ( mi . get name ( ) @$ mi . get parameter types ( ) ) ; continue ; } catch ( no such method exception e ) { } if ( res != null ) return null ; res = mi ; },stores implemented,fail,pre
scope might have set to database in some previous iteration of loop @$ so reset it to false if database tracker <PLACE_HOLDER> no tasks .,scope . database = false ;,tracker has,success,pre
standalone server does n't <PLACE_HOLDER> myid file .,if ( ! my id file . is file ( ) ) { return ; } buffered reader br = new buffered reader ( new file reader ( my id file ) ) ; string my id string ; try { my id string = br . read line ( ) ; } finally { br . close ( ) ; } try { server id = long . parse long ( my id string ) ; mdc . put ( __str__ @$ my id string ) ; } catch ( number format exception e ) { throw new illegal argument exception ( __str__ + my id string + __str__ ) ; },server read,fail,pre
this estimate will not <PLACE_HOLDER> into account the memory saved by inlining the keys .,return vm thin disk region entry off heap object key . class ;,estimate take,success,pre
let 's <PLACE_HOLDER> the viewtree and override all text colors !,stack < view > views to process = new stack < > ( ) ; views to process . add ( root ) ; while ( ! views to process . is empty ( ) ) { view v = views to process . pop ( ) ; if ( v instanceof text view ) { text view text view = ( text view ) v ; text view . set text ( contrast color util . clear color spans ( text view . get text ( ) ) ) ; text view . set text color ( text color ) ; } if ( v instanceof view group ) { view group view group = ( view group ) v ; for ( int i = __num__,'s query,fail,pre
'digest ' already <PLACE_HOLDER> the value from the stream @$ just finish the op,byte [ ] sd = digest . digest ( ) ; digest . reset ( ) ;,'digest reflects,fail,pre
start tx state update during pre <PLACE_HOLDER> phase,set updating tx state during pre commit ( true ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ + __str__ @$ secondary transactional operations . size ( ) ) ; },update commit,success,pre
recursive evaluation should not <PLACE_HOLDER> a suspended event,context . eval ( test source ) ; suspension count . increment and get ( ) ;,evaluation signal,fail,pre
never registered before if null ... also as of current @$ there is ever only 1 notification type per m bean @$ so we do <PLACE_HOLDER> <PLACE_HOLDER> a while loop here,if ( notifications != null ) { set < map . entry < refresh notification type @$ integer > > entries = notifications . entry set ( ) ; for ( map . entry < refresh notification type @$ integer > e : entries ) { integer timer notification id = e . get value ( ) ; if ( null != timer notification id ) { try { refresh timer . remove notification ( timer notification id ) ; } catch ( instance not found exception xptn ) { log stack trace ( level . debug @$ xptn ) ; } } } try { if ( mbean server != null && mbean server . is registered ( refresh timer object name ) ) { mbean server,need do,fail,pre
empty if nobody <PLACE_HOLDER> the specified global permission,query = permission query . builder ( ) . set organization uuid ( organization . get uuid ( ) ) . set permission ( __str__ ) . build ( ) ; expect permissions ( query @$ empty list ( ) ) ;,nobody has,success,pre
unknown slot which basically <PLACE_HOLDER> the exists predicate,if ( ! ( predicates . length == __num__ && predicates [ __num__ ] instanceof exists predicate ) ) { throw new illegal state exception ( __str__ + arrays . to string ( predicates ) ) ; } return true ;,which guarantees,fail,pre
phantom reference does not <PLACE_HOLDER> access to its object @$ so it is mostly useless to have a phantom reference on the image heap . but some jdk code uses it @$ e.g. @$ for marker values @$ so we can not dis<PLACE_HOLDER> phantom reference for the image heap .,if ( receiver instanceof phantom reference ) { return null ; },reference create,fail,pre
ipv 4 <PLACE_HOLDER> last two octets,if ( index > octets . length - __num__ || index > __num__ ) { return false ; },ipv contains,fail,pre
controller and mode events <PLACE_HOLDER> status byte 0 x bc @$ where c is the channel they are sent on .,if ( ( status & __num__ ) == __num__ ) { for ( int i = __num__ ; i < count ; i ++ ) { try { ( ( controller event listener ) event info . get listener ( i ) ) . control change ( event ) ; } catch ( throwable t ) { if ( printer . err ) t . print stack trace ( ) ; } } } return ;,events have,success,pre
set timestamp before moving to cmroot @$ so we can avoid race condition cm <PLACE_HOLDER> the file before setting timestamp,long now = system . current time millis ( ) ; fs . set times ( path @$ now @$ - __num__ ) ; boolean success = false ; if ( fs . exists ( cm path ) && file check sum . equals ignore case ( checksum for ( cm path @$ fs ) ) ) { success = false ; } else { switch ( type ) { case move : { log . info ( __str__ @$ path . to string ( ) @$ cm path . to string ( ) ) ; success = fs . rename ( path @$ cm path ) ; break ; } case copy : { log . info ( __str__ @$ path . to string ( ) @$,cm has,fail,pre
for mwt created core labels <PLACE_HOLDER> all of them to the character off<PLACE_HOLDER>s of the mwt,int lastmwt char begin = - __num__ ; int lastmwt char end = - __num__ ; for ( core label cl : core labels ) { if ( sentence . mwt data . contains key ( cl . index ( ) - __num__ ) ) { if ( sentence . mwt data . get ( cl . index ( ) - __num__ ) == processedmwt tokens ) { cl . set begin position ( doc . doc text . length ( ) ) ; doc . doc text += sentence . mwt tokens . get ( processedmwt tokens ) ; cl . set end position ( doc . doc text . length ( ) ) ; lastmwt char begin = cl . begin position ( ) ; lastmwt,labels convert,fail,pre
if the container is <PLACE_HOLDER> a health check @$ any previous image failure has been resolved,reset image failure ( ) ; update state ( healthchecking ) ;,container running,success,pre
inspect the source c es . just <PLACE_HOLDER> them if none are modified . otherwise <PLACE_HOLDER> to modified c es @$ with modifications .,boolean is modified = false ; for ( int i = __num__ ; i < length ; ++ i ) { long srcce = srcc es [ src index + i ] ; long ce = modifier . modifyce ( srcce ) ; if ( ce == collation . no_ce ) { if ( is modified ) { modifiedc es [ i ] = srcce ; } } else { if ( ! is modified ) { for ( int j = __num__ ; j < i ; ++ j ) { modifiedc es [ j ] = srcc es [ src index + j ] ; } is modified = true ; } modifiedc es [ i ] = ce ; } } if ( is modified,es add,fail,pre
mark a fragment as completing @$ but do n't actually complete it yet . the wait queue should now <PLACE_HOLDER> capacity to accept one more fragment .,task executor service . fragment completing ( r1 . get request id ( ) @$ scheduler fragment completing listener . state . success ) ; submission state = task executor service . schedule ( r4 ) ; assert equals ( scheduler . submission state . accepted @$ submission state ) ; assert equals ( __num__ @$ task executor service . wait queue . size ( ) ) ; assert equals ( __num__ @$ task executor service . completing fragment map . size ( ) ) ; r1 . complete ( ) ; r1 . await end ( ) ;,queue have,success,pre
ima sometimes unexpectedly <PLACE_HOLDER> the ad count in an ad group .,log . w ( tag @$ __str__ + ad count + __str__ + old ad count ) ;,ima decrements,fail,pre
label visitor <PLACE_HOLDER> some legacy special handling of output files .,if ( target instanceof output file ) { rule rule = ( ( output file ) target ) . get generating rule ( ) ; observe edge ( target @$ null @$ rule ) ; visit ( null @$ null @$ rule @$ depth + __num__ @$ count + __num__ ) ; } label visitation utils . visit target exceptionally ( target @$ edge filter @$ ( from target @$ attribute @$ to label ) -> enqueue target ( target @$ attribute @$ to label @$ depth @$ count ) ) ;,visitor does,fail,pre
if right expression is a closure expression @$ store <PLACE_HOLDER> type information,if ( left expression instanceof variable expression ) { if ( right expression instanceof closure expression ) { parameter [ ] parameters = ( ( closure expression ) right expression ) . get parameters ( ) ; left expression . put node meta data ( static types marker . closure_arguments @$ parameters ) ; } else if ( right expression instanceof variable expression && ( ( variable expression ) right expression ) . get accessed variable ( ) instanceof expression && ( ( expression ) ( ( variable expression ) right expression ) . get accessed variable ( ) ) . get node meta data ( static types marker . closure_arguments ) != null ) { variable target variable = find target variable ( ( variable expression ),store parameter,success,pre
actions will <PLACE_HOLDER> key bindings @$ so do n't process them,if ( enable action key bindings ) { return ; },actions enable,fail,pre
set max object size a little less than 1024 1024 @$ because the key of the segment result cache is long if set to 1024 1024 will cause memcached client <PLACE_HOLDER> max size error,memcached cache config . set max object size ( __num__ ) ; memcached cache config . set hosts ( config hosts ) ;,client written,fail,pre
some controls <PLACE_HOLDER> text based on the view dimensions @$ so update now .,update controls ( ) ; surface view sv = ( surface view ) find view by id ( r . id . hardware scaler_surface view ) ; m render thread = new render thread ( sv . get holder ( ) ) ; m render thread . set name ( __str__ ) ; m render thread . start ( ) ; m render thread . wait until ready ( ) ; render handler rh = m render thread . get handler ( ) ; if ( rh != null ) { rh . send set flat shading ( m flat shading checked ) ; rh . send surface created ( ) ; },controls change,fail,pre
only post the callback to stop the service if the service does not <PLACE_HOLDER> an open connection .,if ( m opened count . decrement and get ( ) != __num__ ) { return ; },service have,success,pre
send message to all peers to find out who <PLACE_HOLDER> the transaction,if ( ! tx . is real deal local ( ) ) { find remotetx message reply processor processor = send find remotetx message ( server connection . get cache ( ) @$ tx id ) ; try { processor . wait for replies uninterruptibly ( ) ; } catch ( reply exception e ) { e . handle cause ( ) ; } internal distributed member hosting member = processor . get hosting member ( ) ; if ( hosting member != null ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ hosting member ) ; } if ( tx . get target ( ) == null ) { tx . set target ( hosting member ),who originated,fail,pre
this is necessary to have the rm <PLACE_HOLDER> our vcore allocation request,conf . set class ( capacity scheduler configuration . resource_calculator_class @$ dominant resource calculator . class @$ resource calculator . class ) ; conf . set boolean ( yarn configuration . nm_disk_health_check_enable @$ false ) ; miniyarn cluster = new miniyarn cluster ( test dynamometer infra . class . get name ( ) @$ __num__ @$ minicluster_num_nms @$ __num__ @$ __num__ ) ; miniyarn cluster . init ( conf ) ; miniyarn cluster . start ( ) ; yarn conf = miniyarn cluster . get config ( ) ; minidfs cluster = new minidfs cluster . builder ( conf ) . format ( true ) . num data nodes ( minicluster_num_dns ) . build ( ) ; minidfs cluster . wait cluster up ( ) ; file system,rm handle,fail,pre
jls does not <PLACE_HOLDER> array types to be used as bounds of type variables,return false ;,jls allow,success,pre
grab plugin executions that are bound to the selected lifecycle p<PLACE_HOLDER>es from project . the effective model of the project already contains the plugin executions induced by the project 's packaging type . remember @$ all p<PLACE_HOLDER>es of interest and only those are in the lifecycle mapping @$ if a p<PLACE_HOLDER>e <PLACE_HOLDER> no value in the map @$ we are not interested in any,for ( plugin plugin : project . get build ( ) . get plugins ( ) ) { for ( plugin execution execution : plugin . get executions ( ) ) { if ( execution . get phase ( ) != null ) { map < integer @$ list < mojo execution > > phase bindings = mappings . get ( execution . get phase ( ) ) ; if ( phase bindings != null ) { for ( string goal : execution . get goals ( ) ) { mojo execution mojo execution = new mojo execution ( plugin @$ goal @$ execution . get id ( ) ) ; mojo execution . set lifecycle phase ( execution . get phase ( ) ) ; add mojo,phase has,success,pre
setup <PLACE_HOLDER> and buck config .,path canonical root path = project root . to real path ( ) . normalize ( ) ; immutable map < cell name @$ path > root cell mapping = get cell mapping ( canonical root path ) ; immutable list < string > args = buck args methods . expand at files ( unexpanded command line args @$ root cell mapping ) ;,setup build,fail,pre
now @$ try <PLACE_HOLDER> the reservation dynamically and it should fail,try { store definition defa = test utils . make store definition ( __str__ @$ __num__ ) ; bdb storage . update ( defa ) ; fail ( __str__ ) ; } catch ( storage initialization exception sie ) { },try update,fail,pre
iterate over alluxio children and process <PLACE_HOLDER> children .,for ( map . entry < string @$ inode > inode entry : inode children . entry set ( ) ) { if ( ! inode entry . get value ( ) . is persisted ( ) ) { continue ; } try ( locked inode path descendant = inode path . lock descendant ( inode path . get uri ( ) . join unsafe ( inode entry . get key ( ) ) @$ lock pattern . write_edge ) ) { if ( sync descendant type != descendant type . all ) { sync descendant type = descendant type . none ; } sync result sync result = sync inode metadata ( rpc context @$ descendant @$ sync descendant type @$ status cache ) ; paths to,iterate attributed,fail,pre
do this last because the prior operations could <PLACE_HOLDER> exceptions .,maybe set up metrics recorder ( context @$ configs ) ;,operations throw,success,pre
here client <PLACE_HOLDER> initial connection and fetches topology . only first server in list should be contacted .,try ( grid client client = client ( ) ) { assert equals ( __num__ @$ srvs [ __num__ ] . get connect count ( ) ) ; for ( int i = __num__ ; i < client test rest server . servers_cnt ; i ++ ) assert equals ( __num__ @$ srvs [ i ] . get connect count ( ) ) ; srvs [ __num__ ] . reset counters ( ) ; int contacted srv = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { int failed = contacted srv ; srvs [ failed ] . fail ( ) ; while ( true ) try { client . compute ( ) . refresh topology ( false @$ false,client opens,success,pre
the two <PLACE_HOLDER> the same p collection and can exist in the same stage .,if ( existing consumers . stream ( ) . all match ( collection consumer -> greedyp collection fusers . is compatible ( collection consumer . consuming transform ( ) @$ new consumer . consuming transform ( ) @$ pipeline ) ) ) { existing consumers . add ( new consumer ) ; found siblings = true ; break ; },two share,fail,pre
setup the full outer intersect <PLACE_HOLDER> join 's input obj inspector to include the small table @$ etc .,intersect map join operator . set input obj inspectors ( intercept test desc . input object inspectors ) ;,setup map,success,pre
this is taken care in wrapper which <PLACE_HOLDER> xni callbacks @$ there are no next events,throw new java . io . eof exception ( ) ;,which invokes,fail,pre
get twitter <PLACE_HOLDER> a flow file @$ then it 's sent via s 2 s,tc . add lineage ( create lineage ( prs @$ __num__ @$ __num__ @$ __num__ ) ) ; test ( tc ) ; wait notifications get delivered ( ) ; final lineage lineage = get lineage ( ) ; final node flow = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patha = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathb = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathc = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patht = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathi = lineage . find node (,twitter creates,fail,pre
add the input <PLACE_HOLDER> string and date time to lists,file strings . add ( input line ) ; dt objects . add ( calculateddt ) ;,input csv,fail,pre
we 'll be able to remove this once we have proper typed key subclasses @$ like frame key . <PLACE_HOLDER> the new type both in schema and in schema v 3 @$ so that they are always in sync .,string new type = __str__ + get keyed class type ( ) + __str__ ; __meta . schema_type = new type ; set schema type_do not call ( new type ) ;,3 set,success,pre
ideally @$ we would just install the next phase using the singleton policy @$ however deployment unit phases do not currently support <PLACE_HOLDER>s <PLACE_HOLDER> the deployment using the attached phase builder @$ but only if a builder was not already attached,if ( unit . put attachment ( attachments . deployment_unit_phase_builder @$ new singleton deployment unit phase builder ( support @$ policy ) ) == null ) { singleton logger . root_logger . singleton deployment detected ( policy ) ; service controller < ? > controller = context . get service registry ( ) . get required service ( unit . get service name ( ) ) ; controller . add listener ( this ) ; controller . set mode ( mode . never ) ; },restarts build,fail,pre
if the request does n't <PLACE_HOLDER> a session cookie @$ we 're not going to renew one .,if ( ! request . get cookies ( ) . contains key ( session cookie name ) ) { return ; } cookie request cookie = request . get cookies ( ) . get ( session cookie name ) ; optional < user > optional user = authenticator . authenticate ( request cookie ) ; if ( optional user . is present ( ) ) { session login resource . cookies for user ( optional user . get ( ) ) . for each ( c -> response . get headers ( ) . add ( http headers . set_cookie @$ c ) ) ; },request have,success,pre
none of the results got the data <PLACE_HOLDER> the block .,blocks . add ( no_data ) ;,data find,fail,pre
create a role filesystem which does not have <PLACE_HOLDER> access under a path it still has write access @$ which can be explored in the final step to delete files and directories .,role config = create assumed role config ( ) ; bind role policy statements ( role config @$ statement_s3guard_client @$ statement_allow_sse_kms_rw @$ statement ( true @$ s3_all_buckets @$ s3_all_operations ) @$ new statement ( effects . deny ) . add actions ( s3_all_get ) . add resources ( directory ( no read dir ) ) ) ; readonlyfs = ( s3a file system ) base path . get file system ( role config ) ; verifys3 guard settings ( readonlyfs @$ __str__ ) ;,which read,success,pre
we set member.id to empty here explicitly @$ so that the lookup will succeed as user does n't <PLACE_HOLDER> the exact member.id .,for ( member response member response : response . member responses ( ) ) { member errors . put ( new member identity ( ) . set member id ( join group request . unknown_member_id ) . set group instance id ( member response . group instance id ( ) ) @$ errors . for code ( member response . error code ( ) ) ) ; },user specify,fail,pre
both <PLACE_HOLDER> from db @$ no need to run this .,return ;,both fetched,fail,pre
most commands <PLACE_HOLDER> the object number for p 2 @$ nut not all,int param2 = config . get device ( ) . get properties ( ) . get number ( ) ; logger . debug ( __str__ + config . get object type ( ) ) ; switch ( config . get object type ( ) ) { case unit : { unit unit = ( unit ) config . get device ( ) ; boolean resend to children = false ; if ( command == on off type . on ) { cmd = omni link cmd . cmd_unit_on . get number ( ) ; } else if ( command == on off type . off ) { cmd = omni link cmd . cmd_unit_off . get number ( ) ; } else if ( command == increase decrease,commands need,fail,pre
run the k <PLACE_HOLDER> driver,path answer = new path ( output @$ __str__ ) ; k means driver . run ( conf @$ data @$ initialclusters @$ answer @$ convergence delta @$ max iterations @$ true @$ __num__ @$ false ) ;,k means,success,pre
empty string <PLACE_HOLDER> broadest possible search,owner . set last name ( __str__ ) ;,string means,fail,pre
changing the display name has surely brought a change in the order as well so let 's <PLACE_HOLDER> the others,fire meta contact group event ( find parent meta contact group ( meta contact ) @$ null @$ null @$ meta contact group event . child_contacts_reordered ) ;,'s tell,success,pre
start data <PLACE_HOLDER> operations from several threads .,final atomic boolean stopped = new atomic boolean ( ) ; ignite internal future update fut = multithreaded async ( new callable < void > ( ) { @ override public void call ( ) throws exception { while ( ! stopped . get ( ) ) { ignite node = grid ( thread local random . current ( ) . next int ( __num__ @$ __num__ ) ) ; int key = thread local random . current ( ) . next int ( __num__ @$ large_cache_size ) ; int val = thread local random . current ( ) . next int ( ) ; binary object key obj = key ( node @$ key ) ; if ( thread local random . current ( ) . next,data update,fail,pre
creating the handler starts the preview @$ which can also <PLACE_HOLDER> a runtime exception .,if ( handler == null ) { handler = new capture activity handler ( this @$ decode formats @$ decode hints @$ character set @$ camera manager ) ; } decode or store saved bitmap ( null @$ null ) ;,which throw,success,pre
return false because function types do not <PLACE_HOLDER> a null convention,if ( definition argument properties . get ( i ) . get argument type ( ) == function_type ) { if ( invocation argument convention != invocation argument convention . function ) { return false ; } throw new unsupported operation exception ( __str__ ) ; },types have,success,pre
first see if the file <PLACE_HOLDER> the regular expression !,if ( pattern source != null ) { matcher matcher = pattern source . matcher ( filename ) ; unzip = matcher . matches ( ) ; } if ( unzip ) { if ( ! unzip file ( children [ i ] @$ real targetdirectory @$ real wildcard @$ real wildcard exclude @$ result @$ parent job @$ movetodir @$ real movetodirectory ) ) { update errors ( ) ; } else { update success ( ) ; } },file matches,success,pre
manager <PLACE_HOLDER> this fragment to be paused,if ( get state ( ) == generic fragment . state_paused ) { return ; } if ( util . sdk_int > __num__ ) { perform initialization ( ) ; },manager expects,fail,pre
js 2 e does not <PLACE_HOLDER> this exception,key pair gen . initialize ( __num__ @$ new secure random ( ) ) ; try { key pair gen . initialize ( __num__ @$ new secure random ( ) ) ; fail ( __str__ ) ; } catch ( invalid parameter exception e ) { } key pair gen . initialize ( __num__ @$ null ) ; assert null ( __str__ @$ key pair gen . generate key pair ( ) ) ; assert null ( __str__ @$ key pair gen . gen key pair ( ) ) ; break ; case __num__ : key pair gen . initialize ( pp @$ new secure random ( ) ) ; key pair gen . initialize ( pp ) ; key pair gen . initialize ( __num__ @$ new,e throw,success,pre
exclusion only <PLACE_HOLDER> sense for a union type .,if ( outcome . boolean values == boolean literal set . empty && get native type ( boolean_type ) . is subtype of ( type ) ) { if ( type . is union type ( ) ) { type = type . to maybe union type ( ) . get restricted union ( get native type ( boolean_type ) ) ; } },exclusion makes,success,pre
the page supplier has <PLACE_HOLDER> the page reference count @$ and add pages below also increments the reference count @$ so we need to drop the page supplier reference . the call dereference page is performed outside of synchronized to avoid making a callback while holding a lock .,page references = pages supplier . get pages ( max size ) ;,supplier incremented,success,pre
deregister as a listener to <PLACE_HOLDER> computational load,remove event listener ( ) ;,listener reduce,success,pre
2 nd time launch service to handle if service <PLACE_HOLDER> scenario,system service . launch user service ( user services ) ; verify for launched user services ( ) ;,service fails,fail,pre
before inlining happens remove unused code sees one use of inner c @$ which <PLACE_HOLDER> its removal . after inlining it sees ` this instanceof inner c ` as the only use of inner c. make sure remove unused code recognizes that the value of inner c escapes .,test ( options @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,which triggers,fail,pre
first <PLACE_HOLDER> special cases . if one of the special case methods can not <PLACE_HOLDER> it @$ it returns null .,vector expression ve = null ; if ( udf instanceof genericudf between ) { ve = get between expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf in ) { ve = get in expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf if ) { ve = get if expression ( ( genericudf if ) udf @$ child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf when ) { ve = get when expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudfop positive ) { ve = get identity expression ( child expr,one handle,success,pre
each polling event will <PLACE_HOLDER> a load and comparison of config tree,return optional . of ( instant . now ( ) ) ;,event perform,fail,pre
by default @$ let 's just <PLACE_HOLDER> the x offset .,do { final int x change = mx offset direction * burn_in_shift_step ; m last burn inx offset += x change ; if ( m last burn inx offset > m max horizontal burn in offset || m last burn inx offset < m min horizontal burn in offset ) { m last burn inx offset -= x change ; mx offset direction *= - __num__ ; final int y change = my offset direction * burn_in_shift_step ; m last burn iny offset += y change ; if ( m last burn iny offset > m max vertical burn in offset || m last burn iny offset < m min vertical burn in offset ) { m last burn iny offset -= y change ; my offset direction,let use,fail,pre
default <PLACE_HOLDER> one regionserver only .,test_util . get configuration ( ) . set boolean ( load balancer . tables_on_master @$ true ) ;,default needs,fail,pre
the token has now <PLACE_HOLDER> state to having all windows shown ... what to do @$ what to do ?,if ( m freezing screen ) { show all windows locked ( ) ; stop freezing screen ( false @$ true ) ; if ( debug_orientation ) slog . i ( tag @$ __str__ + this + __str__ + m num interesting windows + __str__ + m num drawn windows ) ; set app layout changes ( finish_layout_redo_wallpaper @$ __str__ ) ; } else { set app layout changes ( finish_layout_redo_anim @$ __str__ ) ; if ( ! get display content ( ) . m opening apps . contains ( this ) && can show windows ( ) ) { show all windows locked ( ) ; } },token changed,success,pre
now test classes should <PLACE_HOLDER> only old junit classes .,test classes . remove all ( new junit classes ) ; throw new j unit exception ( format ( __str__ + __str__ + __str__ @$ test clazz . get name ( ) @$ sorted classes ( new junit classes ) @$ sorted classes ( test classes ) ) ) ;,classes contain,success,pre
this flow covers instance not <PLACE_HOLDER> exception . actual code just eating the exception . i.e actual code just printing the stacktrace @$ whenever an exception of type instance not <PLACE_HOLDER> exception occurs .,mbean container . destroy ( ) ;,exception found,success,pre
0 : id 1 : indexed string <PLACE_HOLDER> indexes,db . execsql ( __str__ + constraint + __str__ + __str__ ) ;,string has,fail,pre
if this is a call to ya <PLACE_HOLDER> special search formats @$ enhance the query with field assignments,if ( ( response writer instanceof y json response writer || response writer instanceof opensearch response writer ) && __str__ . equals ( mmsp . get ( __str__ @$ __str__ ) ) ) { if ( ! mmsp . get map ( ) . contains key ( __str__ ) ) mmsp . get map ( ) . put ( __str__ @$ new string [ ] { q } ) ; if ( ! mmsp . get map ( ) . contains key ( __str__ ) ) mmsp . get map ( ) . put ( __str__ @$ new string [ ] { collection schema . description_txt . get solr field name ( ) + __str__ + collection schema . h4_txt . get solr field name ( ) +,ya use,fail,pre
the bucket move will <PLACE_HOLDER> a destroy region message .,return new distribution message observer ( ) { private volatile boolean done ; @ override public void before send message ( cluster distribution manager dm @$ distribution message message ) { if ( message instanceof destroy region message && ! done ) { task . run ( ) ; done = true ; } } } ;,move send,success,pre
test that the from row function simply <PLACE_HOLDER> the original object back .,simplepojo extracted = registry . get from row function ( simplepojo . class ) . apply ( row ) ; assert same ( pojo @$ extracted ) ;,the returns,success,pre
loading child elements <PLACE_HOLDER> the state of the attribute set 's underlying parser @$ so it needs to happen after obtaining attributes and extracting states .,if ( dr == null ) { int type ; while ( ( type = parser . next ( ) ) == xml pull parser . text ) { } if ( type != xml pull parser . start_tag ) { throw new xml pull parser exception ( parser . get position description ( ) + item_missing_drawable_error ) ; } if ( parser . get name ( ) . equals ( __str__ ) ) { dr = vector drawable compat . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else if ( sdk_int >= lollipop ) { dr = drawable . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else { dr = drawable,elements modifies,success,pre
jump to second <PLACE_HOLDER> back if not equal . reserve space for the two jcc and first <PLACE_HOLDER> back when calculating target address .,final long second write back offset = instructions . size ( ) + first write back . size ( ) + __num__ ; final string second write back goal = string . format ( __str__ @$ instruction . get address ( ) . to long ( ) @$ second write back offset ) ; instructions . add ( reil helpers . create jcc ( base offset + instructions . size ( ) @$ current size @$ comparison result @$ operand size . address @$ second write back goal ) ) ;,jump write,success,pre
now run the k <PLACE_HOLDER> job,fuzzyk means driver . run ( testdata @$ new path ( output @$ __str__ ) @$ fuzzyk means output @$ __num__ @$ __num__ @$ __num__ @$ true @$ true @$ __num__ @$ true ) ; int num iterations = __num__ ; path clusters in = new path ( fuzzyk means output @$ __str__ ) ; representative points driver . run ( conf @$ clusters in @$ new path ( fuzzyk means output @$ __str__ ) @$ fuzzyk means output @$ measure @$ num iterations @$ true ) ; representative points driver . print representative points ( fuzzyk means output @$ num iterations ) ; cluster evaluator evaluator = new cluster evaluator ( conf @$ clusters in ) ;,k means,success,pre
when filtering @$ let the server <PLACE_HOLDER> the page unless we 've set the filter to empty and explicitly said that we want to see the results starting from page 0 .,if ( ! filter . equals ( last filter ) ) { if ( filter . is empty ( ) && page != __num__ ) { page = - __num__ ; } else { page = __num__ ; } },server decide,success,pre
first assume there are no named capture backreferences @$ because the spec says they should only be recognized if the pattern <PLACE_HOLDER> at least one named capture group .,class parser { int pos ; int num capturing groups ; set < string > capturing group names = new hash set < > ( ) ; final int limit = pattern . length ( ) ; boolean look for named capture backreferences ; reg exp tree parse top level ( ) { this . pos = __num__ ; this . num capturing groups = __num__ ; this . look for named capture backreferences = false ; reg exp tree out = parse ( ) ; if ( ! capturing group names . is empty ( ) ) { this . pos = __num__ ; this . num capturing groups = __num__ ; this . look for named capture backreferences = true ; out = parse ( ),pattern contains,success,pre
test file does n't <PLACE_HOLDER> an ec policy,final path file = new path ( dir @$ __str__ ) ; fs . create ( file ) . close ( ) ; assert null ( client . get file info ( file . to string ( ) ) . get erasure coding policy ( ) ) ; contract test utils . assert not erasure coded ( fs @$ file ) ; fs . delete ( file @$ true ) ; final erasure coding policy ec policy1 = get ec policy ( ) ;,file have,success,pre
if component metrics @$ no more information required from tag so <PLACE_HOLDER> the loop,if ( tag . name ( ) . equals ( __str__ ) ) { app id = tag . value ( ) ; },metrics break,success,pre
if there are quotes @$ <PLACE_HOLDER> a document wide list,if ( quote annotator . gather quotes ( this . annotation document ) != null ) build document quotes list ( ) ;,list build,success,pre
if any the fields of struct are <PLACE_HOLDER> null @$ then return true,for ( int i = __num__ ; i < struct fields . size ( ) ; i ++ ) { if ( has any null object ( soi . get struct field data ( o @$ struct fields . get ( i ) ) @$ struct fields . get ( i ) . get field object inspector ( ) ) ) { return true ; } } return false ;,fields representing,success,pre
the sdk <PLACE_HOLDER> an unbounded thread pool because a step may create x writers each requiring their own thread to perform the writes otherwise a writer may block causing deadlock for the step because the writers buffer is full . also @$ the map task executor launches the steps in reverse order and completes them in forward order thus requiring enough threads so that,return new thread pool executor ( __num__ @$ integer . max_value @$ long . max_value @$ time unit . nanoseconds @$ new synchronous queue < > ( ) @$ thread factory builder . build ( ) ) ;,sdk needs,fail,pre
if this next line <PLACE_HOLDER> a clinit @$ it fails,lambda l = dummy method to make check style happy ( static fail if clinit runs :: static method ) ; try { l . run ( __num__ @$ __num__ ) ; fail ( __str__ ) ; } catch ( assertion error ae ) { },line finds,fail,pre
interface <PLACE_HOLDER> no members and is therefore not handled as pojo,if ( modifier . is interface ( clazz . get modifiers ( ) ) ) { return new generic type info < out > ( clazz ) ; },interface has,success,pre
the final part of this check is to verify that the change does actually <PLACE_HOLDER> a change in user .,try { cached security context = security context association . get security context ( ) ; final security context next context = security context factory . create security context ( desired user @$ new current user credential ( connection user . get name ( ) ) @$ new subject ( ) @$ __str__ ) ; security context association . set security context ( next context ) ; context set = true ; remoting context . clear ( ) ; } catch ( exception e ) { logger . error ( __str__ @$ e ) ; throw new ejb access exception ( __str__ ) ; },change indicate,success,pre
sorting on due date asc should <PLACE_HOLDER> the nulls at the end,list < task > tasks = task service . create task query ( ) . order by due date nulls last ( ) . asc ( ) . list ( ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { assert not null ( tasks . get ( i ) . get due date ( ) ) ; } assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get,sorting put,success,pre
we start out by loading an initial configuration where we started to write a task update @$ and then compaction <PLACE_HOLDER> up the earlier record .,expect configure ( ) ; list < consumer record < string @$ byte [ ] > > existing records = arrays . as list ( new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$ __num__ @$ connector_config_keys . get ( __num__ ) @$ configs_serialized . get ( __num__ ) ) @$ new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$ __num__ @$ task_config_keys . get ( __num__ ) @$ configs_serialized . get ( __num__ ) ) @$ new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$,compaction clean,fail,pre
ensure that really late tasks do n't completely <PLACE_HOLDER> the head of the task queue,if ( late > period ) { period = __num__ ; } else if ( late > __num__ ) { period -= late ; },tasks change,fail,pre
subclasses like forall not node might <PLACE_HOLDER> this propagation,insert child left tuple ( sink @$ trg left tuples @$ left tuple @$ right tuple . get propagation context ( ) @$ true ) ;,subclasses maintain,fail,pre
active user count <PLACE_HOLDER> to decrease from queue 2 due to app <PLACE_HOLDER> no pending requests,assert equals ( __num__ @$ queue2 . get abstract users manager ( ) . get num active users ( ) ) ;,count has,success,pre
adapterview does not <PLACE_HOLDER> click listener,if ( view instanceof adapter view ) { return ; } view . set on click listener ( new view . on click listener ( ) { @ override public void on click ( view v ) { if ( on click listener == null ) { return ; } on click listener . on click ( dialog plus . this @$ v ) ; } } ) ;,adapterview support,success,pre
if inheriting all variables then trans executor can <PLACE_HOLDER> variables from parent trans,assert . assert equals ( parent value @$ internal trans . get variable ( variable name ) ) ;,executor initialize,success,pre
both ruby should pass : ruby.jar <PLACE_HOLDER> no jdk restriction,collection < component info > cat infos = contents . load components ( __str__ @$ m @$ false ) ; assert equals ( __num__ @$ cat infos . size ( ) ) ; set < url > urls = new hash set < > ( arrays . as list ( test data . resolve ( __str__ ) . to uri ( ) . tourl ( ) @$ test data . resolve ( __str__ ) . to uri ( ) . tourl ( ) ) ) ; iterator < component info > itc = cat infos . iterator ( ) ; component info ci = itc . next ( ) ; assert true ( urls . remove ( ci . get remoteurl ( ) ) ) ; ci =,ruby.jar has,success,pre
zap : <PLACE_HOLDER> the statement .,extension hook . get hook view ( ) . add option panel ( get options database panel ( ) ) ; extension hook . get hook view ( ) . add option panel ( get options jvm panel ( ) ) ;,zap added,success,pre
unknown backend should <PLACE_HOLDER> default config of registry,thread pool bulkhead bulkhead3 = bulkhead registry . bulkhead ( __str__ ) ; assert that ( bulkhead3 ) . is not null ( ) ; assert that ( bulkhead3 . get bulkhead config ( ) . get core thread pool size ( ) ) . is equal to ( __num__ ) ; assert that ( event consumer registry . get all event consumer ( ) ) . has size ( __num__ ) ;,backend get,success,pre
verify that the read delta call will only return deltas when the previous call <PLACE_HOLDER> null callback .,m callback . clear ( ) ; final long [ ] [ ] new times3 = increase time ( new times2 ) ; write to file ( m headline + uid lines ( m uids @$ new times3 ) ) ; m reader . read delta ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; ++ i ) { m callback . verify ( m uids [ i ] @$ get active time ( new times3 [ i ] ) - get active time ( new times2 [ i ] ) ) ; } m callback . verify no more interactions ( ) ; assert true ( m test file . delete ( ) ) ;,call had,success,pre
if we have a constraints changed intent in the queue do n't <PLACE_HOLDER> a second one . we are treating this intent as special because every time a worker with constraints is complete it kicks off an update for constraint proxies .,if ( command handler . action_constraints_changed . equals ( action ) && has intent with action ( command handler . action_constraints_changed ) ) { return false ; } intent . put extra ( key_start_id @$ start id ) ; synchronized ( m intents ) { boolean has commands = ! m intents . is empty ( ) ; m intents . add ( intent ) ; if ( ! has commands ) { process command ( ) ; } },intent need,fail,pre
the following will <PLACE_HOLDER> the look and feel of the toolbar to match the current design,m recycler view . set toolbar background color ( context compat . get color ( context @$ r . color . primary ) ) ; m recycler view . set toolbar spinner text color ( context compat . get color ( context @$ android . r . color . white ) ) ; m recycler view . set toolbar spinner drawable ( r . drawable . ic_dropdown_primary_30_24dp ) ; if ( build config . information_architecture_available && m is top level ) { m recycler view . set toolbar title ( r . string . reader_screen_title @$ get resources ( ) . get dimension pixel size ( r . dimen . margin_extra_large ) ) ; } else { m recycler view . set toolbar left and right padding (,following change,success,pre
the keys of probe and build sides are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer <PLACE_HOLDER> works well in this case .,final int probe vals per key = __num__ ;,side join,success,pre
no intersection between clip area and segment <PLACE_HOLDER> corner : bottom left,clippable . init ( ) ; segment clipper . clip ( - __num__ @$ - __num__ @$ __num__ @$ __num__ ) ; assert . assert equals ( __num__ @$ points . size ( ) ) ;,intersection computed,success,pre
no need to <PLACE_HOLDER> the bit ; we 're exiting,if ( _is stopped ) { break ; } logger . warn ( __str__ @$ e ) ; break ;,need reset,success,pre
if invalid label exception is received means the requested label doesnt <PLACE_HOLDER> access so killing job in this case .,string diag msg = __str__ + string utils . stringify exception ( e ) ; log . info ( diag msg ) ; job id job id = this . get job ( ) . getid ( ) ; event handler . handle ( new job diagnostics update event ( job id @$ diag msg ) ) ; event handler . handle ( new job event ( job id @$ job event type . job_kill ) ) ; throw e ;,doesnt have,success,pre
check if the current node <PLACE_HOLDER> a child of the appropriate type if it does : increment the count and proceed otherwise : create a new node @$ and to map and set as a child,g tree node current node child = current node child map . get ( key ) ; if ( current node child == null ) { current node child = new function bit patternsg tree node ( key @$ current seq . get instructions ( ) [ level ] @$ current seq . get sizes ( ) [ level ] ) ; ( ( function bit patternsg tree node ) current node child ) . increment count ( __num__ ) ; current node child map . put ( key @$ current node child ) ; current node . add node ( current node child ) ; } else { ( ( function bit patternsg tree node ) current node child ) . increment count ( __num__ ) ;,node has,success,pre
verify script listener has <PLACE_HOLDER> its job @$ on create before flowable entity event was fired,assert equals ( __str__ @$ __str__ @$ task from event . get assignee ( ) ) ; assert equals ( __str__ @$ __num__ @$ task from event . get priority ( ) ) ;,listener done,success,pre
for bootstrap class loader bean deployment archive always <PLACE_HOLDER> the parent resource loader,if ( module == null ) { new bda services . add ( resource loader . class @$ service registry . get ( resource loader . class ) ) ; },archive have,fail,pre
no handlers <PLACE_HOLDER> so close the actual server the done handler needs to be executed on the context that calls close @$ not the context of the actual server,actual server . actual close ( completion ) ;,handlers found,fail,pre
total block <PLACE_HOLDER> fragment size in bytes .,return sb . f_blocks * sb . f_frsize ;,block count,success,pre
do delay <PLACE_HOLDER> coin transaction .,long create account fee = __num__ ; logger . info ( __str__ ) ;,delay send,success,pre
user can <PLACE_HOLDER> value of spark clone configuration in hive config to true,conf . set ( spark clone configuration @$ __str__ ) ; check spark conf ( conf @$ spark clone configuration @$ __str__ ) ;,user override,success,pre
fallback to maven <PLACE_HOLDER> information,hash map < string @$ string > maven versions = get maven versions ( ) ; string [ ] maven libs = { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; for ( string lib : maven libs ) { lib versions . put ( lib @$ maven versions . get ( lib ) ) ; } return lib versions ;,fallback target,fail,pre
if list size <PLACE_HOLDER> pref size @$ close connection,if ( closed || ( pref size > __num__ && conns . size ( ) > pref size ) ) { d ( __str__ @$ conn ) ; td ( __str__ @$ conn ) ; conns . remove ( entry ) ; conn . close connection ( ) ; } else { d ( __str__ @$ conn ) ; td ( __str__ @$ conn ) ; entry = conns . get ( loc ) ; entry . release ( ) ; },size exceeds,success,pre
abort : option to queue a batch with a query which can <PLACE_HOLDER> the procedure to abort .,boolean has abort = arg . contains ( __str__ ) ;,which cause,success,pre
already scrolling to the correct page @$ but not yet there . only <PLACE_HOLDER> instant scrolls because then we need to interrupt the current smooth scroll .,if ( item == m current item && smooth scroll ) { return ; },page perform,fail,pre
if crlf just <PLACE_HOLDER> lf,if ( c == __str__ ) { c = read ( reader ) ; if ( c != __str__ ) { last read = c ; use last read = true ; c = __str__ ; } },crlf return,fail,pre
this property access would be an unknown property error unless the polymer pass had successfully <PLACE_HOLDER> the element definition .,compiler compiler = compile ( options @$ new string [ ] { lines ( __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) } ) ;,pass parsed,success,pre
a client will merge databases if the number of databases <PLACE_HOLDER> the maximum number per client times the amount of clients,int max database files = options . get max database files ( ) * all database files map . key set ( ) . size ( ) ; boolean too many database files = number of database files > max database files ; boolean removed old versions = result . get removed old versions count ( ) > __num__ ; return removed old versions || too many database files || options . is force ( ) ;,number exceeds,success,pre
h base connector does n't <PLACE_HOLDER> local date time use timestamp as conversion class for now .,for ( int j = __num__ ; j < family type . get arity ( ) ; j ++ ) { class clazz = qualifier types [ j ] . get type class ( ) ; if ( local date time . class . equals ( clazz ) ) { clazz = timestamp . class ; } else if ( local date . class . equals ( clazz ) ) { clazz = date . class ; } else if ( local time . class . equals ( clazz ) ) { clazz = time . class ; } hbase schema . add column ( name @$ qualifier names [ j ] @$ clazz ) ; },connector support,success,pre
nodes without outgoing edges into the subgraph formed by the selected nodes are unselected . those are just the nodes which <PLACE_HOLDER> no selected children .,graph . iterate selected ( new i node callback < node type > ( ) { @ override public iteration mode next ( final node type node ) { if ( are all children deselected ( node ) ) { if ( ! deselect . contains ( node ) ) { deselect . add ( node ) ; } } return iteration mode . continue ; } } ) ;,which have,success,pre
more methods than advertised @$ either broken request or not actually socks 5 <PLACE_HOLDER> this check last so that any waiting for data is already <PLACE_HOLDER>ne,return actual readable bytes == ( __num__ + number of authentication methods ) ;,methods do,success,pre
we loaded plain text and the caller <PLACE_HOLDER> styled text @$ that is all we have so return it .,if ( styled ) { return text ; } else { return html . escape html ( text ) ; },text want,fail,pre
a media period may <PLACE_HOLDER> a discontinuity at the current playback position to ensure the renderers are flushed . only <PLACE_HOLDER> the discontinuity externally if the position changed .,if ( period position us != playback info . position us ) { playback info = playback info . copy with new position ( playback info . period id @$ period position us @$ playback info . content position us @$ get total buffered duration us ( ) ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( ) ; period position us = playing period holder . to period time ( renderer position us ) ; maybe trigger pending messages ( playback info . position us @$ period position us ) ; playback info . position us = period position us ;,period report,success,pre
clear res <PLACE_HOLDER> work,res monitor work = null ; m_periodic works . clear ( ) ; m_snapshot completion monitor . shutdown ( ) ; m_periodic work thread . shutdown ( ) ; m_periodic work thread . await termination ( __num__ @$ time unit . days ) ; m_periodic priority work thread . shutdown ( ) ; m_periodic priority work thread . await termination ( __num__ @$ time unit . days ) ; if ( m_elastic service != null ) { m_elastic service . shutdown ( ) ; } if ( m_leader appointer != null ) { m_leader appointer . shutdown ( ) ; } m_global service elector . shutdown ( ) ; if ( m_has started sampler . get ( ) ) { m_sampler . set should stop ( ) ;,res monitor,success,pre
a volt db extension to <PLACE_HOLDER> limit partition rows syntax,case tokens . limit : read ( ) ; process alter table add limit constraint ( t @$ cname ) ; return ;,extension support,success,pre
failed attempt to renew the seed does not <PLACE_HOLDER> any effect,assert user connected ( wc @$ alice ) ; user seed property user seed property = alice . get property ( user seed property . class ) ; user seed property . renew seed ( ) ;,attempt have,success,pre
the order of tasks below is waiting @$ pending @$ running to prevent skipping a task @$ it 's the order in which tasks will <PLACE_HOLDER> state if they do while this is code is executing @$ so a task might be counted twice but never skipped,if ( runner task state . waiting . equals ( state ) ) { collection < ? extends task runner work item > runners known tasks = runner . get known tasks ( ) ; set < string > runner known task ids = runners known tasks . stream ( ) . map ( task runner work item :: get task id ) . collect ( collectors . to set ( ) ) ; final list < any task > waiting tasks = new array list < > ( ) ; for ( task runner work item task : all tasks ) { if ( ! runner known task ids . contains ( task . get task id ( ) ) ) { waiting tasks . add (,tasks change,success,pre
note : do n't <PLACE_HOLDER> schema configuration internally we will get stack overflaw beca<PLACE_HOLDER> xml schema validator will be instantiating xsd handler ...,f schema grammar description = new xsd description ( ) ;,note use,success,pre
if this rel references <PLACE_HOLDER> var and now it needs to be rewritten it must have been pulled above the <PLACE_HOLDER>relate replace the input ref to account for the lhs of the <PLACE_HOLDER>relate,if ( current rel instanceof logical correlate ) { final int left input field count = ( ( logical correlate ) current rel ) . get left ( ) . get row type ( ) . get field count ( ) ; rel data type new type = input ref . get type ( ) ; if ( project pulled above left correlator ) { new type = type factory . create type with nullability ( new type @$ true ) ; } int pos = input ref . get index ( ) ; rex input ref new input ref = new rex input ref ( left input field count + pos @$ new type ) ; if ( ( is count != null ) && is count,references cor,success,pre
all values <PLACE_HOLDER> the same key so they all appear as a single output element,return input . apply ( reify . windows ( ) ) . apply ( with keys . < integer @$ value in single window < t > > of ( __num__ ) . with key type ( new type descriptor < integer > ( ) { } ) ) . apply ( window . into ( new identity window fn < kv < integer @$ value in single window < t > > > ( original window fn . window coder ( ) ) ) . triggering ( never . ever ( ) ) . with allowed lateness ( input . get windowing strategy ( ) . get allowed lateness ( ) ) . discarding fired panes ( ) ) . apply ( group by key . create,values have,success,pre
some cases <PLACE_HOLDER> this absolute path,if ( fs != null ) { if ( root dir test enabled ( ) ) { cleanup dir ( path ( __str__ ) ) ; } cleanup dir ( get test base dir ( ) ) ; },cases use,success,pre
the compressed stream should reset before compressing stream reset state since in gz reset statue will <PLACE_HOLDER> header in the outputstream .,compressed stream . reset ( ) ; compressing stream . reset state ( ) ; compressing stream . write ( buffer @$ offset @$ length ) ; compressing stream . flush ( ) ; compressed stream . to byte array ( ) ; final long finish time = system . nano time ( ) ;,statue put,fail,pre
deselect will <PLACE_HOLDER> the event,if ( item ids != null ) { return deselect ( arrays . as list ( item ids ) ) ; } else { throw new illegal argument exception ( __str__ ) ; },deselect fire,success,pre
user ca n't <PLACE_HOLDER> the cell .,return false ;,user tolerate,fail,pre
create a table which will <PLACE_HOLDER> exceptions for requests,table name table name = table name . value of ( name . get method name ( ) ) ; h table descriptor table desc = new h table descriptor ( table name ) ; table desc . add coprocessor ( error throwing get observer . class . get name ( ) ) ; table desc . add family ( new h column descriptor ( family ) ) ; table table = util . create table ( table desc @$ null ) ; table . put ( new put ( rowkey ) . add column ( family @$ col @$ bytes . to bytes ( __str__ ) ) ) ; thrifth base service handler hbase handler = create handler ( ) ; thrift metrics metrics = get metrics,which throw,success,pre
ok so apparently the index <PLACE_HOLDER> a node way outside node high id,reporter . for index entry ( new index entry ( descriptor @$ context . token name lookup @$ entity id ) ) . node not in use ( record loader . node ( entity id ) ) ;,index assumes,fail,pre
developer is explicitly <PLACE_HOLDER> the dark theme .,if ( args . get boolean ( base dialog builder . arg_use_dark_theme ) ) { use light theme = false ; } else if ( args . get boolean ( base dialog builder . arg_use_light_theme ) ) { use light theme = true ; },developer setting,fail,pre
ensure that this real user even <PLACE_HOLDER> kerberos creds :,user real user = proxy user provider . get user ( ) ; kerberos principal real principal = real user . get kerberos principal ( ) ; if ( real principal == null ) { throw new es hadoop illegal argument exception ( __str__ + real user . get user name ( ) + __str__ + run as user + __str__ ) ; } if ( log . is debug enabled ( ) ) { log . debug ( __str__ + real user . get user name ( ) + __str__ + run as user + __str__ ) ; } credential user provider = proxy user provider ;,user has,success,pre
if the regular output and reprocess output <PLACE_HOLDER> the same size and format @$ they can share one image reader .,if ( input format == reprocess output format && input size . equals ( reprocess output size ) ) { max images *= __num__ ; m share one image reader = true ; },output have,success,pre
user 2 <PLACE_HOLDER> the new room,try { multi user chat muc2 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc2 . join ( __str__ ) ; muc2 . add user status listener ( new default user status listener ( ) { public void kicked ( string actor @$ string reason ) { super . kicked ( actor @$ reason ) ; answer [ __num__ ] = actor ; answer [ __num__ ] = reason ; } } ) ; multi user chat muc3 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc3 . join ( __str__ ) ; muc3 . add participant status listener ( new default participant status listener ( ) { public void kicked ( string participant,user joins,success,pre
rows with ` technology ` <PLACE_HOLDER> ` 170000 ` in the quality numeric string field,regex filtered dimension spec regex spec = new regex filtered dimension spec ( new default dimension spec ( __str__ @$ __str__ @$ value type . long ) @$ __str__ ) ; list filtered dimension spec list filtered spec = new list filtered dimension spec ( new default dimension spec ( __str__ @$ __str__ @$ value type . float ) @$ sets . new hash set ( __str__ ) @$ true ) ; group by query query = make query builder ( ) . set data source ( query runner test helper . data_source ) . set query segment spec ( query runner test helper . first_to_third ) . set dimensions ( regex spec @$ list filtered spec ) . set dim filter ( new in dim filter (,technology has,fail,pre
this code <PLACE_HOLDER> a calendar instance to create a unique name and description for test purposes so that you can easily upload multiple files . you should remove this code from your project and use your own standard names instead .,calendar cal = calendar . get instance ( ) ; snippet . set title ( __str__ + cal . get time ( ) ) ; snippet . set description ( __str__ + __str__ + cal . get time ( ) ) ;,code uses,success,pre
in command line . <PLACE_HOLDER> jmx agent and then start it again with different property values <PLACE_HOLDER> jmx agent again and then start it without property value make sure these properties overridden corectly,system . out . println ( __str__ ) ; something s = do something ( __str__ @$ __str__ + port1 @$ __str__ @$ __str__ ) ; try { test no connect ( port1 ) ; jcmd ( cmd_stop ) ; jcmd ( cmd_start @$ __str__ @$ __str__ + port1 ) ; test connect ( port1 ) ; jcmd ( cmd_stop ) ; jcmd ( cmd_start @$ __str__ + port1 ) ; test no connect ( port1 ) ; } finally { s . stop ( ) ; },overridden start,fail,pre
first things first : let 's flush the buffer to <PLACE_HOLDER> some more room,_flush buffer ( ) ;,buffer give,fail,pre
insert dummy dimension so all subtotals queries <PLACE_HOLDER> result rows with the same shape . use a field name that does not appear in the main query result @$ to assure the result will be null .,string dim name = __str__ + i ; while ( query . get result row position lookup ( ) . get int ( dim name ) >= __num__ ) { dim name = __str__ + dim name ; } new dimensions . add ( default dimension spec . of ( dim name ) ) ;,rows have,success,pre
only chunks in the old from space <PLACE_HOLDER> a remembered set .,final heap impl heap = heap impl . get heap impl ( ) ; final old generation old gen = heap . get old generation ( ) ; if ( that . get space ( ) == old gen . get from space ( ) ) { if ( ! card table . verify ( get card table start ( that ) @$ get first object table start ( that ) @$ get aligned heap chunk start ( that ) @$ that . get top ( ) ) ) { final log verify log = heap . get heap verifier impl ( ) . get witness log ( ) . string ( __str__ ) ; verify log . string ( __str__ ) . string ( __str__ ) .,chunks have,success,pre
main : 1 finished main : 1 <PLACE_HOLDER> 0 on 11 <PLACE_HOLDER> 1 on 11 <PLACE_HOLDER> 2 on 11,system . out . println ( __str__ + thread . current thread ( ) . get id ( ) ) ;,main received,success,pre
if the order by expression list <PLACE_HOLDER> a partition by expression then we wo n't have to sort by it twice . we sort by the partition by expressions first @$ and we do n't care what order we sort by them . so @$ find the sort direction in the order by list and use that in the partition by list @$ and,boolean [ ] dontsort = new boolean [ win expr . get orderby size ( ) ] ; list < abstract expression > order by expressions = win expr . get order by expressions ( ) ; list < sort direction type > order by directions = win expr . get order by directions ( ) ; order by plan node onode = new order by plan node ( ) ; for ( int idx = __num__ ; idx < win expr . get partitionby size ( ) ; ++ idx ) { sort direction type pdir = sort direction type . asc ; abstract expression partition by expression = partition by expressions . get ( idx ) ; int sidx = win expr . get sort index,order contains,success,pre
model name can not <PLACE_HOLDER> reserved keyword @$ e.g . return,if ( is reserved word ( model name ) ) { logger . warn ( model name + __str__ + camelize ( __str__ + model name ) ) ; model name = __str__ + model name ; },name use,success,pre
measure with exactly . that way @$ paged tile layout will only <PLACE_HOLDER> excess height and will be measured last @$ after other views and padding is accounted for .,mqs panel . measure ( width measure spec @$ measure spec . make measure spec ( max qs @$ measure spec . exactly ) ) ; int width = mqs panel . get measured width ( ) ; int height = layout params . top margin + layout params . bottom margin + mqs panel . get measured height ( ) + get padding bottom ( ) ; super . on measure ( measure spec . make measure spec ( width @$ measure spec . exactly ) @$ measure spec . make measure spec ( height @$ measure spec . exactly ) ) ;,layout have,fail,pre
null constraint <PLACE_HOLDER> no data,if ( constraint data != null ) { number flag = ( number ) constraint data . get field value ( __str__ ) ; if ( flag != null ) { this . flag = flag . int value ( ) ; } },constraint implies,fail,pre
when activity is idle @$ we consider the relaunch must be successful @$ so let 's <PLACE_HOLDER> the flag .,r . m relaunch reason = relaunch_reason_none ;,'s set,fail,pre
in the absence of metadata art @$ the controller dialog <PLACE_HOLDER> care of creating it .,if ( is casting ) { builder . put string ( media metadata compat . metadata_key_display_icon_uri @$ image location ) ; },dialog takes,success,pre
load state store with app node <PLACE_HOLDER> config of 2 .,store = zk tester . getrm state store ( create conf for app node split ( __num__ ) ) ; store . setrm dispatcher ( dispatcher ) ; rm state state = store . load state ( ) ; application id app id21 = application id . new instance ( __num__ @$ __num__ ) ; store app ( store @$ dispatcher @$ app id21 @$ submit time @$ start time ) ;,store splitting,fail,pre
the my sql binlog always <PLACE_HOLDER> a year object ...,if ( data instanceof java . time . year ) { r . deliver ( adjust temporal ( java . time . year . of ( ( ( java . time . year ) data ) . get value ( ) ) ) . get ( chrono field . year ) ) ; } else if ( data instanceof java . sql . date ) { r . deliver ( ( ( java . sql . date ) data ) . get year ( ) + __num__ ) ; } else if ( data instanceof string ) { mut data = integer . value of ( ( string ) data ) ; },sql contains,fail,pre
passing null <PLACE_HOLDER> everything,if ( argument passed . equals ( type info factory . void type info ) ) { return __num__ ; },null breaks,fail,pre
no existing <PLACE_HOLDER> info objects mapping to same key as the current <PLACE_HOLDER> so add a new list,if ( sparse array index < __num__ ) { list < note info > duplicates for key = new array list < > ( ) ; duplicates for key . add ( note ) ; duplicates . put ( position @$ duplicates for key ) ; } else { duplicates . value at ( sparse array index ) . add ( note ) ; },no find,fail,pre
ensure that the 2 nn can still <PLACE_HOLDER> a checkpoint .,secondary . do checkpoint ( ) ;,nn perform,success,pre
elf file does not <PLACE_HOLDER> an .llvmbc section,if ( llvmbc == null ) { return null ; },file have,fail,pre
run the buck <PLACE_HOLDER> command,buck kill command handler handler = new buck kill command handler ( project @$ buck command . kill ) ; buck build manager . get instance ( project ) . run buck command while connected to buck ( handler @$ action_title @$ buck module ) ;,buck build,fail,pre
there is a problem with handling simultaneous auto activation after restart and manual activation . to properly catch the moment when cluster activation has finished we temporary <PLACE_HOLDER> auto activation .,disable auto activation = true ; start grids ( __num__ ) ; ig = grid ( __num__ ) ; ig . cluster ( ) . active ( true ) ; cache = ig . cache ( cache name ) ; grid dht partition full map part map = ig . cachex ( cache name ) . context ( ) . topology ( ) . partition map ( false ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { ignite ex ig0 = grid ( i ) ; for ( int p = __num__ ; p < __num__ ; p ++ ) assert equals collections ( ig . affinity ( cache name ) . map partition to primary and backups ( p,activation disable,success,pre
consolidate passed ops at the current slot duration ensuring each snapshot is full . to achieve this we put all passed and existing ops in a list and will merge them to ensure each <PLACE_HOLDER> a snapshot at the current granularity .,final list < historical ops > all ops = new linked list < > ( passed ops ) ; if ( existing ops != null ) { all ops . add all ( existing ops ) ; } if ( debug ) { enforce ops well formed ( all ops ) ; },each has,fail,pre
client app <PLACE_HOLDER> the restore session dangling . we know that it ca n't be in the middle of an actual restore operation because the timeout is suspended while a restore is in progress . clean up now .,if ( backup manager service . get active restore session ( ) != null ) { slog . w ( tag @$ __str__ ) ; backup manager service . get active restore session ( ) . mark timed out ( ) ; post ( backup manager service . get active restore session ( ) . new end restore runnable ( backup manager service @$ backup manager service . get active restore session ( ) ) ) ; },app has,fail,pre
wait for monitoring interval time and verify server is still in <PLACE_HOLDER> mode,resume and wait ( monitoring_interval + __num__ ) ; assert equals ( operation mode . running @$ voltdb . instance ( ) . get mode ( ) ) ;,server running,success,pre
if default material button background has been overwritten @$ we will let app compat button <PLACE_HOLDER> the tinting,super . set support background tint list ( tint ) ;,button handle,success,pre
resources missing from used are <PLACE_HOLDER> none of that resource,if ( i >= used . other resources . length ) { return __num__ ; },resources using,success,pre
this pass <PLACE_HOLDER>es not <PLACE_HOLDER> flow analysis .,test same ( __str__ ) ;,pass do,success,pre
assignment of object arrays <PLACE_HOLDER> availability of widening conversion of component types,return is assignable to ( from type @$ to type ) ;,assignment requires,success,pre
we do not want to truncate logs based on these exceptions . since users can <PLACE_HOLDER> them with config changes the users are able to workaround this if truncations is really needed .,throw e ; if ( fail on corrupted log files ) { throw unable to clean recover ( t ) ; } if ( last transaction != null ) { log entry commit commit entry = last transaction . get commit entry ( ) ; monitor . fail to recover transactions after commit ( t @$ commit entry @$ recovery to position ) ; } else { monitor . fail to recover transactions after position ( t @$ recovery start position ) ; },users confirm,fail,pre
when calling <PLACE_HOLDER> lock task mode for locked mode on both tasks,m lock task controller . start lock task mode ( tr1 @$ false @$ test_uid ) ; m lock task controller . start lock task mode ( tr2 @$ false @$ test_uid ) ;,calling set,success,pre
rm does n't <PLACE_HOLDER> app report and job history server is not configured,client service delegate client service delegate = get client service delegate ( null @$ getrm delegate ( ) ) ; job status job status = client service delegate . get job status ( old job id ) ; assert . assert equals ( __str__ @$ job status . get username ( ) ) ; assert . assert equals ( job status . state . prep @$ job status . get state ( ) ) ;,rm return,fail,pre
if new if old expected old value <PLACE_HOLDER> old value,if ( basic put ( event @$ false @$ false @$ null @$ false ) ) { old value = event . get old value ( ) ; },value require,success,pre
we have to compare in this order @$ because the comparator order <PLACE_HOLDER> special logic when the 'left side ' is a special key .,int cmp = comparator . compare ( key @$ arr [ mid ] ) ;,order has,success,pre
latest <PLACE_HOLDER> sdk for focused test runs,string builder buf = new string builder ( super . get name ( ) ) ; if ( include variant markers in test name || always include variant markers in name ) { buf . append ( __str__ ) . append ( get sdk ( ) . get api level ( ) ) . append ( __str__ ) ; if ( default res mode strategy == res mode strategy . both ) { buf . append ( __str__ ) . append ( resources mode . name ( ) ) . append ( __str__ ) ; } } return buf . to string ( ) ;,latest supported,success,pre
is the outermost query and it will actually get run as a native query . druid 's native query layer will <PLACE_HOLDER> aggregations for the outermost query even if we do n't explicitly ask it to .,final druid query query = to druid query ( false ) ; if ( query != null ) { return get query maker ( ) . run query ( query ) ; } else { return sequences . empty ( ) ; },layer finalize,success,pre
this is an approximation as different host process <PLACE_HOLDER> significantly different times,int tot = __num__ ; for ( host process process : this . get host processes ( ) ) { tot += process . get percentage complete ( ) ; } int latest progress = tot / this . get host processes ( ) . size ( ) ; if ( latest progress != this . progress ) { this . progress = latest progress ; active scan event publisher . publish scan progress event ( this . get id ( ) @$ this . progress ) ; },process execute,fail,pre
the agent can <PLACE_HOLDER> file from user content,string content = s . get channel ( ) . call ( new read files2m callable ( target ) ) ;,agent read,success,pre
... figure out which direct dependencies can possibly <PLACE_HOLDER> aspects attached to them ...,multimap < attribute @$ label > deps with possible aspects = ( ( rule ) target ) . get transitions ( ( rule rule @$ attribute attribute ) -> { for ( aspect aspect with parameters : attribute . get aspects ( rule ) ) { if ( ! aspect with parameters . get definition ( ) . get attributes ( ) . is empty ( ) ) { return true ; } } return false ; } ) ;,dependencies have,success,pre
0 x 1002346 : p 1 repeatable comment <PLACE_HOLDER> p 2 repeatable comment .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ;,comment contains,success,pre
add a keyed stateful map operator @$ which <PLACE_HOLDER> avro for state serialization,event stream = event stream . key by ( event :: get key ) . map ( create artificial keyed state mapper ( ( map function < event @$ event > ) in -> in @$ ( event event @$ complex payload avro last state ) -> { if ( last state != null && ! last state . get str payload ( ) . equals ( keyed_state_oper_with_avro_ser . get name ( ) ) && last state . get inner pay load ( ) . get sequence number ( ) == ( event . get sequence number ( ) - __num__ ) ) { system . out . println ( __str__ ) ; } complex payload avro payload = new complex payload avro ( ) ; payload .,which uses,success,pre
<PLACE_HOLDER> x 1<PLACE_HOLDER><PLACE_HOLDER>18 a 6 : op <PLACE_HOLDER> has reg ref to esi . <PLACE_HOLDER> x 1<PLACE_HOLDER><PLACE_HOLDER>295 a : op 1 has reg ref to cx . <PLACE_HOLDER> x 1<PLACE_HOLDER><PLACE_HOLDER>2 d <PLACE_HOLDER> b : op <PLACE_HOLDER> has reg ref to edi ; op 1 has reg ref to eax . <PLACE_HOLDER> x 1<PLACE_HOLDER><PLACE_HOLDER>33 fe : op <PLACE_HOLDER> both have reg ref to edi .,mtf . initialize ( __str__ @$ new program modifier listener ( ) { @ override public void modify latest ( programdb program ) { int tx id = program . start transaction ( __str__ ) ; boolean commit = false ; try { program context context = program . get program context ( ) ; register esi reg = context . get register ( __str__ ) ; register cx reg = context . get register ( __str__ ) ; reference manager ref mgr = program . get reference manager ( ) ; reference [ ] refs ; reference new ref ; refs = ref mgr . get references from ( addr ( program @$ __str__ ) @$ __num__ ) ; assert equals ( __num__ @$ refs . length,fe 0,success,pre
undefined blocks <PLACE_HOLDER> no source bytes,assert true ( ranges . is empty ( ) ) ;,blocks have,success,pre
cleanup <PLACE_HOLDER> class before tests,compile utils . compiled_cache . invalidate all ( ) ;,cleanup generated,fail,pre
go from the back of the list to front @$ look for the request closes to the beginning that requests the state in which activity will <PLACE_HOLDER> after all callbacks are executed .,int last requested state = undefined ; int last requesting callback = - __num__ ; for ( int i = callbacks . size ( ) - __num__ ; i >= __num__ ; i -- ) { final client transaction item callback = callbacks . get ( i ) ; final int post execution state = callback . get post execution state ( ) ; if ( post execution state != undefined ) { if ( last requested state == undefined || last requested state == post execution state ) { last requested state = post execution state ; last requesting callback = i ; } else { break ; } } },activity come,fail,pre
constant values do n't <PLACE_HOLDER> checking,if ( token . is constant encoding ( ) ) { return ; },values need,success,pre
server socket will <PLACE_HOLDER> connections,socket base server = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( server @$ not null value ( ) ) ; socket base client = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( client @$ not null value ( ) ) ; zmq . set socket option ( server @$ zmq . zmq_zap_domain @$ __str__ ) ; rc = zmq . bind ( server @$ host ) ; assert that ( rc @$ is ( true ) ) ; rc = zmq . connect ( client @$ host ) ; assert that ( rc @$ is ( true ) ) ; int ret = zmq . send ( client @$ __str__ @$ __num__ ) ; assert,socket accept,success,pre
start use bound port will <PLACE_HOLDER> exception,server config server config2 = new server config ( ) ; server config2 . set bound host ( host ) ; server config2 . set port ( port ) ; server config2 . set protocol ( rpc constants . protocol_type_bolt ) ; http2 clear text server server2 = new http2 clear text server ( ) ; server2 . init ( server config2 ) ; boolean error = false ; try { server2 . start ( ) ; } catch ( exception e ) { error = true ; } assert . assert true ( error ) ; server . stop ( ) ; assert . assert false ( server . started ) ; thread . sleep ( __num__ ) ;,port throw,success,pre
0 x <PLACE_HOLDER> 0 f as basic conjoining jamo,string sc ko pat3 = __str__ ;,x means,fail,pre
set zoom . this helps encourage the user to pull back . some devices like the behold <PLACE_HOLDER> a zoom parameter,if ( max zoom string != null || mot zoom values string != null ) { parameters . set ( __str__ @$ string . value of ( ten desired zoom / __num__ ) ) ; },behold return,fail,pre
text node <PLACE_HOLDER> special case handling,if ( f start container . get node type ( ) == node . text_node ) { string s = f start container . get node value ( ) ; string sub = s . substring ( f start offset @$ f end offset ) ; if ( how != clone_contents ) { ( ( text impl ) f start container ) . delete data ( f start offset @$ f end offset - f start offset ) ; collapse ( true ) ; } if ( how == delete_contents ) return null ; frag . append child ( f document . create text node ( sub ) ) ; return frag ; },node requires,fail,pre
this filter will <PLACE_HOLDER> the values from being sent accross the wire . this is good for counting or other scans that are checking for existence and do n't rely on the value .,s . set filter ( new key only filter ( ) ) ;,filter prevent,fail,pre
after the job ran check to see if the input from the localized cache <PLACE_HOLDER> the real string . check if there are 3 instances or not .,path result = new path ( test_root_dir + __str__ ) ; if ( count != __num__ ) return new test result ( job @$ false ) ;,input matches,fail,pre
value <PLACE_HOLDER> yes,xdr . write boolean ( true ) ;,value ends,fail,pre
short form has all other details @$ but if we have collected ram from smaller native processes let 's <PLACE_HOLDER> a summary of that .,if ( extra native ram > __num__ ) { append basic mem entry ( short native builder @$ process list . native_adj @$ - __num__ @$ extra native ram @$ extra native memtrack @$ __str__ ) ; short native builder . append ( __str__ ) ; extra native ram = __num__ ; } append mem info ( full java builder @$ mi ) ;,'s add,fail,pre
if we do n't need a thread pool @$ <PLACE_HOLDER> a dummy one that executes the task synchronously noinspection nullable problems,this . executor = create thread pool ? executors . new fixed thread pool ( threads ) : new executor ( ) { @ override public void execute ( runnable command ) { command . run ( ) ; } } ;,problems create,success,pre
we wo n't compare the candidate display name against the current item . this is to prevent an validation warning if the user <PLACE_HOLDER> the display name to what the existing display name is,if ( item . get name ( ) . equals ( current job name ) ) { continue ; } else if ( display name . equals ( item . get display name ( ) ) ) { return false ; },user changes,fail,pre
do not wrap <PLACE_HOLDER> values . we do not want to impede error signaling .,if ( resolver == null ) { return null ; } return new forwarding name resolver ( resolver ) { @ override public string get service authority ( ) { return authority override ; } } ;,wrap object,fail,pre
upsert <PLACE_HOLDER> a primary key on the table to work,if ( is upsert ) { boolean has pkey = false ; for ( constraint c : cat table . get constraints ( ) ) { if ( c . get type ( ) == constraint type . primary_key . get value ( ) ) { has pkey = true ; break ; } } if ( ! has pkey ) { throw new volt abort exception ( string . format ( __str__ + __str__ @$ table name ) ) ; } },upsert requires,success,pre
user <PLACE_HOLDER> cookies disabled,if ( request . get requested session id ( ) == null ) { system messages system messages = get service ( ) . get system messages ( servlet portlet helper . find locale ( null @$ null @$ request ) @$ request ) ; get service ( ) . write uncached string response ( response @$ json constants . json_content_type @$ vaadin service . create critical notificationjson ( system messages . get cookies disabled caption ( ) @$ system messages . get cookies disabled message ( ) @$ null @$ system messages . get cookies disabledurl ( ) ) ) ; return false ; },user changed,fail,pre
see if any of the installed providers <PLACE_HOLDER> a mapping from the given algorithm name to an oid string,string oid string ; if ( ! init oid table ) { provider [ ] provs = security . get providers ( ) ; for ( int i = __num__ ; i < provs . length ; i ++ ) { for ( enumeration < object > enum_ = provs [ i ] . keys ( ) ; enum_ . has more elements ( ) ; ) { string alias = ( string ) enum_ . next element ( ) ; string upper case alias = alias . to upper case ( locale . english ) ; int index ; if ( upper case alias . starts with ( __str__ ) && ( index = upper case alias . index of ( __str__ @$ __num__ ) ) !=,any have,fail,pre
restli request options <PLACE_HOLDER> precedence over response compression config,return new object [ ] [ ] { { true @$ null @$ restli request options . default_options @$ large id count @$ default_accept_encoding @$ null @$ true } @$ { true @$ null @$ restli request options . default_options @$ small id count @$ default_accept_encoding @$ null @$ false } @$ { true @$ new compression config ( tiny ) @$ restli request options . default_options @$ large id count @$ default_accept_encoding @$ tiny . to string ( ) @$ true } @$ { true @$ new compression config ( tiny ) @$ restli request options . default_options @$ small id count @$ default_accept_encoding @$ tiny . to string ( ) @$ true } @$ { true @$ new compression config ( huge ) @$ restli request,options takes,success,pre
test when third argument <PLACE_HOLDER> nulls and repeats,batch = get batch4 long vectors ( ) ; r = ( long column vector ) batch . cols [ __num__ ] ; batch . cols [ __num__ ] . no nulls = false ; batch . cols [ __num__ ] . is null [ __num__ ] = true ; batch . cols [ __num__ ] . is repeating = true ; expr . evaluate ( batch ) ; assert equals ( true @$ r . is null [ __num__ ] ) ; assert equals ( true @$ r . is null [ __num__ ] ) ; assert equals ( - __num__ @$ r . vector [ __num__ ] ) ; assert equals ( - __num__ @$ r . vector [ __num__ ] ) ; assert equals (,argument has,success,pre
ui.js just <PLACE_HOLDER> up the logging,assert equals ( __str__ @$ messages . get ( __num__ ) . get text ( ) ) ;,ui.js logs,fail,pre
we need to create the first accumulator because combine fn.merger accumulators can <PLACE_HOLDER> the first accumulator,accumt first = combine fn . create accumulator ( ) ; iterable < accumt > accumulators to merge = iterables . concat ( collections . singleton ( first ) @$ accums and instants for merged window . stream ( ) . map ( x -> x . _1 ( ) ) . collect ( collectors . to list ( ) ) ) ; result . add ( windowed value . of ( combine fn . merge accumulators ( accumulators to merge ) @$ timestamp combiner . combine ( accums and instants for merged window . stream ( ) . map ( x -> x . _2 ( ) ) . collect ( collectors . to list ( ) ) ) @$ merged window @$ pane info . no_firing,accumulators change,fail,pre
initializes status message components if the given meta contact <PLACE_HOLDER> a status message .,init display details ( contact . get display details ( ) ) ;,contact reports,fail,pre
mock server to allow dynamically <PLACE_HOLDER> certificates to be accepted,httpsurl connection . set defaultssl socket factory ( new key store factory ( new mock server logger ( ) ) . ssl context ( ) . get socket factory ( ) ) ; mock server = client and server . start client and server ( port factory . find free port ( ) ) ;,server generated,success,pre
for the first grandkid <PLACE_HOLDER> the original parent,for ( filter grandkid : grand kids ) { if ( first ) { first = false ; children . set ( i @$ grandkid ) ; } else { children . add ( ++ i @$ grandkid ) ; } },grandkid replace,success,pre
now prepare the implementation of the build method . <PLACE_HOLDER> bean factory interface,visit build method definition ( annotation metadata @$ collections . empty map ( ) @$ collections . empty map ( ) ) ;,method see,success,pre
go to ocp and check if there a user project has <PLACE_HOLDER> resources,open shift project catalog page . open ( ) ; open shift login page . login ( new_test_user . get name ( ) @$ new_test_user . get password ( ) ) ; open shift project catalog page . wait project ( user_project_name ) ; open shift project catalog page . click on project ( user_project_name ) ; open shift project catalog page . wait resource ( __str__ ) ;,project expected,success,pre
hive does not <PLACE_HOLDER> result set meta data on prepared statement @$ and hive describe does not <PLACE_HOLDER> queries @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,hive support,success,pre
hides the popup menu when the parent window <PLACE_HOLDER> focus .,add component listener ( new component adapter ( ) { @ override public void component resized ( component event evt ) { final window parent window ; component parent = get parent ( ) ; if ( parent instanceof j popup menu ) parent window = swing utilities . get window ancestor ( ( ( j popup menu ) parent ) . get invoker ( ) ) ; else parent window = swing utilities . get window ancestor ( get invoker ( ) ) ; if ( parent window != null ) { if ( ! parent window . is active ( ) ) set visible ( false ) ; parent window . add window listener ( new window adapter ( ) { @ override public void window,window loses,success,pre
issues with stables pages @$ hence we <PLACE_HOLDER> the end back one page,long synced bytes = ( ( position at sync / bits . page size ( ) ) - __num__ ) * bits . page size ( ) ; if ( posix advise . sync_file_range_supported ) { final long retval = posix advise . sync_file_range ( fd @$ sync start @$ synced bytes - sync start @$ posix advise . sync_file_range_sync ) ; if ( retval != __num__ ) { logger . error ( __str__ + retval ) ; logger . error ( __str__ + synced bytes + __str__ + ( synced bytes - sync start ) + __str__ + posix advise . sync_file_range_sync ) ; fc . force ( false ) ; } } else { fc . force ( false ) ; } return synced bytes ;,issues make,fail,pre
otherwise @$ this meeting <PLACE_HOLDER> a new room,heap . offer ( intervals [ i ] ) ;,meeting leaves,fail,pre
authenticated user 's account and <PLACE_HOLDER> requests to use an ssl connection .,list < string > scopes = lists . new array list ( __str__ ) ; try { credential credential = auth . authorize ( scopes @$ __str__ ) ; youtube = new you tube . builder ( auth . http_transport @$ auth . json_factory @$ credential ) . set application name ( __str__ ) . build ( ) ; string channel id = get channel id ( ) ; system . out . println ( __str__ + channel id + __str__ ) ; string video id = get video id ( ) ; system . out . println ( __str__ + video id + __str__ ) ; string text = get text ( ) ; system . out . println ( __str__ + text + __str__ ) ;,account requires,success,pre
this entry <PLACE_HOLDER> multiple entries .,if ( entry . get journal entries count ( ) > __num__ ) { for ( journal entry e : entry . get journal entries list ( ) ) { apply entry ( e ) ; } } else if ( entry . get sequence number ( ) < __num__ ) { m last primary start sequence number = entry . get sequence number ( ) ; } else if ( entry . to builder ( ) . clear sequence number ( ) . build ( ) . equals ( journal entry . get default instance ( ) ) ) { } else { apply single entry ( entry ) ; },entry has,fail,pre
a volt <PLACE_HOLDER> extension to support indexed expressions .,boolean has non column exprs = false ; if ( set == null ) { has non column exprs = true ; set = get base column names ( index exprs ) ; },volt db,success,pre
nn should not <PLACE_HOLDER> the wildcard address by default .,try { conf . set ( dfs_namenode_http_address_key @$ localhost_server_address ) ; cluster = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) . build ( ) ; cluster . wait active ( ) ; string address = cluster . get name node ( ) . get http address ( ) . to string ( ) ; assert false ( __str__ @$ address . starts with ( wildcard_address ) ) ; } finally { if ( cluster != null ) { cluster . shutdown ( ) ; cluster = null ; } } log . info ( __str__ + dfs_namenode_http_bind_host_key ) ;,nn bind,success,pre
if any thread has <PLACE_HOLDER> exception or operation failed then we do n't have to process further .,if ( last exception != null || ! operation status ) { log . warn ( __str__ + __str__ @$ this . operation @$ file . get key ( ) ) ; break ; },thread thrown,fail,pre
so everyone <PLACE_HOLDER> the same priority .,return app priority ;,everyone has,success,pre
verify new paths <PLACE_HOLDER> recovery of previous resources,local resource request lr2 = create local resource request ( user @$ __num__ @$ __num__ @$ local resource visibility . application ) ; localizer context lc2 = new localizer context ( user @$ c id1 @$ null ) ; resource event req event2 = new resource request event ( lr2 @$ local resource visibility . application @$ lc2 ) ; tracker . handle ( req event2 ) ; dispatcher . await ( ) ; path hierarchical path2 = tracker . get path for localization ( lr2 @$ local dir @$ null ) ; long localized id2 = long . parse long ( hierarchical path2 . get name ( ) ) ; assert . assert equals ( localized id1 + __num__ @$ localized id2 ) ; if ( dispatcher,paths cause,fail,pre
ui mode <PLACE_HOLDER> type and night ...,int ui mode type = get ui mode type ( configuration ) ; int res tab type = res tab . ui mode type ( ) ; if ( res tab type != res table_config . ui_mode_type_any ) { ui mode type = res tab type ; } int ui mode night = get ui mode night ( configuration ) ; int res tab night = res tab . ui mode night ( ) ; if ( res tab night != res table_config . ui_mode_night_any ) { ui mode night = res tab night ; } configuration . ui mode = ui mode type | ui mode night ; if ( res tab . density != res table_config . density_default ) { set density ( res tab .,mode file,fail,pre
we move the error classification into extensions which <PLACE_HOLDER> downstream people to see them but still be spec compliant,if ( error classification != null ) { if ( extensions != null ) { extensions = new linked hash map < > ( extensions ) ; } else { extensions = new linked hash map < > ( ) ; } if ( ! extensions . contains key ( __str__ ) ) { extensions . put ( __str__ @$ error classification . to specification ( error ) ) ; } },which allows,success,pre
get data will <PLACE_HOLDER> deferred profiles if necessary,byte [ ] the header = get data ( ic sig head ) ; int to big endian ( rendering intent @$ the header @$ ic hdr rendering intent ) ;,data activate,success,pre
if a topic <PLACE_HOLDER> k partitions @$ and in the previous run @$ each partition recorded its avg time to pull a record @$ then use the geometric mean of these k numbers as the estimated avg time to pull a record in this run .,double est avg millis for topic = geometric mean ( prev avg millis for partitions ) ; this . est avg millis . put ( topic @$ est avg millis for topic ) ; log . info ( string . format ( __str__ @$ topic @$ est avg millis for topic ) ) ; all est avg millis . add ( est avg millis for topic ) ;,topic has,success,pre
check can not <PLACE_HOLDER> namespace via rest because it contains tables .,response = client . delete ( namespace path ) ; namespace path = __str__ + ns name ; assert equals ( __num__ @$ response . get code ( ) ) ;,check post,fail,pre
get a sender from the pool @$ or create a new one if the pool is empty if we ca n't create a new connection then route flow files to failure and yield acquire sender will <PLACE_HOLDER> the routing to failure and yielding,channel sender sender = acquire sender ( context @$ session @$ flow file ) ; if ( sender == null ) { return ; } try { string delimiter = context . get property ( message_delimiter ) . evaluate attribute expressions ( flow file ) . get value ( ) ; if ( delimiter != null ) { delimiter = delimiter . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) ; } if ( delimiter == null ) { process single message ( context @$ session @$ flow file @$ sender ) ; } else { process delimited messages ( context @$ session @$ flow file @$ sender @$ delimiter ) ; } } finally {,sender handle,success,pre
not enough data to decrypt . <PLACE_HOLDER> the buffer so that we keep the data we have but prepare the buffer to be written to again .,logger . debug ( __str__ ) ; stream buffer . compact ( ) ; return __num__ ;,data clear,fail,pre
resolve initial <PLACE_HOLDER> position .,if ( pending initial seek position != null ) { pair < object @$ long > period position = resolve seek position ( pending initial seek position @$ true ) ; pending initial seek position = null ; if ( period position == null ) { handle source info refresh ended playback ( ) ; return ; } new content position us = period position . second ; new period id = queue . resolve media period id for ads ( period position . first @$ new content position us ) ; } else if ( old content position us == c . time_unset && ! timeline . is empty ( ) ) { pair < object @$ long > default position = get period position ( timeline,initial seek,success,pre
no need for lazy details scroll adjuster @$ because the start is always 0 @$ wo n't <PLACE_HOLDER> a bit .,register rpc ( grid client rpc . class @$ new grid client rpc ( ) { @ override public void scroll to start ( ) { scheduler . get ( ) . schedule finally ( new scheduled command ( ) { @ override public void execute ( ) { grid . scroll to start ( ) ; } } ) ; } @ override public void scroll to end ( ) { scheduler . get ( ) . schedule finally ( new scheduled command ( ) { @ override public void execute ( ) { grid . scroll to end ( ) ; lazy details scroller . scroll to row ( data source . size ( ) - __num__ @$ scroll destination . end ) ; } },adjuster change,success,pre
step 5 : if the class object for c is in an erroneous state @$ then initialization is not possible . <PLACE_HOLDER> lc and throw a no class def found error .,if ( is in error state ( ) ) { throw new no class def found error ( __str__ + hub . get name ( ) ) ; },step remove,fail,pre
msp 3 <PLACE_HOLDER> upsert which hsql does not support,if ( ! ishsql ( ) ) { vt = client . call procedure ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ ) . get results ( ) ; assert content of table ( new object [ ] [ ] { { __num__ } } @$ vt [ __num__ ] ) ; assert content of table ( new object [ ] [ ] { { __num__ } } @$ vt [ __num__ ] ) ; assert content of table ( new object [ ] [ ] { { __num__ @$ __str__ } @$ { __num__ @$ __str__ } } @$ vt [ __num__ ] ) ; },msp supports,fail,pre
secondary ce @$ or a ce with a short primary @$ <PLACE_HOLDER> the case bits .,if ( minice1 <= collation fast latin . secondary_mask || collation fast latin . min_short <= minice1 ) { case1 = ( case1 > > ( __num__ - __num__ ) ) + collation fast latin . lower_case ; minice1 |= case1 ; },ce calculate,fail,pre
null should not <PLACE_HOLDER> result,current max = integer max kudaf . aggregate ( null @$ current max ) ; assert that ( __num__ @$ equal to ( current max ) ) ;,null impact,success,pre
any implementations must <PLACE_HOLDER> processors,return super . parse and validate metadata ( target type ) ;,implementations implement,fail,pre
redundant store does n't <PLACE_HOLDER> the indexes,accept ( storage . span consumer ( ) @$ trace ) ; assert that ( row count ( tables . annotations_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_remote_service_name_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_name_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_span_name_index ) ) . is equal to ( __num__ ) ;,store change,success,pre
output <PLACE_HOLDER> 2 cache and ignite cache stats . you may notice that at this point the object is not yet stored in <PLACE_HOLDER> 2 cache @$ because the read was not yet performed .,print stats ( ses factory ) ; system . out . println ( ) ; system . out . println ( __str__ ) ;,output db,fail,pre
6 th <PLACE_HOLDER> @$ success @$ accumulation continues up to this point,scm . add change ( ) . with author ( __str__ ) ; p . get builders list ( ) . clear ( ) ; b = j . build and assert success ( p ) ; assert culprits ( b @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,th build,success,pre
we have specific code that customizes <PLACE_HOLDER> messages . use this when the case .,if ( error . starts with ( __str__ ) || error . starts with ( __str__ ) ) { logger . debug ( error @$ e ) ; } else { string message = format ( __str__ @$ default log message . get ( ) @$ e . get class ( ) . get simple name ( ) @$ error ) ; logger . debug ( message @$ e ) ; },customizes log,success,pre
update once at end of iteration to reduce heap <PLACE_HOLDER> traffic,last ret = cursor = i ; check for comodification ( ) ; return cursor ; return cursor - __num__ ; if ( last ret < __num__ ) throw new illegal state exception ( ) ; check for comodification ( ) ; try { sub list . this . remove ( last ret ) ; cursor = last ret ; last ret = - __num__ ; expected mod count = array list . this . mod count ; } catch ( index out of bounds exception ex ) { throw new concurrent modification exception ( ) ; } if ( last ret < __num__ ) throw new illegal state exception ( ) ; check for comodification ( ) ; try { array list . this . set (,heap write,success,pre
subsequent error <PLACE_HOLDER>s does n't <PLACE_HOLDER> another warning,tracker . process response ( new response ( __str__ @$ null ) @$ __str__ @$ __num__ ) ; assert tracker mismatch count ( tracker @$ __num__ ) ;,triggers trigger,success,pre
do not recycle the old bitmap if such as it may be set as an error state to any of the page views . just let the gc <PLACE_HOLDER> care of it .,m error state = new bitmap drawable ( m context . get resources ( ) @$ error bitmap ) ;,gc take,success,pre
module invoked in <PLACE_HOLDER> privileged,invoke priv ( logout_method ) ;,module do,success,pre
a mapped superclass can <PLACE_HOLDER> no identifier if the id is set below in the hierarchy,if ( mapping type . get identifier mapper ( ) != null ) { @ suppress warnings ( __str__ ) iterator < property > property iterator = mapping type . get identifier mapper ( ) . get property iterator ( ) ; set < singular persistent attribute < ? super x @$ ? > > attributes = build id class attributes ( jpa mapping type @$ property iterator ) ; jpa mapping type . get in flight access ( ) . apply id class attributes ( attributes ) ; },superclass have,success,pre
reposition & <PLACE_HOLDER> the height for new orientation,if ( m is expanded ) { m expanded view container . set translationy ( get expanded viewy ( ) ) ; m expanded bubble . get expanded view ( ) . update view ( ) ; },& change,fail,pre
the peer is <PLACE_HOLDER> away everything is broken,for ( stream holder holder : current streams . values ( ) ) { if ( holder . source channel != null ) { holder . source channel . rst stream ( ) ; } if ( holder . sink channel != null ) { holder . sink channel . rst stream ( ) ; } } frame data . close ( ) ; send go away ( error_no_error ) ; break ;,peer doing,fail,pre
user can not <PLACE_HOLDER> these projects,index for user ( user2 @$ new doc ( metric key @$ __num__ ) @$ new doc ( metric key @$ __num__ ) @$ new doc ( metric key @$ __num__ ) ) ;,user see,success,pre
a volt <PLACE_HOLDER> extension to diagnose array out of bounds .,boolean voltd breclaimed = ( reclaimed node pointer != - __num__ ) ;,volt db,success,pre
only the killed case <PLACE_HOLDER> a message to be sent out to the am .,switch ( result . get end reason ( ) ) { case success : log . debug ( __str__ @$ request id ) ; if ( metrics != null ) { metrics . incr executor total success ( ) ; } break ; case container_stop_requested : log . info ( __str__ @$ request id ) ; if ( metrics != null ) { metrics . incr executor total killed ( ) ; } break ; case kill_requested : log . info ( __str__ @$ request id ) ; if ( killtimer watch . is running ( ) ) { killtimer watch . stop ( ) ; long elapsed = killtimer watch . elapsed ( time unit . milliseconds ) ; log . info ( __str__ @$ elapsed ),case has,fail,pre
reserved thread must have <PLACE_HOLDER> size but not yet added itself to queue . we will spin until it is added .,if ( thread == null ) { thread . yield ( ) ; continue ; },thread reached,fail,pre
otherwise use inject bean method instead which <PLACE_HOLDER> reflective injection,push inject method for index ( inject method visitor @$ inject instance index @$ current method index @$ __str__ ) ;,which performs,fail,pre
register task manager success will <PLACE_HOLDER> monitoring heartbeat target between tm and jm,final task manager location task manager location1 = task manager location future . get ( timeout . to milliseconds ( ) @$ time unit . milliseconds ) ; assert that ( task manager location1 @$ equal to ( task manager location ) ) ;,success trigger,success,pre
this set <PLACE_HOLDER> uniquenames,for ( string name : common names ) { voltxml diff child diff = compute diff ( before . find child ( name ) @$ after . find child ( name ) ) ; if ( ! child diff . is empty ( ) ) { result . m_changed elements . put ( name @$ child diff ) ; } } return result ;,set contains,success,pre
loop over all <PLACE_HOLDER> offsets .,int [ ] jump offsets = switch instruction . jump offsets ; for ( int index = __num__ ; index < jump offsets . length ; index ++ ) { evaluate instruction block ( clazz @$ method @$ code attribute @$ offset + jump offsets [ index ] ) ; },loop seek,fail,pre
in the case there is a directory @$ the <PLACE_HOLDER> digest value should not be converted . otherwise @$ if the <PLACE_HOLDER> digest value is a file @$ convert it using the path and size values .,return fav . get type ( ) . is file ( ) ? file artifact value . create for normal file using path ( path @$ fav . get size ( ) ) : new has digest . byte string digest ( fav . get value fingerprint ( ) . to byte array ( ) ) ;,the has,success,pre
setting address <PLACE_HOLDER> line fields by common convention .,address . set address line ( address_line_0_room_desk_floor @$ address line0 ) ; address . set address line ( address_line_1_number_road_suffix_apt @$ address line1 ) ; address . set address line ( address_line_2_city @$ address line2 ) ; address . set address line ( address_line_3_state_postal_code @$ address line3 ) ; address . set address line ( address_line_4_country @$ address line4 ) ;,address pick,fail,pre
render state list <PLACE_HOLDER> special loading code @$ so we can leave out the null values,if ( o == null ) { if ( ! name . equals ( __str__ ) ) { element before = current element ; append element ( __str__ ) ; current element = before ; } } else { write ( o @$ o . get class ( ) . get name ( ) @$ null ) ; },list has,success,pre
check whether current am could <PLACE_HOLDER> container complete msg,rm app recovered app0 = rm2 . getrm context ( ) . getrm apps ( ) . get ( app0 . get application id ( ) ) ; rm app attempt loaded attempt1 = recovered app0 . get current app attempt ( ) ; assert equals ( __num__ @$ loaded attempt1 . get just finished containers ( ) . size ( ) ) ;,am receive,fail,pre
removals in array list wo n't <PLACE_HOLDER> sorting,boolean changed = new list . remove ( o ) ; list = collections . unmodifiable list ( new list ) ; return changed ;,removals break,success,pre
no shebang and no elf @$ <PLACE_HOLDER> standard shell .,interpreter = termux service . prefix_path + __str__ ;,shebang use,success,pre
due to approximations @$ very close angles can <PLACE_HOLDER> the same cos here we make sure outer cos is bellow inner cos .,if ( ( ( int ) packed angle cos ) == ( ( int ) ( outer angle cos * __num__ ) ) ) { outer angle cos -= __num__ ; } packed angle cos += outer angle cos ; if ( packed angle cos == __num__ ) { throw new illegal argument exception ( __str__ ) ; },angles give,success,pre
let user <PLACE_HOLDER> email client,if ( package name == null ) { for ( intent intent : initial intents ) { grant permission ( context @$ intent @$ intent . get package ( ) @$ attachments ) ; } show chooser ( context @$ initial intents ) ; } else if ( attachment intent . resolve activity ( pm ) != null ) { grant permission ( context @$ attachment intent @$ package name @$ attachments ) ; context . start activity ( attachment intent ) ; } else { acra . log . w ( log_tag @$ __str__ ) ; context . start activity ( resolve intent ) ; },user access,fail,pre
skip block if block error <PLACE_HOLDER> no instruction address,continue ;,error has,success,pre
bitwise or combines the sign bits so any negative value <PLACE_HOLDER> the check .,if ( ( index | limit | bytes . length - limit ) < __num__ ) { throw new array index out of bounds exception ( string . format ( __str__ @$ bytes . length @$ index @$ limit ) ) ; } long offset = index ; final long offset limit = limit ; if ( state != complete ) { if ( offset >= offset limit ) { return state ; } int byte1 = ( byte ) state ; if ( byte1 < ( byte ) __num__ ) { if ( byte1 < ( byte ) __num__ || unsafe util . get byte ( bytes @$ offset ++ ) > ( byte ) __num__ ) { return malformed ; } } else if ( byte1,value fails,success,pre
the user can <PLACE_HOLDER> the hadoop memory,map < string @$ string > variables = new hash map < string @$ string > ( system . getenv ( ) ) ;,user specify,success,pre
might need to reconsider using collector 's <PLACE_HOLDER> time for link statistics . we need to convert to time window 's timestamp . if not @$ it may lead to oom due to mismatch in timeslots .,long timestamp = time window . refine timestamp ( span . get collector accept time ( ) ) ; if ( parent application . get service type ( ) . is user ( ) ) { if ( logger . is trace enabled ( ) ) { logger . trace ( __str__ @$ parent application @$ span . get agent id ( ) @$ span application @$ span . get agent id ( ) ) ; } final link data map source link data = link data duplex map . get source link data map ( ) ; source link data . add link data ( parent application @$ span . get agent id ( ) @$ span application @$ span . get agent id ( ) @$,collector accept,success,pre
the map has no guarantees on order @$ but the lists should <PLACE_HOLDER> up 1:1 with key : val associations preserved .,list < string > keys = conf ( fake config ) . keys ( ) ; list < string > vals = conf ( fake config ) . vals ( ) ; for ( int i = __num__ ; i < fake config . size ( ) ; i ++ ) { assert that ( fake config . get ( keys . get ( i ) ) @$ is ( vals . get ( i ) ) ) ; } map < string @$ string > map = conf ( keys @$ vals ) . as map ( ) ; for ( int i = __num__ ; i < fake config . size ( ) ; i ++ ) { assert that ( vals . get ( i,lists end,fail,pre
parameter in select list <PLACE_HOLDER> a cast .,ddl error test ( __str__ @$ __str__ + __str__ + __str__ @$ __str__ ) ;,parameter needs,success,pre
if a thread has <PLACE_HOLDER> the fd and a close is n't pending then use a deferred close . also decrement fd use count to signal the last thread that releases the fd to close it .,if ( ! close pending ) { close pending = true ; fd use count -- ; socket pre close ( ) ; },thread acquired,success,pre
check has min <PLACE_HOLDER> length,memory . get bytes ( filler addr @$ prgm filler bytes ) ; if ( arrays . equals ( prgm filler bytes @$ target filler bytes ) ) { int filler len = __num__ ; string undef data string rep = undefined data . get default value representation ( ) ; address set set = new address set ( current program @$ filler addr @$ current program . get max address ( ) ) ; address iterator addr iter = set . get addresses ( filler addr . next ( ) @$ true ) ; while ( addr iter . has next ( ) ) { address next addr = addr iter . next ( ) ; if ( listing . get undefined data at ( next addr,check requested,fail,pre
fall back to default value suppose column label <PLACE_HOLDER> table name,if ( column label . contains ( __str__ ) ) { table name = string utils . substring before last ( column label @$ __str__ ) ; },label contains,success,pre
ensure the declared dependency only <PLACE_HOLDER> one possible bundle,if ( coordinates != null && ! coordinates . contains ( bundle dependency coordinate ) ) { if ( coordinates . size ( ) == __num__ ) { final bundle coordinate coordinate = coordinates . stream ( ) . find first ( ) . get ( ) ; final optional < bundle > matching dependency id bundle = get bundle ( coordinate ) ; if ( matching dependency id bundle . is present ( ) ) { final string dependency coordinate str = bundle dependency coordinate . get coordinate ( ) ; logger . warn ( string . format ( __str__ @$ bundle detail . get coordinate ( ) . get coordinate ( ) @$ dependency coordinate str @$ coordinate . get coordinate ( ) ) ) ;,dependency has,success,pre
but our test framework does n't <PLACE_HOLDER> that syntax directly .,session old kathmandu time zone offset session = session . builder ( this . session ) . set time zone key ( time_zone_key ) . set start time ( new date time ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ date_time_zone ) . get millis ( ) ) . build ( ) ; time zone key europe warsaw time zone key = get time zone key ( __str__ ) ; date time zone europe warsaw time zone = get date time zone ( europe warsaw time zone key ) ; session europe warsaw session winter = session . builder ( this . session ) . set time zone key ( europe warsaw time zone key ) . set start time ( new date,framework support,success,pre
note : m result handler <PLACE_HOLDER> main looper @$ so this must not be blocked .,m result handler = new handler ( context . get main looper ( ) ) ; m closed = false ;,handler uses,success,pre
this first query retrieves the last version for each file history matching the three <PLACE_HOLDER> properties . however @$ it does not guarantee that this version is indeed the last version in that particular file history @$ so we need another query to verify that .,try ( prepared statement prepared statement = get statement ( __str__ ) ) { prepared statement . set string ( __num__ @$ filecontent checksum ) ; prepared statement . set long ( __num__ @$ size ) ; prepared statement . set timestamp ( __num__ @$ new timestamp ( modified date . get time ( ) ) ) ; try ( result set result set = prepared statement . execute query ( ) ) { collection < partial file history > file histories = new array list < > ( ) ; while ( result set . next ( ) ) { string file history id = result set . get string ( __str__ ) ; partial file history file history = get last version by file history id,three install,fail,pre
do n't let a single processor <PLACE_HOLDER> the processor thread .,m logger . w ( __str__ @$ e ) ;,processor block,fail,pre
only set the badge number if it exists in the style . defaulting it to 0 means the badge will incorrectly <PLACE_HOLDER> text when the user may want a numberless badge .,if ( a . has value ( r . styleable . badge_number ) ) { set number ( a . get int ( r . styleable . badge_number @$ __num__ ) ) ; } set background color ( read color from attributes ( context @$ a @$ r . styleable . badge_background color ) ) ;,badge show,success,pre
server 2 will <PLACE_HOLDER> cache after creating 10 keys,add listener and put all in client1 . await ( ) ;,server close,success,pre
health check <PLACE_HOLDER> probe config health conditions,probe result consumer . accept ( new probe result ( probe factory . get workspace id ( ) @$ probe factory . get machine name ( ) @$ probe factory . get server name ( ) @$ probe status . passed ) ) ;,check satisfies,success,pre
not possible to expand since we have more than one chunk with a single segment . this is the case when user wants to append a segment with coarser granularity . case metadata storage has segments with granularity hour and segments to append have day granularity . druid shard specs does not <PLACE_HOLDER> multiple partitions for same interval with different granularity .,if ( existing chunks . size ( ) > __num__ ) { throw new illegal state exception ( string . format ( __str__ + __str__ @$ data source @$ segment . get interval ( ) @$ existing chunks . size ( ) ) ) ; },specs support,success,pre
set app <PLACE_HOLDER> time to be consumable by the am .,application id application id = application . get app attempt id ( ) . get application id ( ) ; environment . put ( application constants . app_submit_time_env @$ string . value of ( rm context . getrm apps ( ) . get ( application id ) . get submit time ( ) ) ) ; credentials credentials = new credentials ( ) ; data input byte buffer dibb = new data input byte buffer ( ) ; byte buffer tokens = container . get tokens ( ) ; if ( tokens != null ) { dibb . reset ( tokens ) ; credentials . read token storage stream ( dibb ) ; tokens . rewind ( ) ; },app submit,success,pre
intent is explicit and there 's no receivers . this happens @$ e.g . @$ when a system component sends a broadcast to its own runtime receiver @$ and there 's no manifest receivers for it @$ because this method is called twice for each broadcast @$ for runtime receivers and manifest receivers and the later check would <PLACE_HOLDER> no receivers .,if ( receivers == null || receivers . size ( ) == __num__ ) { return ; },check find,success,pre
this overwrites whatever <PLACE_HOLDER> the user configured in the properties,adjust auto commit config ( properties @$ offset commit mode ) ;,whatever setting,success,pre
add formats which <PLACE_HOLDER> everything but the encoding second,if ( input media format . matches without ( mf @$ mime type key @$ encoding key ) ) { formats . add ( matching count @$ mf . append ( input media format ) ) ; } else { formats . add ( mf . append ( input media format ) ) ; },which accept,fail,pre
app <PLACE_HOLDER> buffer will extended upto current application buffer size we need to <PLACE_HOLDER> the existing content into dst before we can do unwrap again . if there are no space in dst we can break here .,if ( dst . has remaining ( ) ) read += read from app buffer ( dst ) ; else break ;,app read,success,pre
file status.equals only <PLACE_HOLDER>s path field @$ must explicitly <PLACE_HOLDER> all fields,assert . assert false ( src status . get permission ( ) . equals ( dst status . get permission ( ) ) ) ; assert . assert false ( src status . get owner ( ) . equals ( dst status . get owner ( ) ) ) ; assert . assert false ( src status . get group ( ) . equals ( dst status . get group ( ) ) ) ; assert . assert true ( src status . get access time ( ) == dst status . get access time ( ) ) ; assert . assert true ( src status . get modification time ( ) == dst status . get modification time ( ) ) ;,status.equals compare,success,pre
sdk default policy should <PLACE_HOLDER> the client config level max error retry,test actual retries ( client_config_level_max_retry ) ;,policy take,fail,pre
run the <PLACE_HOLDER> fn code now that all side inputs are ready .,for ( bag state < windowed value < inputt > > elements bag : elements bags ) { iterable < windowed value < inputt > > elements = elements bag . read ( ) ; for ( windowed value < inputt > elem : elements ) { simple do fn runner . process element ( elem ) ; } elements bag . clear ( ) ; } side input fetcher . release blocked windows ( ready windows ) ;,the do,success,pre
asif : remove the connection from active map so that destroy can be called on it . we will first collect all the connections which have timed out & then expire them . in that gap even if the client genuinely <PLACE_HOLDER> the connection @$ it will not be returned to the pool as the active map no longer contains it,if ( ( now - then ) > time out ) { this . active cache . remove ( conn ) ; this . expired conns . add ( conn ) ; ++ num conn timed out ; } else { sleep time = then + time out - now ; to continue = false ; },genuinely stops,fail,pre
ugly but this is what hive <PLACE_HOLDER> internally anyway,try { class . for name ( driver name ) ; } catch ( class not found exception ex ) { throw new runtime exception ( ex ) ; },hive does,fail,pre
the last rollback should <PLACE_HOLDER> the 1 st message to get sent to the dlq,m = ( text message ) consumer . receive ( __num__ ) ; assert not null ( m ) ; assert equals ( __str__ @$ m . get text ( ) ) ; session . commit ( ) ;,rollback cause,success,pre
if the caller did n't <PLACE_HOLDER> an explicit time tracker @$ we want to continue tracking under any it has .,if ( r . app time tracker == null && source record != null ) { r . app time tracker = source record . app time tracker ; },caller specify,success,pre
making an uncoordinated call @$ which <PLACE_HOLDER> the proxy to observer node .,dfs . get client ( ) . getha service state ( ) ; dfs . mkdir ( test path @$ fs permission . get default ( ) ) ; assert sent to ( __num__ ) ; thread reader = new thread ( ( ) -> { try { dfs . get file status ( test path ) ; read status . set ( __num__ ) ; } catch ( io exception e ) { e . print stack trace ( ) ; read status . set ( - __num__ ) ; } } ) ;,which activates,fail,pre
check if consumer set <PLACE_HOLDER> msg @$ then let the test <PLACE_HOLDER> .,if ( null != fail msg ) { log . error ( fail msg ) ; fail ( fail msg ) ; },set fail,success,pre
string all <PLACE_HOLDER> fruit names,do xquery test ( xml_snippet @$ __str__ @$ arrays . as list ( fruit names ) ) ;,all entered,fail,pre
one from a source if one exists . if there is a newer source then <PLACE_HOLDER> that source .,try { get envelope ( ) ; } catch ( soap exception e ) { } return document . get document element ( ) ;,one use,success,pre
we ca n't use the same tactic as for intersection since abstract collection only <PLACE_HOLDER> a remove on the first element it encounters .,comparator < t > number comparator = new number aware comparator < t > ( ) ; if ( nlgn sort && ( head instanceof comparable ) ) { set < t > answer ; if ( head instanceof number ) { answer = new tree set < t > ( number comparator ) ; answer . add all ( self ) ; for ( t t : self ) { if ( t instanceof number ) { for ( object t2 : remove me ) { if ( t2 instanceof number ) { if ( number comparator . compare ( t @$ ( t ) t2 ) == __num__ ) answer . remove ( t ) ; } } } else { if ( remove me .,collection does,success,pre
cool union should <PLACE_HOLDER> a byte data type as the last component,dtc = union . get component ( __num__ ) ; assert true ( dtc . get data type ( ) . is equivalent ( new byte data type ( ) ) ) ; dtc = union . get component ( __num__ ) ; assert equals ( s @$ dtc . get data type ( ) ) ;,union have,success,pre
track the regionserver <PLACE_HOLDER> online regions in memory .,synchronized ( rs reports ) { rs reports . put ( server name @$ region names ) ; } if ( region names . is empty ( ) ) { log . trace ( __str__ @$ server name ) ; return ; },regionserver has,fail,pre
the database to query from db the columns to return from the query from db the columns for the where clause the values for the where clause <PLACE_HOLDER> n't group the rows <PLACE_HOLDER> n't filter by row groups the sort order,cursor cursor = query builder . query ( db @$ projection @$ selection @$ selection args @$ null @$ null @$ sort order ) ;,clause do,success,pre
the newly effective conf does not <PLACE_HOLDER> vol 0 and vol 2 .,string [ ] effective volumes = dn . get conf ( ) . get ( dfs_datanode_data_dir_key ) . split ( __str__ ) ; assert equals ( __num__ @$ effective volumes . length ) ; for ( string ev : effective volumes ) { assert that ( new file ( storage location . parse ( ev ) . get uri ( ) ) . get canonical path ( ) @$ is ( not ( any of ( is ( new dirs . get ( __num__ ) ) @$ is ( new dirs . get ( __num__ ) ) ) ) ) ) ; },conf contain,fail,pre
all splits must <PLACE_HOLDER> the same query id,set < string > actual = split completed events . stream ( ) . map ( split completed event :: get query id ) . collect ( to set ( ) ) ; assert equals ( actual @$ immutable set . of ( query completed event . get metadata ( ) . get query id ( ) ) ) ;,splits have,success,pre
the instrumentation runner <PLACE_HOLDER> the package name of the test,args . add ( __str__ @$ get test package ( ) ) ; args . add ( __str__ @$ get target package ( ) ) ; args . add ( __str__ @$ get test runner ( ) ) ; args . add ( __str__ @$ get path to adb executable ( ) ) ; if ( get test filter ( ) . is present ( ) ) { args . add ( __str__ @$ __str__ + get test filter ( ) . get ( ) ) ; } for ( map . entry < string @$ string > arg pair : get environment overrides ( ) . entry set ( ) ) { args . add ( __str__ @$ string . format ( locale . us @$ __str__,runner expects,fail,pre
do a temp <PLACE_HOLDER> cleanup on app 2,eph atm . clean temp containers ( test utils . get mock application id ( __num__ ) ) ; assert . assert equals ( __num__ @$ atm . get node cardinality by op ( node id . from string ( __str__ ) @$ allocation tags . create single app allocation tags ( test utils . get mock application id ( __num__ ) @$ immutable set . of ( __str__ ) ) @$ long :: sum ) ) ;,temp stack,fail,pre
overcautiousness : each event logger has its own events list @$ so one bad test wo n't <PLACE_HOLDER> others .,m logger events = new array list < > ( ) ; m logger = new event logger ( m logger events ) ; local broadcast manager . register receiver ( m logger @$ intent filter ) ;,test blow,fail,pre
window frames may have changed . <PLACE_HOLDER> the input dispatcher about it .,m input monitor . layout input consumers ( dw @$ dh ) ; m input monitor . set update input windows needed lw ( ) ; if ( update input windows ) { m input monitor . update input windows lw ( false ) ; },window tell,success,pre
this from element will be rendered as part of the origins <PLACE_HOLDER> sequence,from element . set text ( __str__ ) ; from elements . add ( from element ) ;,part reference,fail,pre
previously a update readers twice in a row would <PLACE_HOLDER> an npe . in test this would also normally <PLACE_HOLDER> an npe be<PLACE_HOLDER> scan.store is null . so as long as we get through these two calls we are good and the bug was quashed .,try ( store scanner scan = new store scanner ( new scan ( ) @$ scan info @$ get cols ( __str__ @$ __str__ ) @$ scanners ) ) { scan . update readers ( collections . empty list ( ) @$ collections . empty list ( ) ) ; scan . update readers ( collections . empty list ( ) @$ collections . empty list ( ) ) ; scan . peek ( ) ; },readers throw,fail,pre
generate the map <PLACE_HOLDER> operator,return map join processor . convertsmb join to map join ( physical context . get conf ( ) @$ newsmb join op @$ map join pos @$ true ) ;,map join,success,pre
we 've already found a constructor match before @$ this means that power mock can not determine which method to expect since there are two methods with the same name and the same number of arguments but one is <PLACE_HOLDER> wrapper types .,throw exception when multiple constructor matches found ( new java . lang . reflect . constructor [ ] { potential constructor . get java constructor ( ) @$ constructor . get java constructor ( ) } ) ;,one using,success,pre
java does n't <PLACE_HOLDER> simple exploration of resources as directories when the resources are inside a jar file . this searches the contents of the jar to get the list,try { url class url = jar hash . class . get resource ( __str__ ) ; if ( class url != null && class url . get protocol ( ) . equals ( __str__ ) ) { string jar path = class url . get path ( ) . substring ( __num__ @$ class url . get path ( ) . index of ( __str__ ) ) ; jar file jar = new jar file ( url decoder . decode ( jar path @$ __str__ ) ) ; enumeration < jar entry > files = jar . entries ( ) ; while ( files . has more elements ( ) ) { string f name = files . next element ( ) . get name ( ),java allow,success,pre
shut down the platform when user <PLACE_HOLDER> log window .,platform . run later ( ( ) -> { platform . set implicit exit ( true ) ; launcher . stop without platform ( ) ; } ) ;,user closes,fail,pre
method name can not <PLACE_HOLDER> reserved keyword @$ e.g . return,if ( is reserved word ( sanitized operation id ) ) { logger . warn ( operation id + __str__ + underscore ( __str__ + operation id ) ) ; sanitized operation id = __str__ + sanitized operation id ; } return camelize ( sanitized operation id ) ;,name use,success,pre
if this is a segment expression @$ it will already have been dealt with in the translation of the memory access during the call to translate children of node . just <PLACE_HOLDER> the translation .,if ( is segment expression ( current node value ) ) { return partial results . get ( __num__ ) ; } else if ( is memory access ( current node value ) ) { return process simple memory access ( environment @$ segment override @$ size @$ load operand @$ partial results . get ( __num__ ) ) ; } else { throw new internal translation exception ( __str__ ) ; },children return,success,pre
tests might <PLACE_HOLDER> scm id to indicate which scm should be used to find credential we have to do this because api url might be of wire mock server and not github,if ( api url . starts with ( git hubscm source . github_url ) || ( string utils . is not blank ( scm id ) && scm id . equals ( github scm . id ) ) ) { scm = new github scm ( new reachable ( ) { @ override public link get link ( ) { preconditions . check not null ( organization ) ; return organization . get link ( ) . rel ( __str__ ) ; } } ) ; } else { scm = new github enterprise scm ( ( new reachable ( ) { @ override public link get link ( ) { preconditions . check not null ( organization ) ; return organization . get link ( ) .,tests add,success,pre
the ri silently <PLACE_HOLDER> these @$ but android has always logged .,system . loge ( __str__ @$ ex ) ;,ri logs,fail,pre
no deployment defined repository @$ <PLACE_HOLDER> the default,if ( job repository name != null ) { service builder . add dependency ( support . get capability service name ( capabilities . job_repository_capability . get name ( ) @$ job repository name ) @$ job repository . class @$ service . get job repository injector ( ) ) ; } else { service . get job repository injector ( ) . set value ( new immediate value < > ( job repository ) ) ; },deployment use,success,pre
client should have <PLACE_HOLDER> the error,assert that ( __str__ @$ client socket . error latch . await ( __num__ @$ seconds ) @$ is ( true ) ) ; assert that ( __str__ @$ client socket . error . get ( ) @$ instance of ( message too large exception . class ) ) ;,client received,fail,pre
the new code browser selection should <PLACE_HOLDER> both of our vertex selections,assert true ( code browser selection . get min address ( ) . equals ( first selection . get min address ( ) ) ) ; assert true ( code browser selection . get max address ( ) . equals ( second selection . get max address ( ) ) ) ;,selection contain,fail,pre
legacy parser was signaling all created index antlr is parsing only those @$ which will <PLACE_HOLDER> any model changes,int number of created indexes which not make change on tables model = __num__ ; int number of alter view statements = __num__ ; int number of dropped views = __num__ ; assert that ( listener . total ( ) ) . is equal to ( __num__ - number of altered tables which does not exists - number of indexes on non existing tables - number of created indexes which not make change on tables model + number of alter view statements + number of dropped views ) ; listener . for each ( this :: print event ) ;,which prevent,fail,pre
set the usage analyzer as parent to make sure that the usage log <PLACE_HOLDER> the subclass data .,logger . set parent ( logger . get logger ( resource usage analyzer . class . get name ( ) ) ) ;,log contains,success,pre
the splits <PLACE_HOLDER> an implicit dependency on the base apk . this means that we <PLACE_HOLDER> to add the base apk file in addition to the shared libraries .,string base apk name = new file ( info . get base code path ( ) ) . get name ( ) ; string base class path = base apk name ;,splits have,success,pre
offline compaction should not have <PLACE_HOLDER> a new oplog .,assert equals ( original if length @$ if file . length ( ) ) ; connectd sand cache ( ) ; dsf = cache . create disk store factory ( ) ; disk store = dsf . create ( name ) ; af = new attributes factory ( ) ; af . set disk store name ( name ) ; af . set data policy ( data policy . persistent_replicate ) ; r = cache . create region ( __str__ @$ af . create ( ) ) ; assert equals ( __num__ @$ r . size ( ) ) ; assert equals ( __str__ @$ r . get ( __str__ ) ) ; assert equals ( __str__ @$ r . get ( __str__ ) ) ;,compaction created,success,pre
triggers update system ui visibility lw which will <PLACE_HOLDER> the flags as needed,int finish post layout policy lw = m display policy . focus changed lw ( m window @$ m window ) ; assert equals ( window manager policy . finish_layout_redo_layout @$ finish post layout policy lw ) ; assert equals ( __num__ @$ m display policy . m last system ui flags ) ; assert equals ( __num__ @$ m window . m attrs . system ui visibility ) ; assert inset by top bottom ( m window . get content frame lw ( ) @$ status_bar_height @$ nav_bar_height ) ;,which reapply,fail,pre
top <PLACE_HOLDER> top right bottom <PLACE_HOLDER> bottom right,matrix . set poly to poly ( new float [ ] { __num__ @$ __num__ @$ width @$ __num__ @$ __num__ @$ height @$ width @$ height } @$ __num__ @$ m display orientation == __num__ ? new float [ ] { __num__ @$ height @$ __num__ @$ __num__ @$ width @$ height @$ width @$ __num__ } : new float [ ] { width @$ __num__ @$ width @$ height @$ __num__ @$ __num__ @$ __num__ @$ height } @$ __num__ @$ __num__ ) ;,top left,success,pre
if src area is beyond the bounds of the image @$ we must clip it . the transform is based on the specified area @$ not the <PLACE_HOLDER> one .,if ( sx1 < __num__ ) { sx1 = __num__ ; } else if ( sx1 > img width ) { sx1 = img width ; },the provided,fail,pre
embms temp files can <PLACE_HOLDER> arbitrary content .,return __str__ ;,files have,fail,pre
next load keystore data to <PLACE_HOLDER> pd 's,key store ks = key store . get instance ( key store . get default type ( ) ) ; ks . load ( new file input stream ( system . get property ( __str__ @$ __str__ ) + file . separator char + __str__ ) @$ store password ) ; no_store_domain = new protection domain ( new code source ( new url ( __str__ ) @$ ( java . security . cert . certificate [ ] ) null ) @$ null @$ null @$ null ) ;,data verify,fail,pre
add conference <PLACE_HOLDER> button .,invite button = ui component registry . get button factory ( ) . create invite conference button ( ) ; invite button . set tool tip text ( res . get string ( __str__ ) ) ; chat room . add chat room button ( invite button ) ; invite button . add action listener ( this ) ;,conference meet,fail,pre
try to encode two c es as one <PLACE_HOLDER> 32 .,if ( ces length == __num__ ) { long ce0 = ces [ __num__ ] ; long ce1 = ces [ __num__ ] ; long p0 = ce0 > > > __num__ ; if ( ( ce0 & __num__ ) == collation . common_secondary_ce && ( ce1 & __num__ ) == collation . common_tertiary_ce && p0 != __num__ ) { return ( int ) p0 | ( ( ( int ) ce0 & __num__ ) << __num__ ) | ( ( ( int ) ce1 > > __num__ ) & __num__ ) | collation . special_ce32_low_byte | collation . latin_expansion_tag ; } },one byte,fail,pre
if header does n't exist @$ method get id <PLACE_HOLDER> a new random id,if ( response packet . get message ( ) . get headers ( ) . get ( av . messageid tag @$ false ) == null ) { string newid = message . generate messageid ( ) ; hl . add ( new string header ( av . messageid tag @$ newid ) ) ; },id generates,fail,pre
perform cache directive operations <PLACE_HOLDER> a closed file system .,distributed file system dfs1 = ( distributed file system ) cluster . get new file system instance ( __num__ ) ; dfs1 . close ( ) ; try { dfs1 . list cache directives ( null ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic test utils . assert exception contains ( __str__ @$ ioe ) ; } try { dfs1 . add cache directive ( alpha ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic test utils . assert exception contains ( __str__ @$ ioe ) ; } try { dfs1 . modify cache directive ( alpha ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic,operations using,success,pre
if we 're copying in a model we need a model schema <PLACE_HOLDER> 3 of the right class to fill into .,model m = model metrics . model ( ) ; if ( m != null ) { this . model = new model keyv3 ( m . _key ) ; this . model_category = m . _output . get model category ( ) ; this . model_checksum = m . checksum ( ) ; },schema v,success,pre
the data is locked in the cache @$ or we 're ignoring the cache . <PLACE_HOLDER> the cache and read from upstream .,if ( next span == null ) { next data source = upstream data source ; next data spec = new data spec ( uri @$ http method @$ http body @$ read position @$ read position @$ bytes remaining @$ key @$ flags ) ; } else if ( next span . is cached ) { uri file uri = uri . from file ( next span . file ) ; long file position = read position - next span . position ; long length = next span . length - file position ; if ( bytes remaining != c . length_unset ) { length = math . min ( length @$ bytes remaining ) ; } next data spec = new data spec ( file uri,cache enforce,fail,pre
case 23 : true if an attribute named primitive int att name of type int <PLACE_HOLDER> a value equal to int value multiplicated by one we cover javax.management.binary rel query exp with a rel op equal to times,queries . add ( query . eq ( query . attr ( primitive int att name ) @$ query . times ( query . value ( int value ) @$ query . value ( __num__ ) ) ) ) ;,name has,success,pre
check that older snapshot still <PLACE_HOLDER> the old ec policy settings,assert null ( __str__ @$ fs . get erasure coding policy ( snap1 ) ) ; assert equals ( __str__ @$ ec63 policy @$ fs . get erasure coding policy ( snap2 ) ) ; assert null ( __str__ @$ fs . get erasure coding policy ( snap3 ) ) ;,snapshot has,fail,pre
<PLACE_HOLDER> queries without indexes <PLACE_HOLDER> all queries .,for ( int i = __num__ ; i < queries . length ; i ++ ) { try { results [ i ] [ __num__ ] = qs . new query ( __str__ + queries [ i ] ) . execute ( ) ; } catch ( exception e ) { throw new runtime exception ( __str__ + queries [ i ] @$ e ) ; } } qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$,queries run,success,pre
done is always true until the last stream has be put in the queue . then the stream should be <PLACE_HOLDER> good input streams .,synchronized ( done ) { return ! done . get ( ) || ! queue . is empty ( ) ; } if ( fail . get ( ) != null ) { throw new re ( fail . get ( ) ) ; } try { return dequeue ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; throw new runtime exception ( e ) ; },stream spouting,success,pre
the configuration provider will return null instead of throwing out exceptions @$ if there are no configuration files provided . rm will not <PLACE_HOLDER> the remote configuration files @$ and should start successfully .,try { rm = new mockrm ( configuration ) ; rm . init ( configuration ) ; rm . start ( ) ; } catch ( exception ex ) { fail ( __str__ ) ; },rm load,success,pre
now c 2 should <PLACE_HOLDER> c 1 for source tag 1 .,pcm . add constraint ( app id1 @$ source tag1 @$ c2 @$ true ) ; assert . assert equals ( __num__ @$ pcm . get constraints ( app id1 ) . size ( ) ) ; assert . assert equals ( c2 @$ pcm . get constraint ( app id1 @$ source tag1 ) ) ;,c replace,success,pre
synchronized search <PLACE_HOLDER> stability we must create and add new value if there are no needed entry,synchronized ( this . queue ) { current = get entry value ( key @$ hash @$ this . table [ index ( hash @$ this . table ) ] ) ; if ( current != null ) { return current ; } v value = create ( key ) ; objects . require non null ( value @$ __str__ ) ; int index = index ( hash @$ this . table ) ; this . table [ index ] = new cache entry < > ( hash @$ key @$ value @$ this . table [ index ] ) ; if ( ++ this . size >= this . threshold ) { if ( this . table . length == maximum_capacity ) { this . threshold =,search improves,success,pre
setting volume on ui sounds stream type also <PLACE_HOLDER> silent mode,if ( ( ( flags & audio manager . flag_allow_ringer_modes ) != __num__ ) || ( stream == get ui sounds stream type ( ) ) ) { set ringer mode ( get new ringer mode ( stream @$ index @$ flags ) @$ tag + __str__ @$ false ) ; },type controls,success,pre
grid near tx <PLACE_HOLDER> request unmarshalling failed test,read cnt . set ( __num__ ) ; fail optimistic ( ) ;,grid prepare,success,pre
system can <PLACE_HOLDER> the timestamp .,m context . binder . clear calling identity ( ) ; assert equals ( bug report request time @$ dpm . get last bug report request time ( ) ) ;,system retrieve,success,pre
the old rule <PLACE_HOLDER> another package instance and we wa n't to keep the invariant that every rule <PLACE_HOLDER> the package it is contained within,try { rule new rule = builder . create rule ( rule . get label ( ) @$ rule . get rule class object ( ) @$ rule . get location ( ) @$ rule . get attribute container ( ) ) ; new rule . populate output files ( null event handler . instance @$ builder ) ; if ( rule . contains errors ( ) ) { new rule . set contains errors ( ) ; } builder . add rule ( new rule ) ; } catch ( label syntax exception e ) { throw new illegal state exception ( e ) ; },rule has,fail,pre
make assignments . the host on top of the set <PLACE_HOLDER> the smallest total score,for ( string id : scores . key set ( ) ) { server assignment least loaded = host scores . get ( __num__ ) ; least loaded . score += scores . get ( id ) ; balanced assignments . put ( id @$ least loaded . host ) ; host scores . sort ( comparator . comparing long ( o -> o . score ) ) ; } return balanced assignments ;,host has,success,pre
since derby database which is used for unit test does not <PLACE_HOLDER> timezone in date and time type @$ and put sql converts date string into long representation using local timezone @$ we need to use local timezone .,simple date format time format = new simple date format ( time format string ) ; java . util . date parsed local time = time format . parse ( time str ) ; simple date format date format = new simple date format ( date format string ) ; java . util . date parsed local date = date format . parse ( date str ) ;,database support,fail,pre
saving the tool <PLACE_HOLDER> the options,tool = save tool ( env . get project ( ) @$ tool ) ;,tool sets,fail,pre
we only register the app <PLACE_HOLDER> externalizers as all of the infinispan ones are explicitly defined,add reflection for class ( advanced externalizer . class @$ true @$ app only index @$ reflective class @$ collections . empty set ( ) ) ;,app accounted,fail,pre
exclude locals which do n't <PLACE_HOLDER> valid storage,if ( ! var . is valid ( ) ) { continue ; },which have,success,pre
delete local file to make sure that the <PLACE_HOLDER> requests downloads from ha,file blob file = server . get storage location ( job id @$ key ) ; assert true ( blob file . delete ( ) ) ;,the fetch,fail,pre
batch @$ legacy @$ <PLACE_HOLDER> 11,options . set streaming ( false ) ; system . set property ( __str__ @$ __str__ ) ; assert that ( get container image for job ( options ) @$ equal to ( __str__ ) ) ;,batch install,fail,pre
system processes can <PLACE_HOLDER> ui @$ and when they <PLACE_HOLDER> we want to have them trim their memory after the user leaves the ui . to facilitate this @$ here we need to determine whether or not it is currently showing ui .,app . system no ui = true ; if ( app == top_app ) { app . system no ui = false ; app . set current scheduling group ( process list . sched_group_top_app ) ; app . adj type = __str__ ; } else if ( app . has top ui ( ) ) { app . system no ui = false ; app . adj type = __str__ ; } else if ( wpc . has visible activities ( ) ) { app . system no ui = false ; },processes exit,fail,pre
pi array node <PLACE_HOLDER> array length provider and value proxy . we want to treat it as an array length provider @$ therefore we check this case first .,do { if ( current instanceof array length provider ) { return ( ( array length provider ) current ) . find length ( mode @$ constant reflection ) ; } else if ( current instanceof value phi node ) { return phi array length ( ( value phi node ) current @$ mode @$ constant reflection ) ; } else if ( current instanceof value proxy node ) { value proxy node proxy = ( value proxy node ) current ; value node length = array length ( proxy . get original node ( ) @$ mode @$ constant reflection ) ; if ( mode == array length provider . find length mode . canonicalize_read && length != null && ! length . is constant ( ),node implements,success,pre
make a store <PLACE_HOLDER> and write data to it .,store file writer writer = new store file writer . builder ( conf @$ cache conf @$ fs ) . with file path ( f ) . with file context ( meta ) . build ( ) ; write store file ( writer ) ; writer . close ( ) ; reader context context = new reader context builder ( ) . with file system and path ( fs @$ f ) . build ( ) ; h file info file info = new h file info ( context @$ conf ) ; store file reader reader = new store file reader ( context @$ file info @$ cache conf @$ new atomic integer ( __num__ ) @$ conf ) ; file info . init meta and index,store file,success,pre
this can only happen for a throwable <PLACE_HOLDER> not an exception @$ i.e . an error,if ( ! success && ! state changed ) { unlock ( ) ; },throwable handling,fail,pre
make sure both servers can <PLACE_HOLDER> the type,serverb . invoke ( ( ) -> assert equals ( new simple class ( __num__ @$ ( byte ) __num__ ) @$ get cache ( ) . get region ( __str__ ) . get ( __str__ ) ) ) ; servera . invoke ( ( ) -> assert equals ( new simple class ( __num__ @$ ( byte ) __num__ ) @$ get cache ( ) . get region ( __str__ ) . get ( __str__ ) ) ) ;,servers deserialize,success,pre
assertion on auto <PLACE_HOLDER> another dummy data source,metadata source config another dummymd source = ds to onboards map . get ( __str__ ) . get ( __num__ ) . get metadata source config ( ) ; assert . assert equals ( another dummymd source . get class name ( ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . size ( ) @$ __num__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __num__ ) ;,assertion onboard,success,pre
bacause script pid utils.is pid running do n't <PLACE_HOLDER> mac,if ( ! os . is family unix ( ) || os . is family mac ( ) ) { return ; } final int shard count = __num__ ; final string job name = __str__ ; job config job config = new job config ( ) ; job config . set job name ( job name ) ; job config . set cron ( __str__ ) ; job config . set job type ( job type . shell_job . to string ( ) ) ; job config . set job class ( longtime java job . class . get canonical name ( ) ) ; job config . set timeout seconds ( __num__ ) ; job config . set sharding total count ( shard count ) ;,utils.is support,success,pre
drawable container ' constant state has <PLACE_HOLDER> instances . in order to leave the constant state intact in the cache @$ we need to create a new drawable container after added to cache .,if ( dr instanceof drawable container ) { needs new drawable after cache = true ; },state object,fail,pre
if overwritten not <PLACE_HOLDER> @$ it should be which user <PLACE_HOLDER>,verify queue mapping ( new queue mapping ( mapping type . group @$ __str__ @$ __str__ @$ __str__ ) @$ __str__ @$ __str__ @$ __str__ @$ true ) ;,user specified,success,pre
0 x 100415 a : p 1 and p 2 <PLACE_HOLDER> same plate comments .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . plate_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . plate_comment ) ; program merge = new program merge manager ( p1 @$ p2 @$ task monitor adapter . dummy_monitor ) ; program merge . set diff filter ( new program diff filter ( program diff filter . comment_diffs ) ) ; program merge . set merge filter ( new program merge filter ( program merge filter . comments @$ program merge filter . merge ) ) ; address set as = new address set ( ) ; as . add range ( addr ( __num__ ) @$ addr ( __num__ ) ) ; as . add range (,0 have,success,pre
no key id specified @$ <PLACE_HOLDER> no matches,if ( sought keyid == null ) { return mono . just ( collections . empty list ( ) ) ; },id assume,fail,pre
a user in role contractor can only <PLACE_HOLDER> a post if confidential,response = _connector . get response ( __str__ + __str__ + encoded chris + __str__ + __str__ ) ; assert that ( response @$ starts with ( __str__ ) ) ; assert that ( response @$ contains string ( __str__ ) ) ;,user create,fail,pre
if the callback <PLACE_HOLDER> ok @$ we close the dialog,if ( callback != null ) { close = callback . execute ( old master password @$ get new password ( ) ) ; } if ( close ) { dialog = null ; dispose ( ) ; },callback finished,fail,pre
p 2 post comments <PLACE_HOLDER> the p 1 comment string .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . post_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . post_comment ) ; check comment difference ( __num__ ) ;,comments contain,success,pre
we fake only once the expired iterator exception at the specified get records <PLACE_HOLDER> order,if ( ( integer . value of ( shard iterator ) == order of call to expire - __num__ ) && ! expired once already ) { expired once already = true ; throw new expired iterator exception ( __str__ ) ; } else if ( expired once already && ! expired iterator refreshed ) { throw new runtime exception ( __str__ ) ; } else { return new get records result ( ) . with records ( shard itr to record batch . get ( shard iterator ) ) . with millis behind latest ( millis behind latest ) . with next shard iterator ( ( integer . value of ( shard iterator ) == total num of get records calls - __num__ ) ? null :,records fires,fail,pre
we use recursive algorithm with pure complexity and do n't want it to take forever stacked barcode can <PLACE_HOLDER> up to 11 rows @$ so 25 seems reasonable enough,if ( this . rows . size ( ) > __num__ ) { this . rows . clear ( ) ; return null ; },barcode have,success,pre
this flag will be set to false after first incremental load is done . this flag is used by repl copy task to check if duplicate file check is required or not . this flag is used by compaction to check if compaction can be done for this database or not . if compaction is done before first incremental then duplicate check will fail,parameters . put ( repl utils . repl_first_inc_pending_flag @$ __str__ ) ; return parameters ;,compaction does,fail,pre
old line spacing . all lines should <PLACE_HOLDER> their ascent and descents from the first font .,static layout layout = static layout . builder . obtain ( text @$ __num__ @$ text . length ( ) @$ paint @$ para width ) . set include pad ( false ) . set use line spacing from fallbacks ( false ) . build ( ) ; assert equals ( __num__ @$ layout . get line count ( ) ) ; assert equals ( - text size @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * text size @$ layout . get line descent ( __num__ ) ) ; assert equals ( - text size @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * text size @$ layout . get line descent (,lines get,success,pre
m device idle controller.step idle state locked does n't <PLACE_HOLDER> the active case @$ so the state should stay as active .,verify state conditions ( state_active ) ;,state process,fail,pre
jwt proxy container does n't <PLACE_HOLDER> proxy settings since it never does any outbound requests @$ and setting of it may fail accessing internal addresses .,k8s env . get pods data ( ) . entry set ( ) . stream ( ) . filter ( entry -> ! entry . get key ( ) . equals ( jwt_proxy_pod_name ) ) . flat map ( entry -> stream . concat ( entry . get value ( ) . get spec ( ) . get containers ( ) . stream ( ) @$ entry . get value ( ) . get spec ( ) . get init containers ( ) . stream ( ) ) ) . for each ( container -> proxy env vars . for each ( ( k @$ v ) -> container . get env ( ) . add ( new env var ( k @$ v @$ null ) ),container need,success,pre
zero distance <PLACE_HOLDER> a run,if ( distance == __num__ ) { byte x = input [ in offset + ip - __num__ ] ; while ( ip < ip bound ) { if ( input [ in offset + ref ++ ] != x ) { break ; } else { ip ++ ; } } } else { for ( ; ; ) { if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ],distance means,success,pre
new user <PLACE_HOLDER> registration key,new user . set activation key ( random util . generate activation key ( ) ) ; set < authority > authorities = new hash set < > ( ) ; authority repository . find by id ( authorities constants . user ) . if present ( authorities :: add ) ; new user . set authorities ( authorities ) ; user repository . save ( new user ) ; log . debug ( __str__ @$ new user ) ; return new user ;,user gets,success,pre
we can leave the nested loop . every character in the query can <PLACE_HOLDER> at most one character in the term .,term character match found = true ;,character match,success,pre
some types do n't <PLACE_HOLDER> a static initializer @$ that is ok,have looked for static initializer = true ;,types have,success,pre
if remote client <PLACE_HOLDER> video,if ( local video port > __num__ ) { if ( settings manager . get local preferences ( ) . get video device ( ) != null && ! __str__ . equals ( settings manager . get local preferences ( ) . get video device ( ) ) ) { video media session video media session = media manager . create video media session ( call . get remote sdp description ( ) . to string ( ) @$ local video port ) ; if ( video media session != null ) { video media session . start trasmit ( ) ; video media session . start receive ( ) ; call . set video media session ( video media session ) ; } } } evt .,client supports,fail,pre
hayes modem with dos configurator,hayes . accept ( con dos ) ;,instructions pcandidates,fail,pre
our randomly <PLACE_HOLDER> folder significantly .,return name . length ( ) == __num__ ;,our generated,success,pre
in a character class @$ this.chardata <PLACE_HOLDER> one character @$ that is to say @$ a pair of surrogates is composed and stored to this.chardata .,if ( this . context == s_inbrackets ) { switch ( ch ) { case __str__ : ret = t_backsolidus ; if ( this . offset >= this . regexlen ) throw ex ( __str__ @$ this . offset - __num__ ) ; this . chardata = this . regex . char at ( this . offset ++ ) ; break ; case __str__ : if ( this . offset < this . regexlen && this . regex . char at ( this . offset ) == __str__ ) { this . offset ++ ; ret = t_xmlschema_cc_subtraction ; } else ret = t_char ; break ; case __str__ : if ( ! this . is set ( regular expression . xmlschema_mode ) && this . offset <,this.chardata contains,fail,pre
j 2 se does not <PLACE_HOLDER> xalan interpretive,sax transformer factory stf = ( sax transformer factory ) tfactory ;,se support,success,pre
a change in priority is only relevant for static rr os : specifically @$ a regular rro should not <PLACE_HOLDER> its state reset only because a change in priority,if ( the truth . is static overlay package ( ) && the truth . overlay priority != old settings . priority ) { return true ; } return false ;,rro have,success,pre
emergency calling <PLACE_HOLDER> voice capability .,if ( m is voice capable ) { if ( is in call ( ) ) { visible = true ; } else { final boolean sim locked = keyguard update monitor . get instance ( m context ) . is sim pin voice secure ( ) ; if ( sim locked ) { visible = m enable emergency call while sim locked ; } else { visible = m lock pattern utils . is secure ( keyguard update monitor . get current user ( ) ) ; } } },calling set,fail,pre
if the focus changes very quickly before the first request is returned each focus change triggers a new partition and we end up with many duplicate partitions . this is enhanced as the focus change can be much faster than the taking of the assist structure . hence remove the currently queued request and replace it with the one queued after the structure is,cancel current request locked ( ) ; try { final bundle receiver extras = new bundle ( ) ; receiver extras . put int ( extra_request_id @$ request id ) ; final long identity = binder . clear calling identity ( ) ; try { if ( ! activity task manager . get service ( ) . request autofill data ( m assist receiver @$ receiver extras @$ m activity token @$ flags ) ) { slog . w ( tag @$ __str__ + m activity token ) ; } } finally { binder . restore calling identity ( identity ) ; } } catch ( remote exception e ) { },one take,fail,pre
does this target <PLACE_HOLDER> the drop action type being dropped on it ?,int da = e . get drop action ( ) ; if ( ( da & drop actions ) == __num__ ) { return false ; },target have,fail,pre
if the new kproject <PLACE_HOLDER> a different classloader than the original one it <PLACE_HOLDER> to be initialized,if ( class loader != k project . get class loader ( ) ) { k project . init ( ) ; },kproject has,success,pre
resets the content <PLACE_HOLDER> scroll position when swiping the panel down after being maximized .,if ( m content != null && ty > __num__ && get panel state ( ) == panel state . maximized ) { m content . reset content view scroll ( ) ; },content view,success,pre
separately handle the case where the grid <PLACE_HOLDER> no table columns . in this situation @$ there is a single default column .,if ( column count == __num__ ) { this . column = ( column == __num__ ) ? __num__ : - __num__ ; layout ( ) ; return ; } if ( this . column > - __num__ && this . column < column count ) { this . column = - __num__ ; } if ( column < __num__ || column >= grid . get column count ( ) ) return ; this . column = column ; layout ( ) ;,grid has,success,pre
we have converted the sax events generated by tika into a dom object so we can now use the usual html resources from nutch <PLACE_HOLDER> meta directives,html meta processor . get meta tags ( meta tags @$ root @$ base ) ; if ( log . is trace enabled ( ) ) { log . trace ( __str__ + base + __str__ + meta tags . to string ( ) ) ; },resources gathered,fail,pre
fail silently . apparently some other event <PLACE_HOLDER> the debug connection .,if ( ! debugger . is connected ( ) ) { return ; },event closed,success,pre
get the partition id and its crc and validate it . validating the partition id for the chunk separately makes it possible to continue processing chunks from other partitions if only one partition <PLACE_HOLDER> corrupt chunks in the file .,final checksum partition idcrc = m_checksum type == checksum type . crc32c ? new pure java crc32c ( ) : new pure java crc32 ( ) ; chunk lengthb . mark ( ) ; final int next chunk partition id = chunk lengthb . get int ( ) ; final int next chunk partition idcrc = chunk lengthb . get int ( ) ; chunk lengthb . reset ( ) ; byte partition id bytes [ ] = new byte [ __num__ ] ; chunk lengthb . get ( partition id bytes ) ; partition idcrc . update ( partition id bytes @$ __num__ @$ partition id bytes . length ) ; int generated value = ( int ) partition idcrc . get value ( ) ; if,partition has,success,pre
the monitor we are exiting is not verifiably the one on the top of our monitor stack . this <PLACE_HOLDER> a monitor mismatch .,if ( ! actual . is lock reference ( ) || ! expected . equal ( actual ) ) { _monitor_top = bad_monitors ; _monitor_safe = false ; basic block bb = get basic block containing ( bci ) ; bb . set changed ( true ) ; bb . _monitor_top = bad_monitors ; if ( trace monitor mismatch ) { report monitor mismatch ( __str__ ) ; } } else { replace allcts matches ( actual @$ cell type state . make line ref ( bci ) ) ; },monitor generates,fail,pre
create the index <PLACE_HOLDER> node with all its metadata,scan node . set lookup type ( path . lookup type ) ; scan node . set bindings ( path . bindings ) ; scan node . set end expression ( expression util . combine predicates ( expression type . conjunction_and @$ path . end exprs ) ) ; if ( ! path . index . get predicatejson ( ) . is empty ( ) ) { try { scan node . set partial index predicate ( abstract expression . fromjson string ( path . index . get predicatejson ( ) @$ table scan ) ) ; } catch ( json exception e ) { throw new planning error exception ( e . get message ( ) @$ __num__ ) ; } } scan node . set predicate,index scan,success,pre
check strip hidden configurations <PLACE_HOLDER> the property,configuration conf2 = new configuration ( conf ) ; conf2 . set ( hidden config @$ __str__ ) ; conf . strip hidden configurations ( conf2 ) ;,configurations overwrite,fail,pre
crawler url must <PLACE_HOLDER> crawler url must not <PLACE_HOLDER> crawler ip must <PLACE_HOLDER> crawler ip must not <PLACE_HOLDER> crawler country must <PLACE_HOLDER> crawler no depth limit <PLACE_HOLDER> index url must <PLACE_HOLDER> index url must not <PLACE_HOLDER> index content must <PLACE_HOLDER> index content must not <PLACE_HOLDER>,this . default surrogate profile = new crawl profile ( crawl_profile_surrogate @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_surrogate_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ false @$ false @$ true @$ false @$ false @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . nocache @$ __str__ + crawl_profile_surrogate @$ client identification . yacy intranet crawler agent name @$ null @$ null,country match,success,pre
now <PLACE_HOLDER> middlesection of row .,lpart = ( left far delimiter < __num__ ? llength + loffset : left far delimiter ) - left delimiter ; rpart = ( right far delimiter < __num__ ? rlength + roffset : right far delimiter ) - right delimiter ; result = super . compare rows ( left @$ left delimiter @$ lpart @$ right @$ right delimiter @$ rpart ) ; if ( result != __num__ ) { return result ; } else { if ( left delimiter < __num__ && right delimiter >= __num__ ) { return - __num__ ; } else if ( right delimiter < __num__ && left delimiter >= __num__ ) { return __num__ ; } else if ( left delimiter < __num__ && right delimiter < __num__ ) { return,now compare,success,pre
replication factor <PLACE_HOLDER> matter,assert true ( bc known10 . compare to ( bc known1000 ) < __num__ ) ; assert true ( bc unknown10 . compare to ( bc unknown1000 ) < __num__ ) ;,factor does,success,pre
these key managers may <PLACE_HOLDER> both ... see above ...,for ( src = __num__ @$ dst = __num__ ; src < tma . length ; ) { if ( ! ( tma [ src ] instanceof javax . net . ssl . trust manager ) ) { if ( tma [ src ] instanceof x509 trust manager ) { tmaw [ dst ] = ( javax . net . ssl . trust manager ) new x509 trust manager javax wrapper ( ( x509 trust manager ) tma [ src ] ) ; dst ++ ; } } else { tmaw [ dst ] = ( javax . net . ssl . trust manager ) tma [ src ] ; dst ++ ; } src ++ ; },managers implement,success,pre
call this first @$ to let the parent <PLACE_HOLDER> the dialog,new loading dialog ( this @$ r . string . afc_msg_loading @$ false ) { @ override protected void on pre execute ( ) { super . on pre execute ( ) ; switch ( display prefs . get view type ( file chooser activity . this ) ) { case grid : display prefs . set view type ( file chooser activity . this @$ view type . list ) ; break ; case list : display prefs . set view type ( file chooser activity . this @$ view type . grid ) ; break ; } setup view files ( ) ; if ( build . version . sdk_int >= build . version_codes . honeycomb ) activity compat . invalidate options menu ( file chooser,parent show,fail,pre
this point is only reached if the operation <PLACE_HOLDER> more than the allowed retry count,log . warn ( __str__ + __str__ + __str__ @$ m key @$ m upload id @$ m bucket name @$ property key . underfs_cleanup_enabled . get name ( ) @$ last exception ) ;,operation failed,success,pre
else this schema index provider does n't <PLACE_HOLDER> any persistent storage to delete .,delete obsolete indexes = true ;,provider have,success,pre
hopefully given collection wo n't <PLACE_HOLDER> more than 10 items between now and the synchronized block ...,int initial = math . min ( c . size ( ) + __num__ @$ max ) ; int count = __num__ ; array list < t > list = new array list < t > ( initial ) ; synchronized ( c ) { iterator < t > iter = c . iterator ( ) ; while ( iter . has next ( ) && ( count < max ) ) { list . add ( iter . next ( ) ) ; count ++ ; } } return list ;,collection have,fail,pre
selecting a item with enter will <PLACE_HOLDER> the focus and selected item @$ which means that further keyboard selection wo n't work unless we do this :,if ( event . get type int ( ) == event . onkeydown && event . get key code ( ) == key codes . key_enter ) { final menu item item = get selected item ( ) ; super . on browser event ( event ) ; scheduler . get ( ) . schedule deferred ( new scheduled command ( ) { @ override public void execute ( ) { select item ( item ) ; focus ( ) ; } } ) ; } else { super . on browser event ( event ) ; },item lose,success,pre
tree display <PLACE_HOLDER> box,box tree display prefs = box . create vertical box ( ) ; tree display prefs . set border ( border factory . create titled border ( __str__ ) ) ; j panel tree display options = new j panel ( ) ; tree display options . set layout ( new grid layout ( __num__ @$ __num__ ) ) ; j label font name = new j label ( __str__ ) ; final j combo box font picker = new j combo box ( graphics environment . get local graphics environment ( ) . get available font family names ( ) ) ; font picker . set selected item ( preferences . get font ( ) ) ; j label size label = new j label ( __str__,display pick,fail,pre
state change listener <PLACE_HOLDER> final query info and then clears scheduler when the query finishes .,sql query scheduler scheduler = query scheduler . get ( ) ; optional < query info > final query info = state machine . get final query info ( ) ; if ( final query info . is present ( ) ) { return final query info . get ( ) . get query stats ( ) . get total memory reservation ( ) ; } if ( scheduler == null ) { return new data size ( __num__ @$ byte ) ; } return succinct bytes ( scheduler . get total memory reservation ( ) ) ;,listener sets,success,pre
see if we only have a mnemonic and no operands . if so @$ then just <PLACE_HOLDER> the mnemonic to the preview panel and increment our counter .,if ( metadata != null ) { if ( metadata . get operands ( ) == null || metadata . get operands ( ) . size ( ) == __num__ ) { mask container mask container = metadata . get mask container ( ) ; if ( mask container != null && mask container . get value ( ) != null ) { instr size = mask container . get value ( ) . length ; } } else if ( metadata . get operands ( ) != null ) { operand metadata operand = metadata . get operands ( ) . get ( __num__ ) ; if ( operand == null ) { continue ; } instr size = operand . get mask container ( ) .,counter add,success,pre
check that the listener on the animation that was being clone receive the animation <PLACE_HOLDER> events for the clones .,assert true ( only contains ( started animators @$ s1 @$ s2 @$ s3 ) ) ; assert true ( only contains ( canceled animators @$ s3 ) ) ; assert true ( only contains ( ended animators @$ s1 @$ s2 @$ s3 ) ) ;,animation start,fail,pre
the ipport does not <PLACE_HOLDER> any existing h 2 o cloud member or client,throw new illegal argument exception ( __str__ + node idx + __str__ ) ;,ipport require,fail,pre
another call to get groups should <PLACE_HOLDER> 3 groups instead of 2,assert that ( groups . get groups ( __str__ ) . size ( ) ) . is equal to ( __num__ ) ;,call return,fail,pre
verify the distance is zero as long as two nodes <PLACE_HOLDER> the same path . they do n't need to refer to the same object .,node base node1 = new node base ( data nodes [ __num__ ] . get host name ( ) @$ data nodes [ __num__ ] . get network location ( ) ) ; node base node2 = new node base ( data nodes [ __num__ ] . get host name ( ) @$ data nodes [ __num__ ] . get network location ( ) ) ; assert equals ( __num__ @$ cluster . get distance ( node1 @$ node2 ) ) ;,nodes have,success,pre
default constructor <PLACE_HOLDER> same accessibility flags . exception : enum constructor which is always private,if ( ! is enum && ( ( class acces flags & accessibility_flags ) != ( method access flags & accessibility_flags ) ) ) { return false ; } int count = __num__ ; for ( struct method mt : cl . get methods ( ) ) { if ( code constants . init_name . equals ( mt . get name ( ) ) ) { if ( ++ count > __num__ ) { return false ; } } } return true ;,constructor has,fail,pre
class we must guarantee that only one thread at a time <PLACE_HOLDER> our number format .,synchronized ( number format ) { return ( number format ) number format . clone ( ) ; },thread update,fail,pre
quick exit : if the filesystem does not <PLACE_HOLDER> encryption @$ we can exit early .,if ( ! is encryption supported ( ) ) { return device policy manager . encryption_status_unsupported ; },filesystem support,success,pre
every adapter <PLACE_HOLDER> an unique index id,for ( adapter adapter : adapters ) { adapter data observer observer = new adapter data observer ( m total @$ m index gen == null ? m index ++ : m index gen . increment and get ( ) ) ; adapter . register adapter data observer ( observer ) ; has stable ids = has stable ids && adapter . has stable ids ( ) ; layout helper helper = adapter . on create layout helper ( ) ; helper . set item count ( adapter . get item count ( ) ) ; m total += helper . get item count ( ) ; helpers . add ( helper ) ; pair = pair . create ( observer @$ adapter ) ; m index ary,adapter gets,fail,pre
thread does not have a preference so <PLACE_HOLDER> default,if ( b == null ) { return ! share sockets ; } else { return b ; },preference use,fail,pre
kudu does n't support general not predicates @$ but some not operators can be converted into kudu predicates . <PLACE_HOLDER> leaf to predicates below .,translate ( root . get children ( ) . get ( __num__ ) @$ leaves @$ ! is not @$ schema @$ results ) ; return ; case leaf : predicate leaf leaf = leaves . get ( root . get leaf ( ) ) ; if ( schema . has column ( leaf . get column name ( ) ) ) { results . add all ( leaf to predicates ( leaf @$ is not @$ schema ) ) ; } return ; case constant : return ;,some convert,fail,pre
change the default settings for byte data type ; should not <PLACE_HOLDER> the typedef default settings,settings . set long ( __str__ @$ format settings definition . binary ) ; bdefs [ __num__ ] . copy setting ( settings @$ default settings ) ; def settings = td . get default settings ( ) ; assert null ( def settings . get value ( __str__ ) ) ;,settings change,fail,pre
before or after the boolean flag <PLACE_HOLDER> the alias .,construction exception e = assert throws ( __str__ + __str__ @$ construction exception . class @$ ( ) -> construct ( example prefixed foo options . class @$ example bar was named foo option . class ) ) ; assert that ( e ) . has cause that ( ) . is instance of ( duplicate option declaration exception . class ) ; assert that ( e ) . has message that ( ) . contains ( __str__ ) ; e = assert throws ( __str__ + __str__ @$ construction exception . class @$ ( ) -> construct ( example bar was named foo option . class @$ example prefixed foo options . class ) ) ; assert that ( e ) . has cause that ( ),flag introduces,success,pre
if user code does n't <PLACE_HOLDER> end response @$ the framework automati<PLACE_HOLDER>y does with no cause .,log . end response ( ) ; assert that ( log . response duration nanos ( ) ) . is zero ( ) ; assert that ( log . response cause ( ) ) . is same as ( error ) ;,code log,fail,pre
replace spaces because importer ca n't <PLACE_HOLDER> attributes titles in quotes,writer . append ( __str__ ) ;,importer handle,fail,pre
recycler view <PLACE_HOLDER> also the same control to provide better performance .,m column header recycler view . set has fixed size ( has fixed width ) ;,view has,success,pre
now @$ using error.two inner profile @$ which <PLACE_HOLDER> another syntax :,assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ; assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ; assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ;,which supports,fail,pre
no known conditional special case mapping @$ <PLACE_HOLDER> a normal mapping,if ( has slot ( exc word @$ exc_full_mappings ) ) { long value = get slot value and offset ( exc word @$ exc_full_mappings @$ exc offset ) ; full = ( int ) value & __num__ ; exc offset = ( int ) ( value > > __num__ ) + __num__ ; exc offset += full & full_lower ; full >>= __num__ ; exc offset += full & __num__ ; full >>= __num__ ; if ( upper not title ) { full &= __num__ ; } else { exc offset += full & __num__ ; full = ( full > > __num__ ) & __num__ ; } if ( full != __num__ ) { try { out . append ( exceptions @$ exc offset @$ exc,case try,fail,pre
system <PLACE_HOLDER> version which should,d . set scope ( artifact . scope_system ) ; file file = new file ( get basedir ( ) @$ __str__ ) ; assert true ( file . exists ( ) ) ; d . set system path ( file . get canonical path ( ) ) ; artifact = repository system . create dependency artifact ( d ) ;,system download,fail,pre
let linger <PLACE_HOLDER> its course .,callback . assert no callback ( ) ; final int linger timeout ms = m service . m linger delay ms + m service . m linger delay ms / __num__ ; callback . expect callback ( callback entry . lost @$ m cell network agent @$ linger timeout ms ) ;,linger run,success,pre
verify only the other tenant <PLACE_HOLDER> the new state machine,final tenant key value empty tenant key1 = tenant api . get plugin payment state machine config ( plugin_name @$ request options for original tenant ) ; assert . assert equals ( empty tenant key1 . get values ( ) . size ( ) @$ __num__ ) ; final tenant key value tenant key1 other tenant = tenant api . get plugin payment state machine config ( plugin_name @$ request options ) ; assert . assert equals ( tenant key1 other tenant . get key ( ) @$ tenantkv . tenant key . plugin_payment_state_machine_ . to string ( ) + plugin_name ) ; assert . assert equals ( tenant key1 other tenant . get values ( ) . size ( ) @$ __num__ ) ;,tenant has,success,pre
share ud set <PLACE_HOLDER> datanodes that share same upgrade domain with another datanode .,list < t > shareud set = get shareud set ( ud map ) ;,domain contains,fail,pre
no room on current line . so <PLACE_HOLDER> a new line .,pw . println ( ) ; line length = __num__ ;,room write,fail,pre
ok let 's set the default parameter exclusion list <PLACE_HOLDER> the possibility to load it from an external file ...,if ( this . excluded params . is empty ( ) ) { add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_post_data @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_post_data @$ __str__ ) ; add scanner param,list have,fail,pre
bar refcount has <PLACE_HOLDER> 0 @$ a destroying task is scheduled,assert equals ( __num__ @$ scheduled destroy tasks . size ( ) ) ; scheduled destroy task = scheduled destroy tasks . poll ( ) ; assert equals ( shared resource holder . destroy_delay_seconds @$ scheduled destroy task . get delay ( time unit . seconds ) ) ;,refcount reached,success,pre
delete table once more ; the resource not <PLACE_HOLDER> exception swallowed silently,ddbms . destroy ( ) ; verify table not exist ( table name @$ dynamodb ) ; intercept ( io exception . class @$ __str__ @$ __str__ @$ ( ) -> ddbms . list children ( test path ) ) ; ddbms . destroy ( ) ; intercept ( file not found exception . class @$ __str__ @$ __str__ @$ ( ) -> ddbms . prune ( prune mode . all_by_modtime @$ __num__ ) ) ; destroy ( ddbms ) ;,resource found,success,pre
a very slow export decoder must have <PLACE_HOLDER> the export processor shutting down,if ( m_closed ) { export log . info ( __str__ ) ; container . internal discard ( ) ; } else { m_pending container . set ( container ) ; },decoder had,fail,pre
the input object inspectors <PLACE_HOLDER> the keys and values of the big table and small table .,big table standard object inspector = object inspector factory . get standard struct object inspector ( big table column name list @$ big table object inspector list ) ; small table standard object inspector = object inspector factory . get standard struct object inspector ( small table column name list @$ small table object inspector list ) ; input object inspectors = new object inspector [ ] { big table standard object inspector @$ small table standard object inspector } ;,inspectors represent,fail,pre
pkix validation has failed . <PLACE_HOLDER> an exception .,if ( expecting trust manager . has exception ( ) ) { throw expecting trust manager . get exception ( ) ; },pkix throw,success,pre
this should be a rare case : normally this wildcard is already captured . but it does happen if the upper bound of a type variable <PLACE_HOLDER> a wildcard,if ( type instanceof wildcard type ) { return ( ( wildcard type ) type ) . get upper bounds ( ) ; } else if ( type instanceof capture type ) { return ( ( capture type ) type ) . get upper bounds ( ) ; } else if ( type instanceof generic array type ) { return get array exact direct super types ( type ) ; } else { throw new runtime exception ( __str__ + type ) ; },bound contains,success,pre
historic rules may contain null entries when original zoneinfo data <PLACE_HOLDER> non transition data .,if ( historic rules != null ) { for ( int i = __num__ ; i < historic rules . length ; i ++ ) { if ( historic rules [ i ] != null ) { size ++ ; } } },data contains,fail,pre
test validation <PLACE_HOLDER> this behavior,factory . set concurrency checks enabled ( false ) ;,validation expects,success,pre
set alpha will <PLACE_HOLDER> invalidate self and drive the animation .,int partial alpha = ( int ) ( alpha * normalized ) ; super . set alpha ( partial alpha ) ; super . draw ( canvas ) ; super . set alpha ( alpha ) ;,alpha make,fail,pre
the main menu <PLACE_HOLDER> a request to the local crawler page @$ but in case this table is empty @$ the overview page is shown,if ( post != null && post . contains key ( __str__ ) && tabletype == event origin . local_crawling && resultur ls . get stack size ( event origin . local_crawling ) == __num__ ) { tabletype = ( resultur ls . get stack size ( event origin . surrogates ) == __num__ ) ? event origin . unknown : event origin . surrogates ; },menu sends,fail,pre
no previous id @$ always <PLACE_HOLDER> this one,if ( requested session id == null ) { requested session id = id ; session = s ; } else if ( requested session id . equals ( id ) ) { } else if ( session == null || ! is valid ( session ) ) { requested session id = id ; session = s ; } else { if ( s != null && is valid ( s ) ) throw new bad message exception ( __str__ + requested session id + __str__ + id ) ; },id use,fail,pre
this gnarly block of code will <PLACE_HOLDER> all sockets and all thread @$ even if any close fails .,try { server socket . close ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ; } for ( iterator < socket > s = open client sockets . key set ( ) . iterator ( ) ; s . has next ( ) ; ) { try { s . next ( ) . close ( ) ; s . remove ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ; } } try { executor . shutdown ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ;,block release,success,pre
we copy all <PLACE_HOLDER> values from the get value call in case an optimised model is reusing one object to return many values . the number subclasses in the jdk are immutable and so will not be used in this way but other subclasses of number might want to do this to save space and avoid unnecessary heap allocation .,if ( type . get superclass ( ) == java . lang . number . class ) { number n1 = ( number ) data . get value at ( row1 @$ column ) ; double d1 = n1 . double value ( ) ; number n2 = ( number ) data . get value at ( row2 @$ column ) ; double d2 = n2 . double value ( ) ; if ( d1 < d2 ) { return - __num__ ; } else if ( d1 > d2 ) { return __num__ ; } else { return __num__ ; } } else if ( type == java . util . date . class ) { date d1 = ( date ) data . get value at,all returned,success,pre
async <PLACE_HOLDER> duration .,thread . sleep ( __num__ ) ;,async wait,success,pre
row set log should <PLACE_HOLDER> execution as well as performance logs,verify fetched log ( row set log @$ expected logs execution ) ; verify fetched log ( row set log @$ expected logs performance ) ; verify missing contents in fetched log ( row set log @$ expected logs verbose ) ;,log contain,success,pre
default root queue <PLACE_HOLDER> anyone to have admin acl,if ( is capacity scheduler ) { capacity scheduler configuration csconf = new capacity scheduler configuration ( ) ; csconf . set acl ( __str__ @$ queueacl . administer_queue @$ __str__ ) ; csconf . set acl ( __str__ @$ queueacl . administer_queue @$ __str__ ) ; rm . get resource scheduler ( ) . reinitialize ( csconf @$ rm . getrm context ( ) ) ; },queue allows,success,pre
check that the meta group <PLACE_HOLDER> its name .,assert equals ( __str__ @$ new meta group . get group name ( ) @$ renamed group name ) ;,group had,fail,pre
instantiate the class specified in the code or object section of the m <PLACE_HOLDER> tag,object o ; object instance obj inst ; if ( code != null && ser name != null ) { final string msg = __str__ + __str__ ; mlet_logger . logp ( level . finer @$ m let . class . get name ( ) @$ mth @$ msg ) ; mbeans . add ( new error ( msg ) ) ; continue ; } if ( code == null && ser name == null ) { final string msg = __str__ + __str__ ; mlet_logger . logp ( level . finer @$ m let . class . get name ( ) @$ mth @$ msg ) ; mbeans . add ( new error ( msg ) ) ; continue ; } try { if ( code !=,class let,success,pre
the filter ca n't <PLACE_HOLDER> more inserts and is effectively broken,overflowed = true ; throw new overflowed error ( ) ;,filter make,fail,pre
extension registry may be either extension registry or extension registry lite . since the type we are parsing is a full message @$ only a full extension registry could possibly <PLACE_HOLDER> extensions of it . otherwise we will treat the registry as if it were empty .,if ( type . is extension number ( field number ) ) { if ( extension registry instanceof extension registry ) { final extension registry . extension info extension = target . find extension by number ( ( extension registry ) extension registry @$ type @$ field number ) ; if ( extension == null ) { field = null ; } else { field = extension . descriptor ; default instance = extension . default instance ; if ( default instance == null && field . get java type ( ) == descriptors . field descriptor . java type . message ) { throw new illegal state exception ( __str__ + field . get full name ( ) ) ; } } } else { field =,registry contain,success,pre
the class <PLACE_HOLDER> every 5 minutes @$ so the first time it is called is true . the second time is throttled and returns false .,clock . set time ( clock . current time millis ( ) + duration . standard minutes ( __num__ ) . get millis ( ) ) ; assert false ( hot key logger . is throttled ( ) ) ; assert true ( hot key logger . is throttled ( ) ) ;,class runs,fail,pre
binder.java <PLACE_HOLDER> the resource for us .,@ suppress warnings ( __str__ ) final indenting print writer pw = new indenting print writer ( writer @$ __str__ ) ; if ( ! dump utils . check dump permission ( m context @$ tag @$ pw ) ) return ; pw . println ( __str__ ) ; pw . increase indent ( ) ; pw . println ( __str__ ) ; pw . increase indent ( ) ; final tethering configuration cfg = m config ; cfg . dump ( pw ) ; pw . decrease indent ( ) ; pw . println ( __str__ ) ; pw . increase indent ( ) ; m entitlement mgr . dump ( pw ) ; pw . decrease indent ( ) ; synchronized ( m public sync ),binder.java wraps,fail,pre
the script could <PLACE_HOLDER> an object that persists and retains a reference to the bindings,bindings . put ( __str__ @$ null ) ; bindings . put ( __str__ @$ null ) ;,script save,fail,pre
the two crawlers should <PLACE_HOLDER> different storage folders for their intermediate data .,config1 . set crawl storage folder ( crawl storage folder + __str__ ) ; config2 . set crawl storage folder ( crawl storage folder + __str__ ) ; config1 . set politeness delay ( __num__ ) ; config2 . set politeness delay ( __num__ ) ; config1 . set max pages to fetch ( __num__ ) ; config2 . set max pages to fetch ( __num__ ) ;,crawlers use,fail,pre
this pointer may be in work files because archiver did not <PLACE_HOLDER> the file yet @$ check that it is not too far forward .,cur wal segm idx = start . index ( ) ;,archiver read,fail,pre
this node <PLACE_HOLDER> been inactive @$ but other node <PLACE_HOLDER> more recent activity .,if ( ! is inactive ) { updated latest success transfer = latest reported cluster activity ; },node has,success,pre
set the socket <PLACE_HOLDER> limits for the underlying socket .,if ( options . sndbuf != __num__ ) { tcp utils . set tcp send buffer ( fd @$ options . sndbuf ) ; } if ( options . rcvbuf != __num__ ) { tcp utils . set tcp receive buffer ( fd @$ options . rcvbuf ) ; },socket buffer,success,pre
this sets up a channel for each consumer thread . we do n't track channels @$ as the connection will <PLACE_HOLDER> its channels implicitly,try { channel channel = connection . create channel ( ) ; rabbitmq span consumer consumer = new rabbitmq span consumer ( channel @$ collector @$ metrics ) ; channel . basic consume ( builder . queue @$ true @$ consumer tag @$ consumer ) ; } catch ( io exception e ) { throw new illegal state exception ( __str__ + consumer tag @$ e ) ; },connection register,fail,pre
let 's <PLACE_HOLDER> them !,collections . sort ( nodes ) ;,'s verify,fail,pre
and location manager which are in our process . the callbacks in these components may <PLACE_HOLDER> permissions our remote caller does not have .,final long identity = binder . clear calling identity ( ) ; try { final int callback count = callbacks . size ( ) ; for ( int i = __num__ ; i < callback count ; i ++ ) { final active callback callback = callbacks . value at ( i ) ; try { callback . m callback . op active changed ( code @$ uid @$ package name @$ active ) ; } catch ( remote exception e ) { } } } finally { binder . restore calling identity ( identity ) ; },callbacks require,success,pre
sort to get a deterministic canonical order . this probably is n't necessary because the options parser will <PLACE_HOLDER> its own sorting when canonicalizing @$ but it seems like it ca n't hurt .,incompatible changes . sort ( null ) ; return immutable list . copy of ( incompatible changes ) ;,parser do,success,pre
add 1 element @$ which should <PLACE_HOLDER> same view .,m layout manager . expect layouts ( __num__ ) ; m test adapter . add and notify ( __num__ ) ; m layout manager . wait for layout ( __num__ ) ; m activity rule . run on ui thread ( new runnable ( ) { @ override public void run ( ) { assert true ( __str__ @$ target child [ __num__ ] == m recycler view . get child at ( __num__ ) ) ; assert equals ( expected important for accessibility @$ view compat . get important for accessibility ( target child [ __num__ ] ) ) ; } } ) ;,which create,fail,pre
local class b must also <PLACE_HOLDER> the locals and pass them to a 's constructor .,assert translated lines ( translation @$ __str__ @$ __str__ @$ __str__ ) ;,b populate,fail,pre
we do n't <PLACE_HOLDER> permission to read boolean creds s ince the creds <PLACE_HOLDER> no boolean creds we should get an empty set,try { set priv cred set1 = s . get private credentials ( boolean . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { e . print stack trace ( ) ; throw new runtime exception ( __str__ ) ; } system . out . println ( __str__ ) ;,creds have,success,pre
address 2 will <PLACE_HOLDER> the cache,assert . assert equals ( access privilege . read_write @$ matcher . get access privilege ( address2 @$ hostname2 ) ) ; thread . sleep ( __num__ ) ;,address hit,success,pre
user <PLACE_HOLDER> params,if ( source inclusions . length > __num__ ) { return path pattern . create ( source inclusions ) ; },user defined,success,pre
wrap the inner parts of the loop in a catch throwable so that any errors in the loop do n't <PLACE_HOLDER> the entire thread .,try { handle = txn handler . get mutexapi ( ) . acquire lock ( txn store . mutex_key . initiator . name ( ) ) ; started at = system . current time millis ( ) ; long compaction interval = ( prev start < __num__ ) ? prev start : ( started at - prev start ) / __num__ ; prev start = started at ; show compact response current compactions = txn handler . show compact ( new show compact request ( ) ) ; set < compaction info > potentials = txn handler . find potential compactions ( aborted threshold @$ compaction interval ) . stream ( ) . filter ( ci -> check compaction elig ( ci ) ) . collect ( collectors,errors stop,fail,pre
map <PLACE_HOLDER> the same as collection,if ( param instanceof map ) { for ( map . entry < ? @$ ? > i : ( ( map < ? @$ ? > ) param ) . entry set ( ) ) { deferred parameter key = load object instance ( i . get key ( ) @$ existing @$ i . get key ( ) . get class ( ) ) ; deferred parameter val = i . get value ( ) != null ? load object instance ( i . get value ( ) @$ existing @$ i . get value ( ) . get class ( ) ) : null ; setup steps . add ( new serialzation step ( ) { @ override public void handle ( method context context,map works,success,pre
let app restrictions <PLACE_HOLDER>r package <PLACE_HOLDER> app restrictions,dpm . set application restrictions managing package ( admin1 @$ app restrictions manager package ) ; assert equals ( app restrictions manager package @$ dpm . get application restrictions managing package ( admin1 ) ) ;,package handle,fail,pre
verify data object <PLACE_HOLDER> extension,attributes = get data object attributes ( data obj ) ; assert equals ( __num__ @$ attributes . size ( ) ) ; for ( string key : attributes . key set ( ) ) { if ( key . equals ( __str__ ) ) { assert true ( __str__ . equals ( attributes . get ( key ) ) ) ; } else if ( key . equals ( __str__ ) ) { assert true ( __str__ . equals ( attributes . get ( key ) ) ) ; } else { fail ( __str__ ) ; } },object attributes,success,pre
now we know the monitor has <PLACE_HOLDER> the initial value @$ so if we want to test notify behaviour we can trigger by exceeding the threshold .,if ( when == when . in_notify ) { final thread tested thread = new thread ( sensitive thing ) ; final atomic integer notif count = new atomic integer ( ) ; final notification listener listener = new notification listener ( ) { public void handle notification ( notification n @$ object h ) { tested thread . start ( ) ; try { tested thread . join ( ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; } notif count . increment and get ( ) ; } } ; mbs . add notification listener ( monitor name @$ listener @$ null @$ null ) ; observed proxy . set thing ( __num__ ) ; system .,monitor observed,success,pre
we 've found the start of a rule or id . c is its first character @$ and pos <PLACE_HOLDER> past c .,-- pos ;,pos starts,fail,pre
force an extra <PLACE_HOLDER> node in case any guarded nodes are inputs to the state split,if ( current next instanceof abstract begin node && current next instanceof state split && ( ( state split ) current next ) . state after ( ) != null ) { begin node begin = graph ( ) . add ( new begin node ( ) ) ; begin . set node source position ( get no deopt successor position ( ) ) ; begin . set next ( current next ) ; current next = begin ; },extra begin,success,pre
address pointers will only be created if extent of table <PLACE_HOLDER> only undefined types or pointers,builder . create memory ( __str__ @$ __str__ @$ __num__ ) ;,extent contains,success,pre
no primary key @$ but <PLACE_HOLDER> only public key material .,keyset valid keyset = keyset . new builder ( ) . add key ( keyset . key . new builder ( ) . set key data ( test util . create key data ( key data . new builder ( ) . build ( ) @$ __str__ @$ key data . key material type . asymmetric_public ) ) . set key id ( __num__ ) . set status ( key status type . enabled ) . set output prefix type ( output prefix type . tink ) . build ( ) ) . build ( ) ; try { util . validate keyset ( valid keyset ) ; } catch ( general security exception e ) { fail ( __str__ + e ) ; },key requires,fail,pre
set the table <PLACE_HOLDER> id in all of the acid file sinks,if ( ! driver context . get plan ( ) . get acid sinks ( ) . is empty ( ) ) { list < file sink desc > acid sinks = new array list < > ( driver context . get plan ( ) . get acid sinks ( ) ) ; acid sinks . sort ( ( file sink desc fsd1 @$ file sink desc fsd2 ) -> fsd1 . get dir name ( ) . compare to ( fsd2 . get dir name ( ) ) ) ; for ( file sink desc desc : acid sinks ) { table desc table info = desc . get table info ( ) ; final table name tn = hive table name . of nullable ( table,table file,fail,pre
now let 's <PLACE_HOLDER> an identical key for get,binary object builder bldr = grid ( __num__ ) . binary ( ) . builder ( __str__ ) ; bldr . set field ( __str__ @$ __num__ ) ; bldr . set field ( __str__ @$ __str__ ) ; binary object bin key = bldr . build ( ) ; assert equals ( __str__ @$ c . get ( bin key ) ) ;,'s build,success,pre
if the value tag does not <PLACE_HOLDER> a parameter which is a valid field and it is not used within the comments of a valid field @$ return null .,return null ;,tag contain,fail,pre
force refresh since file id should <PLACE_HOLDER> reset,repository item = null ;,id perform,fail,pre
print <PLACE_HOLDER> message only for those that apply correctly or were already applied,if ( found data ) { println ( __str__ + data name + __str__ + current program . get executable path ( ) + __str__ + found list . get ( i ) . to string ( ) ) ; num data found ++ ; },print found,success,pre
the peer may have <PLACE_HOLDER> the socket because of the unmatched server name indication .,ssl socket . close ( ) ; ssl server socket . close ( ) ;,peer closed,success,pre
tell the user which methods <PLACE_HOLDER> this class to be abstract .,modifiers |= m_abstract ;,methods allow,fail,pre
management server <PLACE_HOLDER> an error and closes the stream .,response observer . on error ( status . unknown . as exception ( ) ) ;,server sends,fail,pre
validate selected controller services <PLACE_HOLDER> the api required by the processor,final collection < validation result > referenced service validation results = validate referenced controller services ( validation context ) ; validation results . add all ( referenced service validation results ) ; logger . debug ( __str__ @$ validation context @$ validation results ) ; return validation results ;,services meet,fail,pre
rebuilding will <PLACE_HOLDER> the new list,msg = builder . build ( ) ; assert that ( msg . repeated_double ) . is equal to ( doubles ) ;,rebuilding create,fail,pre
this is a direct call to a method that the static analysis did not see as invoked . this can happen when the receiver is always null . in most cases @$ the method profile also <PLACE_HOLDER> a length of 0 and the below code to kill the invoke would trigger . but not all methods have profiles @$ for example methods with manually,if ( call target . invoke kind ( ) . is direct ( ) && ! ( ( hosted method ) call target . target method ( ) ) . get wrapped ( ) . is simply implementation invoked ( ) ) { unreachable invoke ( graph @$ invoke @$ call target ) ; continue ; },profile has,success,pre
puff @$ everything <PLACE_HOLDER> ok,return return value ;,everything went,success,pre
when drag is enabled mouse drags wo n't <PLACE_HOLDER> the selection in the list @$ so we only set the is adjusting flag when it 's not enabled,list . set value is adjusting ( true ) ;,drags change,success,pre
if the user has not <PLACE_HOLDER> the default group name @$ add it,if ( ! group list . contains ( default group name ) ) { group list . add ( default group name ) ; } for ( int i = __num__ ; i < packages . length ; i ++ ) { package doc pkg = packages [ i ] ; string pkg name = pkg . name ( ) ; string group name = pkg name group map . get ( pkg name ) ; if ( group name == null ) { group name = reg exp group name ( pkg name ) ; } if ( group name == null ) { group name = default group name ; } get pkg list ( group package map @$ group name ) . add ( pkg,user set,fail,pre
transition <PLACE_HOLDER> 2 to active even though <PLACE_HOLDER> 1 still thinks it 's active,banner ( __str__ ) ; name node adapter . abort edit logs ( nn1 ) ; name node adapter . enter safe mode ( nn1 @$ false ) ; cluster . transition to active ( __num__ ) ;,transition nn,success,pre
remove all <PLACE_HOLDER> the list that is being called on . so create a copy,collection < string > nodes diff = new array list < string > ( all node names . size ( ) ) ; nodes diff . add all ( all node names ) ; nodes diff . remove all ( nodes in result ) ; if ( nodes diff . size ( ) > __num__ ) { check result = false ; for ( string node name : nodes diff ) { system . err . println ( key name + __str__ + node name ) ; } } return check result ;,all modifies,success,pre
check if the instruction is <PLACE_HOLDER> a simple enum to a different type .,invocation offset = offset ; clazz . constant pool entry accept ( constant instruction . constant index @$ parameter checker ) ; break ;,instruction generalizing,success,pre
shuffling <PLACE_HOLDER> the loading out across physical hosts better,collections . shuffle ( warehouse ids ) ; m_available warehouse ids . add all ( warehouse ids ) ; boolean do make replicated = true ; for ( load thread load thread : m_load threads ) { load thread . start ( do make replicated ) ; do make replicated = false ; } for ( int ii = __num__ ; ii < m_load threads . length ; ii ++ ) { try { m_load threads [ ii ] . join ( ) ; } catch ( interrupted exception e ) { e . print stack trace ( ) ; system . exit ( - __num__ ) ; } } try { m_volt client . drain ( ) ; } catch ( interrupted exception e ) { return,shuffling maintains,fail,pre
listener <PLACE_HOLDER> later so unneeded events not thrown,text menu . add action listener ( this ) ;,listener comes,fail,pre
'is file name in field ' will always <PLACE_HOLDER> 'false ',concat fields meta = new concat fields meta ( ) ; concat fields meta . set file name in field ( true ) ; assert false ( concat fields meta . is file name in field ( ) ) ; concat fields meta . set file name in field ( false ) ; assert false ( concat fields meta . is file name in field ( ) ) ;,name return,success,pre
initialize transactions if eos is turned on @$ which will block if the previous transaction has not completed yet ; do not <PLACE_HOLDER> the first transaction until the topology has been initialized later,if ( eos enabled ) { initialize transactions ( ) ; },transactions send,fail,pre
process variables should <PLACE_HOLDER> no effect,task service . set variable ( current task . get id ( ) @$ __str__ @$ __str__ ) ; current task = task service . create task query ( ) . single result ( ) ; assert equals ( __num__ @$ ( ( counting task entity ) current task ) . get variable count ( ) ) ; map < string @$ object > local vars = new hash map < > ( ) ; local vars . put ( __str__ @$ __str__ ) ; local vars . put ( __str__ @$ __str__ ) ; local vars . put ( __str__ @$ __str__ ) ; task service . set variables local ( current task . get id ( ) @$ local vars ) ; current task = task,variables have,success,pre
custom <PLACE_HOLDER> configs,pro guard obfuscate step . create ( test android platform target factory . create ( ) @$ java compilation constants . default_java_command_prefix @$ new fake project filesystem ( ) @$ optional . empty ( ) @$ __str__ @$ optional . empty ( ) @$ immutable set . of ( proguard config . get filesystem ( ) . resolve ( proguard config . get relative path ( ) ) @$ proguard config . get filesystem ( ) . resolve ( aapt proguard dir . resolve ( __str__ ) ) ) @$ pro guard obfuscate step . sdk proguard type . none @$ pro guard obfuscate step . default_optimization_passes @$ optional . empty ( ) @$ immutable map . of ( build target paths . get gen path ( library,custom proguard,success,pre
does this node <PLACE_HOLDER> security at all ?,if ( ! supported command classes . contains key ( command class . security ) ) { result = false ; } else { final int command class code = ( byte ) serial message . get message payload byte ( __num__ ) & __num__ ; final command class command class of message = command class . get command class ( command class code ) ; if ( command class of message == null ) { logger . warn ( string . format ( __str__ @$ get node id ( ) @$ command class code @$ serial message ) ) ; result = false ; } else if ( command class . security == command class of message ) { final byte message code = byte . value,node support,success,pre
this command also <PLACE_HOLDER> retrieving rows for an importing job .,string importing jobid = request . get parameter ( __str__ ) ; if ( importing jobid != null ) { long jobid = long . parse long ( importing jobid ) ; importing job job = importing manager . get job ( jobid ) ; if ( job != null ) { project = job . project ; } } if ( project == null ) { project = get project ( request ) ; } response . set header ( __str__ @$ __str__ ) ; map < string @$ language info > prefixes map = new hash map < > ( ) ; for ( string language prefix : meta parser . get language prefixes ( ) ) { language info info = meta parser . get,command supports,success,pre
when prepare deps of patterns function <PLACE_HOLDER> evaluation @$,evaluation context evaluation context = evaluation context . new builder ( ) . set keep going ( keep going ) . set num threads ( loading_phase_threads ) . set event hander ( new reporter ( new event bus ( ) @$ event collector ) ) . build ( ) ; evaluation result < sky value > evaluation result = get skyframe executor ( ) . get driver ( ) . evaluate ( singleton target pattern @$ evaluation context ) ;,function completes,success,pre
second @$ we must ensure that this method must be statically checked for example @$ in a mixed mode where only some methods are statically checked we must not visit a method which <PLACE_HOLDER>d dynamic dispatch . we do not check for an annotation beca<PLACE_HOLDER> some other ast transformations may <PLACE_HOLDER> this visitor without the annotation being explicitely set,if ( ! type checking context . methods to be visited . is empty ( ) && ! type checking context . methods to be visited . contains ( node ) ) return ;,transformations use,success,pre
previous status must be load now as measure matrix <PLACE_HOLDER> the live measure dto which are passed to it,metric . level previous status = load previous status ( metrics @$ db measures ) ; configuration config = project configuration loader . load project configuration ( db session @$ project ) ; debt rating grid debt rating grid = new debt rating grid ( config ) ; measure matrix matrix = new measure matrix ( components @$ metrics per id . values ( ) @$ db measures ) ; formula context impl context = new formula context impl ( matrix @$ debt rating grid ) ; long beginning of leak = get beginning of leak period ( last analysis @$ branch ) ; components . for each ( c -> { issue counter issue counter = new issue counter ( db client . issue dao ( ),matrix contains,fail,pre
for now @$ we can use the deprecated <PLACE_HOLDER> credentials list method .,return client . get credentials list ( ) ;,deprecated get,success,pre
footer <PLACE_HOLDER> an invisible first row,c10 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c10 . get text ( ) ) ; assert equals ( __str__ @$ __str__ @$ c10 . get attribute ( __str__ ) ) ; grid cell element c11 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c11 . get text ( ) ) ; grid cell element c20 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c20 . get text ( ) ) ; grid cell element c21 = get grid element ( ) . get footer cell ( __num__,footer has,success,pre
the procedure runner nt generator has all of the dangerous and slow stuff in it . <PLACE_HOLDER> classfinding @$ instantiation @$ and reflection .,runner generator map . put ( procedure . get type name ( ) @$ new procedure runnernt generator ( clz ) ) ;,runner do,fail,pre
test a call where one partition <PLACE_HOLDER> an error .,api error value = api error . from throwable ( new cluster authorization exception ( null ) ) ; list < replica election result > election results = new array list < > ( ) ; replica election result election result = new replica election result ( ) ; election result . set topic ( topic1 . topic ( ) ) ;,partition has,success,pre
a field filter @$ hence <PLACE_HOLDER> the mappings filtering part as a whole @$ as it requires parsing mappings into a map .,if ( first == mapper plugin . noop_field_filter ) { return second ; } if ( second == mapper plugin . noop_field_filter ) { return first ; } return index -> { predicate < string > first predicate = first . apply ( index ) ; predicate < string > second predicate = second . apply ( index ) ; if ( first predicate == mapper plugin . noop_field_predicate ) { return second predicate ; } if ( second predicate == mapper plugin . noop_field_predicate ) { return first predicate ; } return first predicate . and ( second predicate ) ; } ;,filter do,fail,pre
returning an empty array essentially means that the element does n't <PLACE_HOLDER> any annotations @$ but we know that it is not true since the reason the annotation parsing failed is because some annotation referenced a missing class . however @$ this allows us to defend against crashing the image builder if the above jdk bug is encountered in user code or if the,return new annotation [ __num__ ] ;,element declare,success,pre
before sending client ready @$ lets make sure the stats already <PLACE_HOLDER> 0 queued events,check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ; check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ; check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ;,stats show,fail,pre
our host did not <PLACE_HOLDER> a custom flutter engine . create a flutter engine to back our flutter view .,log . d ( tag @$ __str__ + __str__ ) ; is flutter engine from host = false ;,host specify,fail,pre
do not <PLACE_HOLDER> interface methods with instance methods do not <PLACE_HOLDER> private methods note : private methods from parent classes are not shown here @$ but when doing the multimethod connection step @$ we <PLACE_HOLDER> methods of the parent class with methods of a subclass and in that case we want to keep the private methods,if ( match . is private ( ) || ( ! is non real method ( match ) && match . get declaring class ( ) . is interface ( ) && ! method . get declaring class ( ) . is interface ( ) && ! method . is static ( ) ) ) { } else { cached class methodc = method . get declaring class ( ) ; cached class matchc = match . get declaring class ( ) ; if ( methodc == matchc ) { if ( is non real method ( method ) ) { list . set ( found @$ method ) ; } } else if ( ! methodc . is assignable from ( matchc . get the class (,methods overwrite,success,pre
since the bulk <PLACE_HOLDER> bails out on a single failure @$ this has to be done as a workaround,batch create types ( types def ) ;,bulk create,success,pre
note : parse offset pattern will <PLACE_HOLDER> the given pattern and throws illegal argument exception when pattern is not valid,object [ ] parsed items = parse offset pattern ( gmt offset patterns [ idx ] @$ t . required ( ) ) ; gmt offset pattern items [ idx ] = parsed items ;,pattern parse,fail,pre
normalization scale <PLACE_HOLDER> overflow,assert divide all signs ( new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } @$ new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) ;,scale prevents,fail,pre
bean may have <PLACE_HOLDER> new weak affinity,target = result . get node ( ) ; assert . assert equals ( count ++ @$ result . get value ( ) . int value ( ) ) ;,bean added,fail,pre
second attempt should <PLACE_HOLDER> the same behaviour as exponential backoff is disabled,assert equals ( reconnect backoff ms test @$ delay ) ;,attempt yield,fail,pre
if we 're a relaunch we may need to adjust button <PLACE_HOLDER> state,if ( icicle != null ) { m did acknowledge = icicle . get boolean ( key_did_acknowledge @$ false ) ; m allow button . set enabled ( ! m did acknowledge ) ; m deny button . set enabled ( ! m did acknowledge ) ; },button enabled,fail,pre
self gateway should <PLACE_HOLDER> its fencing token,assert equals ( new fencing token @$ fenced gateway . get fencing token ( ) ) ; assert equals ( new fencing token @$ fenced testing endpoint . get fencing token ( ) ) ; rpc utils . terminate rpc endpoint ( fenced testing endpoint @$ timeout ) ;,gateway retrieve,fail,pre
sort operator <PLACE_HOLDER> a new stage for the moment,return true ;,operator needs,fail,pre
client eventually <PLACE_HOLDER> broken pipe,assert throws ( io exception . class @$ ( ) -> { int i = __num__ ; for ( i = __num__ ; i < __num__ ; i ++ ) { client . get output stream ( ) . write ( __num__ ) ; } } ) ;,client hits,fail,pre
second gbk <PLACE_HOLDER> the output to be materialized,p assert . that ( result ) . contains in any order ( kv . of ( __str__ @$ immutable list . of ( __num__ @$ __num__ @$ __num__ ) ) @$ kv . of ( __str__ @$ immutable list . of ( __num__ ) ) ) ;,gbk requires,fail,pre
ca of the certificate @$ for which this crl is checked @$ has also <PLACE_HOLDER> crl @$ so skip the path validation @$ because is already done,if ( signing cert . equals ( defaultcrl sign cert ) ) { valid certs . add ( signing cert ) ; valid keys . add ( defaultcrl sign key ) ; continue ; } try { cert path builder builder = cert path builder . get instance ( __str__ @$ bouncy castle provider . provider_name ) ; selector = new x509 cert store selector ( ) ; selector . set certificate ( signing cert ) ; extendedpkix parameters temp = ( extendedpkix parameters ) paramspkix . clone ( ) ; temp . set target cert constraints ( selector ) ; extendedpkix builder parameters params = ( extendedpkix builder parameters ) extendedpkix builder parameters . get instance ( temp ) ; if ( cert path certs . contains,certificate signed,success,pre
verify the puts <PLACE_HOLDER> site 2,vm2 . invoke ( ( ) -> wan test base . validate region size ( get test method name ( ) + __str__ @$ __num__ ) ) ;,puts has,fail,pre
see if the input row <PLACE_HOLDER> the filename field ...,int idx = get input row meta ( ) . index of value ( meta . get accepting field ( ) ) ; if ( idx < __num__ ) { throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ meta . get accepting field ( ) ) ) ; },row supports,fail,pre
make sure the type matches what the return statements are <PLACE_HOLDER> .,type type = val needed ? this . type : type . t void ; expression e = new inline method expression ( where @$ type @$ field @$ new compound statement ( where @$ body ) ) ; return val needed ? e . inline value ( env @$ ctx ) : e . inline ( env @$ ctx ) ;,statements doing,fail,pre
dw streams <PLACE_HOLDER> the number of streams in the file . for example @$ a file with audio and video has two streams .,d . write int ( ( int ) largest buffer size ) ;,streams specifies,success,pre
set the screen 's skin <PLACE_HOLDER> the previously generated ui skin data .,widget . set skin ( assets . generate asset ( data @$ ui skin . class ) ) ; selected screen box . set content ( widget ) ;,skin using,success,pre
confirm that we report at least as much throttle time as our server <PLACE_HOLDER> errors for . we will actually report more due to backoff in restarting streams .,assert true ( this . client . get and reset throttle time ( ) > throttle time ) ; stream . close ( ) ; assert true ( stream . await termination ( __num__ @$ time unit . seconds ) ) ;,time wrapped,fail,pre
all other errors <PLACE_HOLDER> a bigger problem @$ so just stop the task .,if ( thread . interrupted ( ) ) { return true ; } else if ( ! response . is valid ( ) ) { return response . get error ( ) . get error code ( ) != error code . io_exception ; } else { return false ; },errors indicate,success,pre
system apps <PLACE_HOLDER> control over where their default storage context is pointed @$ so we 're always explicit when building paths .,final context ce context = context . create credential protected storage context ( ) ; files_dir = ce context . get files dir ( ) ; database_dir = ce context . get database path ( __str__ ) . get parent file ( ) ; root_dir = ce context . get data dir ( ) ; sharedpref_dir = ce context . get shared preferences path ( __str__ ) . get parent file ( ) ; cache_dir = ce context . get cache dir ( ) ; nobackup_dir = ce context . get no backup files dir ( ) ; final context de context = context . create device protected storage context ( ) ; device_files_dir = de context . get files dir ( ) ; device_database_dir = de context,apps have,success,pre
replace load method counters <PLACE_HOLDER> with resolve method and load counters <PLACE_HOLDER> @$ expose klass constants .,replace load method counters ( graph @$ state mapper @$ context ) ;,counters object,fail,pre
<PLACE_HOLDER> existent mapping should <PLACE_HOLDER> ownership mapping,cache . remove owner from stream ( stream @$ owner @$ __str__ ) ; assert null ( __str__ + owner + __str__ @$ cache . get owner ( stream ) ) ; ownership map = cache . get stream owner mapping ( ) ; assert equals ( __str__ + ( num proxies * num streams per proxy - __num__ ) + __str__ @$ num proxies * num streams per proxy - __num__ @$ ownership map . size ( ) ) ; ownership distribution = cache . get stream ownership distribution ( ) ; assert equals ( __str__ + num proxies + __str__ @$ num proxies @$ ownership distribution . size ( ) ) ; set < string > owned streams = ownership distribution . get ( owner ),mapping remove,success,pre
get the tenant <PLACE_HOLDER> folder . if the tenant <PLACE_HOLDER> folder does not exist then exit .,tenant root folder = repository file dao . get file by absolute path ( server repository paths . get tenant root folder path ( the tenant ) ) ; if ( tenant root folder != null ) { tenant home folder = repository file dao . get file by absolute path ( server repository paths . get tenant home folder path ( the tenant ) ) ; if ( tenant home folder == null ) { string owner id = user name utils . get principle id ( the tenant @$ username ) ; repository file sid owner sid = new repository file sid ( owner id @$ type . user ) ; string tenant authenticated role id = role name utils . get principle id ( the,tenant root,success,pre
send key action does n't <PLACE_HOLDER> any incoming cec command @$ hence does not consume it .,return false ;,action consume,fail,pre
actual overlap @$ just <PLACE_HOLDER> the event .,if ( last event . event equals ( cur event ) ) { if ( debug ) slog . d ( tag @$ __str__ + last nanos ) ; } else { assign log id ( cur event ) ; m pending logs . add ( cur event ) ; if ( debug ) slog . d ( tag @$ __str__ + last nanos ) ; },overlap append,fail,pre
and finally @$ find the variati<PLACE_HOLDER> @$ caused by the fact that the sun 's gravitati<PLACE_HOLDER>al pull <PLACE_HOLDER> the mo<PLACE_HOLDER> varies depending <PLACE_HOLDER> which side of the earth the mo<PLACE_HOLDER> is <PLACE_HOLDER>,double variation = __num__ * pi / __num__ * math . sin ( __num__ * ( moon longitude - sun long ) ) ; moon longitude += variation ;,moon is,fail,pre
decorate outgoing exceptions with basic tree information . this is similar to how the constructor appends its information @$ but the constructor has <PLACE_HOLDER> more information at that point so this one is a bit more sparse on information .,with message ( t @$ t . get message ( ) + __str__ + format ( __str__ @$ index file ) ) ; throw t ;,constructor added,fail,pre
0 <PLACE_HOLDER> x already set and we should not block any threads .,return get state ( ) == __num__ ? __num__ : - __num__ ;,0 means,success,pre
then draw the arc . finally @$ as the optimizer <PLACE_HOLDER> n't stroke 'd the path @$ we close and fill it @$ and mark the stroke as closed . note : the lineto to the centre of the object is required @$ because the fill only fills the arc . skipping this includes an extra chord @$ which is n't correct . peter,close block ( ) ; draw arc ( x @$ y @$ w @$ h @$ sa @$ aa ) ; lineto ( x + ( w > > __num__ ) @$ y + ( h > > __num__ ) ) ; close block ( __str__ ) ;,optimizer does,fail,pre
aapt should <PLACE_HOLDER> an error @$ but do a bit of sanity checking here just in case .,if ( old value != null && ! old value . equals ( value ) ) { integer lower = old value . or null ( ) ; integer higher = value . or null ( ) ; if ( ordering . natural ( ) . compare ( old value . or null ( ) @$ value . or null ( ) ) > __num__ ) { lower = higher ; higher = old value . or null ( ) ; } logger . warning ( string . format ( __str__ @$ type @$ name @$ lower @$ higher ) ) ; },aapt throw,fail,pre
if the receiver is not included in the contract @$ unfreeze frozen balance for this account . otherwise @$ unfreeze delegated frozen balance <PLACE_HOLDER> this account .,if ( ! array utils . is empty ( receiver address ) && dynamic store . supportdr ( ) ) { if ( arrays . equals ( receiver address @$ owner address ) ) { throw new contract validate exception ( __str__ ) ; } if ( ! decode util . address valid ( receiver address ) ) { throw new contract validate exception ( __str__ ) ; } account capsule receiver capsule = account store . get ( receiver address ) ; if ( dynamic store . get allow tvm constantinople ( ) == __num__ && receiver capsule == null ) { string readable receiver address = string util . create readable string ( receiver address ) ; throw new contract validate exception ( __str__ + readable,balance distributes,fail,pre
check if two vh <PLACE_HOLDER> the same key @$ if so @$ print that as an error,final int child count = m child helper . get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { view view = m child helper . get child at ( i ) ; view holder other = get child view holder int ( view ) ; if ( other == holder ) { continue ; } final long other key = get changed holder key ( other ) ; if ( other key == key ) { if ( m adapter != null && m adapter . has stable ids ( ) ) { throw new illegal state exception ( __str__ + __str__ + __str__ + other + __str__ + holder ) ; } else {,vh have,success,pre
the set of topics with unknown leader <PLACE_HOLDER> topics with leader election pending as well as topics which may have expired . add the topic again to metadata to ensure it is included and request metadata update @$ since there are messages to send to the topic .,if ( ! result . unknown leader topics . is empty ( ) ) { for ( string topic : result . unknown leader topics ) this . metadata . add ( topic @$ now ) ; log . debug ( __str__ @$ result . unknown leader topics ) ; this . metadata . request update ( ) ; },set contains,success,pre
language agnostic inlining <PLACE_HOLDER> this particular pattern to be present in the graph,assert . assert equals ( __num__ @$ graph . get nodes ( inline decision node . type ) . count ( ) ) ; for ( inline decision node decision node : graph . get nodes ( inline decision node . type ) ) { assert . assert equals ( __num__ @$ decision node . usages ( ) . count ( ) ) ; for ( node usage : decision node . usages ( ) ) { if ( usage instanceof integer equals node ) { final node if node = usage . usages ( ) . first ( ) ; assert . assert true ( if node instanceof if node ) ; final fixed node invoke = ( ( if node ) if node ) . false,inlining requires,fail,pre
get all records <PLACE_HOLDER> the one before the startdate,for ( long f : files ) { if ( start long <= f && f <= end long ) { result as long . add ( f ) ; } },records corresponding,fail,pre
brand new segment @$ not refresh @$ directly <PLACE_HOLDER> the segment,zn record segment metadata zn record = _pinot helix resource manager . get segment metadata zn record ( offline table name @$ segment name ) ; if ( segment metadata zn record == null ) { logger . info ( __str__ @$ segment name @$ raw table name ) ; string crypter = headers . get header string ( file upload download client . custom headers . crypter ) ; process new segment ( segment metadata @$ final segment locationuri @$ current segment location @$ zk downloaduri @$ crypter @$ raw table name @$ segment name @$ move segment to final location ) ; return ; } logger . info ( __str__ @$ segment name @$ raw table name ) ; process existing segment ( segment metadata @$,segment assign,fail,pre
let 's check <PLACE_HOLDER> meta pages .,if ( rec instanceof page snapshot ) { page snapshot snp rec = ( page snapshot ) rec ; assert false ( snapshots . contains key ( snp rec . full page id ( ) ) ) ; snapshots . put ( snp rec . full page id ( ) @$ snp rec ) ; } else if ( rec instanceof meta page update partition data record ) { meta page update partition data record meta rec = ( meta page update partition data record ) rec ; assert true ( snapshots . contains key ( meta rec . full page id ( ) ) ) ; },check added,fail,pre
only used if framework does not <PLACE_HOLDER> the object callback,callbacks [ __num__ ] = new password callback ( __str__ @$ false ) ;,framework implement,fail,pre
a spawn exec a blocks b if a <PLACE_HOLDER> an output consumed by b,multimap < spawn exec @$ spawn exec > blocked by = multimap builder . hash keys ( ) . array list values ( ) . build ( ) ; multimap < spawn exec @$ spawn exec > blocking = multimap builder . hash keys ( ) . array list values ( ) . build ( ) ; for ( spawn exec ex : inputs ) { for ( file s : ex . get inputs list ( ) ) { if ( output producer . contains key ( s . get path ( ) ) ) { spawn exec blocker = output producer . get ( s . get path ( ) ) ; blocked by . put ( ex @$ blocker ) ; blocking . put (,a has,fail,pre
produce a message to broker 1 's test queue and verify that broker 1 's memory usage <PLACE_HOLDER> increased @$ but broker 2 still <PLACE_HOLDER> no memory usage .,send messages ( __str__ @$ test queue @$ __num__ ) ; assert true ( broker1 test queue . get memory usage ( ) . get usage ( ) > __num__ ) ; assert equals ( __num__ @$ broker2 test queue . get memory usage ( ) . get usage ( ) ) ;,broker has,success,pre
animate each child <PLACE_HOLDER> and changing size to match their final locations .,array list < animator > animators = new array list < animator > ( ) ; value animator child animator = value animator . of float ( __num__ @$ __num__ ) ; animators . add ( child animator ) ; for ( int i = __num__ ; i < m layout . get child count ( ) ; i ++ ) { if ( m initial tops . get ( i ) . compare to ( final child tops . get ( i ) ) == __num__ && m initial tops . get ( i + __num__ ) . compare to ( final child tops . get ( i + __num__ ) ) == __num__ ) { continue ; } final view child = m layout . get,child adjust,fail,pre
subscriber <PLACE_HOLDER> and creates durable sub,mqtt client client = create client ( false @$ __str__ @$ listener ) ; final string account_prefix = __str__ ; client . subscribe ( account_prefix + __str__ ) ; client . subscribe ( account_prefix + __str__ ) ; client . subscribe ( account_prefix + __str__ ) ; assert true ( client . get pending delivery tokens ( ) . length == __num__ ) ; string expected result = __str__ ; client . publish ( account_prefix + __str__ @$ expected result . get bytes ( standard charsets . utf_8 ) @$ __num__ @$ false ) ;,subscriber connects,success,pre
bt configures the interface elsewhere : only <PLACE_HOLDER> dhcp .,final inet4 address srv addr = ( inet4 address ) parse numeric address ( bluetooth_iface_addr ) ; return configure dhcp ( enabled @$ srv addr @$ bluetooth_dhcp_prefix_length ) ;,configures supports,fail,pre
note : make sure the array <PLACE_HOLDER> a length of at least 1,int array size = math . max ( default_size @$ list size ) ; list = new light [ array size ] ; dist to owner = new float [ array size ] ; for ( int i = __num__ ; i < list size ; i ++ ) { list [ i ] = lights . get ( i ) ; } arrays . fill ( dist to owner @$ float . negative_infinity ) ;,array has,success,pre
let the first client <PLACE_HOLDER> all messages,msgs client1 += receive all messages ( client1 ) ; client1 . close ( ) ;,client receive,success,pre
rr label matches but app label <PLACE_HOLDER> not . rr label should be set to app label,rr . set node label expression ( enforced label1 ) ; scheduler utils . enforce partition exclusivity ( rr @$ enforced exclusive label set @$ null ) ; assert . assert null ( rr . get node label expression ( ) ) ; rr . set node label expression ( enforced label2 ) ; scheduler utils . enforce partition exclusivity ( rr @$ enforced exclusive label set @$ app label ) ; assert . assert equals ( app label @$ rr . get node label expression ( ) ) ;,label does,success,pre
set paint style @$ style.<PLACE_HOLDER> will <PLACE_HOLDER> the color @$ style.stroke will stroke the color,paint . set style ( paint . style . fill ) ; canvas . draw rect ( get dimension in pixel ( __num__ ) @$ get height ( ) - ( __num__ + random . next int ( ( int ) ( get height ( ) / __num__ ) - __num__ ) ) @$ get dimension in pixel ( __num__ ) @$ get height ( ) - __num__ @$ paint ) ; canvas . draw rect ( get dimension in pixel ( __num__ ) @$ get height ( ) - ( __num__ + random . next int ( ( int ) ( get height ( ) / __num__ ) - __num__ ) ) @$ get dimension in pixel ( __num__ ) @$ get height ( ) - __num__,style.fill stroke,fail,pre
native wrappers for methods do n't <PLACE_HOLDER> a compilation,if ( c != null ) { c . setn method ( nm ) ; },wrappers require,fail,pre
whether new am could <PLACE_HOLDER> container complete msg,allocate response allocate response = am2 . allocate ( new array list < resource request > ( ) @$ new array list < container id > ( ) ) ; list < container status > container statuses = allocate response . get completed containers statuses ( ) ; if ( is container id in container status ( container statuses @$ container id2 ) == false ) { assert . fail ( ) ; } container statuses = attempt2 . get just finished containers ( ) ; if ( is container id in container status ( container statuses @$ container id2 ) ) { assert . fail ( ) ; },am get,success,pre
first request <PLACE_HOLDER> no continuation position .,state tag state tag = new state tag ( state tag . kind . bag @$ encoded tag @$ state family ) ;,request has,success,pre
change link properties should <PLACE_HOLDER> updated tcp buffer size .,link properties lp = new link properties ( ) ; lp . set tcp buffer sizes ( test tcp buffer sizes ) ; m cell network agent . send link properties ( lp ) ; network callback . expect callback ( callback entry . link_properties_changed @$ m cell network agent ) ; verify tcp buffer size change ( test tcp buffer sizes ) ;,properties yield,fail,pre
60 fps would <PLACE_HOLDER> 16 msec sleep here .,uninterruptibles . sleep uninterruptibly ( __num__ @$ time unit . milliseconds ) ;,fps use,fail,pre
add the parceler instance which <PLACE_HOLDER> the proxy to parcel the field data,type name parceler type = parameterized type name . get ( class name . get ( package_parceler @$ __str__ ) @$ class name ) ; builder . add field ( field spec . builder ( parceler type @$ __str__ @$ modifier . static @$ modifier . final ) . initializer ( __str__ @$ parceler type @$ entity generator . type_name ) . build ( ) ) ;,which uses,success,pre
metrics <PLACE_HOLDER> action,toggle split screen mode ( - __num__ @$ - __num__ ) ;,metrics merge,fail,pre
repeatedly bounce a receiving site member which will <PLACE_HOLDER> partition offline exceptions,async invocation < integer > close open invocation = vm3 . invoke async ( ( ) -> close recreate cache ( ny port @$ region name @$ __num__ ) ) ;,which throw,fail,pre
services <PLACE_HOLDER> no distinction between actual & valid,fmt . set locale ( uloc @$ uloc ) ;,services make,success,pre
validate ip here instead of in port mapping parser to guaruntee every instance <PLACE_HOLDER> a valid ip,if ( ip != null && ! inet addresses . is inet address ( this . ip ) ) { throw new illegal argument exception ( ip + __str__ ) ; } this . internal port = internal port ; this . external port = external port ; this . protocol = optional . from nullable ( protocol ) . or ( tcp ) ;,instance has,success,pre
constructor is public @$ but class <PLACE_HOLDER> package scope,system . out . println ( __str__ ) ; system . err . println ( __str__ ) ; ex . print stack trace ( ) ;,class has,success,pre
it is a good practice to remove location requests when the activity is in a paused or stopped state . doing so <PLACE_HOLDER> battery performance and is especially recommended in applications that request frequent location updates .,m fused location client . remove location updates ( m location callback ) . add on complete listener ( this @$ new on complete listener < void > ( ) { @ override public void on complete ( @ non null task < void > task ) { m requesting location updates = false ; set buttons enabled state ( ) ; } } ) ;,state helps,success,pre
mark the view drawn to ensure that it gets <PLACE_HOLDER> properly the next time it is visible and gets <PLACE_HOLDER>,m private flags |= pflag_drawn ;,view stored,fail,pre
this simulates the completion of txnid : id txn <PLACE_HOLDER> 1,long write id = txn mgr2 . get table write id ( __str__ @$ __str__ ) ; add dynamic partitions adp = new add dynamic partitions ( txn mgr2 . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr2 . commit txn ( ) ;,txn update,success,pre
check if remainder of href <PLACE_HOLDER> any illegal characters before proceeding,if ( i < len ) { for ( int j = i ; j < len ; ++ j ) { ch = href . char at ( j ) ; if ( ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) ) { continue ; } if ( xml char . is high surrogate ( ch ) && ++ j < len ) { int ch2 = href . char at ( j ) ; if ( xml char . is low surrogate ( ch2 ) ) { ch2 = xml char . supplemental ( (,remainder contains,success,pre
the registry now <PLACE_HOLDER> native data @$ even if registration threw an exception .,s proxy map . set ( i binder @$ result ) ;,registry contains,fail,pre
handle negatives @$ which <PLACE_HOLDER> last n characters,if ( start < __num__ ) { start = str . length ( ) + start ; },which remove,fail,pre
firsts contains at least one entry @$ always contains everything else . let 's <PLACE_HOLDER> them into the stream to form a unified set of capabilities . woohoo !,from oss = firsts . stream ( ) . map ( first -> immutable map . < string @$ object > builder ( ) . put all ( always ) . put all ( first ) . build ( ) ) . map ( this :: apply transforms ) . map ( map -> map . entry set ( ) . stream ( ) . filter ( entry -> accepted_w3c_patterns . test ( entry . get key ( ) ) ) . collect ( immutable map . to immutable map ( map . entry :: get key @$ map . entry :: get value ) ) ) ; from oss = stream . of ( ) ;,'s insert,fail,pre
if one reader has not <PLACE_HOLDER> any events @$ its 'last offset ' will be null . in this case @$ it must the be the lagging reader .,if ( a offset == null ) { a reader leading = false ; } else if ( b offset == null ) { a reader leading = true ; } else { document a document = source info . create document from offset ( a offset ) ; document b document = source info . create document from offset ( b offset ) ; a reader leading = source info . is position at or before ( b document @$ a document @$ binlog readera . context . gtid source filter ( ) ) ; } if ( a reader leading ) { logger . info ( __str__ ) ; } else { logger . info ( __str__ ) ; },reader received,fail,pre
some tables do not <PLACE_HOLDER> dynamic column,if ( ! ( cm instanceof g table column model ) ) { super . create default columns from model ( ) ; return ; },tables have,fail,pre
this value must not <PLACE_HOLDER> 4 bytes,list buffer . add ( encode ( default_timecode_scale @$ true ) ) ;,value exceed,success,pre
nt proc will <PLACE_HOLDER> a proc that does n't exist,system . out . flush ( ) ; system . out . println ( __str__ ) ; try { client response impl cri = ( client response impl ) client . call procedure ( __str__ @$ run on all partitionsnt proc . call_missing_proc ) ; system . err . println ( cri . tojson string ( ) ) ; fail ( ) ; } catch ( proc call exception e ) { response = ( client response impl ) e . get client response ( ) ; assert equals ( client response . unexpected_failure @$ response . get status ( ) ) ; system . out . println ( __str__ ) ; system . out . println ( response . tojson string ( ) ) ; },proc install,fail,pre
a proper exit will <PLACE_HOLDER> this value .,this . s exit = crawl status . finished_abnormal ; if ( get pause at start ( ) ) { complete pause ( ) ; } else { get frontier ( ) . run ( ) ; },exit set,fail,pre
extra element to make sure we <PLACE_HOLDER> the same formula to compute the length of each element of the array .,start position = new int [ fields . length + __num__ ] ;,element use,fail,pre
charset encoder icu has taken ownership ; its finalizer will <PLACE_HOLDER> the free .,address = __num__ ;,finalizer do,success,pre
verify that the specified packages <PLACE_HOLDER> the provided uid .,int user id = user handle . get user id ( uid ) ; try { application info app info = mi package manager . get application info ( package name @$ __num__ @$ user id ) ; if ( app info == null ) { log . w ( log_tag @$ string . format ( __str__ @$ package name ) ) ; return false ; } else if ( uid != app info . uid ) { string message = string . format ( __str__ @$ package name @$ app info . uid @$ uid ) ; log . w ( log_tag @$ message ) ; throw new security exception ( message ) ; } } catch ( remote exception e ) { log . e (,packages have,fail,pre
not all supported andorid version <PLACE_HOLDER> this variable,if ( m selector position field == null ) { for ( int i = __num__ ; i < get child count ( ) ; i ++ ) { if ( get child at ( i ) . get bottom ( ) == m selector rect . bottom ) { return i + get fixed first visible item ( ) ; } } } else { try { return m selector position field . get int ( this ) ; } catch ( illegal argument exception e ) { e . print stack trace ( ) ; } catch ( illegal access exception e ) { e . print stack trace ( ) ; } },version define,fail,pre
while processing an admin request @$ hdfs failed lock could <PLACE_HOLDER> long time because of multiple hdfs operations @$ especially when the name node is in a different data center . so extend timeout to 5 minutes .,admin client config admin config = new admin client config ( ) . set max connections per node ( cluster . get number of nodes ( ) ) . set max backoff delay ms ( max backoff delay ms ) . set admin socket timeout sec ( __num__ * __num__ ) ;,lock take,success,pre
there is a case that can lead us here . the caller is moving the top activity that is in a task that has multiple activities to pip mode . for that the caller is <PLACE_HOLDER> a new task to host the activity so that we only move the top activity to pip mode and keep other activities in the previous task . there,if ( root == null ) { return result_skip ; },caller creating,success,pre
pointers to <PLACE_HOLDER> columns,int [ ] polyp = new int [ ilbc_constants . enh_ups0 ] ;,pointers pv,fail,pre
use random ids so that one app can not know that another app <PLACE_HOLDER> sessions,int session id ; int tries = __num__ ; do { tries ++ ; if ( tries > max_session_id_create_tries ) { slog . w ( tag @$ __str__ + max_session_id_create_tries + __str__ ) ; return null ; } session id = math . abs ( s random . next int ( ) ) ; } while ( session id == __num__ || session id == no_session || m sessions . index of key ( session id ) >= __num__ ) ; assert caller locked ( component name @$ compat mode ) ;,app has,fail,pre
check correctness of cross if udf <PLACE_HOLDER> right input object,final execution environment env = execution environment . get execution environment ( ) ; data set < tuple3 < integer @$ long @$ string > > ds = collection data sets . get small3 tuple data set ( env ) ; data set < tuple5 < integer @$ long @$ integer @$ string @$ long > > ds2 = collection data sets . get small5 tuple data set ( env ) ; data set < tuple5 < integer @$ long @$ integer @$ string @$ long > > cross ds = ds . cross ( ds2 ) . with ( new tuple5 return right ( ) ) ; list < tuple5 < integer @$ long @$ integer @$ string @$ long > > result = cross ds .,udf returns,success,pre
prefer entry <PLACE_HOLDER> exception over statistics disabled exception,check entry destroyed ( ) ; checktx ( ) ; if ( ! this . local region . statistics enabled ) { throw new statistics disabled exception ( string . format ( __str__ @$ this . local region . get full path ( ) ) ) ; },entry destroyed,success,pre
this makes the dialog <PLACE_HOLDER> up the full width,window manager . layout params lp = new window manager . layout params ( ) ; lp . copy from ( get window ( ) . get attributes ( ) ) ; lp . width = window manager . layout params . match_parent ; lp . height = window manager . layout params . wrap_content ; get window ( ) . set attributes ( lp ) ; account = new account ( get intent ( ) . get string extra ( key_account_name ) @$ get intent ( ) . get string extra ( key_account_type ) ) ; package name = get intent ( ) . get string extra ( key_android_package_name ) ; service = get intent ( ) . get string extra ( key_authtoken ) ; if (,dialog take,success,pre
snap to edge which <PLACE_HOLDER> no pillar nodes,iter = expl . set base node ( __num__ ) ; iter . next ( ) ; res = create location result ( __num__ @$ __num__ @$ iter @$ __num__ @$ edge ) ; query graph query graph6 = lookup ( res ) ; assert equals ( new gh point ( __num__ @$ __num__ ) @$ res . get snapped point ( ) ) ; assert equals ( __num__ @$ res . get closest node ( ) ) ; assert equals ( __num__ @$ get points ( query graph6 @$ __num__ @$ __num__ ) . get size ( ) ) ; assert equals ( __num__ @$ get points ( query graph6 @$ __num__ @$ __num__ ) . get size ( ) ) ;,which has,success,pre
the leak detector should <PLACE_HOLDER> no leaked queries as the query is still running,leak detector . check for memory leaks ( ( ) -> immutable list . of ( create query info ( test query . get id ( ) @$ running ) ) @$ immutable map . of ( test query @$ __num__ ) ) ; assert equals ( leak detector . get number of leaked queries ( ) @$ __num__ ) ;,detector report,success,pre
system tables <PLACE_HOLDER> no settings,execute ( __str__ ) ; for ( object [ ] row : response . rows ( ) ) { assert null ( row [ __num__ ] ) ; } execute ( __str__ ) ; for ( object [ ] row : response . rows ( ) ) { assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( (,tables have,success,pre
it would be better if the on site changed <PLACE_HOLDER> the list of changed sites .,if ( get selected site ( ) == null && m site store . has site ( ) ) { set selected site ( m site store . get sites ( ) . get ( __num__ ) ) ; } if ( get selected site ( ) == null ) { return ; } site model site = m site store . get site by local id ( get selected site ( ) . get id ( ) ) ; if ( site != null ) { m selected site = site ; } if ( get my site fragment ( ) != null ) { get my site fragment ( ) . on site changed ( site ) ; },site checked,fail,pre
how many <PLACE_HOLDER> name ?,int argnr = rep . count nr job entry attributes ( id_jobentry @$ __str__ ) ; allocate ( argnr ) ; for ( int a = __num__ ; a < argnr ; a ++ ) { header name [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; header value [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; },many header,success,pre
top horizontal non <PLACE_HOLDER> part,height = this . crop zone rect . y - this . image rect . y ; if ( height > __num__ ) { g . fill rect ( this . crop zone rect . x @$ this . image rect . y @$ this . crop zone rect . width @$ height ) ; },non croppped,success,pre
do n't let the user click restore if the words area <PLACE_HOLDER> the current wallet words @$ or are an invalid set @$ or if the date field is n't set @$ or if it 's in the future .,restore button . disable property ( ) . bind ( or ( or ( not ( validator . valid ) @$ equal ( orig words @$ words area . text property ( ) ) ) @$ date picker is invalid ) ) ;,area has,fail,pre
check that pause resume wo n't <PLACE_HOLDER> the end handler prematurely,resp . pause ( ) ; resp . resume ( ) ;,resume trigger,fail,pre
for single partition key column table @$ we can merge multiple partitions into a single split by using in clause in a single select query if the partitions <PLACE_HOLDER> the same host list . for multiple partition key columns table @$ we ca n't merge them into a single select query @$ so keep them in a separate split .,boolean single partition key column = true ; string partition key column name = null ; if ( ! partitions . is empty ( ) ) { single partition key column = partitions . get ( __num__ ) . get tuple domain ( ) . get domains ( ) . get ( ) . size ( ) == __num__ ; if ( single partition key column ) { string partition id = partitions . get ( __num__ ) . get partition id ( ) ; partition key column name = partition id . substring ( __num__ @$ partition id . last index of ( __str__ ) - __num__ ) ; } } map < set < string > @$ set < string > > hosts to partition keys,partitions have,success,pre
test case 3 set io permission granted to code that was signed by first signer set factory permission granted to code that was signed by second signer keystore that contains only first keypairs code was singed by first signer and second signer <PLACE_HOLDER> access control exception for set factory permission,system . out . println ( __str__ ) ; cmd = constructcmd ( __str__ @$ policy2 @$ __str__ @$ __str__ ) ; process tools . execute test jvm ( cmd ) . should have exit value ( __num__ ) ;,signer expect,success,pre
due how xml <PLACE_HOLDER> parser works @$ the xml is fully loaded on the ram,super ( false @$ true @$ algorithm_ttml_converter ) ;,xml pull,success,pre
if enable <PLACE_HOLDER> throttle quota @$ make sure that region server throttle quotas are in seconds time unit . because once previous requests <PLACE_HOLDER> their quota and consume region server quota @$ quota in other time units may be refilled in a long time @$ this may affect later requests .,list < pair < boolean @$ timed quota > > list = arrays . as list ( pair . new pair ( throttle . has req num ( ) @$ throttle . get req num ( ) ) @$ pair . new pair ( throttle . has read num ( ) @$ throttle . get read num ( ) ) @$ pair . new pair ( throttle . has write num ( ) @$ throttle . get write num ( ) ) @$ pair . new pair ( throttle . has req size ( ) @$ throttle . get req size ( ) ) @$ pair . new pair ( throttle . has read size ( ) @$ throttle . get read size ( ) ) @$ pair,requests hit,fail,pre
second split <PLACE_HOLDER> the file 3 and file 4 @$ however @$ the locations is undetermined .,if ( split . equals ( splits . get ( __num__ ) ) ) { assert equals ( __num__ @$ file split . get num paths ( ) ) ; expected . clear ( ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ __num__ ) ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ blocksize ) ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ blocksize * __num__ ) ) ; expected . add ( new split ( file4 . get name ( ) @$ blocksize @$ __num__ ) ) ; expected . add ( new split ( file4 . get,split covers,fail,pre
some already <PLACE_HOLDER> files,return arrays . stream ( new object [ ] [ ] { { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip,some paned,fail,pre
version of refine which <PLACE_HOLDER> the file,reader . read line ( ) ;,which reads,fail,pre
ok @$ one or both lists <PLACE_HOLDER> extra data,if ( message == null ) { message = __str__ + __str__ + __str__ + expected copy + __str__ + actual copy + __str__ ; } fail ( message ) ;,ok contain,fail,pre
function will always <PLACE_HOLDER> the first dictionary,dictionary block ineffective block = create dictionary block ( __num__ @$ __num__ ) ; test project range ( ineffective block @$ dictionary block . class @$ projection @$ force yield @$ produce lazy block ) ; test project fast return ignore yield ( ineffective block @$ projection @$ produce lazy block ) ;,function process,fail,pre
each job <PLACE_HOLDER> its own metric log subdirectory,path metrics log dir = new path ( properties . get property ( configuration keys . metrics_log_dir_key ) @$ this . get name ( ) ) ; if ( ! fs . exists ( metrics log dir ) && ! fs . mkdirs ( metrics log dir ) ) { logger . error ( __str__ + this . get name ( ) ) ; return ; },job gets,success,pre
th<PLACE_HOLDER> routine <PLACE_HOLDER> to process the plus sign in the dial string by loop through the network portion @$ post dial portion 1 @$ post dial portion 2 ... etc . if applied,do { string network dial str ; if ( use nanp ) { network dial str = extract network portion ( temp dial str ) ; } else { network dial str = extract network portion alt ( temp dial str ) ; } network dial str = process plus code ( network dial str @$ use nanp ) ; if ( ! text utils . is empty ( network dial str ) ) { if ( ret str == null ) { ret str = network dial str ; } else { ret str = ret str . concat ( network dial str ) ; } } else { rlog . e ( __str__ @$ network dial str ) ; return dial str ; } post dial,routine do,fail,pre
keys in record schema @$ string @$ boolean @$ <PLACE_HOLDER> types,object inputs [ ] [ ] = { { as map ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __str__ @$ __str__ @$ true @$ __str__ @$ byte string . copy avro string ( __str__ @$ false ) ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __str__ @$ __str__ @$ true @$ __str__ @$ byte string . copy avro string ( __str__ @$ false ) @$ __str__ @$,keys bytes,success,pre
asif : a match level of zero <PLACE_HOLDER> exact match .,if ( index expression . equals ( other expression ) && index type == other type ) { match level = get match level ( other definitions @$ index definitions @$ mapping ) ; },level means,success,pre
learn that a specific host has <PLACE_HOLDER> a specific path,access path = __str__ ;,host allowed,fail,pre
onload will <PLACE_HOLDER> appropriate size later,if ( icon instanceof image icon ) { icon . set width ( __str__ ) ; icon . set height ( __str__ ) ; },onload set,success,pre
spinner model did n't <PLACE_HOLDER> new value @$ reset,try { ( ( j formatted text field ) source ) . set value ( last value ) ; } catch ( illegal argument exception iae2 ) { },model like,success,pre
user can <PLACE_HOLDER> these projects,index for user ( user1 @$ new doc ( ) . set languages ( singleton list ( __str__ ) ) @$ new doc ( ) . set languages ( as list ( __str__ @$ __str__ ) ) ) ;,user see,success,pre
make sure the user has not <PLACE_HOLDER> an insane amount of txns .,int max txns = metastore conf . get int var ( conf @$ conf vars . txn_max_open_batch ) ; if ( num txns > max txns ) num txns = max txns ; stmt = db conn . create statement ( ) ; list < long > txn ids = open txns ( db conn @$ stmt @$ rqst ) ; log . debug ( __str__ ) ; db conn . commit ( ) ; return new open txns response ( txn ids ) ; close ( null @$ stmt @$ db conn ) ; unlock internal ( ) ;,user requested,success,pre
should never happen @$ dumpstate always <PLACE_HOLDER> the file .,if ( bugreport file == null ) { log . wtf ( tag @$ __str__ + extra_bugreport + __str__ + intent ) ; return ; },dumpstate writes,fail,pre
group <PLACE_HOLDER> no roles @$ mapping <PLACE_HOLDER> been disabled or no role mappers were found : simply return the group contents .,if ( group contents . is empty ( ) ) { for ( principal role principal : group principals ) { group contents . add ( role principal . get name ( ) ) ; } },group has,success,pre
ensure calculation type getter returns <PLACE_HOLDER> object and value,cal . set calculation type ( calculation type . islamic_umalqura ) ; object ct obj = cal . get calculation type ( ) ; if ( ct obj instanceof calculation type ) { calculation type ct = ( calculation type ) ct obj ; if ( ct != calculation type . islamic_umalqura ) { errln ( __str__ ) ; } } else { errln ( __str__ ) ; } date now = new date ( ) ; cal . set time ( now ) ; date then = cal . get time ( ) ; if ( ! now . equals ( then ) ) { errln ( __str__ ) ; } logln ( then . to string ( ) ) ; cal . add ( calendar .,returns cached,fail,pre
whether device owner <PLACE_HOLDER> camera restriction .,boolean disallow camera globally = false ; if ( is device owner ) { final active admin device owner = get device owner admin locked ( ) ; if ( device owner == null ) { return ; } user restrictions = device owner . user restrictions ; disallow camera globally = device owner . disable camera ; } else { final active admin profile owner = get profile owner admin locked ( user id ) ; user restrictions = profile owner != null ? profile owner . user restrictions : null ; },owner enforces,success,pre
if the merged anomaly 's create time is before last <PLACE_HOLDER> time @$ discard,if ( snapshot . contains key ( snapshot key ) ) { long last notify time = alert snap shot . get latest status ( snapshot @$ snapshot key ) . get last notify time ( ) ; if ( merged anomaly . get created time ( ) < last notify time ) { iterator . remove ( ) ; } },last notify,success,pre
the new one should <PLACE_HOLDER> the current @$ max and total durations that we had when we wrote it,assert equals ( __num__ @$ full . get current duration ms locked ( __num__ ) ) ; assert equals ( __num__ @$ full . get max duration ms locked ( __num__ ) ) ; assert equals ( __num__ @$ full . get total duration ms locked ( __num__ ) ) ;,one have,success,pre
there should never be more than 1 intersecting vertex . but if it happens as a fallback simply <PLACE_HOLDER> everything .,m arr temp vertices . add ( n ) ; m arr temp vertices . add all ( intersections ) ;,fallback catch,fail,pre
if frame queue is full @$ let 's drop everything . if frame queue accepts this frame @$ let 's <PLACE_HOLDER> the buffer as well .,if ( m frame queue . offer ( frame ) ) { int curr size = buffer . length ; int req size = m buffer size ; if ( curr size == req size ) { if ( m buffer mode == buffer_mode_dispatch ) { m buffer callback . on buffer available ( buffer ) ; } else { m buffer queue . offer ( buffer ) ; } } },'s offer,fail,pre
all <PLACE_HOLDER> records whose transaction id is not less than provided transaction id,if ( segment . get first tx id ( ) >= transaction id ) { final first tx id not less than selector selector = new first tx id not less than selector ( transaction id ) ; async read record from entries ( log name @$ reader @$ segment @$ executor service @$ new single entry scan context ( __num__ ) @$ selector ) . when complete ( new future event listener < log record withdlsn > ( ) { @ override public void on success ( log record withdlsn value ) { promise . complete ( optional . of ( selector . result ( ) ) ) ; } @ override public void on failure ( throwable cause ) { promise . complete exceptionally ( cause,all log,success,pre
length of row is <PLACE_HOLDER> @$ copy everything except family,if ( current . last common prefix < bytes . sizeof_short ) { int old row length with size = current . row length with size ; current buffer . get ( current . key buffer @$ current . last common prefix @$ bytes . sizeof_short - current . last common prefix ) ; current . row length with size = bytes . to short ( current . key buffer @$ __num__ ) + bytes . sizeof_short ; system . arraycopy ( current . key buffer @$ old row length with size @$ current . key buffer @$ current . row length with size @$ current . family length with size ) ; current buffer . get ( current . key buffer @$ bytes . sizeof_short @$ current,length different,success,pre
parent window can be null if the child is detached from it 's parent already @$ but someone still <PLACE_HOLDER> a reference to access it . so @$ we return the top parent value we already have instead of null .,if ( current != null ) { top parent = current ; },someone has,success,pre
instrument a logger to <PLACE_HOLDER> the processing of a remove sub advisory simulate a slow thread,slow down appender = new default test appender ( ) { @ override public void do append ( logging event logging event ) { if ( level . debug . equals ( logging event . get level ( ) ) ) { string message = logging event . get message ( ) . to string ( ) ; if ( message . starts with ( __str__ ) && message . contains ( __str__ ) ) { try { consumer demand exists . count down ( ) ; system . err . println ( __str__ + message ) ; time unit . seconds . sleep ( __num__ ) ; } catch ( exception ignored ) { } } } } } ;,logger signal,fail,pre
this user <PLACE_HOLDER> the email field and could be stored @$ but the schema is still incompatible so the entire stream is rejected,record incompatible user = new record ( incompatible ) ; incompatible user . put ( __str__ @$ __num__ ) ; incompatible user . put ( __str__ @$ __str__ ) ; incompatible user . put ( __str__ @$ __str__ ) ; test runner runner = test runners . new test runner ( store in kite dataset . class ) ; runner . set property ( store in kite dataset . kite_dataset_uri @$ dataset uri ) ; runner . assert valid ( ) ; runner . enqueue ( stream for ( incompatible user ) ) ; runner . run ( ) ; runner . assert all flow files transferred ( __str__ @$ __num__ ) ;,user has,success,pre
1 | 1 not yet visible @$ should n't immediately <PLACE_HOLDER> anything,assert equals ( __num__ @$ grid . get row count ( ) ) ; assert cell texts ( __num__ @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ } ) ; select menu path ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,1 show,fail,pre
stolen from sp scheduler . need to update the <PLACE_HOLDER> counters associated with any every partition tasks cleanup <PLACE_HOLDER> counters and collect done counters in this list for further processing .,return new long [ __num__ ] ;,counters completed,fail,pre
a list that is expanded with builder methods <PLACE_HOLDER> the added value at the end,p collection list < long > with one create = pc list . and ( create two ) ; assert that ( with one create . get all ( ) @$ contains ( bounded count @$ max read time count @$ unbounded count @$ create two ) ) ;,list put,fail,pre
and finally @$ let 's <PLACE_HOLDER> strings to be converted too,if ( t == json token . value_string ) { string text = p . get text ( ) . trim ( ) ; if ( __str__ . equals ( text ) || __str__ . equals ( text ) ) { _verify string for scalar coercion ( ctxt @$ text ) ; return boolean . true ; } if ( __str__ . equals ( text ) || __str__ . equals ( text ) ) { _verify string for scalar coercion ( ctxt @$ text ) ; return boolean . false ; } if ( text . length ( ) == __num__ ) { return ( boolean ) _coerce empty string ( ctxt @$ _primitive ) ; } if ( _has textual null ( text ) ) { return,'s allow,success,pre
start the modify procedure & & <PLACE_HOLDER> the executor,long proc id = proc exec . submit procedure ( new modify table procedure ( proc exec . get environment ( ) @$ new td ) ) ; int last step = __num__ ;,procedure kill,success,pre
copy the source files to cmroot . as the client will <PLACE_HOLDER> the source files to another location @$ we should make a copy of the files to cmroot instead of moving it .,if ( need cm recycle ) { cm . recycle ( source path @$ recycle type . copy @$ true ) ; },client copy,fail,pre
registry <PLACE_HOLDER> an empty name,return __str__ ;,registry has,success,pre
if any aggregate is distinct @$ bail out if any aggregate is the grouping id @$ bail out if any aggregate call has a filter @$ bail out if any aggregate functions do not <PLACE_HOLDER> splitting @$ bail out,final immutable bit set bottom aggregate group set = aggregate . get group set ( ) ; final list < aggregate call > top aggregate calls = new array list < > ( ) ; for ( int i = __num__ ; i < aggregate . get agg call list ( ) . size ( ) ; i ++ ) { aggregate call aggregate call = aggregate . get agg call list ( ) . get ( i ) ; if ( aggregate call . is distinct ( ) ) { return ; } if ( aggregate call . get aggregation ( ) . equals ( hive groupingid . instance ) ) { return ; } if ( aggregate call . filter arg >= __num__ ) { return,functions support,success,pre
this will block until zk comes back @$ at which point @$ shutdown will <PLACE_HOLDER>,store . shutdown ( new callback < none > ( ) { @ override public void on error ( throwable e ) { warn ( _log @$ __str__ ) ; } @ override public void on success ( none result ) { info ( _log @$ __str__ ) ; } } ) ;,point call,fail,pre
the suite made here will all be <PLACE_HOLDER> the tests from this class,multi config suite builder builder = new multi config suite builder ( testsql features suite . class ) ;,suite using,success,pre
for single click @$ we handle editing <PLACE_HOLDER> name,if ( evt . get click count ( ) == __num__ && source instanceof j list ) { if ( ( ! fc . is multi selection enabled ( ) || fc . get selected files ( ) . length <= __num__ ) && index >= __num__ && list selection model . is selected index ( index ) && get edit index ( ) == index && edit file == null ) { edit file name ( index ) ; } else { if ( index >= __num__ ) { set edit index ( index ) ; } else { reset edit index ( ) ; } } } else if ( evt . get click count ( ) == __num__ ) { reset edit index ( ),editing file,success,pre
in case of multithreaded evaluation the composite partition aware object sink adapter used by the ot ns will <PLACE_HOLDER> care of enqueueing this inseretion on the propagation queues of the different agendas,if ( partitions enabled ) { propagation entry . insert . execute ( handle @$ context @$ working memory @$ object type conf ) ; } else { working memory . add propagation ( new propagation entry . insert ( handle @$ context @$ working memory @$ object type conf ) ) ; },adapter take,success,pre
return no more than latest detail metrics <PLACE_HOLDER> items .,if ( detail metrics sz > __num__ ) { if ( detail metrics . size ( ) > detail metrics sz ) { grid bounded priority queue < grid cache query detail metrics adapter > latest metrics = new grid bounded priority queue < > ( detail metrics sz @$ qry_detail_metrics_priority_new_cmp ) ; latest metrics . add all ( detail metrics . values ( ) ) ; return latest metrics ; } return new array list < > ( detail metrics . values ( ) ) ; },metrics queue,fail,pre
we just return all dependencies in the days that fall within end ts and lookback as dependency links themselves do n't <PLACE_HOLDER> timestamps .,list < string > indices = index name formatter . format type and range ( type_dependency @$ begin millis @$ end ts ) ; if ( indices . is empty ( ) ) return call . empty list ( ) ; return search . new call ( search request . create ( indices ) @$ body converters . dependency_links ) ;,themselves contain,fail,pre
getting actions for the null item @$ which in this case <PLACE_HOLDER> the body item,for ( handler ah : action handlers ) { final action [ ] actions = ah . get actions ( null @$ this ) ; if ( actions != null ) { for ( action action : actions ) { action set . add ( action ) ; keys . add ( action mapper . key ( action ) ) ; } } },which includes,fail,pre
if its all in one read then we can just take it all @$ otherwise take only the current frame size and the next iteration <PLACE_HOLDER> a new command .,if ( current buffer != null ) { if ( current buffer . remaining ( ) >= plain . remaining ( ) ) { current buffer . put ( plain ) ; } else { byte [ ] fill = new byte [ current buffer . remaining ( ) ] ; plain . get ( fill ) ; current buffer . put ( fill ) ; } if ( current buffer . has remaining ( ) ) { return ; } else { current buffer . flip ( ) ; object command = wire format . unmarshal ( new data input stream ( new nio input stream ( current buffer ) ) ) ; do consume ( command ) ; next frame size = - __num__ ; current,size starts,success,pre
if the incoming vector to copy is random @$ then adding items from the iterator can <PLACE_HOLDER> performance dramatically if the number of elements is large as this vector tries to stay in order as items are added @$ so it 's better to sort the other vector 's elements by index and then add them to this,copy sorted random access sparse vector ( other ) ;,vector degrade,success,pre
disable the usual optimizations for ordering <PLACE_HOLDER> output by outer table only . in case of full <PLACE_HOLDER> @$ the unmatched inner table tuples get appended to the end of the <PLACE_HOLDER> 's output table thus invalidating the outer table <PLACE_HOLDER> order .,if ( m_join type == join type . full ) { m_sort direction = sort direction type . invalid ; return ; },table join,success,pre
if we explicitly <PLACE_HOLDER> the constraint with a <PLACE_HOLDER> which is both a hash <PLACE_HOLDER> and a tree <PLACE_HOLDER> @$ we always get a tree index . this is true even if the column type is hashable .,list < pair < string @$ index type > > passing = arrays . as list ( pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of,which control,fail,pre
if old table <PLACE_HOLDER> the policy @$ but the new table does not @$ then the old table should be dropped . this should be done @$ only if the old table is not in the list of tables to be bootstrapped which is a multi rename case . in case of multi rename @$ only the first rename should do the drop .,if ( ! repl utils . table included in repl scope ( within context . repl scope @$ new name ) ) { if ( old table in bootstrap list ) { return false ; } else { scenario = scenario . drop ; log . info ( __str__ + old name + __str__ + new name ) ; return true ; } },table satisfies,success,pre
if permissions <PLACE_HOLDER> a review before any of the app components can run @$ we launch the review activity and pass a pending intent to start the activity we are to launching now after the review is completed .,if ( a info != null ) { if ( m service . get package manager internal locked ( ) . is permissions review required ( a info . package name @$ user id ) ) { i intent sender target = m service . get intent sender locked ( activity manager . intent_sender_activity @$ calling package @$ calling uid @$ user id @$ null @$ null @$ __num__ @$ new intent [ ] { intent } @$ new string [ ] { resolved type } @$ pending intent . flag_cancel_current | pending intent . flag_one_shot @$ null ) ; intent new intent = new intent ( intent . action_review_permissions ) ; int flags = intent . get flags ( ) ; flags |= intent . flag_activity_exclude_from_recents ;,permissions need,success,pre
an empty query to <PLACE_HOLDER> entities .,query query = query . new builder ( ) . set limit ( int32 value . new builder ( ) . set value ( num entities ) ) . build ( ) ;,query read,success,pre
wait till the socket has <PLACE_HOLDER> the command,barrier . await ( __num__ @$ time unit . milliseconds ) ; return sent packet ;,socket received,fail,pre
if the action bar did n't <PLACE_HOLDER> an action mode @$ start the emulated window one,if ( m action mode == null ) { m action mode = start support action mode from window ( wrapped callback ) ; },bar specify,fail,pre
we can only operate on encodings map because ` fr ` could not <PLACE_HOLDER> target column at all,double global mean for target class = calculate prior mean ( encoding map ) ;,fr support,fail,pre
remove & <PLACE_HOLDER> client return envelope and insert the protocol header and service name @$ then rewrap envelope .,z frame client = msg . unwrap ( ) ; msg . add first ( service frame . duplicate ( ) ) ; msg . add first ( mdp . c_client . new frame ( ) ) ; msg . wrap ( client ) ; msg . send ( socket ) ;,& save,success,pre
we need the table and region to determine if this is from a mob region we do n't need to worry about hfilelink back references @$ because the hfilelink cleaner will <PLACE_HOLDER> them .,path family = file . get parent ( ) ; path region = family . get parent ( ) ; path table = region . get parent ( ) ; table name table name = fs utils . get table name ( table ) ; string mob region = mob_regions . get ( table name ) ; if ( mob region == null ) { string tmp = mob utils . get mob region info ( table name ) . get encoded name ( ) ; if ( tmp == null ) { log . error ( __str__ @$ table name ) ; return false ; } mob region = mob_regions . put if absent ( table name @$ tmp ) ; if ( mob region == null,cleaner remove,fail,pre
limiter should <PLACE_HOLDER> audio by one buffer @$ and there should almost no different in output v.s . input,for ( int i = __num__ ; i < n1a . length ; i ++ ) { if ( math . abs ( out1a [ i ] ) > __num__ ) throw new exception ( __str__ ) ; if ( math . abs ( out2a [ i ] ) > __num__ ) throw new exception ( __str__ ) ; } synth . close ( ) ;,limiter delay,success,pre
one property block can only <PLACE_HOLDER> at most 4 x 8 byte parts @$ one for header and 3 for coordinates,if ( coordinate . length > geometry type . get max supported dimensions ( ) ) { throw new unsupported operation exception ( __str__ + geometry type . get max supported dimensions ( ) + __str__ ) ; },block contain,fail,pre
minimal delay until the java process <PLACE_HOLDER> notices that the process is gone this will not let the test fail predictably if the process is actually in fact going away @$ but it would create frequent failures . not ideal @$ but the best we can do without severely prolonging the test,thread . sleep ( __num__ ) ;,process loop,fail,pre
at this point there should be : 1 k flow <PLACE_HOLDER> in the active queue 9@$001 flow <PLACE_HOLDER> in the swap queue 10 k flow <PLACE_HOLDER> swapped to disk,for ( int i = __num__ ; i < __num__ ; i ++ ) { final flow file record flow file = queue . poll ( exp ) ; assert not null ( flow file ) ; assert equals ( __num__ @$ queue . get queue diagnostics ( ) . get local queue partition diagnostics ( ) . get unacknowledged queue size ( ) . get object count ( ) ) ; assert equals ( __num__ @$ queue . get queue diagnostics ( ) . get local queue partition diagnostics ( ) . get unacknowledged queue size ( ) . get byte count ( ) ) ; queue . acknowledge ( collections . singleton ( flow file ) ) ; assert equals ( __num__ @$ queue . get,files file,fail,pre
something crashed . <PLACE_HOLDER> the thread .,common utils . sleep ms ( __num__ ) ;,something leak,fail,pre
has next call above <PLACE_HOLDER> everything up,return m_inner itr . next ( ) ;,call messes,fail,pre
validate <PLACE_HOLDER> extensions,for ( extension config req ext : request . get extensions ( ) ) { if ( ! extension registry . is available ( req ext . get name ( ) ) ) { throw new illegal argument exception ( __str__ + req ext . get name ( ) + __str__ ) ; } } if ( log . is debug enabled ( ) ) log . debug ( __str__ @$ websocket @$ to uri ) ; init ( ) ; web socket upgrade request ws req = new web socket upgrade request ( this @$ http client @$ request ) ; ws req . set upgrade listener ( upgrade listener ) ; return ws req . send async ( ) ;,validate required,fail,pre
if a vm bug has <PLACE_HOLDER> the referent to get freed without the reference getting cleared @$ looking it up @$ assigning it to a local and doing a gc should cause some sort of exception .,try { string s = string ref . get ( ) ; system . gc ( ) ; test object finalized = true ; } catch ( throwable t ) { error = new assertion failed error ( __str__ + t + __str__ ) ; },bug caused,success,pre
iterate through all the transitions until first millis is reached . use the name key and savings for whatever rule <PLACE_HOLDER> the limit .,long millis = long . min_value ; int save millis = __num__ ; transition first = null ; transition next ; while ( ( next = next transition ( millis @$ save millis ) ) != null ) { millis = next . get millis ( ) ; if ( millis == first millis ) { first = new transition ( first millis @$ next ) ; break ; } if ( millis > first millis ) { if ( first == null ) { for ( rule rule : copy ) { if ( rule . get save millis ( ) == __num__ ) { first = new transition ( first millis @$ rule @$ i standard offset ) ; break ; } } } if (,key hit,fail,pre
execute binding . <PLACE_HOLDER> listeners that the object is now unlocked,fire property change ( __str__ @$ false @$ true ) ;,binding notify,success,pre
see if our node <PLACE_HOLDER> the given key declaration according to the match attribute on xsl : key .,x path match expr = kd . get match ( ) ; double score = match expr . get match score ( xctxt @$ test node ) ; if ( score == kd . get match ( ) . match_score_none ) continue ; return dtm iterator . filter_accept ;,node matches,success,pre
the device is <PLACE_HOLDER> power from the wireless charger . update the rest position asynchronously .,if ( is powered && plug type == battery manager . battery_plugged_wireless ) { m powered wirelessly = true ; m must update rest position = true ; start detection locked ( ) ; } else { m powered wirelessly = false ; if ( m at rest ) { if ( plug type != __num__ && plug type != battery manager . battery_plugged_wireless ) { m must update rest position = false ; clear at rest locked ( ) ; } else { start detection locked ( ) ; } } },device reporting,fail,pre
logger tree should not <PLACE_HOLDER> serialization,assert true ( __str__ + sizea + __str__ + sizeb @$ ( sizea - sizeb ) < __num__ ) ;,tree contain,fail,pre
exception is thrown if the context is a receiver restricted context object . as receiver restricted context is not public @$ the context type can not be checked before calling register receiver . the most likely scenario in which the exception would be thrown would be when a broadcast receiver <PLACE_HOLDER> a dialog to show the user .,m screen off receiver = null ;,receiver triggers,fail,pre
if the dto does n't <PLACE_HOLDER> an entry for the metric @$ add with a value of 0 .,final map < string @$ long > dto metrics = snapshot dto . get status metrics ( ) ; final string field = descriptor . get field ( ) ; if ( ! dto metrics . contains key ( field ) ) { dto metrics . put ( field @$ __num__ ) ; },dto have,success,pre
enter the <PLACE_HOLDER> state .,final completable future < string > start future = start stop . start ( true ) ; assert that ( start future . join ( ) ) . is equal to ( __str__ ) ;,the started,success,pre
this will disable stack <PLACE_HOLDER> lookup inside jul . if someone wants location info @$ they can use their own formatter or use a different logging framework like sl 4 j @$ or log 4 j,record . set source class name ( null ) ; record . set parameters ( params ) ; logger . log ( record ) ;,stack trace,success,pre
read the <PLACE_HOLDER> modifiers .,while ( true ) { read next word ( __str__ + configuration constants . class_keyword + __str__ + java constants . acc_interface + __str__ + java constants . acc_enum + __str__ @$ false @$ true ) ; if ( ! configuration constants . argument_separator_keyword . equals ( next word ) ) { break ; } read next word ( __str__ + configuration constants . allow_shrinking_suboption + __str__ + configuration constants . allow_optimization_suboption + __str__ + configuration constants . allow_obfuscation_suboption + __str__ ) ; if ( configuration constants . include_descriptor_classes_suboption . starts with ( next word ) ) { mark descriptor classes = true ; } else if ( configuration constants . allow_shrinking_suboption . starts with ( next word ) ) { allow shrinking = true ; } else,the declared,fail,pre
check that method under test <PLACE_HOLDER> npe,try { epki . get key spec ( ( key ) null @$ ( provider ) null ) ; fail ( get name ( ) + __str__ ) ; } catch ( null pointer exception ok ) { },method throws,success,pre
the process will <PLACE_HOLDER> an escalation event @$ which is caught and escalated by a user org.flowable.task.service.task,assert equals ( __num__ @$ task service . create task query ( ) . task definition key ( __str__ ) . count ( ) ) ; task task = task service . create task query ( ) . single result ( ) ; assert equals ( __str__ @$ task . get name ( ) ) ;,process throw,success,pre
so let 's <PLACE_HOLDER> thge block,this . start block ( ) ; try { return get result set ( ) ; } catch ( sql exception e ) { throw new dbc exception ( e @$ connection . get data source ( ) ) ; } finally { this . end block ( ) ; },'s start,success,pre
two labels : positive or negative because we are dealing with reviews of different lengths and only one output at the final time step : use padding arrays mask arrays <PLACE_HOLDER> 1 if data is present at that time step for that example @$ or 0 if data is just padding,ind array features mask = nd4j . zeros ( reviews . size ( ) @$ max length ) ; ind array labels mask = nd4j . zeros ( reviews . size ( ) @$ max length ) ; int [ ] temp = new int [ __num__ ] ; for ( int i = __num__ ; i < reviews . size ( ) ; i ++ ) { list < string > tokens = all tokens . get ( i ) ; temp [ __num__ ] = i ; for ( int j = __num__ ; j < tokens . size ( ) && j < max length ; j ++ ) { string token = tokens . get ( j ) ; ind array vector = word,arrays contain,success,pre
if cluster management service is enabled and user did not specify a member id @$ then we will find the applicable members based <PLACE_HOLDER> the what group this regi<PLACE_HOLDER> is <PLACE_HOLDER>,if ( cc service != null && member name orid == null ) { region name = get valid region name ( region path @$ cms ) ; set < string > calculated groups = get groups containing region ( cms @$ region name ) ; if ( calculated groups . is empty ( ) ) { return result model . create error ( __str__ + region name + __str__ ) ; } if ( groups != null && ! calculated groups . contains all ( arrays . as list ( groups ) ) ) { return result model . create error ( __str__ + region name + __str__ ) ; } if ( groups == null ) { groups = calculated groups . stream ( ) .,region included,fail,pre
the bounds should <PLACE_HOLDER> the defined app width and height,assert equals ( info . app width @$ app bounds . width ( ) ) ; assert equals ( info . app height @$ app bounds . height ( ) ) ;,bounds keep,fail,pre
we can not determine which method is the most <PLACE_HOLDER> because one parameter of the first candidate was more <PLACE_HOLDER> and another parameter of the second candidate was more <PLACE_HOLDER> .,if ( best match != null && ! potential match . equals ( best match ) ) { return null ; } else { best match = potential match ; },parameter specific,success,pre
the only way we can find out if we need to quote the properties is by <PLACE_HOLDER> an object name that we 've constructed .,if ( object name . is domain pattern ( ) ) { domain = object name . quote ( domain ) ; } if ( object name . is property value pattern ( __str__ ) ) { properties . put ( __str__ @$ object name . quote ( name ) ) ; } if ( object name . is property value pattern ( __str__ ) ) { properties . put ( __str__ @$ object name . quote ( type ) ) ; } object name = new object name ( domain @$ properties ) ; return object name ;,way creating,fail,pre
we assume that only a switch to available and chat <PLACE_HOLDER> user activity since other mode changes could be also a result of some sort of automatism,reset idle time ( ) ; break ; default : break ;,switch covers,fail,pre
this one will <PLACE_HOLDER> buffer .,wseg = buf . offer ( size ) ; bbuf = wseg . buffer ( ) ; bbuf . put long ( __num__ ) ; wseg . release ( ) ;,one free,fail,pre
since the source ca n't be split @$ the first subtask index will <PLACE_HOLDER> everything,if ( should have readers ) { mockito . when ( mock . get index of this subtask ( ) ) . then return ( __num__ ) ; } else { mockito . when ( mock . get index of this subtask ( ) ) . then return ( parallelism - __num__ ) ; },index read,success,pre
even if ' v ' is a dom node @$ it always derive from object @$ so the <PLACE_HOLDER> bean info returns bean info for object,if ( bi . jaxb type == object . class && dom handler != null ) w . write dom ( v @$ dom handler @$ o @$ field name ) ; else bi . serialize root ( v @$ w ) ;,info get,success,pre
verify that thread identifier does not <PLACE_HOLDER> itself as data is lying,assert not null ( __str__ @$ events map . get ( thread id ) ) ;,identifier exist,fail,pre
register an error handler which <PLACE_HOLDER> validation errors,if ( is validating ( ) ) { result . set error handler ( new default handler ( ) { @ override public void error ( sax parse exception ex ) throws sax exception { throw ex ; } } ) ; },which handles,fail,pre
in each case get <PLACE_HOLDER> link status returns the same <PLACE_HOLDER> status as get <PLACE_HOLDER> status since we 're not calling it on a link and <PLACE_HOLDER> status objects are compared by path .,assert equals ( wrapper . get file status ( file ) @$ wrapper . get file link status ( file ) ) ; assert equals ( wrapper . get file status ( file via link ) @$ wrapper . get file link status ( file via link ) ) ; assert equals ( wrapper . get file status ( file via link ) @$ wrapper . get file link status ( file ) ) ;,status file,success,pre
stopship <PLACE_HOLDER> proper check in split user mode,return code_ok ; if ( ! m injector . user manager is split system user ( ) ) { if ( device owner user id != user handle . user_system ) { return code_not_system_user ; } if ( has user setup completed ( user handle . user_system ) ) { return code_user_setup_completed ; } } else { },stopship do,success,pre
entry <PLACE_HOLDER> no digest,if ( attributes == null ) { return null ; } array list < certificate [ ] > cert chains = new array list < certificate [ ] > ( ) ; iterator < map . entry < string @$ hash map < string @$ attributes > > > it = signatures . entry set ( ) . iterator ( ) ; while ( it . has next ( ) ) { map . entry < string @$ hash map < string @$ attributes > > entry = it . next ( ) ; hash map < string @$ attributes > hm = entry . get value ( ) ; if ( hm . get ( name ) != null ) { string signature file = entry .,entry has,success,pre
superclass does not <PLACE_HOLDER> types,return sample result . class . equals ( arg0 ) ;,superclass use,success,pre
the parsed deployment <PLACE_HOLDER> the deployment @$ the process definitions @$ and the bpmn resource @$ parse @$ and model associated with each process definition .,parsed deployment parsed deployment = parsed deployment builder factory . get builder for deployment and settings ( deployment @$ deployment settings ) . build ( ) ; bpmn deployment helper . verify process definitions do not share keys ( parsed deployment . get all process definitions ( ) ) ; bpmn deployment helper . copy deployment values to process definitions ( parsed deployment . get deployment ( ) @$ parsed deployment . get all process definitions ( ) ) ; bpmn deployment helper . set resource names on process definitions ( parsed deployment ) ;,deployment represents,success,pre
mark all inferred concepts that are required for the insert for persistence explicitly can <PLACE_HOLDER> this potentially expensive check if there are n't any inferred concepts to start with,stream < concept map > explicitly persisted = inserted . peek ( concept map -> { if ( transaction cache . get inferred instances ( ) . find any ( ) . is present ( ) ) { mark concepts for persistence ( concept map . concepts ( ) ) ; } } ) ;,concepts do,fail,pre
doubling from the expected size will <PLACE_HOLDER> exactly one resize @$ except near minimum capacity .,if ( size > __num__ ) { grow using put all ( m @$ size ) ; assert capacity ( m @$ __num__ * initial capacity ) ; },doubling cause,success,pre
on windows @$ java must keep the environment sorted . order is random on unix @$ so this test <PLACE_HOLDER> the sort .,if ( ! windows . is ( ) ) output = sort by lines windowsly ( output ) ; equal ( output @$ expected ) ;,test preserves,fail,pre
note that we ca n't use 'instanceof ' because this class <PLACE_HOLDER> a subclass .,if ( obj . get class ( ) != different real path file value with stored chain . class ) { return false ; } different real path file value with stored chain other = ( different real path file value with stored chain ) obj ; return real rooted path . equals ( other . real rooted path ) && real file state value . equals ( other . real file state value ) && logical chain during resolution . equals ( other . logical chain during resolution ) ;,note has,success,pre
purge on empty with reset <PLACE_HOLDER> stats .,assert equals ( __num__ @$ queue view . get queue size ( ) ) ; assert equals ( __num__ @$ queue view . get enqueue count ( ) ) ; assert equals ( __num__ @$ queue view . get dequeue count ( ) ) ; produce ( __num__ ) ; consume ( __num__ ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ; assert equals ( __num__ @$ queue view . get enqueue count ( ) ) ; assert equals ( __num__ @$ queue view . get dequeue count ( ) ) ; execute purge ( __str__ + get destination name ( ) ) ;,purge queue,fail,pre
interrupted exception can not be thrown by runnable.run @$ so we must wrap it . interrupts can be caught by both the evaluator and the abstract queue visitor . the former will <PLACE_HOLDER> the ie and propagate it as is ; the latter will throw a new ie .,throw scheduler exception . of interruption ( ie @$ sky key ) ;,former observe,fail,pre
restart so that new config <PLACE_HOLDER> place,after class ( ) ; before class ( ) ; final table name table name = table name . value of ( name . get method name ( ) ) ; try ( admin admin = connection . get admin ( ) ; region locator rl = connection . get region locator ( table name ) ) { util . create table ( table name @$ __str__ ) ; h region location loc = rl . get all region locations ( ) . get ( __num__ ) ; region info parent = loc . get region ( ) ; long rid = __num__ ; byte [ ] split key = bytes . to bytes ( __str__ ) ; region info splita = region info builder . new builder,config takes,success,pre
let 's give a namespace to our application window . this way @$ if someone <PLACE_HOLDER> the same theme for different applications @$ we do n't get unwanted style conflicts .,listing . set column alignment ( __str__ @$ table . align_right ) ;,someone resets,fail,pre
use jni <PLACE_HOLDER> address as id,write objectid ( get address value ( handle addr ) ) ;,jni handle,success,pre
only the owning user can <PLACE_HOLDER> the setting .,if ( owning user id != calling user id ) { return false ; },user change,success,pre
this method exists because heap is the place clients should <PLACE_HOLDER> this question @$ and to aggregate all the reasons allocation might be disallowed .,return no allocation verifier . is active ( ) || gc impl . collection in progress . get state ( ) ;,clients accelerate,fail,pre
os gi class loaders must <PLACE_HOLDER> bundle reference,final class loader class loader = info . get class loader ( ) ; settings . put ( org . hibernate . cfg . available settings . scanner @$ new osgi scanner ( ( ( bundle reference ) class loader ) . get bundle ( ) ) ) ; osgi class loader . add class loader ( class loader ) ; final class loader prevcl = thread . current thread ( ) . get context class loader ( ) ; try { thread . current thread ( ) . set context class loader ( class loader ) ; return bootstrap . get entity manager factory builder ( info @$ settings @$ new os gi class loader service impl ( osgi class loader @$ osgi service util ) ),loaders follow,fail,pre
toolbar buttons <PLACE_HOLDER> a null event to this method . tool bar or function compare panel .,if ( is toolbar button action || function comparison panel . is ancestor of ( source component ) ) { listing code comparison panel dual listing panel = function comparison panel . get dual listing panel ( ) ; boolean is showing dual listing = ( dual listing panel != null ) && dual listing panel . is visible ( ) ; boolean source isa dual field panel = is showing dual listing && dual listing panel . is ancestor of ( source component ) && ( source component instanceof field panel ) ; listing panel listing panel = null ; if ( source isa dual field panel ) { listing panel = dual listing panel . get listing panel ( ( field panel ) source component ),buttons pass,success,pre
now turn back on and get data . here we also get the previously <PLACE_HOLDER> byte of urgent data as it is still in the urgent buffer,received string = new string ( my bytes @$ __num__ @$ total bytes read ) ;,previously sent,success,pre
at this point we are ready to add another rowset to 'this ' object check the size of vec <PLACE_HOLDER> type and vec row sets in <PLACE_HOLDER>,vec row sets injoin . add ( c rowset ) ;,size file,fail,pre
update already <PLACE_HOLDER> array .,synchronized ( this ) { if ( observed attribute != null && observed attribute . equals ( attribute ) ) return ; observed attribute = attribute ; cleanup is complex type attribute ( ) ; int index = __num__ ; for ( observed object o : observed objects ) { reset already notified ( o @$ index ++ @$ observed_attribute_error_notified | observed_attribute_type_error_notified ) ; } },update notified,success,pre
the content provider does not <PLACE_HOLDER> canonical uris so we backup the default,if ( canonical sound == null ) { return settings . system . default_notification_uri ; },provider support,success,pre
entities now <PLACE_HOLDER> shorted names if aliased,assert equals ( __str__ @$ custom out ) ;,entities have,fail,pre
fetch the multi auto <PLACE_HOLDER> text view and set an adapter and tokenizer,multi auto complete text view mactv = find view by id ( r . id . widgets_multiautocompletetextview ) ; mactv . set tokenizer ( new multi auto complete text view . comma tokenizer ( ) ) ; mactv . set adapter ( new array adapter < > ( this @$ android . r . layout . simple_dropdown_item_1line @$ cheeses . s cheese strings ) ) ;,auto completed,fail,pre
order matters @$ as the cluster options <PLACE_HOLDER> the event bus options .,if ( allow clustering ) { set event bus options ( conf @$ options ) ; initialize cluster options ( conf @$ options ) ; },options override,fail,pre
no restrictions to default <PLACE_HOLDER> or vr 2 d <PLACE_HOLDER> .,if ( display id == default_display || ( display id != invalid_display && display id == m service . m vr2d display id ) ) { return true ; },restrictions display,success,pre
credentials were verified . verify that the principal has all <PLACE_HOLDER> rules for authentication,if ( ! has allowed authentication rules ( info . get principals ( ) @$ ldap context factory ) ) { throw new naming exception ( __str__ ) ; } return info ;,principal allowed,success,pre
no name @$ kids <PLACE_HOLDER> value,break ;,name have,fail,pre
setters also <PLACE_HOLDER> unknown enum value descriptor .,builder . set field ( optional nested enum field @$ unknown6543 ) ; builder . set repeated field ( repeated nested enum field @$ __num__ @$ unknown4321 ) ; builder . set repeated field ( packed nested enum field @$ __num__ @$ unknown5432 ) ; message = builder . build ( ) ;,setters have,fail,pre
have the factory <PLACE_HOLDER> the object,final object object = this . expression object factory . build object ( this . context @$ name ) ;,factory build,success,pre
request to allocate two <PLACE_HOLDER> priority containers,final string [ ] locations = new string [ ] { host } ; allocator . send request ( create request ( job id @$ __num__ @$ resource . new instance ( __num__ @$ __num__ ) @$ locations @$ false @$ true ) ) ; allocator . schedule all reduces ( ) ; allocator . make remote request ( ) ; nm . node heartbeat ( true ) ; rm . drain events ( ) ; allocator . send request ( create request ( job id @$ __num__ @$ resource . new instance ( __num__ @$ __num__ ) @$ locations @$ false @$ false ) ) ; int assigned container ; for ( assigned container = __num__ ; assigned container < __num__ ; ) { assigned container +=,two distributed,fail,pre
step 3 : determine final launch bounds based on resolved windowing mode and activity requested orientation . we set bounds to empty for fullscreen mode and keep bounds as is for all other windowing modes that 's not freeform mode . one can <PLACE_HOLDER> comments in relevant methods to further understand this step . we skip making adjustments if the params are fully resolved,final int resolved mode = ( launch mode != windowing_mode_undefined ) ? launch mode : display . get windowing mode ( ) ; if ( fully resolved current param ) { if ( resolved mode == windowing_mode_freeform ) { if ( current params . m preferred display id != display id ) { adjust bounds to fit in display ( display @$ out params . m bounds ) ; } adjust bounds to avoid conflict in display ( display @$ out params . m bounds ) ; } } else { if ( source != null && source . in freeform windowing mode ( ) && resolved mode == windowing_mode_freeform && out params . m bounds . is empty ( ) && source . get display id (,one follow,fail,pre
variable column table models <PLACE_HOLDER> default columns and 'found ' columns . we only want to create a key based upon the default columns,return variable table model . get default column count ( ) ;,models have,success,pre
all four static methods must <PLACE_HOLDER> distinct names @$ so that moo.a resolves correctly to foo.a,test ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,methods have,fail,pre
if the object is externalizable @$ call write external . else <PLACE_HOLDER> serializable processing .,if ( current class desc . is externalizable ( ) ) { orb stream . write_octet ( stream format version ) ; externalizable ext = ( externalizable ) obj ; ext . write external ( this ) ; } else { if ( current class desc . for class ( ) . get name ( ) . equals ( __str__ ) ) { this . writeutf ( ( string ) obj ) ; return ; } int stack mark = class desc stack . size ( ) ; try { object stream class next ; while ( ( next = current class desc . get superclass ( ) ) != null ) { class desc stack . push ( current class desc ) ; current class desc =,external do,success,pre
after the initial data encryption key expires @$ key manager should <PLACE_HOLDER> a valid data encryption key using the current block key .,final data encryption key dek after expiration = key manager . new data encryption key ( ) ; assert not equals ( __str__ @$ dek @$ dek after expiration ) ; assert true ( __str__ @$ dek after expiration . expiry date > fake timer . now ( ) ) ;,manager create,fail,pre
start the delete procedure & & <PLACE_HOLDER> the executor,long proc id = proc exec . submit procedure ( new delete table procedure ( proc exec . get environment ( ) @$ table name ) ) ;,procedure kill,success,pre
only one <PLACE_HOLDER> tx at a time @$ but there is a chance two <PLACE_HOLDER>rs race after commit : thus synchronize,synchronized ( tx commit count lock ) { commit count ++ ; if ( debug tx write ) { system . out . println ( __str__ + commit count + __str__ + ( entity type ids affected != null ? entity type ids affected . length : __num__ ) ) ; } },one write,success,pre
the project manager <PLACE_HOLDER> the order of the projects with the most recent being first in the list,for ( url project view : recent views ) { string url path = ghidraurl . get display string ( project view ) ; docking action action = new recent view plugin action ( url path ) ; reopen views list . add ( new view info ( action @$ project view ) ) ; tool . add action ( action ) ; },manager maintains,success,pre
reading the merged bag <PLACE_HOLDER> both the contents,assert that ( bag1 . read ( ) @$ contains in any order ( __str__ @$ __str__ @$ __str__ ) ) ; assert that ( bag2 . read ( ) @$ matchers . empty iterable ( ) ) ;,bag gets,success,pre
verify the successful flow file <PLACE_HOLDER> the expected attributes,final mock flow file mock flow file = test runner . get flow files for relationship ( putorc . rel_success ) . get ( __num__ ) ; mock flow file . assert attribute equals ( putorc . absolute_hdfs_path_attribute @$ orc file . get parent ( ) . to string ( ) ) ; mock flow file . assert attribute equals ( core attributes . filename . key ( ) @$ filename ) ; mock flow file . assert attribute equals ( putorc . record_count_attr @$ __str__ ) ;,file has,success,pre
broken glass fish @$ which call modify handshake before <PLACE_HOLDER> endpoint instance !,if ( end point . get ( ) == null ) { h request . set ( request ) ; } else { end point . get ( ) . handshake request ( request ) ; end point . set ( null ) ; },fish set,fail,pre
dns ur is never <PLACE_HOLDER> a dns precondition,if ( curi . getuuri ( ) . get scheme ( ) . equals ( __str__ ) ) { curi . set prerequisite ( true ) ; return false ; } else if ( curi . getuuri ( ) . get scheme ( ) . equals ( __str__ ) ) { return false ; },ur please,fail,pre
check that start captive portal app <PLACE_HOLDER> the expected command to network monitor .,m cm . start captive portal app ( wifi network ) ; wait for idle ( ) ; verify ( m wi fi network agent . m network monitor ) . launch captive portal app ( ) ;,check reports,fail,pre
db must <PLACE_HOLDER> indices . older versions of anki droid did n't create them for new collections .,fix integrity progress ( progress callback @$ current task ++ @$ total tasks ) ; int ixs = m db . query scalar ( __str__ ) ; if ( ixs < __num__ ) { problems . add ( __str__ ) ; storage . add indices ( m db ) ; } m db . get database ( ) . end transaction ( ) ;,db have,success,pre
schedule <PLACE_HOLDER> node killing if graceful stopping will be not finished within timeout .,executor . schedule ( new runnable ( ) { @ override public void run ( ) { if ( state ( name ) == ignite state . started ) { u . error ( null @$ __str__ + timeout ms + __str__ ) ; runtime . get runtime ( ) . halt ( ignition . kill_exit_code ) ; } } } @$ timeout ms @$ time unit . milliseconds ) ;,schedule started,fail,pre
close the fd via the parent stream 's close method . the parent will reinvoke our close method @$ which is defined in the superclass abstract interruptible channel @$ but the is open logic in that method will <PLACE_HOLDER> this method from being reinvoked .,if ( parent != null ) { ( ( java . io . closeable ) parent ) . close ( ) ; } else { nd . close ( fd ) ; },method prevent,success,pre
every one of our providers will <PLACE_HOLDER> this method @$ so only execute the logic once .,if ( initialization state != initialization state . uninitialized ) { return initialization state != initialization state . has_errors ; },one call,success,pre
need to talk about <PLACE_HOLDER> all design .,inc tag list . set prototype cell value ( __str__ ) ; inc tag scroller = new j scroll pane ( inc tag list ) ; inc tag scroller . set horizontal scroll bar policy ( javax . swing . j scroll pane . horizontal_scrollbar_as_needed ) ; inc tag scroller . set vertical scroll bar policy ( javax . swing . j scroll pane . vertical_scrollbar_as_needed ) ;,need stripping,fail,pre
response <PLACE_HOLDER> type,m ams . add account as user ( m mock account manager response @$ null @$ __str__ @$ null @$ true @$ null @$ user handle . user_system ) ;,response account,success,pre
there is one adjp unary rewrite to ad but otherwise all <PLACE_HOLDER> jj or adjp,non terminal info . put ( __str__ @$ new string [ ] [ ] { { right @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } ) ;,all have,success,pre
app 1 <PLACE_HOLDER> all resource in default partition,assert . assert equals ( __num__ @$ scheduler node2 . get num containers ( ) ) ;,app gets,success,pre
let explicit command line <PLACE_HOLDER> the environment .,m_port = ports . next client ( ) ; m_admin port = ports . next admin ( ) ; m_internal port = ports . next ( ) ; m_zk interface = __str__ + ports . next ( ) ;,line define,fail,pre
upadate container resource should n't be called before start container @$ otherwise @$ node manager can not <PLACE_HOLDER> the container,try { client . update container resource ( container ) ; fail ( __str__ ) ; } catch ( yarn exception e ) { assert true ( __str__ @$ e . get message ( ) . contains ( __str__ ) ) ; },resource find,success,pre
this should be a relatively uncommon case @$ window for this happening is a few milliseconds when an admin explicitly <PLACE_HOLDER> a constraint @$ after the index has been populated . we can improve this later on by replicating the constraint validation logic down here @$ or rethinking where we validate constraints . for now @$ we just kill these transactions .,throw new transaction failure exception ( status . transaction . constraints changed @$ __str__ + __str__ + __str__ @$ latest constraint introducing tx @$ last committed tx when transaction started ) ;,admin introduces,fail,pre
else : no new winner @$ ergo no reason to <PLACE_HOLDER> message to neighbors,messenger . send message ( incident message scope @$ shortest distance seen on this iteration ) ;,reason send,success,pre
style rules should always <PLACE_HOLDER> the lowest priority .,return - __num__ ;,rules have,success,pre
if the stream is sorted then it should also be ordered so the following will also <PLACE_HOLDER> the sort order,return new reference pipeline . stateful op < t @$ t > ( upstream @$ stream shape . reference @$ stream op flag . is_distinct | stream op flag . not_sized ) { < p_in > node < t > reduce ( pipeline helper < t > helper @$ spliterator < p_in > spliterator ) { terminal op < t @$ linked hash set < t > > reduce op = reduce ops . < t @$ linked hash set < t > > make ref ( linked hash set :: new @$ linked hash set :: add @$ linked hash set :: add all ) ; return nodes . node ( reduce op . evaluate parallel ( helper @$ spliterator ) ) ; } @ override public,following preserve,success,pre
write 1000 lines and then save @$ this will reduce disk writing frequency and at the same time will <PLACE_HOLDER> buffer under control,if ( ( line count % __num__ ) == __num__ ) { output . flush ( ) ; },time leave,fail,pre
this overload <PLACE_HOLDER> no uncompression semantics,w . write object ( __num__ @$ value ) ;,overload has,success,pre
invocation in the code under test <PLACE_HOLDER> different argument and should fail immediately this helps with debugging and is essential for mockito strictness,production code . simple method ( mock @$ __num__ ) ;,invocation takes,fail,pre
did n't get m <PLACE_HOLDER> entries field @$ bail out ...,if ( s themed resource cache_m unthemed entries field == null ) { return ; },m unwrapped,fail,pre
add an <PLACE_HOLDER> clause to make sure that the item does n't already exist @$ since it 's supposed to be new,if ( get local save behavior ( ) != save behavior . clobber && ! internal expected value assertions . contains key ( field . name ( ) ) && field . get generate strategy ( ) != dynamodb auto generate strategy . always ) { internal expected value assertions . put ( field . name ( ) @$ new expected attribute value ( ) . with exists ( false ) ) ; },an create,fail,pre
check system properties for the default value then <PLACE_HOLDER> secure settings value @$ if any .,string default delay = system properties . get ( __str__ + settings . global . pac_change_delay @$ default_delays ) ; string val = settings . global . get string ( cr @$ settings . global . pac_change_delay ) ; return ( val == null ) ? default delay : val ;,properties use,success,pre
the user asked for stats to be collected . some stats like number of rows <PLACE_HOLDER> a scan of the data however @$ some other stats @$ like number of files @$ do not <PLACE_HOLDER> a complete scan update the stats which do not <PLACE_HOLDER> a complete scan .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },which require,success,pre
if relationship label is null @$ then legacy label is mentioned at both ends @$ <PLACE_HOLDER> the respective end 's legacy label as relationship label,if ( relationship label == null ) { relationship label = get legacy edge label ( entity type @$ attr name ) ; } if ( attribute == null ) { cardinality cardinality = end def . get cardinality ( ) ; boolean is optional = true ; atlas constraint def constraint = null ; if ( cardinality == cardinality . set ) { attr type name = atlas base type def . get array type name ( attr type name ) ; } if ( relationship def . get relationship category ( ) == relationship category . composition ) { if ( end def . get is container ( ) ) { constraint = new atlas constraint def ( constraint_type_owned_ref ) ; } else { is optional,label use,success,pre
event listener registry <PLACE_HOLDER> 3 ways to register listeners :,event listener registry . add duplication strategy ( new custom duplication strategy ( ) ) ;,listener has,fail,pre
when opearting with groups . the group must <PLACE_HOLDER> entries so changes to take effect . otherwise group will be lost after loggingout,try { this . op set pers presence1 . subscribe ( group @$ this . fixture . userid2 ) ; synchronized ( o ) { o . wait ( __num__ ) ; } } catch ( exception ex ) { fail ( __str__ + group . get group name ( ) + __str__ + ex . get message ( ) ) ; },group have,success,pre
last file should <PLACE_HOLDER> 1 record,mff = runner . get flow files for relationship ( select hiveql . rel_success ) . get ( __num__ ) ; in = new byte array input stream ( mff . to byte array ( ) ) ; assert equals ( __num__ @$ get number of records from stream ( in ) ) ; mff . assert attribute exists ( __str__ ) ; assert equals ( integer . to string ( __num__ ) @$ mff . get attribute ( __str__ ) ) ; assert equals ( __str__ @$ mff . get attribute ( __str__ ) ) ; runner . clear transfer state ( ) ;,file have,success,pre
filter does not change the input ordering . filter rel does not <PLACE_HOLDER> the input . all cor vars produced by filter will have the same output positions in the input rel .,return register ( rel @$ rel builder . build ( ) @$ frame . old to new outputs @$ frame . cor def outputs ) ;,rel permute,success,pre
we return the first <PLACE_HOLDER> location .,if ( paths . size ( ) > __num__ ) { final_symlink = paths . get ( __num__ ) + symlink [ symlink . length - __num__ ] ; } else { final_symlink = symlink [ symlink . length - __num__ ] ; },first found,success,pre
note : this test <PLACE_HOLDER> the fact that limit spec sorts numbers like strings ; we might want to change this in the future .,assert . assert equals ( immutable list . of ( test rows list . get ( __num__ ) @$ test rows list . get ( __num__ ) ) @$ limit fn . apply ( sequences . simple ( test rows list ) ) . to list ( ) ) ;,test honor,fail,pre
this estimate will not <PLACE_HOLDER> into account the memory saved by inlining the keys .,return versioned stats disklru region entry off heap object key . class ;,estimate take,success,pre
it does not sound like it makes a lot of sense to fail the getting of the presence status of the specified signin name just because one of the possibly many operation set presence instances has failed . additionally @$ the native counterpart will <PLACE_HOLDER> any java exception anyway .,if ( t instanceof thread death ) throw ( thread death ) t ; else t . print stack trace ( system . err ) ;,counterpart throw,fail,pre
this loop <PLACE_HOLDER> child loops . loop peeling could explode graph size .,if ( loop . loop ( ) . get children ( ) . size ( ) > __num__ ) { return false ; },loop has,success,pre
loopback <PLACE_HOLDER> empty mac .,if ( hw addr != null && hw addr . length > __num__ ) { string mac = byte array2 hex string ( hw addr ) ; if ( ! macs . contains ( mac ) ) macs . add ( mac ) ; },loopback has,fail,pre
ensure that the schema <PLACE_HOLDER> the virtual columns added,virtual column provider factory . add built in virtual columns to schema ( schema ) ;,schema has,success,pre
note : a null search name will <PLACE_HOLDER> the first pu definition found,string search name = null ;,name return,fail,pre
other users may already <PLACE_HOLDER> a handle on this combining value .,accum = combine fn . create accumulator ( ) ; is cleared = true ;,users have,success,pre
prevent npe and make the unit test <PLACE_HOLDER> equals ordinal position,other . set indextype ( index type . normal ) ; index meta . set indextype ( index type . normal ) ; assertions . assert not equals ( index meta @$ other ) ; other = new index meta ( ) ; other . set indextype ( index type . normal ) ; index meta . set indextype ( index type . normal ) ; assertions . assert equals ( index meta @$ other ) ;,test go,success,pre
user certificate store @$ does not <PLACE_HOLDER> static pins .,if ( info . target sdk version <= build . version_codes . m && ! info . is privileged app ( ) ) { builder . add certificates entry ref ( new certificates entry ref ( user certificate source . get instance ( ) @$ false ) ) ; },store include,fail,pre
use this setting to improve performance if you know that <PLACE_HOLDER>s in content do not <PLACE_HOLDER> the layout size of the recycler view,m recycler view . set has fixed size ( true ) ; recycler view . layout manager layout manager = new linear layout manager ( get activity ( ) ) ; m recycler view . set layout manager ( layout manager ) ; m recycler view . set adapter ( adapter ) ; m recycler view . add item decoration ( new spaces item decoration ( quick return utils . dp2px ( get activity ( ) @$ __num__ ) ) ) ; int header height = get resources ( ) . get dimension pixel size ( r . dimen . facebook_header_height ) ; int footer height = get resources ( ) . get dimension pixel size ( r . dimen . facebook_footer_height ) ; int header translation =,changes change,success,pre
note : scala and wsdl <PLACE_HOLDER> no rules,return arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,scala have,success,pre
end probe phase @$ iterator <PLACE_HOLDER> side elements .,collector . collect ( build iter . get row ( ) ) ; while ( build iter . advance next ( ) ) { collector . collect ( build iter . get row ( ) ) ; },iterator collect,fail,pre
the highest one bit should be 1 @$ which <PLACE_HOLDER> the word as a number is negative,return word < __num__ ;,which treats,fail,pre
dispatch the callback with empty arrays which <PLACE_HOLDER> a cancellation .,on request permissions result ( request code @$ new string [ __num__ ] @$ new int [ __num__ ] ) ; return ;,which enforce,fail,pre
a volt <PLACE_HOLDER> extension to avoid using exceptions for flow control .,if ( ! is option ) { if ( ! prefer to throw ) { return unexpected token ( ) ; } throw unexpected token ( ) ; },volt db,success,pre
user has <PLACE_HOLDER> a connection @$ we can continue ...,if ( connection != null ) { database db = new database ( this @$ connection ) ; db . share variables with ( this ) ; try { db . connect ( parent job . get transaction id ( ) @$ null ) ; string real schemaname = environment substitute ( schemaname ) ; string real tablename = environment substitute ( tablename ) ; if ( db . check table exists ( real tablename ) ) { if ( log . is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ ) + real tablename + base messages . get string ( pkg @$ __str__ ) ) ; } if ( schemaname != null ) { real tablename =,user specified,success,pre
also append sub 1 <PLACE_HOLDER> 2,dfs test util . append file ( hdfs @$ sub1file2 @$ blocksize ) ; hdfs . create snapshot ( dir @$ __str__ ) ; out . close ( ) ;,1 file,success,pre
add all <PLACE_HOLDER> filters to this graph,for ( filter filter : added filters ) { add filter ( filter ) ; },all added,success,pre
the application has both 64 and 32 bit bundled libraries . we check here that the app declares multi arch support @$ and warn if it does n't . we will be lenient here and record both ab is . the primary will be the abi that 's higher on the list @$ i.e @$ a device that 's configured to prefer 64 bit,if ( has32 bit libs && has64 bit libs ) { if ( ( pkg . application info . flags & application info . flag_multiarch ) == __num__ ) { slog . e ( package manager service . tag @$ __str__ + pkg + __str__ ) ; } if ( vm runtime . is64 bit instruction set ( get preferred instruction set ( ) ) ) { primary cpu abi = build . supported_64_bit_abis [ __num__ ] ; secondary cpu abi = build . supported_32_bit_abis [ __num__ ] ; } else { primary cpu abi = build . supported_32_bit_abis [ __num__ ] ; secondary cpu abi = build . supported_64_bit_abis [ __num__ ] ; } } else { primary cpu abi = null ; secondary cpu abi =,device declare,fail,pre
if the next sublist <PLACE_HOLDER> no element @$ add this one and then break @$ otherwise just break,if ( next index - curr index == __num__ ) { next index ++ ; },sublist has,success,pre
since the db is running @$ <PLACE_HOLDER> the lock files,return ! __str__ . equals ( sub path ) && ! sub path . ends with ( __str__ ) ;,files remove,fail,pre
this assignment should n't <PLACE_HOLDER> 'tile.foo ' to be inferred as undefined above .,test types with common externs ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,assignment cause,success,pre
bind and start to <PLACE_HOLDER> incoming connections .,server bootstrap . bind ( new inet socket address ( address @$ port ) ) ;,bind accept,success,pre
the following boolean logic <PLACE_HOLDER> the table above,if ( left == left2 && right == right2 && key length <= r2 . key length && r2 . pattern . region matches ( __num__ @$ pattern @$ __num__ @$ len ) ) { return ( flags == r2 . flags ) || ( ! ( ( flags & anchor_start ) != __num__ ) && ! ( ( flags & anchor_end ) != __num__ ) ) || ( ( ( r2 . flags & anchor_start ) != __num__ ) && ( ( r2 . flags & anchor_end ) != __num__ ) ) ; },logic covers,fail,pre
close all the given <PLACE_HOLDER> media items on error case .,if ( error string != null ) { for ( media item item : playlist ) { if ( item instanceof file media item ) { ( ( file media item ) item ) . increase ref count ( ) ; ( ( file media item ) item ) . decrease ref count ( ) ; } } throw new illegal argument exception ( error string ) ; },given file,success,pre
effective host name minus domain must not <PLACE_HOLDER> any dots,string effective host without domain = host . substring ( __num__ @$ host . length ( ) - cookie domain . length ( ) ) ; return effective host without domain . index of ( __str__ ) == - __num__ ;,name contain,success,pre
visit the class type after interface types which is the order the obj c compiler <PLACE_HOLDER> the hierarchy .,type mirror class type = null ; for ( type mirror super type : direct supertypes ( type ) ) { if ( ! result ) { return false ; } if ( is class ( super type ) ) { class type = super type ; } else { visit type hierarchy objc order ( super type @$ visitor ) ; } } if ( class type != null && result ) { result = visit type hierarchy objc order ( class type @$ visitor ) ; } return result ;,compiler describes,fail,pre
! f create entity ref <PLACE_HOLDER> move children of entity ref before the entity ref . remove entity ref .,f current node index = f deferred document impl . get parent node ( f current node index @$ false ) ;,ref do,fail,pre
remove core attributes since the user does not <PLACE_HOLDER> them @$ unless they are in the attribute list . attribute list always wins,for ( string core attribute : core attributes ) { if ( ! attributes . contains ( core attribute ) ) { result . remove ( core attribute ) ; } },user need,fail,pre
unit test version does not <PLACE_HOLDER> launch wake lock,do nothing ( ) . when ( this ) . acquire launch wakelock ( ) ; do return ( m keyguard controller ) . when ( this ) . get keyguard controller ( ) ; m launching activity wake lock = mock ( power manager . wake lock . class ) ; initialize ( ) ;,version support,fail,pre
if padding color <PLACE_HOLDER> background color @$ return null to indicate we do n't have to paint it .,if ( padding color == background color ) { return null ; } return padding color ;,color matches,fail,pre
process height points <PLACE_HOLDER> priorityqueue @$ becase for same pos x @$ we always want the highest point,priority queue < integer > queue = new priority queue < integer > ( __num__ @$ collections . reverse order ( ) ) ; queue . offer ( __num__ ) ; int prev = queue . peek ( ) ; for ( height point p : height points ) { if ( p . height < __num__ ) { queue . offer ( - p . height ) ; } else { queue . remove ( p . height ) ; } int curr peak = queue . peek ( ) ; if ( curr peak != prev ) { rst . add ( new int [ ] { p . x @$ curr peak } ) ; prev = curr peak ; } } return rst ;,points function,fail,pre
find the place the new entry should go @$ ensuring an entry with the same name does n't already <PLACE_HOLDER> along the way,directory entry prev = null ; directory entry curr = table [ index ] ; while ( curr != null ) { if ( curr . name ( ) . equals ( entry . name ( ) ) ) { if ( overwrite existing ) { if ( prev != null ) { prev . next = entry ; } else { table [ index ] = entry ; } entry . next = curr . next ; curr . next = null ; entry . file ( ) . increment link count ( ) ; return ; } else { throw new illegal argument exception ( __str__ + entry . name ( ) + __str__ ) ; } } prev = curr ; curr = curr .,entry occur,fail,pre
check if font <PLACE_HOLDER> correct number of letters,if ( font . length != __num__ ) { system . out . println ( __str__ + font . length + __str__ ) ; system . out . println ( __str__ ) ; } for ( int i = __num__ ; i < font . length ; i ++ ) { letter = font [ i ] ; b = __num__ ; if ( ( letter . length != n ) && ( n == - __num__ ) ) { n = letter . length ; } else if ( letter . length != n ) { system . out . println ( __str__ + i + __str__ + letter . length + __str__ + n + __str__ ) ; system . out . println ( __str__ ),font has,success,pre
delete range is complete before range so not relevant @$ <PLACE_HOLDER> next delete range,delete range = delete range it . has next ( ) ? delete range it . next ( ) : null ; break ; case range1_completely_after_range2 :,range use,fail,pre
at this point either m <PLACE_HOLDER> country isos or m blacklisted country isos is null . we assume no countries are to be excluded . here @$ we correct this assumption based on the contents of either lists .,set < string > excluded countries = new hash set < > ( ) ; if ( m whitelisted country isos == null ) { excluded countries . add all ( m blacklisted country isos ) ; } else { excluded countries . add all ( country info map . key set ( ) ) ; excluded countries . remove all ( m whitelisted country isos ) ; },m allowed,fail,pre
broker <PLACE_HOLDER> a concrete member id to be allowed to join the group . update member id and send another join group request in next cycle .,if ( error == errors . member_id_required ) { synchronized ( abstract coordinator . this ) { abstract coordinator . this . generation = new generation ( offset commit request . default_generation_id @$ join response . data ( ) . member id ( ) @$ null ) ; abstract coordinator . this . reset state and rejoin ( ) ; } future . raise ( error ) ; } else { log . error ( __str__ @$ error . message ( ) ) ; future . raise ( new kafka exception ( __str__ + error . message ( ) ) ) ; },broker requires,success,pre
args can <PLACE_HOLDER> things like location macros @$ so extract any inputs we find .,for ( arg arg : preprocessor flags . get other flags ( ) . get all flags ( ) ) { buildable support . derive inputs ( arg ) . for each ( input consumer ) ; },args contain,success,pre
this is not the last row @$ so our offset should <PLACE_HOLDER> the next row to be used ...,if ( event row number < ( total number of rows - __num__ ) ) { this . current row number = event row number ; this . restart rows to skip = this . current row number + __num__ ; return offset using position ( this . restart rows to skip ) ; },offset show,fail,pre
the users of this method do not <PLACE_HOLDER> a nano clock,return clock . system ( zone ) ;,users need,success,pre
this method will be called on rotation suggestion changes even if the proposed rotation is not valid for the top app . <PLACE_HOLDER> invalid rotation choices as a signal to remove the rotate button if shown .,if ( ! is valid ) { set rotate suggestion button state ( false ) ; return ; },method disable,fail,pre
and it is not always true that a bigger array <PLACE_HOLDER> more memory than a smaller one,return ( ) -> arrays . stream ( value type . array types ( ) ) . filter ( t -> t != value type . string_array && t != value type . string_alphanumeric_array && t != value type . string_ascii_array && t != value type . string_bmp_array ) . iterator ( ) ;,array holds,fail,pre
the test program creates a class file from the header stored above and adding the content of a source debug extension attribute made of the character 0 x 02 repeated 68000 times . this attribute does n't <PLACE_HOLDER> the syntax specified in jsr 45 but it 's fine because this test just checks that the jvm is able to load a class file with,byte [ ] buf = new byte [ header . length + attr size ] ; for ( int i = __num__ ; i < header . length ; i ++ ) { buf [ i ] = header [ i ] ; } for ( int i = __num__ ; i < attr size ; i ++ ) { buf [ header . length + i ] = ( byte ) __num__ ; } class c = loader . define class ( __str__ @$ buf @$ __num__ @$ buf . length ) ; system . out . println ( __str__ ) ;,attribute support,fail,pre
we hold the wake lock as long as the service is <PLACE_HOLDER> commands .,synchronized ( this ) { if ( ! m service processing ) { m service processing = true ; try { m run wake lock . acquire ( __num__ * __num__ * __num__ ) ; m launch wake lock . release ( ) ; } catch ( throwable e ) { file log . e ( e ) ; m service processing = false ; } } },service processing,success,pre
customers do n't <PLACE_HOLDER> this exception,if ( ! ( boolean ) result ) { throw new entry not found exception ( __str__ ) ; },customers need,fail,pre
check if the workers <PLACE_HOLDER> home ...,assert true ( appender . log contains ( __str__ ) ) ; assert true ( appender . log contains ( __str__ ) ) ; assert true ( appender . log contains ( __str__ ) ) ;,workers hit,fail,pre
tests <PLACE_HOLDER> proxy util.ste array to step array,new throwable proxy ( t ) ;,tests throwable,success,pre
only labels changed . since we do n't know which properties this entity has let 's <PLACE_HOLDER> all indexes for the changed labels .,if ( properties . length == __num__ ) { set . matching descriptors ( descriptors @$ changed entity tokens ) ; } else if ( changed entity tokens . length == __num__ ) { set . matching descriptors for partial list of properties ( descriptors @$ unchanged entity tokens @$ properties ) ; } else { set . matching descriptors ( descriptors @$ changed entity tokens ) ; set . matching descriptors for partial list of properties ( descriptors @$ unchanged entity tokens @$ properties ) ; },'s include,success,pre
we catch everything because right now the masters do not require authentication . so delay reporting errors to the user until the servers <PLACE_HOLDER> 401 unauthorized .,log . debug ( __str__ + __str__ + __str__ @$ e ) ;,servers return,success,pre
negative offset <PLACE_HOLDER> offscreen,return cursor offset >= __num__ ;,offset means,success,pre
open a new region which <PLACE_HOLDER> this wal,table descriptor htd = table descriptor builder . new builder ( table name . value of ( __str__ ) ) . set column family ( column family descriptor builder . of ( b ) ) . build ( ) ; region info hri = region info builder . new builder ( htd . get table name ( ) ) . build ( ) ; chunk creator . initialize ( mem storelab impl . chunk_size_default @$ false @$ __num__ @$ __num__ @$ __num__ @$ null ) ; test_util . create localh region ( hri @$ htd @$ wal ) . close ( ) ; region server services rs services = mock ( region server services . class ) ; when ( rs services . get server name ( ),which holds,fail,pre
rank 2 <PLACE_HOLDER> the second largest coordinate .,i2 = rankx >= __num__ ? __num__ : __num__ ; j2 = ranky >= __num__ ? __num__ : __num__ ; k2 = rankz >= __num__ ? __num__ : __num__ ; l2 = rankw >= __num__ ? __num__ : __num__ ;,rank denotes,success,pre
script object mirror <PLACE_HOLDER> some object equality problems for entries,if ( o instanceof map ) { if ( depth > __num__ || ! seen . add ( o ) ) { return true ; } map map = ( map ) o ; map . for each ( ( k @$ v ) -> { if ( recurse cyclic ( depth + __num__ @$ v @$ seen ) ) { map . put ( k @$ __str__ + v . get class ( ) . get name ( ) ) ; } } ) ; } else if ( o instanceof list ) { if ( depth > __num__ || ! seen . add ( o ) ) { return true ; } list list = ( list ) o ; int count = list . size,mirror has,success,pre
json stringer can <PLACE_HOLDER> an oom when the json to handle is too big .,response . result = null ; response . error = m object mapper . convert value ( e . get message ( ) @$ json object . class ) ; json object = m object mapper . convert value ( response @$ json object . class ) ; response string = json object . to string ( ) ;,stringer throw,fail,pre
harmony expected 10 @$ but the ri and android <PLACE_HOLDER> 0 .,assert equals ( __num__ @$ f . get channel ( ) . position ( ) ) ;,ri have,fail,pre
should flush <PLACE_HOLDER> 1 and <PLACE_HOLDER> 2 and then run <PLACE_HOLDER> 3 immediately,assert true ( latch3 . await ( __num__ @$ time unit . milliseconds ) ) ; assert equals ( __num__ @$ latch1 . get count ( ) ) ; assert equals ( __num__ @$ latch2 . get count ( ) ) ;,flush handle,fail,pre
the map reduce tokens are provided so that tasks can <PLACE_HOLDER> jobs if they wish to . the tasks authenticate to the job tracker via the map reduce delegation tokens .,if ( system . getenv ( __str__ ) != null ) { conf . set ( __str__ @$ system . getenv ( __str__ ) ) ; } return conf ;,tasks see,fail,pre
add a custom equality assertion because the resulting geometry may <PLACE_HOLDER> its constituent points in a different order,bi function < object @$ object @$ boolean > equality function = ( left @$ right ) -> { if ( left == null && right == null ) { return true ; } if ( left == null || right == null ) { return false ; } ogc geometry left geometry = ogc geometry . from text ( left . to string ( ) ) ; ogc geometry right geometry = ogc geometry . from text ( right . to string ( ) ) ; return left geometry . difference ( right geometry ) . is empty ( ) && right geometry . difference ( left geometry ) . is empty ( ) ; } ;,geometry have,success,pre
base length next line <PLACE_HOLDER> both positive lhs.exp and also scale mismatch,if ( scale != - lhs . exp ) reqdig = ( reqdig + scale ) + lhs . exp ; reqdig = ( reqdig - ( ( rhs . mant . length - __num__ ) ) ) - rhs . exp ;,length contains,fail,pre
need to set the system property that the log 4 j 2 configuration reads in order to determine the script log <PLACE_HOLDER> name . once that 's set @$ the log configuration must be 'kicked ' to pick up the change .,system . set property ( __str__ @$ file . get absolute path ( ) ) ; if ( initialized ) { ( ( logger context ) log manager . get context ( false ) ) . reconfigure ( ) ; },log file,success,pre
stream still <PLACE_HOLDER> data . buffer starvation occurred . audio decoder thread will fill the data and start the channel again .,reclaim channel = true ;,stream has,success,pre
stream must not <PLACE_HOLDER> data sent by server after reset .,assert false ( stream1 data latch . await ( __num__ @$ time unit . seconds ) ) ;,stream contain,fail,pre
gms join leave mistakenly <PLACE_HOLDER> an old view id when joining @$ making it a rogue member,gms join leave member id . set vm view id ( - __num__ ) ; member identifier previous member id = services . get member factory ( ) . create ( member data builder . new builder ( gms join leave member id . get inet address ( ) @$ gms join leave member id . get host name ( ) ) . set membership port ( gms join leave member id . get membership port ( ) ) . build ( ) ) ; previous member id . set vm view id ( __num__ ) ; previous member id . get member data ( ) . setuuid ( gms join leave member id . get member data ( ) . getuuid ( ) ) ; gms membership,member has,fail,pre
make sure the master <PLACE_HOLDER> all of the reports,waiter . wait for ( test_util . get configuration ( ) @$ __num__ * __num__ @$ new predicate < exception > ( ) { @ override public boolean evaluate ( ) throws exception { map < region info @$ long > region sizes = quota manager . snapshot region sizes ( ) ; log . trace ( __str__ + region sizes ) ; return num regions == count regions for table ( tn @$ region sizes ) && table size <= get table size ( tn @$ region sizes ) ; } } ) ; map < table name @$ long > sizes = test_util . get admin ( ) . get space quota table sizes ( ) ; long size = sizes . get ( tn ),master has,success,pre
get namespace edits dirs <PLACE_HOLDER> duplicates across edits and shared.edits,collection < uri > edits dirs = fs namesystem . get namespace edits dirs ( conf ) ; assert equals ( __num__ @$ edits dirs . size ( ) ) ;,dirs removes,success,pre
user 3 <PLACE_HOLDER> the permission,user dto user1 = db . users ( ) . insert user ( ) ; user dto user2 = db . users ( ) . insert user ( ) ; user dto user3 = db . users ( ) . insert user ( ) ; organization dto organization = db . organizations ( ) . insert ( ) ; group dto group1 = db . users ( ) . insert group ( organization ) ; db . users ( ) . insert permission on group ( group1 @$ administer ) ; db . users ( ) . insert permission on group ( group1 @$ provision_projects ) ; db . users ( ) . insert member ( group1 @$ user1 ) ; db . users ( ) . insert,user has,success,pre
class d <PLACE_HOLDER> a directly applied parameter annotation whose value includes to an annotation class that is missing .,test parameter annotation ( d . class . get declared method ( __str__ @$ object . class ) @$ true ) ;,d has,success,pre
seems awkward to have the stream <PLACE_HOLDER> itself .,super ( wrapped @$ wrapped . get http request ( ) ) ; this . wrapped = wrapped ;,stream wrapped,fail,pre
if there are more new rows than how many new dom rows got added @$ the top row logical index necessarily gets shifted down by that difference because recycling does n't replace any logical rows @$ just shifts them off the visual range @$ and the inserted rows that do n't fit to the visual range also <PLACE_HOLDER> the other rows down . if,if ( number of rows > added row count ) { update top row logical index ( number of rows - added row count ) ; },rows move,fail,pre
this situation is possible only if a callee <PLACE_HOLDER> an exception which type extends throwable directly,if ( cause instanceof command action execution exception ) { command action execution exception command action execution exception = ( command action execution exception ) cause ; cause = command action execution exception . get cause ( ) ; },callee throws,success,pre
the response should <PLACE_HOLDER> the exact string that the schema registry has .,assert . assert equals ( __num__ @$ metadata schemas list . size ( ) ) ; for ( register response metadata entry r : metadata schemas list ) { if ( r . get version ( ) == __num__ ) { assert . assert equals ( metadata schema1 @$ r . get schema ( ) ) ; assert . assert true ( arrays . equals ( meta schema digest1 @$ r . get crc32 ( ) ) ) ; } else { assert . assert equals ( metadata schema2 @$ r . get schema ( ) ) ; assert . assert true ( arrays . equals ( meta schema digest2 @$ r . get crc32 ( ) ) ) ; } } easy mock . verify ( mock,response contain,success,pre
unlike status.from throwable which <PLACE_HOLDER> the unknown code for these .,status st = status proto . from throwable ( e ) ; return st != null ? st : status . new builder ( ) . set code ( code . internal . get number ( ) ) . set message ( e . get message ( ) ) . build ( ) ;,which contains,fail,pre
list <PLACE_HOLDER> all languages,try ( byte provider byte provider = get byte provider ( __str__ @$ __str__ + __str__ + __str__ + __str__ ) ) { check validxml load spec ( byte provider @$ loader . find supported load specs ( byte provider ) @$ __str__ @$ endian . little @$ __str__ @$ null @$ __str__ ) ; },list includes,success,pre
this assumes the adapter has already <PLACE_HOLDER> up any partially created buffers .,if ( overlay ) { check remove address space ( start . get address space ( ) ) ; } throw new cancelled exception ( ) ; program . db error ( e ) ;,adapter picked,fail,pre
this branch coordinates fragment task or completed transaction task @$ <PLACE_HOLDER> the tasks until all the sites on the node receive the task . task with newer sp handle will,if ( task . need coordination ( ) && m_scoreboard enabled ) { coordinated task queue offer ( task ) ; } else { task queue offer ( task ) ; },task queue,fail,pre
failed to read the file . there are two possibilities . either this file is in old format which does not <PLACE_HOLDER> a magic seq in the beginning or this is not a valid file at all . try reading it as a file in old format,fis . close ( ) ; fis = new file input stream ( f ) ; dis = new data input stream ( new buffered input stream ( fis @$ __num__ * __num__ ) ) ; read disk store record ( dis @$ f ) ;,which have,success,pre
only let the system <PLACE_HOLDER> remote display routes for now .,if ( client record != null ) { if ( ! client record . m trusted ) { route types &= ~ media router . route_type_remote_display ; } if ( client record . m route types != route types || client record . m active scan != active scan ) { if ( debug ) { slog . d ( tag @$ client record + __str__ + integer . to hex string ( route types ) + __str__ + active scan ) ; } client record . m route types = route types ; client record . m active scan = active scan ; client record . m user record . m handler . send empty message ( user handler . msg_update_discovery_request ) ; } },system track,fail,pre
set the low threshold to 50 instead of 60 set the high threshold to 60 instead of 70 node 2 now should not <PLACE_HOLDER> new shards allocated to it @$ and shards can not remain,disk settings = settings . builder ( ) . put ( disk threshold settings . cluster_routing_allocation_disk_threshold_enabled_setting . get key ( ) @$ true ) . put ( disk threshold settings . cluster_routing_allocation_low_disk_watermark_setting . get key ( ) @$ __str__ ) . put ( disk threshold settings . cluster_routing_allocation_high_disk_watermark_setting . get key ( ) @$ __str__ ) . put ( disk threshold settings . cluster_routing_allocation_disk_flood_stage_watermark_setting . get key ( ) @$ __str__ ) . build ( ) ; deciders = new allocation deciders ( new hash set < > ( arrays . as list ( new same shard allocation decider ( settings . empty @$ cluster settings ) @$ make decider ( disk settings ) ) ) ) ; strategy = new allocation service ( deciders @$ new test,2 have,success,pre
verify append <PLACE_HOLDER> or not,verify ( wal @$ expect append ? times ( __num__ ) : never ( ) ) . append data ( ( h region info ) any ( ) @$ ( wal key impl ) any ( ) @$ ( wal edit ) any ( ) ) ;,append occured,fail,pre
if the text wrapper does n't <PLACE_HOLDER> a start index,if ( quotes . get ( i ) . start index == - __num__ ) { if ( closing quote == - __num__ ) { closing quote = i ; } else { if ( quotes . get ( i ) . end index < quotes . get ( closing quote ) . end index ) { closing quote = i ; } } },wrapper have,success,pre
table mutation <PLACE_HOLDER> a current schema to mutate,assert ( change type == change type . create || this . old schema != null ) ;,mutation requires,success,pre
insert the video . the command sends three arguments . the first specifies which information the api request is <PLACE_HOLDER> and which information the api response should return . the second argument is the video resource that contains metadata about the new video . the third argument is the actual video content .,you tube . videos . insert video insert = youtube . videos ( ) . insert ( __str__ @$ video object defining metadata @$ media content ) ;,request working,fail,pre
make sure the preferences has only one <PLACE_HOLDER> column,table sort state saved sort state = get sort state from preference state ( preference state @$ column model state ) ; assert equals ( __num__ @$ saved sort state . get sorted column count ( ) ) ; column sort state column sort state = saved sort state . get column sort state ( column zero ) ; assert not null ( column sort state ) ; sort direction sort direction = column sort state . get sort direction ( ) ; assert true ( sort direction . is ascending ( ) ) ;,preferences ordered,fail,pre
open the first file & read the required rows in the buffer @$ <PLACE_HOLDER> if it fails @$ exception will <PLACE_HOLDER> process loop,open next file ( ) ;,exception catch,fail,pre
while wbmp reader does not <PLACE_HOLDER> ext wbmp headers,if ( type != __num__ || fix header field != __num__ ) { return false ; },reader support,success,pre
go through each message and make sure the <PLACE_HOLDER>ed fields are null then call the getters which will <PLACE_HOLDER> the data again to show the marshalled data exists,for ( message reference ref : messages ) { activemq text message message = ( activemq text message ) ref . get message ( ) ; field properties field = message . class . get declared field ( __str__ ) ; properties field . set accessible ( true ) ; field text field = activemq text message . class . get declared field ( __str__ ) ; text field . set accessible ( true ) ; assert null ( text field . get ( message ) ) ; assert null ( properties field . get ( message ) ) ; assert not null ( message . get properties ( ) ) ; assert not null ( message . get text ( ) ) ; } consumer . close,which load,fail,pre
no need to sync because noone <PLACE_HOLDER> access to new info yet,new info . policy entries . add ( pe ) ;,noone has,success,pre
list applicable roles may <PLACE_HOLDER> role which was already added explicitly above .,return stream . concat ( roles @$ list applicable roles ( principal @$ list role grants ) . map ( role grant :: get role name ) . filter ( predicate . is equal ( admin_role_name ) . negate ( ) ) ) . distinct ( ) ;,roles contain,fail,pre
make sure the new comment <PLACE_HOLDER> the same level as the old one,new comment . level = this . get ( index ) . level ; this . set ( index @$ new comment ) ; return true ;,comment has,success,pre
the new tree does n't <PLACE_HOLDER> this component . we only need to remove all its children from the list .,if ( new root is null ) { final int current items count = current root . get count ( ) ; removed components . add ( current root ) ; final change set change set = change set . acquire change set ( current root . get count ( ) @$ new root @$ enable stats ) ; for ( int i = __num__ ; i < current items count ; i ++ ) { change set . add change ( change . remove ( __num__ ) ) ; } return change set ; },tree contain,fail,pre
optional validation if the response <PLACE_HOLDER> a destination,if ( has text ( saml response . get destination ( ) ) && ! recipient . equals ( saml response . get destination ( ) ) ) { throw auth exception ( invalid_destination @$ __str__ + saml response . get destination ( ) ) ; } string issuer = saml response . get issuer ( ) . get value ( ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ + issuer ) ; } if ( ! has text ( issuer ) || ( ! issuer . equals ( token . get idp entity id ( ) ) ) ) { string message = string . format ( __str__ @$ issuer @$ token . get idp entity id,response includes,fail,pre
slow mode <PLACE_HOLDER> an async thread,if ( mode == getrss mode . ps ) { if ( thread != null ) { if ( thread . is alive ( ) ) return ; else thread = null ; } thread = new thread ( new runnable ( ) { @ override public void run ( ) { sample system now ( medium @$ large ) ; } } ) ; thread . start ( ) ; } else { sample system now ( medium @$ large ) ; },mode needs,fail,pre
description nodes will <PLACE_HOLDER> the both our description @$ if it exists @$ and any nested stat descriptions . ours will always be first,if ( description nodes . get length ( ) > __num__ ) { element description node = ( element ) description nodes . item ( __num__ ) ; if ( description node . get parent node ( ) . get node name ( ) . equals ( type node . get node name ( ) ) ) { description = extract description ( description node ) ; } },nodes contain,success,pre
both <PLACE_HOLDER> global foo,check symbol ( symbols [ __num__ ] @$ __str__ @$ true ) ;,both added,success,pre
put connect controls into environment . copy them first since caller <PLACE_HOLDER> the array .,if ( conn ctls != null ) { control [ ] copy = new control [ conn ctls . length ] ; system . arraycopy ( conn ctls @$ __num__ @$ copy @$ __num__ @$ conn ctls . length ) ; env . put ( bind_controls_property @$ copy ) ; },caller owns,success,pre
any user can <PLACE_HOLDER> table functions ; queries like ` select 1 ` might be used to do simple connection checks,return null ;,user provide,fail,pre
note that we can not do any size checking here @$ as the correct component count depends on the drawing step . gl should <PLACE_HOLDER> such errors then @$ and we will report them to the user .,if ( m values != null ) { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ __num__ ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m values ) ; } else { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ m vbo ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m offset ) ; },note prevent,fail,pre
multiload 3 items and ensure that multiload <PLACE_HOLDER> 2 from the database & 1 from the cache .,list < simple entity > entities = session . by multiple ids ( simple entity . class ) . with ( cache mode . normal ) . enable session check ( true ) . enable ordered return ( true ) . enable return of deleted entities ( true ) . multi load ( ids ( __num__ ) ) ; assert equals ( __num__ @$ entities . size ( ) ) ; simple entity deleted entity = entities . get ( __num__ ) ; assert not null ( deleted entity ) ; final entity entry entry = ( ( shared session contract implementor ) session ) . get persistence context ( ) . get entry ( deleted entity ) ; assert true ( entry . get status ( ),multiload pulls,success,pre
keep glyph cache panel width from ever going down so the canvas game container does n't <PLACE_HOLDER> sizes and flicker .,glyph cache panel = new j panel ( ) { private int max width ; public dimension get preferred size ( ) { dimension size = super . get preferred size ( ) ; max width = math . max ( max width @$ size . width ) ; size . width = max width ; return size ; } } ;,container expand,fail,pre
create server validator eagerly so that we can conveniently get the trusted certificates clients need it anyway eventually @$ and servers will not <PLACE_HOLDER> the little extra footprint,validator v = get validator ( validator . var_tls_server ) ; trusted certs = v . get trusted certificates ( ) ; server validator = v ; if ( debug != null && debug . is on ( __str__ ) ) { show trusted certs ( ) ; },servers add,fail,pre
make sure constantinople config is not <PLACE_HOLDER> parent 's block reward,return new constants adapter ( super . get constants ( ) ) { @ override public big integer getblock_reward ( ) { return big integer . ten ; } } ;,config using,success,pre
for delayed binding step partitioning meta does not <PLACE_HOLDER> schema name when using in constructor so we have to set it explicitly . see equals implementation for step partitioning meta .,step partitioning meta part meta = new step partitioning meta ( __str__ @$ schema ) ;,meta specify,fail,pre
to prevent task <PLACE_HOLDER> resize animation may flicking when playing app transition animation & ime window enter animation in parallel @$ make sure app transition is done and then start to animate for ime .,if ( m animating for ime && ! m display content . m app transition . is running ( ) ) { return animate for ime ( now ) ; },task set,fail,pre
empty input <PLACE_HOLDER> empty output .,string [ ] [ ] [ ] inputs outputs = { { { } @$ { } } @$ { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__,input produces,success,pre
one <PLACE_HOLDER> only @$ valid transformations ; space in the beginning @$ transform quoted,list < object [ ] > parameters = arrays . < object [ ] > as list ( new object [ ] { new string item ( __str__ ) @$ __str__ @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . state ) } @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . command ) } @$ null } @$ new object [ ] { string item with state ( __str__ @$ new string type ( __str__ ) ) @$ __str__ @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . state ) } @$ new itemio connection [ ] { new itemio,one bytes,fail,pre
queue c 111 <PLACE_HOLDER> sa and aq @$ both from parent,assert true ( c111 . has access ( queueacl . administer_queue @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . administer_queue @$ __str__ ) ) ; assert true ( c111 . has access ( queueacl . submit_applications @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . submit_applications @$ __str__ ) ) ; reset ( c ) ;,111 has,success,pre
unless this is the last replacment names @$ we <PLACE_HOLDER> the used name . the lastreplacement name applies any additional entries .,if ( names . size ( ) > __num__ ) { names . remove ( __num__ ) ; } callback . copy ( null ) ;,names remove,fail,pre
say xml <PLACE_HOLDER> a type attribute that is removed in the new version,string existingxml = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; cache config cache config = service . un marshall ( existingxml ) ; list elements = cache config . get custom cache elements ( ) ; assert that ( elements . get ( __num__ ) ) . is instance of ( element one . class ) ;,xml has,success,pre
let handle timeout <PLACE_HOLDER> care of finishing the page,if ( ! timeout flag ) handler . post delayed ( new web view status checker ( ) @$ __num__ ) ;,timeout take,success,pre
add document level entity <PLACE_HOLDER> info,if ( doc . contains key ( mentions annotation . class ) ) { for ( core map mention : doc . get ( mentions annotation . class ) ) { builder . add mentions ( to proto mention ( mention ) ) ; } keys to serialize . remove ( mentions annotation . class ) ; builder . set has entity mentions annotation ( true ) ; } else { builder . set has entity mentions annotation ( false ) ; },entity mentions,success,pre
instead of expiring the session directly here @$ accumulate a list of session ids that need to be expired . this is an efficiency measure : as the expiration <PLACE_HOLDER> the session data store doing a delete @$ it is most efficient if it can be done as a bulk operation to eg reduce roundtrips to the persistent store . only do this if,if ( session . is expired at ( now ) ) { if ( _session id manager . get session house keeper ( ) != null && _session id manager . get session house keeper ( ) . get interval sec ( ) > __num__ ) { _candidate session ids for expiry . add ( session . get id ( ) ) ; if ( log . is debug enabled ( ) ) log . debug ( __str__ @$ session . get id ( ) ) ; } } else { _session cache . check inactive session ( session ) ; },expiration has,fail,pre
anchor changed @$ qualified name of children <PLACE_HOLDER> an update,update child categories ( store object @$ store object . get children categories ( ) @$ impacted categories @$ false ) ;,name needs,success,pre
initialize the input method manager and start a daemon thread if the user <PLACE_HOLDER> multiple input methods to choose from . otherwise @$ just keep the instance .,if ( imm . has multiple input methods ( ) ) { imm . initialize ( ) ; thread imm thread = new thread ( imm @$ thread name ) ; imm thread . set daemon ( true ) ; imm thread . set priority ( thread . norm_priority + __num__ ) ; imm thread . start ( ) ; } input method manager = imm ;,user specified,fail,pre
removing any quad item to that item will <PLACE_HOLDER> the item,final quad item < t > quad item = new quad item < > ( item ) ; synchronized ( m quad tree ) { m items . remove ( quad item ) ; m quad tree . remove ( quad item ) ; },item remove,success,pre
both items <PLACE_HOLDER> the same type,return __num__ ;,items have,fail,pre
no incoming flow file containing a query @$ and an exception <PLACE_HOLDER> no outbound flowfile . there should be no flow files on either relationship,runner . assert all flow files transferred ( abstract executesql . rel_failure @$ __num__ ) ; runner . assert all flow files transferred ( abstract executesql . rel_success @$ __num__ ) ;,exception causes,success,pre
this is strange xml stream reader <PLACE_HOLDER>s xml stream exception xml event reader does n't <PLACE_HOLDER> xml stream exception,boolean next = false ; try { next = fxml reader . has next ( ) ; } catch ( xml stream exception ex ) { return false ; } return next ;,reader throw,success,pre
check that the vertices <PLACE_HOLDER> the trigger checkpoint message for the second checkpoint,verify ( vertex1 . get current execution attempt ( ) @$ times ( __num__ ) ) . trigger checkpoint ( eq ( checkpoint2 id ) @$ eq ( timestamp + __num__ ) @$ any ( checkpoint options . class ) ) ; verify ( vertex2 . get current execution attempt ( ) @$ times ( __num__ ) ) . trigger checkpoint ( eq ( checkpoint2 id ) @$ eq ( timestamp + __num__ ) @$ any ( checkpoint options . class ) ) ;,vertices received,success,pre
control multicast <PLACE_HOLDER> agent to inject proxy,agent = multicast discovery agent factory . create discovery agent ( new uri ( discovery address ) ) ; agent . register service ( proxy . get url ( ) . to string ( ) ) ; agent . start ( ) ; do reconnect ( ) ;,multicast deprecate,fail,pre
get the calculated base dirs which <PLACE_HOLDER> the overlays,str = props . get property ( __str__ ) ; if ( ! string util . is blank ( str ) ) { resource collection bases = new resource collection ( string util . csv split ( str ) ) ; web app . set war ( null ) ; web app . set base resource ( bases ) ; },which have,fail,pre
the size of call <PLACE_HOLDER> the size of the input callback argument .,size += size of ( get original callback argument ( ) ) ; return size ;,size gets,success,pre
since byte buffers can <PLACE_HOLDER> all kinds of crazy places it 's harder to keep track of which blocks are kept alive by what byte buffer . so we make a guess .,if ( c instanceof byte buffer extended cell ) { byte buffer extended cell bb cell = ( byte buffer extended cell ) c ; byte buffer bb = bb cell . get value byte buffer ( ) ; if ( bb != last block ) { context . increment response block size ( bb . capacity ( ) ) ; last block = bb ; } } else { byte [ ] value array = c . get value array ( ) ; if ( value array != last block ) { context . increment response block size ( value array . length ) ; last block = value array ; } },buffers escape,fail,pre
this is not needed @$ the underlying functor does not <PLACE_HOLDER> access to this,throw new unsupported operation exception ( __str__ ) ;,functor have,success,pre
record job <PLACE_HOLDER> failure .,if ( ! internal && ctx . event ( ) . is recordable ( evt_job_failed ) ) evts = add event ( evts @$ evt_job_failed @$ __str__ + job ) ; try { byte [ ] res bytes = null ; byte [ ] ex bytes = null ; byte [ ] attr bytes = null ; boolean loc = ctx . local node id ( ) . equals ( snd node . id ( ) ) && ! ctx . config ( ) . is marshal local jobs ( ) ; map < object @$ object > attrs = job ctx . get attributes ( ) ; if ( ! loc ) { try { res bytes = u . marshal ( marsh @$ res ) ;,job send,fail,pre
test that a null element <PLACE_HOLDER> null pointer exception,caught = false ; try { f inst . redefine classes ( new class definition [ ] { null } ) ; } catch ( null pointer exception npe ) { caught = true ; } assert true ( caught ) ;,element throws,success,pre
single selection model should n't <PLACE_HOLDER> selection column to begin with,assert false ( __str__ @$ get grid element ( ) . get cell ( __num__ @$ __num__ ) . is element present ( by . tag name ( __str__ ) ) ) ; set selection model single ( ) ; header = get grid element ( ) . get header cell ( __num__ @$ __num__ ) ; assert false ( __str__ @$ header . is element present ( by . tag name ( __str__ ) ) ) ;,model have,success,pre
return to the last <PLACE_HOLDER> tab if triggered from the content area .,if ( m gaia service type != account management screen helper . gaia_service_type_none ) { if ( is added ( ) ) get activity ( ) . finish ( ) ; } return true ;,return added,fail,pre
big query encodes numeric values to avro using the bytes type with the decimal logical type . avro coder ca n't <PLACE_HOLDER> logical types to schemas directly @$ so we need to get the schema for the bird class defined below @$ then replace the field used to test numeric with a field that has the appropriate schema .,big decimal birthday money = new big decimal ( __str__ ) ; schema birthday money schema = schema . create ( type . bytes ) ; logical type birthday money logical type = logical types . decimal ( birthday money . precision ( ) @$ birthday money . scale ( ) ) ;,coder convert,fail,pre
test user 1 can not <PLACE_HOLDER> user 2 's note,authorization service . set owners ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set readers ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set runners ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set writers ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; list < paragraph info > paragraph list1 = null ; try { paragraph list1 = notebook server . get paragraph list ( user1 id,user get,success,pre
process command line property definitions these can potentially <PLACE_HOLDER> multiple times,list < cl option > cl options = parser . get arguments ( ) ; for ( cl option option : cl options ) { string name = option . get argument ( __num__ ) ; string value = option . get argument ( __num__ ) ; switch ( option . get descriptor ( ) . get id ( ) ) { case cl option . text_argument : throw new illegal argument exception ( __str__ + option . get argument ( ) ) ; case propfile2_opt : log . info ( __str__ @$ name ) ; try ( file input stream fis = new file input stream ( new file ( name ) ) ) { properties tmp = new properties ( ) ; tmp . load (,these occur,success,pre
represents separator @$ which <PLACE_HOLDER> no temporal field,verify pattern parsing ( __str__ @$ new array list < > ( list . of ( null @$ chrono field . year @$ null @$ chrono field . month_of_year @$ null @$ chrono field . day_of_month @$ null ) ) ) ;,which has,success,pre
at this point @$ the given name does not <PLACE_HOLDER> any session name,throw new api exception ( api exception . type . illegal_parameter @$ action_param_session ) ; case action_unset_active_session : site = extension . get http sessions site ( api utils . get authority ( params . get string ( action_param_site ) ) @$ false ) ; if ( site == null ) { throw new api exception ( api exception . type . illegal_parameter @$ action_param_site ) ; } site . unset active session ( ) ; return api response element . ok ; case action_add_session_token : extension . add http session token ( api utils . get authority ( params . get string ( action_param_site ) ) @$ params . get string ( action_param_token_name ) ) ; return api response element . ok ; case action_remove_session_token : extension,name contain,fail,pre
create boring attribute which <PLACE_HOLDER> a relation edge,attribute type < string > attribute type = tx . put attribute type ( __str__ @$ attribute type . data type . string ) ; attribute < string > attribute = attribute type . create ( __str__ ) ; entity type entity type = tx . put entity type ( __str__ ) . has ( attribute type ) ; entity entity = entity type . create ( ) ; entity . has ( attribute ) ; relation impl relation = relation impl . from ( entity . relations ( ) . iterator ( ) . next ( ) ) ;,which has,fail,pre
create back buffer when not printing @$ and its graphics 2 d then <PLACE_HOLDER> drawing parameters for that graphics 2 d object,if ( is printing ) g2 = ( graphics2d ) g ; else { back buffer = ( buffered image ) this . create image ( w @$ h ) ; g2 = back buffer . create graphics ( ) ; g2 . set color ( color . white ) ; g2 . fill rect ( __num__ @$ __num__ @$ w @$ h ) ; g2 . set color ( color . black ) ; },printing define,fail,pre
only one address must <PLACE_HOLDER> the local address,if ( found > __num__ ) { string msg = __str__ + __str__ + dfs_nameservice_id + __str__ + dfs_ha_namenode_id_key ; throw new hadoop illegal argument exception ( msg ) ; },address match,success,pre
table <PLACE_HOLDER> capabilities,t capabilities = t capabilities . replace all ( __str__ @$ __str__ ) . to upper case ( ) ;,table managed,fail,pre
the values in the linear axis will not <PLACE_HOLDER> values after the decimal point .,vertical axis . set label format ( __str__ ) ; categorical axis horizontal axis = new categorical axis ( ) ; horizontal axis . set label interval ( __num__ ) ; horizontal axis . set label fit mode ( axis label fit mode . multi_line ) ; area series . set vertical axis ( vertical axis ) ; area series . set horizontal axis ( horizontal axis ) ;,values have,success,pre
query success @$ <PLACE_HOLDER> the right balance @$ bandwidth and the account name .,account query result = query account ( test key002 @$ blocking stub full ) ;,success get,success,pre
this view ddl ca n't be used if the table already <PLACE_HOLDER> data @$ only use it on empty tables,if ( this . new schema . view rep != null && this . start == __num__ ) { batch . add ( this . new schema . view rep . ddl for view ( ) ) ; } this . start = system . nano time ( ) ; if ( change type == change type . create ) { log . info ( __str__ ) ; } else { log . info ( __str__ ) ; } return batch . execute ( ) ;,table has,fail,pre
this handler <PLACE_HOLDER> a single string from the message and prints it to the standard output .,try { string string = text message . get text ( ) ; system . out . println ( string ) ; } catch ( javax . jms . jms exception jmse ) { jmse . print stack trace ( ) ; } system . out . println ( __str__ + __str__ ) ;,handler reads,success,pre
check the json itself <PLACE_HOLDER> sense,json object json obj = new json object ( responsejson ) ; json array results = json obj . getjson array ( __str__ ) ; assert equals ( __num__ @$ response . results . length ) ; json object table = results . getjson object ( __num__ ) ; json array data = table . getjson array ( __str__ ) ; assert equals ( __num__ @$ data . length ( ) ) ; json array row = data . getjson array ( __num__ ) ; assert equals ( __num__ @$ row . length ( ) ) ; long value = row . get long ( __num__ ) ; assert equals ( __num__ @$ value ) ;,itself makes,success,pre
the compressed bzip 2 stream should start with the identifying characters bz . caller of cb zip 2 output stream i.e . this class must <PLACE_HOLDER> these characters .,if ( super . out != null ) { out . write ( header . get bytes ( standard charsets . utf_8 ) ) ; },bzip write,success,pre
now let 's <PLACE_HOLDER> an identical key for get,binary object builder bldr = grid ( __num__ ) . binary ( ) . builder ( __str__ ) ; bldr . set field ( __str__ @$ __num__ ) ; bldr . set field ( __str__ @$ __num__ ) ;,'s build,success,pre
provide an empty key store since trust manager impl does n't support null key stores . trust manager impl will <PLACE_HOLDER> cert store to lookup certificates .,key store store = key store . get instance ( key store . get default type ( ) ) ; store . load ( null ) ; m delegate = new trust manager impl ( store @$ null @$ cert store ) ;,impl use,success,pre
user <PLACE_HOLDER> not issue admin permission on these 2 issues,issue dto not authorized issue1 = db . issues ( ) . insert ( rule @$ project2 @$ project2 @$ i -> i . set type ( bug ) . set status ( status_open ) . set resolution ( null ) ) ; issue dto not authorized issue2 = db . issues ( ) . insert ( rule @$ project2 @$ project2 @$ i -> i . set type ( bug ) . set status ( status_open ) . set resolution ( null ) ) ; bulk change ws response response = call ( builder ( ) . set issues ( as list ( authorized issue1 . get key ( ) @$ not authorized issue1 . get key ( ) @$ not authorized issue2 . get key (,user has,success,pre
create a verifier but do not require exact verification because the verifier sockets may <PLACE_HOLDER> rows out of order @$ due to the fact that socket exporter acknowledges rows when written on the sending socket @$ not when <PLACE_HOLDER>d on the destination host .,m_verifier = new export test expected data ( m_server sockets @$ false @$ false @$ kfactor + __num__ ) ;,sockets receive,success,pre
get user inputs and set the map source this bit <PLACE_HOLDER> the key from the manifest,alert dialog builder . set cancelable ( false ) . set positive button ( __str__ @$ new dialog interface . on click listener ( ) { public void on click ( dialog interface dialog @$ int id ) { map box tile source b = new map box tile source ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ ) ; b . set mapbox mapid ( user input box id . get text ( ) . to string ( ) ) ; b . set access token ( user input token . get text ( ) . to string ( ) ) ; m map view . set tile source ( b ) ; } } ) . set negative button ( __str__ @$ new dialog,bit deletes,fail,pre
start editing when a key is typed . ui classes can <PLACE_HOLDER> this behavior by setting the client property j table.auto starts edit to boolean.false .,if ( ! ret value && condition == when_ancestor_of_focused_component && is focus owner ( ) && ! boolean . false . equals ( get client property ( __str__ ) ) ) { component editor component = get editor component ( ) ; if ( editor component == null ) { if ( e == null || e . getid ( ) != key event . key_pressed ) { return false ; } int code = e . get key code ( ) ; if ( code == key event . vk_shift || code == key event . vk_control || code == key event . vk_alt ) { return false ; } int lead row = get selection model ( ) . get lead selection index ( ) ;,classes override,fail,pre
concurrent hash map does not <PLACE_HOLDER> synch . here,for ( abstract thread group thread group : groups ) { stopped all = stopped all && thread group . verify threads stopped ( ) ; } return stopped all ;,map need,success,pre
a @$ should <PLACE_HOLDER> b 's conflicting copy,clienta . down ( ) ; assert file list equals ( clienta . get local files exclude locked and no read ( ) @$ clientb . get local files exclude locked and no read ( ) ) ; assert sql database equals ( clienta . get database file ( ) @$ clientb . get database file ( ) ) ;,a get,fail,pre
this is tough for hotspot @$ but graal <PLACE_HOLDER> all allocations .,consume ( el ) ;,graal breakes,fail,pre
data <PLACE_HOLDER> one,final list < adapter item > data set one = new array list < > ( ) ; adapter item data set one0 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one1 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one2 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one3 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one4 = new adapter item ( __num__ @$ __str__ ) ; data set one . add ( data set one0 ) ; data set one . add ( data set one1 ) ; data set one . add ( data set one2 ) ; data set,data set,success,pre
case when 2 txn <PLACE_HOLDER> same source and keys,string key = __str__ ; string old val = __str__ ; map < string @$ string > key vals1 = new hash map < string @$ string > ( ) ; key vals1 . put ( key @$ old val ) ; map < string @$ string > key vals2 = new hash map < string @$ string > ( ) ; string new val = __str__ ; key vals2 . put ( key @$ new val ) ; list < transaction state . per source transactional update > old db updates = generate updates for schema2 ( source ids @$ key vals1 @$ scn ) ; list < transaction state . per source transactional update > new db updates = generate updates for schema2 ( source ids,txn have,success,pre
if the class is null @$ the driver <PLACE_HOLDER> no user code,if ( user code function type != null ) { this . stub = init stub ( user code function type ) ; },driver generates,fail,pre
the map will <PLACE_HOLDER> the request object,map < string @$ string > request = new hash map < > ( ) ; request . put ( __str__ @$ _sessionid ) ; int random = _server . get random ( __num__ ) ; int uri = random % __num__ ; boolean blocking = ( random / __num__ ) > __num__ ; int delay = ( blocking && uri % __num__ == __num__ ) ? random / __num__ : __num__ ; request . put ( __str__ @$ uri + __str__ ) ;,map contain,fail,pre
if the user 's <PLACE_HOLDER> a query @$ warn if they do n't allow conditions .,if ( null != query ) { if ( query . index of ( substitute_token ) == - __num__ ) { log . warn ( __str__ + substitute_token + __str__ + query + __str__ ) ; } },user specifying,fail,pre
destroy is called on each member . if the region destroy is successful on one member @$ we deem the destroy action successful @$ since if one member destroy successfully @$ the subsequent destroy on a another member would probably <PLACE_HOLDER> region destroyed exception,list < cli function result > results list = execute and get function result ( region destroy function . instance @$ region path @$ region members list ) ; result model result = result model . create member status result ( results list ) ; xml entity xml entity = find xml entity ( results list ) ;,destroy throw,success,pre
modern apps always <PLACE_HOLDER> densities .,application density = display metrics . density_device ; application scale = __num__ ; application inverted scale = __num__ ; final int expandable = __num__ ; final int large_screens = __num__ ; final int xlarge_screens = __num__ ; int size info = __num__ ;,apps report,fail,pre
create a simple rule which just <PLACE_HOLDER> a file .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; rule key input rule key = new rule key ( __str__ ) ; build rule rule = new failing input rule key build rule ( target @$ filesystem @$ params ) ; graph builder . add to index ( rule ) ;,which writes,success,pre
m total length <PLACE_HOLDER> the padding already,child left = get padding left ( ) + right - left - m total length ; break ; case gravity . center_horizontal :,length contains,success,pre
ensure id 1 is not blocking id 2 @$ and id 2 is <PLACE_HOLDER> id 1,twitter1 . destroy block ( rw private . id ) ; rw private message . create friendship ( id1 . id ) ; string message = __str__ + new date ( ) . to string ( ) ; direct message sent = twitter1 . send direct message ( rw private . id @$ message ) ; assert equals ( rw private . id @$ sent . get recipient id ( ) ) ; assert equals ( id1 . id @$ sent . get sender id ( ) ) ; assert equals ( message @$ sent . get text ( ) ) ; assert equals ( sent @$ twitter object factory . create direct message ( twitter object factory . get rawjson ( sent ) ) ) ;,id blocking,fail,pre
these should only <PLACE_HOLDER> the given table,session s = open session ( ) ; transaction t = s . begin transaction ( ) ; int count = s . create query ( __str__ ) . set string ( __str__ @$ __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; count = s . create query ( __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; t . commit ( ) ; s . close ( ) ; data . cleanup ( ) ;,these reference,fail,pre
node 0 <PLACE_HOLDER> 2 children,assert equals ( __num__ @$ node0 . get leaf count ( ) ) ;,node has,success,pre
the partition name field <PLACE_HOLDER> the table name .,_table name with type = message . get partition name ( ) ; _logger = logger factory . get logger ( _table name with type + __str__ + timeboundary refresh message handler . class ) ;,field contains,fail,pre
recursively <PLACE_HOLDER> methods for all superinterfaces .,for ( class doc superintf : intf . interfaces ( ) ) { if ( ! collect remote methods ( superintf @$ table ) ) { errors = true ; } } return ! errors ;,recursively collect,success,pre
somone <PLACE_HOLDER> this thread @$ behave as if timer cancelled,synchronized ( queue ) { new tasks may be scheduled = false ; queue . clear ( ) ; },somone killed,success,pre
create a number of consumers to read of the messages and start them with a handler which simply <PLACE_HOLDER> the message ids in a map and checks for a duplicate,for ( int i = __num__ ; i < consumer_count ; i ++ ) { receivers [ i ] = new threaded message receiver ( test_queue_name @$ new i message handler ( ) { @ override public void on message ( message message ) throws exception { synchronized ( lock ) { int current = message count . increment and get ( ) ; if ( current % __num__ == __num__ ) { logger . info ( __str__ + message . getjms messageid ( ) + __str__ + ( ( text message ) message ) . get text ( ) ) ; } if ( messages . contains key ( message . getjms messageid ( ) ) ) { duplicate signal . count down ( ) ; logger,which holds,fail,pre
verify that the text field <PLACE_HOLDER> the min address since the current offset is 0,assert equals ( program . get min address ( ) @$ ai . get address ( ) ) ; run swing ( ( ) -> ai . set value ( __str__ ) ) ; press button by text ( d . get component ( ) @$ __str__ ) ;,field has,success,pre
the following will <PLACE_HOLDER> an entry into the spark preferences window,preference mypreference = new otr preferences ( ) ; spark manager . get preference manager ( ) . add preference ( mypreference ) ;,following add,success,pre
we <PLACE_HOLDER> a previous notion of who <PLACE_HOLDER> focus . clear it .,if ( m focused != child ) { if ( m focused != null ) { m focused . un focus ( focused ) ; } m focused = child ; } if ( m parent != null ) { m parent . request child focus ( this @$ focused ) ; },who had,success,pre
if application was stopped and execution services did not <PLACE_HOLDER> termination @$ these codes will be executed .,platform . run later ( ( ) -> { if ( controllers . get stage ( ) != null ) { controllers . get stage ( ) . close ( ) ; emit status ( loading state . done ) ; } } ) ;,services finish,success,pre
should trigger <PLACE_HOLDER> stealing of <PLACE_HOLDER>er 5,worker1 . dispose ( ) ;,trigger p,fail,pre
we documented that using distance in the where clause <PLACE_HOLDER> the index which is n't precise so treating lte & lt the same should be acceptable,switch ( parent operator name ) { case lte operator . name : case lt operator . name : return lat lon point . new distance query ( column name @$ lon lat . gety ( ) @$ lon lat . getx ( ) @$ distance ) ; case gte operator . name : if ( distance - geo utils . tolerance <= __num__ ) { return queries . new match all query ( ) ; } case gt operator . name : return queries . not ( lat lon point . new distance query ( column name @$ lon lat . gety ( ) @$ lon lat . getx ( ) @$ distance ) ) ; case eq operator . name : return eq distance ( parent,clause determines,fail,pre
first do a sanity check and see if this address <PLACE_HOLDER> any conflicts .,if ( ! has conflict ( addr ) ) { return ; } monitor . set message ( __str__ ) ; boolean ask user = chosen conflict option == ask_user ;,address has,success,pre
if the current cluster does n't <PLACE_HOLDER> the node @$ fallback to something machine local and then rack local .,list < node > rack nodes = get network topology ( ) . get datanodes in rack ( network location ) ; if ( rack nodes != null ) { for ( node rack node : rack nodes ) { if ( ( ( datanode descriptor ) rack node ) . get ip addr ( ) . equals ( host ) ) { node = ( datanode descriptor ) rack node ; break ; } } if ( node == null && ! rack nodes . is empty ( ) ) { node = ( datanode descriptor ) ( rack nodes . get ( thread local random . current ( ) . next int ( rack nodes . size ( ) ) ) ) ; } },cluster contain,success,pre
check that the job name <PLACE_HOLDER> only allowed characters,if ( ! name_version_pattern . matcher ( job id name ) . matches ( ) ) { errors . add ( format ( __str__ @$ recomputed id . get name ( ) ) ) ; },name contains,success,pre
ensure comments are stripped . these <PLACE_HOLDER> syntax errors if not stripped .,final source file src file1 = source file . from code ( __str__ @$ line_joiner . join ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,these cause,success,pre
free and that values can move past them . we do n't need to be concerned with exposing the getter or setter here but the decomposer does not <PLACE_HOLDER> a method of exposing properties @$ only variables .,helper move expression ( __str__ @$ __str__ @$ __str__ ) ; helper move expression ( __str__ @$ __str__ @$ __str__ ) ;,decomposer have,success,pre
this collation implies that this serial aggregate <PLACE_HOLDER> its input to be sorted in an order that is one of permutations of the fields from this collation,immutable bit set group by = aggr . get group set ( ) ; list < rel data type field > row type list = aggr . get row type ( ) . get field list ( ) ; list < rel field collation > collation fields = new array list < > ( ) ; for ( int index = group by . next set bit ( __num__ ) ; index != - __num__ ; index = group by . next set bit ( index + __num__ ) ) { preconditions . check state ( index < row type list . size ( ) ) ; collation fields . add ( new rel field collation ( index ) ) ; } return rel collations . of (,aggregate needs,fail,pre
p 1 has primary key index on column a : group by c should not <PLACE_HOLDER> its index to speed up .,check seq scan ( pn @$ __str__ @$ __str__ @$ __str__ ) ; assert not null ( aggregate plan node . get inline aggregation node ( pn ) ) ; assert null ( pn . get inline plan node ( plan node type . limit ) ) ;,group change,fail,pre
check only for maximum @$ that 's enough because default ca n't <PLACE_HOLDER> maximum,if ( maximum application lifetime <= __num__ ) { return ( lifetime requested by app <= __num__ ) ? default application lifetime : lifetime requested by app ; } if ( lifetime requested by app <= __num__ ) { return default application lifetime ; } else if ( lifetime requested by app > maximum application lifetime ) { return maximum application lifetime ; } return lifetime requested by app ; read lock . unlock ( ) ;,default exceed,success,pre
if this is a bulk op @$ and concurrency checks are enabled @$ we need to save the version tag in case we retry . make record bulk op version tag after record sequence number @$ so that record bulk op start in a retry bulk op would not incorrectly <PLACE_HOLDER> the saved version tag in recorded bulk op version tags,if ( lr . get concurrency checks enabled ( ) && ( event . get operation ( ) . is put all ( ) || event . get operation ( ) . is remove all ( ) ) && lr . get server proxy ( ) == null ) { record bulk op event ( event @$ membershipid ) ; },start set,fail,pre
be aware the legacy parser is not <PLACE_HOLDER> truncate events,parser . signal truncate table ( table id @$ ctx ) ; super . enter truncate table ( ctx ) ;,parser receiving,fail,pre
form should not <PLACE_HOLDER> a description tooltip,check tooltip ( $ ( form element . class ) . first ( ) @$ null ) ;,form have,success,pre
for some reason ` <PLACE_HOLDER> 0 type ` and ` <PLACE_HOLDER> 1 type ` are n't equal . this is good @$ but unexpected .,assert that ( test1 type ) . is not same instance as ( test0 type ) ;,reason test,success,pre
end of the post <PLACE_HOLDER> event .,trace . trace end ( trace . trace_tag_activity_manager ) ; if ( parsed args . m invoke with != null ) { wrapper init . exec application ( parsed args . m invoke with @$ parsed args . m nice name @$ parsed args . m target sdk version @$ vm runtime . get current instruction set ( ) @$ pipe fd @$ parsed args . m remaining args ) ; throw new illegal state exception ( __str__ ) ; } else { if ( ! is zygote ) { return zygote init . zygote init ( parsed args . m target sdk version @$ parsed args . m remaining args @$ null ) ; } else { return zygote init . child zygote init ( parsed args,end execute,fail,pre
check that a field of type string does not <PLACE_HOLDER> booleans,try { completed form . set answer ( __str__ @$ true ) ; fail ( __str__ ) ; } catch ( illegal argument exception e ) { },field allow,fail,pre
make sure the subject <PLACE_HOLDER> a principal,assert false ( client principals . is empty ( ) ) ;,subject has,success,pre
to do : handle if user does n't <PLACE_HOLDER> k best,if ( flags . usek best ) { int k = flags . k best ; crf . classify and write answersk best ( test file @$ k @$ reader and writer ) ; } else if ( flags . print label value ) { crf . print label information ( test file @$ reader and writer ) ; } else { log . info ( __str__ ) ; crf . classify and write answers ( test file @$ reader and writer @$ true ) ; },user specify,fail,pre
allow that balance is not exact . fyi @$ get region server threads does not <PLACE_HOLDER> master thread though it is a regionserver so we have to check master and then below the regionservers .,for ( jvm cluster util . region server thread rst : cluster . get region server threads ( ) ) { regions = rst . get region server ( ) . get regions ( ) ; int rs actual count = regions . size ( ) ; check count ( rs actual count @$ rs count ) ; } h master old master = cluster . get master ( ) ; cluster . kill master ( old master . get server name ( ) ) ; old master . join ( ) ; while ( cluster . get master ( ) == null || cluster . get master ( ) . get server name ( ) . equals ( old master . get server name ( ) ),threads have,fail,pre
if no restrictions were saved @$ m user manager.get application restrictions <PLACE_HOLDER>s null @$ but dpm method should <PLACE_HOLDER> an empty bundle as per java doc,return bundle != null ? bundle : bundle . empty ; m injector . binder restore calling identity ( id ) ;,method return,success,pre
partition <PLACE_HOLDER> 1 from <PLACE_HOLDER> 2 .,partition ( member1 @$ member2 ) ;,partition part,fail,pre
this test case <PLACE_HOLDER> a relation plan where multiple positions share the same symbol,assert query ( __str__ ) ;,case generates,success,pre
extended relocations <PLACE_HOLDER> an <PLACE_HOLDER>ition entry,if ( nreloc > __num__ ) { nreloc ++ ; } file offset += ( nreloc * image_relocation . totalsize ) ;,relocations add,success,pre
schedule an event immediately to <PLACE_HOLDER> the delays are being honored .,handler . post ( new runnable ( ) { @ override public void run ( ) { events . add ( __str__ ) ; } } ) ; assert true ( latch . await ( __num__ @$ seconds ) ) ; assert equals ( as list ( __str__ @$ __str__ ) @$ events ) ;,event ensure,success,pre
also the follower may have just sent a leader check @$ which <PLACE_HOLDER> no response,cluster . stabilise ( math . max ( default millis ( follower_check_timeout_setting ) + default millis ( follower_check_interval_setting ) + default_delay_variability + default_cluster_state_update_delay + default_cluster_state_update_delay @$ default millis ( leader_check_timeout_setting ) + default millis ( leader_check_interval_setting ) + default_delay_variability ) ) ;,which receives,success,pre
authorization header must <PLACE_HOLDER> a payload,if ( auth header base64 string == null || auth header base64 string . is empty ( ) ) { throw new http authentication exception ( __str__ + __str__ ) ; } return auth header base64 string ;,header have,success,pre
finally @$ the default overall <PLACE_HOLDER> platform .,return toolchain . get default platform ( ) ;,default supported,fail,pre
validate that other vm no longer <PLACE_HOLDER> colocated buckets,host . get host ( __num__ ) . getvm ( othervm ) . invoke ( new serializable runnable ( ) { @ override public void run ( ) { for ( int i = __num__ ; i < region path . length ; i ++ ) { partitioned region pr = ( partitioned region ) get cache ( ) . get region ( region path [ i ] ) ; bucket bucket = pr . get region advisor ( ) . get bucket ( __num__ ) ; assert false ( __str__ @$ bucket . is hosting ( ) ) ; bucket region bucket region = bucket . get bucket advisor ( ) . get proxy bucket region ( ) . get hosted bucket region ( ) ; assert,vm hosts,success,pre
call into chatty <PLACE_HOLDER> test case to get the nice formatting .,if ( ! actual . starts with ( expected ) ) { assert that ( actual ) . is equal to ( expected ) ; },call join,fail,pre
no reports right after we <PLACE_HOLDER> this table .,assert equals ( __num__ @$ get region reports for table ( quota manager . snapshot region sizes ( ) @$ tn ) ) ;,reports read,fail,pre
we need to strip 'delete ' or else jpa operations.delete will <PLACE_HOLDER> the wrong query,string delete query string = query string . substring ( __str__ . length ( ) ) ; result handle delete count ; if ( use named params ) { result handle parameters = generate parameters object ( named parameter to index @$ method creator ) ; delete count = method creator . invoke static method ( method descriptor . of method ( jpa operations . class @$ __str__ @$ long . class @$ class . class @$ string . class @$ parameters . class ) @$ method creator . read instance field ( entity class field descriptor @$ method creator . get this ( ) ) @$ method creator . load ( delete query string ) @$ parameters ) ; } else { result handle params array =,'delete return,fail,pre
by default @$ we should see no restriction ; the unicode set should <PLACE_HOLDER> all characters .,set = sc . get allowed chars ( ) ; tmp set = new unicode set ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ tmp set @$ set ) ;,set contain,fail,pre
if the context url <PLACE_HOLDER> a different protocol @$ discard it because we ca n't use it .,if ( protocol != null && context != null && ! protocol . equals ( context . protocol ) ) { context = null ; },url has,success,pre
note that chunk size only <PLACE_HOLDER> chunk size in case we actually save chunks otherwise there is a risk file and chunks are not compatible,if ( ! saved chunks ) { try { save chunks ( chunk size ) ; } catch ( io exception ioe ) { throw new mongo exception ( __str__ @$ ioe ) ; } } super . save ( ) ;,size compares,fail,pre
open a connection @$ thus we can not <PLACE_HOLDER> them here !,key chain . choose private key alias ( m activity @$ new key chain alias callback ( ) { @ override public void alias ( string alias ) { timber . d ( __str__ @$ alias ) ; set alias ( alias ) ; } } @$ null @$ null @$ null @$ - __num__ @$ get alias ( ) ) ;,connection call,fail,pre
the above method will <PLACE_HOLDER> the open as long as the user does not cancel the request,if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . save_policy_file ) == __num__ ) { string filename = ( ( j text field ) tw . get component ( tool window . mw_filename_textfield ) ) . get text ( ) ; if ( filename == null || filename . length ( ) == __num__ ) { tool dialog td = new tool dialog ( policy tool . get message ( __str__ ) @$ tool @$ tw @$ true ) ; td . display save as dialog ( tool dialog . noaction ) ; } else { try { tool . save policy ( filename ) ; message format form = new message format ( policy tool .,method perform,success,pre
the test is just checking to make sure thaat the producer and consumer does not hang due to the network hops take to route the message <PLACE_HOLDER> the producer to the consumer .,assert true ( __str__ @$ r > __num__ ) ; assert true ( __str__ @$ p > __num__ ) ;,message reach,fail,pre
the following 'deltas ' <PLACE_HOLDER> all kinds of delta files including insert & delete deltas .,final list < parsed delta > deltas = new array list < parsed delta > ( ) ; list < parsed delta > working = new array list < parsed delta > ( ) ; list < path > original directories = new array list < > ( ) ; final list < path > obsolete = new array list < > ( ) ; final list < path > aborted directories = new array list < > ( ) ; list < hdfs file status with id > children with id = try list located hdfs status ( use file ids @$ fs @$ candidate directory ) ; txn base best base = new txn base ( ) ; final list < hdfs file status with id,'deltas contains,fail,pre
rename tables to make them <PLACE_HOLDER> the filter and enable acid tables .,primary . run ( __str__ + primary db name ) . run ( __str__ ) . run ( __str__ ) . run ( __str__ ) ; dump with clause = arrays . as list ( __str__ + hive conf . conf vars . repl_bootstrap_acid_tables . varname + __str__ @$ __str__ + repl utils . repl_dump_include_acid_tables + __str__ ) ; replicated tables = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; bootstrap tables = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } ; replicate and verify ( repl policy @$ null @$ last repl id @$ dump with clause @$ null @$ bootstrap tables @$ replicated tables ) ;,them satisfy,success,pre
if the left index has not <PLACE_HOLDER> the right side of array must now sort the right partition .,if ( lo <= hi0 ) quick sort ( lo @$ hi0 @$ key @$ is ascending ) ;,index reached,success,pre
processor <PLACE_HOLDER> capabilities,capabilities . clear ( ) ; capabilities . add ( __str__ ) ; sethms client ( __str__ @$ ( string [ ] ) ( capabilities . to array ( new string [ __num__ ] ) ) ) ; parts = client . get partitions by names ( db name @$ tbl name @$ part values @$ false @$ null ) ; for ( partition part : parts ) { assert equals ( __str__ @$ - __num__ @$ part . get sd ( ) . get num buckets ( ) ) ; } tbl name = __str__ ; properties = new string builder ( ) ; properties . append ( __str__ ) ; properties . append ( __str__ ) ; properties . append ( capabilities_key ) . append (,processor related,fail,pre
now format first <PLACE_HOLDER> and copy the storage directory from that node to the others .,int nn index = nn counter ; collection < uri > prevnn dirs = null ; for ( nn conf nn : nameservice . getn ns ( ) ) { init name node conf ( conf @$ ns id @$ ns counter @$ nn . get nn id ( ) @$ manage name dfs dirs @$ manage name dfs dirs @$ nn index ) ; collection < uri > namespace dirs = fs namesystem . get namespace dirs ( conf ) ; if ( format ) { for ( uri name dir uri : namespace dirs ) { file name dir = new file ( name dir uri ) ; if ( name dir . exists ( ) && ! file util . fully delete ( name dir,format create,fail,pre
ensure an empty file is created if there is no string map . this avoids confusing some <PLACE_HOLDER> tools that expect to see the file @$ even if it is empty .,if ( compiler . get string map ( ) == null ) { if ( ! ( new file ( config . string map output path ) . create new file ( ) ) ) { throw new io exception ( __str__ + config . string map output path ) ; } } else { compiler . get string map ( ) . save ( config . string map output path ) ; },some build,success,pre
verify that just changing the <PLACE_HOLDER> rule rule key changes the calculated rule key .,assert not equals ( build result ( create builder ( graph builder ) . set reflectively ( __str__ @$ explicit build target source path . of ( fake1 . get build target ( ) @$ paths . get ( __str__ ) ) ) ) @$ build result ( create builder ( graph builder ) . set reflectively ( __str__ @$ explicit build target source path . of ( fake2 . get build target ( ) @$ paths . get ( __str__ ) ) ) ) ) ;,the build,success,pre
validate that the first savepoint does not <PLACE_HOLDER> its private states .,verify ( subtask state1 @$ never ( ) ) . discard state ( ) ; verify ( subtask state2 @$ never ( ) ) . discard state ( ) ;,savepoint contain,fail,pre
test empty group id and use site url that causes <PLACE_HOLDER> syntax exception,url = __str__ ; mockito . when ( mock result . get site url ( ) ) . then return ( url ) ; mockito . when ( mock result . get group id ( ) ) . then return ( __str__ ) ; assert . assert equals ( url @$ utils . get group id ( mock result ) ) ;,causes null,fail,pre
validate property <PLACE_HOLDER> key that can not be resolved,object [ ] [ ] input = { { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,property has,success,pre
list <PLACE_HOLDER> an object from another thread .,list . add ( obj from another thread . get ( ) ) ;,list contains,success,pre
inner loop <PLACE_HOLDER> all 9 compression levels .,for ( int i = __num__ ; i <= __num__ ; i ++ ) { system . out . println ( __str__ + j + __str__ + i + __str__ ) ; byte [ ] zipped = new byte [ __num__ * data_size ] ; deflater deflater = new deflater ( i ) ; deflater . set input ( input ) ; deflater . finish ( ) ; deflater . deflate ( zipped ) ; deflater . end ( ) ; byte [ ] output = new byte [ data_size ] ; inflater inflater = new inflater ( ) ; inflater . set input ( zipped ) ; inflater . finished ( ) ; inflater . inflate ( output ) ; inflater . end ( ) ; assert,loop do,fail,pre
verify that interim reasons mask <PLACE_HOLDER> one or more reasons not included in the reasons mask,boolean one or more = false ; for ( int i = __num__ ; i < interim reasons mask . length && ! one or more ; i ++ ) { if ( interim reasons mask [ i ] && ! ( i < reasons mask . length && reasons mask [ i ] ) ) { one or more = true ; } } if ( ! one or more ) { return false ; },mask includes,success,pre
if an user <PLACE_HOLDER> customizations for projects he perhaps just <PLACE_HOLDER> the key value for project without a name but the code expects a name for the project . therefore we fill the name according to the project key which is the same .,for ( entry < string @$ project > entry : cfg . get projects ( ) . entry set ( ) ) { if ( entry . get value ( ) . get name ( ) == null ) { entry . get value ( ) . set name ( entry . get key ( ) ) ; } },user change,fail,pre
if scope is limited to this iterator @$ then do n't <PLACE_HOLDER> any more iterators in this scope,if ( scope . get limit ( ) == itr ) { continue next_scope ; },then offer,fail,pre
keep first row in deduplicate will not <PLACE_HOLDER> retraction,list < object > expected output = new array list < > ( ) ; expected output . add ( record ( __str__ @$ __num__ @$ __num__ ) ) ; expected output . add ( record ( __str__ @$ __num__ @$ __num__ ) ) ; assertor . assert output equals sorted ( __str__ @$ expected output @$ test harness . get output ( ) ) ; test harness . close ( ) ;,row send,success,pre
regular expression <PLACE_HOLDER> boxes .,assert button state ( __str__ @$ true @$ false ) ; @ suppress warnings ( __str__ ) j combo box < charset > combo box = ( j combo box < charset > ) find component by name ( pane @$ __str__ ) ; assert not null ( combo box ) ;,expression exported,fail,pre
reset namenode backup address because windows does not <PLACE_HOLDER> port used previously properly .,backup_config . set ( dfs config keys . dfs_namenode_backup_address_key @$ this_host ) ;,windows manage,fail,pre
if a meta key is active but the lookup with the meta key did not <PLACE_HOLDER> anything @$ try some other meta keys @$ because the user might have pressed shift when they meant alt @$ or vice versa .,if ( meta != __num__ ) { key data kd = new key data ( ) ; char [ ] accepted = get accepted chars ( ) ; if ( event . get key data ( kd ) ) { for ( int i = __num__ ; i < kd . meta . length ; i ++ ) { if ( ok ( accepted @$ kd . meta [ i ] ) ) { return kd . meta [ i ] ; } } } },lookup yield,fail,pre
set actions should <PLACE_HOLDER> the previous actions @$ so we call clear here .,if ( assigned != null && assigned . length > __num__ ) { actions . clear ( ) ; actions . add all ( arrays . as list ( assigned ) ) ; },actions overwrite,fail,pre
compiler options also <PLACE_HOLDER> types @$ but uses preconditions and therefore wo n't generate a useful exception .,if ( flags . defines != null ) { validate primitive types ( flags . defines ) ; options . set define replacements ( to map ( flags . defines ) ) ; },options require,fail,pre
zipkin did n't <PLACE_HOLDER> the spans @$ as they should n't have been readable @$ due to the error,assert that ( zipkin . get traces ( ) ) . is empty ( ) ;,zipkin store,success,pre
still an override might <PLACE_HOLDER> the content of the repository .,if ( rule == null ) { repository delegator function . repository_overrides . get ( env ) ; return ; },override change,success,pre
yes @$ we are assuming that test queries do n't <PLACE_HOLDER> quoted question marks .,int param count = string utils . count matches ( sql @$ __str__ ) ; boolean m_infer = m_by default infer partitioning ; boolean m_forcesp = m_by default infer partitioning ; m_by default infer partitioning = false ; m_by default plan for single partition = true ; abstract plan node pn = compilesp with join order ( sql @$ param count @$ null ) ; m_by default infer partitioning = m_infer ; m_by default plan for single partition = m_forcesp ; return pn ;,queries contain,success,pre
the last 4 bytes of the file <PLACE_HOLDER> the major and minor version universally,baos dos . write int ( materialize version ( major version @$ minor version ) ) ; baos . write to ( output stream ) ;,bytes encode,success,pre
compute <PLACE_HOLDER>s of point in box <PLACE_HOLDER> system,temp vars vars = temp vars . get ( ) ; vector3f closest = vars . vect1 ; point . subtract ( center @$ closest ) ;,coordinates bounds,fail,pre
available everywhere . let 's <PLACE_HOLDER> different package .,super public . public method ( ) ;,'s use,fail,pre
will call <PLACE_HOLDER> blob method on the underlying connection,method m = connection . class . get method ( __str__ @$ new class [ ] { } ) ;,call get,fail,pre
in role editor each object may <PLACE_HOLDER> different privilege set,if ( is role editor ( ) ) { permission table . remove all ( ) ; if ( ! common utils . is empty ( objects ) ) { class < ? > object type = objects . get ( __num__ ) . get class ( ) ; for ( postgre privilege type pt : postgre privilege type . values ( ) ) { if ( ! pt . is valid ( ) || ! pt . supports type ( object type ) ) { continue ; } table item priv item = new table item ( permission table @$ swt . left ) ; priv item . set text ( __num__ @$ pt . name ( ) ) ; priv item . set data ( pt,object have,success,pre
we know check if both member <PLACE_HOLDER> events,assert equals ( __str__ + __str__ @$ __num__ @$ op set1 room collector . collected events . size ( ) ) ; assert equals ( __str__ + __str__ @$ __num__ @$ op set2 room collector . collected events . size ( ) ) ; chat room member presence change event member event = ( chat room member presence change event ) op set1 room collector . collected events . get ( __num__ ) ; assert equals ( __str__ @$ chat room member presence change event . member_joined @$ member event . get event type ( ) ) ; assert equals ( __str__ @$ fixture . userid2 @$ member event . get chat room member ( ) . get contact address ( ) ) ; assert equals ( __str__,member received,success,pre
verify that this connection <PLACE_HOLDER> modification,connection . verify can update ( ) ;,connection performs,fail,pre
inspect the memstore contents to see whether the memstore <PLACE_HOLDER> only edits with seq id smaller than the flush seq id . if so @$ we can discard those edits .,drop mem store contents for seq id ( flush . get flush sequence number ( ) @$ null ) ;,memstore contains,success,pre
illegal access error is expected note : logback <PLACE_HOLDER> context right after shutdown initiated . it is problematic to see log output system out could help,logger . warn ( __str__ + e . get message ( ) ) ;,logback stores,fail,pre
the replica sets <PLACE_HOLDER> different names ...,if ( ! this . replica sets by name . key set ( ) . equals ( prior state . replica sets by name . key set ( ) ) ) { return true ; },sets have,success,pre
test that toggling does n't <PLACE_HOLDER> the listener .,parole listener . rearm latch ( ) ; set app idle enabled ( m controller @$ true ) ; parole listener . await on latch ( stable_charging_threshold * __num__ / __num__ ) ; assert true ( parole listener . m on parole ) ; assert equals ( last update time @$ parole listener . get last parole change time ( ) ) ; parole listener . rearm latch ( ) ; set app idle enabled ( m controller @$ false ) ; parole listener . await on latch ( stable_charging_threshold * __num__ / __num__ ) ; assert true ( parole listener . m on parole ) ; assert equals ( last update time @$ parole listener . get last parole change time ( ) ) ;,toggling change,fail,pre
both output pages must <PLACE_HOLDER> the same dictionary,assert equals ( dictionary block2 . get dictionary ( ) @$ dictionary block . get dictionary ( ) ) ;,pages have,success,pre
use properties <PLACE_HOLDER> attribute,use properties file = false ; properties file = __str__ ;,properties file,success,pre
check if first column <PLACE_HOLDER> 0,if ( first col zero ) { for ( int i = __num__ ; i < matrix . length ; ++ i ) { matrix [ i ] [ __num__ ] = __num__ ; } },column has,fail,pre
ensure process owner <PLACE_HOLDER> queued command directory,if ( ! cmd dir . exists ( ) ) { cmd dir . mkdir ( ) ; return ; },owner creates,success,pre
inline fragments may not <PLACE_HOLDER> a type condition,string type condition = type name == null ? __str__ : wrap ( __str__ @$ type ( type name ) @$ __str__ ) ; string directives = directives ( node . get directives ( ) ) ; string selection set = node ( node . get selection set ( ) ) ; out . printf ( __str__ @$ comments ( node ) ) ; out . printf ( __str__ @$ spaced ( __str__ @$ type condition @$ directives @$ selection set ) ) ;,fragments have,success,pre
nested groupby only <PLACE_HOLDER> time condition for inner most query,test query ( planner_config_require_time_condition @$ __str__ + __str__ + __str__ + __str__ @$ calcite tests . regular_user_auth_result @$ immutable list . of ( group by query . builder ( ) . set data source ( new query data source ( group by query . builder ( ) . set data source ( calcite tests . datasource1 ) . set interval ( query segment spec ( intervals . utc ( date times . of ( __str__ ) . get millis ( ) @$ joda utils . max_instant ) ) ) . set granularity ( granularities . all ) . set dimensions ( dimensions ( new default dimension spec ( __str__ @$ __str__ ) ) ) . set aggregator specs ( aggregators ( new long sum aggregator factory ( __str__,groupby requires,success,pre
no more room to display the comments below ; do n't <PLACE_HOLDER> them,if ( total comments found > max display lines ) { return ; },comments show,fail,pre
the test table <PLACE_HOLDER> 500 rows @$ so total query time should be ~ 2500 ms,try { stmt . execute query ( __str__ + __str__ + table name + __str__ + table name + __str__ ) ; fail ( __str__ ) ; } catch ( sql timeout exception e ) { assert not null ( e ) ; system . err . println ( e . to string ( ) ) ; } catch ( sql exception e ) { fail ( __str__ + e ) ; e . print stack trace ( ) ; },table has,success,pre
if user <PLACE_HOLDER> the name of the project manually and leaves off the extension @$ try to open or create using the extension,if ( ! create && filename . last index of ( __str__ ) > path . last index of ( file . separator ) ) { msg . show error ( get class ( ) @$ tool . get tool frame ( ) @$ __str__ @$ __str__ + file . get name ( ) + __str__ ) ; continue ; },user sets,fail,pre
optional union field @$ field value is <PLACE_HOLDER> any record,object [ ] [ ] inputs = { { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ + __str__ @$ __str__ + __str__ @$ __str__ + __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__,field missing,fail,pre
need constantly invalidate view in order to get max <PLACE_HOLDER> rate .,m layout . get view tree observer ( ) . add on pre draw listener ( this ) ;,max enter,fail,pre
rollback should n't be called before start container @$ otherwise @$ node manager can not <PLACE_HOLDER> the container,try { client . rollback last re initialization ( container . get id ( ) ) ; fail ( __str__ ) ; } catch ( yarn exception e ) { assert true ( __str__ @$ e . get message ( ) . contains ( __str__ ) ) ; },rollback find,success,pre
the dummy protocol suite <PLACE_HOLDER> the nice property that it can be run by just one player .,int no of parties = __num__ ; run test ( f @$ eval strategy @$ log performance @$ no of parties ) ;,suite has,success,pre
we want to avoid looking into the future . so @$ if the cells of the operation specify a timestamp @$ or the operation itself <PLACE_HOLDER> a timestamp @$ then we use the maximum ts found . otherwise @$ we bound the get to the current server time . we add 1 to the timerange since the upper bound of a timerange is exclusive,long latest ts = math . max ( op ts @$ latest cell ts ) ; if ( latest ts == __num__ || latest ts == h constants . latest_timestamp ) { latest ts = environment edge manager . current time ( ) ; } get . set time range ( __num__ @$ latest ts + __num__ ) ;,itself specifies,success,pre
store <PLACE_HOLDER> default ssl context to restore after test .,final ssl context dflt ssl ctx = ssl context . get default ( ) ;,store defaulted,fail,pre
if we could n't find the region because the cache is <PLACE_HOLDER> @$ throw a cache <PLACE_HOLDER> exception,if ( rgn == null ) { if ( cache . is closed ( ) ) { throw new cache closed exception ( ) ; } throw new region not found exception ( string . format ( __str__ @$ this . region path ) ) ; },cache closed,success,pre
some problems in key conversion @$ so the params do not <PLACE_HOLDER> the key types,continue ;,params affect,fail,pre
seed <PLACE_HOLDER>h of the target peer @$ needed for network stability check if we are the right target and requester <PLACE_HOLDER> correct information about this peer,if ( ( sb . peers . my seed ( ) == null ) || ( ! ( sb . peers . my seed ( ) . hash . equals ( youare ) ) ) ) { return prop ; },hash has,success,pre
the super implementation does not <PLACE_HOLDER> the following parameters,identification protocol = params . get endpoint identification algorithm ( ) ; algorithm constraints = params . get algorithm constraints ( ) ; prefer local cipher suites = params . get use cipher suites order ( ) ; collection < sni matcher > matchers = params . getsni matchers ( ) ; if ( matchers != null ) { sni matchers = params . getsni matchers ( ) ; },implementation handle,success,pre
reset pig stats @$ otherwise you may <PLACE_HOLDER> the pig stats of last job in the same thread because pig stats is thread local variable,pig stats . start ( pig server . get pig context ( ) . get execution engine ( ) . instantiate pig stats ( ) ) ; pig script listener script listener = new pig script listener ( ) ; script state . get ( ) . register listener ( script listener ) ; listener map . put ( context . get paragraph id ( ) @$ script listener ) ; pig server . register script ( tmp script file . get absolute path ( ) ) ; schema schema = pig server . dump schema ( alias ) ; boolean schema known = ( schema != null ) ; if ( schema known ) { for ( int i = __num__ ; i < schema . size,stats get,success,pre
binding is used to group related field <PLACE_HOLDER>ters together . it is essential for action insert fact col and action <PLACE_HOLDER> field col 52 columns as these represent single fields and need to be grouped together it is not essential for i action 's as these contain their own list of fields . if a brl fragment does not <PLACE_HOLDER> the binding use a,if ( binding == null ) { binding = action . to string ( ) ; } final labelled action a = new labelled action ( ) ; a . bound name = binding ; a . action = action ; a . is update = is update ; actions . add ( a ) ;,fragment adjust,fail,pre
cnxns typically <PLACE_HOLDER> many watches @$ so use default cap here,if ( paths == null ) { paths = new hash set < > ( ) ; watch2 paths . put ( watcher @$ paths ) ; },cnxns have,success,pre
this code is only synchronously <PLACE_HOLDER> a single native method to trigger and asynchronous sync cycle @$ so 5 minutes is generous .,try { if ( ! semaphore . try acquire ( __num__ @$ time unit . minutes ) ) { log . w ( tag @$ __str__ ) ; sync result . stats . num io exceptions ++ ; } } catch ( interrupted exception e ) { log . w ( tag @$ __str__ @$ e ) ; sync result . stats . num io exceptions ++ ; },code requiring,fail,pre
the line <PLACE_HOLDER> the regular expression .,string class name = null ; string source file = null ; int line number = __num__ ; string type = null ; string field name = null ; string method name = null ; string arguments = null ;,line contains,fail,pre
make sure browser context menu does not <PLACE_HOLDER> the test,get command executor ( ) . execute script ( __str__ @$ e @$ x @$ y ) ; new actions ( get driver ( ) ) . move to element ( e @$ getx offset ( e @$ x coord ) @$ gety offset ( e @$ y coord ) ) . context click ( ) . move by offset ( - __num__ @$ - __num__ ) . click ( ) . perform ( ) ;,menu break,fail,pre
could <PLACE_HOLDER> comparing exact message @$ but since it 's informational <PLACE_HOLDER> looser :,if ( ! desc . contains ( __str__ ) ) { fail ( __str__ + desc ) ; } if ( ! desc . contains ( __str__ ) ) { fail ( __str__ + desc ) ; },informational do,fail,pre
some <PLACE_HOLDER> local properties @$ which are matched exactly,global properties gp = new global properties ( ) ; local properties lp = local properties . for grouping ( new field list ( __num__ @$ __num__ ) ) ; requested local properties req lp = new requested local properties ( ) ; req lp . set grouped fields ( new field list ( __num__ @$ __num__ ) ) ; to map1 . set required global props ( null ) ; to map1 . set required local props ( req lp ) ; to map2 . set required global props ( null ) ; to map2 . set required local props ( null ) ; feedback properties meet requirements report report = map2 . check partial solution properties met ( target @$ gp @$ lp ) ; assert,some required,success,pre
export the data @$ which <PLACE_HOLDER> a file chooser to be shown,execute on swing without blocking ( ( ) -> key binding utils . export key bindings ( options ) ) ; file selected file = find and test file chooser ( null @$ test_filename ) ; return selected file ;,which causes,success,pre
calculator corner <PLACE_HOLDER> size,float corner radius top left ; float corner radius top right ; float corner radius bottoml right ; float corner radius bottom left ; if ( skeleton attribute child . get corner radius ( ) != integer . min_value ) { corner radius top left = corner radius top right = corner radius bottoml right = corner radius bottom left = get corner radius ( rectangle rect @$ skeleton attribute child . get corner radius ( ) ) ; } else { corner radius top left = skeleton attribute child . get corner radius top left ( ) != integer . min_value ? get corner radius ( rectangle rect @$ skeleton attribute child . get corner radius top left ( ) ) : __num__ ; corner radius top,corner set,fail,pre
preserve any localizations already associated with the video . if the video does not <PLACE_HOLDER> any localizations @$ create a new array . append the provided localization to the list of localizations associated with the video .,map < string @$ video localization > localizations = video . get localizations ( ) ; if ( localizations == null ) { localizations = new array map < string @$ video localization > ( ) ; video . set localizations ( localizations ) ; } video localization video localization = new video localization ( ) ; video localization . set title ( title ) ; video localization . set description ( description ) ; localizations . put ( language @$ video localization ) ;,video have,success,pre
work correctly @$ so let 's <PLACE_HOLDER> problem right away,java type value type = _target type ;,let report,fail,pre
stack trace of the cause should <PLACE_HOLDER> three frames : test error do fn.nested function beta test error do fn.nested function alpha test error do fn.start bundle,assert that ( stack trace frame strings ( exn . get cause ( ) ) @$ contains ( contains string ( __str__ ) @$ contains string ( __str__ ) @$ contains string ( __str__ ) ) ) ; assert that ( exn . to string ( ) @$ contains string ( __str__ ) ) ;,trace contain,success,pre
starting a pulse while docking should <PLACE_HOLDER> wakeup gesture,m status bar . m doze service host . pulse while dozing ( mock ( doze host . pulse callback . class ) @$ doze log . pulse_reason_docking ) ; verify ( m status bar window view ) . suppress wake up gesture ( eq ( true ) ) ;,docking dispatch,fail,pre
the ri explicitly <PLACE_HOLDER> this idiocy in the socket options.set option documentation .,if ( on && timeout < __num__ ) { throw new illegal argument exception ( __str__ ) ; } if ( on ) { impl . set option ( socket options . so_linger @$ integer . value of ( timeout ) ) ; } else { impl . set option ( socket options . so_linger @$ boolean . false ) ; },ri guarantees,success,pre
chunked encoding only <PLACE_HOLDER> sense to do when the payload is signed,if ( ! is payload signing enabled ( request ) || is chunked encoding disabled ( request ) ) { return false ; } if ( request . get original request object ( ) instanceof put object request || request . get original request object ( ) instanceof upload part request ) { return true ; } return false ;,encoding makes,success,pre
let the gridmix record <PLACE_HOLDER> itself .,return new record writer < k @$ gridmix record > ( ) { @ override public void write ( k ignored @$ gridmix record value ) throws io exception { value . write ( file out ) ; } @ override public void close ( task attempt context ctxt ) throws io exception { file out . close ( ) ; } } ;,record write,fail,pre
support the xa rollback to do to write xa <PLACE_HOLDER> log and judge xa statue to judge if send xa end,if ( session . get xatxid ( ) != null && conn instanceof mysql connection ) { mysql connection mysql con = ( mysql connection ) conn ; string xa tx id = session . get xatxid ( ) ; coordinator log entry coordinator log entry = multi node coordinator . in memory repository . get ( xa tx id ) ; if ( coordinator log entry != null ) { write check point = true ; for ( int i = __num__ ; i < coordinator log entry . participants . length ; i ++ ) { if ( coordinator log entry . participants [ i ] . resource name . equals ( conn . get schema ( ) ) ) { coordinator log entry . participants,xa commit,fail,pre
ensure that record <PLACE_HOLDER> corresponds to a data db record,if ( rec != null ) { if ( ! rec . has same schema ( datadb adapter . data_schema ) ) { return true ; } dt = code mgr . get data type ( rec ) ; if ( dt == null ) { msg . error ( this @$ __str__ + address ) ; } } else { dt = code mgr . get data type ( addr ) ; },record provided,success,pre
only proxy cache implementation <PLACE_HOLDER> a json formatter that has reference to user attributes,return new json formatter ( ) ;,implementation holds,fail,pre
partition ca n't <PLACE_HOLDER> this name,reserved partition values . add ( hive conf . get var ( conf @$ conf vars . defaultpartitionname ) ) ; reserved partition values . add ( hive conf . get var ( conf @$ conf vars . default_zookeeper_partition_name ) ) ;,partition use,fail,pre
finally check if either sender <PLACE_HOLDER> an exception,exception e ; e = s1 . get exception ( ) ; if ( e != null ) throw e ; e = s2 . get exception ( ) ; if ( e != null ) throw e ;,sender threw,success,pre
old clients do n't <PLACE_HOLDER> the checksum .,checksum = new byte [ __num__ ] ;,clients send,success,pre
uh oh ... it looks like the provider 's process has been <PLACE_HOLDER>ed on us . we need to wait for a new process to be started @$ and make sure its death does n't <PLACE_HOLDER> our process .,if ( ! success ) { slog . i ( tag @$ __str__ + cpr . name . flatten to short string ( ) + __str__ + r ) ; boolean last ref = dec provider count locked ( conn @$ cpr @$ token @$ stable ) ; check time ( start time @$ __str__ ) ; app died locked ( cpr . proc ) ; check time ( start time @$ __str__ ) ; if ( ! last ref ) { return null ; } provider running = false ; conn = null ; } else { cpr . proc . verified adj = cpr . proc . set adj ; },death stop,fail,pre
check <PLACE_HOLDER> namespace properties as xml @$ json and protobuf .,string namespace path = __str__ + ns name ; response = client . get ( namespace path ) ; assert equals ( __num__ @$ response . get code ( ) ) ; response = client . get ( namespace path @$ constants . mimetype_xml ) ; assert equals ( __num__ @$ response . get code ( ) ) ; namespaces instance model model = fromxml ( response . get body ( ) ) ; check namespace properties ( model . get properties ( ) @$ ns properties ) ; response = client . get ( namespace path @$ constants . mimetype_json ) ; assert equals ( __num__ @$ response . get code ( ) ) ; model = json mapper . read value ( response . get body,check get,success,pre
set up key for wikidict ; if caseless <PLACE_HOLDER> lower case version of surface form,string mention surface form key ; if ( wikidict caseless ) mention surface form key = surface form . to lower case ( ) ; else mention surface form key = surface form ;,caseless encode,fail,pre
only consider entries with absolute path names . this allows storing ur is in the database without the media scanner <PLACE_HOLDER> them .,if ( path != null && path . starts with ( __str__ ) ) { boolean exists = false ; try { exists = os . access ( path @$ android . system . os constants . f_ok ) ; } catch ( errno exception e1 ) { } if ( ! exists && ! mtp constants . is abstract object ( format ) ) { string mime type = media file . get mime type for file ( path ) ; if ( ! media file . is play list mime type ( mime type ) ) { deleter . delete ( row id ) ; if ( path . to lower case ( locale . us ) . ends with ( __str__ ) ) { deleter,ur deciding,fail,pre
items to <PLACE_HOLDER> suscription,item = new privacy item ( privacy item . type . subscription . name ( ) @$ true @$ i ) ; item . set value ( privacy rule . subscription_both ) ; original privacy items [ i ] = item ; i = i + __num__ ; item = new privacy item ( privacy item . type . subscription . name ( ) @$ false @$ i ) ; item . set value ( privacy rule . subscription_from ) ; original privacy items [ i ] = item ; i = i + __num__ ; item = new privacy item ( privacy item . type . subscription . name ( ) @$ true @$ i ) ; item . set value ( privacy rule . subscription_to ),items allow,fail,pre
partitioned tables do n't <PLACE_HOLDER> table desc set on the fetch task . instead they <PLACE_HOLDER> a list of partition desc objects @$ each with a table desc . let 's try to fetch the desc for the first partition and use it 's deserializer .,if ( td == null && ft . get work ( ) != null && ft . get work ( ) . get part desc ( ) != null ) { if ( ft . get work ( ) . get part desc ( ) . size ( ) > __num__ ) { td = ft . get work ( ) . get part desc ( ) . get ( __num__ ) . get table desc ( ) ; } } if ( td == null ) { log . info ( __str__ ) ; } else { string table name = __str__ ; list < field schema > lst = null ; try { lst = hive meta store utils . get fields from deserializer ( table,tables have,success,pre
given on lock screen and stack scroller <PLACE_HOLDER> a nonzero height,given lock screen ( ) ; m notification stack height = __num__ ; m keyguard status height = empty_height ;,scroller has,success,pre
if the string does n't <PLACE_HOLDER> the year value for some reason @$ then return the gregorian string .,if ( p == - __num__ ) { return s ; } p += year field . length ( ) ; string builder sb = new string builder ( s . substring ( __num__ @$ p ) ) ;,string contain,fail,pre
now we check if the rectangle of this actor in screen coordinates is in the rectangle spanned by the camera 's view . this assumes that the camera <PLACE_HOLDER> no zoom and is not rotated !,actor rect . set ( stagex @$ stagey @$ get width ( ) @$ get height ( ) ) ; cam rect . set ( camera . position . x - camera . viewport width / __num__ @$ camera . position . y - camera . viewport height / __num__ @$ camera . viewport width @$ camera . viewport height ) ; visible = cam rect . overlaps ( actor rect ) ; return ! visible ;,camera has,success,pre
this block of code can <PLACE_HOLDER> annotation if it 's parent annotation is <PLACE_HOLDER>d,if ( annotation type matches ( element @$ parent element @$ include annotations @$ false @$ include jackson annotations @$ seen annotations @$ lines @$ imports resolver @$ element type @$ nullability ) ) { return true ; },block include,success,pre
date time <PLACE_HOLDER> date,final plan phase effective plan phase = catalog . find phase ( plan phase . get name ( ) @$ effective date @$ last change plan date ) ; return compute usages ( is cancelled or blocked @$ effective plan phase ) ;,time created,fail,pre
getter also <PLACE_HOLDER> the rigid body,physics . get rigid body ( entity ) ;,getter returns,fail,pre
result will <PLACE_HOLDER> our rows @$ and elapsed time between snapshots,final entry entry = new entry ( ) ; final network stats result ; if ( recycle != null && recycle . capacity >= left . size ) { result = recycle ; result . size = __num__ ; result . elapsed realtime = delta realtime ; } else { result = new network stats ( delta realtime @$ left . size ) ; } for ( int i = __num__ ; i < left . size ; i ++ ) { entry . iface = left . iface [ i ] ; entry . uid = left . uid [ i ] ; entry . set = left . set [ i ] ; entry . tag = left . tag [ i ] ; entry .,result contain,fail,pre
this captures all <PLACE_HOLDER> messages @$ allowing us to verify log message was written .,final log interceptor log interceptor = new log interceptor ( ) ; test helper . drop all schemas ( ) ; test helper . executeddl ( __str__ ) ; configuration . builder config builder = test helper . default config ( ) . with ( postgres connector config . snapshot_mode @$ snapshot mode . initial_only . get value ( ) ) ; start ( postgres connector . class @$ config builder . build ( ) ) ; assert connector is running ( ) ; wait for available records ( __num__ @$ time unit . milliseconds ) ; stop connector ( value -> assert that ( log interceptor . contains warn message ( no_monitored_tables_warning ) ) . is false ( ) ) ;,all logged,success,pre
the following two 'applys ' <PLACE_HOLDER> multiple inputs to our pipeline @$ one for each of our two input sources .,p collection < table row > events table = p . apply ( big queryio . read table rows ( ) . from ( gdelt_events_table ) ) ; p collection < table row > country codes = p . apply ( big queryio . read table rows ( ) . from ( country_codes ) ) ; p collection < string > formatted results = join events ( events table @$ country codes ) ; formatted results . apply ( textio . write ( ) . to ( options . get output ( ) ) ) ; p . run ( ) . wait until finish ( ) ;,'applys make,fail,pre
this level has no more <PLACE_HOLDER> bits @$ pop back up a level .,if ( next active slot bit number > __num__ ) { int index shift above = index shift + __num__ ; virtual index += __num__ << index shift above ; virtual index &= ~ ( ( __num__ << index shift above ) - __num__ ) ; return - virtual index ; },level selected,fail,pre
optionally check the byte after this frame <PLACE_HOLDER> sync word .,if ( ! try read ( pes buffer @$ adts scratch . data @$ __num__ ) ) { return true ; } adts scratch . set position ( __num__ ) ; int frame size = adts scratch . read bits ( __num__ ) ; if ( frame size <= __num__ ) { return false ; },frame covers,fail,pre
the method <PLACE_HOLDER> arguments which need to be canonicalized,clause buffer . insert ( __num__ @$ __str__ ) ; compiled value cv = null ; for ( int j = this . args . size ( ) ; j > __num__ ; ) { cv = ( compiled value ) this . args . get ( -- j ) ; cv . generate canonicalized expression ( clause buffer @$ context ) ; clause buffer . insert ( __num__ @$ __str__ ) ; } clause buffer . delete char at ( __num__ ) . insert ( __num__ @$ __str__ ) . insert ( __num__ @$ this . method name ) ;,method has,fail,pre
vpn is <PLACE_HOLDER> cell | wi fi .,m service . set underlying networks for vpn ( new network [ ] { m cell network agent . get network ( ) @$ m wi fi network agent . get network ( ) } ) ; wait for idle ( ) ;,vpn using,success,pre
client should <PLACE_HOLDER> the socket @$ but let 's hold it open .,assert equals ( - __num__ @$ client . get input stream ( ) . read ( ) ) ;,client close,success,pre
exception is <PLACE_HOLDER> not permitted exception,assert that ( result . failed ( ) . get ( ) ) . is instance of ( call not permitted exception . class ) ;,exception run,fail,pre
verify that a second call should still <PLACE_HOLDER> absolute values,final long [ ] [ ] times2 = increase time ( times1 ) ; write to file ( uid lines ( m uids @$ times2 ) ) ; m reader . read absolute ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; i ++ ) { m callback . verify ( m uids [ i ] @$ times2 [ i ] ) ; } m callback . verify no more interactions ( ) ; m callback . clear ( ) ; assert true ( m test file . delete ( ) ) ;,call return,success,pre
the contact already <PLACE_HOLDER> its ok,try { op set presence2 . subscribe ( fixture . userid1 ) ; } catch ( operation failed exception ex ) { if ( ex . get error code ( ) != operation failed exception . subscription_already_exists ) { throw ex ; } else { } },contact exist,success,pre
ensure payload generation does n't <PLACE_HOLDER> an exception,byte [ ] serialized = new exec checking security manager ( ) . call wrapped ( new callable < byte [ ] > ( ) { public byte [ ] call ( ) throws exception { final string command = args . length > __num__ && args [ __num__ ] != null ? args [ __num__ ] : get default test cmd ( ) ; system . out . println ( __str__ + command + __str__ ) ; object payload < ? > payload = clazz . new instance ( ) ; final object obj before = payload . get object ( command ) ; system . out . println ( __str__ ) ; byte [ ] ser = serializer . serialize ( obj before ) ; utils,generation throw,success,pre
make sure the fs and the found root dir <PLACE_HOLDER> the same scheme,log . debug ( __str__ + file system . get default uri ( fs . get file system ( ) . get conf ( ) ) ) ; log . debug ( __str__ + file system . get default uri ( fs . get configuration ( ) ) ) ;,fs have,success,pre
cocoa dragged event <PLACE_HOLDER> the information about which mouse button is being dragged . use it to determine the peer that should receive the dragged event .,if ( id == mouse event . mouse_dragged ) { target peer = mouse down target [ target idx ] ; mouse click buttons &= ~ modifiers ; } else if ( id == mouse event . mouse_released ) { target peer = mouse down target [ target idx ] ; if ( ( modifiers & event button mask ) == __num__ ) { mouse down target [ target idx ] = null ; } },event holds,fail,pre
image alt <PLACE_HOLDER> numeric attachment id of the image in the site 's media library,add metadata property ( metadata @$ __str__ @$ __str__ ) ; add metadata property ( metadata @$ __str__ @$ __str__ ) ;,alt specify,fail,pre
create the base list of classes which <PLACE_HOLDER> possible methods to be overloaded,this . class list = new linked hash set < class > ( ) ; this . class list . add ( super class ) ; if ( generate delegate field ) { class list . add ( delegate class ) ; collections . add all ( this . class list @$ delegate class . get interfaces ( ) ) ; } if ( interfaces != null ) { collections . add all ( this . class list @$ interfaces ) ; } this . proxy name = proxy name ( ) ; this . empty body = empty body ;,which have,success,pre
<PLACE_HOLDER> action builder constructs <PLACE_HOLDER> runner action with a 'null ' shell path only when we use the native <PLACE_HOLDER> wrapper . something clearly went wrong .,if ( os . get current ( ) == os . windows && ! action . is using test wrapper instead of test setup script ( ) ) { preconditions . check not null ( action . get sh executable maybe ( ) @$ __str__ @$ action ) ; args . add ( action . get sh executable maybe ( ) . get path string ( ) ) ; args . add ( __str__ ) ; args . add ( __str__ ) ; },constructs build,fail,pre
for mvcc caches we need to wait until updated value becomes visible for consequent readers . when mvcc transaction completes @$ it 's updates are not visible immediately for the new transactions . this is caused by the lag between transaction completes on the node and mvcc coordinator <PLACE_HOLDER> this transaction from the active list .,grid test utils . run async ( new runnable ( ) { @ override public void run ( ) { string v ; while ( ! thread . current thread ( ) . is interrupted ( ) ) { v = cache . get ( key ) ; if ( v == null ) do sleep ( __num__ ) ; else { log . info ( __str__ + id ) ; ( ( ( grid future adapter ) ( ( ignite future impl ) promise ) . internal future ( ) ) ) . on done ( __str__ ) ; break ; } } } } ) ;,coordinator removing,fail,pre
& & ! special <PLACE_HOLDER> arg case,if ( no arg && ! super list . is empty ( ) && ! has no arg constructor ( c node ) ) { create no arg constructor ( c node @$ modifiers ) ; },& have,fail,pre
<PLACE_HOLDER> at height 0 is just the regular tx <PLACE_HOLDER> itself .,if ( height == __num__ ) { return hashes . get ( pos ) ; },tx hash,success,pre
adding null where the complete tasks header <PLACE_HOLDER> a lot of logic for us,m tasks . add ( null ) ;,header does,fail,pre
which segment <PLACE_HOLDER> the bucket,bucket segment index = bucket > > > num buckets per segment bits ;,segment contains,success,pre
delete the rows inserted from cache utils <PLACE_HOLDER> table @$ otherwise conflict in pk 's,jta obj . delete rows ( this . tbl name ) ; context ctx = cache . getjndi context ( ) ; user transaction ta = ( user transaction ) ctx . lookup ( __str__ ) ; connection conn = null ; try { ta . begin ( ) ; data source da = ( data source ) ctx . lookup ( __str__ ) ; conn = da . get connection ( ) ; statement stmt = conn . create statement ( ) ; string sqlstr = __str__ + this . tbl name + __str__ + this . tblid fld + __str__ + __str__ + this . tbl name fld + __str__ + __str__ ; stmt . execute update ( sqlstr ) ; ta . commit ( ),rows managed,fail,pre
then check if total estimated file size <PLACE_HOLDER> user specified value,if ( total estimated export size > user specified limit ) { string builder sb = new string builder ( ) ; sb . append ( __str__ ) . append ( total estimated export size ) . append ( __str__ ) . append ( cli strings . export_logs__filesizelimit ) . append ( __str__ ) . append ( user specified limit ) . append ( __str__ ) ; return result model . create error ( sb . to string ( ) ) ; },size exceeds,success,pre
determine if this update is <PLACE_HOLDER> the bundle for the processor,if ( ! existing coordinate . equals ( incoming coordinate ) ) { if ( ! existing coordinate . get group ( ) . equals ( incoming coordinate . get group ( ) ) || ! existing coordinate . get id ( ) . equals ( incoming coordinate . get id ( ) ) ) { throw new illegal argument exception ( string . format ( __str__ @$ get identifier ( ) @$ existing coordinate . get coordinate ( ) @$ incoming coordinate . get coordinate ( ) ) ) ; } },update changing,success,pre
test database meta data queries which do not <PLACE_HOLDER> a parent statement,database meta data md = this . con . get meta data ( ) ; assert true ( md . get connection ( ) == this . con ) ; rs = md . get catalogs ( ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get columns ( null @$ null @$ null @$ null ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get functions ( null @$ null @$ null ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get imported keys (,which have,success,pre
parse <PLACE_HOLDER> color .,int color start = start ; for ( int i = start + __num__ ; i < end ; i ++ ) { char ch = str . char at ( i ) ; if ( ch != __str__ ) continue ; color named color = colors . get ( str . sub sequence ( color start @$ i ) . to string ( ) ) ; if ( named color == null ) return - __num__ ; color color = color pool . obtain ( ) ; color stack . add ( color ) ; color . set ( named color ) ; return i - start ; },parse generated,fail,pre
no handlers <PLACE_HOLDER> so close the actual server the done handler needs to be executed on the context that calls close @$ not the context of the actual server,actual server . actual close ( context @$ completion ) ;,handlers found,fail,pre
if it is no longer pending someone called unschedule async <PLACE_HOLDER> so we do n't need to <PLACE_HOLDER> the entry @$ but if we have a version tag we need to record the operation to update the rvv,if ( tag != null ) { disk entry . helper . do async flush ( tag @$ region ) ; },async flush,fail,pre
device idle controller <PLACE_HOLDER> these to local services in the constructor @$ so we have to remove them after each test @$ otherwise @$ subsequent tests will fail .,local services . remove service for test ( app state tracker . class ) ; local services . remove service for test ( device idle controller . local service . class ) ;,controller adds,success,pre
fully recovery <PLACE_HOLDER> much longer time,assert equals ( load balancer simulator . get point ( __str__ @$ default partition accessor . default_partition_id @$ uri1 ) @$ __num__ ) ; assert equals ( load balancer simulator . get point ( __str__ @$ default partition accessor . default_partition_id @$ uri2 ) @$ __num__ ) ;,recovery takes,fail,pre
gana ya gana small ya kana ya kana small ya gana yu gana small yu kana yu kana small yu gana <PLACE_HOLDER> gana small <PLACE_HOLDER> kana <PLACE_HOLDER> kana small <PLACE_HOLDER>,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,kana i,fail,pre
check for a vertex hit first @$ otherwise @$ we <PLACE_HOLDER> edge hits when we are hovering over a vertex @$ due to how edges are interpreted as existing all the way to the center point of a vertex,v vertex = get pick support ( ) . get vertex ( viewer layout @$ p . getx ( ) @$ p . gety ( ) ) ; if ( vertex != null ) { return new vertex tool tip info ( vertex @$ event ) ; } e edge = get pick support ( ) . get edge ( viewer layout @$ p . getx ( ) @$ p . gety ( ) ) ; if ( edge != null ) { return new edge tool tip info ( edge @$ event ) ; },check receive,fail,pre
does the package <PLACE_HOLDER> code ? if not @$ there wo n't be any artifacts .,if ( ! package dex optimizer . can optimize package ( pkg ) ) { continue ; } if ( pkg . code path == null ) { slog . w ( tag @$ __str__ + pkg + __str__ ) ; continue ; },package support,fail,pre
empty bundles do n't <PLACE_HOLDER> watermarks and should n't trigger downstream execution @$ so filter them out,if ( ! iterables . is empty ( committed . get elements ( ) ) ) { completed . add ( committed ) ; },bundles add,fail,pre
ie does n't fire popstate correctly with certain hash changes . <PLACE_HOLDER> the missing event with history handler .,if ( browser info . get ( ) . isie ( ) ) { history . add value change handler ( evt -> { final string new location = browser . get window ( ) . get location ( ) . to string ( ) ; if ( ! new location . equals ( current location ) ) { current location = new location ; get rpc proxy ( ui server rpc . class ) . popstate ( browser . get window ( ) . get location ( ) . to string ( ) ) ; } } ) ; current location = browser . get window ( ) . get location ( ) . to string ( ) ; },ie register,fail,pre
should return <PLACE_HOLDER> map,resp = post ( __str__ @$ collections . empty_map ) ; assert not null ( resp ) ; map m = ( map ) resp . get ( __str__ ) ; assert true ( m . is empty ( ) ) ; resp = post ( __str__ @$ immutable map . of ( __str__ @$ immutable list . of ( __str__ @$ test pipeline impl . class . get name ( ) ) ) ) ; assert not null ( resp ) ; m = ( map ) resp . get ( __str__ ) ; assert not null ( m ) ; assert equals ( __num__ @$ m . size ( ) ) ; map v = ( map ) m . get ( __str__ ) ; assert,return empty,success,pre
determine if we are working with a dlgtemplate or dlgtemplateex structure . the first 4 bytes will <PLACE_HOLDER> specific values if it 's a dlgtemplateex .,try { boolean ex = mem buffer . get short ( __num__ ) == __num__ && mem buffer . get short ( __num__ ) == - __num__ ; temp offset = add dlg template structure ( mem buffer @$ comps @$ temp offset @$ ex ) ; temp offset = add dialog menu array ( mem buffer @$ comps @$ temp offset ) ; temp offset = add dialog class array ( mem buffer @$ comps @$ temp offset ) ; temp offset = add dialog title array ( mem buffer @$ comps @$ temp offset ) ; byte get style = mem buffer . get byte ( __num__ ) ; if ( ( get style & ds_setfont ) > __num__ ) { temp offset = add dialog,bytes contain,fail,pre
now @$ iterate over all the <PLACE_HOLDER> interfaces,for ( class < ? > iface : interfaces ) { for ( membert member : getter . get members ( iface ) ) { if ( member . is annotation present ( anno ) ) { matches . add ( member ) ; } } } return matches ;,iterate implemented,fail,pre
this may happen if an app <PLACE_HOLDER> a recorded usage @$ but <PLACE_HOLDER> been uninstalled .,continue ;,app has,success,pre
this methods <PLACE_HOLDER> a directory for new files to read in for streaming .,javad stream < string > log data = jssc . text file stream ( flags . get instance ( ) . get logs directory ( ) ) ; javad stream < apache access log > access logsd stream = log data . map ( new functions . parse from log line ( ) ) . cache ( ) ; final log analyzer total log analyzer total = new log analyzer total ( ) ; final log analyzer windowed log analyzer windowed = new log analyzer windowed ( ) ;,methods opens,fail,pre
default generator does n't <PLACE_HOLDER> any mechanism for emitting supporting files other than by a mustache template @$ so we 're obliged to serialize the caches to json strings and use templates to write them .,if ( load test data from file ) { try { if ( test data cache . root ( ) . is dirty ( ) ) { byte array output stream out = new byte array output stream ( ) ; test data cache . root ( ) . flush ( out ) ; string test data json = new string ( out . to byte array ( ) @$ __str__ ) ; objs . put ( __str__ @$ test data json ) ; supporting files . add ( new supporting file ( __str__ @$ test data file . get absolute path ( ) ) ) ; } } catch ( cache exception | unsupported encoding exception e ) { logger . error ( __str__ + test data,generator provide,success,pre
virtual or physical memory over limit . <PLACE_HOLDER> the container and remove the corresponding process tree,if ( is memory over limit ) { log . warn ( msg ) ; if ( ! p tree . check pid pgrpid for match ( ) ) { log . error ( __str__ + __str__ @$ p id ) ; } event dispatcher . get event handler ( ) . handle ( new container kill event ( container id @$ container exit status @$ msg ) ) ; tracking containers . remove ( container id ) ; log . info ( __str__ @$ p id ) ; },memory kill,fail,pre
130 <PLACE_HOLDER> band .,if ( eq values [ __num__ ] == __num__ ) { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) __num__ ) ; } else if ( eq values [ __num__ ] < __num__ ) { if ( eq values [ __num__ ] == __num__ ) { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) - __num__ ) ; } else { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) ( - ( __num__ - eq values [ __num__ ] ) * __num__ ) ) ; } } else if,130 hz,success,pre
test the same data and model with prior @$ should <PLACE_HOLDER> the same model except for the intercept,glm = new glm ( params ) ; model3 = glm . train model ( ) . get ( ) ; double lambda = model3 . _output . _submodels [ model3 . _output . _best_lambda_idx ] . lambda_value ; params . _lambda_search = false ; params . _lambda = new double [ ] { lambda } ; model metrics mm3 = model metrics . get fromdkv ( model3 @$ fr ) ; assert equals ( __str__ + model3 . _output . _training_metrics . _mse + __str__ + mm3 . _mse @$ model3 . _output . _training_metrics . _mse @$ mm3 . _mse @$ __num__ ) ; assert equals ( __str__ + ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev + __str__,data get,success,pre
fails with primitive classes ; <PLACE_HOLDER> the wrapper class . thanks @$ java .,a = ( e [ ] ) array . new instance ( afclz @$ splits . length ) ;,fails use,fail,pre
fragment factory needs to be set before calling the super.on create @$ otherwise the activity crashes when it is recreating and there is a fragment which <PLACE_HOLDER> no default constructor .,super . on create ( saved instance state ) ;,which has,success,pre
catch exceptions since set daemon can <PLACE_HOLDER> a security exception to be thrown under netscape in the applet mode,set daemon ( true ) ;,daemon cause,success,pre
to avoid race condition of testcase @$ atleast <PLACE_HOLDER> 40 times with sleep of 50 ms,verify entity for timelinev2 ( app entity file @$ application metrics constants . finished_event_type @$ __num__ @$ __num__ @$ __num__ @$ false ) ;,atleast check,success,pre
reached end of string . always a <PLACE_HOLDER> position .,if ( p2 >= f text . length ( ) ) { break ; },a break,success,pre
if the app is <PLACE_HOLDER> backup @$ tell the backup manager about it,final backup record backup target = m backup targets . get ( app . user id ) ; if ( backup target != null && app . pid == backup target . app . pid ) { if ( debug_backup || debug_cleanup ) slog . d ( tag_cleanup @$ __str__ + backup target . app info + __str__ ) ; m handler . post ( new runnable ( ) { @ override public void run ( ) { try { i backup manager bm = i backup manager . stub . as interface ( service manager . get service ( context . backup_service ) ) ; bm . agent disconnected for user ( app . user id @$ app . info . package name ) ; } catch,app perform,fail,pre
if the metafile <PLACE_HOLDER> a session @$ remove it from the map .,try { dbus event buffer meta info mi = new dbus event buffer meta info ( f ) ; mi . load meta info ( ) ; if ( mi . is valid ( ) ) { string session id = mi . get session id ( ) ; session file map . remove ( session id ) ; } } catch ( dbus event buffer meta info . dbus event buffer meta info exception e ) { log . warn ( __str__ + f . get name ( ) + __str__ @$ e ) ; return ; },metafile has,fail,pre
remove the <PLACE_HOLDER> node and the order by node and replace them with a merge <PLACE_HOLDER> node . leave the order by node inline in the merge <PLACE_HOLDER> node @$ since we need it to calculate the merge .,plan . clear children ( ) ; receive node . remove from graph ( ) ; merge receive plan node mrnode = new merge receive plan node ( ) ; mrnode . add inline plan node ( onode ) ; mrnode . add and link child ( send node ) ; plan . add and link child ( mrnode ) ; return plan ;,node receive,success,pre
skip due to android devices that have broken scrolltop will may <PLACE_HOLDER> odd scrolling here .,if ( browser info . get ( ) . is touch device ( ) ) { return ; },will throw,fail,pre
note : we shouldnt ' have to do this @$ since j file chooser <PLACE_HOLDER> the filter to the choosable filters list when the filter is set . lets be paranoid just in case someone overrides set file filter in j file chooser .,file filter current filter = get file chooser ( ) . get file filter ( ) ; boolean found = false ; if ( current filter != null ) { for ( file filter filter : filters ) { if ( filter == current filter ) { found = true ; } } if ( ! found ) { get file chooser ( ) . add choosable file filter ( current filter ) ; } } return get file chooser ( ) . get file filter ( ) ;,chooser adds,success,pre
query must <PLACE_HOLDER> only results from single region .,validate clients ( region id @$ clients2 ) ; if ( region id == unmapped_region ) fail ( ) ;,query produce,success,pre
this test <PLACE_HOLDER> the baseline structure used in subsequent tests . if this fails @$ the rest will fail .,project dependency graph graph = three projects depending ona single ( ) ; final list < maven project > sorted projects = graph . get sorted projects ( ) ; assert equals ( a project @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender1 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender2 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender3 @$ sorted projects . get ( __num__ ) ) ;,test preserves,fail,pre
reopen <PLACE_HOLDER> the use of the reopened session for the same query that we gave it out for ; so @$ as we would have failed an active query @$ fail the user before it 's started .,future . set exception ( new runtime exception ( __str__ + session . get reason for kill ( ) ) ) ; return ;,reopen makes,fail,pre
for some reason @$ the offset is not stored @$ so the restored values <PLACE_HOLDER> the offset from the default jvm timezone .,if ( time as timestamp remappingh2 dialect . class . equals ( get remapping dialect class ( ) ) ) { return get original property value ( ) . with offset same local ( offset date time . now ( ) . get offset ( ) ) ; } else { return get original property value ( ) . with nano ( __num__ ) . with offset same local ( offset date time . now ( ) . get offset ( ) ) ; },values contain,fail,pre
remove all items <PLACE_HOLDER> iterator,iterator iterator = observable collection . iterator ( ) ; while ( iterator . has next ( ) ) { iterator . next ( ) ; iterator . remove ( ) ; },items using,success,pre
this will obtain any <PLACE_HOLDER> outbound data @$ or will process the outbound app data .,try { synchronized ( write lock ) { hs status = write record ( output record @$ ea ) ; } } catch ( ssl exception e ) { throw e ; } catch ( io exception e ) { throw new ssl exception ( __str__ @$ e ) ; },any sent,fail,pre
removing these calls and the three location assertions above will cause the test to fail due to the strict stubs . without the alterations to stack trace cleaner provider @$ the failure messages will <PLACE_HOLDER> an item in the stack trace inside the powermock libraries .,assert that ( something with static method . do static one ( ) @$ is ( __str__ ) ) ; assert that ( something with static method . do static two ( ) @$ is ( __str__ ) ) ; something with static method . do static void ( ) ;,messages find,fail,pre
producer will <PLACE_HOLDER> it 's own destination validation so always use the destination based send method otherwise we might violate a jms rule .,message producer . send ( destination @$ message @$ delivery mode @$ priority @$ time to live ) ;,producer do,success,pre
check if the caller <PLACE_HOLDER> enough privileges to embed activities and launch to private displays .,final int start any perm = m service . check permission ( internal_system_window @$ calling pid @$ calling uid ) ; if ( start any perm == permission_granted ) { if ( debug_tasks ) slog . d ( tag @$ __str__ + __str__ ) ; return true ; },caller has,success,pre
<PLACE_HOLDER> the creation of a bucket @$ which should <PLACE_HOLDER> the destruction of this vm .,assert that thrown by ( ( ) -> region . put ( __str__ @$ __str__ ) ) . is instance of ( cancel exception . class ) ;,which trigger,success,pre
first see if the file <PLACE_HOLDER> the regular expression !,if ( pattern != null ) { matcher matcher = pattern . matcher ( item . get name ( ) . geturi ( ) ) ; get it = matcher . matches ( ) ; } if ( patternexclude != null ) { matcher matcherexclude = patternexclude . matcher ( item . get name ( ) . geturi ( ) ) ; get itexclude = matcherexclude . matches ( ) ; } boolean take = take this file ( item @$ new file name ) ; if ( get it && ! get itexclude && take ) { if ( log . is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ item . get name ( ) .,file matches,success,pre
since file download task <PLACE_HOLDER> the actual exception with download exception . we should extract it letting the error message clearer .,if ( ! is dependents succeeded ( ) ) { exception t = task . get exception ( ) ; if ( t instanceof download exception ) throw new library download exception ( library @$ t . get cause ( ) ) ; else throw new library download exception ( library @$ t ) ; } else { if ( xz ) unpack library ( jar @$ files . read all bytes ( xz file . to path ( ) ) ) ; if ( ! checksum valid ( jar @$ library . get checksums ( ) ) ) { jar . delete ( ) ; throw new io exception ( __str__ + library ) ; } },task wraps,success,pre
unmatched files contains any files that had no association to a program <PLACE_HOLDER> the user a chance to search the project for it @$ and import it if not found,if ( ! unmatched files . is empty ( ) ) { unmatched files = search project for matching files or fail ( unmatched files @$ program manager @$ monitor @$ programs opened ) ; } return unmatched files ;,files give,success,pre
the new code can <PLACE_HOLDER> extra separator chars,compare ( __str__ @$ __str__ ) ; compare ( __str__ @$ __str__ ) ;,code add,fail,pre
if the client supplied connection credentials @$ the authentication filter will <PLACE_HOLDER> a normal authentication @$ so we should exit immediately :,if ( credentials available ( conn ) ) { return ; },filter do,fail,pre
wait a minute and you should <PLACE_HOLDER> other 6 requests executed,wait minute quota ( ) ;,minute get,success,pre
did n't enclose table pattern within single quotes . table name and include list not allowed . table name and exclude list not allowed . abrubtly <PLACE_HOLDER> dot . with square brackets two dots with empty list more than two list,string [ ] invalid repl policies = new string [ ] { primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ } ;,name use,fail,pre
finally check account notes did <PLACE_HOLDER> reset,assert . assert null ( retrieved account . get notes ( ) ) ;,notes get,success,pre
cooked string can be null only for tagged template literals . a tagged template literal would <PLACE_HOLDER> the default case below .,if ( node . has one child ( ) ) { return check not null ( node . get first child ( ) . get cooked string ( ) ) ; } else { throw new malformed exception ( __str__ @$ node ) ; },literal complete,fail,pre
any mode but primary & proxy can <PLACE_HOLDER> any mode :,if ( m != primary && m != proxy ) for ( igfs mode n : igfs mode . values ( ) ) assert true ( igfs utils . can contain ( m @$ n ) ) ;,primary contain,success,pre
async <PLACE_HOLDER> the message back .,member . remote . send string ( data @$ null ) ;,async send,fail,pre
crawl job needs to be sure all beans have <PLACE_HOLDER> finished signal before teardown,this . is stop complete = true ; app ctx . publish event ( new stop complete event ( this ) ) ;,beans sent,fail,pre
any errors here should <PLACE_HOLDER> the thread come to a halt and be recognized by the writer,flush all ( ) ;,errors cause,fail,pre
we keep a separate list of actions for each object newly obtained from a view @$ and perform a shallow copy during get clone . that way the list of actions performed <PLACE_HOLDER>s all actions performed on the view by the tree of nodes initialized from it . note that initializing two nodes with the same view will not merge the two lists @$,shadow obtained . performed action and args list = new array list < > ( ) ; shadow obtained . view = view ; s allocation count ++ ; if ( shadow obtained . m origin node id == __num__ ) { shadow obtained . m origin node id = s allocation count ; } strict equality node wrapper wrapper = new strict equality node wrapper ( obtained instance ) ; obtained instances . put ( wrapper @$ thread . current thread ( ) . get stack trace ( ) ) ; ordered instances . put ( s allocation count @$ wrapper ) ; return obtained instance ;,list contain,success,pre
we should recalculate geometry just before calculation of the thumb movement direction . it is important for the case @$ when j slider is a cell editor in j table . <PLACE_HOLDER> 6348946 .,calculate geometry ( ) ; final boolean first click = ( current mousex == - __num__ ) && ( current mousey == - __num__ ) ; current mousex = e . getx ( ) ; current mousey = e . gety ( ) ; if ( slider . is request focus enabled ( ) ) { slider . request focus ( ) ; } boolean is mouse event in thumb = thumb rect . contains ( current mousex @$ current mousey ) ;,calculation see,success,pre
skip non union r <PLACE_HOLDER> java,fake buildable context buildable context = new fake buildable context ( ) ; list < step > steps = dummyr dot java . get build steps ( fake build context . noop_context @$ buildable context ) ; assert equals ( __str__ @$ __num__ @$ steps . size ( ) ) ; path r dot java src folder = dummyr dot java . getr dot java src folder ( dummyr dot java . get build target ( ) @$ filesystem ) ; path r dot java bin folder = compiler output paths . get classes dir ( dummyr dot java . get build target ( ) @$ filesystem ) ; path r dot java output folder = dummyr dot java . get path to output dir ( dummyr dot,r dot,success,pre
the immersive mode confirmation took the focus from m last <PLACE_HOLDER> window which was controlling the system ui visibility . so if m last <PLACE_HOLDER> window can still receive keys @$ we let it keep controlling the visibility .,if ( win candidate . get attrs ( ) . token == m immersive mode confirmation . get window token ( ) ) { final boolean last focus can receive keys = ( m last focused window != null && m last focused window . can receive keys ( ) ) ; win candidate = is status bar keyguard ( ) ? m status bar : last focus can receive keys ? m last focused window : m top fullscreen opaque window state ; if ( win candidate == null ) { return __num__ ; } },focus focused,success,pre
reset the complex type <PLACE_HOLDER> information such that it is recalculated again .,cleanup is complex type attribute ( ) ; int index = __num__ ; for ( observed object o : observed objects ) { reset already notified ( o @$ index ++ @$ observed_attribute_error_notified | observed_attribute_type_error_notified ) ; },type attribute,success,pre
verify ask after registration but before launch . do n't kill @$ should be null . now <PLACE_HOLDER> a task with the id,listener . register pending task ( task @$ wid ) ; result = listener . get task ( context ) ; assert null ( result ) ;,registration put,success,pre
string <PLACE_HOLDER> 2 ints and a reference on a string,int string size = jvm size utils . get object header size ( ) + ( __num__ * __num__ ) + jvm size utils . get reference size ( ) ;,string contains,fail,pre
src 1 <PLACE_HOLDER> no quota to accommodate new rename node,fs . set quota ( src1 . get parent ( ) @$ __num__ @$ hdfs constants . quota_dont_set ) ; create file ( src1 ) ; fs . set quota ( src1 . get parent ( ) @$ __num__ @$ hdfs constants . quota_dont_set ) ; rename ( dst1 @$ src1 @$ false @$ true @$ rename . overwrite ) ;,src has,success,pre
the rest <PLACE_HOLDER> response data parameter . this can potentially be refactored .,throwable error ; try { rest li service exception service exception = response data . get response envelope ( ) . get exception ( ) ; final rest li response response = _response handler . build partial response ( _method @$ response data ) ; error = new rest li response exception ( service exception @$ response ) ; } catch ( throwable throwable ) { logger . error ( __str__ @$ response data . get response envelope ( ) . get exception ( ) ) ; error = throwable ; } _wrapped callback . on error ( error ) ;,rest needs,fail,pre
for every pixel in this row <PLACE_HOLDER> the color,while ( off < row limit ) { pixels [ off ++ ] = index into gradients arrays ( g ) ; g += dgdx ; },pixel reset,fail,pre
note : other parent readers <PLACE_HOLDER> everything in ctor @$ but union does it in start stripe .,this . tags = new run length byte reader ( data stream ) ;,readers own,fail,pre
sql server and oracle does n't normally <PLACE_HOLDER> order by in subqueries ...,if ( ! ( get dialect ( ) instanceof sql server dialect ) && ! ( get dialect ( ) instanceof oracle8i dialect ) ) { dc4 . get executable criteria ( session ) . add order ( order . asc ( __str__ ) ) . list ( ) ; } session . create criteria ( enrolment . class @$ __str__ ) . add ( subqueries . eq ( __str__ @$ dc4 ) ) . list ( ) ; session . delete ( enrolment2 ) ; session . delete ( gavin ) ; session . delete ( course ) ; t . commit ( ) ; session . close ( ) ;,server support,success,pre
ca n't use high watermark directly @$ as the partition map may <PLACE_HOLDER> different precision . for example @$ high watermark may be specified to seconds @$ but partition map could be specified to hour or date .,long highest watermark = collections . max ( partition map . values ( ) ) ; for ( map . entry < long @$ long > entry : partition map . entry set ( ) ) { long partition high watermark = entry . get value ( ) ; if ( partition high watermark . equals ( highest watermark ) ) { partitions . add ( new partition ( entry . get key ( ) @$ partition high watermark @$ true @$ has user specified high watermark ) ) ; } else { partitions . add ( new partition ( entry . get key ( ) @$ partition high watermark @$ false ) ) ; } },map have,success,pre
preserve the same order that github <PLACE_HOLDER> api returns,map < string @$ scm organization > org map = new linked hash map < > ( ) ;,github generated,fail,pre
parse float <PLACE_HOLDER> an exponent or a sign,if ( s . matches ( __str__ ) ) { throw new number format exception ( __str__ ) ; } try { float value = float . parse float ( s ) ; if ( value < __num__ || value > __num__ ) { throw new number format exception ( __str__ ) ; } return value ; } catch ( number format exception e ) { throw new number format exception ( __str__ ) ; },float requires,fail,pre
deep copy the matchers and logs @$ which <PLACE_HOLDER> the builder to be reused .,for ( matcher and error m : matchers and logs ) { this . matchers and logs . add ( m ) ; },which forces,fail,pre
now generate another check @$ but without having updated the package . the package tracker should <PLACE_HOLDER> the last check failed and trigger again .,simulate package installation ( package versions ) ;,tracker explain,fail,pre
fancier parser would <PLACE_HOLDER> line number and column ...,if ( ! actual str . equals ignore case ( expected str ) ) { throw new io exception ( __str__ + printable ( expected str ) + __str__ + printable ( actual str ) ) ; },parser change,fail,pre
no instance <PLACE_HOLDER> correct pool configured,try { driver . assign instances ( instance partitions type . offline @$ instance configs ) ; fail ( ) ; } catch ( illegal state exception e ) { assert equals ( e . get message ( ) @$ __str__ ) ; } for ( int i = __num__ ; i < num instances ; i ++ ) { instance config instance config = instance configs . get ( i ) ; if ( i < num instances / __num__ ) { instance config . get record ( ) . set map field ( instance . pool_key @$ collections . singleton map ( offline_tag @$ __str__ ) ) ; } else { instance config . get record ( ) . set map field ( instance . pool_key,instance has,success,pre
traverse the children which can be either just <PLACE_HOLDER> tables list or both <PLACE_HOLDER> and exclude tables lists .,tree old policy tables list node = old repl policy tree . get child ( __num__ ) ; assert ( old policy tables list node . get type ( ) == tok_repl_tables ) ; set repl dump tables list ( old policy tables list node @$ old repl scope ) ;,list include,success,pre
this function needs its stack looked at . <PLACE_HOLDER> its stack purge to invalid @$ so wo n't be added to list again if called recursively .,func . set stack purge size ( function . invalid_stack_depth_change ) ; func list . add ( __num__ @$ func ) ;,stack set,success,pre
instead of getting this directly from the lp @$ use reflection so that a package which <PLACE_HOLDER> grammatical structure does n't necessarily have to use lexicalized parser,try { method method = lp . get class ( ) . get method ( __str__ ) ; params = ( treebank lang parser params ) method . invoke ( lp ) ; params . set generate original dependencies ( generate original dependencies ) ; } catch ( exception cnfe ) { throw new runtime exception ( cnfe ) ; },which provides,fail,pre
we can now utilize shader program 's hash map which may be better than gl <PLACE_HOLDER> uniform location,shader program . set strict mode ( false ) ;,gl determine,fail,pre
if specified <PLACE_HOLDER> kerberos,try { if ( ! kerberos . is empty ( ) ) { config . enable kerberos authentication ( kerberos ) ; } m_client = get client ( config @$ servers @$ port ) ; } catch ( exception exc ) { system . err . println ( exc . get message ( ) ) ; return - __num__ ; },specified enable,success,pre
and will throw exception . for cases when users will <PLACE_HOLDER> rollback as result of that as well we need to support chain of rollback calls but still fail on rollback @$ commit,if ( ! is open ( ) && failure ) { return ; } failure ( ) ; close transaction ( ) ;,users reject,fail,pre
this special create table is called locally to master . therefore @$ no rpc <PLACE_HOLDER> no need to use nonce to detect duplicated rpc call .,long proc id = this . procedure executor . submit procedure ( new create table procedure ( procedure executor . get environment ( ) @$ table descriptor @$ new regions ) ) ; return proc id ;,rpc provided,fail,pre
download attributes and return <PLACE_HOLDER> metadata only if the blob exists .,if ( null != blob && blob . exists ( get instrumented context ( ) ) ) { log . debug ( __str__ @$ key ) ; try { blob . download attributes ( get instrumented context ( ) ) ; blob properties properties = blob . get properties ( ) ; if ( retrieve folder attribute ( blob ) ) { log . debug ( __str__ @$ key ) ; return new file metadata ( key @$ properties . get last modified ( ) . get time ( ) @$ get permission status ( blob ) @$ blob materialization . explicit @$ hadoop block size ) ; } else { log . debug ( __str__ @$ key ) ; return new file metadata ( key @$ get,attributes file,success,pre
verify new file is <PLACE_HOLDER> it 's own edeks @$ with new keyversions @$ and can be decrypted correctly .,assert key version changed ( recreated @$ fei orig ) ; final string content = dfs test util . read file ( fs @$ recreated ) ; assert equals ( content orig @$ content ) ;,file storing,fail,pre
this call not only <PLACE_HOLDER> the error gui on @$ but also wires our special error display wrapper .,set errorgui enabled ( true ) ;,call requests,fail,pre
server socket will <PLACE_HOLDER> connections,socket base server = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( server @$ not null value ( ) ) ; zmq . set socket option ( server @$ zmq . zmq_curve_server @$ true ) ; zmq . set socket option ( server @$ zmq . zmq_curve_secretkey @$ server secret ) ; zmq . set socket option ( server @$ zmq . zmq_identity @$ __str__ ) ; rc = zmq . bind ( server @$ host ) ; assert that ( rc @$ is ( true ) ) ; host = ( string ) zmq . get socket option ext ( server @$ zmq . zmq_last_endpoint ) ; int port = test utils . port ( host ) ;,socket accept,success,pre
unmanaged a ms <PLACE_HOLDER> return amrm token,assert . assert not null ( rm client . getamrm token ( app id ) ) ; return app id ;,ms do,success,pre
create second expired certificate whose key usage extension does not <PLACE_HOLDER> code signing,create alias ( second_key_alias ) ; issue cert ( second_key_alias @$ __str__ @$ __str__ @$ __str__ @$ __str__ + validity * __num__ + __str__ @$ __str__ @$ integer . to string ( validity ) ) ;,extension allow,success,pre
in case default constructor someday <PLACE_HOLDER> something,this ( ) ;,constructor does,success,pre
we allow one fewer output buffer due to the way that media codec renderer and the underlying decoders <PLACE_HOLDER> the end of stream . this should be tightened up in the future .,decoder counters util . assert total buffer count ( tag + audio_tag_suffix @$ audio counters @$ audio counters . input buffer count - __num__ @$ audio counters . input buffer count ) ; decoder counters util . assert total buffer count ( tag + video_tag_suffix @$ video counters @$ video counters . input buffer count - __num__ @$ video counters . input buffer count ) ;,renderer handle,success,pre
make sure vm 1 did n't <PLACE_HOLDER> the bucket,assert that ( vm1 . invoke ( ( ) -> get bucket list ( partitioned region name ) ) ) . is empty ( ) ; vm1 . invoke ( ( ) -> get cache ( ) . close ( ) ) ; async invocation < void > create partitioned region onvm0 = vm0 . invoke async ( ( ) -> create partitioned region ( __num__ @$ - __num__ @$ __num__ @$ true ) ) ; create partitioned region onvm0 . join ( seconds . to millis ( __num__ ) ) ;,vm create,success,pre
if there is no existing named view <PLACE_HOLDER> one,add to cache ( named view detail @$ false ) ; return ;,view add,success,pre
if we have found the account index for this protocol provider <PLACE_HOLDER> this index,if ( index != null ) { return integer . parse int ( index ) ; } else { return create account index ( protocol provider @$ account root prop name ) ; },index use,fail,pre
support for automatic static linking of standard libraries . this works because all of the jdk <PLACE_HOLDER> system.load library or jdk.internal.loader.boot loader with literal string arguments . if such a library is in our list of static standard libraries @$ add the library to the linker command .,if ( libname != null && native library support . singleton ( ) . is preregistered builtin library ( libname ) && registered libraries . put if absent ( libname @$ boolean . true ) != boolean . true ) { native libraries . add library ( libname @$ true ) ; },all extends,fail,pre
mock suspend @$ which <PLACE_HOLDER> supervisor 1 and sets suspended state in metadata @$ flipping to supervisor 2 in test supervisor spec implementation of create suspended spec,reset all ( ) ; metadata supervisor manager . insert ( easy mock . eq ( __str__ ) @$ easy mock . capture ( captured insert ) ) ; supervisor2 . start ( ) ; supervisor1 . stop ( true ) ; replay all ( ) ; manager . suspend or resume supervisor ( __str__ @$ true ) ; assert . assert equals ( __num__ @$ manager . get supervisor ids ( ) . size ( ) ) ; assert . assert equals ( captured insert . get value ( ) @$ manager . get supervisor spec ( __str__ ) . get ( ) ) ; assert . assert true ( captured insert . get value ( ) . suspended ) ; verify all ( ) ;,which stops,success,pre
nm 1 <PLACE_HOLDER> 50 heartbeats,capacity scheduler cs = ( capacity scheduler ) rm1 . get resource scheduler ( ) ; rm node rm node1 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ; rm node rm node2 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm2 . get node id ( ) ) ; scheduler node scheduler node1 = cs . get scheduler node ( nm1 . get node id ( ) ) ;,nm do,success,pre
if <PLACE_HOLDER>able is declared to be <PLACE_HOLDER>n by the proxy method @$ then no catch blocks are necessary @$ because the invoke can @$ at most @$ <PLACE_HOLDER> <PLACE_HOLDER>able anyway .,if ( ex . is assignable from ( throwable . class ) ) { unique list . clear ( ) ; break ; } else if ( ! throwable . class . is assignable from ( ex ) ) { continue ; },invoke throw,success,pre
check size will <PLACE_HOLDER> a new file if we exceeded the max bytes setting,writer . check size ( ) ; if ( writer . get position ( ) != position ) { position = writer . get position ( ) ; },size open,success,pre
use in alert <PLACE_HOLDER> removal of x,test same ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,use prevent,fail,pre
asif asif : the iter operands passed are null @$ as a not null value can <PLACE_HOLDER> only if there <PLACE_HOLDER> a single filter operand in original group junction,filter results = filter . filter evaluate ( context @$ ! is conditioning needed ? intermediate results : null @$ this . complete expansion @$ null @$ this . indpndnt itr @$ _operator == literal_and @$ is conditioning needed @$ false ) ;,value change,fail,pre
a media player created by a video view should already <PLACE_HOLDER> its m subtitle controller set .,if ( m subtitle controller == null ) { set subtitle anchor ( ) ; } if ( ! m subtitle controller . has renderer for ( f format ) ) { context context = activity thread . current application ( ) ; m subtitle controller . register renderer ( new srt renderer ( context @$ m event handler ) ) ; },player have,success,pre
mp 3 live streams commonly <PLACE_HOLDER> seekable metadata @$ despite being unseekable .,if ( icy headers != null && extractor instanceof mp3 extractor ) { ( ( mp3 extractor ) extractor ) . disable seeking ( ) ; } if ( pending extractor seek ) { extractor . seek ( position @$ seek time us ) ; pending extractor seek = false ; } while ( result == extractor . result_continue && ! load canceled ) { load condition . block ( ) ; result = extractor . read ( input @$ position holder ) ; if ( input . get position ( ) > position + continue loading check interval bytes ) { position = input . get position ( ) ; load condition . close ( ) ; handler . post ( on continue loading requested runnable ),streams return,fail,pre
redex access the constructed proguard command line and then <PLACE_HOLDER> and opens a bunch of the files listed there .,return immutable list . < source path > builder ( ) . add all ( classpath entries to dex source paths ) . add all ( rich stream . from ( proguard config ) . collect ( collectors . to list ( ) ) ) . add all ( proguard configs ) . build ( ) ;,access creates,fail,pre
test <PLACE_HOLDER> 0 bits .,test array . set position ( __num__ ) ; result [ __num__ ] = __num__ ; test array . read bits ( result @$ __num__ @$ __num__ ) ; assert that ( result [ __num__ ] ) . is equal to ( ( byte ) __num__ ) ;,test has,fail,pre
successful <PLACE_HOLDER> request results in a response with empty body .,if ( status code == __num__ ) { if ( http method . connect . equals ( method ) ) { done = true ; queue . clear ( ) ; return true ; } },successful connect,success,pre
killed jobs might not <PLACE_HOLDER> counters,if ( total counters != null ) { json object j groups = new json object ( ) ; for ( counter group counter group : total counters ) { string group name = counter group . get name ( ) ; counter group total group = total counters . get group ( group name ) ; counter group map group = map counters . get group ( group name ) ; counter group reduce group = reduce counters . get group ( group name ) ; iterator < counter > ctr itr = total group . iterator ( ) ; json array j group = new json array ( ) ; while ( ctr itr . has next ( ) ) { json object j counter =,jobs have,success,pre
we are tolerant here because frameworks such as avro <PLACE_HOLDER> a boxed type even though the field is primitive,if ( ! box primitive ( parameter type ) . equals ( box primitive ( field type ) ) ) { return null ; },frameworks require,fail,pre
this is too long @$ and the ri just <PLACE_HOLDER> the input string ...,string long input = make puny string ( __num__ ) ; assert equals ( long input @$ idn . to unicode ( long input ) ) ;,ri drops,fail,pre
simple date format <PLACE_HOLDER> this behavior,date time formatter test = fmt . with locale ( locale . english ) . with decimal style ( decimal style . standard ) ; format format = test . to format ( ) ; parse position pos = new parse position ( __num__ ) ; format . parse object ( ( string ) null @$ pos ) ;,format has,success,pre
the healthy path will never <PLACE_HOLDER> this synchronized block,synchronized ( resource holder . bad_filters ) { long last time = resource holder . bad_filters . get ( f ) ; if ( last time == null || last time + time unit . minutes . to millis ( interval ) < system . current time millis ( ) ) { resource holder . bad_filters . put ( f @$ system . current time millis ( ) ) ; return level . warning ; } else { return level . fine ; } },path use,fail,pre
let basic date click <PLACE_HOLDER>r <PLACE_HOLDER> calendar dates @$ and update only the other parts of ui here,calendar component . set handler ( new basic date click handler ( ) { @ override public void date click ( date click event event ) { super . date click ( event ) ; switch to day view ( ) ; } } ) ;,handler handle,success,pre
this will actually mark the parent as a dir @$ so that lists of that dir will <PLACE_HOLDER> up the tombstone,put file ( child @$ now @$ null ) ;,lists pick,success,pre
check whether a provider can provide an implementation that 's closer to the requested locale than what the java <PLACE_HOLDER> itself can provide .,try { return new simple date format ( time style @$ date style @$ loc ) ; } catch ( missing resource exception e ) { return new simple date format ( __str__ ) ; },java object,fail,pre
some tests only run on linux @$ those wo n't <PLACE_HOLDER> a client on other os,if ( client != null ) client . close ( ) ;,tests have,fail,pre
now new job requests should succeed as list operation <PLACE_HOLDER> no cancel threads .,job runnable = concurrent list jobs ( __num__ @$ config @$ false @$ false @$ list job helper . get delayed resonse answer ( __num__ @$ new array list < job item bean > ( ) ) ) ; assert true ( job runnable . exception == null ) ;,operation has,success,pre
we bypass the standard <PLACE_HOLDER> method that resolve the hostname,assert that ( slave . get channel ( ) . call ( new url builder callable ( __str__ ) ) @$ not ( equal to ( slave . get channel ( ) . call ( new url builder callable ( __str__ ) ) ) ) ) ;,standard equals,success,pre
expressions <PLACE_HOLDER> linked conditionals ; statements do not .,return node util . is statement ( parent ) ? empty : propagate ( false ) ;,expressions have,fail,pre
for joda time types @$ return <PLACE_HOLDER> type for java.util.date .,if ( raw type instanceof class && abstract instant . class . is assignable from ( ( class < ? > ) raw type ) ) { return type factory . create java type ( date . class ) ; } else if ( raw type instanceof class && byte string . class . is assignable from ( ( class < ? > ) raw type ) ) { return type factory . create java type ( byte [ ] . class ) ; } return type factory . create java type ( ( class ) raw type ) ;,return object,fail,pre
check any changes do not <PLACE_HOLDER> already encoded strings,string password = __str__ ; string obfuscate = __str__ ; assert equals ( password @$ password . deobfuscate ( obfuscate ) ) ;,changes affect,fail,pre
at this point all subregions are destroyed and this region has been marked as destroyed and post destroy region has been called for each region . the only detail left is <PLACE_HOLDER> this region from the parent subregion map @$ and sending listener events,assert . assert true ( is destroyed ) ;,left removing,fail,pre
put records into the queues <PLACE_HOLDER> the queue distributor . each worker will pull and process .,try { record record = records . next ( ) ; id distributor . distribute ( record @$ record consumer ) ; progress . add ( __num__ ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; break ; },queues invoke,fail,pre
this call to shutdown will eventually <PLACE_HOLDER> a call to population cancelled on the monitor below,dbms . shutdown ( ) ;,call creat,fail,pre
make sure the fragment either <PLACE_HOLDER> no dependency or dependencies have been dealt with,set < node > node with fragment = connected nodes . stream ( ) . filter ( node -> ! node . get fragments without dependency ( ) . is empty ( ) || ! node . get fragments with dependency visited ( ) . is empty ( ) ) . collect ( collectors . to set ( ) ) ;,fragment has,success,pre
cut off to maximum queue lifetime if update lifetime is <PLACE_HOLDER> queue lifetime .,new expire time . put ( application timeout type . lifetime @$ updatedlifetime in millis ) ; new timeout iniso8601 format . put ( application timeout type . lifetime @$ times . formatiso8601 ( updatedlifetime in millis . long value ( ) ) ) ;,lifetime expecting,fail,pre
if the expanded child <PLACE_HOLDER> the same height as the collapsed one we hide it .,if ( m expanded child != null && m expanded child . get height ( ) != __num__ ) { if ( ( ! m is heads up && ! m heads up animating away ) || m heads up child == null || ! m containing notification . can show heads up ( ) ) { if ( m expanded child . get height ( ) <= m contracted child . get height ( ) ) { expandable = false ; } } else if ( m expanded child . get height ( ) <= m heads up child . get height ( ) ) { expandable = false ; } } if ( m expanded child != null ) { m expanded wrapper . update expandability,child has,success,pre
create a job launcher instance depending on the configuration . the same properties object is used for both system and job configuration properties because azkaban <PLACE_HOLDER> configuration properties in the .job file and in the .properties file into the same properties object .,this . job launcher = this . closer . register ( job launcher factory . new job launcher ( job props @$ job props @$ null @$ metadata tags ) ) ;,azkaban encapsulates,fail,pre
nor the admin <PLACE_HOLDER> that access,jenkins rule . web client wc = j . create web client ( ) ; wc . login ( __str__ ) ; wc . get options ( ) . set throw exception on failing status code ( false ) ; page page = wc . go to ( __str__ + __num__ + __str__ ) ; assert equals ( __num__ @$ page . get web response ( ) . get status code ( ) ) ; assert request was blocked and reset flag ( ) ;,admin has,fail,pre
server did not <PLACE_HOLDER> the invalidation @$ so do n't leave an invalid entry here,if ( ! force new entry && event . no version received from server ( ) ) { return false ; },server perform,success,pre
rotate the queue so that each filter <PLACE_HOLDER> the descriptions in a different order,for ( filter filter : filters ) { mutating descriptions . add last ( mutating descriptions . poll first ( ) ) ; for ( description description : descriptions ) { if ( filter . should run ( description ) ) { add description for filter to map ( descriptions run @$ filter @$ description ) ; } } },filter enters,fail,pre
server socket will <PLACE_HOLDER> connections,socket base req = zmq . socket ( ctx @$ bind type ) ; assert that ( req @$ not null value ( ) ) ; boolean rc = zmq . bind ( req @$ address ) ; assert that ( rc @$ is ( true ) ) ; socket base router = zmq . socket ( ctx @$ connect type ) ; assert that ( router @$ not null value ( ) ) ; string host = ( string ) zmq . get socket option ext ( req @$ zmq . zmq_last_endpoint ) ; assert that ( host @$ not null value ( ) ) ; rc = zmq . connect ( router @$ host ) ; assert that ( rc @$ is ( true ) ),socket accept,success,pre
tag change from 'unresolved ' to 'string ' does not <PLACE_HOLDER> atomically . we just look at the object at the corresponding index and decide based on the oop type .,if ( ctag . is string ( ) ) { symbol sym = cpool . get unresolved string at ( cp index ) ; return __str__ + sym . as string ( ) + __str__ ; } else if ( ctag . is klass ( ) || ctag . is unresolved klass ( ) ) { constant pool . cp slot obj = cpool . get slot at ( cp index ) ; if ( obj . is resolved ( ) ) { klass k = obj . get klass ( ) ; return __str__ + k . get name ( ) . as string ( ) + __str__ + k . get address ( ) + __str__ ; } else if ( obj . is unresolved (,change happen,success,pre
default null means no default @$ so the tag is irrelevant since it is used to re<PLACE_HOLDER> a <PLACE_HOLDER>tings sub<PLACE_HOLDER> their defaults . also it is irrelevant if the system <PLACE_HOLDER> the canonical default .,if ( default value == null ) { tag = null ; default from system = false ; },system use,fail,pre
make sure the holder that we send to the peer does <PLACE_HOLDER> an accurate region version holder for our local version,return create copy ( this . my id @$ cloned holders @$ this . local version . get ( ) @$ gc versions @$ this . localgc version . get ( ) @$ false @$ cloned local holder ) ;,holder contain,fail,pre
white rectangle detector returns <PLACE_HOLDER> inside of the rectangle . i want <PLACE_HOLDER> on the edges .,float centerx = ( pointa . getx ( ) + pointb . getx ( ) + pointc . getx ( ) + pointd . getx ( ) ) / __num__ ; float centery = ( pointa . gety ( ) + pointb . gety ( ) + pointc . gety ( ) + pointd . gety ( ) ) / __num__ ; pointa = move away ( pointa @$ centerx @$ centery ) ; pointb = move away ( pointb @$ centerx @$ centery ) ; pointc = move away ( pointc @$ centerx @$ centery ) ; pointd = move away ( pointd @$ centerx @$ centery ) ; result point point bs ; result point point ds ;,returns leak,fail,pre
check if the statements <PLACE_HOLDER> a call to check not null or require non null,if ( is expression ) { expression expression = ( expression ) stat ; if ( expression instanceof assignment ) expression = ( ( assignment ) expression ) . expression ; if ( ! ( expression instanceof message send ) ) return null ; message send invocation = ( message send ) expression ; if ( ! arrays . equals ( invocation . selector @$ check_not_null ) && ! arrays . equals ( invocation . selector @$ require_non_null ) ) return null ; if ( invocation . arguments == null || invocation . arguments . length == __num__ ) return null ; expression first argument = invocation . arguments [ __num__ ] ; if ( ! ( first argument instanceof single name reference ) ) return null ;,statements contains,success,pre
then we should be able to start a transaction @$ though perhaps not be able to finish it . this is what the individual test methods will be <PLACE_HOLDER> . the test passes when transaction.close completes within the test timeout @$ that is @$ it did n't deadlock .,start log rotation latch . await ( ) ;,methods doing,success,pre
let 's <PLACE_HOLDER> a lot of constant rows to test the rle,row . set field value ( __num__ @$ null ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; row . set field value ( __num__ @$ null ) ; for ( int i = __num__ ; i < __num__ ; ++ i ) { writer . add row ( row ) ; } union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; writer . add row ( row ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; writer . add row ( row ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ),'s write,fail,pre
noop build rule implements <PLACE_HOLDER> input based rule key @$ which can not add build rule for rule keys .,return false ;,implements build,fail,pre
make copy of ground items because we are going to modify them here @$ and the array list <PLACE_HOLDER> our desired behaviour here,if ( plugin . is hot key pressed ( ) ) { ground item list = new array list < > ( ground item list ) ; final java . awt . point awt mouse pos = new java . awt . point ( mouse pos . getx ( ) @$ mouse pos . gety ( ) ) ; ground item ground item = null ; for ( ground item item : ground item list ) { item . set offset ( offset map . compute ( item . get location ( ) @$ ( k @$ v ) -> v != null ? v + __num__ : __num__ ) ) ; if ( ground item != null ) { continue ; } if ( plugin . get,list reflects,fail,pre
do n't let focus issues <PLACE_HOLDER> the popup list,panel . set ignore focus ( true ) ; return p ;,issues hide,success,pre
this member <PLACE_HOLDER> a lower size that ca n't fit buckets of size 30,partition member info impl details3 = build details ( member3 @$ __num__ @$ __num__ @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ) ; model . add region ( __str__ @$ arrays . as list ( details1 @$ details2 @$ details3 ) @$ new fake offline details ( ) @$ true ) ; assert equals ( __num__ @$ do moves ( new composite director ( false @$ false @$ true @$ true ) @$ model ) ) ; assert equals ( collections . empty list ( ) @$ bucket operator . creates ) ;,member has,success,pre
minimum two level <PLACE_HOLDER> artifact size,try ( in memory artifact cache in memory artifact cache = new in memory artifact cache ( ) ; two level artifact cache decorator two level cache = new two level artifact cache decorator ( in memory artifact cache @$ test project filesystems . create project filesystem ( tmp . get root ( ) ) @$ buck event bus for tests . new instance ( ) @$ true @$ __num__ @$ optional . empty ( ) ) ) { lazy path dummy file = lazy path . of instance ( tmp . new file ( ) ) ; string test metadata key = __str__ ; two level cache . store ( artifact info . builder ( ) . add rule keys ( dummy rule key ) . set,level stored,success,pre
treat this as deterministic for reporting purposes : delete statements <PLACE_HOLDER> just one row that is the number of rows affected,boolean order is deterministic = true ; boolean has limit or offset = m_parsed delete . has limit or offset ( ) ;,statements have,fail,pre
if this thread created mid key @$ block until the other thread <PLACE_HOLDER> a dep on it .,if ( order == order . before && thread . current thread ( ) . equals ( first thread . get ( ) ) ) { tracking awaiter . instance . await latch and track exceptions ( other thread winning @$ __str__ ) ; } else if ( order == order . after && ! thread . current thread ( ) . equals ( first thread . get ( ) ) ) { other thread winning . count down ( ) ; },thread has,fail,pre
if primitive just <PLACE_HOLDER> the primitive class not the boxed version,if ( type mirror . get kind ( ) . is primitive ( ) ) { class type = type name . get ( type mirror ) ; },primitive take,fail,pre
another thread already <PLACE_HOLDER> this path and is processing it . wait for the other thread to finish @$ by locking the existing read lock .,if ( existing lock != null ) { write lock . unlock ( ) ; write lock = null ; read lock = existing lock . read lock ( ) ; read lock . lock ( ) ; if ( m cache . get if present ( alluxio uri . get path ( ) ) != null ) { return false ; } } else { mount table . resolution resolution = m mount table . resolve ( alluxio uri ) ; if ( resolution . get mount id ( ) != mount info . get mount id ( ) ) { return false ; } boolean exists in ufs ; try ( closeable resource < under file system > ufs resource = resolution . acquire ufs resource,thread loaded,fail,pre
queue level max allocation ca n't <PLACE_HOLDER> the cluster setting,capacity scheduler cs = new capacity scheduler ( ) ; cs . set conf ( new yarn configuration ( ) ) ; cs . setrm context ( resource manager . getrm context ( ) ) ; capacity scheduler configuration conf = new capacity scheduler configuration ( ) ; setup queue configuration ( conf ) ; set max alloc mb ( conf @$ yarn configuration . default_rm_scheduler_maximum_allocation_mb ) ; set max alloc vcores ( conf @$ yarn configuration . default_rm_scheduler_maximum_allocation_vcores ) ; long larger mem = yarn configuration . default_rm_scheduler_maximum_allocation_mb + __num__ ; long larger vcores = yarn configuration . default_rm_scheduler_maximum_allocation_vcores + __num__ ; cs . init ( conf ) ; cs . start ( ) ; cs . reinitialize ( conf @$ mock context ) ; check queue,allocation override,fail,pre
the output stream must be flushed before the length is obtained as the flush can <PLACE_HOLDER> the length forward .,section output stream . flush ( ) ; long length = file channel . position ( ) - sub section offset ; if ( length == __num__ ) { log . warn ( __str__ + __str__ @$ name . to string ( ) ) ; return ; } summary . add sections ( file summary . section . new builder ( ) . set name ( name . name ) . set length ( length ) . set offset ( sub section offset ) ) ; sub section offset += length ;,flush move,success,pre
java ca n't have a root whose value is null . instead of setting null @$ the method <PLACE_HOLDER> user data so that other methods are able to know the root should be nil .,if ( new_root == context . nil ) { get document ( ) . get document element ( ) . set user data ( nokogiri helpers . root_node_invalid @$ boolean . true @$ null ) ; return new_root ; } xml node new root = as xml node ( context @$ new_root ) ; i ruby object root = root ( context ) ; if ( root . is nil ( ) ) { node new root node ; if ( get document ( ) == new root . get owner document ( ) ) { new root node = new root . node ; } else { new root node = get document ( ) . import node ( new root . node @$ true ) ;,method set,fail,pre
backslash and <PLACE_HOLDER> quote need <PLACE_HOLDER> the escapement for both java and haskell,special char replacements . remove ( __str__ ) ; special char replacements . remove ( __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ;,backslash take,fail,pre
the primary zygote did n't match . <PLACE_HOLDER> the secondary .,if ( m zygote secondary socket address != null ) { attempt connection to secondary zygote ( ) ; if ( secondary zygote state . matches ( abi ) ) { return secondary zygote state ; } },primary try,success,pre
incorrect for not yet <PLACE_HOLDER> 3 rd party storage components .,return call . empty list ( ) ;,incorrect support,fail,pre
authorization check for kill query will be in kill query impl as both admin or operation owner can <PLACE_HOLDER> the operation . which is not directly supported in authorizer,if ( driver context . get query state ( ) . get hive operation ( ) != hive operation . kill_query ) { command authorizer . do authorization ( driver context . get query state ( ) . get hive operation ( ) @$ sem @$ context . get cmd ( ) ) ; } session state . get perf logger ( ) . perf log end ( class_name @$ perf logger . do_authorization ) ;,owner execute,fail,pre
<PLACE_HOLDER>nt <PLACE_HOLDER> actual config in check mode .,boolean configcheck = boolean . parse boolean ( config . get property ( export manager . config_check_only @$ __str__ ) ) ; if ( configcheck ) { return ; } m_is krb = boolean . parse boolean ( config . get property ( __str__ @$ __str__ ) ) ; if ( m_is krb ) { m_context = new login context ( __str__ ) ; m_context . login ( ) ; } connect ( ) ; if ( m_is hdfs && endpoint expander . has date conversion ( m_endpoint ) ) { runnable rotator = new runnable ( ) { @ override public void run ( ) { try { roll ( ) ; } catch ( throwable t ) { m_logger . error ( __str__ + throwables .,dont do,success,pre
wait for the present complete semaphore to be signaled to ensure that the image wo n't be rendered to until the presentation engine has fully <PLACE_HOLDER> ownership to the application @$ and it is okay to render to the image .,demo_draw_build_cmd ( ) ; long buffer lp2 = stack . malloc long ( __num__ ) ; vk submit info submit_info = vk submit info . malloc stack ( stack ) . s type ( vk_structure_type_submit_info ) . p next ( null ) . wait semaphore count ( __num__ ) . p wait semaphores ( lp . put ( __num__ @$ image acquired semaphore ) ) . p wait dst stage mask ( ip . put ( __num__ @$ vk_pipeline_stage_bottom_of_pipe_bit ) ) . p command buffers ( pp . put ( __num__ @$ draw_cmd ) ) . p signal semaphores ( lp2 . put ( __num__ @$ draw complete semaphore ) ) ; check ( vk queue submit ( queue @$ submit_info @$ vk_null_handle ) ) ; vk present,engine taken,fail,pre
this flow covers instance not <PLACE_HOLDER> exception . actual code just eating the exception . i.e actual code just printing the stacktrace @$ whenever an exception of type instance not <PLACE_HOLDER> exception occurs .,mbean container . bean removed ( null @$ managed ) ;,exception found,success,pre
the entries in the original logs are <PLACE_HOLDER> regions considering the sequence file header @$ the middle corruption should affect at least half of the entries,int good entries = ( num_writers - __num__ ) * entries ; int first half entries = ( int ) math . ceil ( entries / __num__ ) - __num__ ; int all regions count = split and count ( num_writers @$ - __num__ ) ; assert true ( __str__ @$ regions . size ( ) * ( good entries + first half entries ) <= all regions count ) ;,entries spanding,fail,pre
we can assume here that the request <PLACE_HOLDER> all the parameters needed for authentication etc .,try { o auth2 access token token = retrieve token ( request @$ resource @$ get parameters for token request ( resource @$ request ) @$ get headers for token request ( request ) ) ; if ( token == null ) { throw new user redirect required exception ( resource . get user authorization uri ( ) @$ request . to single value map ( ) ) ; } return token ; } catch ( user redirect required exception e ) { throw new user redirect required exception ( e . get redirect uri ( ) @$ request . to single value map ( ) ) ; },request contains,success,pre
get tunnel interface record ; if no such interface is found @$ will <PLACE_HOLDER> illegal argument exception,tunnel interface record tunnel interface info = user record . m tunnel interface records . get resource or throw ( tunnel resource id ) ; try { m srv config . get netd instance ( ) . interface add address ( tunnel interface info . m interface name @$ local addr . get address ( ) . get host address ( ) @$ local addr . get prefix length ( ) ) ; } catch ( remote exception e ) { throw e . rethrow from system server ( ) ; },record throw,success,pre
this drop target does n't <PLACE_HOLDER> any flavors .,return false ;,target support,fail,pre
fast source : source produce 8 before @$ and 8 after invocation . buffered source should <PLACE_HOLDER> all 16 at once .,mock source = new mock split source ( ) . set batch size ( __num__ ) ; try ( split source source = new buffering split source ( mock source @$ __num__ ) ) { mock source . increase available splits ( __num__ ) ; listenable future < next batch result > next batch future = get next batch ( source @$ __num__ ) ; assert false ( next batch future . is done ( ) ) ; mock source . increase available splits ( __num__ ) ; require future value ( next batch future ) . assert size ( __num__ ) . assert no more splits ( false ) ; },source collect,fail,pre
check that cluster status <PLACE_HOLDER> the correct active and backup masters,assert not null ( active ) ; cluster metrics status = active . get cluster metrics ( ) ; assert true ( status . get master name ( ) . equals ( active name ) ) ; assert equals ( __num__ @$ status . get backup master names ( ) . size ( ) ) ;,status reports,success,pre
although the streaming ops <PLACE_HOLDER> multiple keys @$ we only query one key here,byte array key ; try { if ( key format . equals ( admin parser utils . opt_json ) ) { object key object ; string key serializer name = key serializer def . get name ( ) ; if ( is avro schema ( key serializer name ) ) { schema key schema = schema . parse ( key serializer def . get current schema info ( ) ) ; json decoder decoder = new json decoder ( key schema @$ key string ) ; generic datum reader < object > datum reader = new generic datum reader < object > ( key schema ) ; key object = datum reader . read ( null @$ decoder ) ; } else if ( key serializer name .,ops support,success,pre
sorani <PLACE_HOLDER> stemmer,if ( __str__ . equals ignore case ( language ) ) { return new sorani stem filter ( token stream ) ; } else if ( __str__ . equals ignore case ( language ) ) { return new snowball filter ( token stream @$ new swedish stemmer ( ) ) ; } else if ( __str__ . equals ignore case ( language ) || __str__ . equals ignore case ( language ) ) { return new swedish light stem filter ( token stream ) ; } else if ( __str__ . equals ignore case ( language ) ) { return new snowball filter ( token stream @$ new turkish stemmer ( ) ) ; },sorani label,fail,pre
verify that the configuration now <PLACE_HOLDER> three rules .,configuration = s3 client . get bucket lifecycle configuration ( bucket name ) ; system . out . println ( __str__ + configuration . get rules ( ) . size ( ) ) ;,configuration has,success,pre
validate router failure response <PLACE_HOLDER> nn failure response .,method m = client protocol . class . get method ( __str__ @$ string . class @$ string . class @$ enum set writable . class ) ; string bad path = __str__ ; enum set writable < create flag > create flag writable = new enum set writable < create flag > ( create flag ) ; compare responses ( router protocol @$ nn protocol @$ m @$ new object [ ] { bad path @$ __str__ @$ create flag writable } ) ;,response matches,success,pre
does the handler type map <PLACE_HOLDER> a count and address .,if ( handler type count == __num__ || handler type map address == null || ( is relative ( ) && image base address . equals ( handler type map address ) ) ) { throw new invalid data type exception ( get name ( ) + __str__ ) ; },map contain,fail,pre
next task <PLACE_HOLDER> less than 10 ms and should be only aggregated,try ( silent closeable c2 = profiler . profile ( profiler task . action_check @$ __str__ ) ) { profiler . log simple task ( blaze clock . instance ( ) . nano time ( ) @$ profiler task . vfs_stat @$ __str__ ) ; long start time = blaze clock . instance ( ) . nano time ( ) ; clock . advance millis ( __num__ ) ; profiler . log simple task ( start time @$ profiler task . vfs_stat @$ __str__ ) ; },task takes,success,pre
agg <PLACE_HOLDER> stats,if ( is spout ) { map mm = new hash map ( ) ; map acked = client stats util . get map by key ( stats @$ acked ) ; for ( object win : acked . key set ( ) ) { mm . put ( win @$ agg spout lat and count ( ( map ) comp lat stats . get ( win ) @$ ( map ) acked . get ( win ) ) ) ; } mm = swap map order ( mm ) ; w2comp lat wgt avg = client stats util . get map by key ( mm @$ comp_lat_total ) ; w2acked = client stats util . get map by key ( mm @$ acked ) ; } else,agg sent,fail,pre
disable opening <PLACE_HOLDER> window during setup,if ( ! is user setup complete ( ) ) { return ; },opening open,fail,pre
if the value is 1 byte and the byte <PLACE_HOLDER> null @$ attempt to create the entry . this test needs to be moved to data serializer or data serializer.null needs to be publicly accessible .,boolean result = false ; if ( value == null ) { result = region . basic bridge create ( key @$ null @$ true @$ callback arg @$ server connection . get proxyid ( ) @$ true @$ new eventid holder ( event id ) @$ false ) ; } else { result = region . basic bridge put ( key @$ value @$ null @$ is object @$ callback arg @$ server connection . get proxyid ( ) @$ true @$ new eventid holder ( event id ) ) ; },byte represents,success,pre
in the worst possible case all the unique values come in consecutive order & hence only 5 iterations will <PLACE_HOLDER> the result,query observer old = query observer holder . set instance ( new query observer adapter ( ) { @ override public void after iteration evaluation ( object result ) { num [ __num__ ] += __num__ ; } @ override public void before iteration evaluation ( compiled value ritr @$ object curr object ) { if ( data . contains ( curr object ) ) { num repeat [ __num__ ] += __num__ ; } else { data . add ( curr object ) ; } } } ) ; string query string = __str__ ; query = qs . new query ( query string ) ; result = ( select results ) query . execute ( ) ; assert equals ( ( __num__ + num repeat [,values change,fail,pre
photos and albums have <PLACE_HOLDER> ds start<PLACE_HOLDER>ng from 1,get album summary ( resp writer @$ ( long ) new random ( ) . next int ( __num__ ) + __num__ ) ; purge all photos ( resp writer ) ; try { latch . await ( ) ; } catch ( interrupted exception e ) { resp writer . println ( e . get message ( ) ) ; },photos nested,fail,pre
now that we know its not a primitive @$ lets just <PLACE_HOLDER> the passed classloader to handle the request .,return class . for name ( class name @$ false @$ cl ) ;,lets call,fail,pre
parse root <PLACE_HOLDER> mapper,document mapper . builder doc builder = new document mapper . builder ( ( root object mapper . builder ) root object type parser . parse ( type @$ mapping @$ parser context ) @$ mapper service ) ; iterator < map . entry < string @$ object > > iterator = mapping . entry set ( ) . iterator ( ) ;,root object,success,pre
generate entities that their string a field <PLACE_HOLDER> a value over the length limitation,if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__ ; i ++ ) { validation demo . union field with inline record union = new validation demo . union field with inline record ( ) ; union . set my enum ( my enum . foofoo ) ; validation demos . add ( new validation demo ( ) . set stringa ( __str__ ) . set inta ( current criteria . get inta ( ) ) . set stringb ( __str__ ) . set union field with inline record ( union ) ) ; } } else if ( current criteria . get inta ( ) == __num__ ) { for ( int i,string has,success,pre
put i ps in h in such a way that we believe the mostfront <PLACE_HOLDER> more chances to get connected,h . add ( domains . chop zoneid ( ipx ) ) ;,mostfront has,fail,pre
this version of hadoop does not <PLACE_HOLDER> ec stats for mr,return ( field != null ) ;,version have,fail,pre
releasing composite should <PLACE_HOLDER> the remaining components,new composite . release ( ) ; assert equals ( __num__ @$ new composite . ref cnt ( ) ) ; assert equals ( __num__ @$ s1 . ref cnt ( ) ) ; assert equals ( __num__ @$ s2 . ref cnt ( ) ) ; assert equals ( __num__ @$ s3 . ref cnt ( ) ) ; assert equals ( __num__ @$ b1 . ref cnt ( ) ) ;,composite release,success,pre
given that we do not delete @$ an empty slot <PLACE_HOLDER> no match .,if ( value ref == __num__ ) { return - __num__ ; },slot means,success,pre
optionally <PLACE_HOLDER> the table truncated between early truncates @$ but always restore the rows towards the end of the iterations .,if ( leave truncated -- <= __num__ ) { volt queuesql ( renewbase0 ) ; volt executesql ( ) ; volt queuesql ( captureview1 ) ; volt queuesql ( captureview2 ) ; after views = volt executesql ( ) ; validate same ( before views [ __num__ ] @$ after views [ __num__ ] ) ; validate same ( before views [ __num__ ] @$ after views [ __num__ ] ) ; } else { volt queuesql ( captureview1 ) ; volt queuesql ( captureview2 ) ; after views = volt executesql ( ) ; validate purged ( after views ) ; },optionally drop,fail,pre
let 's use whatever is currently thrown exception ... may <PLACE_HOLDER> tho,verify exception ( e @$ __str__ ) ;,use fail,fail,pre
create a new x path result <PLACE_HOLDER> reuse result <PLACE_HOLDER> passed in ? the constructor will check the compatibility of type and xobj and throw an exception if they are not compatible .,return new x path result impl ( type @$ xobj @$ context node @$ m_xpath ) ;,result object,success,pre
if both <PLACE_HOLDER> @$ take longest,if ( got positive && got negative ) { if ( positive suffix . length ( ) > negative suffix . length ( ) ) { got negative = false ; } else if ( positive suffix . length ( ) < negative suffix . length ( ) ) { got positive = false ; } },both exist,fail,pre
avoid having to make sure that the array <PLACE_HOLDER> length below .,if ( stop times . length == __num__ ) return collections . empty list ( ) ;,array has,success,pre
move all the if node 's <PLACE_HOLDER> siblings .,move all following ( if node @$ if node . get parent ( ) @$ new dest block ) ; report change to enclosing scope ( if node ) ;,node nested,fail,pre
problem when retrieving a simple role : either role not <PLACE_HOLDER> or not readable @$ so raises a role not <PLACE_HOLDER> exception .,if ( ! ( multi role flg ) ) { try { relation service . throw role problem exception ( pb type @$ role name ) ; return null ; } catch ( invalid role value exception exc ) { throw new runtime exception ( exc . get message ( ) ) ; } } else { result = new role unresolved ( role name @$ null @$ pb type ) ; },role found,success,pre
the request should n't <PLACE_HOLDER> the key event delivery .,new thread ( new runnable ( ) { public void run ( ) { try { thread . sleep ( __num__ ) ; } catch ( exception ex ) { } system . out . println ( __str__ + t2 ) ; t2 . request focus ( ) ; } } ) . start ( ) ; f . set visible ( true ) ; util . wait for idle ( robot ) ; test ( ) ; if ( passed ) system . out . println ( __str__ ) ;,request trigger,fail,pre
actual data does n't matter . just <PLACE_HOLDER> more than 4 k worth,byte b = __str__ ; for ( int i = __num__ ; i < ( __num__ + __num__ ) ; i ++ ) os . write ( b ) ; os . close ( ) ; t . close ( ) ;,data send,success,pre
one last sync whose transactions are not expected to be seen in the input streams because the journal nodes have not <PLACE_HOLDER> their concept of the committed transaction id yet,write txns ( stm @$ __num__ @$ __num__ ) ; future throws ( new io exception ( ) ) . when ( spies . get ( __num__ ) ) . get journaled edits ( __num__ @$ quorum journal manager . qjm_rpc_max_txns_default ) ; future throws ( new io exception ( ) ) . when ( spies . get ( __num__ ) ) . get journaled edits ( __num__ @$ quorum journal manager . qjm_rpc_max_txns_default ) ; list < edit log input stream > streams = new array list < > ( ) ; qjm . select input streams ( streams @$ __num__ @$ true @$ true ) ;,nodes accepted,fail,pre
find which one <PLACE_HOLDER> type argument and return it,type variable < ? > tv = base class . get type parameters ( ) [ __num__ ] ; while ( ! superclasses . is empty ( ) ) { current type = superclasses . pop ( ) ; if ( current type instanceof parameterized type ) { parameterized type pt = ( parameterized type ) current type ; class < ? > raw type = ( class < ? > ) pt . get raw type ( ) ; int arg index = arrays . as list ( raw type . get type parameters ( ) ) . index of ( tv ) ; if ( arg index > - __num__ ) { type type arg = pt . get actual type arguments ( ) [ arg,one has,fail,pre
important ? if source <PLACE_HOLDER> native ids @$ need to store,if ( ! _has native type ids ) { _has native type ids = other . can write type id ( ) ; } if ( ! _has native object ids ) { _has native object ids = other . can write object id ( ) ; } _may have native ids = _has native type ids | _has native object ids ; json parser p = other . as parser ( ) ; while ( p . next token ( ) != null ) { copy current structure ( p ) ; } return this ;,source has,success,pre
interceptor changes <PLACE_HOLDER> value .,interceptor . ret interceptor = new before remove interceptor ( new ignite bi tuple ( false @$ __num__ ) ) ;,changes return,success,pre
events can <PLACE_HOLDER> coming in the moment the input initializer is created . the pruner must be setup and initialized here so that it sets up it 's structures to <PLACE_HOLDER> accepting events . setting it up in initialize leads to a window where events may come in before the pruner is initialized @$ which may cause it to drop events . no dynamic,pruner = null ;,events intercept,fail,pre
verify that the comparison <PLACE_HOLDER> the correct result for all values in both directions .,for ( int i = __num__ ; i < test_keys . length ; ++ i ) { for ( int j = __num__ ; j < test_keys . length ; ++ j ) { byte key left = test_keys [ i ] ; byte key right = test_keys [ j ] ; int cmp = left . compare to ( right ) ; if ( i < j && ! ( cmp < __num__ ) ) { fail ( string . format ( __str__ @$ left @$ right @$ cmp @$ i @$ j ) ) ; } else if ( i == j && ! ( cmp == __num__ ) ) { fail ( string . format ( __str__ @$ left @$ right @$ cmp @$ i,comparison gives,success,pre
registers a callback to be invoked whenever a user <PLACE_HOLDER> a preference .,get preference screen ( ) . get shared preferences ( ) . register on shared preference change listener ( this ) ;,user changes,success,pre
when the target process is reset @$ the breakpoint deleted message will never arrive . it is therefore necessary to delete the deleting breakpoints manually . <PLACE_HOLDER> case 2109 for an example of what can happen .,if ( manager . get breakpoint status ( address @$ breakpoint type . regular ) == breakpoint status . breakpoint_deleting ) { addresses to remove . add ( address ) ; } else if ( manager . get breakpoint status ( address @$ breakpoint type . regular ) != breakpoint status . breakpoint_disabled ) { addresses to disable . add ( address ) ; },message see,success,pre
mark <PLACE_HOLDER> possible duplicate to true for all events in this bucket before it becomes primary on the node,while ( itr . has next ( ) ) { object key = itr . next ( ) ; object sender event = get nolru ( key @$ true @$ false @$ false ) ; if ( sender event != null ) { ( ( gateway sender event impl ) sender event ) . set possible duplicate ( true ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ sender event ) ; } } i ++ ; },mark set,success,pre
validate the xml <PLACE_HOLDER> and return the status,boolean status = validator . validate ( ) ; if ( ! status ) { log . log error ( validator . get error message ( ) ) ; result . set result ( false ) ; result . set nr errors ( validator . get nr errors ( ) ) ; result . set log text ( validator . get error message ( ) ) ; },xml object,fail,pre
do n't add the original policy if it was an expansion flag @$ which <PLACE_HOLDER> no value @$ but do add it if there was either no expansion or if it was a valued flag with implicit requirements .,if ( ! is expansion ) { expanded policies . add ( original policy ) ; } return expanded policies ;,which has,fail,pre
get twitter <PLACE_HOLDER> a flow file @$ then it 's sent via s 2 s,tc . add lineage ( create lineage ( prs @$ __num__ @$ __num__ @$ __num__ ) ) ; test ( tc ) ; wait notifications get delivered ( ) ; final lineage lineage = get lineage ( ) ; final node flow = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patha = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathb = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathc = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patht = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathi = lineage . find node (,twitter creates,fail,pre
assuming the default configuration <PLACE_HOLDER> the correct factories set . users can specify a particular factory by providing a configuration .,if ( conf == null ) { conf = default conf ; },configuration has,success,pre
aet should <PLACE_HOLDER> any ttl set unless it is zero,message . set time to live ( __num__ ) ; message . set text ( __str__ ) ; sender . send ( message ) ; sender . close ( ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ; thread . sleep ( __num__ ) ;,aet override,success,pre
now we want to save the normalizer to a binary file . for doing this @$ one can <PLACE_HOLDER> the normalizer serializer .,normalizer serializer serializer = normalizer serializer . get default ( ) ;,one override,fail,pre
turn off persistent ipc @$ so that the dfs client can <PLACE_HOLDER> nn restart,conf . set int ( common configuration keys public . ipc_client_connection_maxidletime_key @$ __num__ ) ; minidfs cluster cluster = null ; fs data output stream stream ; try { cluster = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) . build ( ) ; file system fs = cluster . get file system ( ) ; dfs util client . getnn address ( conf ) . get port ( ) ; stream = fs . create ( file_path @$ true @$ block_size @$ ( short ) __num__ @$ block_size ) ; stream . write ( data_before_restart ) ; stream . write ( ( byte ) __num__ ) ; stream . hflush ( ) ; cluster . restart name node ( ),client survive,success,pre
let the hash builder operators <PLACE_HOLDER> their accounted memory,partitions no longer needed . set ( null ) ; lock . write lock ( ) . lock ( ) ; try { arrays . fill ( partitions @$ null ) ; lookup source supplier = null ; close cached lookup sources ( ) ; } finally { lock . write lock ( ) . unlock ( ) ; },operators release,fail,pre
delete items that do <PLACE_HOLDER> a keychain item ref .,for ( enumeration e = deleted entries . keys ( ) ; e . has more elements ( ) ; ) { string alias = ( string ) e . next element ( ) ; object entry = deleted entries . get ( alias ) ; if ( entry instanceof trusted cert entry ) { if ( ( ( trusted cert entry ) entry ) . cert ref != __num__ ) { _remove item from keychain ( ( ( trusted cert entry ) entry ) . cert ref ) ; _release keychain item ref ( ( ( trusted cert entry ) entry ) . cert ref ) ; } } else { certificate cert elem ; key entry key entry = ( key entry ) entry ; if,items have,success,pre
if a <PLACE_HOLDER> file has been added or removed @$ reconstruct the <PLACE_HOLDER> file tree .,build file trees . invalidate ( cell ) ;,the build,success,pre
if a topic has k partitions @$ and in the previous run @$ each partition <PLACE_HOLDER> its avg time to pull a record @$ then use the geometric mean of these k numbers as the estimated avg time to pull a record in this run .,double est avg millis for topic = geometric mean ( prev avg millis for partitions ) ; this . est avg millis . put ( topic @$ est avg millis for topic ) ; log . info ( string . format ( __str__ @$ topic @$ est avg millis for topic ) ) ; all est avg millis . add ( est avg millis for topic ) ;,partition has,fail,pre
just pressing the button does not <PLACE_HOLDER> the border ...,set button pressed ( true ) ; assert equals ( empty border button . get border ( ) @$ empty border button . no_button_border ) ;,button draw,fail,pre
does the cell <PLACE_HOLDER> special tag which indicates that the replicated cell visiblilty tags have been modified,if ( is system or super user ( ) ) { tag modified tag = null ; iterator < tag > tags iterator = private cell util . tags iterator ( cell ) ; while ( tags iterator . has next ( ) ) { tag tag = tags iterator . next ( ) ; if ( tag . get type ( ) == tag type . string_vis_tag_type ) { modified tag = tag ; break ; } } pair . set first ( true ) ; pair . set second ( modified tag ) ; return pair ; },cell have,fail,pre
since the system will disconnect and attempt to reconnect a new system the old reference to dtc.system can <PLACE_HOLDER> trouble @$ so we first null it out .,serializable callable fd = new serializable callable ( __str__ ) { @ override public object call ( ) throws exception { null system ( ) ; final locator old locator = locator . get locator ( ) ; final distributed system msys = cache . get distributed system ( ) ; membership manager helper . crash distributed system ( msys ) ; if ( old locator != null ) { wait criterion wc = new wait criterion ( ) { @ override public boolean done ( ) { return msys . is reconnecting ( ) || msys . get reconnected system ( ) != null ; } @ override public string description ( ) { return __str__ + old locator ; } } ; geode awaitility . await,reference cause,success,pre
now flush messages and message <PLACE_HOLDER> commands .,for ( command msg : message cache . values ( ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + ( msg . is message ( ) ? ( ( message ) msg ) . get message id ( ) : msg ) ) ; } transport . oneway ( msg ) ; },messages grant,fail,pre
although we are in doze and would normally allow the device to suspend @$ the doze service has explicitly <PLACE_HOLDER> the display to remain in the on state which means we should hold the display suspend blocker .,if ( m display power request . policy == display power request . policy_doze && m display power request . doze screen state == display . state_on ) { return true ; },service asked,fail,pre
framework classes should be loaded from smalivm 's generated framework jar . this is because the object instantiator will <PLACE_HOLDER> an empty default constructor and one is added whenever classes are built .,if ( class manager . get framework class names ( ) . contains ( internal name ) ) { class < ? > klazz = cached classes . get ( name ) ; if ( klazz != null ) { return klazz ; } klazz = find class ( name ) ; cached classes . put ( name @$ klazz ) ; return klazz ; } return super . load class ( name @$ resolve ) ;,instantiator have,fail,pre
interceptor disables <PLACE_HOLDER> and changes return value .,interceptor . ret interceptor = new before remove interceptor ( new ignite bi tuple ( true @$ __num__ ) ) ;,disables remove,success,pre
make sure decompiler <PLACE_HOLDER> response,native out . flush ( ) ;,decompiler receives,success,pre
open ssl only <PLACE_HOLDER> pkcs 5 padding .,native crypto . evp_cipher_ctx_set_padding ( cipher ctx . get context ( ) @$ padding == padding . pkcs5padding ) ; mode block size = native crypto . evp_cipher_ctx_block_size ( cipher ctx . get context ( ) ) ; called update = false ;,ssl supports,success,pre
the message indicated some error trying to start : do call <PLACE_HOLDER> stop keepalive .,handle stop keepalive ( nai @$ slot @$ reason ) ;,call handle,success,pre
we take the max of the default and whatever the user put in here . each node 's resources can be the sum of several operations @$ so the simplest thing to do is get the max . the situation we want to avoid is that the user <PLACE_HOLDER> low resources on one node @$ and when that node is combined with a bunch,if ( on heap == null ) { on heap = on heap default ; } else { on heap = math . max ( on heap . double value ( ) @$ on heap default . double value ( ) ) ; } if ( off heap == null ) { off heap = off heap default ; } else { off heap = math . max ( off heap . double value ( ) @$ off heap default . double value ( ) ) ; } if ( cpu load == null ) { cpu load = cpu load default ; } else { cpu load = math . max ( cpu load . double value ( ) @$ cpu load default . double value (,user sets,success,pre
parquet <PLACE_HOLDER> excessive logging at info level,parquet logger = logger . get logger ( __str__ ) ; parquet logger . set level ( level . warning ) ;,parquet generates,fail,pre
compilation <PLACE_HOLDER> the tree . this is unrelated to testing recoverable js ast .,return new recoverable js ast ( ast @$ true ) ;,compilation reuse,fail,pre
we need to make sure the section <PLACE_HOLDER> all the way to the shelf,if ( section == last section ) { min bottom position = ( int ) ( view state . get final translationy ( m shelf ) + m shelf . get intrinsic height ( ) ) ; },section goes,success,pre
if this method did not <PLACE_HOLDER> remote exception as required @$ generate the error but continue @$ so that multiple such errors can be reported .,if ( ! has remote exception ) { env . error ( __str__ @$ intf . qualified name ( ) @$ method . name ( ) + method . signature ( ) ) ; errors = true ; continue next method ; },method have,fail,pre
because entries collection is <PLACE_HOLDER> @$ remove and decrement value,if ( entries . is empty ( ) ) { synchronized ( entries ) { if ( entries . is empty ( ) ) { if ( value to entries map . remove ( new key @$ entries ) ) { num index keys . decrement and get ( ) ; internal index stats . inc num keys ( - __num__ ) ; } } } },collection populated,fail,pre
this should never happen but the api has <PLACE_HOLDER> input as nullable,iterable < field > fields new = iterables . transform ( fields @$ new function < field @$ field > ( ) { @ override public schema . field apply ( field input ) { if ( null == input ) { return null ; } field field = new field ( input . name ( ) @$ input . schema ( ) @$ input . doc ( ) @$ input . default value ( ) @$ input . order ( ) ) ; return field ; } } ) ;,api captured,fail,pre
set the mode to streaming first so relay will <PLACE_HOLDER> the scn,cp . set consumption mode ( dbus client mode . online_consumption ) ;,relay establish,fail,pre
if slider <PLACE_HOLDER> number indicator,if ( number indicator != null ) { number indicator . indicator . x = x ; number indicator . indicator . finaly = utils . get relative top ( this ) - get height ( ) / __num__ ; number indicator . indicator . final size = get height ( ) / __num__ ; number indicator . number indicator . set text ( __str__ ) ; },slider has,success,pre
return null since neither dm nor dls are shutting down can not <PLACE_HOLDER> super.cancel in progress because it 's abstract,return null ;,null call,success,pre
vp 9 introduced profiles around 2016 @$ so some vp 9 codecs may not <PLACE_HOLDER> any supported profiles . determine the level for them using the info they provide .,if ( prof levs . length == __num__ && m mime . equals ignore case ( media format . mimetype_video_vp9 ) ) { codec profile level prof lev = new codec profile level ( ) ; prof lev . profile = codec profile level . vp9 profile0 ; prof lev . level = video capabilities . equivalentvp9 level ( info ) ; prof levs = new codec profile level [ ] { prof lev } ; } profile levels = prof levs ; if ( m mime . to lower case ( ) . starts with ( __str__ ) ) { m audio caps = audio capabilities . create ( info @$ this ) ; m audio caps . get default format ( m default format ) ;,codecs have,fail,pre
cdh <PLACE_HOLDER> different names for parquet,if ( __str__ . equals ( input format name ) || __str__ . equals ( input format name ) ) { return mapred parquet input format . class ; } class < ? > clazz = conf . get class by name ( input format name ) ; return ( class < ? extends input format < ? @$ ? > > ) clazz . as subclass ( input format . class ) ;,cdh uses,success,pre
do a <PLACE_HOLDER> file status with empty dir flag,s3a file status status = get status with empty dir flag ( fs @$ base ) ; assert non empty dir ( status ) ; if ( isddb ) { list metric . assert diff equals ( __str__ @$ __num__ ) ; get metric . assert diff equals ( __str__ @$ __num__ ) ; log . info ( __str__ ) ; },a get,success,pre
audio track.release can <PLACE_HOLDER> some time @$ so we call it on a background thread .,final audio track to release = audio track ; audio track = null ; if ( pending configuration != null ) { configuration = pending configuration ; pending configuration = null ; } audio track position tracker . reset ( ) ; releasing condition variable . close ( ) ; new thread ( ) { @ override public void run ( ) { try { to release . flush ( ) ; to release . release ( ) ; } finally { releasing condition variable . open ( ) ; } } } . start ( ) ;,track.release take,success,pre
complete task c @$ which will <PLACE_HOLDER> task d,cmmn runtime service . trigger plan item instance ( get plan item instance id by name and state ( plan item instances @$ __str__ @$ active ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert no plan item instance ( plan item instances @$ __str__ ) ;,which start,success,pre
now new job requests should succeed as status operation <PLACE_HOLDER> no cancel threads .,job runnable = concurrent jobs status ( __num__ @$ config @$ false @$ false @$ status job helper . get delayed resonse answer ( __num__ @$ status bean ) ) ; assert true ( job runnable . exception == null ) ;,operation has,success,pre
sets <PLACE_HOLDER> values to 1 so cache metrics have correct values .,try ( ignite data streamer < integer @$ long > ldr = g . data streamer ( default_cache_name ) ) { ldr . per node parallel operations ( __num__ ) ; ldr . receiver ( new incrementing updater ( ) ) ; for ( int i = __num__ ; i < cnt ; i ++ ) ldr . add data ( i % ( cnt / __num__ ) @$ __num__ ) ; },sets block,fail,pre
initializes the amazon <PLACE_HOLDER> client .,amazon pinpoint pinpoint client = amazon pinpoint client builder . standard ( ) . with region ( regions . us_east_1 ) . build ( ) ; system . out . format ( __str__ @$ endpoints file name @$ application id ) ; try { create import job result import result = pinpoint client . create import job ( create import job request ) ; string job id = import result . get import job response ( ) . get id ( ) ; get import job result get import job result = null ; string job status = null ; do { get import job result = pinpoint client . get import job ( new get import job request ( ) . with job id ( job id,amazon pinpoint,success,pre
junit do n't <PLACE_HOLDER> any way to order tests,pre auth ( ) ; missing auth ( ) ; valid auth ( ) ; valid auth2 ( ) ;,junit offer,fail,pre
fetch the certificates via ldap . ldap cert store <PLACE_HOLDER> its own caching mechanism @$ see the class description for more info . safe cast since xsel is an x 509 certificate selector .,return ( collection < x509 certificate > ) ldap cert store . get certificates ( xsel ) ;,store has,success,pre
if new available range <PLACE_HOLDER> the row @$ try again,if ( range . with length ( first row index @$ number of rows ) . contains ( row index ) ) { registration . get ( ) . remove ( ) ; wait until visible ( row index @$ destination @$ when visible ) ; },range contains,success,pre
there is possibility that we 'll replay txns for a node which was created and then deleted in the fuzzy range @$ and it 's not exist in the snapshot @$ so replay the creation might <PLACE_HOLDER> the cversion and pzxid @$ need to check and only update when it 's larger .,if ( parentc version > parent . stat . get cversion ( ) ) { parent . stat . set cversion ( parentc version ) ; parent . stat . set pzxid ( zxid ) ; } data node child = new data node ( data @$ longval @$ stat ) ; parent . add child ( child name ) ; nodes . post change ( parent name @$ parent ) ; node data size . add and get ( get node size ( path @$ child . data ) ) ; nodes . put ( path @$ child ) ; ephemeral type ephemeral type = ephemeral type . get ( ephemeral owner ) ; if ( ephemeral type == ephemeral type . container ) { containers .,replay change,fail,pre
make an empty path <PLACE_HOLDER> all .,if ( path == null ) { path = __str__ ; },path encompasses,fail,pre
test new configuration should <PLACE_HOLDER> other items,conf = new configuration ( ) ; conf . set ( default_key_acl_prefix + __str__ @$ __str__ ) ; acls . set keyac ls ( conf ) ; assert default key acl ( acls @$ key op type . decrypt_eek @$ __str__ ) ; assert . assert true ( acls . key acls . is empty ( ) ) ; assert . assert true ( acls . whitelist key acls . is empty ( ) ) ; assert . assert equals ( __str__ + acls . default key acls @$ __num__ @$ acls . default key acls . size ( ) ) ;,configuration overwrite,fail,pre
can specify orientation if app does n't <PLACE_HOLDER> parent .,assert equals ( screen_orientation_landscape @$ m token . get orientation ( ) ) ; m token . set fills parent ( true ) ; m token . set hidden ( true ) ; m token . sending to bottom = true ;,app have,fail,pre
list <PLACE_HOLDER> an unmanaged object,list . add ( unmanaged ) ;,list contains,success,pre
as we do n't have a bind we need a unique name . let 's <PLACE_HOLDER> the same as the generated invoker,identifier = generated class name ; context . add declaration ( generated class name @$ object . class ) ;,name call,fail,pre
nn should not <PLACE_HOLDER> the wildcard address by default .,j cluster = new mini journal cluster . builder ( conf ) . format ( true ) . num journal nodes ( num_jn ) . build ( ) ; jn = j cluster . get journal node ( __num__ ) ; string address = get rpc server address ( jn ) ; assert that ( __str__ @$ address @$ not ( __str__ + wildcard_address ) ) ; log . info ( __str__ + dfs_journalnode_rpc_bind_host_key ) ;,nn bind,success,pre
set last <PLACE_HOLDER> type or string by default :,if ( saved type != - __num__ && saved type < type combo box . get item count ( ) ) { type combo box . set selected index ( saved type ) ; } else { type combo box . set selected item ( new supported column type wrapper ( string . class ) ) ; },set saved,success,pre
activity scopes often <PLACE_HOLDER> transient values like task id . make sure bundler service runner is n't stymied by that .,new process ( __str__ ) ; activity = new faux activity ( ) ; activity . create ( bundle ) ; assert that ( activity . root bundler . last loaded ) . is not null ( ) ; assert that ( activity . child bundler . last loaded ) . is not null ( ) ;,scopes contain,fail,pre
remain only first item as exo player does n't <PLACE_HOLDER> adaptive streaming for url list,return list . sub list ( __num__ @$ __num__ ) ;,player support,success,pre
add all nodes from the follow pos of the start node to the follow pos set of the end node @$ which will <PLACE_HOLDER> the effect of letting matches transition from a match state at end node to the second char of a match starting with start node .,end node . f follow pos . add all ( start node . f follow pos ) ;,which have,success,pre
special case : java.lang.annotation.target must not <PLACE_HOLDER> repeated values in its value member,if ( a . annotation type . type . tsym != syms . annotation target type . tsym || a . args . tail == null ) return is valid ; if ( ! a . args . head . has tag ( assign ) ) return false ;,java.lang.annotation.target have,success,pre
control case : admin can <PLACE_HOLDER> default value .,j . submit ( wc . with basic api token ( admin ) . get page ( p @$ __str__ ) . get form by name ( __str__ ) ) ; j . wait until no activity ( ) ; free style build b1 = p . get last build ( ) ; assert equals ( __num__ @$ b1 . get number ( ) ) ; j . assert log contains ( __str__ @$ j . assert build status success ( b1 ) ) ;,admin choose,fail,pre
foo loads <PLACE_HOLDER> own secondary files .,list < string > bar secondaries = m bar user0 . get secondary dex paths ( ) ; notify dex load ( m foo user0 @$ bar secondaries @$ m user0 ) ;,loads bar,success,pre
lower android versions <PLACE_HOLDER> a reference table with 1024 entries only,for ( int i = __num__ ; i < __num__ ; i ++ ) { assert true ( jni test . create and delete int array ( ) ) ; system . out . print ( i ) ; },versions have,success,pre
do this so that the editor is referencing the current renderer from the tree . the renderer can potentially <PLACE_HOLDER> each time laf <PLACE_HOLDER>s .,set default editor ( tree table model . class @$ new tree table cell editor ( ) ) ;,renderer change,success,pre
this function <PLACE_HOLDER> itself recursively,iso run p last iso run = bd . iso runs [ bd . iso run last ] ; opening q opening ; int k @$ opening position @$ closing position ; for ( k = opening index + __num__ ; k < p last iso run . limit ; k ++ ) { q opening = bd . openings [ k ] ; if ( q opening . match >= __num__ ) continue ; if ( new prop position < q opening . context pos ) break ; if ( new prop position >= q opening . position ) continue ; if ( new prop == q opening . context dir ) break ; opening position = q opening . position ; dir props [ opening position,function places,fail,pre
there are 12 issues in total @$ with 10 issues per page @$ the page 2 should only <PLACE_HOLDER> 2 elements,search response result = under test . search ( query . build ( ) @$ new search options ( ) . set page ( __num__ @$ __num__ ) ) ; assert that ( result . get hits ( ) . get hits ( ) ) . has size ( __num__ ) ; assert that ( result . get hits ( ) . get total hits ( ) ) . is equal to ( __num__ ) ; result = under test . search ( issue query . builder ( ) . build ( ) @$ new search options ( ) . set offset ( __num__ ) . set limit ( __num__ ) ) ; assert that ( result . get hits ( ) . get hits ( ) ),issues have,fail,pre
the application will <PLACE_HOLDER> request when waiting for a message @$ which will in turn <PLACE_HOLDER> this on the transport thread .,stream . transport state ( ) . request messages from deframer ( __num__ ) ;,application setup,fail,pre
the install session was aborted @$ <PLACE_HOLDER> up the pending install .,if ( ! success ) { delete rollback ( rollback ) ; return null ; },session clean,success,pre
check if the given <PLACE_HOLDER> task has a child <PLACE_HOLDER> task that contains the target map work if it does not @$ then remove the target from dpp op,if ( ! task contains dependent map work ( task @$ target map work ) ) { to remove . add ( target info ) ; pruning sink op . remove from source event ( target map work @$ target info . part key @$ target info . column name @$ target info . column type ) ; log . info ( __str__ + target map work . get name ( ) + __str__ + base work . get name ( ) + __str__ ) ; },child build,fail,pre
getting action for the null item @$ which in this case <PLACE_HOLDER> the body item,for ( handler ah : action handlers ) { final action [ ] aa = ah . get actions ( null @$ this ) ; if ( aa != null ) { for ( int ai = __num__ ; ai < aa . length ; ai ++ ) { final string akey = action mapper . key ( aa [ ai ] ) ; action set . add ( aa [ ai ] ) ; keys . add ( akey ) ; } } },which contains,fail,pre
fs dir name because fs stats <PLACE_HOLDER> fs dir name,final service metric event . builder builder = builder ( ) . set dimension ( __str__ @$ dir ) ;,stats needs,fail,pre
empty clozes <PLACE_HOLDER> first ord,if ( ords . is empty ( ) && allow empty ) { return new array list < > ( arrays . as list ( new integer [ ] { __num__ } ) ) ; },clozes get,fail,pre
emit max uncommitted offsets messages @$ and fail all of them . then ensure that the spout will <PLACE_HOLDER> them when the <PLACE_HOLDER> backoff has passed,try ( simulated time simulated time = new simulated time ( ) ) { kafka spout < string @$ string > spout = spout with mocked consumer setup helper . setup spout ( spout config @$ conf @$ context mock @$ collector mock @$ consumer mock @$ partition ) ; map < topic partition @$ list < consumer record < string @$ string > > > records = new hash map < > ( ) ; int num records = spout config . get max uncommitted offsets ( ) ; records . put ( partition @$ spout with mocked consumer setup helper . create records ( partition @$ __num__ @$ num records ) ) ; when ( consumer mock . poll ( any long ( ) ) ),spout pull,fail,pre
any change to an input file may <PLACE_HOLDER> program behavior @$ even if only by changing line numbers in error messages .,path fragment extension file = extension label . to path fragment ( ) ; try ( mutability mutability = mutability . create ( __str__ @$ extension file ) ) { starlark thread thread = rule class provider . create rule class starlark thread ( extension label @$ mutability @$ starlark semantics @$ event handler @$ file . get content hash code ( ) @$ import map @$ package factory . get native module ( in workspace ) @$ repository mapping ) ; exec and export ( file @$ extension label @$ event handler @$ thread ) ; event . replay events on ( env . get listener ( ) @$ event handler . get events ( ) ) ; for ( postable post : event handler . get,change implement,fail,pre
utf <PLACE_HOLDER> java string methods,buffered string tmp str = new buffered string ( ) ; for ( int i = __num__ ; i < chk . _len ; i ++ ) { if ( chk . isna ( i ) ) new chk . addna ( ) ; else { string str = chk . at str ( tmp str @$ i ) . to string ( ) ; new chk . add num ( calc entropy ( str ) ) ; } },utf requires,success,pre
checks the status of the rpc call @$ <PLACE_HOLDER> an exception in case of error,check status ( resp . get status ( ) ) ; operation state op state = operation state . get operation state ( resp . get operation state ( ) ) ; hivesql exception op exception = null ; if ( op state == operation state . error ) { op exception = new hivesql exception ( resp . get error message ( ) @$ resp . get sql state ( ) @$ resp . get error code ( ) ) ; } return new operation status ( op state @$ resp . get task status ( ) @$ resp . get operation started ( ) @$ resp . get operation completed ( ) @$ resp . is set has result set ( ) ? resp . is,checks wraps,fail,pre
those following presences would <PLACE_HOLDER> leak information that there is some file satisfying that pattern inside,assert that ( workspace content @$ all of ( not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) ) ) ;,presences create,fail,pre
and if still not found @$ let 's <PLACE_HOLDER> default ?,if ( name == null ) { name = _default type id ( cls ) ; },'s use,fail,pre
this check is required because element impl currently <PLACE_HOLDER> soap body element,if ( ( element instanceof soap body element ) && ! ( element . get class ( ) . equals ( element impl . class ) ) ) { return ( soap element ) element ; } else { return replace element withsoap element ( element @$ ( element impl ) create body element ( name impl . copy element name ( element ) ) ) ; },impl extends,fail,pre
now write the attributes <PLACE_HOLDER> the configuration object .,configuration . write xml attrs ( xml @$ config stats . m configuration ) ; xml . end tag ( null @$ config_tag ) ;,attributes using,fail,pre
wait for all non <PLACE_HOLDER> tasks to be completed,try { boolean acquired = sem . try acquire ( max_waiting_time @$ time unit . milliseconds ) ; assert . assert true ( __str__ @$ acquired ) ; assert . assert equals ( __str__ @$ list . size ( ) @$ nb_add ) ; for ( int i = __num__ ; i < nb_add ; i ++ ) { assert . assert true ( __str__ @$ i < nb_add ) ; } } catch ( interrupted exception e ) { assert . assert false ( __str__ @$ true ) ; },non join,fail,pre
if the id is a generated unique id then this could affect .q <PLACE_HOLDER> golden <PLACE_HOLDER>s for tests that run explain queries .,return __str__ + id + __str__ ;,.q do,fail,pre
check if client reconnected . if so @$ notify apps . <PLACE_HOLDER> oct 35 @$ 2018,log . debug ( __str__ @$ username + __str__ + user id + __str__ @$ client id ) ; sip peer manager . hangup ( peer id @$ client id @$ notify apps ) ;,client observing,fail,pre
if the number of posts on this blog that use this tag is higher than previous @$ <PLACE_HOLDER> this as the most popular tag @$ and <PLACE_HOLDER> the second most popular tag to the current most popular tag,int post count = json this tag . opt int ( __str__ ) ; if ( post count > popular count ) { next most popular tag = most popular tag ; most popular tag = this tag name ; popular count = post count ; } else if ( next most popular tag == null ) { next most popular tag = this tag name ; },number set,success,pre
now round robin assignment with the modified servers list should <PLACE_HOLDER> the primary as the regionserver assignee,assignment map = balancer . round robin assignment ( regions @$ servers ) ; set < server name > server with primary = assignment map . key set ( ) ; assert true ( server before . contains all ( server with primary ) ) ;,assignment have,fail,pre
verify that the includes file <PLACE_HOLDER> all properties,path file resource = new path ( config ) ; conf . add resource ( file resource ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; tear down ( ) ;,file contains,success,pre
assume indirect definitions references <PLACE_HOLDER> the result,for ( node n : refs ) { if ( reference map . is call target ( n ) ) { node call node = reference map . get call or new node for target ( n ) ; if ( node util . is expression result used ( call node ) ) { return false ; } seen use = true ; } else if ( is candidate definition ( n ) ) { seen candidate definiton = true ; } else { if ( ! optimize calls . is allowed reference ( n ) ) { return false ; } } },references reuse,fail,pre
since hot swap passes <PLACE_HOLDER> one file at a time @$ namespaces seen will not include any provides earlier than this current file .,if ( ! in hot swap && import type . must be ordered ( ) && ! namespaces seen . contains ( namespace ) ) { t . report ( call @$ late_provide_error @$ namespace ) ; },passes add,fail,pre
expected ip <PLACE_HOLDER> the given ip address @$ return,if ( inet address . get by name ( expectedip ) . equals ( inet address . get by name ( ip address ) ) ) { return ; },ip matches,success,pre
first <PLACE_HOLDER> p 1 @$ p 3 are able to <PLACE_HOLDER> p 2 is blocked by p 1,wait and assert timestamp ( p1keya @$ __num__ @$ __num__ ) ; wait and assert timestamp ( p2keya @$ __num__ @$ - __num__ ) ; wait and assert timestamp ( p3keyb @$ __num__ @$ __num__ ) ; assert equals ( true @$ locka . get ( ) ) ; assert equals ( true @$ lockb . get ( ) ) ;,1 verify,fail,pre
we need to pass on input method events since some host input method adapters send them through the java event queue instead of directly to the component @$ and the input context also <PLACE_HOLDER> the java composition window,if ( are input methods enabled ( ) ) { if ( ( ( e instanceof input method event ) && ! ( this instanceof composition area ) ) || ( e instanceof input event ) || ( e instanceof focus event ) ) { input context input context = get input context ( ) ; if ( input context != null ) { input context . dispatch event ( e ) ; if ( e . is consumed ( ) ) { if ( ( e instanceof focus event ) && focus log . is loggable ( platform logger . level . finest ) ) { focus log . finest ( __str__ + e ) ; } return ; } } } } else { if (,context tens,fail,pre
note : semgrex can <PLACE_HOLDER> two named nodes to the same node . in this case @$ we simply @$ check the named nodes @$ and if there are any collisions @$ we throw out this <PLACE_HOLDER> .,while ( matcher . find ( ) ) { set < string > node names = matcher . get node names ( ) ; set < indexed word > seen = generics . new hash set ( ) ; for ( string name : node names ) { indexed word curr = matcher . get node ( name ) ; if ( seen . contains ( curr ) ) break next match ; seen . add ( curr ) ; } if ( predicate test != null ) { if ( ! predicate test . test ( matcher ) ) continue ; } semantic graph tgt = semantic graph factory . duplicate keep nodes ( sg ) ; node map = generics . new hash map ( ),semgrex resolve,fail,pre
easiest ways is <PLACE_HOLDER> json object,json object json to encode = new json object ( ) ; json to encode . put ( __str__ @$ custom message . get status code ( ) ) ; json to encode . put ( __str__ @$ custom message . get result code ( ) ) ; json to encode . put ( __str__ @$ custom message . get summary ( ) ) ;,ways construct,fail,pre
register a pre <PLACE_HOLDER> process that will touch the collection and delete the entity,( ( event source ) s ) . get action queue ( ) . register process ( new before transaction completion process ( ) { @ override public void do before transaction completion ( session implementor session ) { q to delete . get fums ( ) . size ( ) ; } } ) ; s . delete ( q to delete ) ; boolean ok = false ; try { s . get transaction ( ) . commit ( ) ; } catch ( lazy initialization exception e ) { ok = true ; s . get transaction ( ) . rollback ( ) ; } catch ( transaction exception te ) { if ( te . get cause ( ) instanceof lazy initialization exception ),pre commit,success,pre
we used to report an error here if the superclass was not resolved . having moved the call to 'check supers ' from 'basic check ' into 'resolve type structure ' @$ the errors reported here should have already been reported . furthermore @$ error recovery can <PLACE_HOLDER> out the superclass @$ which would cause a spurious error from the test here .,env . dt exit ( __str__ + this ) ;,recovery throw,fail,pre
if permissions <PLACE_HOLDER> a review before any of the app components can run @$ we drop the broadcast and if the calling app is in the foreground and the broadcast is explicit we launch the review ui passing it a pending intent to send the skipped broadcast .,if ( ! request start target permissions review if needed locked ( r @$ filter . package name @$ filter . owning user id ) ) { r . delivery [ index ] = broadcast record . delivery_skipped ; return ; } r . delivery [ index ] = broadcast record . delivery_delivered ;,permissions need,success,pre
check types so we can make sure our exported externs <PLACE_HOLDER> type information .,options . set check symbols ( true ) ; return options ;,types have,success,pre
default transfer handler does n't <PLACE_HOLDER> drop so we do n't want drop handling,if ( tree . get drop target ( ) instanceof ui resource ) { tree . set drop target ( null ) ; },handler support,success,pre
eat the exception as it does n't affect the state of existing tables . expect @$ user to manually drop this path when exception and so <PLACE_HOLDER> a warning .,log . warn ( __str__ @$ delete old data loc @$ dbname + __str__ + name ) ;,exception log,fail,pre
reset attempts <PLACE_HOLDER> and update cumulative wait time .,backoff = reset backoff ( duration @$ nano clock @$ start nanos ) ;,attempts delay,fail,pre
use a label if it is marked with one trailing star @$ even if the label string <PLACE_HOLDER> the same when all contractions are suppressed .,if ( item . char at ( item . length ( ) - __num__ ) == __str__ && item . char at ( item . length ( ) - __num__ ) != __str__ ) { item = item . substring ( __num__ @$ item . length ( ) - __num__ ) ; check distinct = false ; } else { check distinct = true ; },string looks,fail,pre
the answer does n't <PLACE_HOLDER> the question . that 's not good .,if ( ! query . get question ( ) . equals ( response . get question ( ) ) ) { badresponse = true ; badresponse_error = __str__ ; log manager . i ( this @$ __str__ + badresponse_error ) ; return ; },answer match,success,pre
this function does n't <PLACE_HOLDER> 'att index ' . we are adding the attribute later after we have figured out that current attribute is not namespace declaration since scan attribute value does n't <PLACE_HOLDER> att index parameter therefore we can safely add the attribute later..,xml string tmp str = get string ( ) ; string localpart = f attributeq name . localpart ; string prefix = f attributeq name . prefix != null ? f attributeq name . prefix : xml symbols . empty_string ; boolean isns decl = f bind namespaces & ( prefix == xml symbols . prefix_xmlns || prefix == xml symbols . empty_string && localpart == xml symbols . prefix_xmlns ) ; scan attribute value ( tmp str @$ f temp string2 @$ f attributeq name . rawname @$ attributes @$ attr index @$ isvc @$ f current element . rawname @$ isns decl ) ; string value = null ;,value use,success,pre
init logic <PLACE_HOLDER> parameters,local < member > method = code . get parameter ( __num__ @$ member type id ) ; local < xposed bridge . additional hook info > hook info = code . get parameter ( __num__ @$ hook info type id ) ;,logic get,success,pre
some drivers do not <PLACE_HOLDER> an incorrect length . then the name is '\0 ' terminated .,if ( location == shader program constants . location_invalid ) { length = __num__ ; while ( length < shader program . name_container_size && shader program . name_container [ length ] != __str__ ) { length ++ ; } name = new string ( shader program . name_container @$ __num__ @$ length ) ; location = gles20 . gl get attrib location ( this . m programid @$ name ) ; if ( location == shader program constants . location_invalid ) { throw new shader program link exception ( __str__ + name + __str__ ) ; } },drivers report,success,pre
write out the <PLACE_HOLDER> field and <PLACE_HOLDER> method options @$ if any .,if ( class specification . field specifications != null || class specification . method specifications != null ) { writer . print ( __str__ ) ; writer . println ( configuration constants . open_keyword ) ; write field specification ( class specification . field specifications ) ; write method specification ( class specification . method specifications ) ; writer . println ( configuration constants . close_keyword ) ; } else { writer . println ( ) ; },the keep,success,pre
see if clazz <PLACE_HOLDER> any of the configured base classes for this method,for ( string baseclass name : preserve overrides . get ( method name ) ) { class < ? > baseclass = load from internal ( baseclass name ) ; check state ( ! baseclass . is interface ( ) @$ __str__ @$ baseclass name ) ; if ( ! baseclass . is assignable from ( clazz ) ) { continue ; } for ( method m : clazz . get superclass ( ) . get methods ( ) ) { if ( method name . equals ( m . get name ( ) ) && descriptor . equals ( type . get method descriptor ( m ) ) && baseclass . equals ( m . get declaring class ( ) ) ) { return true ; },clazz has,fail,pre
trigger check to see if parent size has <PLACE_HOLDER> @$ recalculate layouts,resize timer = new timer ( ) { @ override public void run ( ) { perform size check ( ) ; resize timer . schedule ( monitor_parent_timer_interval ) ; } } ;,size reverted,fail,pre
only first or second operator <PLACE_HOLDER> dpp pruning,if ( dpps op1 . size ( ) != dpps op2 . size ( ) ) { return false ; },first needs,fail,pre
using random uuid ensures that multiple clusters can be launched by a same test @$ if it stops & <PLACE_HOLDER> them,path test dir = get data test dir ( __str__ + get randomuuid ( ) . to string ( ) ) ; cluster test dir = new file ( test dir . to string ( ) ) . get absolute file ( ) ;,& abort,fail,pre
now <PLACE_HOLDER> the 1 big write .,file . seek ( next write batch . offset ) ; if ( max stat > __num__ ) { if ( stat idx < max stat ) { stats [ stat idx ++ ] = sequence . get length ( ) ; } else { long all = __num__ ; for ( ; stat idx > __num__ ; ) { all += stats [ -- stat idx ] ; } log . trace ( __str__ @$ all / max stat ) ; } } file . write ( sequence . get data ( ) @$ sequence . get offset ( ) @$ sequence . get length ( ) ) ; replication target replication target = journal . get replication target ( ) ; if ( replication target !=,now do,success,pre
action client env <PLACE_HOLDER> the environment where values from action environment are overridden .,action client env . put all ( client env ) ; if ( command . builds ( ) ) { for ( map . entry < string @$ string > entry : options . get options ( core options . class ) . action environment ) { if ( entry . get value ( ) == null ) { visible action env . add ( entry . get key ( ) ) ; } else { visible action env . remove ( entry . get key ( ) ) ; action client env . put ( entry . get key ( ) @$ entry . get value ( ) ) ; } } for ( map . entry < string @$ string > entry : options . get,env contains,success,pre
currently project views can not have <PLACE_HOLDER> views .,if ( view . get configuration ( ) . get module ( ) == null ) { return views ; } final string query = __str__ ; try ( prepared statement statement = provider . get connection ( ) . get connection ( ) . prepare statement ( query ) ) { final list < i navi view > module views = view . get configuration ( ) . get module ( ) . get content ( ) . get view container ( ) . get views ( ) ; statement . set int ( __num__ @$ view . get configuration ( ) . get id ( ) ) ; final result set result set = statement . execute query ( ) ; if ( result set ==,views generated,fail,pre
if the mapping list <PLACE_HOLDER> the generic type name @$ begin to check this generic type name class .,if ( lite pal attr . get instance ( ) . get class names ( ) . contains ( generic type name ) ) { class < ? > reverse dynamic class = class . for name ( generic type name ) ; field [ ] reverse fields = reverse dynamic class . get declared fields ( ) ; boolean reverse associations = false ; for ( field reverse field : reverse fields ) { if ( ! modifier . is static ( reverse field . get modifiers ( ) ) ) { class < ? > reverse field type class = reverse field . get type ( ) ; if ( class name . equals ( reverse field type class . get name ( ) ) ),list contains,success,pre
now that the store is <PLACE_HOLDER> @$ try and get all the messages sent,return receive messages ( messages expected @$ session @$ one phase ) ;,store sane,success,pre
the two approaches should <PLACE_HOLDER> the same hash ring,assert false ( seq add tokens . is empty ( ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert equals ( batch add tokens @$ seq add tokens ) ;,approaches use,fail,pre
should not <PLACE_HOLDER> : average on string,try { tuple ds . aggregate ( aggregations . sum @$ __num__ ) ; assert . fail ( ) ; } catch ( unsupported aggregation type exception iae ) { },not work,success,pre
use the record stream processor list if it is configured . this list can <PLACE_HOLDER> both all record stream processor types,if ( ! this . record stream processors . is empty ( ) ) { for ( record stream processor stream processor : this . record stream processors ) { stream = stream processor . process stream ( stream @$ this . task state ) ; } } else { if ( this . converter instanceof multi converter ) { for ( converter cverter : ( ( multi converter ) this . converter ) . get converters ( ) ) { stream = cverter . process stream ( stream @$ this . task state ) ; } } else { stream = this . converter . process stream ( stream @$ this . task state ) ; } },list contain,success,pre
make the user agent tester <PLACE_HOLDER> its states and make sure we do n't get notifications as we 're now unsubscribed .,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = supported status set2 . get ( jabber status enum . free_for_chat ) ;,tester change,success,pre
the second time should <PLACE_HOLDER> the certificate,has certificate = info . get transport context ( ) instanceof x509 certificate [ ] ;,time add,fail,pre
new chunk <PLACE_HOLDER> a new dsg,statement dsg = new declaration statement group ( this @$ module ) ; dsg stack . push ( statement dsg ) ;,chunk creates,fail,pre
vm 1 <PLACE_HOLDER> and frees key 1,vm1 . invoke ( new serializable runnable ( ) { @ override public void run ( ) { logger . info ( __str__ ) ; connect distributed system ( ) ; d lock service dls = ( d lock service ) d lock service . create ( dls name @$ get system ( ) @$ true @$ true @$ false ) ; assert that ( dls . lock ( key1 @$ - __num__ @$ - __num__ ) ) . is true ( ) ; logger . info ( __str__ ) ; dls . unlock ( key1 ) ; assert that ( dls . get token ( key1 ) ) . is not null ( ) ; dls . free resources ( key1 ) ; d lock token token,vm locks,success,pre
fails for e.g . broken sockets silently <PLACE_HOLDER> exceptions and just record the failed path,fails . add ( path ) ;,sockets throw,fail,pre
noinspection unchecked ca n't <PLACE_HOLDER> target entity type .,if ( relation info . to one getter != null ) { to one to one = relation info . to one getter . get to one ( entity ) ; if ( to one != null ) { to one . get target ( ) ; } } else { if ( relation info . to many getter == null ) { throw new illegal state exception ( __str__ + relation info ) ; } list to many = relation info . to many getter . get to many ( entity ) ; if ( to many != null ) { to many . size ( ) ; } },unchecked set,fail,pre
let the <PLACE_HOLDER>or <PLACE_HOLDER> the class referenced in the local variable .,local variable info . referenced class accept ( class visitor ) ;,visitor visit,success,pre
if we found ourselves closed after the counter increment @$ <PLACE_HOLDER> the counter again and do not forward the request,if ( this . closed || this . request queue . is closed ( ) ) { this . requests not returned . decrement and get ( ) ; final notification listener listener ; synchronized ( listener lock ) { listener = all requests processed listener ; all requests processed listener = null ; } if ( listener != null ) { listener . on notification ( ) ; } throw new io exception ( __str__ + request ) ; },request check,fail,pre
we have finished processing this element . <PLACE_HOLDER> the depth .,int new depth = depth . decrement and get ( ) ;,element decrement,success,pre
in all chunks except the last this chunk also <PLACE_HOLDER> care of the detection in the seam @$ but for the last one there 's no seam at the end .,long end = last ? to exclusive - __num__ : to exclusive ; for ( long i = from inclusive ; i < end ; i ++ ) { detect ( same group detector @$ i ) ; if ( ++ local progress == __num__ ) { progress . add ( local progress ) ; local progress = __num__ ; } } progress . add ( local progress ) ;,chunk takes,success,pre
sensors do n't <PLACE_HOLDER> manifolds .,m_manifold . point count = __num__ ; evaluate ( m_manifold @$ xfa @$ xfb ) ; touching = m_manifold . point count > __num__ ;,sensors support,fail,pre
another wrapping panel so the borderlayout <PLACE_HOLDER> source list buttons panel does n't get forced to take up the full east cell of the containing panel .,j panel button wrapper panel = new j panel ( ) ; button wrapper panel . add ( source list buttons panel ) ; source list panel . add ( button wrapper panel @$ border layout . east ) ; source list model . add list data listener ( new list data listener ( ) { @ override public void interval removed ( list data event e ) { contents changed ( e ) ; } @ override public void interval added ( list data event e ) { contents changed ( e ) ; } @ override public void contents changed ( list data event e ) { boolean has selection = source list . get selected indices ( ) . length > __num__ ; remove source,panel url,fail,pre
wait till client <PLACE_HOLDER> the event,test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { int events = counting consumer . get num data events ( ) ; log . info ( __str__ + events + __str__ ) ; return events == __num__ ; } } @$ __str__ @$ __num__ * __num__ @$ log ) ;,client gets,success,pre
do the <PLACE_HOLDER> step,t res = reducer . reduce ( match @$ record ) ;,the reduce,success,pre
bring up wifi @$ then validate it . previous versions would immediately <PLACE_HOLDER> down cell @$ but it 's arguably correct to linger it @$ since it was the default network before it validated .,m wi fi network agent = new test network agent wrapper ( transport_wifi ) ; m wi fi network agent . connect ( true ) ; callback . expect available callbacks unvalidated ( m wi fi network agent ) ;,versions bring,fail,pre
target <PLACE_HOLDER> 2 :,assert equals ( input tier1 . size ( ) @$ __num__ ) ; assert equals ( input tier1 . get ( __num__ ) . get amount ( ) @$ new big decimal ( __str__ ) ) ;,target cost,fail,pre
most qjmha cluster tests do n't <PLACE_HOLDER> data nodes @$ so we 'll make this the default,this . dfs builder = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) ;,tests have,fail,pre
host @$ port @$ ssl @$ ssl socket factory @$ ssl parameters @$ <PLACE_HOLDER> verifier,jedis utils . add set end point interceptor ( target @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ; return target . to bytecode ( ) ;,factory hostname,success,pre
if the arc <PLACE_HOLDER> a readable variable @$ then two cases :,if ( is readable ( arc ) ) { if ( pos == ( length - __num__ ) ) { result = new long [ depth + __num__ ] ; result [ depth + __num__ ] = __num__ ; result [ depth ] = arc ; checker . add ( depth @$ result @$ depth @$ __num__ ) ; try { checker . check current oid ( ) ; } catch ( snmp status exception e ) { throw new snmp status exception ( snmp status exception . no such object ) ; } finally { checker . remove ( depth @$ __num__ ) ; } handlers . add ( this @$ depth @$ varbind ) ; return result ; } } else if ( is nested arc,arc contains,fail,pre
parse <PLACE_HOLDER> info @$ if present,if ( status info . data . available ( ) > __num__ ) { this . failure info = status info . data . get unaligned bit string ( ) . to boolean array ( ) ; },parse failed,fail,pre
say hello to equinox who <PLACE_HOLDER> its own protocol . we use introspection like there is no tomorrow to get access to the file,if ( url . get protocol ( ) . equals ( __str__ ) ) { url connection con = url . open connection ( ) ; con . set use caches ( resource . get default use caches ( ) ) ; if ( bundle_entry_field == null ) { bundle_entry_field = con . get class ( ) . get declared field ( __str__ ) ; bundle_entry_field . set accessible ( true ) ; } object bundle entry = bundle_entry_field . get ( con ) ; if ( match ( bundle entry . get class ( ) . get name ( ) @$ file_bundle_entry_classes ) ) { if ( file_field == null ) { file_field = bundle entry . get class ( ) . get declared field ( __str__,who declares,fail,pre
we do n't have permission to read integer creds we should get an empty set even though the private creds <PLACE_HOLDER> an integer cred . no security exception either !,try { set priv cred set1 = s . get private credentials ( integer . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { system . out . println ( __str__ ) ; } system . out . println ( __str__ ) ; set priv cred set2 = s . get private credentials ( ) ; if ( priv cred set2 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set2 . size ( ) ) ; },creds contain,fail,pre
a volt db extension to <PLACE_HOLDER> variable scale decimals,scale = __num__ ;,extension support,fail,pre
if input <PLACE_HOLDER> letters,if ( text . matches ( __str__ ) ) { assert equals ( pos . get error index ( ) >= __num__ @$ true ) ; assert equals ( pos . get index ( ) @$ __num__ ) ; assert equals ( parsed @$ null ) ; } else { assert equals ( pos . get index ( ) @$ expected index @$ __str__ + lc text ) ; assert equals ( pos . get error index ( ) @$ expected error index @$ __str__ + lc text ) ; if ( expected != null ) { assert equals ( parsed . query ( temporal queries . zone id ( ) ) @$ expected ) ; assert equals ( parsed . query ( temporal queries . offset (,input contains,fail,pre
server logs should not <PLACE_HOLDER> local logs .,verify no more interactions ( local logs ) ;,logs retrieve,success,pre
when ac is on @$ <PLACE_HOLDER> ac based user auth check,try { if ( authorization enabled && access controller available && ! is system or super user ( ) ) { user user = visibility utils . get active user ( ) ; throw new access denied exception ( __str__ + ( user != null ? user . get short name ( ) : __str__ ) + __str__ ) ; } if ( authorization enabled ) { check calling user auth ( ) ; } for ( byte string authbs : auths ) { label auths . add ( authbs . to byte array ( ) ) ; } operation status [ ] op status = this . visibility label service . clear auths ( request user @$ label auths ) ; log result ( true @$ __str__,check do,success,pre
the above connection request should <PLACE_HOLDER> a 'connecting ' state transition to be replicated,while ( node statuses . is empty ( ) ) { thread . sleep ( __num__ ) ; } final node connection status connecting status = node statuses . get ( __num__ ) ; assert equals ( node connection state . connecting @$ connecting status . get state ( ) ) ; assert equals ( requested node id @$ connecting status . get node identifier ( ) ) ;,request cause,fail,pre
start <PLACE_HOLDER> operation .,executors . execute ( new runnable ( ) { @ override public void run ( ) { try { list < string > keys = new array list < string > ( test entries . key set ( ) ) ; while ( ! rebalancing token . get ( ) ) { int index = ( int ) ( math . random ( ) * keys . size ( ) ) ; try { list < versioned < byte [ ] > > values = server side routing storerw . get ( new byte array ( byte utils . get bytes ( keys . get ( index ) @$ __str__ ) ) @$ null ) ; assert equals ( __str__ @$ __num__ @$ values . size ( ),start get,success,pre
simultaneously null <PLACE_HOLDER> we are not connected to the db @$ <PLACE_HOLDER> undesirable state so throw exception,if ( conn == null && ps == null && rs == null ) { throw new sql exception ( res bundle . handle get object ( __str__ ) . to string ( ) ) ; },exception indicates,fail,pre
the user <PLACE_HOLDER> an empty destination to skip the copy,if ( s . trim ( ) . ends with ( __str__ ) || s . trim ( ) . length ( ) == __num__ ) { continue ; },user specified,success,pre
ensure root <PLACE_HOLDER> the correct inode id last inode id should be root inode id and inode map size should be 1,int inode count = __num__ ; long expected last inode id = i node id . root_inode_id ; assert equals ( fsn . dir . root dir . get id ( ) @$ i node id . root_inode_id ) ; assert equals ( expected last inode id @$ last id ) ; assert equals ( inode count @$ fsn . dir . get inode map size ( ) ) ;,root has,success,pre
try to load the key as a pem <PLACE_HOLDER> public key,if ( pem reader . is pem ( data ) ) { try { return new loaded key ( pem reader . load public key ( data ) ) ; } catch ( runtime exception | general security exception e ) { throw new signature exception ( __str__ @$ e ) ; } },pem malformed,fail,pre
chrome always <PLACE_HOLDER> insertion requests .,return true ;,chrome generates,fail,pre
update permission on an outdated acl @$ retry should <PLACE_HOLDER> things going,principal sid user1 = new principal sid ( __str__ ) ; mutable acl record child acl2 = acl service . upsert ace ( child acl outdated @$ user1 @$ acl permission . administration ) ; assert . assert equals ( parent oid @$ child acl2 . get acl record ( ) . get parent domain object info ( ) ) ; assert . assert equals ( acl permission . administration @$ child acl2 . get acl record ( ) . get permission ( user1 ) ) ;,retry keep,success,pre
note that 'rollback ' <PLACE_HOLDER> the writer .,if ( writer != null ) { writer . rollback ( ) ; },'rollback closes,success,pre
we do n't want the packager closing the stream . v 1 creates a tar output stream @$ which then gets closed @$ which in turn <PLACE_HOLDER> the underlying output stream @$ and we want to protect ourselves against that .,try ( final output stream buffered out = new buffered output stream ( raw out ) ) { final output stream out = new non closeable output stream ( buffered out ) ; for ( final flow file flow file : contents ) { bin . get session ( ) . read ( flow file @$ false @$ new input stream callback ( ) { @ override public void process ( final input stream raw in ) throws io exception { try ( final input stream in = new buffered input stream ( raw in ) ) { final map < string @$ string > attributes = new hash map < > ( flow file . get attributes ( ) ) ; attributes . put ( __str__ @$,which closes,success,pre
let 's <PLACE_HOLDER> the index of item geopoint x field ...,data . index of fielditempointx = data . input row meta . index of value ( meta . get geo point lat ( ) ) ; if ( data . index of fielditempointx < __num__ ) { log error ( base messages . get string ( pkg @$ __str__ @$ meta . get geo point lat ( ) ) ) ; throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ meta . get geo point lat ( ) ) ) ; },'s take,success,pre
the abort slow consumer strategy kicks in here and sends a message down to the client to close itself . the client does not close because it is in the middle of the transaction . meanwhile the failover transport <PLACE_HOLDER> the close command and removes the consumer from its state .,assert true ( __str__ @$ wait . wait for ( new wait . condition ( ) { @ override public boolean is satisified ( ) throws exception { return aborting slow consumer ; } } @$ __num__ * __num__ ) ) ;,transport executes,fail,pre
bk <PLACE_HOLDER> time safety margin,int thread sleep time = idle reader error threshold - __num__ - __num__ ;,bk wait,fail,pre
query the superclass @$ which <PLACE_HOLDER> argument validation .,final path p = make qualified ( path ) ; switch ( validate path capability args ( p @$ capability ) ) { case common path capabilities . fs_acls : case common path capabilities . fs_append : case common path capabilities . fs_concat : case common path capabilities . fs_permissions : case common path capabilities . fs_snapshots : case common path capabilities . fs_storagepolicy : case common path capabilities . fs_xattrs : return true ; case common path capabilities . fs_symlinks : return false ; default : return super . has path capability ( p @$ capability ) ; },which triggers,success,pre
add pending remote input view after starting work challenge @$ as starting work challenge will <PLACE_HOLDER> all previous pending review view,m pending work remote input view = clicked ;,challenge close,fail,pre
assert that root does not <PLACE_HOLDER> s 1,assert true ( ! root . contains ( frag one ) ) ;,root contain,success,pre
does not <PLACE_HOLDER> rec,if ( scope activity . get activities ( ) . contains ( current activity ) || scope activity . equals ( current activity ) ) { candiadate execution = execution ; } else if ( current activity != null && current activity . contains ( ( activity impl ) scope activity ) ) { break ; },not contain,fail,pre
we have backed up this app before . check whether the version of the backup <PLACE_HOLDER> the version of the current app ; if they do n't match @$ the app has been updated and we need to store its metadata again . in either case @$ take it out of m existing so that we do n't consider it deleted later .,if ( m existing . contains ( pack name ) ) { m existing . remove ( pack name ) ; if ( info . get long version code ( ) == m state versions . get ( pack name ) . version code ) { continue ; } },version matches,success,pre
basic split <PLACE_HOLDER> divider.get preferred size for the reason . i leave it in hopes of having this used at some point .,if ( c != basic split pane divider . this || split pane == null ) { return new dimension ( __num__ @$ __num__ ) ; } dimension button min size = null ; if ( split pane . is one touch expandable ( ) && left button != null ) { button min size = left button . get minimum size ( ) ; } insets insets = get insets ( ) ; int width = get divider size ( ) ; int height = width ; if ( orientation == j split pane . vertical_split ) { if ( button min size != null ) { int size = button min size . height ; if ( insets != null ) { size += insets .,split requires,fail,pre
otherwise @$ no display text @$ just <PLACE_HOLDER> the executable name,string program info = text [ __num__ ] ; return get display text for file path or name ( program info ) ;,text show,fail,pre
skip pass all the finally blocks because both the fall through and return will also <PLACE_HOLDER> all the finally blocks .,node prefinally follows = follow ; follow = skip finally nodes ( follow ) ; if ( prefinally follows != follow ) { if ( ! is pure ( exit expr ) ) { return n ; } },return find,fail,pre
the selector <PLACE_HOLDER> prematurely many times in a row . rebuild the selector to work around the problem .,if ( selector_auto_rebuild_threshold > __num__ && select cnt >= selector_auto_rebuild_threshold ) { logger . warn ( __str__ @$ select cnt @$ selector ) ; rebuild selector ( ) ; return true ; },selector occurs,fail,pre
test what happens if a user accidentally <PLACE_HOLDER> the same id in multiple layouts too .,parsed android data direct = android data builder . of ( source ) . add resource ( __str__ @$ android data builder . resource type . layout @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . layout @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . value @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . value @$ __str__ ) . create manifest ( __str__ @$ __str__ @$ __str__ ) . build parsed ( ) ;,user uses,success,pre
a volt db extension to <PLACE_HOLDER> a crash when a is not an instance of double,if ( ! ( a instanceof double ) ) { a = new double ( a . to string ( ) ) ; },extension avoid,success,pre
j rockit throws this exception instead of returning null as the javadocs say it should . <PLACE_HOLDER> bug 36348,this . set collection usage unsupported ( mp ) ;,36348 see,success,pre
an inactive context may not <PLACE_HOLDER> a multicaster so we use our multicaster to call all of the context 's listeners instead,if ( context instanceof abstract application context ) { for ( application listener < ? > listener : ( ( abstract application context ) context ) . get application listeners ( ) ) { this . initial multicaster . add application listener ( listener ) ; } } this . initial multicaster . set error handler ( new logging error handler ( ) ) ; this . initial multicaster . multicast event ( event ) ;,context have,success,pre
only one geometry @$ let 's not wrap it in another node unless the node <PLACE_HOLDER> children .,if ( primitives . length == __num__ && children == null ) { spatial = primitives [ __num__ ] ; } else { node node = new node ( ) ; for ( geometry primitive : primitives ) { node . attach child ( primitive ) ; } spatial = node ; },node has,success,pre
where the code <PLACE_HOLDER> the mask !,assert equals ( expected @$ format information . decode format information ( unmasked_test_format_info @$ masked_test_format_info ) ) ;,code handles,fail,pre
create all connectors before adding @$ so a broken connector does not <PLACE_HOLDER> the system half updated,materialized connector connector = new materialized connector ( catalog name @$ create connector ( catalog name @$ factory @$ properties ) ) ; connector handle resolver connector handle resolver = connector . get connector ( ) . get handle resolver ( ) . or else get ( factory . get connector factory ( ) :: get handle resolver ) ; check argument ( connector handle resolver != null @$ __str__ @$ factory ) ; materialized connector information schema connector = new materialized connector ( create information schema catalog name ( catalog name ) @$ new information schema connector ( catalog name . get catalog name ( ) @$ node manager @$ metadata manager @$ access control manager ) ) ; catalog name system id = create system tables,connector make,fail,pre
we can only set up the real icq test suites when the accounts.properties file <PLACE_HOLDER> the two test accounts,if ( icq test agent name != null ) { icq slick fixture . tester agent = new icq tester agent ( icq test agent name ) ; string icq test agent pwd = system . get property ( testing_impl_pwd_prop_name @$ null ) ; if ( icq slick fixture . tester agent . register ( icq test agent pwd ) ) { if ( ! icq slick fixture . online testing disabled ) { icq slick fixture . tester agent . set authorization required ( ) ; try { initialize tested contact list ( ) ; } catch ( exception ex ) { logger . error ( __str__ @$ ex ) ; } string offline msg body = __str__ + __str__ ; icq slick fixture . offline msg,file contains,fail,pre
wait a little bit to let the delete <PLACE_HOLDER> effect .,thread . sleep ( __num__ ) ;,delete take,success,pre
note the api requires a source <PLACE_HOLDER> locator because the html and xml formatters display a page of code annotated with coverage information . having the source <PLACE_HOLDER>s is not actually needed for generating the lcov report ...,visitor . visit bundle ( bundle coverage @$ new i source file locator ( ) { @ override public reader get source file ( string package name @$ string file name ) throws io exception { return null ; } @ override public int get tab width ( ) { return __num__ ; } } ) ;,source file,success,pre
class <PLACE_HOLDER> exception,try { set perms = new hash set ( ) ; perms . add ( new object ( ) ) ; view . set permissions ( perms ) ; throw new runtime exception ( __str__ ) ; } catch ( class cast exception x ) { },class cast,success,pre
more complicated . vh 2 <PLACE_HOLDER> a higher version @$ but <PLACE_HOLDER> some exceptions that vh 1 does not have .,region version holder vh1 = new region version holder ( member ) ; region version holder vh2 = new region version holder ( member ) ; bit set bs1 = new bit set ( ) ; bs1 . set ( __num__ @$ __num__ ) ; bs1 . set ( __num__ @$ __num__ ) ; record versions ( vh1 @$ bs1 ) ; bit set bs2 = new bit set ( ) ; bs2 . set ( __num__ @$ __num__ ) ; bs2 . set ( __num__ @$ __num__ ) ; record versions ( vh2 @$ bs2 ) ;,vh has,success,pre
before conf object is passed in @$ <PLACE_HOLDER> has already processed it and used <PLACE_HOLDER> specific configs to overwrite hadoop common ones . hence we just need to source hadoop.proxyuser configs here .,map < string @$ string > filter config = authentication filter initializer . get filter config map ( conf @$ config prefix ) ;,ones level,fail,pre
in order to allow programs to use a single component as the display for multiple tabs @$ we will not change the visible compnent if the currently selected tab <PLACE_HOLDER> a null component . this is a bit dicey @$ as we do n't explicitly state we support this in the spec @$ but since programs are now depending on this @$ we 're,if ( selected component != null ) { if ( selected component != visible component && visible component != null ) { if ( swing utilities . find focus owner ( visible component ) != null ) { should change focus = true ; } } set visible component ( selected component ) ; } final rectangle bounds = tab pane . get bounds ( ) ; final int num children = tab pane . get component count ( ) ; if ( num children > __num__ ) { switch ( tab placement ) { case left : total tab width = calculate tab area width ( tab placement @$ run count @$ max tab width ) ; cx = insets . left + total tab width +,tab has,success,pre
only class object type can <PLACE_HOLDER> property specific,if ( ! ( object type instanceof class object type ) ) { left declared mask = all set bit mask . get ( ) ; return ; },type use,success,pre
middle of an existing chain download . <PLACE_HOLDER> a couple of peers .,peer group . start ( ) ;,middle start,fail,pre
nb : if rule acts as a sub @$ do not <PLACE_HOLDER> type overlap,boolean sub head = head . get atom ( ) . is type ( ) ; if ( sub head ) { body . get atoms ( ) . stream ( ) . filter ( atomic :: is type ) . filter ( at -> at . get var name ( ) . equals ( head . get atom ( ) . get var name ( ) ) ) . for each ( all atoms :: remove ) ; } all atoms . add ( head . get atom ( ) ) ; return reasoner query factory . create ( all atoms ) ;,nb expect,fail,pre
if imageloading failed @$ other thread may have called image loader which will <PLACE_HOLDER> out image @$ hence we check for it .,synchronized ( this ) { if ( image != null ) { if ( ( new state & width_flag ) == width_flag || width == __num__ ) { width = new width ; } if ( ( new state & height_flag ) == height_flag || height == __num__ ) { height = new height ; } } else { create text = true ; if ( ( new state & width_flag ) == width_flag ) { width = new width ; } if ( ( new state & height_flag ) == height_flag ) { height = new height ; } } state = state | new state ; state = ( state | loading_flag ) ^ loading_flag ; },which set,fail,pre
singles <PLACE_HOLDER> special exceptions,object [ ] [ ] should fail = { { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ @$ parse exception . class } @$ { __str__ } @$ { __str__ } @$ { __str__ @$ parse exception . class } @$ { __str__ @$ parse exception . class } @$ { __str__ } @$,singles throw,fail,pre
desired schema does not <PLACE_HOLDER> virtual columns or partition columns .,type description result = type description . create struct ( ) ; for ( int i = __num__ ; i < schema evolution type descrs . size ( ) ; i ++ ) { result . add field ( schema evolution column names . get ( i ) @$ schema evolution type descrs . get ( i ) ) ; } return result ;,schema have,fail,pre
hmmm @$ did this member have a restart ? determine which member dir might be a <PLACE_HOLDER> for us,if ( ! baseline dir . exists ( ) ) { baseline dir = find baseline for this member ( member backup location dir . get parent ( ) @$ disk store ) ; },dir store,fail,pre
thread a 1 <PLACE_HOLDER> pool a with thread a 1,if ( threada1 . in event loop ( ) ) { await ( arrival barrier ) ; return poola1 ; } else if ( threada2 . in event loop ( ) ) { await ( arrival barrier ) ; await ( release barrier ) ; return poola2 ; },1 gets,success,pre
visiting the configuration page should n't <PLACE_HOLDER> authorities,html page pg = wc . go to ( __str__ ) ; j . submit ( pg . get form by name ( __str__ ) ) ; p = u . get property ( last granted authorities property . class ) ; assert authorities ( p @$ __str__ ) ; assert authorities ( u . impersonate ( ) @$ __str__ ) ;,page change,success,pre
string table <PLACE_HOLDER> str index table,int st off = sit off + numb strings * __num__ ;,table offset,fail,pre
the test succeeds if we get the no class def <PLACE_HOLDER> error .,try { in . read object ( ) ; } catch ( no class def found error e ) { if ( e == ncdfe ) { system . err . println ( __str__ + e . to string ( ) ) ; } else { throw e ; } },def found,success,pre
semijoins may have <PLACE_HOLDER> task level cycles @$ examine those,connect terminal ops ( proc ctx . parse context ) ; boolean cycle free = false ; while ( ! cycle free ) { cycle free = true ; set < set < operator < ? > > > components = get components ( proc ctx ) ; for ( set < operator < ? > > component : components ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ ) ; for ( operator < ? > co : component ) { log . debug ( __str__ + co . get name ( ) + __str__ + co . get identifier ( ) ) ; } } if ( component . size ( ) != __num__ ) { log,semijoins reduce,fail,pre
should n't happen if this is only being called by a single thread . plumber.add should be <PLACE_HOLDER> out indexes before they fill up .,throw new ise ( e @$ __str__ ) ;,plumber.add setting,fail,pre
assert that the total sockets quota <PLACE_HOLDER> a reasonable limit .,assert true ( __str__ @$ ! open udp encap sockets . is empty ( ) ) ; assert true ( __str__ @$ open udp encap sockets . size ( ) < max_num_encap_sockets ) ;,quota has,success,pre
also non colored cumulatives <PLACE_HOLDER> names !,if ( m has colored cumulative ) { used color = new color wrap ( themes . get color from attr ( m chart view . get context ( ) @$ m colors [ i - __num__ ] ) ) ; } else { if ( m chart type == stats . chart type . intervals ) { name = m chart view . get resources ( ) . get string ( r . string . stats_cumulative_percentage ) ; } },cumulatives have,success,pre
if the dto is null @$ it is an indication that the user does not <PLACE_HOLDER> permissions . however @$ we do n't want to just throw an access denied exception because we would rather ensure that all of the appropriate actions are taken by the pluggable authorizer . as a result @$ we attempt to find the component as a processor and fall,if ( dto == null ) { authorizable authorizable ; try { authorizable = lookup . get processor ( entity . get id ( ) ) . get authorizable ( ) ; } catch ( final resource not found exception rnfe ) { authorizable = lookup . get controller service ( entity . get id ( ) ) . get authorizable ( ) ; } if ( require read ) { authorizable . authorize ( authorizer @$ request action . read @$ user ) ; } if ( require write ) { authorizable . authorize ( authorizer @$ request action . write @$ user ) ; } } else if ( affected componentdto . component_type_processor . equals ( dto . get reference type ( ) ) ) {,user have,success,pre
if there are remote deps @$ block on them fragment responses indicating failure will throw an exception which will propagate out of handle <PLACE_HOLDER> frag response and cause procedure runner to do the right thing and cause rollback .,while ( ! check done receiving frag responses ( ) ) { fragment response message msg = poll for responses ( ) ; if ( trace log != null ) { final int batch idx = m_remote work . get current batch index ( ) ; trace log . add ( ( ) -> volt trace . end async ( __str__ @$ misc utils . hs id pair txn id to string ( m_mbox . geths id ( ) @$ msg . m_sourcehs id @$ txn id @$ batch idx ) @$ __str__ @$ byte . to string ( msg . get status code ( ) ) ) ) ; } boolean expected msg = handle received frag response ( msg ) ; if ( expected msg ),block receive,fail,pre
for distinct count @$ because multiple groups <PLACE_HOLDER> same value @$ so there is no guarantee on the order of groups @$ just check the value,group by result distinct count group by result = distinct count group by result list . get ( i ) ; assert . assert equals ( distinct count group by result . get value ( ) @$ expected group index / ( num_groups / max_size_of_set ) + __num__ @$ error_message ) ;,groups have,success,pre
dom <PLACE_HOLDER> 3 : namespace uri is never empty string .,this . namespaceuri = namespaceuri ; if ( namespaceuri != null ) { this . namespaceuri = ( namespaceuri . length ( ) == __num__ ) ? null : namespaceuri ; } int colon1 = qname . index of ( __str__ ) ; int colon2 = qname . last index of ( __str__ ) ; owner document . check namespacewf ( qname @$ colon1 @$ colon2 ) ; if ( colon1 < __num__ ) { local name = qname ; if ( owner document . error checking ) { owner document . checkq name ( null @$ local name ) ; if ( qname . equals ( __str__ ) && ( namespaceuri == null || ! namespaceuri . equals ( namespace context . xmlns_uri ) ) || (,dom level,success,pre
disable quota <PLACE_HOLDER> while in standby .,dir . disable quota checks ( ) ; edit log tailer = new edit log tailer ( this @$ conf ) ; edit log tailer . start ( ) ; if ( ! is observer && standby should checkpoint ) { standby checkpointer = new standby checkpointer ( conf @$ this ) ; standby checkpointer . start ( ) ; },quota check,fail,pre
inspect <PLACE_HOLDER> intents to see if we need to migrate extras . we do n't promote clip data to the parent @$ since chooser activity will already start the picked item as the caller @$ and we ca n't combine the flags in a safe way .,if ( action_chooser . equals ( action ) ) { boolean migrated = false ; try { final intent intent = get parcelable extra ( extra_intent ) ; if ( intent != null ) { migrated |= intent . migrate extra stream to clip data ( ) ; } } catch ( class cast exception e ) { } try { final parcelable [ ] intents = get parcelable array extra ( extra_initial_intents ) ; if ( intents != null ) { for ( int i = __num__ ; i < intents . length ; i ++ ) { final intent intent = ( intent ) intents [ i ] ; if ( intent != null ) { migrated |= intent . migrate extra stream to clip data,inspect generated,fail,pre
the iterator already <PLACE_HOLDER> it 's load next batch future @$ we must complete it exceptionally,current page . complete exceptionally ( t ) ; return ;,iterator handled,fail,pre
the root object can <PLACE_HOLDER> the surrounding braces . this token should be the first field 's key @$ or part of it @$ so put it back .,put back ( t ) ; missing curly = true ; result = parse object ( false ) ;,object contain,fail,pre
internally @$ the code does n't permit nullable listeners @$ so we lazily initialize dummy instances if the developer did n't <PLACE_HOLDER> a real listener .,m on drag initiated listener = ( m on drag initiated listener != null ) ? m on drag initiated listener : new on drag initiated listener ( ) { @ override public boolean on drag initiated ( @ non null motion event e ) { return false ; } } ; m on item activated listener = ( m on item activated listener != null ) ? m on item activated listener : new on item activated listener < k > ( ) { @ override public boolean on item activated ( @ non null item details lookup . item details < k > item @$ @ non null motion event e ) { return false ; } } ; m on context click listener = (,developer specify,fail,pre
stream buffer <PLACE_HOLDER> field in model .,igfs cfg props . add ( __str__ ) ;,buffer attributes,fail,pre
remove the stack trace so we do not pollute the <PLACE_HOLDER> log .,return exceptions . clear trace ( new file service exception ( ) ) ;,the find,fail,pre
if the list accepts the key events and the key event was a click @$ the text view <PLACE_HOLDER> the selected item from the drop down as its content,switch ( key code ) { case key event . keycode_enter : case key event . keycode_dpad_center : case key event . keycode_tab : if ( event . has no modifiers ( ) ) { perform completion ( ) ; } return true ; },view gets,success,pre
now scheduler health <PLACE_HOLDER> last container allocated @$ aggregated allocation account will not be changed,assert . assert equals ( __num__ @$ sh . get allocation count ( ) . long value ( ) ) ; assert . assert equals ( resource . new instance ( __num__ * __num__ @$ __num__ ) @$ sh . get resources allocated ( ) ) ; assert . assert equals ( __num__ @$ sh . get aggregate allocation count ( ) . long value ( ) ) ; assert . assert equals ( __str__ @$ sh . get last allocation details ( ) . get node id ( ) . to string ( ) ) ; assert . assert equals ( __str__ @$ sh . get last allocation details ( ) . get queue ( ) ) ; task task_0_2 = new task ( application_0 @$ priority_0,health has,fail,pre
exception could occur if a symbol element is <PLACE_HOLDER> an important attribute such as address or length,string message = e . get message ( ) ; if ( message == null ) { message = e . get class ( ) . get simple name ( ) ; } message = __str__ + message ; msg . error ( this @$ message @$ e ) ; throw new io exception ( message @$ e ) ;,element missing,success,pre
a list of all available <PLACE_HOLDER>back <PLACE_HOLDER> levels .,final list < string > log levels = arrays . as list ( level . all . to string ( ) @$ level . off . to string ( ) @$ level . error . to string ( ) @$ level . warn . to string ( ) @$ level . info . to string ( ) @$ level . debug . to string ( ) @$ level . trace . to string ( ) ) ; final argument parser parser = argument parsers . new argument parser ( __str__ ) . default help ( true ) . description ( __str__ + __str__ ) ; parser . add argument ( __str__ @$ __str__ ) . type ( string . class ) . nargs ( __str__ ) . required,list log,success,pre
make sure the env <PLACE_HOLDER> bulk inserts with generated ids ...,if ( ! supports bulk insert id generation ( integer versioned . class ) ) { skip log . report skip ( __str__ @$ __str__ ) ; return ; } session s = open session ( ) ; transaction t = s . begin transaction ( ) ; integer versioned entity = new integer versioned ( __str__ ) ; s . save ( entity ) ; s . create query ( __str__ ) . list ( ) ; t . commit ( ) ; s . close ( ) ; long initial id = entity . get id ( ) ; int initial version = entity . get version ( ) ; s = open session ( ) ; t = s . begin transaction ( ) ;,env supports,success,pre
no costs known . <PLACE_HOLDER> the same assumption as above on the heuristic costs,final long sampled = ( long ) ( heuristic_cost_base * __num__ ) ; costs . add heuristic network cost ( heuristic_cost_base + sampled ) ; costs . add heuristic disk cost ( __num__ * sampled ) ;,costs make,fail,pre
single interface <PLACE_HOLDER> property .,string single interface enabled prop = util activator . get resources ( ) . get settings string ( single_window_interface_enabled ) ; boolean is enabled = false ; if ( single interface enabled prop != null ) is enabled = boolean . parse boolean ( single interface enabled prop ) ; else is enabled = boolean . parse boolean ( util activator . get resources ( ) . get settings string ( __str__ ) ) ;,interface enabled,success,pre
verify that a second call will only <PLACE_HOLDER> deltas .,m callback . clear ( ) ; final long [ ] [ ] new times1 = increase time ( times ) ; write to file ( m headline + uid lines ( m uids @$ new times1 ) ) ; m reader . read delta ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; ++ i ) { m callback . verify ( m uids [ i ] @$ get active time ( new times1 [ i ] ) - get active time ( times [ i ] ) ) ; } m callback . verify no more interactions ( ) ;,call return,success,pre
<PLACE_HOLDER> all the mappers with proper context <PLACE_HOLDER> first mapper,outputqueue = chain . create blocking queue ( ) ; chain . add mapper ( context @$ outputqueue @$ __num__ ) ;,mappers add,success,pre
safe to delegate since this reader does not <PLACE_HOLDER> the index,return in . get reader cache helper ( ) ;,reader use,fail,pre
the text lines total height is larger than this view @$ <PLACE_HOLDER> them to the top and bottom of the view .,if ( line1 height + line2 height > height ) { if ( child != m text line1 ) { vertical offset = height - line2 height ; } } else { vertical offset = ( height - line1 height - line2 height ) / __num__ ; if ( child == m text line2 ) { vertical offset += line1 height ; if ( m suggestion . has answer ( ) && m suggestion . get answer ( ) . get second line ( ) . has image ( ) ) { vertical offset += get resources ( ) . get dimension pixel offset ( r . dimen . omnibox_suggestion_answer_line2_vertical_spacing ) ; } } if ( line1 height != line2 height ) { vertical offset += ( line2,lines move,fail,pre
current dest <PLACE_HOLDER> no distinct keys .,if ( current distinct keys . is empty ( ) ) { list < expr node desc > combined list = combine expr node lists ( target spray keys @$ target distinct keys ) ; if ( ! match expr lists ( combined list @$ current spray keys ) ) { continue ; } } else { if ( target distinct keys . is empty ( ) ) { list < expr node desc > combined list = combine expr node lists ( current spray keys @$ current distinct keys ) ; if ( ! match expr lists ( combined list @$ target spray keys ) ) { continue ; } else { new distinct key lists . remove ( i ) ; new spray key lists .,dest has,success,pre
uri may <PLACE_HOLDER> ip @$ so make sure we check it too by converting ours @$ if necessary,string to match host = to match . get host ( ) ;,uri contain,success,pre
we set 0 this time as the last updated : can <PLACE_HOLDER> eg . when we use an old dynamo table,pm . set last updated ( __num__ ) ; metadata store ms = mock ( metadata store . class ) ; when ( ms . get ( path @$ false ) ) . then return ( pm ) ; i ttl time provider time provider = mock ( i ttl time provider . class ) ; when ( time provider . get now ( ) ) . then return ( __num__ ) ; when ( time provider . get metadata ttl ( ) ) . then return ( __num__ ) ;,updated happen,success,pre
the classic translator does not <PLACE_HOLDER> this information,return null ;,translator support,fail,pre
no ties <PLACE_HOLDER> 1,arrays . fill ( hits @$ __num__ ) ; actual_label = __num__ ; pred_dist = new double [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ; update hits ( __num__ @$ actual_label @$ pred_dist @$ hits ) ; assert . assert true ( arrays . equals ( hits @$ new double [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) ) ;,ties combine,fail,pre
packing <PLACE_HOLDER> time,long test packed date time = packed local date time . pack ( test date ) ;,packing date,fail,pre
assert that the tab layout did not <PLACE_HOLDER> and add the new items,assert not equals ( __str__ @$ new item count @$ tab layout . get tab count ( ) ) ;,layout update,success,pre
list result may <PLACE_HOLDER> continuation references,if ( ( answer . status != ldap client . ldap_success ) || ( answer . referrals != null ) ) { process return code ( answer @$ name ) ; } return new ldap naming enumeration ( this @$ answer @$ name @$ cont ) ;,result contain,success,pre
motion events <PLACE_HOLDER> the raw float distance :,final float x = is horizontal ? m requested drag distance : __num__ ; final float y = is horizontal ? __num__ : m requested drag distance ; m recycler view . scroll by ( offsetx @$ offsety ) ; add fake motion event ( time @$ motion event . action_move @$ x @$ y ) ; return true ;,events carry,fail,pre
computing the canonical path of a win 32 file should <PLACE_HOLDER> the true case of filenames @$ rather than just using the input case,file y = new file ( user dir @$ f . get path ( ) ) ; string ans = y . get path ( ) ; check ( ans @$ __str__ ) ; check ( ans @$ __str__ ) ; check ( ans @$ __str__ ) ;,file take,fail,pre
if a topic was not <PLACE_HOLDER>ed in the previous run @$ use this.avg est avg millis as the estimated avg time to <PLACE_HOLDER> a record in this run @$ which is the geometric mean of all topics whose avg times to <PLACE_HOLDER> a record in the previous run are known .,this . avg est avg millis = geometric mean ( all est avg millis ) ;,times insert,fail,pre
find all the methods with the specific name and which <PLACE_HOLDER> just 1 parameter .,if ( methods == null || methods . is empty ( ) ) { methods = class reflection index util . find all methods ( reflection index @$ class index @$ name @$ __num__ ) ; },which have,fail,pre
sometimes the former set and the newer set are execute in same millisecond which <PLACE_HOLDER> the later set to fail with obsolete version exception . add 5 ms sleep .,thread . sleep ( __num__ ) ;,which causes,success,pre
initial method of data <PLACE_HOLDER> access,set property ( hsqldb_nio_data_file @$ true ) ;,method file,success,pre
table <PLACE_HOLDER> capabilities,tbl name = __str__ ; table_params = new string builder ( ) ; table_params . append ( capabilities_key ) . append ( __str__ ) . append ( __str__ ) ; t props . put ( __str__ @$ tbl name ) ; t props . put ( __str__ @$ table_params . to string ( ) ) ; try { table = create table with capabilities ( t props ) ; log . info ( __str__ ) ; } catch ( exception e ) { log . info ( __str__ ) ; fail ( __str__ + e . get message ( ) + __str__ ) ; },table has,success,pre
instrumentation can <PLACE_HOLDER> and relaunch even persistent processes,force stop package locked ( ii . target package @$ - __num__ @$ true @$ false @$ true @$ true @$ false @$ user id @$ __str__ ) ;,instrumentation stop,fail,pre
decode <PLACE_HOLDER> pixel stream .,datum = bits = count = first = top = pi = bi = __num__ ; for ( i = __num__ ; i < npix ; ) { if ( top == __num__ ) { if ( bits < code_size ) { if ( count == __num__ ) { count = read block ( ) ; if ( count <= __num__ ) { break ; } bi = __num__ ; } datum += ( ( ( int ) block [ bi ] ) & __num__ ) << bits ; bits += __num__ ; bi ++ ; count -- ; continue ; } code = datum & code_mask ; datum >>= code_size ; bits -= code_size ; if ( ( code > available ) || ( code == end_of_information,decode cached,fail,pre
retrieve the replication configuration and verify that the configuration <PLACE_HOLDER> the rule we just set .,bucket replication configuration replication config = s3 client . get bucket replication configuration ( source bucket name ) ; replication rule rule = replication config . get rule ( __str__ ) ; system . out . println ( __str__ + rule . get destination config ( ) . get bucketarn ( ) ) ; system . out . println ( __str__ + rule . get priority ( ) ) ; system . out . println ( __str__ + rule . get status ( ) ) ;,configuration contains,fail,pre
we found counter objects which <PLACE_HOLDER> failure,if ( ! verify unexpected values ( counters ) ) { success = false ; },which trigger,fail,pre
we need to make a copy here since get previous work unit states <PLACE_HOLDER> immutable work unit states for which add all is not supported,if ( state . get prop as boolean ( configuration keys . overwrite_configs_in_statestore @$ configuration keys . default_overwrite_configs_in_statestore ) ) { work unit state work unit state copy = new work unit state ( work unit state . get workunit ( ) @$ state ) ; work unit state copy . add all ( work unit state ) ; work unit state copy . override with ( state ) ; previous work unit states . add ( work unit state copy ) ; } else { previous work unit states . add ( work unit state ) ; },states returns,success,pre
read message length will <PLACE_HOLDER> stream corrupted exception if the marker bytes incorrect,int msg size = tcp transport . read message length ( new bytes array ( minimal header ) ) ; if ( msg size == - __num__ ) { socket . get output stream ( ) . flush ( ) ; } else { final byte [ ] buffer = new byte [ msg size ] ; input . read fully ( buffer ) ; int expected size = tcp header . marker_bytes_size + tcp header . message_length_size + msg size ; try ( bytes stream output output = new releasable bytes stream output ( expected size @$ big arrays ) ) { output . write ( minimal header ) ; output . write ( buffer ) ; consume network reads ( mock channel @$ output . bytes,length throw,success,pre
although this is the same size as the bitmap that is likely already loaded @$ the lifecycle is different and interactions are on a different thread . thus to simplify @$ this source will <PLACE_HOLDER> its own bitmap .,bitmap preview = decode preview ( source @$ preview size ) ; if ( preview . get width ( ) <= gl_size_limit && preview . get height ( ) <= gl_size_limit ) { m preview = new bitmap texture ( preview ) ; } else { log . w ( tag @$ string . format ( __str__ + __str__ @$ m width @$ m height @$ preview . get width ( ) @$ preview . get height ( ) ) ) ; },source create,fail,pre
check that iterator always <PLACE_HOLDER> all the values .,try { int iter cnt = __num__ ; for ( int i = __num__ ; i < iter cnt ; i ++ ) { collection < integer > cp = new hash set < > ( original ) ; cp . remove all ( linked map . key set ( ) ) ; assert true ( __str__ + cp @$ cp . is empty ( ) ) ; } info ( __str__ + ( system . current time millis ( ) - start ) + __str__ ) ; } finally { run . set ( false ) ; fut . get ( ) ; },iterator produces,fail,pre
if the app is honeycomb mr 1 or earlier @$ <PLACE_HOLDER> its async task implementation to use the pool executor . normally @$ we use the serialized executor as the default . this has to happen in the main thread so the main looper is set right .,if ( data . app info . target sdk version <= android . os . build . version_codes . honeycomb_mr1 ) { async task . set default executor ( async task . thread_pool_executor ) ; },executor override,fail,pre
only <PLACE_HOLDER> 10 numbers .,buffered writer . write ( string . format ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; buffered writer . write ( string . format ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; buffered writer . close ( ) ;,only holds,fail,pre
mini hs 2 cluster is up .. let it run until someone <PLACE_HOLDER> the test,while ( true ) { thread . sleep ( __num__ ) ; },someone starts,fail,pre
flags @$ currently we never calculate the crc and if we dont calculate them cant <PLACE_HOLDER> orig values . tags are not experimental and we never create extended header to <PLACE_HOLDER> things simple .,extended = false ; experimental = false ; footer = false ;,them keep,success,pre
this means that the embedded finder could not <PLACE_HOLDER> a version .,if ( version == null ) { return null ; },finder find,fail,pre
this check <PLACE_HOLDER> exception when removal already happened during finishing animation,if ( j < additions . size ( ) ) { additions . remove ( j ) ; } if ( additions . is empty ( ) ) { m additions list . remove ( additions ) ; },check throws,fail,pre
test that it emits when time <PLACE_HOLDER> the time limit,consumer . accept ( value in global window ( new byte [ __num__ ] ) ) ;,time passed,success,pre
calendar should <PLACE_HOLDER> undefined size,assert true ( calendar . get width ( ) < __num__ ) ; assert true ( calendar . get height ( ) < __num__ ) ;,calendar have,success,pre
the main thread 's context class loader is set to the current thread 's context class loader which is a native image class loader . the class loader feature object replacer will <PLACE_HOLDER> the original app class loader from the native image class loader .,main thread = new thread ( main group @$ __str__ ) ; main thread . set daemon ( false ) ;,replacer unwrap,success,pre
package and private classes must <PLACE_HOLDER> package to be visible to each other,if ( ! objects . equals ( root invoker class . get package ( ) @$ root target class . get package ( ) ) ) { return false ; },package share,success,pre
verify <PLACE_HOLDER> quota on the stores as well .,for ( quota type quota type : new quota type [ ] { quota type . storage_space @$ quota type . get_throughput } ) { long new quota = __num__ + new random ( ) . next int ( __num__ ) ; admin client . quota mgmt ops . set quota ( new store name @$ quota type @$ new quota @$ service ) ; for ( integer node id : cluster . get node ids ( ) ) { long retrieved quota = get quota for node ( new store name @$ quota type @$ node id ) ; assert equals ( __str__ @$ new quota @$ retrieved quota . long value ( ) ) ; } },verify set,success,pre
the variable reference <PLACE_HOLDER> a declared module dependency .,if ( module graph . depends on ( var module @$ curr module ) ) { t . report ( n @$ violated_module_dep_error @$ curr module . get name ( ) @$ var module . get name ( ) @$ var name ) ; } else { t . report ( n @$ missing_module_dep_error @$ curr module . get name ( ) @$ var module . get name ( ) @$ var name ) ; },reference has,fail,pre
if the version is 0 then we are upgrading from a file format that did not know about periodic syncs . in that case do n't <PLACE_HOLDER> the list since we want the default @$ which is a daily periodic sync . otherwise <PLACE_HOLDER> out this default list since we will populate it later with the periodic sync descriptions that are read from the,if ( version > __num__ ) { authority . periodic syncs . clear ( ) ; } event log . write event ( __num__ @$ __str__ @$ - __num__ @$ __str__ + info . account + __str__ + authority name + __str__ + user id ) ;,case clear,success,pre
release <PLACE_HOLDER> 2 's am container on node 2 .,scheduler . handle ( app removed event2 ) ; assert equals ( __str__ @$ __num__ @$ queue1 . get am resource usage ( ) . get memory size ( ) ) ; scheduler . update ( ) ;,release app,success,pre
do not check the port number 0 because a user may <PLACE_HOLDER> his or her server to be bound on multiple arbitrary ports .,if ( p . local address ( ) . get port ( ) > __num__ ) { for ( int i = __num__ ; i < distinct ports . size ( ) ; i ++ ) { final server port port = distinct ports . get ( i ) ; if ( port . local address ( ) . equals ( p . local address ( ) ) ) { final server port merged = new server port ( port . local address ( ) @$ sets . union ( port . protocols ( ) @$ p . protocols ( ) ) ) ; distinct ports . set ( i @$ merged ) ; found = true ; break ; } } } if ( ! found ),user expect,fail,pre
need to breakdown commandline into parts @$ as spaces in command line will <PLACE_HOLDER> failures .,list < string > exec commands = split and unescape command line ( cmd line ) ; system . out . printf ( __str__ @$ cmd line ) ; system . out . printf ( __str__ @$ jetty home dir . get absolute path ( ) ) ; pb cmd = new process builder ( exec commands ) ; pid = pb cmd . start ( ) ; console parser parser = new console parser ( ) ; list < string [ ] > jmx list = parser . new pattern ( __str__ @$ __num__ ) ; list < string [ ] > conn list = parser . new pattern ( __str__ @$ __num__ ) ;,spaces cause,success,pre
expect errors . only <PLACE_HOLDER> one of two necessary configurability rules :,scratch . file ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ; write hello rules ( true ) ;,errors validate,fail,pre
for ssl the exception may not come since the server can <PLACE_HOLDER> socket before handshake message is sent from client . however exception should come in any region operations .,if ( gen . class code ( ) . equals ( credential generator . class code . ssl ) ) { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ no_exception ) ) ; client2 . invoke ( ( ) -> do puts ( __num__ @$ other_exception ) ) ; } else { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ authreq_exception ) ) ; },server close,success,pre
record the sequence of seeks and reads which <PLACE_HOLDER> a failure .,int [ ] seeks = new int [ __num__ ] ; int [ ] reads = new int [ __num__ ] ; try ( fs data input stream stm = get file system ( ) . open ( random seek file ) ) { for ( int i = __num__ ; i < limit ; i ++ ) { int seek off = r . next int ( buf . length ) ; int to read = r . next int ( math . min ( buf . length - seek off @$ __num__ ) ) ; seeks [ i % seeks . length ] = seek off ; reads [ i % reads . length ] = to read ; verify read ( stm @$ buf @$,which trigger,success,pre
source <PLACE_HOLDER> some different metadata .,if ( testing mode . equals ( mode_source_based ) ) { return ; },source requires,fail,pre
the activity that we are finishing may be over the lock screen . in this case @$ we do not want to consider activities that can not be shown on the lock screen as running and should proceed with finishing the activity if there is no valid next top running activity . note that if this finishing activity is <PLACE_HOLDER> task @$ we do,final activity display display = get display ( ) ; final activity record next = display . top running activity ( true ) ;,activity running,fail,pre
shared libraries <PLACE_HOLDER> a null parent : this has the side effect of having canonicalized shared libraries using application loaders cache @$ which is the behavior we want .,return application loaders . get default ( ) . get shared library class loader with shared libraries ( jars @$ m application info . target sdk version @$ is bundled app @$ library search path @$ library permitted path @$ null @$ null @$ shared libraries ) ;,libraries has,fail,pre
my sql connector currently does not <PLACE_HOLDER> comment on table,assert query fails ( __str__ @$ __str__ ) ;,connector support,success,pre
let 's actually <PLACE_HOLDER> cache on non affinity server .,srv3 . cache ( query utils . create table cache name ( query utils . dflt_schema @$ __str__ ) ) ;,let enforce,fail,pre
parent name only <PLACE_HOLDER> sense when there is a single obvious parent,if ( refed parent names . size ( ) == __num__ ) { logger . warn ( __str__ + __str__ @$ composed schema . get name ( ) ) ; return refed parent names . get ( __num__ ) ; } return null ;,name makes,success,pre
cleanup flavor c : the user does not <PLACE_HOLDER> any buffered data to persist between panes .,if ( should discard ) { reduce fn . clear state ( renamed context ) ; },user need,fail,pre
template location . this is the location which templates will be read from . the generator will <PLACE_HOLDER> the resource stream to attempt to read the templates .,embedded template dir = template dir = template directory ;,generator use,success,pre
frequency with which to <PLACE_HOLDER> data for later plotting,int plot everyn minibatches = __num__ ;,which store,fail,pre
