find the union of the icon and text rects,rectangle r = new rectangle ( ) ; r . set bounds ( text rect ) ; r = swing utilities . compute union ( icon rect . x @$ icon rect . y @$ icon rect . width @$ icon rect . height @$ r ) ;
will add q 1 as a query being executed,thread . sleep ( __num__ ) ; obj store . mark scheduled executions timed out ( __num__ ) ; try ( persistence manager pm = persistence manager provider . get persistence manager ( ) ) { m scheduled execution execution = pm . get object by id ( m scheduled execution . class @$ poll result . get execution id ( ) ) ; assert equals ( query state . timed_out @$ execution . get state ( ) ) ; }
first dvc should be the total row count indicator,final dim value count first dvc = iterator . next ( ) ; final long total rows = first dvc . num rows ; if ( ! __str__ . equals ( first dvc . dim ) || ! __str__ . equals ( first dvc . value ) ) { throw new illegal state exception ( __str__ ) ; }
create the initial file of old file len,final path p = new path ( __str__ + old file len + __str__ + flushed bytes1 + __str__ + flushed bytes2 ) ; log . info ( __str__ + p ) ; fs data output stream out = fs . create ( p @$ false @$ conf . get int ( common configuration keys . io_file_buffer_size_key @$ __num__ ) @$ replication @$ block_size ) ; out . write ( contents @$ __num__ @$ old file len ) ; out . close ( ) ;
keep the current item in the valid range,if ( m cur item == ii . position ) { new curr item = math . max ( __num__ @$ math . min ( m cur item @$ adapter count - __num__ ) ) ; need populate = true ; }
send first produce request with multiple messages .,future < record metadata > request1 = append to accumulator ( tp0 ) ; append to accumulator ( tp0 ) ; sender . run once ( ) ; string node id = client . requests ( ) . peek ( ) . destination ( ) ; node node = new node ( integer . value of ( node id ) @$ __str__ @$ __num__ ) ; assert equals ( __num__ @$ client . in flight request count ( ) ) ;
set up the drawer 's list view with items and click listener,m pane list . set adapter ( new array adapter < string > ( this @$ r . layout . left_menu_layout @$ m animal titles ) ) ; m pane list . set on item click listener ( new drawer item click listener ( ) ) ;
verify that finished processes are removed after refresh,proc42 . finish ( __num__ ) ; process tracker . explicit run one iteration ( ) ; process tracker . verify external process info ( __num__ @$ proc41b @$ create params ( __str__ ) ) ; process tracker . verify no process info ( __num__ ) ; assert equals ( __num__ @$ process tracker . processes info . size ( ) ) ;
prepare today header text color paint .,m today header text paint = new paint ( paint . anti_alias_flag ) ; m today header text paint . set text align ( paint . align . center ) ; m today header text paint . set text size ( m text size ) ; m today header text paint . set typeface ( typeface . default_bold ) ; m today header text paint . set color ( m today header text color ) ;
m settings should be created before build input method list locked,m settings = new input method settings ( m res @$ context . get content resolver ( ) @$ m method map @$ user id @$ ! m system ready ) ; update current profile ids ( ) ; additional subtype utils . load ( m additional subtype map @$ user id ) ; m switching controller = input method subtype switching controller . create instance locked ( m settings @$ context ) ;
object used to corrupt the cache .,date interval format dif = date interval format . get instance ( __str__ @$ u locale . english ) ; calendar from = calendar . get instance ( ) ; calendar to = calendar . get instance ( ) ; from . set ( __num__ @$ __num__ @$ __num__ ) ; to . set ( __num__ @$ __num__ @$ __num__ ) ;
add a note re supported date range,if ( date == null ) { if ( locale . equals ( japanese_imperial ) ) { throw new parse exception ( __str__ + locale + __str__ + __str__ + source + __str__ + parse pattern . pattern ( ) @$ __num__ ) ; } throw new parse exception ( __str__ + source + __str__ + parse pattern . pattern ( ) @$ __num__ ) ; }
not enough memory there @$ take less,if ( ! memory control . request ( buffersize + __num__ * __num__ * __num__ @$ true ) ) { long lessmem = math . min ( max write buffer / __num__ @$ memory control . available ( ) - ( __num__ * __num__ * __num__ ) / __num__ ) ; buffersize = math . max ( __num__ @$ ( int ) ( lessmem / recordsize ) ) * recordsize ; }
check if the comparison is supported for this type,if ( ! object inspector utils . compare supported ( valueoi ) ) { throw new udf argument exception ( __str__ + func_name + __str__ + __str__ + valueoi . get type name ( ) + __str__ + __str__ ) ; } result = new boolean writable ( false ) ; return primitive object inspector factory . writable boolean object inspector ;
slice is waiting ; handle it first,if ( first > __num__ ) { unread ( ) ; break ; } else { boolean comp = ( ch == __str__ ) ; boolean one letter = true ; ch = next ( ) ; if ( ch != __str__ ) unread ( ) ; else one letter = false ; return family ( one letter @$ comp ) ; }
return the last target class @$ if any .,while ( true ) { class optimization info info = class optimization info . get class optimization info ( clazz ) ; if ( info == null ) { return target class ; } clazz = info . get target class ( ) ; if ( clazz == null ) { return target class ; } target class = clazz ; }
use empty string as the key for absent namespace,if ( desc . get context type ( ) == xsd description . context_import || desc . from instance ( ) ) { string namespace = desc . get target namespace ( ) ; string ns = namespace == null ? xml symbols . empty_string : namespace ; location array templa = location pairs . get ( ns ) ; if ( templa != null ) loc = templa . get first location ( ) ; }
create working directory for this region server .,string dir = server working dir ( server type @$ rs port ) ; string conf str = generate config ( server type @$ rs port @$ dir ) ; log . debug ( __str__ + dir ) ; new file ( dir ) . mkdirs ( ) ; write string to file ( conf str @$ dir + __str__ ) ;
always read annotation written by marshal output stream .,object annotation = read location ( ) ; class loader default loader = skip default resolve class ? null : latest user defined loader ( ) ; string codebase = null ; if ( ! use codebase only && annotation instanceof string ) { codebase = ( string ) annotation ; } return rmi class loader . load proxy class ( codebase @$ interfaces @$ default loader ) ;
avoid trying to create accessibility node info for removed children,if ( vh != null && ! vh . is removed ( ) && ! m child helper . is hidden ( vh . item view ) ) { on initialize accessibility node info for item ( m recycler view . m recycler @$ m recycler view . m state @$ host @$ info ) ; }
clearing text to remove the predefined one in the template,title . clear text ( ) ; xslf text paragraph p = title . add new text paragraph ( ) ; xslf text run r1 = p . add new text run ( ) ; r1 . set text ( __str__ ) ; r1 . set font color ( new color ( __num__ @$ __num__ @$ __num__ ) ) ; r1 . set font size ( __num__ ) ;
user 1 checks the presence of user 2 in the room,presence = muc . get occupant presence ( room + __str__ ) ; assert null ( __str__ @$ presence ) ; presence = muc . get occupant presence ( room + __str__ ) ; assert not null ( __str__ @$ presence ) ; assert equals ( __str__ @$ room + __str__ @$ presence . get from ( ) ) ;
need to trim to avoid whitespace around the coordinates such as tabs,string [ ] coordinates = coordinates string . trim ( ) . split ( __str__ ) ; for ( string coordinate : coordinates ) { lat lng alts array . add ( convert to lat lng alt ( coordinate ) ) ; } return lat lng alts array ;
0 is passed as readpoint because the test bypasses store,this ( null @$ scan @$ scan info @$ columns != null ? columns . size ( ) : __num__ @$ __num__ @$ scan . get cache blocks ( ) @$ scan type . user_scan ) ; this . matcher = user scan query matcher . create ( scan @$ scan info @$ columns @$ oldest unexpiredts @$ now @$ null ) ; seek all scanner ( scan info @$ scanners ) ;
add one to one 's complement to generate two 's complement,for ( int i = result . length - __num__ ; i >= __num__ ; i -- ) { result [ i ] = ( int ) ( ( result [ i ] & long_mask ) + __num__ ) ; if ( result [ i ] != __num__ ) break ; } return result ;
does it matches the bom artifact name,return dependencies . stream ( ) . filter ( dependency -> __str__ . equals ( dependency . get scope ( ) ) ) . filter ( dependency -> __str__ . equals ( dependency . get type ( ) ) ) . any match ( dependency -> dependency . get artifact id ( ) . equals ( mojo utils . template_property_quarkus_platform_artifact_id_value ) && dependency . get group id ( ) . equals ( mojo utils . template_property_quarkus_platform_group_id_value ) ) ;
if not @$ add an extra trailing field !,if ( pos + len < maxsize ) { text file input field field = new text file input field ( __str__ + dummynr @$ pos + len @$ maxsize - pos - len ) ; field . set ignored ( true ) ; fields . add ( field ) ; }
search through the running processes for a matching cmdline and kill it,string command = string . format ( __str__ + __str__ @$ process name ) ; try { execute command with error checking ( string . format ( __str__ @$ package name @$ command ) ) ; event bus . post ( console event . warning ( __str__ + process name ) ) ; } catch ( adb helper . command failed exception e ) { event bus . post ( console event . warning ( __str__ + __str__ @$ process name ) ) ; }
test group by column not in display columns,try { client . call procedure ( __str__ @$ __str__ + tb + __str__ ) ; fail ( ) ; } catch ( proc call exception ex ) { assert true ( ex . get message ( ) . contains ( __str__ ) ) ; }
retrieve key @$ default value and error text,string [ ] array = contents . split ( __str__ @$ __num__ ) ; array [ __num__ ] = array [ __num__ ] . trim ( ) ; if ( __str__ . equals ( array [ __num__ ] ) ) { return null ; }
assert we 're no longer run as :,assert false ( subject . is run as ( ) ) ; assert equals ( __str__ @$ subject . get principal ( ) ) ; assert true ( subject . has role ( __str__ ) ) ; assert false ( subject . has role ( __str__ ) ) ; assert false ( subject . has role ( __str__ ) ) ; assert null ( subject . get previous principals ( ) ) ;
at least one number and designator must be seen after p,boolean designator = false ; int end date = index of ( str @$ start @$ len @$ __str__ ) ; if ( end date == - __num__ ) { end date = len ; } else if ( duration type == yearmonthduration_type ) { throw new schema date time exception ( ) ; }
verify that there are no warning or error in vm output,command line option test . verify samejvm startup ( null @$ new string [ ] { rtm generic command line option test . rtm_instr_error @$ unrecongnized option } @$ exit code . ok @$ command line option test . unlock_experimental_vm_options @$ __str__ ) ; command line option test . verify samejvm startup ( null @$ new string [ ] { rtm generic command line option test . rtm_instr_error @$ unrecongnized option } @$ exit code . ok @$ command line option test . unlock_experimental_vm_options @$ __str__ ) ;
mock interrupt exception when peek lock only once,mockito . do throw ( new zk peek lock interrupt exception ( __str__ ) ) . do call real method ( ) . when ( spy ) . peek lock ( mockito . any string ( ) ) ; try { spy . lock ( path ) ; fail ( __str__ ) ; } catch ( exception e ) { assert true ( e instanceof zk peek lock interrupt exception ) ; }
now determine the small table results .,int first small table output column ; first small table output column = ( order [ __num__ ] == pos big table ? big table retain size : __num__ ) ; int small table output count = __num__ ; next output column = first small table output column ;
copy all basic code points to the output,int b = __num__ ; for ( int i = __num__ ; i < input . length ( ) ; i ++ ) { final char c = input . char at ( i ) ; if ( is basic ( c ) ) { output . append ( c ) ; b ++ ; } }
throw the packet away and silently continue,if ( ( ! connected address . equals ( peek address ) ) || ( connected port != peek port ) ) { datagram packet tmp = new datagram packet ( new byte [ __num__ ] @$ __num__ ) ; get impl ( ) . receive ( tmp ) ; } else { stop = true ; }
verify that this container is properly tracked,assert not null ( get process tree info ( get container id ( __num__ ) ) ) ; assert equals ( __num__ @$ get process tree info ( get container id ( __num__ ) ) . get pmem limit ( ) ) ; assert equals ( __num__ @$ get process tree info ( get container id ( __num__ ) ) . get vmem limit ( ) ) ;
create the problem set and cfd from files,ps = null ; cfd = null ; try { ps = new problem set ( prob set path ) ; cfd = new cumulative feature driver ( cfd path ) ; } catch ( exception e ) { logger . logln ( __str__ @$ log out . stderr ) ; e . print stack trace ( ) ; }
then close the reader gracefully so that the long.max watermark is emitted,synchronized ( tester . get checkpoint lock ( ) ) { tester . close ( ) ; } for ( org . apache . hadoop . fs . path file : files created ) { hdfs . delete ( file @$ false ) ; }
add the package name of the class for better accuracy .,if ( field class . get class type ( ) . get descriptor ( ) . contains ( __str__ ) ) { resource names . add ( field class . get package name ( ) + __str__ + field name . get string ( ) ) ; }
no overlap @$ just move the indexes to their new positions,if ( new start < start ) { m_map_ [ start > > > shift_ ] = new start ; for ( i = data_block_length ; i > __num__ ; -- i ) { m_data_ [ new start ++ ] = m_data_ [ start ++ ] ; } } else { m_map_ [ start > > > shift_ ] = start ; new start += data_block_length ; start = new start ; }
the setting of servers is done by a call to discovery service,list < server > servers = new array list < server > ( ) ; for ( string svc : is alive map . key set ( ) ) { servers . add ( new server ( svc ) ) ; } lb . add servers ( servers ) ;
reorder in fetch session @$ and update log start offset,if ( updated percentage != __num__ && counter % ( __num__ / updated percentage ) == __num__ ) { fetches . remove ( topic partition ) ; fetches . put ( topic partition @$ new fetch request . partition data ( __num__ @$ __num__ @$ __num__ @$ optional . empty ( ) ) ) ; }
invoking via reflection to avoid a strong dependency on jdk 9,try { method get pid = process . class . get method ( __str__ ) ; return ( long ) get pid . invoke ( process ) ; } catch ( exception e ) { log . warn ( e @$ __str__ ) ; }
checkstyle : stop magic number name check,if ( get exponent ( ) == all_one_exponent ) { if ( get bit ( __num__ @$ get fraction ( ) ) ) { if ( get bit ( __num__ @$ get fraction ( ) ) ) { return true ; } } else { return true ; } }
roll time ahead to be sure timer is due to fire,calendar tomorrow = calendar . get instance ( ) ; tomorrow . add ( calendar . day_of_year @$ __num__ ) ; process engine configuration . get clock ( ) . set current time ( tomorrow . get time ( ) ) ;
always require an ack to prevent misordering of messages,internal distributed system system = this . advisee . get system ( ) ; processor = new update attributes reply processor ( system @$ recipients ) ; update attributes message message = get update attributes message ( processor @$ recipients ) ; mgr . put outgoing ( message ) ; this . processor = processor ;
can launch the 5 th attempt successfully,rm2 . wait for state ( app1 . get application id ( ) @$ rm app state . accepted ) ; mockam am5 = rm2 . wait for newam to launch and register ( app1 . get application id ( ) @$ __num__ @$ nm1 ) ; clock . reset ( ) ; rm2 . wait for state ( am5 . get application attempt id ( ) @$ rm app attempt state . running ) ;
simulate the case the same key is built again inside the extension,property key fake extension key = new property key . builder ( fake key name ) . build ( ) ; assert equals ( __str__ @$ m configuration . get ( fake extension key ) ) ; assert true ( property key . from string ( fake key name ) . is built in ( ) ) ;
make sure all cipher suites names are expected,set remaining cipher suites = new hash set < string > ( expected ) ; set unknown cipher suites = new hash set < string > ( ) ; for ( string cipher suite : cipher suites ) { boolean removed = remaining cipher suites . remove ( cipher suite ) ; if ( ! removed ) { unknown cipher suites . add ( cipher suite ) ; } } assert equals ( __str__ @$ collections . empty_set @$ unknown cipher suites ) ; return remaining cipher suites ;
create a module the binds string.class and char sequence.class,module module = new abstract module ( ) { @ override protected void configure ( ) { bind ( string . class ) . to instance ( __str__ ) ; bind ( char sequence . class ) . to instance ( __str__ ) ; } } ;
skip the edge through which we came,for ( job edge e : v . get inputs ( ) ) { if ( e == edge ) { continue ; } intermediate data set source = e . get source ( ) ; if ( remaining . contains ( source . get producer ( ) ) ) { has new predecessors = true ; break ; } }
the indices are converted to little endian order,byte [ ] indices = new byte [ elements ] ; byte [ ] vx = memory state . get big integer ( index varnode @$ false ) . to byte array ( ) ; for ( int j = __num__ ; j < vx . length && j < elements ; j ++ ) indices [ j ] = vx [ vx . length - j - __num__ ] ;
validate that change of content is reflected in the other linked files,append to file ( x1_one @$ str3 ) ; assert true ( fetch file contents ( x1_one ) . equals ( str1 + str3 ) ) ; assert true ( fetch file contents ( x11_one ) . equals ( str1 + str3 ) ) ; assert true ( fetch file contents ( x1 ) . equals ( str1 + str3 ) ) ;
found a control sequence @$ discard it and revert to normal state,if ( ( b >= __str__ && b <= __str__ ) || ( b >= __str__ && b <= __str__ ) ) { discard escape buffer ( ) ; state = state . normal ; } else if ( b == __num__ ) { dump escape buffer ( ) ; state = state . after_escape ; add byte to escape buffer ( b ) ; } else { dump escape buffer ( ) ; state = state . normal ; }
10 seconds next job is in 20 seconds : expire the event,assert equals ( __num__ @$ wm . get time to next job ( ) ) ; wm . insert ( new order event ( __str__ @$ __str__ @$ __num__ ) ) ; wm . fire all rules ( ) ; assert equals ( __num__ @$ results . size ( ) ) ; assert equals ( __num__ @$ ( ( number ) results . get ( __num__ ) ) . int value ( ) ) ;
if the number is out of range,s = new scanner ( __str__ ) ; assert false ( s . has next long ( ) ) ; try { s . next long ( ) ; fail ( ) ; } catch ( input mismatch exception expected ) { }
method info.value getter | | method info.value setter,string cast = cast ( method info . return type @$ method info . annotations ) ; if ( index == null && cast . length ( ) > __num__ ) { out . print ( __str__ + cast . substring ( __num__ @$ cast . length ( ) - __num__ ) + __str__ ) ; }
now use the same loader to load class my test,class my test_class = new my diff class loader ( my_test ) . load class ( my_test ) ; try { my test_class . new instance ( ) ; } catch ( exception e ) { throw new runtime exception ( __str__ ) ; }
done building model ; produce a score column with cluster choices,fr2 = kmm . score ( te ) ; assert . assert true ( kmm . test java scoring ( te @$ fr2 @$ __num__ ) ) ; if ( tr != null ) tr . delete ( ) ; if ( te != null ) te . delete ( ) ; if ( fr2 != null ) fr2 . delete ( ) ; if ( fr != null ) fr . delete ( ) ; if ( kmm != null ) kmm . delete ( ) ;
client enqueue close frame should result in a client write failure,final string orig close reason = __str__ ; client socket . get session ( ) . close ( status code . normal @$ orig close reason ) ; assert that ( __str__ @$ client socket . error latch . await ( __num__ @$ seconds ) @$ is ( true ) ) ; assert that ( __str__ @$ client socket . error . get ( ) @$ instance of ( eof exception . class ) ) ;
special case types that have 0 or 1 flag .,switch ( type ) { case type_settings : case type_ping : return flags == flag_ack ? __str__ : binary [ flags ] ; case type_priority : case type_rst_stream : case type_goaway : case type_window_update : return binary [ flags ] ; }
clobber existing entry @$ count remains unchanged,++ mod count ; enqueue notification ( key @$ hash @$ value reference @$ removal cause . replaced ) ; set value ( e @$ key @$ value @$ now ) ; evict entries ( e ) ; return entry value ;
these servers should not appear in the routing table,for ( server instance server instance : routing table . key set ( ) ) { if ( server instance . equals ( helix disabled instance ) || server instance . equals ( shutting down instance ) || server instance . equals ( queries disabled instance ) ) { return false ; } }
retain the scn val every now and then,if ( ctr % _static config . get flush itvl ( ) == __num__ ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + ctr ) ; } try { write scn to file ( ) ; } catch ( io exception e ) { log . error ( __str__ + _scn @$ e ) ; throw new databus exception ( __str__ + _scn @$ e ) ; } }
... except the current node if we have content for it,for ( text impl n = first text node in current run ( ) ; n != null ; ) { if ( n == this && content != null && content . length ( ) > __num__ ) { set data ( content ) ; result = this ; n = n . next text node ( ) ; } else { node to remove = n ; n = n . next text node ( ) ; parent . remove child ( to remove ) ; } }
forward again . still have n't reset yet .,m injected current time millis = start_time + interval - __num__ ; assert equals ( __num__ @$ m manager . get remaining call count ( ) ) ; assert equals ( start_time + interval @$ m manager . get rate limit reset time ( ) ) ;
take web.xml from the managed test .,war . set webxml ( fine immutable web failover test case . class . get package ( ) @$ __str__ ) ; war . add as web inf resource ( fine immutable web failover test case . class . get package ( ) @$ __str__ @$ __str__ ) ; return war ;
not using field schema.equals as comments can be different,if ( ! old col . get name ( ) . equals ( new col . get name ( ) ) || ! old col . get type ( ) . equals ( new col . get type ( ) ) ) { return false ; }
its a new entity and has n't been declared .,if ( index == - __num__ ) { if ( fdtd grammar != null ) fdtd grammar . internal entity decl ( name @$ text @$ non normalized text @$ augs ) ; if ( fdtd handler != null ) { fdtd handler . internal entity decl ( name @$ text @$ non normalized text @$ augs ) ; } }
only allocate the new system logger once,if ( result == null ) { logger new logger = new logger ( name @$ resource bundle name @$ null @$ get owner ( ) @$ true ) ; do { if ( add local logger ( new logger ) ) { result = new logger ; } else { result = find logger ( name ) ; } } while ( result == null ) ; }
we were told how big to be,if ( spec mode == measure spec . exactly ) { result = spec size ; } else if ( spec mode == measure spec . at_most ) { result = spec size ; } else { result = canvas size ; }
am 1 is missed from memory rm state store,mem store . remove application attempt internal ( am1 . get application attempt id ( ) ) ; application state data app0 state = mem store . get state ( ) . get application state ( ) . get ( app0 . get application id ( ) ) ; application attempt state data am2 state = app0 state . get attempt ( am2 . get application attempt id ( ) ) ;
reset ticket and wait half of the time left,ticket . reset ( ) ; time . set ( time . get ( ) + full timeout / __num__ ) ; rc = tickets . execute ( ) ; assert that ( rc @$ is ( __num__ ) ) ; assert that ( invoked . get ( ) @$ is ( __num__ ) ) ;
the sidebar fits inside the window @$ do not resize and move,if ( expand resize type == expand resize type . keep_window_size && new window width > get minimum size ( ) . width ) { new window width = get width ( ) ; new windowx = getx ( ) ; }
no ranked routing support and no route given or null session id,ri = new route iterator factory ( single @$ mod_cluster ) . iterator ( __str__ ) ; assert . assert false ( ri . has next ( ) ) ; ri = new route iterator factory ( single @$ mod_cluster ) . iterator ( null ) ; assert . assert false ( ri . has next ( ) ) ;
make sure the target is an instant app .,if ( ! is instant app ( package name @$ user id ) ) { return null ; } synchronized ( m packages ) { return m instant app registry . get instant app android idl pw ( package name @$ user id ) ; }
notify the power manager when ready .,if ( ready && must notify ) { synchronized ( m lock ) { if ( ! m pending request changed locked ) { m display ready locked = true ; if ( debug ) { slog . d ( tag @$ __str__ ) ; } } } send on state changed with wakelock ( ) ; }
next @$ a job failed event is dispatched,event = listener . get events received ( ) . get ( __num__ ) ; assert equals ( activiti event type . job_execution_failure @$ event . get type ( ) ) ; check event context ( event @$ the job ) ;
distribute total radio active time in to this app .,final long app packets = entry . rx packets + entry . tx packets ; final long app radio time = ( radio time * app packets ) / total packets ; u . note mobile radio active time locked ( app radio time ) ;
select a camera if no explicit camera requested,if ( ! explicit request ) { int index = __num__ ; while ( index < num cameras ) { camera . camera info camera info = new camera . camera info ( ) ; camera . get camera info ( index @$ camera info ) ; if ( camera info . facing == camera . camera info . camera_facing_back ) { break ; } index ++ ; } camera id = index ; }
get the number of minimally matching egl configurations,int [ ] num_config = new int [ __num__ ] ; egl . egl choose config ( display @$ s_config attribs2 @$ null @$ __num__ @$ num_config ) ; int num configs = num_config [ __num__ ] ; if ( num configs <= __num__ ) { throw new illegal argument exception ( __str__ ) ; }
state and commmand converters used by this converter .,this . add state converter ( new integer decimal type converter ( ) ) ; this . add state converter ( new integer percent type converter ( ) ) ; this . add command converter ( new big decimal command converter ( ) ) ; this . add state converter ( new big decimal decimal type converter ( ) ) ; this . add command converter ( new integer command converter ( ) ) ;
move mouse pointer to the center of the fist combo box,point p = cb1 . get location on screen ( ) ; dimension d = cb1 . get size ( ) ; system . out . println ( d . width + __str__ + d . height ) ; robot . mouse move ( p . x + d . width / __num__ @$ p . y + d . height / __num__ ) ;
user task should be active after the timer has triggered,assert that ( cmmn task service . create task query ( ) . case instance id ( case instance . get id ( ) ) . count ( ) ) . is equal to ( __num__ ) ; assert that ( process rule . get task service ( ) . create task query ( ) . process instance id ( process instance . get id ( ) ) . count ( ) ) . is equal to ( __num__ ) ;
adhoc task not part of deployment @$ cleanup,if ( task != null && task . get id ( ) != null ) { task service . delete task ( task . get id ( ) ) ; if ( process engine configuration . get history level ( ) . is at least ( history level . audit ) ) { history service . delete historic task instance ( task . get id ( ) ) ; } }
handle case when min values were larger than necessary,if ( dimension == null ) { int rows = calculate number of rows ( source code words @$ error correction code words @$ min cols ) ; if ( rows < min rows ) { dimension = new int [ ] { min cols @$ min rows } ; } } if ( dimension == null ) { throw new writer exception ( __str__ ) ; } return dimension ;
query on null value @$ should return one value,process instance query query = runtime service . create process instance query ( ) . variable value equals ( __str__ @$ null ) ; list < process instance > process instances = query . list ( ) ; assert not null ( process instances ) ; assert equals ( __num__ @$ process instances . size ( ) ) ; assert equals ( process instance1 . get id ( ) @$ process instances . get ( __num__ ) . get id ( ) ) ;
start puts in region on local site,vm4 . invoke ( ( ) -> wan test base . do puts with key as string ( get test method name ( ) @$ __num__ ) ) ; log writer utils . get log writer ( ) . info ( __str__ ) ;
nested representation is newer than top lever representation .,try { category1_1 = ( category ) s . merge ( category1_1 ) ; fail ( __str__ ) ; } catch ( persistence exception e ) { assert typing ( stale object state exception . class @$ e . get cause ( ) ) ; } finally { tx . rollback ( ) ; s . close ( ) ; }
validation on index usage with queries over a pr,vm0 . invoke ( pr queryd unit helper . get cache serializable runnable for index usage check ( ) ) ; vm1 . invoke ( pr queryd unit helper . get cache serializable runnable for index usage check ( ) ) ; vm2 . invoke ( pr queryd unit helper . get cache serializable runnable for index usage check ( ) ) ; vm3 . invoke ( pr queryd unit helper . get cache serializable runnable for index usage check ( ) ) ;
found a candidate . just use this .,if ( ! bounds conflict ( task bounds to check @$ m tmp bounds ) && display bounds . contains ( m tmp bounds ) ) { in out bounds . set ( m tmp bounds ) ; if ( debug ) append log ( __str__ + in out bounds ) ; return ; }
exception thrown and handled before any async processing,if ( in wait ) { response = execute request ( http version ) ; } else { try ( stackless logging log = new stackless logging ( http channel state . class ) ) { response = execute request ( http version ) ; } }
initialize the task by initializing all its processor nodes in the topology,log . trace ( __str__ ) ; for ( final processor node node : topology . processors ( ) ) { processor context . set current node ( node ) ; try { node . init ( processor context ) ; } finally { processor context . set current node ( null ) ; } }
add a shutdown hook to remove the temp file .,if ( t == null ) { access controller . do privileged ( ( privileged action < void > ) ( ) -> { thread group roottg = thread group utils . get root thread group ( ) ; t = new thread ( roottg @$ temp file deletion hook :: run hooks ) ; t . set context class loader ( null ) ; runtime . get runtime ( ) . add shutdown hook ( t ) ; return null ; } ) ; }
force properties to be clean as well,configuration manager . get config instance ( ) . clear ( ) ; hystrix command key key = hystrix . get current thread executing command ( ) ; if ( key != null ) { system . out . println ( __str__ + key + __str__ ) ; }
exact match for resources matched to the root path,routes . produce ( new route build item ( standalone . deployment root path @$ handler @$ false ) ) ; string match path = standalone . deployment root path ; if ( match path . ends with ( __str__ ) ) { match path += __str__ ; } else { match path += __str__ ; }
out of range for whole batch .,if ( use min max && ( key < min || key > max ) ) { join result = join util . join result . nomatch ; } else { join result = hash set . contains ( key @$ hash set results [ __num__ ] ) ; }
ensure second shift selection added instead of moved,assert equals ( __str__ @$ get logs ( ) . get ( __num__ ) ) ; selected = select . get all selected options ( ) ; assert equals ( __num__ @$ selected . size ( ) ) ; assert equals ( __str__ @$ selected . get ( __num__ ) . get text ( ) ) ; assert equals ( __str__ @$ selected . get ( __num__ ) . get text ( ) ) ;
allocate space for all configs and ask again .,egl config [ ] configs = new egl config [ num configs [ __num__ ] ] ; if ( ! egl . egl get configs ( display @$ configs @$ num configs [ __num__ ] @$ num configs ) ) { get err print writer ( ) . println ( __str__ ) ; return gl extensions ; }
load invoice @$ invoice lines should not be loaded,invoice invoice = session . get ( invoice . class @$ _invoice . get id ( ) ) ; assert . assert equals ( __str__ + __str__ @$ __num__ @$ stats . get entity count ( ) ) ; receipt receipt = new receipt ( receipt_a ) ; receipt . set invoice ( invoice ) ; session . persist ( receipt ) ; return invoice ;
only add the ship the folder @$ not the contents,assert . assert false ( effective ship files . contains ( lib file ) ) ; assert . assert true ( effective ship files . contains ( lib folder ) ) ; assert . assert false ( descriptor . get ship files ( ) . contains ( lib file ) ) ; assert . assert false ( descriptor . get ship files ( ) . contains ( lib folder ) ) ;
check that we 're not inside a deep learning task 2,assert ( ( ! dlp . _replicate_training_data || h2o . cloud . size ( ) == __num__ ) == ! _run_local ) ; if ( ! _run_local ) { _localmodel . add_processed_global ( _localmodel . get_processed_local ( ) ) ; _localmodel . set_processed_local ( __num__ ) ; if ( _chunk_node_count > __num__ ) _localmodel . div ( _chunk_node_count ) ; if ( _localmodel . get_params ( ) . _elastic_averaging ) _sharedmodel = deep learning model info . time average ( _localmodel ) ; } else { _sharedmodel = _localmodel ; }
release the partition if it was successfully removed,if ( partition == previous ) { partition . release ( ) ; result partitionid partition id = partition . get partition id ( ) ; log . debug ( __str__ @$ partition id . get partition id ( ) @$ partition id . get producer id ( ) ) ; }
artifact 2 is a tree artifact generated by action template .,special artifact artifact2 = create derived tree artifact only ( __str__ ) ; create fake tree file artifact ( artifact2 @$ __str__ @$ __str__ ) ; create fake tree file artifact ( artifact2 @$ __str__ @$ __str__ ) ; actions . add ( actions test util . create dummy spawn action template ( artifact1 @$ artifact2 ) ) ;
creating a duplicate index @$ should throw a index exists exception,vm1 . invoke ( pr queryd unit helper . get cache serializable runnable for duplicatepr index create ( partitioned_region_name @$ __str__ @$ __str__ @$ null @$ __str__ ) ) ; vm2 . invoke ( pr queryd unit helper . get cache serializable runnable for duplicatepr index create ( partitioned_region_name @$ __str__ @$ __str__ @$ null @$ __str__ ) ) ; vm3 . invoke ( pr queryd unit helper . get cache serializable runnable for duplicatepr index create ( partitioned_region_name @$ __str__ @$ __str__ @$ null @$ __str__ ) ) ;
make sure both are still traversed .,method = mock classc . class . get method ( __str__ ) ; ann = skylark interface utils . get skylark callable ( method ) ; assert that ( ann ) . is not null ( ) ; assert that ( ann . doc ( ) ) . is equal to ( __str__ ) ;
load nss softoken dependencies in advance to avoid resolver issues,safe reload ( libdir + system . map library name ( nspr_prefix + __str__ ) ) ; safe reload ( libdir + system . map library name ( nspr_prefix + __str__ ) ) ; safe reload ( libdir + system . map library name ( nspr_prefix + __str__ ) ) ; safe reload ( libdir + system . map library name ( __str__ ) ) ; safe reload ( libdir + system . map library name ( __str__ ) ) ; return true ;
parse ciphers in challenge ; mark each that server supports,for ( int i = __num__ ; i < token count ; i ++ ) { token = parser . next token ( ) ; for ( int j = __num__ ; j < cipher_tokens . length ; j ++ ) { if ( token . equals ( cipher_tokens [ j ] ) ) { server ciphers [ j ] |= cipher_masks [ j ] ; server cipher strs [ j ] = token ; logger . log ( level . fine @$ __str__ @$ token ) ; } } }
continue ramping up if the target rate has not been reached yet,if ( next target rate != target rate ) { _scheduler . schedule ( ( ) -> set rate and ramp up ( transaction id @$ target permits per period @$ period milliseconds @$ burst @$ ramp up permits per seconds ) @$ one_second_in_ms @$ time unit . milliseconds ) ; }
completion of txnid : id txn update 2,write id = txn mgr . get table write id ( __str__ @$ __str__ ) ; adp = new add dynamic partitions ( txn mgr . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . delete ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr . commit txn ( ) ;
we must never reject on because of thread pool capacity on replicas,transport service . register request handler ( transport replica action @$ in -> new concrete replica request < > ( in @$ replica request reader ) @$ executor @$ true @$ true @$ new replica operation transport handler ( ) ) ;
change the setting and assert a successful change .,try { set setting via front end api and assert successful change ( type @$ fake_setting_name @$ fake_setting_value @$ user id ) ; } finally { set string via front end api setting ( type @$ fake_setting_name @$ null @$ user id ) ; }
write the stack variable length @$ not the datatype length,attrs . add attribute ( __str__ @$ var . get length ( ) @$ true ) ; string comment = var . get comment ( ) ; if ( comment == null || comment . length ( ) == __num__ ) { writer . write element ( __str__ @$ attrs ) ; } else { writer . start element ( __str__ @$ attrs ) ; write regular comment ( writer @$ comment ) ; writer . end element ( __str__ ) ; }
verify that batching of the ib rs occurred .,metrics record builder rb = get metrics ( __str__ ) ; long batched = metrics asserts . get long counter ( __str__ @$ rb ) ; assert true ( batched > __num__ ) ; if ( cluster != null ) { cluster . shutdown ( ) ; }
scheduled should be chosen to run next,wait for query state ( query runner @$ scheduled5 @$ running ) ; cancel query ( query runner @$ backfill2 ) ; cancel query ( query runner @$ backfill3 ) ; cancel query ( query runner @$ backfill4 ) ; cancel query ( query runner @$ scheduled5 ) ; wait for query state ( query runner @$ scheduled5 @$ failed ) ;
create a name space with x attributes,path dir = new path ( __str__ ) ; hdfs . mkdirs ( dir ) ; hdfs . setx attr ( dir @$ __str__ @$ __str__ . get bytes ( ) ) ; hdfs . setx attr ( dir @$ __str__ @$ __str__ . get bytes ( ) ) ;
try to spread out the scheduling when starting up,synchronized ( m_poll queue ) { int n = m_poll queue . size ( ) ; long poll delay = n * d . get poll interval ( ) / ( a num dev > __num__ ? a num dev : __num__ ) ; add to poll queue ( d @$ system . current time millis ( ) + poll delay ) ; m_poll queue . notify ( ) ; }
also clear temporary lost component stored in window,if ( window != null ) { window real window = ( window ) window ; if ( real window . get temporary lost component ( ) == comp ) { real window . set temporary lost component ( null ) ; } }
keep the write for using it in visit,this . writer = writer ; store scan < io exception > scan = index store view . visit nodes ( array utils . empty_int_array @$ always_true_int @$ null @$ this @$ true ) ; scan . run ( ) ; return count ;
insert the new entry into the db .,try ( sq lite database db = helper . get writable database ( ) ) { db . insert ( db helper . table_name @$ db helper . timestamp_col @$ values ) ; } catch ( sql exception sqle ) { log . w ( tag @$ sqle ) ; }
send events to ensure state is updated,if ( event . get focused cell ( ) . get column ( ) == selection column . this ) { select all check box . set value ( ! select all check box . get value ( ) @$ true ) ; }
only retry on a subset of service exceptions,if ( exception instanceof amazon service exception ) { amazon service exception ase = ( amazon service exception ) exception ; if ( retry utils . is retryable service exception ( ase ) ) return true ; if ( retry utils . is throttling exception ( ase ) ) return true ; if ( retry utils . is clock skew error ( ase ) ) return true ; }
check if char is same as any other symbol,if ( ch == symbols . get decimal separator ( ) || ch == symbols . get grouping separator ( ) ) { buffer . append ( quote ) ; buffer . append ( ch ) ; buffer . append ( quote ) ; } else { buffer . append ( ch ) ; }
explicit check required in case table is empty .,final int expected mod count = mod count ; hashtable entry < ? @$ ? > [ ] tab = table ; for ( hashtable entry < ? @$ ? > entry : tab ) { while ( entry != null ) { action . accept ( ( k ) entry . key @$ ( v ) entry . value ) ; entry = entry . next ; if ( expected mod count != mod count ) { throw new concurrent modification exception ( ) ; } } }
wait for apps to be processed by mock preemption thread,for ( int i = __num__ ; i < __num__ ; ++ i ) { if ( preemption thread . unique apps added ( ) >= __num__ ) { break ; } thread . sleep ( __num__ ) ; } assert not null ( __str__ @$ scheduler . get context ( ) . get starved apps ( ) ) ; assert equals ( __str__ + __str__ @$ __num__ @$ preemption thread . unique apps added ( ) ) ;
when app is already installed @$ prefer same medium,if ( params . install location == package info . install_location_auto ) { if ( existing info != null ) { if ( ( existing info . flags & application info . flag_external_storage ) != __num__ ) { prefer = recommend_install_external ; } else { prefer = recommend_install_internal ; } } else { prefer = recommend_install_internal ; } check both = true ; } else { prefer = recommend_install_internal ; check both = false ; }
can handle 0 length streams gracefully .,url empty txt url = get class ( ) . get resource ( __str__ ) ; if ( empty txt url == null ) { throw new runtime exception ( __str__ ) ; } input stream empty stream = new buffered input stream ( empty txt url . open stream ( ) ) ; int length = archive helper . get bytes from input stream ( empty stream ) . length ; assert equals ( length @$ __num__ ) ; empty stream . close ( ) ;
the node is the root @$ release the kraken !,final hep planner tmp planner = new hep planner ( program ) ; tmp planner . set root ( node ) ; node = tmp planner . find best exp ( ) ; final hive rel field trimmer field trimmer = new hive rel field trimmer ( null @$ rel builder factory . create ( node . get cluster ( ) @$ null ) @$ fetch stats ) ; call . transform to ( field trimmer . trim ( node ) ) ; triggered = true ;
a standard transaction as wallets would create,transaction tx = fake tx builder . create fake tx ( unittest ) ; assert false ( tx . is opt in fullrbf ( ) ) ; tx . get inputs ( ) . get ( __num__ ) . set sequence number ( transaction input . no_sequence - __num__ ) ; assert true ( tx . is opt in fullrbf ( ) ) ;
setup the producer and send the message .,stub connection connection = create connection ( ) ; connection info connection info = create connection info ( ) ; connection info . set client id ( __str__ ) ; session info session info = create session info ( connection info ) ; producer info producer info = create producer info ( session info ) ; connection . send ( connection info ) ; connection . send ( session info ) ; connection . send ( producer info ) ;
type common to all declarations in the same statement,java type definition base type = node . get type node ( ) . get type definition ( ) ; if ( base type != null ) { node . set type definition ( base type . with dimensions ( node . get array depth ( ) ) ) ; }
there is a valid bucket pruning filter,if ( bs . cardinality ( ) < num buckets ) { top . get conf ( ) . set included buckets ( bs ) ; top . get conf ( ) . set num buckets ( num buckets ) ; }
check consistency : canonical classes must be from 0 to 255,int cc = integer . parse int ( line . substring ( start @$ end ) ) ; if ( cc != ( cc & __num__ ) ) system . err . println ( __str__ + line ) ; canonical class . put ( value @$ cc ) ; end = line . index of ( __str__ @$ start = end + __num__ ) ;
... and let them know of the event,jingle session request request = new jingle session request ( this @$ init jin ) ; for ( int i = __num__ ; i < jingle session request listeners . length ; i ++ ) { jingle session request listeners [ i ] . session requested ( request ) ; }
read savable array and convert to map values,if ( value instanceof intid map ) { intid map in = ( intid map ) value ; savable [ ] values = resolvei ds ( in . values ) ; value = int savable map fromkv ( in . keys @$ values ) ; field data . put ( field . alias @$ value ) ; }
need to override passed in class if class is lambda .,try { cls = class . for name ( cls name @$ true @$ cls ldr ) ; } catch ( class not found exception e ) { throw new ignite checked exception ( __str__ + cls name @$ e ) ; }
treat return types as object for the purposes of this method .,if ( super bt == type . bt_addr ) { supertype = type . object ; super bt = type . bt_object ; } if ( sub bt == type . bt_addr ) { subtype = type . object ; sub bt = type . bt_object ; } if ( ( super bt != type . bt_object ) || ( sub bt != type . bt_object ) ) { return supertype . is intlike ( ) && subtype . is intlike ( ) ; }
seek to the first kv on this row,if ( this . is seek to row ) { should scan keys values = ( scanner . seek to ( private cell util . create first on row ( this . row ) ) != - __num__ ) ; } else { should scan keys values = scanner . seek to ( ) ; }
find the number of the last cleanup,for ( cleanup remote file cleanup remote file : cleanup files . values ( ) ) { cleanup number = math . max ( cleanup number @$ cleanup remote file . get cleanup number ( ) ) ; } return cleanup number ;
no c because it is too large to fit,assert . assert equals ( result list . get ( __num__ ) . get string ( ) @$ __str__ ) ; assert . assert equals ( result list . get ( __num__ ) . get string ( ) @$ __str__ ) ;
verify if token has been updated in ugi .,tokens = new array list < > ( ugi . get tokens ( ) ) ; assert equals ( __num__ @$ tokens . size ( ) ) ; assert equals ( timeline delegation token identifier . kind_name @$ tokens . get ( __num__ ) . get kind ( ) ) ; assert equals ( collector token . decode identifier ( ) @$ tokens . get ( __num__ ) . decode identifier ( ) ) ;
gather the setter 's details but only the ones we need,if ( setter != null ) { if ( param type == null ) { param type = setter . get parameter type ( __num__ ) ; } for ( final annotation field annotation : setter . annotations ( ) ) { if ( ! param annotations . contains ( field annotation ) ) { param annotations . add ( field annotation ) ; } } }
do n't want this to happen ...,if ( on insert row == true ) { throw new sql exception ( res bundle . handle get object ( __str__ ) . to string ( ) ) ; } row current row = ( row ) get current row ( ) ; if ( current row . get updated ( ) == true ) { current row . clear updated ( ) ; notify row changed ( ) ; }
add the server to gray server list,if ( need put to gray servers ( get failed event . get failed type ( ) ) ) { add to gray list ( new ps location ( request . get context ( ) . get actual server id ( ) @$ request . get context ( ) . get location ( ) ) ) ; } get data split ( ) ; break ;
copy the bytes from this piece .,int current piece remaining = current piece size - current piece index ; int count = math . min ( current piece remaining @$ bytes remaining ) ; if ( b != null ) { current piece . copy to ( b @$ current piece index @$ offset @$ count ) ; offset += count ; } current piece index += count ; bytes remaining -= count ;
convert a deprecated language code used by java to a new code,if ( language . length ( ) > __num__ && is language ( language ) ) { if ( language . equals ( __str__ ) ) { language = __str__ ; } else if ( language . equals ( __str__ ) ) { language = __str__ ; } else if ( language . equals ( __str__ ) ) { language = __str__ ; } tag . _language = language ; }
now do the actual status notification testing,contact presence event collector contact pres evt collector = new contact presence event collector ( fixture . userid2 @$ new status ) ; operation set presence1 . add contact presence status listener ( contact pres evt collector ) ; synchronized ( contact pres evt collector ) { operation set presence2 . publish presence status ( new status @$ __str__ ) ; contact pres evt collector . wait for event ( __num__ ) ; operation set presence1 . remove contact presence status listener ( contact pres evt collector ) ; }
move this region to the master regionserver,region plan plan = new region plan ( region @$ server . get key ( ) @$ master server name ) ; if ( plans == null ) { plans = new array list < > ( ) ; } plans . add ( plan ) ;
eintr not listed as a possible error,if ( stdlib . realpath ( path @$ resolved ) . is null ( ) ) { throw throw unix exception ( errno ( ) ) ; } else { result = to byte array ( resolved ) ; } return result ;
without width attribute caption will not render on the web,caption attributes . set value ( attr_dimen_width @$ meta data . get width ( ) ) ; caption extensions kt . set image caption ( m content @$ m tapped media predicate @$ meta data . get caption ( ) @$ caption attributes ) ;
compare rows . cache row length .,int left row length = left . get row length ( ) ; int right row length = right . get row length ( ) ; int diff = byte buffer utils . compare to ( left . get row byte buffer ( ) @$ left . get row position ( ) @$ left row length @$ right . get row byte buffer ( ) @$ right . get row position ( ) @$ right row length ) ; if ( diff != __num__ ) { return diff ; }
spill all elements gathered up to now,num buffers spilled += full buffers . size ( ) ; while ( full buffers . size ( ) > __num__ ) { current writer . write block ( full buffers . remove first ( ) ) ; } current writer . write block ( current ) ; num buffers spilled ++ ;
1 should be replace by 0 .,logger . debug ( __str__ @$ window range ) ; assert . assert equals ( window range . get from ( ) @$ __num__ ) ; assert . assert equals ( window range . get to ( ) @$ time unit . minutes . to millis ( __num__ ) ) ;
verify get keys can relogin with proxy user,user group information another ugi = user group information . create proxy user ( __str__ @$ client ugi ) ; another ugi . do as ( new privileged exception action < void > ( ) { @ override public void run ( ) throws exception { thread . sleep ( __num__ ) ; key provider kp = create provider ( uri @$ conf ) ; kp . get keys ( ) ; return null ; } } ) ; return null ;
find setter method for the given property name .,string setter name = __str__ + capitalize ( name ) ; method setter = null ; for ( method method : type . get methods ( ) ) { if ( method . get name ( ) . equals ( setter name ) ) { setter = method ; break ; } } if ( setter == null ) { sourced . add error ( __str__ @$ type . get name ( ) @$ setter name ) ; return ; }
ask for versions that do not exist .,kvs = getn versions ( ht @$ cf @$ row idx @$ col idx @$ arrays . as list ( __num__ @$ __num__ ) ) ; assert equals ( __num__ @$ kvs == null ? __num__ : kvs . length ) ;
traverse virtual edges and normal edges but no shortcuts !,assert equals ( gh utility . as set ( from res . get closest node ( ) ) @$ gh utility . get neighbors ( explorer . set base node ( __num__ ) ) ) ; assert equals ( gh utility . as set ( to res . get closest node ( ) @$ __num__ ) @$ gh utility . get neighbors ( explorer . set base node ( __num__ ) ) ) ;
now add our own session id to the new record,secure random sr = new secure random ( ) ; byte [ ] sess id = new byte [ new sess id len ] ; sr . next bytes ( sess id ) ; new record . put ( ( byte ) new sess id len ) ; new record . put ( sess id ) ;
should match @$ port in range and host the same,assert true ( hostname port with range . matches ( uri . create ( __str__ + host1 + __str__ ) ) ) ; assert true ( hostname port with range . matches ( uri . create ( __str__ + host1 + __str__ ) ) ) ; assert true ( hostname port with range . matches ( uri . create ( __str__ + host1 + __str__ ) ) ) ;
shutdown if no request in 10 seconds .,try { ss . set so timeout ( __num__ ) ; for ( ; ; ) { socket s = ss . accept ( ) ; ( new worker ( s ) ) . start ( ) ; } } catch ( exception e ) { }
bottom left and top right corner points relative to origin,final float world originx = x + originx ; final float world originy = y + originy ; float fx = - originx ; float fy = - originy ; float fx2 = width - originx ; float fy2 = height - originy ;
verify that both the functions are available .,replica . run ( __str__ + replicated db name ) . run ( __str__ + replicated db name ) . verify result ( tuple . last replication id ) . run ( __str__ + replicated db name + __str__ ) . verify results ( new string [ ] { replicated db name + __str__ + func name1 @$ replicated db name + __str__ + func name2 } ) ;
wait for other side to close down .,if ( separate server thread ) { if ( server thread != null ) { server thread . join ( ) ; } } else { if ( client thread != null ) { client thread . join ( ) ; } }
workaround to set default for old schema,if ( namespace == namespace . web_1_1 || namespace == namespace . web_1_0 ) { if ( ! ssl . has defined ( webssl definition . key_alias . get name ( ) ) ) { ssl . get ( webssl definition . key_alias . get name ( ) ) . set ( __str__ ) ; } }
query on single short variable @$ should result in 2 matches,execution query query = runtime service . create execution query ( ) . variable value equals ( __str__ @$ short var ) ; list < execution > executions = query . list ( ) ; assert not null ( executions ) ; assert equals ( __num__ @$ executions . size ( ) ) ;
the coinbase in the genesis block is not spendable,list < transaction > genesis transactions = new linked list < > ( ) ; stored undoable block stored genesis = new stored undoable block ( params . get genesis block ( ) . get hash ( ) @$ genesis transactions ) ; put ( stored genesis header @$ stored genesis ) ; set chain head ( stored genesis header ) ; set verified chain head ( stored genesis header ) ; this . params = params ;
so : first find possible custom instances,if ( _list type == null ) { _list deserializer = _clear if std impl ( _find custom deser ( ctxt @$ tf . construct collection type ( list . class @$ ob type ) ) ) ; } else { _list deserializer = _find custom deser ( ctxt @$ _list type ) ; }
set the shell size @$ based upon previous time ...,set size ( ) ; get data ( ) ; input . set changed ( backup changed ) ; shell . open ( ) ; while ( ! shell . is disposed ( ) ) { if ( ! display . read and dispatch ( ) ) { display . sleep ( ) ; } } return stepname ;
read the apply numeric classifiers from disk,boolean disk apply numeric classifiers = ois . read boolean ( ) ; if ( props . get property ( __str__ ) != null ) { this . apply numeric classifiers = boolean . parse boolean ( props . get property ( __str__ ) ) ; } else { this . apply numeric classifiers = disk apply numeric classifiers ; } this . ner language = ner_language_default ;
make sure the class path is complete,file cfile = new file ( class path ) ; string absolute path = cfile . get absolute path ( ) ; if ( ! class path . equals ( absolute path ) ) { class path = absolute path ; } if ( ! class path . ends with ( file . separator ) ) { class path = class path + file . separator ; }
sticky bit omitted on sb omitted test dir should behave like reset,hdfs . set permission ( sb omitted test dir @$ new fs permission ( ( short ) __num__ ) ) ; log . info ( __str__ @$ sb omitted test dir . get name ( ) @$ hdfs . get file status ( sb omitted test dir ) . get permission ( ) ) ; assert false ( hdfs . get file status ( sb omitted test dir ) . get permission ( ) . get sticky bit ( ) ) ;
audio record start begin signal custom code block,try { if ( m audio recorder != null ) { m audio recorder . start recording ( ) ; } else { log . w ( tag @$ __str__ ) ; } } catch ( illegal state exception e ) { post listener . on error ( video capture error . encoder_error @$ __str__ @$ e ) ; return ; }
wait until the server has finished attempting to close the cq,try { assert that ( finish . await ( __num__ @$ seconds ) ) . described as ( __str__ ) . is true ( ) ; } catch ( interrupted exception e ) { e . print stack trace ( ) ; thread . current thread ( ) . interrupt ( ) ; }
only the succeed reconstructions are recorded .,final data node metrics metrics = get datanode ( ) . get metrics ( ) ; metrics . increc reconstruction read time ( read end - start ) ; metrics . increc reconstruction decoding time ( decode end - read end ) ; metrics . increc reconstruction write time ( write end - decode end ) ; update position in block ( to reconstruct len ) ; clear buffers ( ) ;
throws null pointer exception if key null,final int hash = this . entry creator . key hash code ( key @$ this . compare values ) ; return segment for ( hash ) . remove ( key @$ hash @$ no_object_token @$ null @$ null @$ null ) ;
in unbuffered mode only show message wrapper if there is an error,if ( ! is buffered ( ) ) { set message and buttons wrapper visible ( error message != null ) ; } if ( state == state . active || state == state . saving ) { for ( column < ? @$ t > c : grid . get columns ( ) ) { grid . get editor ( ) . set editor column error ( c @$ error columns . contains ( c ) ) ; } }
set new configuration with queue b removed .,cs conf = new capacity scheduler configuration ( conf ) ; setup queue configuration onlya ( cs conf ) ; string diags = __str__ + __str__ ; verify app recovery with wrong queue config ( cs conf @$ app2 @$ diags @$ mem store @$ state ) ;
synchronization is harmless for local thread engine @$ necessary for shared engine,synchronized ( engine ) { engine . put ( __str__ @$ curi ) ; engine . put ( __str__ @$ app ctx ) ; try { engine . eval ( __str__ ) ; } catch ( script exception e ) { logger . log ( level . warning @$ e . get message ( ) @$ e ) ; } finally { engine . put ( __str__ @$ null ) ; engine . put ( __str__ @$ null ) ; } }
leave msb bit off @$ so use 63 bits,assert that ( uleb128 . size ( ( __num__ << __num__ ) - __num__ ) @$ equal to ( __num__ ) ) ; assert that ( uleb128 . size ( long . max_value ) @$ equal to ( __num__ ) ) ; assert that ( uleb128 . size ( long . min_value ) @$ equal to ( __num__ ) ) ; assert that ( uleb128 . size ( - __num__ ) @$ equal to ( __num__ ) ) ;
if the button is pressed and the mouse is over it,if ( button model . is pressed ( ) && button model . is armed ( ) ) { internal background color = metal look and feel . get primary control shadow ( ) ; ul light highlight color = internal background color ; main item color = dark highlight color ; }
what 's the last occurance : decimal point !,int idx_dot = field . index of ( __str__ ) ; int idx_com = field . index of ( __str__ ) ; if ( idx_dot > idx_com ) { dfs . set decimal separator ( __str__ ) ; decimal = __str__ ; dfs . set grouping separator ( __str__ ) ; grouping = __str__ ; } else { dfs . set decimal separator ( __str__ ) ; decimal = __str__ ; dfs . set grouping separator ( __str__ ) ; grouping = __str__ ; }
create remote file in local staging dir to simulate,path staging dir = get staging dir ( ) ; test commons . get file utils ( ) . create file in dir ( staging dir @$ remote uri1 ) ; file remote dir1 = test commons . get file utils ( ) . create directory ( staging dir @$ remote uri2 ) ; test commons . get file utils ( ) . create file in dir ( remote dir1 @$ __str__ ) ;
cast the message as a text message .,try { javax . jms . text message text message = ( javax . jms . text message ) a message ; try { string string = text message . get text ( ) ; system . out . println ( string ) ; } catch ( javax . jms . jms exception jmse ) { jmse . print stack trace ( ) ; } } catch ( java . lang . runtime exception rte ) { rte . print stack trace ( ) ; }
display it in the text view .,int size = data == null ? __num__ : data . length ; string builder sb = new string builder ( ) ; sb . append ( good ? __str__ : __str__ ) ; sb . append ( __str__ ) . append ( size ) . append ( __str__ ) ; if ( good && reports != null ) { display log report ( sb @$ reports ) ; m report text . set text ( sb . to string ( ) ) ; }
setup all the redirect handles @$ including the return file type,err file = paths . get ( base . to string ( ) @$ uuid + __str__ ) ; out file = paths . get ( base . to string ( ) @$ uuid + __str__ ) ; result file = paths . get ( base . to string ( ) @$ uuid + __str__ ) ;
run once to get to a startup state,configure ( __num__ @$ __num__ @$ __num__ ) ; voltdb . ignore crash = true ; thread dutthread = new thread ( ) { @ override public void run ( ) { try { m_dut . accept promotion ( ) ; } catch ( exception e ) { } } } ; dutthread . start ( ) ;
remove line continuation characters from string literals,if ( js token . kind == ecmascript5 parser constants . string_literal || js token . kind == ecmascript5 parser constants . unterminated_string_literal ) { return token . get image ( ) . replace all ( __str__ @$ __str__ ) ; } return token . get image ( ) ;
so wait around here a while .,waiter . wait for ( am . get configuration ( ) @$ __num__ @$ ( ) -> am . get region states ( ) . get region state node ( region info ) != null ) ; region state node region node = am . get region states ( ) . get region state node ( region info ) ;
two lines using platform 's line separator,list < string > expected = arrays . as list ( __str__ @$ __str__ ) ; files . write ( tmpfile @$ expected @$ us_ascii ) ; assert true ( files . size ( tmpfile ) > __num__ @$ __str__ ) ; lines = files . read all lines ( tmpfile @$ us_ascii ) ; assert true ( lines . equals ( expected ) @$ __str__ ) ;
look for xerces 1 and xerces 2 parsers separately,try { final string xerces2_version_class = __str__ ; class clazz = object factory . find provider class ( xerces2_version_class @$ object factory . find class loader ( ) @$ true ) ; field f = clazz . get field ( __str__ ) ; string parser version = ( string ) f . get ( null ) ; h . put ( version + __str__ @$ parser version ) ; } catch ( exception e ) { h . put ( version + __str__ @$ class_notpresent ) ; }
can this number simply be added to the next interval ?,if ( number == ( entry after . get start ( ) - __num__ ) ) { if ( ! appended ) { entry after . set start ( number ) ; } else { entry after . set start ( entry before . get start ( ) ) ; list . remove ( prev entry pos ) ; if ( window start index > prev entry pos ) window start index -- ; } prepended = true ; }
create a separate subprocess just for this argument .,subprocess builder subprocess builder = new subprocess builder ( windows subprocess factory . instance ) ; subprocess builder . set working directory ( new file ( __str__ ) ) ; subprocess builder . set argv ( print arg exe @$ arg . original ) ; process = subprocess builder . start ( ) ; process . wait for ( ) ; assert that ( process . exit value ( ) ) . is equal to ( __num__ ) ;
anonymous classes are private @$ they should be eliminated elsewhere,return modifier . is public ( t . get modifiers ( ) ) && ( t . is interface ( ) || t . is enum ( ) || modifier . is static ( t . get modifiers ( ) ) ) ;
try to delete the data type @$ should error out,try { meta store . delete element type ( namespace @$ element type ) ; fail ( __str__ ) ; } catch ( meta store dependencies exists exception e ) { list < string > dependencies = e . get dependencies ( ) ; assert not null ( dependencies ) ; assert equals ( __num__ @$ dependencies . size ( ) ) ; assert equals ( customer dimension . get id ( ) @$ dependencies . get ( __num__ ) ) ; }
alas @$ we have to pull the recycler directly here ...,segmented string writer sw = new segmented string writer ( _generator factory . _get buffer recycler ( ) ) ; default serializer provider ctxt = _serializer provider ( ) ; try { _config and write value ( ctxt @$ _generator factory . create generator ( ctxt @$ sw ) @$ value ) ; } catch ( json processing exception e ) { throw e ; } catch ( io exception e ) { throw json mapping exception . from unexpectedioe ( e ) ; }
modern is simpler : do n't care about anything but a grave,if ( ! full ) { for ( int i = __num__ ; i < decomp . length ( ) ; ++ i ) { char c = decomp . char at ( i ) ; if ( c == __str__ || c == __str__ || c == __str__ || c == __str__ || c == __str__ || c == __str__ ) return false ; } return true ; }
right node is elliminated and all entries stored in left node,if ( new left key count <= max key count ) { move keys left ( left node @$ right node @$ right key count ) ; node mgr . delete node ( right node ) ; return delete child ( right key ) ; }
draw the other dimensions around the fact ...,if ( ! table list . is empty ( ) ) { list < table point > dim points = get circle points ( center @$ center . x - max width / __num__ - __num__ @$ center . y - __num__ @$ table list ) ; points . add all ( dim points ) ; }
wait until the last async operation is finished .,last future . get ( ) ; m cache update service = shortcut info compat saver impl . create executor service ( ) ; m disk io service = shortcut info compat saver impl . create executor service ( ) ; m shortcut info saver = new shortcut info compat saver impl ( m context @$ m cache update service @$ m disk io service ) ;
check if at @$ or could be @$ head,for ( ; ; ) { node h = head ; if ( h == pred || h == s || h == null ) return ; if ( ! h . is matched ( ) ) break ; node hn = h . next ; if ( hn == null ) return ; if ( hn != h && cas head ( h @$ hn ) ) h . forget next ( ) ; }
try to connect to a relay on localhost,config builder . get runtime ( ) . get relay ( __str__ ) . set host ( __str__ ) ; config builder . get runtime ( ) . get relay ( __str__ ) . set port ( __num__ ) ; config builder . get runtime ( ) . get relay ( __str__ ) . set sources ( person_source ) ;
try to remove the workspace owner from the members list,workspace share . wait invite member dialog closed ( ) ; workspace share . wait member name in share list ( system admin name ) ; workspace share . click on remove member button ( system admin name ) ; workspace details . click on delete button in dialog window ( ) ; dashboard . wait notification message ( __str__ ) ; dashboard . wait notification is closed ( ) ;
check that the roll happened correctly for the given data,long expected files = total events / roll count ; if ( total events % roll count > __num__ ) expected files ++ ; assert . assert equals ( __str__ + lists . new array list ( f list ) @$ expected files @$ f list . length ) ;
use local deploy mode and data format,conf . set ( angel conf . angel_deploy_mode @$ __str__ ) ; conf . set ( angelml conf . ml_data_input_format ( ) @$ string . value of ( data type ) ) ; conf . set ( angelml conf . ml_model_type ( ) @$ angelml conf . default_ml_model_type ( ) ) ;
replicate and verify if only 2 tables are replicated to target .,string [ ] replicated tables = new string [ ] { __str__ @$ __str__ } ; string [ ] bootstrap tables = new string [ ] { } ; last repl id = replicate and verify ( repl policy @$ null @$ last repl id @$ null @$ null @$ bootstrap tables @$ replicated tables ) ;
caution : this removes the old list of relation mentions !,for ( relation mention r : relations ) { if ( ! r . get type ( ) . equals ( relation mention . unrelated ) ) { logger . fine ( __str__ + r ) ; } } sentence . set ( machine reading annotations . relation mentions annotation . class @$ relations ) ;
creating colocated region data store node on the vm 0 .,vm0 . invoke ( pr queryd unit helper . get cache serializable runnable for local region creation ( colo name @$ new portfolio . class ) ) ; portfolio [ ] portfolio = create portfolios and positions ( cnt dest ) ; new portfolio [ ] new portfolio = create new portfolios and positions ( cnt dest ) ;
compute the number of unit that needs to be borrowed .,big decimal borrow = buf [ i ] . abs ( ) . divide ( factors [ i - __num__ ] @$ big decimal . round_up ) ; if ( buf [ i ] . signum ( ) > __num__ ) { borrow = borrow . negate ( ) ; }
count the trailing zeros in the magnitude,if ( signum < __num__ ) { int mag trailing zero count = __num__ @$ j ; for ( j = mag . length - __num__ ; mag [ j ] == __num__ ; j -- ) mag trailing zero count += __num__ ; mag trailing zero count += integer . number of trailing zeros ( mag [ j ] ) ; bc += mag trailing zero count - __num__ ; }
delete first char of element and returns number,int [ ] number = collection . stream ( ) . map to int ( ( s ) -> integer . parse int ( s . substring ( __num__ ) ) ) . to array ( ) ; system . out . println ( __str__ + arrays . to string ( number ) ) ;
read the standard length atom header .,if ( atom header bytes read == __num__ ) { if ( ! input . read fully ( atom header . data @$ __num__ @$ atom . header_size @$ true ) ) { return false ; } atom header bytes read = atom . header_size ; atom header . set position ( __num__ ) ; atom size = atom header . read unsigned int ( ) ; atom type = atom header . read int ( ) ; }
testing that the return by skip is valid,input stream check input = support_ resources . get stream ( __str__ ) ; checked input stream check in = new checked input stream ( check input @$ new crc32 ( ) ) ; check in . read ( ) ; check in . close ( ) ; try { check in . read ( ) ; fail ( __str__ ) ; } catch ( io exception ee ) { }
verify jar with second key in strict mode,analyzer = process tools . execute command ( jarsigner @$ __str__ @$ __str__ @$ __str__ @$ keystore @$ __str__ @$ password @$ __str__ @$ password @$ updated_signed_jarfile @$ second_key_alias ) ; int expected exit code = has_expired_cert_exit_code + bad_extended_key_usage_exit_code + has_unsigned_entry_exit_code + not_signed_by_alias_exit_code ; check verifying ( analyzer @$ expected exit code @$ bad_extended_key_usage_verifying_warning @$ has_expired_cert_verifying_warning @$ has_unsigned_entry_verifying_warning @$ not_signed_by_alias_verifying_warning ) ;
send any committed text to the text component,if ( committed character count > __num__ ) { input method context . dispatch committed text ( ( ( component ) event . get source ( ) ) @$ text @$ committed character count ) ; if ( is composition area visible ( ) ) { composition area . update window location ( ) ; } }
ensure that all classes are properly loaded,test root node root node = new test root node ( default @$ new test repeating node ( ) ) ; call target target = runtime . create call target ( root node ) ; target . call ( __num__ ) ; root node = new test root node ( configured @$ new test repeating node ( ) ) ; target = runtime . create call target ( root node ) ; target . call ( __num__ ) ;
deletion is async : wait util they are no longer reported,final set < string > remaining = new hash set < > ( topics ) ; while ( ! remaining . is empty ( ) ) { final set < string > topic names = admin client . list topics ( ) . names ( ) . get ( ) ; remaining . retain all ( topic names ) ; }
singleton behaviour : create the cache instance if required .,if ( hive client cache == null ) { synchronized ( i meta store client . class ) { if ( hive client cache == null ) { hive client cache = new hive client cache ( hive conf ) ; } } } try { return hive client cache . get ( hive conf ) ; } catch ( login exception e ) { throw new io exception ( __str__ @$ e ) ; }
using different key returns a different object,assert . assert not equals ( task broker . get shared resource at scope ( new test factory < gobblin scope types > ( ) @$ new test resource key ( __str__ ) @$ gobblin scope types . job ) @$ resource ) ;
ask user to report all bugs which are n't timeout errors,if ( ! timeout occured ( e2 ) ) { anki droid app . send exception report ( e2 @$ __str__ ) ; } data . success = false ; data . result = new object [ ] { __str__ } ; return data ;
if names are different in some way,bookmark contents = __str__ ; plate comment contents = __str__ ; if ( name analysis . num names ( ) == __num__ ) { plate comment contents = plate comment contents + __str__ ; bookmark contents = bookmark contents + __str__ ; } else { plate comment contents = plate comment contents + __str__ ; bookmark contents = bookmark contents + __str__ ; }
it is a directory so merge everything in the directories,for ( file status sub from : fs . list status ( from . get path ( ) ) ) { path sub to = new path ( to @$ sub from . get path ( ) . get name ( ) ) ; merge paths ( fs @$ sub from @$ sub to @$ context ) ; }
set the information of the injector .,string injector pid = registry . get plugin id ( step plugin type . class @$ im ) ; step meta injector step = new step meta ( injector pid @$ injector stepname @$ im ) ; trans meta . add step ( injector step ) ;
sort a two element list by swapping if necessary,if ( lo == hi - __num__ ) { if ( a [ lo ] > a [ hi ] ) { int t = a [ lo ] ; a [ lo ] = a [ hi ] ; a [ hi ] = t ; } return ; }
function to create a stream using a random boxed spliterator,function < function < splittable random @$ integer > @$ int stream > rbsf = sf -> stream support . stream ( new random boxed spliterator < > ( new splittable random ( ) @$ __num__ @$ size @$ sf ) @$ false ) . map to int ( i -> i ) ;
ip file is assigned to the first device,list < fpga device > devices = fpga resource handler . get fpga allocator ( ) . get allowed fpga ( ) ; fpga device device = devices . get ( __num__ ) ; assert equals ( __str__ @$ expected_hash @$ device . get aocx hash ( ) ) ;
if its input does not change at all between builds .,assert action executions ( new execution counting action factory ( ) { @ override public execution counting action create ( artifact input @$ artifact output @$ atomic integer execution counter ) { return unconditional execution ? new execution counting cache bypassing action ( input @$ output @$ execution counter ) : new execution counting action ( input @$ output @$ execution counter ) ; } } @$ change artifact . dont_change @$ callables . < void > returning ( null ) @$ expect action is . not_dirtied ) ;
add every relevant class to the ic attribute :,if ( ic refs . contains ( ic . this class ) || ic . outer class == this . this class ) { if ( verbose > __num__ ) utils . log . fine ( __str__ + ic ) ; impliedi cs . add ( ic ) ; }
condition 2 : graph is connected @$ ignoring isolated vertices,int s = non isolated vertex ( g ) ; breadth first paths bfs = new breadth first paths ( g @$ s ) ; for ( int v = __num__ ; v < g . v ( ) ; v ++ ) if ( g . degree ( v ) > __num__ && ! bfs . has path to ( v ) ) return false ; return true ;
gets the depth of the meta node in the oid tree,final int depth = handlers . get oid depth ( h ) ; for ( enumeration < snmp mib sub request > rqs = handlers . get sub requests ( h ) ; rqs . has more elements ( ) ; ) { meta . get ( rqs . next element ( ) @$ depth ) ; }
s 2 is asked to commit .,segment completion mgr . _seconds += __num__ ; params = new request . params ( ) . with instance id ( s2 ) . with offset ( s2 offset ) . with segment name ( segment name str ) ; response = segment completion mgr . segment consumed ( params ) ; assert . assert equals ( response . get status ( ) @$ segment completion protocol . controller response status . commit ) ;
saving the original value of the param @$ for restoration purposes,string old param = client . get config ( ) . get ( __str__ @$ __str__ ) ; string original path = client . addam env ( ) . get ( __str__ ) ; client . get config ( ) . set ( __str__ @$ __str__ ) ; string new path = client . addam env ( ) . get ( __str__ ) ; assert . assert equals ( original path + __str__ @$ new path ) ;
if a registered builder was n't found then use the default,media source builder builder = source type builder != null ? source type builder . builder : new default media source builder ( ) ; return builder . build ( context @$ uri @$ user agent @$ handler @$ transfer listener ) ;
extract and reconstruct query pattern with the required i ds,answer proto . concept map explainable = explanation req . get explainable ( ) ; pattern query pattern = graql . parse pattern ( explainable . get pattern ( ) ) ; explanation explanation = tx . explanation ( query pattern ) ; transaction . res response = response builder . transaction . explanation ( explanation ) ; on next response ( response ) ;
restore the corresponding record set for every input channel,final map < integer @$ array deque < serialization test type > > serialized records = new hash map < > ( ) ; for ( int i = __num__ ; i < number of channels ; i ++ ) { serialized records . put ( i @$ new array deque < > ( ) ) ; }
fallback to the label labeling us if it exists,if ( name == null ) { object o = get client property ( j label . labeled_by_property ) ; if ( o instanceof accessible ) { accessible context ac = ( ( accessible ) o ) . get accessible context ( ) ; if ( ac != null ) { name = ac . get accessible name ( ) ; } } } return name ;
get interacted view from x @$ y coordinate .,view child = m recycler view . find child view under ( e . getx ( ) @$ e . gety ( ) ) ; if ( child != null ) { get table view listener ( ) ; recycler view . view holder holder = m recycler view . get child view holder ( child ) ; get table view listener ( ) . on row header long pressed ( holder @$ holder . get adapter position ( ) ) ; }
block originally on a 1 @$ a 2 @$ b 1,list < datanode storage info > orig storages = get storages ( __num__ @$ __num__ @$ __num__ ) ; list < datanode descriptor > orig nodes = get nodes ( orig storages ) ; block info block info = add block on nodes ( test index @$ orig nodes ) ;
or the one from the super persistent class,component propagated mapper = identifier mapper ; if ( propagated mapper == null ) { if ( super mapped superclass != null ) { propagated mapper = super mapped superclass . get identifier mapper ( ) ; } if ( propagated mapper == null && super persistent class != null ) { propagated mapper = super persistent class . get identifier mapper ( ) ; } } return propagated mapper ;
major compaction for all region servers .,for ( h region server rs : rs list ) admin . major compact region server ( rs . get server name ( ) ) . get ( ) ; thread . sleep ( __num__ ) ; int count after major compaction = count store files in families ( regions @$ families ) ; assert equals ( __num__ @$ count after major compaction ) ;
create the repository function everything flows through .,builder . add sky function ( sky functions . repository @$ new repository loader function ( ) ) ; repository delegator function repository delegator function = new repository delegator function ( repository handlers @$ skylark repository function @$ is fetch @$ client environment supplier @$ directories @$ managed directories knowledge ) ; builder . add sky function ( sky functions . repository_directory @$ repository delegator function ) ; filesystem = runtime . get file system ( ) ;
string for this locator is accumulated in this buffer,string builder locatorsb = new string builder ( ) ; int port index = locator . index of ( __str__ ) ; if ( port index < __num__ ) { port index = locator . last index of ( __str__ ) ; } if ( port index < __num__ ) { throw new illegal argument exception ( string . format ( __str__ @$ value ) ) ; }
check that the client user is insecure,assert not same ( ugi @$ client ugi ) ; assert equals ( authentication method . simple @$ client ugi . get authentication method ( ) ) ; assert equals ( client username @$ client ugi . get user name ( ) ) ; client conf . set ( user . hbase_security_conf_key @$ __str__ ) ; server conf . set boolean ( rpc server . fallback_to_insecure_client_auth @$ true ) ; call rpc service ( user . create ( client ugi ) ) ;
binds the request metrics to the current request .,request . setaws request metrics ( aws request metrics ) ; request . add handler context ( handler context key . signing_region @$ get signing region ( ) ) ; request . add handler context ( handler context key . service_id @$ __str__ ) ; request . add handler context ( handler context key . operation_name @$ __str__ ) ; request . add handler context ( handler context key . advanced_config @$ advanced config ) ; aws request metrics . end event ( field . request marshall time ) ;
transfer opening transition to new display .,if ( prev dc . m opening apps . remove ( this ) ) { m display content . m opening apps . add ( this ) ; m display content . prepare app transition ( prev dc . m app transition . get app transition ( ) @$ true ) ; m display content . execute app transition ( ) ; }
just add it to the queue if there is capacity,final work queue queue = ( ( thread pool ) executor ) . get queue ( ) ; if ( ! queue . enqueue ( task ) ) { logger . warning ( rejection message ( executor ) ) ; rejections . increment and get ( ) ; throw exception ( executor ) ; }
save the content to the file specified by the user,string save as = parse result . get param value as string ( cli strings . netstat__file ) ; if ( save as == null ) { return result ; } file file = new file ( save as ) . get absolute file ( ) ; result . save file to ( file . get parent file ( ) ) ; return result ;
set other optimizations options to false here,if ( boolean . true . equals ( opt options . get ( __str__ ) ) ) invokedynamic = true ; if ( boolean . false . equals ( opt options . get ( __str__ ) ) ) optimize for int = false ; if ( invokedynamic ) optimize for int = false ;
alias inner fields to avoid retrieving entire struct as a string .,results execute = query ( __str__ ) ; assert equals ( __str__ @$ execute . get row ( __num__ ) . get ( __str__ ) ) ; assert equals ( __str__ @$ execute . get row ( __num__ ) . get ( __str__ ) ) ; assert equals ( __str__ @$ execute . get row ( __num__ ) . get ( __str__ ) ) ;
quote string values which starts from number,if ( ! string value . is empty ( ) && ! ( row [ i ] instanceof number ) && ! ( row [ i ] instanceof date ) && character . is digit ( string value . char at ( __num__ ) ) ) { quote = true ; }
now verify the values of the marker data,for ( map . entry < string @$ string > value : repo env . entry set ( ) ) { if ( ! marker data . contains key ( __str__ + value . get key ( ) ) ) { return false ; } string marker value = marker data . get ( __str__ + value . get key ( ) ) ; if ( ! objects . equals ( marker value @$ value . get value ( ) ) ) { return false ; } } return true ;
test if two rules get enabled as well,string [ ] two rules = { __str__ @$ __str__ } ; string result en = check with optionsv2 ( english @$ german @$ __str__ @$ two rules @$ nothing @$ false ) ; assert true ( __str__ + result en @$ result en . contains ( __str__ ) ) ; assert true ( __str__ + result en @$ result en . contains ( __str__ ) ) ;
baseline : include type @$ verify things work :,string full json = mapper . write value as string ( input ) ; external bean with default output = mapper . read value ( full json @$ external bean with default . class ) ; assert not null ( output ) ; assert not null ( output . bean ) ;
verify 1 outstanding scheduler key is removed,assert . assert equals ( __num__ @$ scheduler keys . size ( ) ) ; res reqs = ( ( capacity scheduler ) scheduler ) . get application attempt ( attempt id ) . get app scheduling info ( ) . get all resource requests ( ) ;
initialize the server to its hosing region counter map,map < server name @$ integer > server to hosting region counter map = new hash map < > ( ) ; map < server name @$ integer > primaryrs to region counter map = new hash map < > ( ) ; map < server name @$ set < server name > > primary to sec terrs map = new hash map < > ( ) ;
create an ec dir and write a test file in it,final path ec dir = new path ( __str__ ) ; path ec file = new path ( ec dir @$ __str__ ) ; path non ec file = new path ( ec dir @$ __str__ ) ; fs . mkdirs ( ec dir ) ;
construct corner points @$ start from top left and go counter clockwise,final float p1x = fx ; final float p1y = fy ; final float p2x = fx ; final float p2y = fy2 ; final float p3x = fx2 ; final float p3y = fy2 ; final float p4x = fx2 ; final float p4y = fy ; float x1 ; float y1 ; float x2 ; float y2 ; float x3 ; float y3 ; float x4 ; float y4 ;
we are done with the first array .,if ( ( store . length - first array position - len ) == __num__ ) { encoded arrays . remove ( __num__ ) ; first array position = __num__ ; } else { first array position = first array position + len ; }
run on multiple text files @$ based off crf classifier code,string text files = props . get property ( __str__ ) ; if ( text files != null ) { list < file > files = new array list < > ( ) ; for ( string filename : text files . split ( __str__ ) ) { files . add ( new file ( filename ) ) ; } ncc . classify files and write answers ( files ) ; }
cache the position of the destination field,if ( data . index of destination < __num__ ) { string real destination fieldname = meta . get destination ( ) ; data . index of destination = data . previous row meta . index of value ( real destination fieldname ) ; if ( data . index of destination < __num__ ) { throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ real destination fieldname ) ) ; } }
now read what was written @$ waking up the server for write,input stream client input = client . get input stream ( ) ; while ( size . get and decrement ( ) > __num__ ) { client input . read ( ) ; } assert null ( failure . get ( ) ) ;
apply processing relevant for any kind of tag opening,if ( this . scraper != null ) { this . scraper . scrape any tag opening ( tag ) ; } if ( this . scraper != null && this . scraper . is tag0 ( tagname ) ) { this . scraper . scrape tag0 ( tag ) ; }
test with null list of part names,stats = ms client . get partition column statistics ( db name @$ tbl name @$ null @$ lists . new array list ( __str__ ) @$ constants . hive_engine @$ current write ids ) ; assert equals ( __num__ @$ stats . size ( ) ) ;
check the request to find close connection option header,input stream is = s . get input stream ( ) ; message header mh = new message header ( is ) ; string conn header = mh . find value ( __str__ ) ; if ( conn header != null && conn header . equals ignore case ( __str__ ) ) { has close header = true ; } print stream out = new print stream ( new buffered output stream ( s . get output stream ( ) ) ) ;
check for exact signature matches across all certs .,signature [ ] certs = m certs . to array ( new signature [ __num__ ] ) ; if ( pkg . m signing details != signing details . unknown && ! signature . are exact match ( certs @$ pkg . m signing details . signatures ) ) { if ( certs . length > __num__ || ! pkg . m signing details . has certificate ( certs [ __num__ ] ) ) { return null ; } }
not synthetic class and not inner class .,if ( ! class metadata . is synthetic ( ) && ! class metadata . is inner class ( ) ) { this . interface caches . put ( operand . get interface internal name ( ) @$ class metadata . get class internal name ( ) ) ; }
make sure the full stdout is read .,while ( stdout . ready ( ) ) { process log . append ( char streams . to string ( stdout ) ) ; } if ( process . exit value ( ) != __num__ ) { throw new runtime exception ( string . format ( __str__ @$ action ) + __str__ + process log ) ; } return process log . to string ( ) ;
bitmask will be created lazily during the blit phase,try { cached data = x11 surface data . create data ( x11gc @$ w @$ h @$ x11gc . get color model ( ) @$ null @$ __num__ @$ get transparency ( ) ) ; } catch ( out of memory error oome ) { }
we assume here that there are no core apps left .,for ( package parser . package p : others ) { if ( p . core app ) { throw new illegal state exception ( __str__ ) ; } m dexopt commands . add all ( generate package dexopts ( p @$ package manager service . reason_first_boot ) ) ; }
now recursively search through super interfaces .,for ( soot class interfaze : c . get interfaces ( ) ) { resolved entry e = resolve ( interfaze ) ; if ( e != null ) { return e ; } } c = c . has superclass ( ) ? c . get superclass ( ) : null ;
match ! skip to the start of the pattern .,if ( buffer [ i ] == __str__ && buffer [ i + __num__ ] == __str__ && buffer [ i + __num__ ] == __str__ && buffer [ i + __num__ ] == __str__ ) { input . skip fully ( i ) ; return true ; }
exactly 1 colon . split into host : port .,if ( colon pos >= __num__ && host port string . index of ( __str__ @$ colon pos + __num__ ) == - __num__ ) { host = host port string . substring ( __num__ @$ colon pos ) ; port string = host port string . substring ( colon pos + __num__ ) ; } else { host = host port string ; }
intercept the volume keys to affect only remote volume .,switch ( key code ) { case key event . keycode_volume_down : if ( action == key event . action_down ) controller . set remote volume ( - __num__ ) ; return true ; case key event . keycode_volume_up : if ( action == key event . action_down ) controller . set remote volume ( __num__ ) ; return true ; default : return false ; }
and now delegate to the engine to render it .,engine . render ( data @$ __str__ @$ res -> { if ( res . succeeded ( ) ) { ctx . response ( ) . end ( res . result ( ) ) ; } else { ctx . fail ( res . cause ( ) ) ; } } ) ;
hmm @$ allow indices at endpoints ? for now @$ yes .,if ( index < - __num__ || index > character count ) { throw new illegal argument exception ( __str__ ) ; } ensure cache ( ) ; if ( index == - __num__ || index == character count ) { return ( byte ) ( text line . is directionltr ( ) ? __num__ : __num__ ) ; } return text line . get char level ( index ) ;
verify whether the sql operation log is generated and fetch correctly .,operation handle operation handle = client . execute statement ( session handle @$ sql @$ null ) ; row set row set log = client . fetch results ( operation handle @$ fetch orientation . fetch_first @$ __num__ @$ fetch type . log ) ; verify fetched log ( row set log @$ expected logs verbose ) ;
if no progress @$ throttle a bit,if ( next row count == current row count ) { try { thread . sleep ( __num__ ) ; } catch ( exception e2 ) { } } current row count = next row count ; log . info ( __str__ + table name . to upper case ( ) + __str__ + current row count + __str__ + rows loaded . get ( ) ) ; log . info ( __str__ + table name . to upper case ( ) + __str__ ) ;
add any explicit orm.xml references passed in,final list < string > explicit orm xml list = ( list < string > ) configuration values . remove ( available settings . xml_file_names ) ; if ( explicit orm xml list != null ) { explicit orm xml list . for each ( metadata sources :: add resource ) ; } return attribute converter definitions ;
broker will die on commit reply so this will hang till restart,executors . new single thread executor ( ) . execute ( new runnable ( ) { public void run ( ) { log . info ( __str__ ) ; try { session . commit ( ) ; } catch ( jms exception e ) { assert true ( e instanceof transaction rolled back exception ) ; log . info ( __str__ @$ e ) ; } commit done latch . count down ( ) ; log . info ( __str__ ) ; } } ) ;
variant case is not normalized in lenient variant mode,string source = __str__ ; string target = source ; string defaulted = __str__ ; builder builder = new builder ( ) ; string result = builder . set variant ( source ) . build ( ) . get variant ( ) ; assert equals ( __str__ @$ target @$ result ) ; result = builder . set variant ( __str__ ) . build ( ) . to language tag ( ) ; assert equals ( __str__ @$ __str__ @$ result ) ;
apply expansion state of the dropped node to the new node .,if ( tree . is expanded ( dest node . get tree path ( ) ) ) { program node newnode = tree . get child ( dest node @$ drop node . get name ( ) ) ; if ( newnode != null ) { tree . match expansion state ( drop node @$ newnode ) ; } }
test existing directory with no files case,file new dir = new file ( tmp . get path ( ) @$ __str__ ) ; new dir . mkdir ( ) ; assert . assert true ( __str__ @$ new dir . exists ( ) ) ; files = file util . list ( new dir ) ; assert . assert equals ( __str__ @$ __num__ @$ files . length ) ; new dir . delete ( ) ; assert . assert false ( __str__ @$ new dir . exists ( ) ) ;
get service ticket from the authorization header,string service ticket base64 = get auth header ( request @$ auth type ) ; byte [ ] in token = base64 . decode base64 ( service ticket base64 . get bytes ( ) ) ; gss context . accept sec context ( in token @$ __num__ @$ in token . length ) ;
close the client without changing the node,zk . close ( ) ; watched event event = queue . poll ( __num__ @$ time unit . seconds ) ; assert not null ( __str__ @$ event ) ; assert equals ( __str__ @$ event . event type . none @$ event . get type ( ) ) ; assert equals ( __str__ @$ event . keeper state . closed @$ event . get state ( ) ) ; if ( zk != null ) { zk . close ( ) ; }
the current billing event happens at the same date,if ( prev computed fixed item != null && prev computed fixed item . get start date ( ) . compare to ( cur local effective date ) == __num__ && prev computed fixed item . get subscription id ( ) . compare to ( current billing event . get subscription id ( ) ) == __num__ ) { return true ; } else { return false ; }
ensure the parent configuration does not leak down to us,dispatcher servlet . set detect all handler adapters ( false ) ; dispatcher servlet . set detect all handler exception resolvers ( false ) ; dispatcher servlet . set detect all handler mappings ( false ) ; dispatcher servlet . set detect all view resolvers ( false ) ; return dispatcher servlet ;
dynamic cache lookup is only for assets .,if ( key != null ) { synchronized ( s dynamic cache lock ) { final typeface typeface = s dynamic typeface cache . get ( key ) ; if ( typeface != null ) { return typeface ; } } }
kudu does n't support not in .,if ( is not ) { return collections . empty list ( ) ; } else { list < object > values = leaf . get literal list ( ) . stream ( ) . map ( ( object v ) -> to kudu value ( v @$ column ) ) . collect ( collectors . to list ( ) ) ; return collections . singleton list ( kudu predicate . new in list predicate ( column @$ values ) ) ; }
see 5083555 check if src component is in any modal blocked window,component c = native container ; while ( ( c != null ) && ! ( c instanceof window ) ) { c = c . get parent_ no client code ( ) ; } if ( ( c == null ) || ( ( window ) c ) . is modal blocked ( ) ) { return ; }
should not work : index out of bounds,try { tuple ds . aggregate ( aggregations . sum @$ __num__ ) ; assert . fail ( ) ; } catch ( illegal argument exception iae ) { } catch ( exception e ) { assert . fail ( ) ; }
try to put a multiple cells with different rows,final h base client service h base client service = runner . get process context ( ) . get property ( test processor . hbase_client_service ) . as controller service ( h base client service . class ) ; h base client service . put ( table name @$ arrays . as list ( put flow file1 @$ put flow file2 ) ) ;
delay a frame to eat the key typed event .,if ( keycode == keycode2 ) { gdx . app . post runnable ( new runnable ( ) { public void run ( ) { result ( object ) ; if ( ! cancel hide ) hide ( ) ; cancel hide = false ; } } ) ; }
step 3 invoke the method with the output stream,stream . println ( four_indent + __str__ ) ; symtab entry mtype = m . type ( ) ; if ( mtype != null ) util . write initializer ( four_indent @$ __str__ @$ __str__ @$ mtype @$ write input stream read ( __str__ @$ mtype ) @$ stream ) ;
... and each possible labeling for that clique,for ( int k = __num__ ; k < label index . size ( ) ; k ++ ) { float p = probs [ j ] [ k ] ; for ( int n = __num__ ; n < data [ d ] [ e ] [ j ] . length ; n ++ ) { e [ data [ d ] [ e ] [ j ] [ n ] ] [ k ] += p ; } }
the activity was register in the host .,if ( target class == null ) { if ( has set up ) return ; if ( intent . has category ( intent . category_launcher ) ) { return ; } small . set up on demand ( ) ; return ; }
every third row should be fetched in distributed case .,assert true ( loc ? ! res . is empty ( ) : res . size ( ) >= cache_size / __num__ && res . size ( ) <= cache_size / __num__ ) ; list < integer > keys = new array list < > ( ) ; for ( list < ? > r : res ) { assert equals ( __num__ @$ r . size ( ) ) ; keys . add ( ( integer ) r . get ( __num__ ) ) ; }
only make reference if other reference or symbol exists,memory spaces = new array list < > ( ) ; for ( address space space : program . get address factory ( ) . get address spaces ( ) ) { if ( ! space . is loaded memory space ( ) ) { continue ; } if ( space == default addr space ) { memory spaces . add ( __num__ @$ space ) ; } else { memory spaces . add ( space ) ; } }
use a new thread to connect to each server,for ( final string server : server array ) { new thread ( new runnable ( ) { @ override public void run ( ) { connect to one server with retry ( server ) ; connections . count down ( ) ; } } ) . start ( ) ; }
offsets plus one to skip the in use byte,int offset = ( position . log_version . id * get record size ( ) ) + __num__ ; long value = cursor . get long ( offset ) + __num__ ; cursor . put long ( offset @$ value ) ; check for decoding errors ( cursor @$ position . log_version . id @$ normal ) ; version field = value ;
this should work @$ because this bucket is still available .,vm0 . invoke ( ( ) -> { check data ( bucket id onvm0 @$ bucket id onvm0 + __num__ @$ __str__ ) ; check data ( bucket id onvm1 @$ bucket id onvm1 + __num__ @$ null ) ; create data ( bucket id onvm1 @$ bucket id onvm1 + __num__ @$ __str__ ) ; } ) ;
and the rule key is stable .,gen aidl gen aidl rule no deps3 = create gen aidl rule ( immutable sorted set . of ( path to aidl ) ) ; rule key rule key3 = factory . build ( gen aidl rule no deps3 ) ; assert equals ( rule key2 @$ rule key3 ) ;
our payload is simply the offset and size of our element,int el offset = ( int ) already decided . get ( el ) . get decided value ( layout decision . kind . offset ) ; int el size = ( int ) already decided . get ( el ) . get decided value ( layout decision . kind . size ) ; out . write4 byte ( el offset ) ; out . write4 byte ( el size ) ;
add the packets panel to the information panel,information panel . add ( packets panel @$ border layout . center ) ; tabbed pane . add ( __str__ @$ new j scroll pane ( information panel ) ) ; tabbed pane . set tool tip text at ( __num__ @$ __str__ ) ;
wait for all validation queries to finish,log . info ( string . format ( __str__ @$ this . futures . size ( ) ) ) ; this . exec . shutdown ( ) ; this . exec . await termination ( __num__ @$ time unit . hours ) ; boolean one future failure = false ;
after 4 seconds should remain node local,clock . tick sec ( __num__ ) ; assert equals ( node type . node_local @$ scheduler app . get allowed locality level by time ( prio @$ node locality delay ms @$ rack locality delay ms @$ clock . get time ( ) ) ) ;
each compiled registry is merged into the main registry,type registry . merge ( schema parser . parse ( schema file1 ) ) ; type registry . merge ( schema parser . parse ( schema file2 ) ) ; type registry . merge ( schema parser . parse ( schema file3 ) ) ; graphql schema graphql schema = schema generator . make executable schema ( type registry @$ build runtime wiring ( ) ) ;
by default it 's enabled when fragment is resumed,assert true ( fragment . is controls overlay auto hide enabled ( ) ) ; fragment . m autohide timer after playing in ms = __num__ ; activity test rule . run on ui thread ( new runnable ( ) { public void run ( ) { fragment . set controls overlay auto hide enabled ( false ) ; } } ) ; assert no slide out ( fragment ) ;
expected to pass since user 2 belongs to role 2,callable = ( ) -> { partialdd bean . to be invoked by role2 ( ) ; try { partialdd bean . to be invoked only by role1 ( ) ; assert . fail ( __str__ ) ; } catch ( ejb access exception ejbae ) { } return null ; } ;
no presenter found in cache @$ most likely caused by process death,presenter = create view id and presenter ( ) ; if ( debug ) { log . d ( debug_tag @$ __str__ + mosby view id + __str__ + presenter + __str__ + delegate callback . get mvp view ( ) ) ; } return presenter ;
always add the out of the box auto deployment strategies as last,deployment strategies . add ( new default auto deployment strategy ( deployment properties ) ) ; deployment strategies . add ( new single resource auto deployment strategy ( deployment properties ) ) ; deployment strategies . add ( new resource parent folder auto deployment strategy ( deployment properties ) ) ; configuration . set deployment strategies ( deployment strategies ) ; return configuration ;
was previous blocked and not in memory @$ so add,if ( left tuple . get blocker ( ) == null ) { insert child left tuple ( sink @$ trg left tuples @$ ltm @$ left tuple @$ right tuple . get propagation context ( ) @$ true ) ; }
notify listener of a new message .,if ( lsnr != null ) lsnr . on message received ( desc @$ msg ) ; else if ( log . is debug enabled ( ) ) log . debug ( __str__ + __str__ + desc + __str__ + msg + __str__ ) ;
identify the flowfile lookupvalue and search against the row map,string ff lookup value = context . get property ( query_input ) . evaluate attribute expressions ( flow file ) . get value ( ) ; if ( row map . contains key ( ff lookup value ) ) { flow file = session . put all attributes ( flow file @$ row map . get ( ff lookup value ) ) ; flow files matched . add ( flow file ) ; } else { flow files not matched . add ( flow file ) ; }
use identity set for collect inner packages,set < j package > inner packages = collections . new set from map ( new identity hash map < > ( ) ) ; for ( j package pkg : pkg map . values ( ) ) { inner packages . add all ( pkg . get inner packages ( ) ) ; }
base all offsets off of cur item .,final int item count = m items . size ( ) ; float offset = cur item . offset ; int pos = cur item . position - __num__ ; m first offset = cur item . position == __num__ ? cur item . offset : - float . max_value ; m last offset = cur item . position == n - __num__ ? cur item . offset + cur item . width factor - __num__ : float . max_value ;
recreate icon factory to update default adaptive icon scale .,m bubble icon factory = new bubble icon factory ( m context ) ; set up flyout ( ) ; for ( bubble b : m bubble data . get bubbles ( ) ) { b . get icon view ( ) . set bubble icon factory ( m bubble icon factory ) ; b . get icon view ( ) . update views ( ) ; b . get expanded view ( ) . apply theme attrs ( ) ; }
allow upgrade since imported project archives must always be upgraded,domain object dobj = df . get domain object ( consumer @$ true @$ false @$ monitor ) ; if ( ! ( dobj instanceof data type archive ) ) { if ( dobj != null ) { dobj . release ( consumer ) ; df . delete ( ) ; } throw new io exception ( __str__ + filename ) ; } list < domain object > results = new array list < domain object > ( ) ; results . add ( dobj ) ; return results ;
process pending aborts and reads outside of synchronized lock,aborted buffers = immutable set . copy of ( this . aborted buffers ) ; this . aborted buffers . clear ( ) ; pending reads = immutable list . copy of ( this . pending reads ) ; this . pending reads . clear ( ) ;
extracts and returns the 'location ' header .,list < string > location headers = response . get header ( __str__ ) ; if ( location headers . size ( ) != __num__ ) { throw build registry error exception ( __str__ + location headers . size ( ) ) ; } string location header = location headers . get ( __num__ ) ; return response . get request url ( ) . tourl ( location header ) ;
retrieve the invoice and make sure it belongs to the right account,final invoice item model dao invoice item = transactional . get by id ( invoice item id . to string ( ) @$ context ) ; if ( invoice item == null ) { throw new invoice api exception ( error code . invoice_item_not_found @$ invoice item id ) ; } transactional . update item fields ( invoice item id . to string ( ) @$ amount @$ null @$ null @$ context ) ; return null ;
map the conversion function over all bids .,bid bid = c . element ( ) ; c . output ( new bid ( bid . auction @$ bid . bidder @$ bid . price @$ bid . date time @$ c . side input ( side input map ) . get ( bid . bidder % configuration . side input row count ) ) ) ;
verify that undeploying a nonexistent job from a host fails,final string output = cli ( __str__ @$ __str__ @$ bogus_job . to string ( ) @$ test host ( ) ) ; final job deploy response job deploy response = json . read ( output @$ job deploy response . class ) ; assert equals ( job deploy response . status . job_not_found @$ job deploy response . get status ( ) ) ;
creates an edge to ' h ',while ( l != e ) { edge b = new edge ( ) ; b . info = edge . exception ; b . successor = h ; if ( ( l . status & label . jsr ) == __num__ ) { b . next = l . successors ; l . successors = b ; } else { b . next = l . successors . next . next ; l . successors . next . next = b ; } l = l . successor ; }
write the private key to the specified file .,pkcs8 encoded key spec pkcs8 encoded key spec = new pkcs8 encoded key spec ( private key . get encoded ( ) ) ; file output stream private key output stream = new file output stream ( dir + file . separator + private key name ) ; private key output stream . write ( pkcs8 encoded key spec . get encoded ( ) ) ; private key output stream . close ( ) ;
make sure if it is a copied buffer .,actual value . set byte ( __num__ @$ ( byte ) ( actual value . get byte ( __num__ ) + __num__ ) ) ; assert false ( buffer . get byte ( i ) == actual value . get byte ( __num__ ) ) ; actual value . release ( ) ;
carry any resulting overflow all the way to the end .,for ( int i = __num__ ; i < assignments . length ; i ++ ) { if ( assignments [ i ] >= dimensions [ i ] ) { assignments [ i ] = __num__ ; if ( i < assignments . length - __num__ ) { assignments [ i + __num__ ] ++ ; } } else { break ; } } return assignments ;
ensure data getting federated from managing node,long start = membermx bean . get member up time ( ) ; geode awaitility . await ( ) . until asserted ( ( ) -> assert that ( membermx bean . get member up time ( ) ) . is greater than ( start ) ) ;
confirm current cluster is imbalanced on all fronts :,assert false ( verify nodes balanced in each zone ( current cluster ) ) ; assert false ( verify zones balanced ( current cluster ) ) ; partition balance current pb = new partition balance ( current cluster @$ current stores ) ;
preserve a vowel with dialytika in precomposed form if it exists .,if ( ( data & has_dialytika ) != __num__ ) { if ( upper == __str__ ) { upper = __str__ ; data &= ~ has_either_dialytika ; } else if ( upper == __str__ ) { upper = __str__ ; data &= ~ has_either_dialytika ; } }
fall back behavior as defined in tr 35,if ( numbers keyword . equals ( __str__ ) || numbers keyword . equals ( __str__ ) ) { numbers keyword = __str__ ; } else if ( numbers keyword . equals ( __str__ ) ) { numbers keyword = __str__ ; } else { break ; }
increases in size are cool ; shrinks not so much,if ( old type == new type ) { if ( old type == volt type . string && old in bytes == false && new in bytes == true ) { return old size * __num__ <= new size ; } return old size <= new size ; }
negative test to get an attribute of a mbean as jsonp,result = read output ( new url ( base url @$ __str__ ) ) ; log . info ( __str__ + result ) ; assert re find ( __str__ @$ result ) ; assert re find ( __str__ @$ result ) ; assert re find ( __str__ @$ result ) ;
now register only one service and verify that another one is removed,cache . add service for querying ( u0 @$ r1 @$ new service info ( t1 @$ uid1 ) ) ; assert equals ( __num__ @$ cache . get all services size ( u0 ) ) ; assert equals ( __num__ @$ cache . get persistent services size ( u0 ) ) ;
type of command to recognize advisory message,if ( message type == command types . activemq_message ) { object data = message . get data structure ( ) ; if ( data != null ) { ap map . put ( __str__ @$ data . get class ( ) . get simple name ( ) ) ; } }
token ratio is up to 3 decimal places,this . token ratio = ( int ) ( token ratio * three_decimal_places_scale_up ) ; this . max tokens = ( int ) ( max tokens * three_decimal_places_scale_up ) ; this . threshold = this . max tokens / __num__ ; token count . set ( this . max tokens ) ;
repeat the original operation at the new context,try { attrs = ref ctx . get attributes ( name @$ attr ids ) ; break ; } catch ( ldap referral exception re ) { lre = re ; continue ; } finally { ref ctx . close ( ) ; }
kick off an initial load of 20 items,page keyed data source . load initial params < page key > params = new page keyed data source . load initial params < > ( __num__ @$ false ) ; m data source . load initial ( params @$ m initial callback ) ;
than the old jme normalize as this method is commonly used .,float length = x * x + y * y + z * z ; if ( length != __num__ && length != __num__ ) { length = __num__ / fast math . sqrt ( length ) ; x *= length ; y *= length ; z *= length ; } return this ;
prior to handshaking @$ activate the handshake,if ( ! handshaker . activated ( ) ) { if ( connection state == cs_renegotiate ) { handshaker . activate ( protocol version ) ; } else { handshaker . activate ( null ) ; } if ( handshaker instanceof client handshaker ) { handshaker . kickstart ( ) ; } else { if ( connection state == cs_handshake ) { } else { handshaker . kickstart ( ) ; handshaker . handshake hash . reset ( ) ; } } }
create an instance of the packing class,try { packing = reflection utils . new instance ( packing class ) ; } catch ( illegal access exception | instantiation exception | class not found exception e ) { throw new packing exception ( string . format ( __str__ @$ packing class ) @$ e ) ; }
assert that reply is the correct error packet,assert equals ( initiatorjid @$ argument . get value ( ) . get to ( ) ) ; assert equals ( iq . type . error @$ argument . get value ( ) . get type ( ) ) ; assert equals ( stanza error . condition . resource_constraint @$ argument . get value ( ) . get error ( ) . get condition ( ) ) ;
ensure the command after the genrule setup is correct .,matcher m = setup_command_pattern . matcher ( command ) ; if ( m . matches ( ) ) { command = m . group ( __str__ ) ; } assert that ( command ) . is equal to ( expected ) ;
validate that value set must be between 0 and 255,try { the socket . set traffic class ( __num__ ) ; fail ( __str__ ) ; } catch ( illegal argument exception e ) { } try { the socket . set traffic class ( - __num__ ) ; fail ( __str__ ) ; } catch ( illegal argument exception e ) { }
expression . those listed below are the only valid kinds .,switch ( expr . get parent ( ) . get token ( ) ) { case and : case comma : case hook : case or : return true ; case arraylit : case objectlit : return expr . is spread ( ) ; default : return node util . is statement ( expr . get parent ( ) ) ; }
get the model evaluator in the detection config,collection < ? extends model evaluator < ? extends abstract spec > > model evaluators = config . get components ( ) . values ( ) . stream ( ) . filter ( component -> component instanceof model evaluator ) . map ( component -> ( model evaluator < ? extends abstract spec > ) component ) . collect ( collectors . to list ( ) ) ; if ( model evaluators . is empty ( ) ) { model evaluators = instantiate default evaluators ( config ) ; }
make sure that the chosen node is in the target .,int i = __num__ ; for ( ; i < targets . length && ! storages [ __num__ ] . equals ( targets [ i ] ) ; i ++ ) ; assert true ( i < targets . length ) ;
and ensure that the absolute path is also present .,if ( ! paths . get ( entry . get key ( ) ) . is absolute ( ) ) { cached values . put ( filesystem . resolve ( entry . get key ( ) ) @$ hash code . from string ( entry . get value ( ) ) ) ; }
fail to add a label with post,response = r . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . query param ( __str__ @$ not user name ) . accept ( media type . application_json ) . entity ( __str__ @$ media type . application_json ) . post ( client response . class ) ;
configured for durable metadata prepare all the paths needed for the checkpoints,if ( checkpoints directory != null ) { check state ( file system != null ) ; final path checkpoint dir = create checkpoint directory ( checkpoints directory @$ checkpoint id ) ; file system . mkdirs ( checkpoint dir ) ; return new persistent metadata checkpoint storage location ( file system @$ checkpoint dir @$ max state size ) ; } else { return new non persistent metadata checkpoint storage location ( max state size ) ; }
add additional filename information for model imports in the apis,list < map < string @$ object > > imports = ( list < map < string @$ object > > ) operations . get ( __str__ ) ; for ( map < string @$ object > im : imports ) { im . put ( __str__ @$ im . get ( __str__ ) ) ; im . put ( __str__ @$ get modelname from model filename ( im . get ( __str__ ) . to string ( ) ) ) ; } return operations ;
if i sync @$ should see double the edits .,wal . sync ( ) ; reader = wals . create reader ( fs @$ wal path ) ; count = __num__ ; while ( ( entry = reader . next ( entry ) ) != null ) count ++ ; assert equals ( total * __num__ @$ count ) ; reader . close ( ) ;
the qualifier is a compound key hence match individual values,assert equals ( event id @$ event column name . get id ( ) ) ; assert equals ( exp ts @$ event column name . get timestamp ( ) ) ; assert equals ( exp key @$ event column name . get info key ( ) ) ; object value = e . get value ( ) ;
any kind of random delay works for the test,return _consistent delay for prop map . compute if absent ( prop @$ s -> math . max ( __num__ @$ _delay ms + ( ( int ) new random ( ) . next gaussian ( ) * delay_standard_deviation ) ) ) ;
length to read is too large @$ resize the buffer,if ( _octet buffer . length < _octet buffer length ) { byte [ ] new octet buffer = new byte [ _octet buffer length ] ; system . arraycopy ( _octet buffer @$ _octet buffer offset @$ new octet buffer @$ __num__ @$ octets in buffer ) ; _octet buffer = new octet buffer ; } else { system . arraycopy ( _octet buffer @$ _octet buffer offset @$ _octet buffer @$ __num__ @$ octets in buffer ) ; }
add this and the next one unless bracket,if ( cur char == __str__ ) { buff . append ( cur char ) ; if ( maybe append one more ( i + __num__ @$ s @$ buff ) ) { i ++ ; } found = true ; }
resume puts on server vm @$ add another 100 .,server . invoke ( ( ) -> { region < string @$ string > region = cluster startup rule . get cache ( ) . get region ( __str__ ) ; for ( int i = __num__ ; i < number_puts ; i ++ ) { region . put ( __str__ + i @$ __str__ + i ) ; } region . put ( __str__ @$ __str__ ) ; } ) ;
binds the request metrics to the current request .,request . setaws request metrics ( aws request metrics ) ; request . add handler context ( handler context key . signing_region @$ get signing region ( ) ) ; request . add handler context ( handler context key . service_id @$ __str__ ) ; request . add handler context ( handler context key . operation_name @$ __str__ ) ; request . add handler context ( handler context key . advanced_config @$ advanced config ) ; aws request metrics . end event ( field . request marshall time ) ;
acknowledge the last messages @$ if they were not yet acknowledged .,if ( ( num per sess > __num__ ) && ( ( cur - sess_start ) > __num__ ) ) msg . acknowledge ( ) ; if ( cur < num msg ) log ( __str__ + consumer tag + __str__ ) ;
multiple providers are not supported @$ so will use first available provider,logger . debug ( __str__ @$ provider . get item names ( ) ) ; transaction = null ; reverse order = false ; if ( command instanceof up down type ) { if ( ( up down type ) command == up down type . up ) { reverse order = true ; } }
do not run on devices with emulated external storage .,if ( environment . is external storage emulated ( ) ) { return ; } install from raw resource ( __str__ @$ r . raw . install_loc_sdcard @$ __num__ @$ true @$ false @$ - __num__ @$ package info . install_location_prefer_external ) ;
are we on the final line of the script ?,if ( ( current index + __num__ ) == durations . length || stop file created ( ) ) { should run = false ; } else { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + current index + __str__ + read probs [ current index ] + __str__ + write probs + __str__ + durations [ current index ] ) ; } current index ++ ; }
pass in null name @$ should cause exception,try { runtime service . create process instance query ( ) . variable value equals ignore case ( null @$ __str__ ) . single result ( ) ; fail ( __str__ ) ; } catch ( activiti illegal argument exception ae ) { assert equals ( __str__ @$ ae . get message ( ) ) ; }
shutdown thread context factory on fjp common thread,return completable future . all of ( futures . to array ( new completable future [ futures . size ( ) ] ) ) . when complete async ( ( r @$ e ) -> { thread context factory thread factory = this . thread factory ; if ( thread factory != null ) { thread factory . close ( ) ; } logger . info ( __str__ ) ; } ) ;
check that there is no 'cased ' letter after the index,for ( i = index + character . char count ( src . code point at ( index ) ) ; ( i < len ) && ! word boundary . is boundary ( i ) ; i += character . char count ( ch ) ) { ch = src . code point at ( i ) ; if ( is cased ( ch ) ) { return false ; } } return true ;
create a connection but do n't use it .,string scheme = http scheme . https . as string ( ) ; string host = __str__ ; int port = connector . get local port ( ) ; http destination destination = ( http destination ) client . get destination ( scheme @$ host @$ port ) ; duplex connection pool connection pool = ( duplex connection pool ) destination . get connection pool ( ) ;
expect an exception because no eviction plan is feasible,m thrown . expect ( worker out of space exception . class ) ; m thrown . expect message ( exception message . no_space_for_block_allocation_timeout . get message ( m test dir1 . get capacity bytes ( ) @$ m test dir1 . to block store location ( ) @$ free_space_timeout_ms @$ temp_block_id ) ) ; m block store . create block ( session_id1 @$ temp_block_id @$ m test dir1 . to block store location ( ) @$ m test dir1 . get capacity bytes ( ) ) ;
exception from the filter itself is fatal,err code = http servlet response . sc_forbidden ; authentication ex = ex ; if ( log . is debug enabled ( ) ) { log . debug ( __str__ + ex . get message ( ) @$ ex ) ; } else { log . warn ( __str__ + ex . get message ( ) ) ; }
binds the request metrics to the current request .,request . setaws request metrics ( aws request metrics ) ; request . add handler context ( handler context key . signing_region @$ get signing region ( ) ) ; request . add handler context ( handler context key . service_id @$ __str__ ) ; request . add handler context ( handler context key . operation_name @$ __str__ ) ; request . add handler context ( handler context key . advanced_config @$ advanced config ) ; aws request metrics . end event ( field . request marshall time ) ;
update the block type for this view,convert view . set tag ( r . id . note_block_tag_id @$ note block . get block type ( ) ) ; note block . set background color ( m background color ) ; return note block . configure view ( convert view ) ;
can get the answer with one operation @$ thus one roundoff .,if ( exp <= max small ten ) { r value = d value * small10pow [ exp ] ; if ( must set round dir ) { t value = r value / small10pow [ exp ] ; round dir = ( t value == d value ) ? __num__ : ( t value < d value ) ? __num__ : - __num__ ; } return ( is negative ) ? - r value : r value ; }
after the process is started @$ we have compensate event subscriptions :,assert equals ( __num__ @$ create event subscription query ( ) . event type ( __str__ ) . activity id ( __str__ ) . count ( ) ) ; assert equals ( __num__ @$ create event subscription query ( ) . event type ( __str__ ) . activity id ( __str__ ) . count ( ) ) ;
this time there should be no messages on this queue,restart broker ( connection @$ session ) ; queue viewm bean queue view = get proxy to queue ( name . get method name ( ) ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ;
test timestamps that result in integer size millis,final result < topn result value > result2 = new result < > ( date times . utc ( __num__ ) @$ new topn result value ( collections . singleton list ( immutable map . of ( __str__ @$ dim value @$ __str__ @$ __num__ @$ __str__ @$ dim value @$ __str__ @$ __num__ ) ) ) ) ;
change tab key to transfer focus to the next element,alert solution . add key listener ( new key adapter ( ) { @ override public void key pressed ( java . awt . event . key event evt ) { if ( evt . get key code ( ) == key event . vk_tab ) { alert solution . transfer focus ( ) ; } } } ) ;
this expects that method check column data has been executed,if ( chunk layout != null ) { int sum = __num__ ; for ( long num per chunk : chunk layout ) { sum += num per chunk ; } throw if ( sum > num rows @$ __str__ ) ; } else { chunk layout = new long [ ] { num rows } ; }
increase timeout if a full gc occurred after restarting stop watch,if ( rem <= __num__ ) { long timeout increase = get quorum timeout increase millis ( __num__ @$ millis ) ; if ( timeout increase > __num__ ) { et += timeout increase ; } else { throw new timeout exception ( ) ; } }
groovy currently resolves this to last found so traverse in reverse order,for ( int i = imported classes and packages . size ( ) - __num__ ; i >= __num__ ; i -- ) { string name = imported classes and packages . get ( i ) ; if ( ! aliases . contains value ( name ) && name . ends with ( slash name ) ) { type name = name ; break ; } }
can launch the 5 th attempt successfully,rm2 . wait for state ( app1 . get application id ( ) @$ rm app state . accepted ) ; mockam am5 = rm2 . wait for newam to launch and register ( app1 . get application id ( ) @$ __num__ @$ nm1 ) ; clock . reset ( ) ; rm2 . wait for state ( am5 . get application attempt id ( ) @$ rm app attempt state . running ) ;
connect using the delegation token passed via configuration object,system . out . println ( __str__ ) ; store token in job conf ( token ) ; url = __str__ + host + __str__ + port + __str__ ; con = driver manager . get connection ( url ) ; system . out . println ( __str__ + url ) ; run test ( ) ; con . close ( ) ;
both general and specific container types should be considered supported,_verify is found ( collection . class ) ; _verify is found ( list . class ) ; _verify is found ( map . class ) ; _verify is found ( set . class ) ; _verify is found ( array list . class ) ; _verify is found ( hash map . class ) ; _verify is found ( linked hash map . class ) ; _verify is found ( hash set . class ) ;
clear state for this processor @$ eradicating timestamp,runner . get state manager ( ) . clear ( scope . cluster ) ; assert . assert equals ( __str__ @$ __num__ @$ runner . get state manager ( ) . get state ( scope . cluster ) . to map ( ) . size ( ) ) ;
create the filter for the query id appender,plugin entry filter entry = new plugin entry ( ) ; filter entry . set class name ( test filter . class . get name ( ) ) ; plugin type < test filter > filter type = new plugin type < > ( filter entry @$ test filter . class @$ __str__ ) ; node filter node = new node ( query id appender node @$ __str__ @$ filter type ) ;
binds the request metrics to the current request .,request . setaws request metrics ( aws request metrics ) ; request . add handler context ( handler context key . signing_region @$ get signing region ( ) ) ; request . add handler context ( handler context key . service_id @$ __str__ ) ; request . add handler context ( handler context key . operation_name @$ __str__ ) ; request . add handler context ( handler context key . advanced_config @$ advanced config ) ; aws request metrics . end event ( field . request marshall time ) ;
