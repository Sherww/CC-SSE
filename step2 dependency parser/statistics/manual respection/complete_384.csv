match rows not equal expect all keys in all but one row,expected rows = this . num rows - __num__ ; expected keys = this . cols per row ; f = new row filter ( compare operator . not_equal @$ new binary comparator ( bytes . to bytes ( __str__ ) ) ) ; s = new scan ( ) ; s . set filter ( f ) ; verify scan no early out ( s @$ expected rows @$ expected keys ) ;,rows expect keys
clear fields so that we do n't keep extra references around,focus mgr = null ; awt event multicaster . save ( s @$ window listenerk @$ window listener ) ; awt event multicaster . save ( s @$ window focus listenerk @$ window focus listener ) ; awt event multicaster . save ( s @$ window state listenerk @$ window state listener ) ;,we keep references
create servers and regions put domain objects,final int port1 = start replicated cache server ( server1 ) ; server1 . invoke ( new serializable callable ( __str__ ) { @ override public object call ( ) throws exception { old observer = query observer holder . set instance ( new query result tracking observer ( ) ) ; return null ; } } ) ;,servers put objects
the folder should have only one command .,path log dir = workspace . resolve ( __str__ ) ; file [ ] command log directories list = ( log dir . to file ( ) ) . list files ( file :: is directory ) ; assert equals ( command log directories list . length @$ __num__ ) ;,folder have command
top right button makes top right button gone,final button top right button = find view by id ( r . id . top right button ) ; top right button . set on click listener ( new view . on click listener ( ) { public void on click ( view v ) { top right button . set visibility ( view . gone ) ; } } ) ;,button makes button
assume that attributes and namespaces immediately follow the element .,if ( dtm . element_node == type ) { while ( dtm . null != ( identity = get next node identity ( identity ) ) ) { type = _type ( identity ) ; if ( type == dtm . attribute_node ) { return identity ; } else if ( dtm . namespace_node != type ) { break ; } } },attributes follow element
if we have graphics configuration use it to get screen bounds,if ( gc != null ) { screen bounds = gc . get bounds ( ) ; insets = toolkit . get screen insets ( gc ) ; } else { screen bounds = new rectangle ( toolkit . get screen size ( ) ) ; insets = new insets ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; },we have graphics
method name can not use reserved keyword @$ e.g . return,if ( is reserved word ( operation id ) ) { string new operation id = underscore ( __str__ + operation id ) ; logger . warn ( operation id + __str__ + new operation id ) ; return new operation id ; } return underscore ( operation id ) ;,name use keyword
create the table schema specify an encryption algorithm without a key,htd = new h table descriptor ( table name . value of ( __str__ @$ __str__ ) ) ; h column descriptor hcd = new h column descriptor ( __str__ ) ; string algorithm = conf . get ( h constants . crypto_key_algorithm_conf_key @$ h constants . cipher_aes ) ; hcd . set encryption type ( algorithm ) ; htd . add family ( hcd ) ;,schema specify algorithm
at least one stack trace should contain transaction deadlock exception .,if ( has cause ( e @$ transaction timeout exception . class ) && has cause ( e @$ transaction deadlock exception . class ) ) { if ( deadlock . compare and set ( false @$ true ) ) u . error ( log @$ __str__ + transaction deadlock exception . class . get simple name ( ) @$ e ) ; },trace contain exception
we do n't want multiple instruments with the same class name,if ( ! class names used . contains ( class name ) ) { class names used . add ( class name ) ; instrument reflection reflection = new service loader instrument reflection ( provider ) ; list . add ( new instrument cache ( id @$ name @$ version @$ class name @$ internal @$ services class names @$ reflection ) ) ; },we want instruments
validate config so that we do n't get an npe,if ( user group information . is security enabled ( ) ) { check if property exists ( keytab_property_key ) ; check if property exists ( username_property_key ) ; try { security util . login ( conf @$ properties . get string ( keytab_property_key ) @$ properties . get string ( username_property_key ) ) ; } catch ( io exception ex ) { throw new metrics exception ( __str__ + ex . to string ( ) + __str__ @$ ex ) ; } },we get npe
check if any of the references cross the module boundaries .,if ( check modules && ref . module != null ) { if ( ref . module != fn module && ! module graph . depends on ( ref . module @$ fn module ) ) { is removable = false ; check modules = false ; } },any cross boundaries
committing the transaction should cause the unsent batch to be flushed,transaction manager . begin commit ( ) ; sender . run once ( ) ; assert false ( accumulator . has undrained ( ) ) ; assert true ( accumulator . has incomplete ( ) ) ; assert false ( transaction manager . has in flight transactional request ( ) ) ; assert false ( response future . is done ( ) ) ;,transaction cause batch
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result interface . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result interface . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
load a version of apple that does define that method5,apple . load new version ( __str__ @$ retrieve rename ( __str__ @$ __str__ ) ) ; result result = run unguarded ( caller clazz @$ __str__ @$ new object [ ] { new int [ ] { __num__ } } ) ; assert equals ( __num__ @$ ( ( int [ ] ) result . return value ) [ __num__ ] ) ;,that define method
now we do the update of the hiding toggles .,this . column hider . hiding column = false ; this . column hider . update toggles order ( ) ; refresh header ( ) ; this . header . update col spans ( ) ; this . footer . update col spans ( ) ;,we do update
async put all 2 from client 1,async invocation < void > put all2 in client1 = client1 . invoke async ( ( ) -> { do put all ( get cache ( ) . get region ( region name ) @$ __str__ @$ one_hundred ) ; } ) ; put all1 in client1 . await ( ) ; put all2 in client1 . await ( ) ;,async put 2
verify we can poll the last buffer,acking container cont4 = s . poll ( ) . get ( ) ; cont4 . update start time ( system . current time millis ( ) ) ; assert equals ( __num__ @$ cont4 . m_last seq no ) ; cont4 . discard ( ) ; s . shutdown ( ) ;,we poll buffer
special case : if both constants are not equal then return 0,if ( is const ) { if ( prev const != null && ! prev const . equals ( ( ( expr node constant desc ) leaf ) . get value ( ) ) ) { return __num__ ; } return num rows ; },case return 0
we must insert zeros between point and int val,buf = new string builder ( __num__ - insertion point + int string . length ( ) ) ; buf . append ( signum < __num__ ? __str__ : __str__ ) ; for ( int i = __num__ ; i < - insertion point ; i ++ ) buf . append ( __str__ ) ; buf . append ( int string ) ;,we insert zeros
check if we selected victim correctly .,if ( f . eq ( data cache ( victim ) . affinity ( ) . map key to node ( data_key ) . id ( ) @$ attacker . cluster ( ) . local node ( ) . id ( ) ) ) { ignite tmp = victim ; victim = attacker ; attacker = tmp ; },we selected victim
make sure we have two t ms running in either mode,long start time = system . nano time ( ) ; while ( system . nano time ( ) - start time < time unit . nanoseconds . convert ( __num__ @$ time unit . seconds ) && ! ( verify strings in named log files ( new string [ ] { __str__ } @$ __str__ ) ) ) { log . info ( __str__ ) ; sleep ( __num__ ) ; } log . info ( __str__ ) ;,we have ms
we generate a state to verify the content of the builder,parse query . state state = query . get builder ( ) . build ( ) ; assert equals ( __num__ @$ state . order ( ) . size ( ) ) ; assert true ( state . order ( ) . contains ( string . format ( __str__ @$ __str__ ) ) ) ;,we generate state
generate the link rule that builds the test binary .,cxx link and compile rules cxx link and compile rules = cxx description enhancer . create build rules for cxx binary description arg ( context . get target graph ( ) @$ build target @$ project filesystem @$ graph builder @$ cell roots @$ cxx buck config @$ cxx platform @$ args @$ get implicit framework deps ( build target . get target configuration ( ) @$ args ) @$ flavored strip style @$ flavored linker map mode ) ;,that builds test
it has children @$ check to see which one is hit .,if ( num children > __num__ ) { final point p2 = ax component . get location on screen ( ) ; final point localp2 = new point ( ( int ) ( hit pointx - p2 . getx ( ) ) @$ ( int ) ( hit pointy - p2 . gety ( ) ) ) ; return c accessible . getc accessible ( ax component . get accessible at ( localp2 ) ) ; },it has children
if we hit a double colon add the appropriate hex strings,if ( double colon index != - __num__ ) { int number to insert = hex strings length - hex strings . size ( ) ; for ( int i = __num__ ; i < number to insert ; i ++ ) { hex strings . add ( double colon index @$ __str__ ) ; } } byte [ ] ip byte array = new byte [ __num__ ] ;,colon add strings
counter reached the target for two candidates .,signal . get counts ( ) . put ( default_count_name @$ __num__ ) ; release candidate . accept ( __num__ @$ __num__ ) ; assert equals ( __num__ @$ released . size ( ) ) ; assert equals ( __num__ @$ waiting . size ( ) ) ; assert equals ( __num__ @$ signal . get count ( default_count_name ) ) ;,counter reached target
oci let 's see if we have an database name,if ( database name != null && database name . length ( ) > __num__ ) { if ( hostname != null && hostname . length ( ) > __num__ && port != null && port . length ( ) > __num__ ) { return __str__ + hostname + __str__ + port + __str__ + database name + __str__ ; } else { return __str__ + database name ; } } else { throw new kettle database exception ( __str__ ) ; },we have name
note that this class does not support forward mode .,if ( ! forward ) { if ( trusted pub key != null ) { prev pub key = trusted pub key ; } else { prev pub key = null ; } } else { throw new cert path validator exception ( __str__ ) ; },class support mode
ensure we can still load the package,web view provider response response = m web view update service impl . wait for and get provider ( ) ; assert equals ( web view factory . libload_success @$ response . status ) ; assert equals ( primary package @$ response . package info . package name ) ; m test system impl . set package info ( create package info ( primary package @$ true @$ true @$ true @$ null @$ __num__ ) ) ;,we load package
media metadata supports all the same fields as metadata editor,if ( m metadata builder != null ) { string metadata key = media metadata . get key from metadata editor key ( key ) ; if ( metadata key != null ) { m metadata builder . put long ( metadata key @$ value ) ; } },metadata supports fields
check that number of fields matches the number of subtypes,if ( ! lenient ) { class < ? > original type as class = null ; if ( is class type ( original type ) ) { original type as class = type to class ( original type ) ; } check not null ( original type as class @$ __str__ ) ; int field count = count fields in class ( original type as class ) ; if ( field count > sub types info . length ) { return null ; } },number matches number
ensure that a 7 th node heartbeat does not allocate more containers,scheduler . handle ( node event ) ; drain events onrm ( ) ; assert equals ( __num__ @$ scheduler . get root queue metrics ( ) . get allocated containers ( ) ) ; scheduler . reinitialize ( conf @$ resource manager . getrm context ( ) ) ; scheduler . update ( ) ; scheduler . handle ( node event ) ; drain events onrm ( ) ;,heartbeat allocate containers
test if the two writes are ok file length,assert equals ( __str__ @$ temp . length ( ) @$ orig data bytes . length ) ; file input stream in = new file input stream ( temp ) ; byte [ ] buffer = new byte [ in . available ( ) ] ; in . read ( buffer ) ; in . close ( ) ; string content = new string ( buffer ) ;,writes file length
we 've found the initialization data .,if ( buffer data [ offset ] == __num__ && buffer data [ offset + __num__ ] == __num__ && buffer data [ offset + __num__ ] == __num__ && buffer data [ offset + __num__ ] == __num__ ) { byte [ ] initialization data = arrays . copy of range ( buffer data @$ offset @$ buffer data . length ) ; return new pair < > ( mime types . video_vc1 @$ collections . singleton list ( initialization data ) ) ; },we found data
pinot currently does not support group by with distinct,try { sql = __str__ ; calcite sql parser . compile to pinot query ( sql ) ; assert . fail ( __str__ ) ; } catch ( exception e ) { assert . assert true ( e instanceof sql compilation exception ) ; assert . assert true ( e . get message ( ) . contains ( __str__ ) ) ; },pinot support group
check the master has set the created field,final string output2 = cli ( __str__ @$ test job name and version @$ __str__ ) ; final job job = json . read ( output2 @$ job . class ) ; assert not null ( job . get created ( ) ) ;,master set field
verify the record that made it does not have an xmin,source record rec = assert record inserted ( __str__ @$ pk_field @$ __num__ ) ; assert source info ( rec @$ __str__ @$ __str__ @$ __str__ ) ; struct source = ( ( struct ) rec . value ( ) ) . get struct ( __str__ ) ; assert that ( source . get int64 ( __str__ ) ) . is greater than ( __num__ ) ; assert that ( consumer . is empty ( ) ) . is true ( ) ;,it have xmin
ask the user do they really want to do this ?,j label label = new j label ( policy tool . get message ( __str__ ) ) ; tw . add new component ( this @$ label @$ crpe_label1 @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ grid bag constraints . both @$ tool window . bottom_padding ) ;,user do this
setting volume on ui sounds stream type also controls silent mode,if ( ( ( flags & audio manager . flag_allow_ringer_modes ) != __num__ ) || ( stream == get ui sounds stream type ( ) ) ) { set ringer mode ( get new ringer mode ( stream @$ index @$ flags ) @$ tag + __str__ @$ false ) ; },type controls mode
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
invalidated the one required edits journal .,invalidate edits dir at index ( __num__ @$ false @$ false ) ; journal and stream non required jas = get journal and stream ( __num__ ) ; edit log file output stream non required spy = spy on stream ( non required jas ) ;,one required journal
second upload of same data should yield a different blob key,blob key key1a2 = put ( cache @$ job id1 @$ new byte array input stream ( data ) @$ transient_blob ) ; assert not null ( key1a2 ) ; verify key different hash equals ( key1a @$ key1a2 ) ; transient blob key key1b = ( transient blob key ) put ( cache @$ job id1 @$ new byte array input stream ( data2 ) @$ transient_blob ) ; assert not null ( key1b ) ;,upload yield key
we 'll compose all of the info into a string,string writer s out = new string writer ( ) ; print writer out = new print writer ( s out ) ; t . print stack trace ( out ) ; out . close ( ) ; this . error = s out . to string ( ) ;,we compose all
we then inspect the mask tree and apply an arbitrary projection,final greeting cloned greeting = custom_metadata_greeting . clone ( ) ; if ( metadata projection != null && metadata projection . get operations ( ) . size ( ) == __num__ && metadata projection . get operations ( ) . get ( greeting . fields ( ) . message ( ) ) == mask operation . positive_mask_op ) { cloned greeting . remove tone ( ) ; },we inspect tree
now connect the socket and validate that we get the right port,the socket . connect ( inet address . get local host ( ) @$ port number ) ; assert true ( __str__ + the socket . get port ( ) + __str__ + port number @$ the socket . get port ( ) == port number ) ;,we get port
array list matches all three object type nodes,final default fact handle h1 = new default fact handle ( __num__ @$ new array list ( ) ) ; rete . assert object ( h1 @$ pctx factory . create propagation context ( __num__ @$ propagation context . type . insertion @$ null @$ null @$ null ) @$ ksession ) ;,list matches nodes
app 1 has 6 containers @$ and app 2 has 2 containers,assert . assert equals ( __num__ @$ scheduler app1 . get live containers ( ) . size ( ) ) ; assert . assert equals ( __num__ @$ scheduler app2 . get live containers ( ) . size ( ) ) ;,app has containers
remove all elements . should set array attribute to null,init ( ) ; struct map . clear ( ) ; complex entity . set attribute ( __str__ @$ struct map ) ; response = entity store . create or update ( new atlas entity stream ( complex entities info ) @$ false ) ; updated complex entity = response . get first updated entity by type name ( entity_type_with_complex_collection_attr ) ; validate entity ( complex entities info @$ get entity from store ( updated complex entity ) ) ;,elements set attribute
client 1 and 2 fetch the key,c1 = cache . fetch ( key ) ; c2 = cache . fetch ( key ) ; assert equals ( __num__ @$ c1 . get revision ( ) ) ; assert equals ( __num__ @$ c2 . get revision ( ) ) ;,client fetch key
not a file..so let 's skip it !,if ( ftp file . is dir ( ) ) { get it = false ; if ( is debug ( ) ) { log debug ( base messages . get string ( pkg @$ __str__ @$ filename ) ) ; } },'s skip it
otherwise we 'll use the default computed value,final int inset top = last insets != null ? last insets . get system window inset top ( ) : __num__ ; final int min height = view compat . get minimum height ( this ) ; if ( min height > __num__ ) { return math . min ( ( min height * __num__ ) + inset top @$ get height ( ) ) ; },we use value
for extra safety clear the mockito repository .,mock repository . clear ( ) ; redefine classes ( test class @$ agent class register ) ; final class loader context class loader = thread . current thread ( ) . get context class loader ( ) ; new mock policy initializer impl ( test class ) . initialize ( context class loader ) ;,safety clear mockito
generate enough delta files so that initiator can trigger auto compaction,for ( int i = __num__ ; i < __num__ ; i ++ ) { run statement on driver ( __str__ + tbl name + __str__ + ( i + __num__ ) + __str__ + ( i + __num__ ) + __str__ + ( i + __num__ ) + __str__ ) ; },initiator trigger compaction
wait all volumes are reaching expected state,for ( volume vs : volumes ) { log . info ( __str__ @$ vs . get volume id ( ) . to string ( ) ) ; vs . handle ( new validate volume event ( vs ) ) ; vs . handle ( new controller publish volume event ( vs ) ) ; },volumes reaching state
we need app targets for direct targets to get displayed,list < resolved component info > resolved component infos = create resolved components for test ( __num__ ) ; when ( s overrides . resolver list controller . get resolvers for intent ( mockito . any boolean ( ) @$ mockito . any boolean ( ) @$ mockito . isa ( list . class ) ) ) . then return ( resolved component infos ) ;,we need targets
this record size is more than the max allowed size,int record size = max buffer size + extra max buffer size + __num__ ; binary input archive ia = get binary input archive ( record size @$ max buffer size @$ extra max buffer size ) ; try { ia . read string ( __str__ ) ; fail ( __str__ ) ; } catch ( io exception e ) { assert true ( __str__ + e @$ e . get message ( ) . starts with ( binary input archive . unreasonble_length ) ) ; },max allowed size
opens swapped realm in order to load column index .,realm . get instance ( config for swapped ) . close ( ) ; realm . execute transaction ( new realm . transaction ( ) { @ override public void execute ( realm realm ) { final string and int obj = realm object tests . this . realm . create object ( string and int . class ) ; obj . set str ( __str__ ) ; obj . get str ( ) ; } } ) ;,opens swapped realm
ensure end offset points to end of chunk .,if ( length >= __num__ ) { long tmp len = start offset + length ; if ( tmp len % chunk size != __num__ ) { tmp len += ( chunk size - tmp len % chunk size ) ; } if ( tmp len < end ) { end = tmp len ; } else if ( chunk checksum != null ) { this . last chunk checksum = chunk checksum ; } },end offset points
wait auto hide play controls done :,polling check . wait for ( __num__ @$ new polling check . polling check condition ( ) { @ override public boolean can proceed ( ) { return ( ( playback support fragment ) details fragment . m video support fragment ) . m bg alpha == __num__ ; } } ) ;,auto hide controls
we already have a buffer that 's big enough,if ( peer app data . capacity ( ) > bytes ) { if ( peer app data . capacity ( ) - peer app data . position ( ) < bytes ) { peer app data . compact ( ) ; peer app data . flip ( ) ; } },we have buffer
restore with a v 2 serializer that has a different schema,test keyed map state upgrade ( new map state descriptor < > ( state name @$ int serializer . instance @$ new test type . v1 test type serializer ( ) ) @$ new map state descriptor < > ( state name @$ int serializer . instance @$ new test type . reconfiguration requiring test type serializer ( ) ) ) ;,that has schema
ensure a timer throws an null pointer exception if date is null,t = new timer ( ) ; test task = new timer test task ( ) ; try { t . schedule at fixed rate ( test task @$ null @$ __num__ ) ; fail ( __str__ ) ; } catch ( null pointer exception expected ) { } t . cancel ( ) ;,timer throws exception
checks if the solution is actually feasible provided our gold .,double cost = ( health * __num__ ) + ( armor * __num__ ) ; if ( cost > __num__ ) { return __num__ - cost ; } else { long fitness = ( health * ( __num__ + armor ) ) / __num__ ; return fitness ; },checks provided gold
flush metric snapshots when time is aligned @$ check every sec .,this . flush metric thread cycle = __num__ ; log . info ( __str__ @$ check meta thread cycle @$ flush metric thread cycle ) ; this . local mode = storm config . local_mode ( conf ) ; this . cluster name = config extension . get cluster name ( conf ) ; refreshable components . register refreshable ( this ) ; log . info ( __str__ ) ;,snapshots check sec
if key already exists while creating the blob else update it,iterator < string > key iterator = blob store . list keys ( ) ; while ( key iterator . has next ( ) ) { if ( key iterator . next ( ) . equals ( key ) ) { log . debug ( __str__ @$ key ) ; is success = true ; break ; } },blob update it
unknown backend should get default config of registry,retry retry3 = retry registry . retry ( __str__ ) ; assert that ( retry3 ) . is not null ( ) ; assert that ( retry3 . get retry config ( ) . get max attempts ( ) ) . is equal to ( __num__ ) ; assert that ( event consumer registry . get all event consumer ( ) ) . has size ( __num__ ) ;,backend get config
and now we can safely remove them from the map .,synchronized ( this ) { for ( int i = __num__ ; i < n ; i ++ ) { pending temp whitelist ptw = list [ i ] ; int index = m pending temp whitelist . index of key ( ptw . target uid ) ; if ( index >= __num__ && m pending temp whitelist . value at ( index ) == ptw ) { m pending temp whitelist . remove at ( index ) ; } } },we remove them
check if we at least found the correct language :,if ( ! is default && ! res bund locale . equals ( loc ) ) { if ( res bund locale . get language ( ) . equals ( loc . get language ( ) ) ) { log . info ( __str__ @$ loc @$ res bund locale ) ; } else { log . error ( __str__ @$ loc ) ; } },we found language
we assign the same tags as the open nlp chunker,if ( regex . phrase type == np ) { if ( i == match . start index ( ) ) { new tag = new chunk tag ( __str__ ) ; } else { new tag = new chunk tag ( __str__ ) ; } } else { new tag = new chunk tag ( regex . phrase type . name ( ) ) ; },we assign tags
make sure the file has the correct extension,if ( ( selected file != null ) && ! selected file . get name ( ) . ends with ( preferences_file_extension ) ) { selected file = new file ( selected file . get absolute path ( ) + preferences_file_extension ) ; },file has extension
update thumbnails container according to video status .,if ( thumbnail container != null ) { if ( conference member != null ) thumbnail container . update thumbnail ( conference member @$ ( video != null ) ) ; else if ( call peer != null ) thumbnail container . update thumbnail ( call peer @$ ( video != null ) ) ; else if ( is local user ) thumbnail container . update thumbnail ( ( video != null ) ) ; },update thumbnails container
difficult case @$ cfg changed the serial config has changed,if ( cfg . serial differs ( new cfg ) ) { logger . info ( __str__ ) ; stop ( ) ; startup ( new cfg ) ; return ; } if ( cfg . network differs ( new cfg ) ) { setup modem ( new cfg ) ; },case changed config
check that admin can get task variables,security util . log in as ( __str__ ) ; assert that ( task admin runtime . variables ( task payload builder . variables ( ) . with task id ( task1 . get id ( ) ) . build ( ) ) ) . extracting ( variable instance :: get name @$ variable instance :: get value ) . contains exactly ( tuple ( __str__ @$ __str__ ) @$ tuple ( __str__ @$ __str__ ) ) ;,admin get variables
if load class fails @$ we should propagate the exception .,throw e ; inflate exception ie = new inflate exception ( attrs . get position description ( ) + __str__ + constructor . get class ( ) . get name ( ) ) ; ie . init cause ( e ) ; throw ie ;,we propagate exception
this algorithm has substantial complexity for large argument streams !,seq buffer < ? extends u > buffer = seq buffer . of ( other ) ; return flat map ( t -> buffer . seq ( ) . filter ( u -> predicate . test ( t @$ u ) ) . on empty ( null ) . map ( u -> tuple . < t @$ u > tuple ( t @$ u ) ) ) . on close ( other :: close ) ;,algorithm has complexity
if two clients @$ then they get half the rows each .,opts . set num client threads ( __num__ ) ; opts = performance evaluation . calculate rows and size ( opts ) ; assert equals ( default per client run rows @$ opts . get per client run rows ( ) ) ;,they get rows
normal purge does n't change stats .,execute purge ( get destination name ( ) ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ; assert equals ( __num__ @$ queue view . get enqueue count ( ) ) ; assert equals ( __num__ @$ queue view . get dequeue count ( ) ) ;,purge change stats
second call must not log another message,assert that ( under test . is operational ( ) ) . is false ( ) ; assert that ( memory appender . events ) . extracting ( i logging event :: get level @$ i logging event :: get message ) . contains only once ( tuple ( level . info @$ __str__ ) ) ;,call log message
check pipeline runner correctly catches sdk errors .,try { p . run ( ) ; fail ( __str__ ) ; } catch ( runtime exception exn ) { assert that ( exn @$ not ( instance of ( user code exception . class ) ) ) ; assert that ( exn . get message ( ) @$ contains string ( __str__ ) ) ; assert that ( exn @$ instance of ( illegal state exception . class ) ) ; },runner catches errors
check the media period information matches the new timeline .,if ( ! can keep media period holder ( period holder @$ period info ) ) { return ! remove after ( previous period holder ) ; } else if ( ! can keep after media period holder ( period holder @$ period info . duration us ) ) { return ! remove after ( period holder ) ; },information matches timeline
catch errors and rethrow as conversion exception so we get location details,try { wrap . test plan = ( hash tree ) context . convert another ( wrap @$ get next type ( reader ) ) ; } catch ( no class def found error | exception e ) { throw create conversion exception ( e ) ; } return wrap ;,we get details
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
if wo n't fit @$ use array list version,array list < e > al = new array list < e > ( ) ; for ( node < e > q = first ( ) ; q != null ; q = succ ( q ) ) { e item = q . item ; if ( item != null ) al . add ( item ) ; } return al . to array ( a ) ;,array list version
do n't let the new entry get g ced,warmed . add ( entry of ( entry . get key ( ) @$ new value ) ) ; object new key = new object ( ) ; assert null ( cache . as map ( ) . put ( new key @$ entry . get value ( ) ) ) ;,entry get ced
we expect native functions to be defined externally .,if ( ! modifier . is native ( function . get modifiers ( ) ) ) { print ( __str__ ) ; } print ( get function signature ( function @$ true ) ) ; if ( function . returns retained ( ) ) { print ( __str__ ) ; } println ( __str__ ) ;,we expect functions
required features expect activity launch options in,verify ( m mock account manager response ) . on error ( eq ( account manager . error_code_management_disabled_for_account_type ) @$ any string ( ) ) ; verify ( m mock context ) . start activity as user ( m intent captor . capture ( ) @$ eq ( user handle . system ) ) ;,features expect options
for now we only support one datasource but this will change,instance handle < data source > data source handle = arc . container ( ) . instance ( data source . class ) ; if ( ! data source handle . is available ( ) ) { throw new illegal state exception ( __str__ + persistence unit name ) ; } runtime settings builder . put ( available settings . datasource @$ arc . container ( ) . instance ( data source . class ) . get ( ) ) ;,we support datasource
the activity manager can not resolve activities that have been removed,try { if ( atm . get activity class for token ( sessions to remove . value at ( i ) ) != null ) { sessions to remove . remove at ( i ) ; i -- ; num sessions to remove -- ; } } catch ( remote exception e ) { slog . w ( tag @$ __str__ @$ e ) ; },manager resolve activities
this takes care of setting up coord streams for spouts and bolts,for ( global stream id s : _batch ids . key set ( ) ) { string b = _batch ids . get ( s ) ; ret . put ( new global stream id ( s . get_component id ( ) @$ trident bolt executor . coord_stream ( b ) ) @$ b ) ; } return ret ;,this takes care
last token already finished line ; reset silently,if ( m line finished ) { m line finished = false ; return ; } int i = __num__ ; do { for ( ; i < m tail ; i ++ ) { if ( m buffer [ i ] == __str__ ) { consume buf ( i + __num__ ) ; return ; } } } while ( fill buf ( ) > __num__ ) ;,token finished line
verify we got a row back .,assert true ( location row id != - __num__ ) ; log . d ( log_tag @$ __str__ + location row id ) ; content values updated values = new content values ( values ) ; updated values . put ( location entry . _id @$ location row id ) ; updated values . put ( location entry . column_city_name @$ __str__ ) ;,we got row
see if we find a list of partitions,list < string > partitions = schema partitions map . get ( partition schema ) ; if ( partitions == null ) { partitions = new array list < string > ( ) ; schema partitions map . put ( partition schema @$ partitions ) ; },we find list
the key was removed so we should delete it too .,if ( ! store . is remote blob exists ( resource . get key ( ) ) ) { map < string @$ ? extends locally cached blob > set = rsrc . get value ( ) ; if ( remove blob ( resource @$ set ) ) { bytes over -= resource . get size on disk ( ) ; log . info ( __str__ @$ resource . get key ( ) ) ; i . remove ( ) ; } },we delete it
draw the status bar foreground drawable if we have a top inset,if ( status bar foreground != null && get top inset ( ) > __num__ ) { int save count = canvas . save ( ) ; canvas . translate ( __num__ @$ - current offset ) ; status bar foreground . draw ( canvas ) ; canvas . restore to count ( save count ) ; },foreground have inset
make sure we can get an interator over multiple address spaces,code unit iterator iter = program . get listing ( ) . get code units ( true ) ; int cnt = __num__ ; while ( iter . has next ( ) ) { iter . next ( ) ; ++ cnt ; } assert equals ( __num__ @$ cnt ) ;,we get interator
metastore schema only allows maximum 255 for constraint name column,if ( constraint name != null && constraint name . length ( ) > constraint_max_length ) { throw new semantic exception ( error msg . invalid_cstr_syntax . get msg ( __str__ + constraint name + __str__ + constraint_max_length ) ) ; },schema allows 255
silently fails and uses the default instance .,lazy field lite field = new lazy field lite ( test util . get extension registry ( ) @$ byte string . copy from utf8 ( __str__ ) ) ; assert equals ( test all types . get default instance ( ) @$ field . get value ( test all types . get default instance ( ) ) ) ; assert equals ( __num__ @$ field . get serialized size ( ) ) ; assert equals ( byte string . empty @$ field . to byte string ( ) ) ;,silently fails instance
deleting this edge would violate the attribute 's lower bound .,if ( ! attr def . get is optional ( ) && element edges . size ( ) <= attr def . get values min count ( ) ) { throw new atlas base exception ( __str__ + property name + __str__ + graph helper . get vertex details ( out vertex ) + __str__ + graph helper . get edge details ( element edge ) ) ; },edge violate attribute
assert that pipeline 0 will have no more drivers,wait until equals ( scan operator factory0 :: is overall no more operators @$ true @$ assert_wait_timeout ) ; wait until equals ( join operator factorya :: is overall no more operators @$ true @$ assert_wait_timeout ) ; wait until equals ( join operator factoryc :: is overall no more operators @$ true @$ assert_wait_timeout ) ;,pipeline have drivers
assuming the list order always follows the parsed html,assert false ( __str__ @$ attributes . get ( __num__ ) instanceof boolean attribute ) ; assert true ( __str__ @$ attributes . get ( __num__ ) instanceof boolean attribute ) ; assert false ( __str__ @$ attributes . get ( __num__ ) instanceof boolean attribute ) ; assert equals ( html @$ el . outer html ( ) ) ;,order follows html
this defaults the rate to 0 if no more data points exists,double new value = last value ; if ( has next internal ( ) ) { current data point = next internal ( ) ; new value = current data point . get double value ( ) ; } double diff = new value - last value ; return ( m_data point factory . create data point ( current data point . get timestamp ( ) @$ diff ) ) ;,this defaults rate
create a simple rule which just writes a file .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; rule key input rule key = new rule key ( __str__ ) ; build rule rule = new failing input rule key build rule ( target @$ filesystem @$ params ) ; graph builder . add to index ( rule ) ;,which writes file
this will trigger class loading and parsing of the supplied file .,try { zone rules provider . get available zone ids ( ) ; fail ( ) ; } catch ( exception in initializer error e ) { assert that ( e ) . has cause that ( ) . is instance of ( illegal state exception . class ) ; assert that ( e ) . has cause that ( ) . has message that ( ) . is equal to ( __str__ ) ; },this trigger loading
user a can not impersonate user c @$ it fails .,for ( string servlet : new string [ ] { __str__ @$ __str__ @$ __str__ } ) { httpurl connection conn = auth url . open connection ( new url ( serverurl + servlet + __str__ ) @$ token ) ; assert . assert equals ( httpurl connection . http_forbidden @$ conn . get response code ( ) ) ; },a impersonate c
now increment the iterator that gave us the best result,for ( peekable iterator < t > iterator : iterators ) { if ( ! iterator . has next ( ) ) { continue ; } t t = iterator . peek ( ) ; int result = comparator . compare ( lowest @$ t ) ; if ( result == __num__ ) { iterator . next ( ) ; return lowest ; } } throw new assert exception ( __str__ ) ;,that gave result
we send diff and forward any packet starting last processed zxid,assert op type ( leader . diff @$ db . last processed zxid @$ db . last processed zxid ) ; assert equals ( __num__ @$ learner handler . get queued packets ( ) . size ( ) ) ; reset ( ) ;,diff starting zxid
top left bottom left bottom right top right,back bottom card . set card vertices ( new float [ ] { __num__ @$ back bitmap . get height ( ) / __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ back bitmap . get width ( ) @$ __num__ @$ __num__ @$ back bitmap . get width ( ) @$ back bitmap . get height ( ) / __num__ @$ __num__ } ) ;,bottom left right
the stp loop that zeros 16 bytes in each iteration .,masm . align ( crb . target . word size * __num__ ) ; masm . bind ( main loop ) ; masm . stp ( __num__ @$ zr @$ zr @$ a arch64 address . create post indexed immediate address ( base @$ __num__ ) ) ; masm . bind ( main check ) ; masm . subs ( __num__ @$ size @$ size @$ __num__ ) ; masm . branch conditionally ( a arch64 assembler . condition flag . ge @$ main loop ) ;,stp loop bytes
row objects only make sense for body section of grid .,t row object ; if ( section == section . body && row >= __num__ && row < grid . get data source ( ) . size ( ) ) { row object = grid . get data source ( ) . get row ( row ) ; } else { row object = null ; },objects make sense
first case : we complete the inner usertask .,task service . complete ( user task . get id ( ) ) ; user task = task service . create task query ( ) . single result ( ) ; assert not null ( user task ) ; assert equals ( __str__ @$ user task . get task definition key ( ) ) ;,we complete usertask
we expect categorical values to be already encoded,easy predict model wrapper . config cfg = new easy predict model wrapper . config ( ) . set use external encoding ( true ) . set convert invalid numbers to na ( true ) . set convert unknown categorical levels to na ( true ) . set model ( mojo mdl ) ; easy predict model wrapper easy predict = new easy predict model wrapper ( cfg ) ; return new h2o mojo model ( easy predict ) ;,we expect values
we already handled rooted paths with empty relative part .,preconditions . check state ( ! strings . is null or empty ( expected ) @$ arg . get path ( ) ) ; dirent child = parent list . get dirents ( ) . maybe get dirent ( expected ) ; return ( child != null && expected . equals ( child . get name ( ) ) ) ? path casing lookup value . good : path casing lookup value . bad ;,we handled paths
similarly @$ experiment flag can override the whitelist .,string whitelist = override whitelist != null ? override whitelist : default whitelist ; if ( __str__ . equals ( whitelist ) ) { return true ; } for ( string oob pkg name : whitelist . split ( __str__ ) ) { if ( package names in same process . contains ( oob pkg name ) ) { return true ; } } return false ;,flag override whitelist
get the join keys from parent reduce sink operators,for ( operator < ? extends operator desc > parent op : parent ops ) { reduce sink desc rsconf = ( ( reduce sink operator ) parent op ) . get conf ( ) ; byte tag = ( byte ) rsconf . get tag ( ) ; list < expr node desc > keys = rsconf . get key cols ( ) ; key expr map . put ( tag @$ keys ) ; } context . set key expr map ( key expr map ) ;,keys reduce operators
we have new content : change ci connection :,w connection . add modify listener ( new modify listener ( ) { public void modify text ( modify event e ) { ci = trans meta . find database ( w connection . get text ( ) ) ; set flags ( ) ; } } ) ;,we have content
clean configuration file when num exceed max version,try ( fs data output stream output stream = file system . create ( temp scheduler config path ) ) { clean configuration file ( ) ; v sched conf . write xml ( output stream ) ; log . info ( __str__ + temp scheduler config path ) ; } catch ( io exception e ) { log . info ( __str__ + temp scheduler config path @$ e ) ; throw e ; },num exceed version
always cleanup any created deployments @$ even if the test failed,list < dmn deployment > deployments = dmn repository service . create deployment query ( ) . list ( ) ; for ( dmn deployment deployment : deployments ) { dmn repository service . delete deployment ( deployment . get id ( ) ) ; },any created deployments
member 1 has a lower weight @$ and all of the primaries,partition member info impl details1 = build details ( member1 @$ __num__ @$ __num__ @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) ;,member has weight
another consumer beat us to it failed the cas,final long offset = calc element offset ( c index @$ mask ) ; final e e = lp element ( buffer @$ offset ) ; so element ( buffer @$ offset @$ null ) ; so sequence ( s buffer @$ seq offset @$ c index + mask + __num__ ) ;,consumer beat us
the day is invalid @$ kill the node .,if ( ! has bounds ) { m temp rect . set empty ( ) ; node . set content description ( __str__ ) ; node . set bounds in parent ( m temp rect ) ; node . set visible to user ( false ) ; return ; },day kill node
check if we can delete the index safely .,if ( latest timestamp of first index <= earliest event time ) { final file indexing directory = index dirs . get ( __num__ ) ; get index manager ( ) . remove index ( indexing directory ) ; index config . remove index directory ( indexing directory ) ; delete directory ( indexing directory ) ; if ( max event id > - __num__ ) { index config . set min id indexed ( max event id + __num__ ) ; } },we delete index
for colocated regions you ca n't locally destroy a partitioned region .,if ( root . is destroyed ( ) || root instanceof ha region || root instanceof partitioned region ) { continue ; } try { root . local destroy region ( __str__ ) ; } catch ( throwable t ) { logger . error ( __str__ + region full path @$ t ) ; },you destroy region
external table should also check the underlying file size .,if ( status == status . pass && meta store utils . is external table ( table . gett table ( ) ) ) { log . warn ( __str__ @$ table . get complete name ( ) ) ; status = status . unavailable ; },table check size
something into edit text . enable the button .,rename dialog . get button ( alert dialog . button_positive ) . set enabled ( true ) ; alert dialogs helper . set button text color ( new int [ ] { dialog interface . button_positive } @$ get accent color ( ) @$ rename dialog ) ;,something enable button
'otheritems ' represents the hash code for the previous versions .,int otheritems = ( lenient ? __num__ : __num__ ) | ( first day of week << __num__ ) | ( minimal days in first week << __num__ ) | ( zone . hash code ( ) << __num__ ) ; long t = get millis of ( this ) ; return ( int ) t ^ ( int ) ( t > > __num__ ) ^ otheritems ;,'otheritems represents code
we did n't find one cache so setup,if ( _s == null ) { fallback semaphore per circuit . put if absent ( command key . name ( ) @$ new tryable semaphore actual ( properties . fallback isolation semaphore max concurrent requests ( ) ) ) ; return fallback semaphore per circuit . get ( command key . name ( ) ) ; } else { return _s ; },we find setup
if we start foo then foobar requests will be handled by foo,foo . start ( ) ; assert that ( connector . get response ( __str__ ) @$ matchers . contains string ( __str__ ) ) ; assert that ( connector . get response ( __str__ ) @$ matchers . contains string ( __str__ ) ) ; assert that ( connector . get response ( __str__ ) @$ matchers . contains string ( __str__ ) ) ;,we start foo
ns 0 is slow and renew lease should take a long time,long t0 = monotonic now ( ) ; router protocol . renew lease ( __str__ ) ; long t = monotonic now ( ) - t0 ; assert true ( __str__ + t + __str__ @$ t > time unit . seconds . to millis ( __num__ ) ) ;,lease take time
tolerate a schema that omits the trailing space,if ( trailing space < __num__ ) { tag name = string . substring ( pos [ __num__ ] @$ string . length ( ) - __num__ ) ; } else { tag name = string . substring ( pos [ __num__ ] @$ trailing space ) ; } values = read tag ( tag name @$ string @$ pos ) ;,that omits space
verify rows are what they should be,grid cell element cell = grid . get cell ( __num__ @$ __num__ ) ; string text before = cell . get text ( ) ; cell . click ( ) ; assert equals ( __str__ @$ text before @$ cell . get text ( ) ) ;,they be what
method name can not use reserved keyword @$ e.g . return,if ( is reserved word ( operation id ) ) { string new operation id = underscore ( __str__ + operation id ) ; logger . warn ( operation id + __str__ + new operation id ) ; return new operation id ; },name use keyword
... then fetch the uvs for the specified uv set name ...,list < vector2f > uvs for name = uvs for material . get ( uv set name ) ; if ( uvs for name == null ) { uvs for name = new array list < vector2f > ( ) ; uvs for material . put ( uv set name @$ uvs for name ) ; },then fetch uvs
for whitespaces @$ we skip all following ws,for ( ; i < len - __num__ ; i ++ ) { ch = sb . char at ( i + __num__ ) ; if ( ch != __num__ && ch != __num__ && ch != __num__ && ch != __num__ ) break ; },we skip ws
ensure disconnect does not reset blackout period if already disconnected,time . sleep ( reconnect backoff max ms test ) ; assert true ( client . can connect ( node @$ time . milliseconds ( ) ) ) ; client . disconnect ( node . id string ( ) ) ; assert true ( client . can connect ( node @$ time . milliseconds ( ) ) ) ;,disconnect reset period
return a copy so the caller ca n't change the fields .,if ( fields . length > __num__ ) { object stream field [ ] dup = new object stream field [ fields . length ] ; system . arraycopy ( fields @$ __num__ @$ dup @$ __num__ @$ fields . length ) ; return dup ; } else { return fields ; },caller change fields
security off @$ so default should just return simple name,test constructor success ( __str__ @$ __str__ ) ; test constructor success ( __str__ @$ __str__ ) ; test constructor success ( __str__ @$ __str__ ) ; test constructor success ( __str__ @$ __str__ ) ; test constructor success ( __str__ @$ __str__ ) ;,default return name
else we could have read anything .,assert equals ( ( integer ) i @$ actual ) ; assert true ( __str__ + e @$ safe ) ; assert true ( __str__ + __str__ + e + __str__ + i + __str__ @$ cache . lost partitions ( ) . contains ( i ) ) ;,we read anything
single dot the end matches this package and all its subpackages .,if ( p . ends with ( __str__ ) ) { string patprefix = p . substring ( __num__ @$ p . length ( ) - __num__ ) ; if ( path . starts with ( patprefix ) ) { if ( path . length ( ) == patprefix . length ( ) || path . char at ( patprefix . length ( ) ) == __str__ ) { return true ; } } },end matches package
this can select remote @$ but latency aware policy will prefer local,builder . with load balancing policy ( new token aware policy ( new latency aware policy . builder ( cassandra . local dc != null ? dc aware round robin policy . builder ( ) . with local dc ( cassandra . local dc ) . build ( ) : new round robin policy ( ) ) . build ( ) ) ) ;,policy prefer local
test that i can send another request on the same connection .,request = client . new request ( host @$ port ) ; listener = new future response listener ( request ) ; connection . send ( request @$ listener ) ; consume ( input @$ false ) ; http response = __str__ + __str__ + __str__ ; output . write ( http response . get bytes ( standard charsets . utf_8 ) ) ; output . flush ( ) ; listener . get ( __num__ @$ time unit . seconds ) ;,i send request
we 've found the 'fmt ' chunk,if ( fmt == wave file format . fmt_magic ) { break ; } else { length = rllong ( dis ) ; nread += __num__ ; if ( length % __num__ > __num__ ) length ++ ; nread += dis . skip bytes ( length ) ; },we found chunk
now lets check that the consumer has received some messages,list < message > messages = consumer . flush messages ( ) ; log . info ( __str__ ) ; for ( iterator < message > iter = messages . iterator ( ) ; iter . has next ( ) ; ) { object message = iter . next ( ) ; log . info ( __str__ + message ) ; } assert equals ( __str__ @$ producer . get message count ( ) @$ messages . size ( ) ) ;,consumer received messages
does the output stream have delayed visibility,switch ( capability . to lower case ( locale . english ) ) { case commit constants . stream_capability_magic_output : case commit constants . stream_capability_magic_output_old : return ! put tracker . output immediately visible ( ) ; case stream capabilities . hflush : case stream capabilities . hsync : return false ; default : return false ; },stream delayed visibility
set the last time we have seen this link to now,if ( inlinks . size ( ) > __num__ ) { datum . get meta data ( ) . put ( orphan_key_writable @$ new int writable ( now ) ) ; } else { orphaned score ( url @$ datum ) ; },we seen link
setting property setting partition attributes to partition attributes factory,int lmax ; try { lmax = integer . parse int ( local max memory ) ; } catch ( number format exception nfe ) { throw new illegal argument exception ( __str__ + local max memory + __str__ ) ; } pa = paf . set local max memory ( lmax ) . set redundant copies ( redundancy ) . create ( ) ;,property attributes factory
then views should have intermediate alpha value .,assert that ( m view fade out . get alpha ( ) ) . is greater than ( __num__ ) ; assert that ( m view fade out . get alpha ( ) ) . is less than ( __num__ ) ; assert that ( m view fade out . get visibility ( ) ) . is equal to ( view . visible ) ;,views have value
compute the total number of map & reduce tasks submitted,int total map tasks = cluster stats . get submitted map tasks ( ) ; int total reduce tasks = cluster stats . get submitted reduce tasks ( ) ; if ( log . is debug enabled ( ) ) { log . debug ( __str__ + total map tasks ) ; log . debug ( __str__ + total reduce tasks ) ; log . debug ( __str__ + max map load ) ; log . debug ( __str__ + max reduce load ) ; },& reduce tasks
need i pv 4 address to identify interface,inet4 address target = net . any inet4 address ( interf ) ; if ( target == null ) throw new io exception ( __str__ ) ; int target address = net . inet4 as int ( target ) ; net . set interface4 ( fd @$ target address ) ;,i pv address
single part drains all @$ one by one,list < single partmime drain reader callback impl > single partmime reader callbacks = _current multi partmime reader callback . get single partmime reader callbacks ( ) ; assert . assert equals ( single partmime reader callbacks . size ( ) @$ __num__ ) ;,part drains all
then we try a sequence of random chunk sizes,list < integer > chunk sizes = new array list < > ( ) ; for ( int i = __num__ ; i < lines / __num__ ; i ++ ) { chunk sizes . add ( i ) ; } return chunk sizes ;,we try sequence
we hit a block jumping upwards @$ let 's destroy it !,tiled map tile layer layer = ( tiled map tile layer ) map . get layers ( ) . get ( __str__ ) ; layer . set cell ( ( int ) tile . x @$ ( int ) tile . y @$ null ) ; koala . position . y = tile . y + tile . height ;,let destroy it
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
run through messages until we reach ours .,while ( next . get callback ( ) != this ) { m manager . execute ( next ) ; m manager . recycle ( next ) ; next = m manager . next ( ) ; } m manager . execute ( next ) ;,we reach ours
avoid unnecessary schema changes as this trips up cockroach db,try { if ( schema . get name ( ) . equals ( original schema name or search path ) || ! schema . exists ( ) ) { return ; } do change current schema or search path to ( schema . get name ( ) ) ; } catch ( sql exception e ) { throw new flyway sql exception ( __str__ + schema @$ e ) ; },this trips db
another consumer beat us to it failed the cas,final int offset = calc element offset ( c index @$ mask ) ; final e e = lp element ( buffer @$ offset ) ; so element ( buffer @$ offset @$ null ) ; so sequence ( s buffer @$ seq offset @$ c index + mask + __num__ ) ; return e ;,consumer beat us
verify that the refresh does indeed show 2 p ms,final list < payment method > methods = payment method processor . refresh payment methods ( mock payment provider plugin . plugin_name @$ account @$ plugin_properties @$ call context @$ internal call context ) ; assert . assert equals ( methods . size ( ) @$ __num__ ) ; check payment method exists with status ( methods @$ existingpm id @$ true ) ; check payment method exists with status ( methods @$ new pm id @$ true ) ;,refresh show ms
we can save space by removing the log of deletions,m db . execute ( __str__ ) ; m usn += __num__ ; m models . before upload ( ) ; m tags . before upload ( ) ; m decks . before upload ( ) ; mod schema no check ( ) ; m ls = m scm ;,we save space
measure ourselves @$ this should set the measured dimension flag back,if ( cache index < __num__ || s ignore measure cache ) { on measure ( width measure spec @$ height measure spec ) ; m private flags3 &= ~ pflag3_measure_needed_before_layout ; } else { long value = m measure cache . value at ( cache index ) ; set measured dimension raw ( ( int ) ( value > > __num__ ) @$ ( int ) value ) ; m private flags3 |= pflag3_measure_needed_before_layout ; },this set flag
trying to run the operation again will throw lock failed exception,awaitility . await ( ) . at most ( __num__ * timeout @$ time unit . seconds ) . until true ( run plugin operation in background ( plugin operation @$ callback @$ true ) ) ; assert . assert equals ( callback . get run count ( ) @$ __num__ ) ;,operation throw exception
peek at the next record so we can update the min timestamp,if ( delegate . has next ( ) ) { next = delegate . next ( ) ; min timestamp = next == null ? long . max_value : next . get key ( ) . time ( ) ; } else { next = null ; min timestamp = long . max_value ; } evictions ++ ;,we update timestamp
see if we have any data at all .,if ( result . is empty ( ) ) { log . error ( __str__ + row key str + __str__ ) ; print locations ( result ) ; return false ; } if ( ! verify values && ! verify cf and column integrity ) { return true ; },we have data
ok @$ we just process the usage command instead .,if ( thrown . is empty ( ) ) { return this . usage command . process ( source @$ __str__ @$ args . next if present ( ) . map ( string :: to lower case ) . or else ( null ) ) ; } else { throw new nucleus command exception ( thrown ) ; },we process command
specify applications to be installed and configured when emr creates the cluster,application hive = new application ( ) . with name ( __str__ ) ; application spark = new application ( ) . with name ( __str__ ) ; application ganglia = new application ( ) . with name ( __str__ ) ; application zeppelin = new application ( ) . with name ( __str__ ) ;,emr creates cluster
some teritories do not have currencies @$ like antarctica,locale loc = new locale ( __str__ @$ __str__ ) ; try { currency curr = currency . get instance ( loc ) ; assert null ( __str__ @$ curr ) ; } catch ( illegal argument exception e ) { fail ( __str__ + e ) ; },teritories have currencies
second gbk forces the output to be materialized,p assert . that ( result ) . contains in any order ( kv . of ( __str__ @$ immutable list . of ( __num__ @$ __num__ @$ __num__ ) ) @$ kv . of ( __str__ @$ immutable list . of ( __num__ ) ) ) ;,gbk forces output
keep doing this until we get a legit hfile,while ( true ) { file status [ ] hf fss = fs . list status ( fam dir ) ; if ( hf fss . length == __num__ ) { continue ; } for ( file status hfs : hf fss ) { if ( ! hfs . is directory ( ) ) { return hfs . get path ( ) ; } } },we get hfile
| | ft instanceof g fx define compacted font,if ( main panel . is internal flash viewer selected ( ) ) { image panel . set timelined ( main panel . make timelined ( font tag @$ font page num ) @$ font tag . get swf ( ) @$ __num__ ) ; },fx define font
for hour table we store only data for last 3 months,ps hour . set timestamp ( __num__ @$ new timestamp ( now . minus ( graph period . three_months . number of points + __num__ @$ chrono unit . hours ) . to epoch milli ( ) ) @$ date time utils . utc_calendar ) ; minute records removed = ps minute . execute update ( ) ; hour records removed = ps hour . execute update ( ) ; connection . commit ( ) ;,we store data
test that generated policy file has included both policies,assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( class loader perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ; assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( socket perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ;,file included policies
first filtering @$ should get 5 servers,assert equals ( filtered . size ( ) @$ __num__ ) ; server s1 = filtered . get ( __num__ ) ; server s2 = filtered . get ( __num__ ) ; server s3 = filtered . get ( __num__ ) ; server s4 = filtered . get ( __num__ ) ; server s5 = filtered . get ( __num__ ) ;,first get servers
num of keys should get zero as all values are destroyed,for ( int i = __num__ ; i < __num__ ; i ++ ) { region . destroy ( integer . to string ( i ) ) ; } assert equals ( __num__ @$ key index1 stats . get num updates ( ) ) ; assert equals ( __num__ @$ key index1 stats . get number of keys ( ) ) ; qs . remove index ( key index2 ) ;,num get zero
note : spinner models require use of long values,if ( bit offset >= __num__ ) { last bit = bit offset ; if ( bit offset <= start bit ) { int start = math . min ( start bit @$ bit offset ) ; bit offset model . set value ( ( long ) start ) ; } long bit size = math . abs ( bit offset - start bit ) + __num__ ; bit size model . set value ( bit size ) ; },models require use
check sui ds @$ note all suid for enum is 0 l,if ( __num__ != class desc . get serial versionuid ( ) || __num__ != super class . get serial versionuid ( ) ) { throw new invalid class exception ( super class . get name ( ) @$ __str__ + super class + __str__ + super class ) ; } byte tc = nexttc ( ) ;,sui ds all
when the node has only one child,if ( parent == x ) { x = parent . left == null ? parent . right : parent . left ; } else if ( x . left == null ) { if ( parent . left == x ) parent . left = x . right ; else parent . right = x . right ; } else { if ( parent . left == x ) parent . left = x . left ; else parent . right = x . left ; },node has child
these two message should not match the wild card .,connection1 . send ( create message ( producer info1 @$ activemq destination . create destination ( __str__ @$ destination type ) @$ delivery mode ) ) ; connection1 . send ( create message ( producer info1 @$ activemq destination . create destination ( __str__ @$ destination type ) @$ delivery mode ) ) ;,message match card
load the inner class reference first see if it messes up usage,tester . set source file ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . create stub jar ( ) . add stub jar to classpath ( ) . set source file ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . create stub jar ( ) . add stub jar to classpath ( ) . set source file ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . test can compile ( ) ;,it messes usage
invoke the method to ensure it has a type profile,try { for ( int i = __num__ ; i < __num__ ; i ++ ) { method . invoke ( receiver @$ args ) ; } } catch ( exception e ) { throw new runtime exception ( e ) ; },it has profile
load a version of apple that does define that method,apple . load new version ( __str__ @$ retrieve rename ( __str__ @$ __str__ ) ) ; result result = run unguarded ( caller clazz @$ __str__ @$ new object [ ] { ( byte ) __num__ } ) ; assert equals ( ( byte ) __num__ @$ result . return value ) ;,that define method
not using jndi so we use the core services directly,sf = activemq client . create server locator withoutha ( new transport configuration ( invm connector factory . class . get name ( ) ) ) . create session factory ( ) ; session = sf . create session ( ) ;,we use services
these address types have meaningless address offsets,if ( address . is external address ( ) || address . is variable address ( ) || address . is register address ( ) ) { return string representation . compare to ( other location . string representation ) ; },types have offsets
this thread is doing the real work and must finish the future,if ( own lock grantor future result ) { assert . assert true ( this . lock grantor future result == lock grantor future result ref ) ; if ( get lock grantor id failed ) { lock grantor future result ref . cancel ( false ) ; } else { lock grantor future result ref . set ( the lock grantor id ) ; } this . lock grantor future result = null ; },thread doing work
update the entity @$ which will increment the version field,user transaction tx1 = session context . get user transaction ( ) ; try { tx1 . begin ( ) ; em . join transaction ( ) ; em . merge ( employee ) ; em . flush ( ) ; tx1 . commit ( ) ; return get employee notx ( employee . get id ( ) ) ; } catch ( exception e ) { throw new runtime exception ( __str__ @$ e ) ; },which increment field
other inconsistencies will produce class cast exception below .,notification listener listener = ( notification listener ) arguments [ __num__ ] ; notification filter filter = ( notification filter ) arguments [ __num__ ] ; object handback = arguments [ __num__ ] ; emitter . add notification listener ( listener @$ filter @$ handback ) ; delegate to function service ( object name @$ method name @$ null @$ signature ) ; return null ;,inconsistencies produce exception
first check if we have a default override ...,string default memory override = system . get property ( __str__ ) ; if ( default memory override != null ) { return bytes . parse ( default memory override ) ; } double ratio of free mem = __num__ ; string default memory ratio override = system . get property ( __str__ ) ; if ( default memory ratio override != null ) { ratio of free mem = double . parse double ( default memory ratio override ) ; },we have override
state has changed @$ invalidate saved value,if ( ev . get_atom ( ) == xwm . xa_wm_state . get atom ( ) ) { state changed = true ; state changed ( ev . get_time ( ) @$ saved state @$ getwm state ( ) ) ; },state changed invalidate
lets probe filetype of manifest @$ we should not detect xml,file manifest file = new file ( output + file . separator + __str__ ) ; byte [ ] magic = test utils . read header of file ( manifest file @$ __num__ ) ; assert false ( arrays . equals ( this . xml header @$ magic ) ) ;,we detect xml
adapter calls this when user taps reply icon,m comment adapter . set reply listener ( new reader comment adapter . request reply listener ( ) { @ override public void on request reply ( long comment id ) { set reply to comment id ( comment id @$ true ) ; } } ) ;,taps reply icon
look for dominating blocks that have existing nodes,if ( ! replaced ) { for ( block d : block to existing . get keys ( ) ) { if ( strictly dominates ( d @$ b ) ) { use . replace first input ( node @$ block to existing . get ( d ) ) ; break ; } } },that have nodes
create a job with test build wrappers,free style project project = rule . create free style project ( ) ; project . get build wrappers list ( ) . add ( new decorated wrapper ( ) ) ; project . get build wrappers list ( ) . add ( new echo wrapper ( ) ) ;,job build wrappers
check that we do n't receive too many buffers,if ( ++ number of buffers per channel [ boe . get ( ) . get channel index ( ) ] > number of expected buffers per channel ) { throw new illegal state exception ( __str__ + __str__ + boe . get ( ) . get channel index ( ) + __str__ ) ; },we receive buffers
server triggers exception after destroying 5 keys,region . get attributes mutator ( ) . set cache writer ( new throwing cache writer < > ( new cache listener action < > ( operation . destroy @$ creates -> { if ( creates >= __num__ ) { throw new cache writer exception ( __str__ ) ; } } ) ) ) ;,server triggers exception
if the observer was triggered @$ it would have cleared itself,vm1 . invoke ( new serializable runnable ( __str__ ) { @ override public void run ( ) { assert null ( distribution message observer . get instance ( ) ) ; region region = get cache ( ) . get region ( __str__ ) ; version tag tag = ( ( local region ) region ) . get version tag ( __num__ ) ; assert equals ( __num__ @$ tag . get entry version ( ) ) ; } } ) ;,it cleared itself
no default path @$ just expand the top level,tree path path = new tree path ( this . site tree . get path to root ( ( tree node ) this . site tree . get root ( ) ) ) ; this . get tree site ( ) . expand path ( path ) ;,path expand level
ok we have some match and need to update,if ( ! root zookeeper . equals ( last prefix ) && ! ( __str__ . equals ( last prefix ) ) ) { update count ( last prefix @$ __num__ ) ; update bytes ( last prefix @$ data == null ? __num__ : data . length ) ; },we have match
we also call the cache release to decrease the usage count,if ( ! instance . is discarded ( ) && ! to discard ) { root_logger . tracef ( __str__ @$ instance . get id ( ) ) ; instance . get component ( ) . get cache ( ) . release ( instance ) ; },we call release
check if we have any caption updates to report,if ( is new subtitle data available ( ) ) { subtitle subtitle = create subtitle ( ) ; if ( ! input buffer . is decode only ( ) ) { subtitle output buffer output buffer = available output buffers . poll first ( ) ; output buffer . set content ( input buffer . time us @$ subtitle @$ format . offset_sample_relative ) ; release input buffer ( input buffer ) ; return output buffer ; } },we have updates
check that data streamer correctly split data by nodes .,for ( int i = __num__ ; i < keys_count ; i ++ ) { if ( g . affinity ( default_cache_name ) . is primary ( n @$ i ) ) assert not null ( c . local peek ( i ) ) ; else assert null ( c . local peek ( i ) ) ; },streamer split data
play the shutter sound to notify that we 've taken a screenshot,m screenshot layout . post ( new runnable ( ) { @ override public void run ( ) { m camera sound . play ( media action sound . shutter_click ) ; m screenshot view . set layer type ( view . layer_type_hardware @$ null ) ; m screenshot view . build layer ( ) ; m screenshot animation . start ( ) ; } } ) ;,we taken screenshot
current listing is exhausted & fetch a new listing,if ( i >= this listing . get partial listing ( ) . length && this listing . has more ( ) ) { this listing = dfs . list paths ( src @$ this listing . get last name ( ) @$ need location ) ; if ( this listing == null ) { throw new file not found exception ( __str__ + src + __str__ ) ; } i = __num__ ; },& fetch listing
fetch the batch and make sure it contains the right state,final batch batch after step1 = batch retriever . fetch batch ( batch name ) ; assert . assert not null ( __str__ @$ batch after step1 ) ; assert . assert equals ( __str__ @$ success fully completed steps @$ batch after step1 . get step names ( ) ) ;,it contains state
generated extensions and their related generated messages .,if ( containing type default instance == null ) { throw new illegal argument exception ( __str__ ) ; } if ( descriptor . get lite type ( ) == wire format . field type . message && message default instance == null ) { throw new illegal argument exception ( __str__ ) ; } this . containing type default instance = containing type default instance ; this . default value = default value ; this . message default instance = message default instance ; this . descriptor = descriptor ;,extensions generated messages
verify that we can persist an object with a couple map mappings,vice president vp sales = new vice president ( ) ; vp sales . name = __str__ ; company company = new company ( ) ; company . conference room extensions . put ( __str__ @$ __str__ ) ; company . organization . put ( __str__ @$ vp sales ) ; s . persist ( company ) ; s . flush ( ) ; s . clear ( ) ;,we persist object
run a word count example keeping tasks that match this pattern,test result result ; final path in dir = new path ( __str__ ) ; final path out dir = new path ( __str__ ) ; result = launch word count ( job conf @$ in dir @$ out dir @$ __str__ + __str__ @$ __num__ @$ __num__ @$ sys dir ) ; assert equals ( __str__ + __str__ @$ result . output ) ;,that match pattern
if the type has no properties then return object,if ( prop names . is empty ( ) ) { return get native type ( js type native . object_type ) ; } immutable map . builder < string @$ js type > props = new immutable map . builder < > ( ) ;,properties return object
create threads and make them run transactions concurrently .,thread thread id [ ] = new thread [ num_threads ] ; for ( int i = __num__ ; i < num_threads ; i ++ ) { transactions trans = new transactions ( namesystem @$ num_transactions ) ; thread id [ i ] = new thread ( trans @$ __str__ + i ) ; thread id [ i ] . start ( ) ; },them run transactions
mock we 've stored the c 2 states,mock state store for container ( c3 @$ assigned ) ; fpga resource handler . reacquire container ( get container id ( __num__ ) ) ; assert . assert equals ( __num__ @$ fpga resource handler . get fpga allocator ( ) . get used fpga count ( ) ) ; assert . assert equals ( __num__ @$ fpga resource handler . get fpga allocator ( ) . get available fpga count ( ) ) ;,we stored the
forward the call change event to the call change listeners .,try { for ( call change listener l : get call change listeners ( ) ) l . call state changed ( ev ) ; } finally { if ( call change event . call_state_change . equals ( ev . get property name ( ) ) && call state . call_ended . equals ( ev . get new value ( ) ) ) { remove call ( call ) ; } },call change event
this will test additional segment allocation .,plc . set initial size ( size / __num__ ) ; plc . set max size ( size ) ; plc . set empty pages pool size ( empty_pages_pool_size ) ; plc . set eviction threshold ( eviction_threshold ) ; plc . set name ( default_policy_name ) ; db cfg . set default data region configuration ( plc ) ; db cfg . set page size ( page_size ) ; cfg . set data storage configuration ( db cfg ) ; return cfg ;,this test allocation
we need app targets for direct targets to get displayed,list < resolved component info > resolved component infos = create resolved components for test ( __num__ ) ; when ( s overrides . resolver list controller . get resolvers for intent ( mockito . any boolean ( ) @$ mockito . any boolean ( ) @$ mockito . isa ( list . class ) ) ) . then return ( resolved component infos ) ;,we need targets
method name can not use reserved keyword @$ e.g . return,if ( is reserved word ( operation id ) ) { logger . warn ( operation id + __str__ + underscore ( sanitize name ( __str__ + operation id ) ) ) ; operation id = __str__ + operation id ; } return underscore ( sanitize name ( operation id ) ) ;,name use keyword
sessions that are n't logged in have no username,if ( session == null || session . get username ( ) == null ) { return get local properties file ( ) ; } else { file profile dir = new file ( rune lite . profiles_dir @$ session . get username ( ) . to lower case ( ) ) ; return new file ( profile dir @$ settings_file_name ) ; },sessions have username
check that corrupting our array does n't affect other callers .,parameters [ __num__ ] = string . class ; parameters = method . get parameter types ( ) ; assert equals ( __num__ @$ parameters . length ) ; assert equals ( expected parameters [ __num__ ] @$ parameters [ __num__ ] ) ;,that corrupting array
then we read the databases etc ...,for ( object id id : get databasei ds ( false ) ) { database meta database meta = load database meta ( id @$ null ) ; database meta . share variables with ( job meta ) ; job meta . add or replace database ( database meta ) ; },we read etc
its not unique @$ add a suitable index ...,string stub = name . substring ( __num__ @$ name . length ( ) - ext . length ( ) - __num__ ) ; int index = __num__ ; do { index ++ ; name = stub + __str__ + index + __str__ + ext ; } while ( this . get script impl ( name ) != null ) ; return name ;,its add index
if the text wrapper does n't have a start index,if ( quotes . get ( i ) . start index == - __num__ ) { if ( closing quote == - __num__ ) { closing quote = i ; } else { if ( quotes . get ( i ) . end index < quotes . get ( closing quote ) . end index ) { closing quote = i ; } } },wrapper have index
sanity check last child has focus with a larger width .,assert true ( m grid view . get child at ( __num__ ) . get width ( ) < m grid view . get child at ( m grid view . get child count ( ) - __num__ ) . get width ( ) ) ; assert equals ( m grid view . get padding left ( ) @$ m grid view . get child at ( m grid view . get child count ( ) - __num__ ) . get left ( ) ) ;,child has focus
the entry will create the connection when needed,basic pool entry entry = new basic pool entry ( op @$ rospl . get route ( ) @$ ref queue ) ; pool lock . lock ( ) ; try { rospl . created entry ( entry ) ; num connections ++ ; issued connections . add ( entry . get weak ref ( ) ) ; } finally { pool lock . unlock ( ) ; } return entry ;,entry create connection
the users list will always contain a crate user,set < user > users = user manager service . get users ( null @$ null ) ; assert that ( users @$ contains ( crate_user ) ) ; users = user manager service . get users ( new users meta data ( ) @$ new users privileges meta data ( ) ) ; assert that ( users @$ contains ( crate_user ) ) ;,list contain user
optionally check the byte after this frame matches sync word .,if ( ! try read ( pes buffer @$ adts scratch . data @$ __num__ ) ) { return true ; } adts scratch . set position ( __num__ ) ; int frame size = adts scratch . read bits ( __num__ ) ; if ( frame size <= __num__ ) { return false ; },frame matches word
to kick in . metamodel will clean this up ...,try { session factory builder sf builder = metadata . get session factory builder ( ) ; populate ( sf builder @$ standard service registry ) ; schema management tool coordinator . process ( metadata @$ standard service registry @$ runtime settings . get settings ( ) @$ delayed drop registry not available impl . instance ) ; } catch ( exception e ) { throw persistence exception ( __str__ @$ e ) ; },metamodel clean this
unknown namespace should throw an exception .,try { final byte [ ] xattr = fs . getx attr ( path @$ __str__ ) ; assert . fail ( __str__ ) ; } catch ( exception e ) { generic test utils . assert exception contains ( __str__ + __str__ + __str__ @$ e ) ; },namespace throw exception
after incrementing time we should be getting the new sampled values,clock . add duration ( sampling_duration_increment ) ; assert . assert equals ( tracker . get stats ( ) . get sample max pool size ( ) @$ pool_size + __num__ ) ; assert . assert equals ( tracker . get stats ( ) . get sample max checked out ( ) @$ checked_out + __num__ ) ; assert . assert equals ( tracker . get stats ( ) . get sample max wait time ( ) @$ wait_time + __num__ ) ;,we getting values
the server only accepts fields that are replicated from owner,replicated field metadata < ? @$ ? > replicated field metadata = ( replicated field metadata < ? @$ ? > ) field metadata ; return replicated field metadata . is replicated ( ) && replicated field metadata . get replication info ( ) . value ( ) . is replicate from owner ( ) ;,server accepts fields
exclude test which does n't support current infrastructure,groups . remove ( test group . singleuser ) ; groups . remove ( test group . multiuser ) ; groups . remove ( test group . github ) ; groups . remove ( test group . under_repair ) ; groups . remove ( test group . flaky ) ; if ( ! groups . is empty ( ) && ! groups . contains ( infrastructure . to string ( ) . to lower case ( ) ) ) { annotation . set enabled ( false ) ; },which support infrastructure
fullscreen task does n't use bounds for compute frame,window state w = create window ( match_parent @$ match_parent ) ; w . m attrs . gravity = gravity . left | gravity . top ; final int bottom content inset = __num__ ; final int top content inset = __num__ ; final int bottom visible inset = __num__ ; final int top visible inset = __num__ ; final int left stable inset = __num__ ; final int right stable inset = __num__ ;,task use bounds
this method will throw an exception in the middle of startup message.process,when ( distribution manager . get transport ( ) ) . then throw ( new illegal state exception ( __str__ ) ) ; startup message startup message = new startup message ( ) ; startup message . set sender ( id ) ; startup message . process ( distribution manager ) ;,method throw exception
all subscribers should receive the same value,for ( test subscriber < integer > sub : subscribers ) { sub . await terminal event ( __num__ @$ time unit . milliseconds ) ; system . out . println ( __str__ + sub . get on next events ( ) ) ; sub . assert no errors ( ) ; sub . assert values ( __num__ ) ; },subscribers receive value
core profile requires vao to be bound .,if ( caps . contains ( caps . core profile ) ) { gl3 . gl gen vertex arrays ( int buf16 ) ; int vao id = int buf16 . get ( __num__ ) ; gl3 . gl bind vertex array ( vao id ) ; },profile requires vao
this should not crash . it 'll still consume the quota .,assert true ( m manager . add dynamic shortcuts ( list ( ) ) ) ; assert equals ( __num__ @$ m manager . get remaining call count ( ) ) ; assert shortcut ids ( assert all not key fields only ( m manager . get dynamic shortcuts ( ) ) @$ __str__ @$ __str__ @$ __str__ ) ; m injected current time millis += interval ;,it consume quota
make sure we insert only one file per checksum whichever comes first,if ( initial cached entries . contains key ( key ) ) { log . warn ( __str__ + key + __str__ + initial cached entries . get ( key ) + __str__ + file name + __str__ ) ; } else { initial cached entries . put ( key @$ file name ) ; },we insert file
client should have sent a single request for a single page,assert equals ( exchange client . get status ( ) . get buffered pages ( ) @$ __num__ ) ; assert true ( exchange client . get status ( ) . get buffered bytes ( ) > __num__ ) ; assert status ( exchange client . get status ( ) . get page buffer client statuses ( ) . get ( __num__ ) @$ location @$ __str__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ ) ;,client sent request
type list can be short .. we just repeat last spec,for ( object x : predictor list ) { if ( i types . has next ( ) ) { last type = i types . next ( ) ; } type map . put ( x . to string ( ) @$ last type ) ; },we repeat spec
these have same hri but different locations so should be different .,assert false ( hrl3 . equals ( hrl4 ) ) ; h region location hrl5 = new h region location ( hrl4 . get region ( ) @$ hrl4 . get server name ( ) @$ hrl4 . get seq num ( ) + __num__ ) ; assert true ( hrl4 . equals ( hrl5 ) ) ;,these have hri
make sure we have one param that needs url encoding,buffer . append string ( __str__ + url encoder . encode ( __str__ @$ __str__ ) + __str__ @$ __str__ ) ; req . headers ( ) . set ( __str__ @$ string . value of ( buffer . length ( ) ) ) ; req . headers ( ) . set ( __str__ @$ __str__ ) ; req . end ( buffer ) ;,we have param
method name can not use reserved keyword @$ e.g . return,if ( is reserved word ( operation id ) ) { logger . warn ( operation id + __str__ + underscore ( sanitize name ( __str__ + operation id ) ) ) ; return underscore ( sanitize name ( __str__ + operation id ) ) ; },name use keyword
we have at least enough memory and must not allocate,if ( size needed == len ) { return array ; } if ( ! may allocate ) { if ( size needed <= len ) { return array ; } throw new out of memory error ( __str__ + label ) ; },we have memory
any fully contained view beats a partially contained view,if ( view is fully contained ) { focus candidate = view ; found fully contained focusable = true ; } else if ( view is closer to vertical boundary && view is closer to horizontal boundary ) { focus candidate = view ; },view beats view
test an app that has n't had any activity .,execution stats expected stats = new execution stats ( ) ; execution stats input stats = new execution stats ( ) ; input stats . window size ms = expected stats . window size ms = __num__ * hour_in_millis ; input stats . job count limit = expected stats . job count limit = __num__ ; input stats . session count limit = expected stats . session count limit = __num__ ;,that had activity
use a large capacity that does n't require expansion @$ yet .,vector map join fast multi key hash map map = new vector map join fast multi key hash map ( false @$ large_capacity @$ load_factor @$ large_wb_size @$ - __num__ ) ; verify fast row hash map verify table = new verify fast row hash map ( ) ; vector random row source value source = new vector random row source ( ) ; value source . init ( random @$ vector random row source . supported types . all @$ __num__ @$ false @$ false ) ;,that require expansion
belt and braces incase the failed login no longer throws exceptions .,w1 = ( xml page ) wc1 . go to ( __str__ @$ __str__ ) ; assert that ( w1 @$ hasx path ( __str__ @$ is ( __str__ ) ) ) ; w2 = ( xml page ) wc2 . go to ( __str__ @$ __str__ ) ; assert that ( w2 @$ hasx path ( __str__ @$ is ( __str__ ) ) ) ;,login throws exceptions
handle es 6 object lit shorthand assignments,var val var = t . get scope ( ) . get var ( key . get string ( ) ) ; if ( val var != null && val var . get name node ( ) == var . get name node ( ) ) { key is export = true ; break ; },es object assignments
now set the values in the row !,for ( int i = __num__ ; i < row meta . size ( ) ; i ++ ) { value meta interface v = row meta . get value meta ( i ) ; object object = data [ i ] ; try { set value ( ps @$ v @$ object @$ i + __num__ ) ; } catch ( kettle database exception e ) { throw new kettle database exception ( __str__ + row meta @$ e ) ; } },now set values
we have a single task slot which we first have to release,if ( single logical slot future . is done ( ) && ! single logical slot future . is completed exceptionally ( ) ) { final single logical slot single logical slot = single logical slot future . get now ( null ) ; single logical slot . release ( cause ) ; },we have slot
we need topmost desktop of them all .,if ( is desktop window ( laux ) ) { i desktop = i ; w desktop = laux ; } else if ( i bottom < __num__ && toplevels . contains ( long . value of ( laux ) ) && laux != mytopl ) { i bottom = i ; w bottom = laux ; },we need desktop
since the windowing is nondeterministic we only check the sums,actual counts = new tree map < > ( ) ; for ( string line : output lines ) { string [ ] splits = line . split ( __str__ @$ - __num__ ) ; string word = splits [ __num__ ] ; long count = long . parse long ( splits [ __num__ ] ) ; actual counts . merge ( word @$ count @$ ( a @$ b ) -> a + b ) ; } return actual counts . equals ( expected word counts ) ;,we check sums
completing last task should finish the process instance,org . flowable . task . api . task last task = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . single result ( ) ; task service . complete ( last task . get id ( ) ) ; assert equals ( __num__ @$ runtime service . create process instance query ( ) . active ( ) . count ( ) ) ;,task finish instance
assignability rules do not reflect our needs,upper bounds = get uppermost bounds ( upper bounds ) ; stricter upper bounds = get uppermost bounds ( stricter upper bounds ) ; for ( type upper bound : upper bounds ) { if ( ! covariant types . is assignable from at least one ( upper bound @$ stricter upper bounds ) ) { return false ; } } return true ;,rules reflect needs
make sure we get back the right key directly from api,assert array equals ( keypass @$ provider . get credential entry ( dfs_server_https_keypassword_key ) . get credential ( ) ) ; assert array equals ( storepass @$ provider . get credential entry ( dfs_server_https_keystore_password_key ) . get credential ( ) ) ; assert array equals ( trustpass @$ provider . get credential entry ( dfs_server_https_truststore_password_key ) . get credential ( ) ) ;,we get key
if it does n't pass the op @$ skip it,if ( comparator != null && compare value ( get compare operator ( ) @$ comparator @$ c ) ) return return code . skip ; stamp set . add ( c . get timestamp ( ) ) ; if ( drop dependent column ) { return return code . skip ; } return return code . include ;,it pass op
user does not have the required authority,if ( ! allowed . result ( ) ) { ctx . response ( ) . set status code ( __num__ ) . end ( ) ; return ; } ctx . response ( ) . put header ( __str__ @$ __str__ ) ; ctx . response ( ) . end ( __str__ ) ;,user have authority
ensure the ie contains the correct leading bytes,byte [ ] lead buffer bytes = arrays . copy of range ( buffer @$ buffer ptr @$ expected lead bytes . length ) ; if ( ! arrays . equals ( lead buffer bytes @$ expected lead bytes ) ) { return false ; },ie contains bytes
all succeeded @$ set rollback to false,rollback = false ; if ( rollback ) { for ( ; ; ) { i -- ; if ( i < __num__ ) { break ; } region state node region node = region nodes . get ( i ) ; region node . unset procedure ( procs [ i ] ) ; } } unlock ( region nodes ) ;,all set rollback
if some files have errors move its to separate message,if ( not uploaded files paths . size ( ) > __num__ ) { string message id = message manager . get instance ( ) . create file message ( account @$ user @$ not uploaded files ) ; set error for message ( message id @$ generate error description for files ( not uploaded files paths @$ errors ) ) ; },errors move its
if we do n't recognize the domain @$ throw an exception .,if ( ! host . ends with ( __str__ ) ) { throw new illegal argument exception ( __str__ + host + __str__ ) ; } string service and region = host . substring ( __num__ @$ host . index of ( __str__ ) ) ;,we recognize domain
test that we made 3 attempts using the same server,verify ( get context ( ) @$ times ( num attempts ) ) . search ( any string ( ) @$ any string ( ) @$ any ( object [ ] . class ) @$ any ( search controls . class ) ) ;,we made attempts
focus painted same color as text on basic ? ?,int width = b . get width ( ) ; int height = b . get height ( ) ; g . set color ( get focus color ( ) ) ; basic graphics utils . draw dashed rect ( g @$ dashed rect gapx @$ dashed rect gapy @$ width - dashed rect gap width @$ height - dashed rect gap height ) ;,focus painted color
should not have any effect as invalidated values are destroyed,for ( int i = __num__ ; i < __num__ ; i ++ ) { region . destroy ( integer . to string ( i ) ) ; } assert equals ( __num__ @$ key index1 stats . get number of keys ( ) ) ; assert equals ( __num__ @$ key index1 stats . get number of values ( ) ) ; assert equals ( __num__ @$ key index1 stats . get num updates ( ) ) ;,not have effect
create listener that forces the dispatching to fail,listener = new test exception flowable event listener ( true ) ; second listener = new test flowable event listener ( ) ; dispatcher . add event listener ( listener ) ; dispatcher . add event listener ( second listener ) ; assert that thrown by ( ( ) -> dispatcher . dispatch event ( event ) ) . is exactly instance of ( runtime exception . class ) . has message ( __str__ ) ;,that forces dispatching
we disable truffle ast inlining to not inline the callee,try ( truffle options override scope o = truffle compiler options . override options ( shared truffle compiler options . truffle function inlining @$ false ) ) { structured graph graph = partial eval ( caller with exception @$ new object [ __num__ ] @$ allow assumptions . yes @$ truffle compiler . create compilation identifier ( caller with exception ) ) ; assert . assert equals ( __num__ @$ graph . get nodes ( ) . filter ( unwind node . class ) . count ( ) ) ; },we disable ast
we only want auctions in category 10 .,p collection < kv < long @$ auction > > auctions by seller id = events windowed . apply ( nexmark query util . just_new_auctions ) . apply ( name + __str__ @$ filter . by ( auction -> auction . category == __num__ ) ) . apply ( __str__ @$ nexmark query util . auction_by_seller ) ;,we want auctions
first check whether we have a file .,file lock file = get lock file ( ) ; if ( lock file . exists ( ) ) { inet socket address lock address = read lock file retrying ( lock file ) ; if ( lock address != null ) { if ( inter instance connect ( lock address @$ args ) == success ) { return already_started ; } } lock file . delete ( ) ; },we have file
assume that attributes and namespace nodes immediately follow the element,while ( dtm . null != ( identity = get next node identity ( identity ) ) ) { int type = _type ( identity ) ; if ( type == dtm . attribute_node ) { return identity ; } else if ( type != dtm . namespace_node ) { break ; } } return dtm . null ;,attributes follow element
we ca n't set something to null .,if ( cert == null ) { throw new null pointer exception ( __str__ ) ; } final byte [ ] encoded ; try { encoded = cert . get encoded ( ) ; } catch ( certificate encoding exception e ) { throw new key store exception ( e ) ; } if ( ! m key store . put ( credentials . ca_certificate + alias @$ encoded @$ m uid @$ key store . flag_none ) ) { throw new key store exception ( __str__ ) ; },we set something
if we have a null row we go with the default,if ( row == null ) return - __num__ ; for ( int probe = __num__ ; probe < row . length ; probe ++ ) { tag = row [ probe ++ ] ; if ( tag == sym || tag == - __num__ ) { return row [ probe ] ; } },we have row
otherwise we risk concurrent modification exception .,list < lottie listener < t > > listeners copy = new array list < > ( success listeners ) ; for ( lottie listener < t > l : listeners copy ) { l . on result ( value ) ; },we risk exception
check if statistical model detected a split,if ( use statistical model ) { string mwt tag key = string . format ( __str__ @$ token . word ( ) . to lower case ( ) @$ token . tag ( ) ) . to lower case ( ) ; if ( statistical multi word token mapping . contains key ( mwt tag key ) ) token words = statistical multi word token mapping . get ( mwt tag key ) . stream ( ) . collect ( collectors . to list ( ) ) ; },model detected split
assuming we will get one before this loop expires,boolean found value = false ; for ( int i = __num__ ; i < __num__ ; i ++ ) { int random row = performance evaluation . generate random row ( random @$ opts . total rows ) ; if ( random row > __num__ ) { found value = true ; break ; } } assert true ( __str__ @$ found value ) ;,we get one
verify that we switch the setting to point to the current package,mockito . verify ( m test system impl ) . update user setting ( mockito . any object ( ) @$ mockito . eq ( non chosen package ) ) ; assert equals ( non chosen package @$ m test system impl . get user chosen web view provider ( null ) ) ; check preparation phases for package ( non chosen package @$ __num__ ) ;,we switch setting
you can set your own upgrade patch if you need,abstract patch upgrade patch processor = new upgrade patch ( ) ; tinker installer . install ( app like @$ load reporter @$ patch reporter @$ patch listener @$ sample result service . class @$ upgrade patch processor ) ; is installed = true ;,you set patch
check if this node has a declaration for this namespace,if ( ! declared ) { final string old = _accessed prefixes . get ( prefix ) ; if ( old != null ) { if ( old . equals ( uri ) ) return ; else prefix = stable . generate namespace prefix ( ) ; } },node has declaration
error arises because user has specified case sensitive affinity column name,grid test utils . assert throws ( null @$ new callable < object > ( ) { @ override public object call ( ) throws exception { execute ( __str__ ) ; return null ; } } @$ ignitesql exception . class @$ __str__ ) ;,user specified name
the lists can not contain duplicate subchannels,return other == this || ( stickiness state == other . stickiness state && list . size ( ) == other . list . size ( ) && new hash set < > ( list ) . contains all ( other . list ) ) ;,lists contain subchannels
make sure the cache can not access the ha store directly,if ( ! cache has access to fs ) { cache config . set string ( blob server options . storage_directory @$ temporary folder . new folder ( ) . get absolute path ( ) ) ; cache config . set string ( high availability options . ha_storage_path @$ temporary folder . new folder ( ) . get path ( ) + __str__ ) ; },cache access store
verify that a second call wo n't read the proc file again,assert true ( m test file . delete ( ) ) ; actual freqs = m reader . read freqs ( m power profile ) ; assert array equals ( freqs [ i ] @$ actual freqs ) ; assert false ( err msg @$ m reader . per cluster times available ( ) ) ;,call read file
confirm that peer with state a will reject replication request .,verify replication request rejection ( util1 @$ true ) ; verify replication request rejection ( util2 @$ false ) ; util1 . get admin ( ) . disable replication peer ( peer_id ) ; write ( util1 @$ __num__ @$ __num__ ) ; thread . sleep ( __num__ ) ;,peer reject request
are we watching the same file twice @$ that is bad !,if ( global configuration . asserts mode && cmp == __num__ ) { if ( file2 . get absolute file ( ) . to string ( ) . equals ( file . get absolute file ( ) . to string ( ) ) ) { log . severe ( __str__ + file . get absolute file ( ) . to string ( ) ) ; } },we watching file
keep this code it also validates the min and max offset,this . min offset = min offset ; this . max offset = max offset ; min address = new generic address ( this @$ min offset ) ; max address = new generic address ( this @$ max offset ) ;,it validates min
check that the standard java one can read it too,string from java = new java . io . data input stream ( new byte array input stream ( buff ) ) . readutf ( ) ; assert that ( from java ) . is equal to ( cat face ) ;,one read it
random walk check if time to change direction,if ( temp time - ai . last change of direction at > direction change time ) { direction change time = ( long ) ( ai . move update time * random . next double ( ) * ai . straight lined ) ; random walk ( world pos @$ ai ) ; entity . save component ( ai ) ; },time change direction
ensure we can reopen log with no error,channel . stop ( ) ; channel = create file channel ( ) ; channel . start ( ) ; assert . assert true ( channel . is open ( ) ) ; transaction = channel . get transaction ( ) ; transaction . begin ( ) ; assert . assert null ( channel . take ( ) ) ; transaction . commit ( ) ; transaction . close ( ) ;,we reopen log
this throwable is saving the call stack which created this runnable .,if ( is tracing enabled ) { final thread thread = thread . current thread ( ) ; m tracing throwable = new throwable ( new string builder ( message_part_1 ) . append ( thread . get id ( ) ) . append ( message_part_2 ) . append ( thread . get name ( ) ) . to string ( ) ) ; } else { m tracing throwable = null ; },throwable saving stack
verify the successful flow file has the expected attributes,final mock flow file mock flow file = test runner . get flow files for relationship ( putorc . rel_success ) . get ( __num__ ) ; mock flow file . assert attribute equals ( putorc . absolute_hdfs_path_attribute @$ orc file . get parent ( ) . to string ( ) ) ; mock flow file . assert attribute equals ( core attributes . filename . key ( ) @$ filename ) ; mock flow file . assert attribute equals ( putorc . record_count_attr @$ __str__ ) ;,file has attributes
'otheritems ' represents the hash code for the previous versions .,int otheritems = ( lenient ? __num__ : __num__ ) | ( first day of week << __num__ ) | ( minimal days in first week << __num__ ) | ( zone . hash code ( ) << __num__ ) ; long t = get millis of ( this ) ; return ( int ) t ^ ( int ) ( t > > __num__ ) ^ otheritems ;,'otheritems represents code
this simulates the completion of txnid : id txn update 1,long write id = txn mgr2 . get table write id ( __str__ @$ __str__ ) ; add dynamic partitions adp = new add dynamic partitions ( txn mgr2 . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr2 . commit txn ( ) ;,this simulates completion
we ca n't do anything if left tuple memory is empty,if ( left tuple != null ) { do right updates process children ( child left tuple @$ left tuple @$ right tuple @$ staged left tuples @$ context entry @$ constraints @$ sink @$ it @$ trg left tuples ) ; },we do anything
calls find subroutine recursively on exception handler successors,list < try catch block node > insn handlers = handlers [ insn ] ; if ( insn handlers != null ) { for ( int i = __num__ ; i < insn handlers . size ( ) ; ++ i ) { try catch block node tcb = insn handlers . get ( i ) ; find subroutine ( insns . index of ( tcb . handler ) @$ sub @$ calls ) ; } },calls find recursively
different items @$ we can use comparison and may avoid lookup,if ( existing != item ) { final int cmp = m callback . compare ( existing @$ item ) ; if ( cmp == __num__ ) { m data [ index ] = item ; if ( contents changed ) { m callback . on changed ( index @$ __num__ @$ m callback . get change payload ( existing @$ item ) ) ; } return ; } },items use comparison
if we left party raid was started or we left raid,if ( client . get var ( var player . in_raid_party ) == - __num__ && ( ! in raid chambers || ! config . scout overlay in raid ( ) ) ) { overlay . set scout overlay shown ( false ) ; },we left raid
touching the tree artifact directory should have no effect,file system utils . touch file ( out1 . get path ( ) ) ; assert that ( new filesystem value checker ( null @$ null ) . get dirty action values ( evaluator . get values ( ) @$ batch statter @$ modified file set . everything_modified ) ) . is empty ( ) ;,directory have effect
actual column data type is unknown @$ use expected column data type,if ( actual data type instanceof unknown data type ) { return expected data type ; } if ( hack ignore int big int mismatch ) { if ( expected data type instanceof integer data type && actual data type instanceof big integer data type ) return actual data type ; },type use data
begin boilerplate code so parent classes can restore state,if ( ! ( state instanceof saved state ) ) { super . on restore instance state ( state ) ; return ; } saved state ss = ( saved state ) state ; super . on restore instance state ( ss . get super state ( ) ) ;,classes restore state
assert that all cells in the result have the same key,assert . assert equals ( __num__ @$ bytes . compare to ( row @$ __num__ @$ row . length @$ cell . get row array ( ) @$ cell . get row offset ( ) @$ cell . get row length ( ) ) ) ;,cells have key
and we ask s 3 to discard because it was ahead .,params = new request . params ( ) . with instance id ( s3 ) . with offset ( s3 offset ) . with segment name ( segment name str ) . with reason ( segment completion protocol . reason_time_limit ) ; response = segment completion mgr . segment consumed ( params ) ; assert . assert equals ( response . get status ( ) @$ controller response status . discard ) ;,we ask 3
either a jre or a jdk build image,if ( files . exists ( home . resolve ( __str__ ) ) ) { path classes = home . resolve ( __str__ ) ; if ( files . is directory ( classes ) ) { result . add ( new jdk archive ( classes ) ) ; } result . add all ( add jar files ( home . resolve ( __str__ ) ) ) ; } else { throw new runtime exception ( __str__ + home + __str__ ) ; },jre build image
make sure we get back the right key,assert array equals ( bindpass @$ provider . get credential entry ( bindpass alias ) . get credential ( ) ) ; ldap groups mapping mapping = new ldap groups mapping ( ) ; assert . assert equals ( __str__ @$ mapping . get password from credential providers ( conf @$ bindpass alias @$ __str__ ) ) ;,we get key
this is considered a success since we got a response .,successes . increment and get ( ) ; log . debug ( __str__ ) ; sink . publish read failure ( table name . get name as string ( ) @$ server name ) ; log . error ( dnrioe . to string ( ) @$ dnrioe ) ; sink . publish read failure ( table name . get name as string ( ) @$ server name ) ; log . error ( e . to string ( ) @$ e ) ;,we got response
transaction stops now @$ may be syncs that need a callback,if ( xa resource . xa_rdonly == response . get result ( ) ) { list < transaction context > l ; synchronized ( ended_xa_transaction_contexts ) { l = ended_xa_transaction_contexts . remove ( x ) ; } if ( l != null ) { if ( ! l . is empty ( ) ) { log . debug ( __str__ @$ xid ) ; for ( transaction context ctx : l ) { ctx . after commit ( ) ; } } } },that need callback
a null cq implies an empty column qualifier,if ( null != accumulo column mapping . get column qualifier ( ) ) { cq = new text ( accumulo column mapping . get column qualifier ( ) ) ; } pairs . add ( new pair < text @$ text > ( cf @$ cq ) ) ;,cq implies qualifier
verify that skip trash option really skips the trash for rmr,string [ ] args = new string [ __num__ ] ; args [ __num__ ] = __str__ ; args [ __num__ ] = __str__ ; args [ __num__ ] = my path . to string ( ) ; int val = - __num__ ;,option skips trash
we may be nuking an existing local,if ( result == null ) { result = insn . get result ( ) ; if ( result != null && primary state . get ( result . get reg ( ) ) != null ) { primary state . remove ( primary state . get ( result . get reg ( ) ) ) ; } continue ; },we nuking local
make sure we can set an action that already had a binding,docking action bound action = get bound action ( ) ; show dialog ( bound action ) ; assert equals ( __str__ @$ key entry field . get text ( ) ) ; trigger text ( key entry field @$ __str__ ) ; press dialogok ( ) ; accelerator key = bound action . get key binding ( ) ; assert not null ( accelerator key ) ; assert equals ( accelerator key . get key code ( ) @$ key event . vk_q ) ;,we set action
unicode now contains the four hex digits which represents our unicode character,if ( unicode . length ( ) == __num__ ) { try { int value = integer . parse int ( unicode . to string ( ) @$ __num__ ) ; out . write ( ( char ) value ) ; unicode . set length ( __num__ ) ; in unicode = false ; had slash = false ; } catch ( number format exception nfe ) { throw new runtime exception ( __str__ + unicode @$ nfe ) ; } },which represents character
this will create a region with 3 files,assert equals ( __num__ @$ region . get store ( family ) . get storefiles count ( ) ) ; list < path > store files = new array list < > ( __num__ ) ; for ( h store file sf : region . get store ( family ) . get storefiles ( ) ) { store files . add ( sf . get path ( ) ) ; },this create region
since we failed sync @$ free the blocks in bucket allocator,for ( int i = __num__ ; i < entries . size ( ) ; ++ i ) { if ( bucket entries [ i ] != null ) { bucket allocator . free block ( bucket entries [ i ] . offset ( ) ) ; bucket entries [ i ] = null ; } },we failed sync
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result interface . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result interface . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
if we have a deferred resize @$ apply it now,if ( m defer resize to normal bounds until rotation == display rotation ) { m motion helper . animate to unexpanded state ( normal bounds @$ m saved snap fraction @$ m normal movement bounds @$ m movement bounds @$ m is minimized @$ true ) ; m saved snap fraction = - __num__ ; m defer resize to normal bounds until rotation = - __num__ ; },we have resize
make sure the members see the new updates in those secondary buckets,vm0 . invoke ( ( ) -> check data ( __num__ @$ __num__ + num buckets @$ __str__ ) ) ; vm1 . invoke ( ( ) -> check data ( __num__ @$ __num__ + num buckets @$ __str__ ) ) ;,members see updates
the substitute value is set . serialize it,if ( this . substitute value != null ) { is serializing value . set ( boolean . true ) ; result = entry event impl . serialize ( this . substitute value ) ; is serializing value . set ( boolean . false ) ; return result ; },substitute serialize it
only place where we use call context,tenant = tenant user api . create tenant ( new default tenant ( uuid @$ init @$ init @$ external key @$ api key @$ api secret ) @$ call context ) ; test call context = new default call context ( null @$ tenant . get id ( ) @$ __str__ @$ call origin . external @$ user type . test @$ __str__ @$ __str__ @$ uuid @$ clock ) ;,we use context
if we touched the high mark @$ then add a new one,if ( c == max_value ) { ensure capacity ( len + __num__ ) ; list [ len ++ ] = high ; } if ( i > __num__ && c == list [ i - __num__ ] ) { system . arraycopy ( list @$ i + __num__ @$ list @$ i - __num__ @$ len - i - __num__ ) ; len -= __num__ ; },we touched mark
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result interface . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result interface . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
check that the test stdout contains all the expected output,try { string out data = file system utils . read content ( out err . get output path ( ) @$ utf_8 ) ; assert that ( out data ) . contains ( __str__ ) ; } catch ( io exception e ) { fail ( __str__ + out err . get output path ( ) ) ; },stdout contains output
null store file type means 'autodetect from file extension ',x509 trust manager tm = x509 util . create trust manager ( x509 test context . get trust store file ( key store file type . pkcs12 ) . get absolute path ( ) @$ x509 test context . get trust store password ( ) @$ null @$ true @$ true @$ true @$ true ) ;,type means 'autodetect
x stream may overwrite even the final field .,if ( repository browser != rb ) { try { field f = scm descriptor . class . get declared field ( __str__ ) ; f . set accessible ( true ) ; f . set ( this @$ rb ) ; } catch ( no such field exception | illegal access exception e ) { logger . log ( warning @$ __str__ @$ e ) ; } },stream overwrite field
choosing from wp media requires a site,view wp media = m bottom bar . find view by id ( r . id . icon_wpmedia ) ; if ( m site == null ) { wp media . set visibility ( view . gone ) ; } else { wp media . set on click listener ( new view . on click listener ( ) { @ override public void on click ( view v ) { do icon clicked ( photo picker icon . wp_media ) ; } } ) ; },choosing requires site
only arrays or objects can have children .,if ( get value ( ) . get type ( ) != json tree value . type . array && get value ( ) . get type ( ) != json tree value . type . object ) { return false ; },arrays have children
the single digit date has two spaces @$ so trim it,fmt2 . search pattern . add ( __str__ ) ; fmt2 . replace pattern . add ( __str__ ) ; fmt2 . date format . add ( new simple date format ( syslog_timestamp_format_rfc3164_1 @$ locale . english ) ) ; fmt2 . add year = true ; formats . add ( fmt1 ) ; formats . add ( fmt2 ) ;,date has spaces
after ~400 kb more @$ we should have done an automatic sync,for ( int i = __num__ ; i < __num__ ; i ++ ) { log . log delete ( onekb @$ __num__ @$ false ) ; } mockito . verify ( mock metrics @$ mockito . times ( __num__ ) ) . add sync ( mockito . any long ( ) ) ; log . close ( ) ;,we done sync
can not disclose category ids or values because they could contain pii,builder . append ( __str__ ) ; helper . append redacted ( builder @$ m category ids ) ; builder . append ( __str__ ) ; helper . append redacted ( builder @$ m values ) ; return builder . append ( __str__ ) . to string ( ) ;,they contain pii
the returned new ld lib path is terminated with ' : ',string new ld lib path = extract and check ( profile dir @$ no_focus_library_name @$ path_prefix + __str__ @$ path_prefix + __str__ ) ; if ( existing ld lib path != null && ! existing ld lib path . equals ( __str__ ) ) { new ld lib path += existing ld lib path ; } env builder . put ( __str__ @$ new ld lib path ) ;,the returned path
we fake the missing command by using reset and relative commands .,conn . queue ( this . addr @$ ! this . addr . is group ( ) @$ pck generator . var reset ( this . var @$ is2013 ) ) ; conn . queue ( this . addr @$ ! this . addr . is group ( ) @$ pck generator . var rel ( this . var @$ lcn defs . rel var ref . current @$ value . to native ( ) @$ is2013 ) ) ;,we fake command
test deprecated h cat add partitions desc api .,h cat add partition desc add ptn2 = h cat add partition desc . create ( db name @$ table name @$ null @$ second ptn ) . build ( ) ; client . add partition ( add ptn2 ) ; map < string @$ string > third ptn = new hash map < string @$ string > ( ) ; third ptn . put ( __str__ @$ __str__ ) ; third ptn . put ( __str__ @$ __str__ ) ;,cat add api
6 th build @$ success @$ accumulation continues up to this point,scm . add change ( ) . with author ( __str__ ) ; p . get builders list ( ) . clear ( ) ; b = j . build and assert success ( p ) ; assert culprits ( b @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,th build success
this constructor will use a default entity resolver,threaded xml pull parser impl parser = new threaded xml pull parser impl ( new byte array input stream ( xxe xml . get bytes ( ) ) @$ test name . get method name ( ) @$ new test error handler ( ) @$ false @$ __num__ ) ; parser . start ( __str__ ) ; xml element x1 = parser . next ( ) ; assert false ( __str__ @$ x1 . get text ( ) . contains ( file contents ) ) ;,constructor use resolver
make sure it all contains jre 's locales for compatibility .,loc list . add all ( arrays . as list ( locale provider adapter . forjre ( ) . get available locales ( ) ) ) ; locale [ ] tmp = new locale [ loc list . size ( ) ] ; loc list . to array ( tmp ) ; return tmp ;,it contains locales
finally @$ have digit list parse the digits into a value .,if ( digit list . fits into long ( status [ status_positive ] @$ is parse integer only ( ) ) ) { got double = false ; long result = digit list . get long ( ) ; if ( long result < __num__ ) { got long minimum = true ; } } else { double result = digit list . get double ( ) ; },list parse digits
first instance that has statistics type name,if ( __str__ . equals ( type ) ) { max memory = ( number ) get attribute ( vm memory usage stats [ i ] @$ __str__ @$ default val ) ; used memory = ( number ) get attribute ( vm memory usage stats [ i ] @$ __str__ @$ default val ) ; break ; },that has name
keep reading until we can read at least one sample after seek,try { while ( extractor read result == extractor . result_continue && track output . get sample count ( ) == num sample before seek ) { extractor read result = extractor . read ( extractor input @$ position holder ) ; } } finally { util . close quietly ( data source ) ; },we read sample
special case to accept all log file extensions .,if ( log copier . this . log file extensions . is empty ( ) ) { return true ; } return log copier . this . log file extensions . contains ( files . get file extension ( log file path . get name ( ) ) ) ;,case file extensions
since we use jwt @$ session is not necessary,http . cors ( ) . and ( ) . authorize requests ( ) . request matchers ( cors utils :: is pre flight request ) . permit all ( ) . any request ( ) . authenticated ( ) . and ( ) . exception handling ( ) . authentication entry point ( unauthorized handler ) . and ( ) . session management ( ) . session creation policy ( session creation policy . stateless ) . and ( ) . csrf ( ) . disable ( ) ;,we use jwt
validate that the subtasks states have registered their shared states .,verify ( subtask state1 @$ times ( __num__ ) ) . register shared states ( any ( shared state registry . class ) ) ; verify ( subtask state2 @$ times ( __num__ ) ) . register shared states ( any ( shared state registry . class ) ) ;,states registered states
post events that run a step for 100 ms .,step event . started step event started = step event . started ( step short name @$ step description @$ step uuid ) ; event bus . post without configuring ( configure test event at time ( step event started @$ __num__ @$ time unit . milliseconds @$ __num__ ) ) ;,that run step
get partition keys response table so we can copy it,final volt table partition keys ; switch ( type ) { case integer : partition keys = hashinator . m_integer partition keys . get ( ) ; break ; case string : partition keys = hashinator . m_string partition keys . get ( ) ; break ; case varbinary : partition keys = hashinator . m_varbinary partition keys . get ( ) ; break ; default : return null ; },we copy it
check if the user has this permission defined in the context,for ( role role : this . user name to user . get ( user . name ) . roles ) { for ( permission permitted : role . permissions ) { if ( permitted . implies ( context ) ) { return true ; } } } return false ;,user has permission
ensure len represents integral number of frames,len -= len % line . get format ( ) . get frame size ( ) ; log ( __str__ ) ; try { line . read ( buffer @$ offset @$ len ) ; log ( __str__ ) ; failed ++ ; } catch ( array index out of bounds exception ex ) { log ( __str__ + ex ) ; } line stopper . force ( ) ;,len represents number
debugger will keep stepping thru this loop,for ( int ii = __num__ ; ii < __num__ ; ii ++ ) { answer ++ ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception ee ) { system . out . println ( __str__ + ii ) ; } },debugger keep stepping
error on open @$ create exception and signal failure .,if ( is awaiting open ( ) ) { log . warn ( __str__ @$ this ) ; exception open error ; if ( has remote error ( ) ) { open error = amqp support . convert to exception ( get endpoint ( ) . get remote condition ( ) ) ; } else { open error = get open abort exception ( ) ; } failed ( open error ) ; } else { remotely closed ( connection ) ; },error create failure
first @$ we validate the input .,error label . set text ( __str__ ) ; string text to server = name field . get text ( ) ; if ( ! field verifier . is valid name ( text to server ) ) { error label . set text ( __str__ ) ; return ; },we validate input
an item can have an arrow or a check icon at once,if ( use check and arrow ( ) && ( ! __str__ . equals ( get acc text ( ) ) ) ) { lr . get acc rect ( ) . width = max acc or arrow width ; } else { lr . get arrow rect ( ) . width = max acc or arrow width ; },item have arrow
with a more substantial file that has real functionality,try { return existing target . get file name ( ) . equals ( init_py ) && project filesystem . get file size ( existing target ) == __num__ ; } catch ( io exception e ) { return false ; },that has functionality
hide notification first @$ as tie managed profile lock takes time,hide encryption notification ( new user handle ( user id ) ) ; if ( m user manager . get user info ( user id ) . is managed profile ( ) ) { tie managed profile lock if necessary ( user id @$ null ) ; },lock takes time
to do : handle if user does n't provide k best,if ( flags . usek best ) { int k = flags . k best ; crf . classify and write answersk best ( test file @$ k @$ reader and writer ) ; } else if ( flags . print label value ) { crf . print label information ( test file @$ reader and writer ) ; } else { log . info ( __str__ ) ; crf . classify and write answers ( test file @$ reader and writer @$ true ) ; },user provide k
triggering the task should start the child case instance,cmmn runtime service . trigger plan item instance ( plan item instance . get id ( ) ) ; assert equals ( __num__ @$ cmmn runtime service . create case instance query ( ) . count ( ) ) ; assert equals ( __str__ @$ cmmn history service . create historic variable instance query ( ) . case instance id ( case instance . get id ( ) ) . variable name ( __str__ ) . single result ( ) . get value ( ) ) ;,task start instance
this should n't throw an exception !,m web view update service impl . notify relro creation completed ( ) ; web view provider response response = m web view update service impl . wait for and get provider ( ) ; assert equals ( web view factory . libload_failed_listing_webview_packages @$ response . status ) ;,this throw exception
the handshake reply is n't sent until we have processed the message,context supplier . get ( ) . set packet handled ( true ) ; if ( ! continue handshake ) { logger . error ( fmlhsmarker @$ __str__ ) ; } else { fml network constants . handshake channel . reply ( new fml handshake messages . c2s acknowledge ( ) @$ context supplier . get ( ) ) ; },we processed message
see if we can find a file in the default location,if ( filename == null ) { url path to jar = this . get class ( ) . get protection domain ( ) . get code source ( ) . get location ( ) ; string tmp = null ; try { tmp = new file ( path to jar . touri ( ) ) . get parent ( ) + file . separator + default_prop_filename ; } catch ( exception e ) { tmp = null ; } filename = tmp ; },we find file
this should cause red asterisk to the vertical layout,table . set required ( true ) ; table . add value change listener ( event -> { object value = table . get value ( ) ; if ( value != null ) { notification . show ( __str__ ) ; } else { notification . show ( __str__ ) ; } } ) ;,this cause asterisk
the file in snapshot should n't have any encryption info,final path snapshotted zone file = new path ( snap1 + __str__ + zone . get name ( ) + __str__ + zone file . get name ( ) ) ; file encryption info fe info = get file encryption info ( snapshotted zone file ) ; assert null ( __str__ @$ fe info ) ; assert equals ( __str__ @$ contents @$ dfs test util . read file ( fs @$ snapshotted zone file ) ) ;,file have info
so we construct a generic hash using two long values,final string s = ( base64 order . enhanced coder . encode longsb ( math . abs ( r0 ) @$ __num__ ) . to string ( ) + base64 order . enhanced coder . encode longsb ( math . abs ( r1 ) @$ __num__ ) . to string ( ) ) ; return ascii . get bytes ( s ) ;,we construct hash
if the user sets a message id @$ use it .,final string message id = one way feature . get message id ( ) ; if ( ! is message id added && message id != null ) { headers . add ( new string header ( av . messageid tag @$ message id ) ) ; },user sets id
cell is correct but it does not have focused style,if ( cell with focus style != cell . get element ( ) ) { if ( cell with focus style != null ) { set style name ( cell with focus style @$ cell focus style name @$ false ) ; } cell with focus style = cell . get element ( ) ; set style name ( cell with focus style @$ cell focus style name @$ true ) ; },it focused style
make build sleep a while so it blocks new builds,project . get builders list ( ) . add ( new test builder ( ) { public boolean perform ( abstract build < ? @$ ? > build @$ launcher launcher @$ build listener listener ) throws interrupted exception @$ io exception { build started . signal ( ) ; build should complete . block ( ) ; return true ; } } ) ;,it blocks builds
we have a classloader @$ but does it have this plugin ?,try { ucl . load class ( plugin . get class map ( ) . values ( ) . iterator ( ) . next ( ) ) ; } catch ( class not found exception ignored ) { add to class loader ( plugin @$ ( kettleurl class loader ) ucl ) ; },it have classloader
we have cached type name as symbol in reference type,while ( iter . has next ( ) ) { reference type impl type = ( reference type impl ) iter . next ( ) ; if ( type name sym . equals ( type . type name as symbol ( ) ) ) { list . add ( type ) ; } },we cached name
let 's join these two data sets together @$ by customer id,join join = new join . builder ( join . join type . inner ) . set join columns ( __str__ ) . set schemas ( customer info schema @$ customer purchases schema ) . build ( ) ; javardd < list < writable > > joined data = spark transform executor . execute join ( join @$ customer info @$ purchase info ) ; list < list < writable > > joined data list = joined data . collect ( ) ;,'s join sets
check @$ that the count call also returns 1,rest quote mock mvc . perform ( get ( __str__ + filter ) ) . and expect ( status ( ) . is ok ( ) ) . and expect ( content ( ) . content type ( media type . application_json_utf8_value ) ) . and expect ( content ( ) . string ( __str__ ) ) ;,call returns 1
trim off the \n since println will do it for us,if ( end char == __str__ ) { word end -- ; if ( word end > __num__ && target . char at ( word end - __num__ ) == __str__ ) { word end -- ; } } else if ( end char == __str__ ) { line length += __num__ ; },println do it
... except for the ui ds that have allow rules .,synchronized ( m rules lock ) { final sparse int array rules = get uid firewall ruleslr ( chain ) ; exempt uids = new int [ rules . size ( ) ] ; for ( int i = __num__ ; i < exempt uids . length ; i ++ ) { if ( rules . value at ( i ) == firewall_rule_allow ) { exempt uids [ num uids ] = rules . key at ( i ) ; num uids ++ ; } } },that allow rules
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
check that the build works with crates with duplicate names,assert that ( workspace . run buck command ( __str__ @$ __str__ ) . assert success ( __str__ ) . get stdout ( ) @$ matchers . all of ( contains string ( __str__ ) @$ contains string ( __str__ ) @$ contains string ( __str__ ) ) ) ;,the build works
start playback and wait until playback reaches second window .,action schedule action schedule = new action schedule . builder ( __str__ ) . pause ( ) . seek ( __num__ ) . wait for seek processed ( ) . seek ( __num__ ) . seek ( __num__ ) . wait for playback state ( player . state_ready ) . seek ( __num__ ) . play until start of window ( __num__ ) . seek ( __num__ ) . seek ( __num__ ) . play ( ) . build ( ) ;,playback reaches window
set custom logger that uses previous custom file handler for logging,op = util . create add operation ( addr_logger ) ; linked list < model node > handlers = new linked list < model node > ( ) ; handlers . add ( new model node ( file_handler_name ) ) ; op . get ( __str__ ) . set ( handlers ) ; management operations . execute operation ( management client . get controller client ( ) @$ op ) ;,that uses handler
test we can first start the zk cluster by itself,configuration conf = test_util . get configuration ( ) ; test_util . start minidfs cluster ( __num__ ) ; test_util . start minizk cluster ( ) ; conf . set int ( h constants . zk_session_timeout @$ __num__ ) ; conf . set class ( h constants . hbase_master_loadbalancer_class @$ mock load balancer . class @$ load balancer . class ) ;,we start cluster
we only do validation when the message should not be skipped .,if ( record . offset ( ) >= next fetch offset ) { maybe ensure valid ( record ) ; if ( ! current batch . is control batch ( ) ) { return record ; } else { next fetch offset = record . offset ( ) + __num__ ; } },we do validation
create a buffer that can only hold two pages,broadcast output buffer buffer = create broadcast buffer ( create initial empty output buffers ( broadcast ) @$ new data size ( page size * __num__ @$ byte ) @$ memory context @$ direct executor ( ) ) ; output buffer memory manager memory manager = buffer . get memory manager ( ) ; memory manager . set no block on full ( ) ;,that hold pages
expect that we fetch stats from the previous upstream .,in order . verify ( m hardware @$ times ( __num__ ) ) . get forwarded stats ( eq ( ethernet iface ) ) ; lp . set interface name ( ethernet iface ) ; offload . set upstream link properties ( lp ) ;,we fetch stats
create a zip file and get an input stream from it,for ( int i = __num__ ; i < jars . length + __num__ ; i ++ ) { zip file zf = new instrumented zip file ( jars [ rnd . next int ( jars . length ) ] ) ; zip entry ze = zf . get entry ( __str__ ) ; if ( ze != null ) { input stream is = zf . get input stream ( ze ) ; break ; } },zip file stream
... but the three requests that follow requests include an authorization header,for ( int i = __num__ ; i < __num__ ; i ++ ) { request = server . take request ( ) ; assert equals ( __str__ @$ request . get request line ( ) ) ; assert contains ( request . get headers ( ) @$ __str__ + simple authenticator . base_64_credentials ) ; },requests include header
those do n't have any effect ...,test harness . process watermark ( new watermark ( __num__ ) ) ; test harness . process watermark ( new watermark ( __num__ ) ) ; expected output . add ( new watermark ( __num__ ) ) ; expected output . add ( new watermark ( __num__ ) ) ; assertor . assert output equals sorted ( __str__ @$ expected output @$ test harness . get output ( ) ) ; test harness . close ( ) ;,those have effect
verify that nn binds wildcard address now .,try { cluster = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) . build ( ) ; cluster . wait active ( ) ; string address = get lifeline rpc server address ( cluster ) ; assert that ( __str__ + address + __str__ @$ address @$ is ( __str__ + wildcard_address ) ) ; } finally { if ( cluster != null ) { cluster . shutdown ( ) ; } },binds wildcard address
check that changed returns correct rollback .,assert equals ( __num__ @$ changed . size ( ) ) ; assert true ( changed . contains ( data with pending backup ) ) ; assert true ( changed . contains ( data with recent restore ) ) ; assert true ( changed . contains ( data for restore ) ) ;,check returns rollback
this reverses the positioning shown in on layout .,if ( check drawer view absolute gravity ( changed view @$ gravity . left ) ) { offset = ( float ) ( child width + left ) / child width ; } else { final int width = get width ( ) ; offset = ( float ) ( width - left ) / child width ; } set drawer view offset ( changed view @$ offset ) ; changed view . set visibility ( offset == __num__ ? invisible : visible ) ; invalidate ( ) ;,this reverses positioning
for lists @$ we will be appending entries,object ob = mapper . reader for updating ( strs ) . read value ( __str__ ) ; assert same ( strs @$ ob ) ; assert equals ( __num__ @$ strs . size ( ) ) ; assert equals ( __str__ @$ strs . get ( __num__ ) ) ; assert equals ( __str__ @$ strs . get ( __num__ ) ) ; assert equals ( __str__ @$ strs . get ( __num__ ) ) ; assert equals ( __str__ @$ strs . get ( __num__ ) ) ;,we appending entries
propagate delegation related props from launcher job to mr job,if ( user provider . is hadoop security enabled ( ) ) { if ( system . getenv ( __str__ ) != null ) { job . get configuration ( ) . set ( __str__ @$ system . getenv ( __str__ ) ) ; } },delegation related props
see if we have input streams leading to this step !,if ( input . length > __num__ ) { cr = new check result ( check result interface . type_result_ok @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; } else { cr = new check result ( check result interface . type_result_error @$ base messages . get string ( pkg @$ __str__ ) @$ step meta ) ; remarks . add ( cr ) ; },we have input
lift violated policy to prevent infinite recursion .,s vm policy . m callback executor . execute ( ( ) -> { vm policy old policy = allow vm violations ( ) ; try { listener . on vm violation ( violation ) ; } finally { set vm policy ( old policy ) ; } } ) ;,lift violated policy
split it using a combined file input format,in format = new dummy input format ( ) ; file input format . set input paths ( job @$ dir1 + __str__ + dir2 ) ; in format . set min split size rack ( f1 . get len ( ) ) ; splits = in format . get splits ( job ) ; system . out . println ( __str__ + splits . size ( ) ) ;,it using format
the existing file should not add properties that already exist,file = load file ( __str__ ) ; assertions . assert true ( file . contains ( __str__ ) ) ; assertions . assert false ( file . contains ( __str__ ) ) ; assertions . assert true ( file . contains ( __str__ ) ) ; assertions . assert true ( file . contains ( __str__ ) ) ;,file add properties
skip const as it automatically meets the criteria .,js doc info info = n . getjs doc info ( ) ; if ( info != null && info . has const annotation ( ) ) { break ; } for ( node name : node util . find lhs nodes in node ( n ) ) { if ( convention . is constant ( name . get string ( ) ) ) { t . report ( name @$ missing_const_property @$ name . get string ( ) ) ; } } break ; default : break ;,it meets criteria
if we get here then send reply possibly with an exception,if ( send failure message ) { if ( thr != null ) { rex = new reply exception ( thr ) ; } if ( rex == null ) { rex = new reply exception ( __str__ ) ; } filter info message . send ( dm @$ get sender ( ) @$ this . processor id @$ ( local region ) lcl rgn @$ rex ) ; },we send reply
media metadata supports all the same fields as metadata editor,if ( m metadata builder != null ) { string metadata key = media metadata . get key from metadata editor key ( key ) ; if ( metadata key != null ) { m metadata builder . put bitmap ( metadata key @$ bitmap ) ; } },metadata supports fields
the next name may be unnecessary escaped . save the last recorded path name @$ so that we can restore the peek state in case we fail to find a match .,string last path name = path names [ stack size - __num__ ] ; string next name = next name ( ) ; result = find name ( next name @$ options ) ; if ( result == - __num__ ) { peeked = peeked_buffered_name ; peeked string = next name ; path names [ stack size - __num__ ] = last path name ; },we restore state
