then reading the value one more time should still yield the set <PLACE_HOLDER>,assert true ( array util . equals ( value @$ node1 . get property ( key ) ) @$ format ( __str__ @$ strings . pretty print ( value ) @$ strings . pretty print ( read value ) ) ) ; transaction . commit ( ) ;,time yield
parent needs <PLACE_HOLDER> if it is n't relative ? connector itself needs <PLACE_HOLDER> if it is n't undefined ? children does n't care ?,js array string needs size = js array object . create array ( ) . cast ( ) ; if ( ! is undefined in direction ( connector @$ direction ) ) { needs size . push ( connector . get connector id ( ) ) ; } if ( ! is relative in direction ( connector @$ direction ) ) { server connector parent = connector . get parent ( ) ; if ( parent instanceof component connector ) { needs size . push ( parent . get connector id ( ) ) ; } } return needs size ;,itself needs
head requests should not set a content length by passing it through this api @$ but should instead manually set the required <PLACE_HOLDER> .,if ( is head request ( ) ) { if ( content len >= __num__ ) { final logger logger = server . get logger ( ) ; string msg = __str__ ; logger . warning ( msg ) ; } no content to send = true ; content len = __num__ ; } else { if ( content len == __num__ ) { if ( http10 ) { o . set wrapped stream ( new undef length output stream ( this @$ ros ) ) ; close = true ; } else { rsp hdrs . set ( __str__ @$ __str__ ) ; o . set wrapped stream ( new chunked output stream ( this @$ ros ) ) ; } } else { if ( content,requests set
if declaration does not provide <PLACE_HOLDER> @$ there is no connection to make,if ( usage == null || declaration == null || declaration . length == __num__ ) return ; if ( usage . length != declaration . length ) return ;,declaration provide
now let 's load the various <PLACE_HOLDER> we 're passing we start at index 2 because the first variable we pass is the owner instance and at this point it is already on the stack,for ( int i = __num__ ; i < local variable params . length ; i ++ ) { parameter param = local variable params [ i ] ; string name = param . get name ( ) ; load reference ( name @$ controller ) ; if ( param . get node meta data ( closure writer . use existing reference . class ) == null ) { param . set node meta data ( closure writer . use existing reference . class @$ boolean . true ) ; } },"s" load
peer 2 advertises the <PLACE_HOLDER> but does not receive it yet .,inbound ( p2 @$ inv ) ; assert true ( outbound ( p2 ) instanceof get data message ) ; assert equals ( __num__ @$ tx . get confidence ( ) . num broadcast peers ( ) ) ; assert null ( event [ __num__ ] ) ;,peer advertises
when partition could not be fetched from metastore @$ it is not known whether the partition was added . deleting the partition when aborting commit has the <PLACE_HOLDER> of deleting partition not added in this transaction . not deleting the partition may leave garbage behind . the former is much more dangerous than the latter . therefore @$ the partition is not added to,batch completely added = false ;,commit has
if we have parents combine their <PLACE_HOLDER>,while ( parent node != null ) { if ( parent == current spatial ) { transform trans = new transform ( ) ; trans . set scale ( current spatial . get local scale ( ) ) ; shape transform . combine with parent ( trans ) ; parent node = null ; } else { shape transform . combine with parent ( current spatial . get local transform ( ) ) ; parent node = current spatial . get parent ( ) ; current spatial = parent node ; } },parents combine
do n't allow partitioned tables in subqueries . this restriction stems from the lack of confidence that the planner can reliably identify all cases of adequate and inadequate partition key join <PLACE_HOLDER> across different levels of correlated subqueries .,if ( ! m_partitioning . was specified as single ( ) ) { for ( abstract expression e : subquery exprs ) { assert ( e instanceof select subquery expression ) ; select subquery expression sub expr = ( select subquery expression ) e ; if ( ! sub expr . get subquery scan ( ) . get is replicated ( ) ) { m_recent error msg = in_exists_scalar_error_message ; return null ; } } },cases join
bn has a trailing <PLACE_HOLDER> : the web sphere class loader would return null for a raw directory name without it .,try { enumeration < url > urls = root . get resources ( bn ) ; if ( urls == null ) { return null ; } url visitor v = new url visitor ( ) { @ override public void visit ( string s ) { if ( s . ends with ( __str__ ) ) { string locstr = s . substring ( __num__ @$ s . length ( ) - __num__ ) ; names . add ( locstr ) ; } } } ; while ( urls . has more elements ( ) ) { url url = urls . next element ( ) ; url handler handler = url handler . get ( url ) ; if ( handler != null ) { handler,bn has
if this project has correlated <PLACE_HOLDER> @$ create value generator and produce the correlated variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel ) ; },project correlated
when visibility changes and the user has a <PLACE_HOLDER> selected @$ unselect it and make sure their callback gets called .,if ( changed view == this && visibility != visible && m grabbed state != on trigger listener . no_handle ) { cancel grab ( ) ; },changes has
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
match the container @$ to reduce the risk of issues . the preview should never be drawn while the surface has this <PLACE_HOLDER> .,if ( surface rect == null ) { surface view . layout ( __num__ @$ __num__ @$ get width ( ) @$ get height ( ) ) ; } else { surface view . layout ( surface rect . left @$ surface rect . top @$ surface rect . right @$ surface rect . bottom ) ; },surface has
a window frame may contain just the start <PLACE_HOLDER> or in the between style of expressing a window frame both boundaries are specified .,boundary spec start = process boundary ( ( ast node ) node . get child ( __num__ ) ) ; if ( node . get child count ( ) > __num__ ) { end = process boundary ( ( ast node ) node . get child ( __num__ ) ) ; },frame contain
create a walker which walks the <PLACE_HOLDER> in a dfs manner while maintaining the operator stack .,map < rule @$ node processor > op rules = new linked hash map < rule @$ node processor > ( ) ; op rules . put ( new rule reg exp ( __str__ @$ app master event operator . get operator name ( ) + __str__ ) @$ new remove dynamic pruning by size ( ) ) ;,which walks
we accept the case that a crl issuer provide status <PLACE_HOLDER> for itself .,if ( ! certakid . equals ( crlakid ) ) { if ( issues ( cert impl @$ crl impl @$ provider ) ) { prev key = cert impl . get public key ( ) ; } else { indirectcrl = true ; } },issuer provide
see if the avro schema has any fields that are n't in the record schema @$ and if those fields have a default <PLACE_HOLDER> then we want to populate it in the generic record being produced,for ( final field field : avro schema . get fields ( ) ) { final optional < record field > record field = record schema . get field ( field . name ( ) ) ; if ( ! record field . is present ( ) && rec . get ( field . name ( ) ) == null && field . default val ( ) != null ) { rec . put ( field . name ( ) @$ field . default val ( ) ) ; } } return rec ;,fields have
zip files have 2 second <PLACE_HOLDER> .,cal . set ( calendar . second @$ rand . next int ( __num__ ) * __num__ ) ;,files have
we 'll never have two dots nor will a type name end or begin with dot . so no need to consider <PLACE_HOLDER> at the beginning @$ end @$ or adjacent to dots .,if ( index == __num__ || index == class name . length ( ) - __num__ || current . char at ( index - __num__ ) == __str__ || current . char at ( index + __num__ ) == __str__ ) { search start = index + __num__ ; continue ; } return index ;,need consider
when layout is frozen @$ rv does not intercept the motion event . a child view e.g . a button may still get the <PLACE_HOLDER> .,if ( m layout frozen ) { return false ; },button get
if execute <PLACE_HOLDER>ong is used for the initia<PLACE_HOLDER> execution of the node and the uninitia<PLACE_HOLDER>ized case is not checked then this execute <PLACE_HOLDER>ong method might return 0 <PLACE_HOLDER> instead of 2 <PLACE_HOLDER>. this test verifies that this particu<PLACE_HOLDER>ar case does not happen .,assert . assert equals ( __num__ @$ node . execute long ( truffle . get runtime ( ) . create virtual frame ( new object [ ] { __num__ } @$ new frame descriptor ( ) ) ) ) ;,method return
table may or may not be using <PLACE_HOLDER> . only the ser de can tell us .,abstract ser de deserializer = null ; try { class < ? > clazz = conf . get class by name ( serde lib ) ; if ( ! abstract ser de . class . is assignable from ( clazz ) ) { return true ; } deserializer = reflection util . new instance ( conf . get class by name ( serde lib ) . as subclass ( abstract ser de . class ) @$ conf ) ; } catch ( exception ex ) { log . warn ( __str__ + serde lib + __str__ @$ ex ) ; return true ; },table using
get base xml file result set <PLACE_HOLDER>,j2d analyzer . read results ( basexml file name ) ; j2d analyzer . single result set holder basesrsh = ( j2d analyzer . single result set holder ) j2d analyzer . results . element at ( __num__ ) ; enumeration base enum_ = basesrsh . get key enumeration ( ) ; vector base keyvector = new vector ( ) ; while ( base enum_ . has more elements ( ) ) { base keyvector . add ( base enum_ . next element ( ) ) ; } string base keys [ ] = new string [ base keyvector . size ( ) ] ; base keyvector . copy into ( base keys ) ; j2d analyzer . sort ( base keys ) ;,xml file
client can still write <PLACE_HOLDER> even though server is closed ? ? ?,client . get output stream ( ) . write ( __num__ ) ;,client write
gathering geometries in the sub graph . this must be done in the update phase as the gathering might add a matparam <PLACE_HOLDER>,targets . clear ( ) ; this . spatial . depth first traversal ( target locator ) ;,gathering add
test that the commit correctly created the <PLACE_HOLDER> in the region,assert not null ( my region . get ( not affected key ) ) ; assert null ( my region . get ( key1 ) ) ; assert true ( my region . contains key ( key1 ) ) ; assert null ( my region . get ( key2 ) ) ; assert true ( my region . contains key ( key2 ) ) ;,commit created
nb . this is always encoded with the implicit tag the checks only make <PLACE_HOLDER> if we assume implicit tagging @$ with explicit tagging the form is always constructed .,for ( int i = __num__ ; i < seq . length ; i ++ ) { der value opt = seq [ i ] ; if ( opt . is context specific ( tag_before ) && ! opt . is constructed ( ) ) { if ( not before != null ) { throw new certificate parsing exception ( __str__ ) ; } opt . reset tag ( der value . tag_ generalized time ) ; str = new der input stream ( opt . to byte array ( ) ) ; not before = str . get generalized time ( ) ; } else if ( opt . is context specific ( tag_after ) && ! opt . is constructed ( ) ) { if (,checks make
if the <PLACE_HOLDER> has no requested minimal size @$ we 'd like to enforce a minimal size so that the user can not render the <PLACE_HOLDER> too small to manipulate . we do n't need to do this for the pinned stack as the bounds are controlled by the system .,if ( ! in pinned windowing mode ( ) && m stack != null ) { final int default min size dp = m service . m root activity container . m default min size of resizeable task dp ; final activity display display = m service . m root activity container . get activity display ( m stack . m display id ) ; final float density = ( float ) display . get configuration ( ) . density dpi / display metrics . density_default ; final int default min size = ( int ) ( default min size dp * density ) ; if ( min width == invalid_min_size ) { min width = default min size ; } if ( min height == invalid_min_size ),user render
set up an empty panel to be used as a gui place holder when a conflict in another merger already made a <PLACE_HOLDER> that resolved the conflict .,empty conflict panel = new vertical choices panel ( ) ; empty conflict panel . clear ( ) ;,conflict made
any attempt to modify keys will cause unsupported operation <PLACE_HOLDER>,socket channel sc = socket channel . open ( ) ; sc . configure blocking ( false ) ; selection key key3 = sc . register ( selector @$ selection key . op_read ) ; try { key set2 . add ( key3 ) ; fail ( __str__ ) ; } catch ( unsupported operation exception e ) { },attempt cause
only map owned ref and relationship <PLACE_HOLDER> when follow references is set to true,if ( entity with ext info == null ) { entity with ext info = entity graph retriever . to atlas entity with ext info ( guid @$ ! follow references ) ; if ( entity with ext info != null ) { context . cache ( entity with ext info ) ; if ( log . is debug enabled ( ) ) { log . debug ( __str__ @$ guid ) ; } } },map owned
intern @$ as most entities have multiple surface forms add the <PLACE_HOLDER>,dictionary . put ( surface form @$ link ) ; i += __num__ ;,forms add
test that the module map action contains the header tree <PLACE_HOLDER> as both the public header and part of the action inputs .,assert that ( module map action . get public headers ( ) ) . contains ( headers ) ; assert that ( module map action . get inputs ( ) ) . contains ( headers ) ; action execution context dummy action execution context = new action execution context ( null @$ null @$ action input prefetcher . none @$ action key context @$ null @$ null @$ null @$ immutable map . of ( ) @$ immutable map . of ( ) @$ dummy_artifact_expander @$ null @$ null ) ; byte array output stream module map stream = new byte array output stream ( ) ; byte array output stream umbrella header stream = new byte array output stream ( ) ; module map action . new,action contains
blob got <PLACE_HOLDER>,assert equals ( constants . sqlite_blob @$ st . column_type ( __num__ ) ) ;,blob got
contract created by create 2 @$ does n't have <PLACE_HOLDER>,assert . assert equals ( __num__ @$ smart contract . get abi ( ) . get entrys count ( ) ) ;,contract have
count timer fired <PLACE_HOLDER>,int timer fired count = __num__ ; list < flowable event > events received = listener . get events received ( ) ; for ( flowable event event received : events received ) { if ( flowable engine event type . timer_fired == event received . get type ( ) ) { timer fired count ++ ; } } listener . clear events received ( ) ; assert equals ( __num__ @$ timer fired count ) ; process engine configuration . reset clock ( ) ;,timer fired
if we do n't support md 5 then the client must choose an <PLACE_HOLDER> as we can not fall back to md 5 .,if ( ! supported algorithms . contains ( digest algorithm . md5 ) ) { mandatory tokens . add ( digest authorization token . algorithm ) ; },client choose
maven turns empty <PLACE_HOLDER> into null entries @$ and its possible to have empty <PLACE_HOLDER> in jib.to.<PLACE_HOLDER>,session properties . put ( __str__ @$ __str__ ) ; try { test plugin configuration . get target image additional tags ( ) ; assert . fail ( ) ; } catch ( illegal argument exception ex ) { assert . assert equals ( __str__ @$ ex . get message ( ) ) ; },maven turns
this is used to set data type @$ which defines a python <PLACE_HOLDER> of classes,string full suffix = suffix ; if ( __str__ . equals ( suffix ) ) { full suffix = __str__ + suffix ; } if ( model utils . is nullable ( p ) ) { full suffix = __str__ + suffix ; } if ( model utils . is free form object ( p ) && model utils . get additional properties ( p ) == null ) { return prefix + __str__ + full suffix ; } if ( ( model utils . is map schema ( p ) || p . get type ( ) == __str__ ) && model utils . get additional properties ( p ) != null ) { schema inner = model utils . get additional properties ( p ) ;,which defines
in order to find out whether the divide generates the exact <PLACE_HOLDER> @$ we avoid calling the above divide method . 'quotient ' holds the return big decimal object whose scale will be set to 'scl ' .,big decimal quotient ; int scl = check scale non zero ( preferred scale + yscale - xscale + mcp ) ; if ( check scale non zero ( ( long ) mcp + yscale - xscale ) > __num__ ) { int raise = check scale non zero ( ( long ) mcp + yscale - xscale ) ; big integer rb = big multiply power ten ( xs @$ raise ) ; quotient = divide and round ( rb @$ ys @$ scl @$ rounding mode @$ check scale non zero ( preferred scale ) ) ; } else { int new scale = check scale non zero ( ( long ) xscale - mcp ) ; int raise = check scale non zero ( (,divide generates
note : as of this writing @$ analysis was placing a <PLACE_HOLDER> from this address . the test is also placing a <PLACE_HOLDER> there . drop the analysis ref and keep the test ref .,programdb p = builder . get program ( ) ; reference manager rm = p . get reference manager ( ) ; reference [ ] refs = rm . get references from ( builder . addr ( __str__ ) ) ; for ( reference reference : refs ) { builder . delete reference ( reference ) ; } builder . create memory reference ( __str__ @$ __str__ @$ ref type . write @$ source type . user_defined @$ __num__ ) ; builder . add function variable ( func @$ new local variable impl ( __str__ @$ __num__ @$ byte data type . data type @$ builder . get register ( __str__ ) @$ builder . get program ( ) ) ) ; builder . create function ( __str__,test placing
the code is written so that even completely incorrect approximations will still yield the correct <PLACE_HOLDER> eventually @$ but in practice this branch should almost never be entered @$ and even then the loop should not run more than once .,if ( approx cmp > __num__ ) { do { approx log10 -- ; approx pow = approx pow . divide ( big integer . ten ) ; approx cmp = approx pow . compare to ( x ) ; } while ( approx cmp > __num__ ) ; } else { big integer next pow = big integer . ten . multiply ( approx pow ) ; int next cmp = next pow . compare to ( x ) ; while ( next cmp <= __num__ ) { approx log10 ++ ; approx pow = next pow ; approx cmp = next cmp ; next pow = big integer . ten . multiply ( approx pow ) ; next cmp = next pow . compare to (,approximations yield
no awb modes <PLACE_HOLDER> ? that 's unpossible !,if ( awb avail == null || awb avail . size ( ) == __num__ ) { log . w ( tag @$ __str__ ) ; awb avail = new array list < integer > ( __num__ ) ; awb avail . add ( control_awb_mode_auto ) ; },awb modes
the contact already exist its <PLACE_HOLDER>,logger . info ( __str__ ) ; object o = new object ( ) ; synchronized ( o ) { o . wait ( __num__ ) ; },contact exist
set the text to an invalid value @$ which will trigger an <PLACE_HOLDER> and an error message,set offset ( offset @$ __str__ ) ; check offset fields enabled state ( info fields @$ true ) ; check offset fields enabled state ( offset buttons @$ false ) ;,which trigger
if the table has property external <PLACE_HOLDER> @$ update table type accordingly,string table type = tbl . get table type ( ) ; boolean is external = boolean . parse boolean ( tbl . get parameters ( ) . get ( __str__ ) ) ; if ( table type . managed_table . to string ( ) . equals ( table type ) ) { if ( is external ) { table type = table type . external_table . to string ( ) ; } } if ( table type . external_table . to string ( ) . equals ( table type ) ) { if ( ! is external ) { table type = table type . managed_table . to string ( ) ; } } principal type owner principal type = tbl . get owner type ( ),table has
first line : sentence <PLACE_HOLDER> for all nodes ; we recover the words from the original tokens the first two tokens in this line indicate : docid @$ sentence <PLACE_HOLDER>,for ( indexed word node : graph . vertex set ( ) ) { if ( ! output header ) { string doc id = node . get ( core annotations . docid annotation . class ) ; if ( doc id != null && doc id . length ( ) > __num__ ) pw . print ( doc id ) ; else pw . print ( __str__ ) ; pw . print ( __str__ ) ; pw . print ( node . get ( core annotations . sentence index annotation . class ) ) ; output header = true ; } pw . print ( __str__ ) ; pw . print ( node . index ( ) ) ; if ( node . copy count ( ),tokens indicate
server has sent <PLACE_HOLDER> since last check,if ( last bytes written . compare and set ( previous bytes @$ current bytes ) ) { return false ; },server sent
we are only running remotely if both the distribution is there and if the distribution is actually contains <PLACE_HOLDER> .,clustered partitioning = trans meta . get slave step copy partition distribution ( ) != null && ! trans meta . get slave step copy partition distribution ( ) . get distribution ( ) . is empty ( ) ;,distribution contains
this category node uses organization <PLACE_HOLDER>,return organization node . comparator ;,node uses
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,return super . load symlinks ( ) ;,tools |
composition needs one <PLACE_HOLDER> to be the container .,if ( relationship category == relationship category . composition ) { throw new atlas base exception ( atlas error code . relationshipdef_composition_no_container @$ name ) ; } else if ( relationship category == relationship category . aggregation ) { throw new atlas base exception ( atlas error code . relationshipdef_aggregation_no_container @$ name ) ; },composition needs
thread polling every 5 seconds to update the <PLACE_HOLDER> set seconds which is used in filter <PLACE_HOLDER>s bolt to filter the <PLACE_HOLDER>s,try { if ( ! poll ) { word set = parse file ( file name ) ; poll time = system . current time millis ( ) ; poll = true ; } else { if ( ( system . current time millis ( ) - poll time ) > __num__ ) { word set = parse file ( file name ) ; poll time = system . current time millis ( ) ; } } } catch ( io exception exp ) { throw new runtime exception ( exp ) ; } if ( word set != null && ! word set . contains ( word ) ) { collector . emit ( new values ( word ) ) ; },seconds update
rows to skip <PLACE_HOLDER>,label wl rows to skip = new label ( shell @$ swt . right ) ; wl rows to skip . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl rows to skip ) ; form data fdl rows to skip = new form data ( ) ; fdl rows to skip = new form data ( ) ; fdl rows to skip . left = new form attachment ( __num__ @$ __num__ ) ; fdl rows to skip . top = new form attachment ( last control @$ margin ) ; fdl rows to skip . right = new form attachment ( middle @$ - margin ) ; wl rows to skip . set layout,rows skip
use the messages hostname as default domain when generating set <PLACE_HOLDER>,for ( http cookie c : msg . get response header ( ) . get http cookies ( msg . get request header ( ) . get host name ( ) ) ) session . get http state ( ) . add cookie ( convert cookie ( c ) ) ; return session ;,generating set
original has changed @$ but snapshot still has old <PLACE_HOLDER> .,do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ; restart ( false ) ; do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ; restart ( true ) ; do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ;,snapshot has
unix domain socket address . create the underlying <PLACE_HOLDER> for the unix domain socket .,if ( value . starts with ( unix_domain_socket_prefix ) ) { string file path = value . substring ( unix_domain_socket_prefix . length ( ) ) ; file file = new file ( file path ) ; if ( ! file . is absolute ( ) ) { throw new illegal argument exception ( __str__ + file path ) ; } try { if ( file . create new file ( ) ) { file . delete on exit ( ) ; } } catch ( io exception ex ) { throw new runtime exception ( ex ) ; } return new domain socket address ( file ) ; } else { string [ ] parts = value . split ( __str__ @$ __num__ ) ; if ( parts,address create
new file @$ should preserve <PLACE_HOLDER>,string r = test col . get media ( ) . add file ( path ) ; assert equals ( __str__ @$ r ) ;,new preserve
since b and c have the same <PLACE_HOLDER> @$ we can expect them to appear in either order,assertion . has table section ( ) . has column ( __str__ ) . contains exactly ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,b have
schedule low pri first . when high pri is scheduled @$ it takes away the <PLACE_HOLDER> from the low pri task . when the high pri finishes @$ low pri gets the <PLACE_HOLDER> back .,try { priority high pri = priority . new instance ( __num__ ) @$ low pri = priority . new instance ( __num__ ) ; tez task attemptid task1 = test task scheduler service wrapper . generate task attempt id ( ) @$ task2 = test task scheduler service wrapper . generate task attempt id ( ) ; ts wrapper . ts . update guaranteed count ( __num__ ) ; ts wrapper . control scheduler ( true ) ; ts wrapper . allocate task ( task1 @$ null @$ low pri @$ new object ( ) ) ; ts wrapper . await total task allocations ( __num__ ) ; task info ti1 = ts wrapper . ts . get task info ( task1 ) ; assert true (,pri gets
now @$ fsck should show healthy fs and should not show any open <PLACE_HOLDER>,out str = run fsck ( conf @$ __num__ @$ true @$ top dir ) ; system . out . println ( out str ) ; assert true ( out str . contains ( namenode fsck . healthy_status ) ) ; assert false ( out str . contains ( __str__ ) ) ; assert false ( out str . contains ( __str__ ) ) ; util . cleanup ( fs @$ top dir ) ;,fsck show
the full <PLACE_HOLDER> should not contain the organization folder <PLACE_HOLDER>,map links ; for ( map map : pipelines ) { assert . assert equals ( __str__ @$ map . get ( __str__ ) ) ; if ( map . get ( __str__ ) . equals ( __str__ ) ) { map . get ( __str__ ) . equals ( __str__ ) ; map . get ( __str__ ) . equals ( __str__ ) ; check links ( ( map ) map . get ( __str__ ) @$ __str__ ) ; } else if ( map . get ( __str__ ) . equals ( __str__ ) ) { map . get ( __str__ ) . equals ( __str__ ) ; map . get ( __str__ ) . equals ( __str__ ) ; check links ( ( map ),name contain
distributed system.disconnect may have already destroyed the <PLACE_HOLDER>,distributed lock service . destroy ( abstract gateway sender . lock_service_name ) ;,system.disconnect destroyed
begin @$ do and if increases block <PLACE_HOLDER>,if ( ( __str__ . equals ( keyword . get text ( ) ) || __str__ . equals ( keyword . get text ( ) ) || __str__ . equals ( keyword . get text ( ) ) && ! __str__ . equals ( previous keyword . get text ( ) ) ) ) { context . increase block depth ( ) ; } else if ( __str__ . equals ( keyword . get text ( ) ) ) { context . decrease block depth ( ) ; },increases block
rvv does not contain this <PLACE_HOLDER> so it is retained,result . add ( id ) ;,rvv contain
the added node adds a new <PLACE_HOLDER> .,return true ;,node adds
windows has a <PLACE_HOLDER>,file = file . replace ( __str__ @$ __str__ ) ; known paths . add ( file ) ;,windows has
nobody understood this <PLACE_HOLDER>,throw new web service exception ( __str__ + lexical ) ;,nobody understood
normalization will remove <PLACE_HOLDER> @$ so no uplevel references left,assert that ( create ( __str__ ) . contains uplevel references ( ) ) . is false ( ) ;,normalization remove
update the switches insode the <PLACE_HOLDER> 200 switch program service,e service . update errors ( node root ) ;,switches insode
id types can use int <PLACE_HOLDER> .,if ( type == scalars . graphqlid && string value . matches ( __str__ ) ) { return int value . new int value ( ) . value ( new big integer ( string value ) ) . build ( ) ; },types use
without the metadata the status and health check ur ls will not be set and the status page and health check url paths will not include the context path so set <PLACE_HOLDER> here,if ( string utils . has text ( management context path ) ) { instance . set health check url path ( management context path + instance . get health check url path ( ) ) ; instance . set status page url path ( management context path + instance . get status page url path ( ) ) ; },here set
quick answers requires the js <PLACE_HOLDER> .,return contextual search field trial . is quick answers enabled ( ) ;,answers requires
legacy security behavior : setup the security context if a sas current is available and invoke the component . one of the ejb security interceptors will authenticate and authorize the <PLACE_HOLDER> .,security context legacy context = null ; if ( this . legacy security domain != null && ( identity principal != null || principal != null ) ) { final object final credential = identity principal != null ? this . sas current : credential ; final principal final principal = identity principal != null ? identity principal : principal ; if ( wild fly security manager . is checking ( ) ) { legacy context = access controller . do privileged ( ( privileged exception action < security context > ) ( ) -> { security context sc = security context factory . create security context ( this . legacy security domain ) ; sc . get util ( ) . create subject info ( final principal,behavior authenticate
continuous build environment does n't support localhost <PLACE_HOLDER> .,return ;,environment support
case insense . should include root @$ html @$ <PLACE_HOLDER>,elements p2 = doc . select ( __str__ ) ;,case include
note that this class does not support forward <PLACE_HOLDER> .,if ( ! forward ) { if ( trusted pub key != null ) { prev pub key = trusted pub key ; } else { prev pub key = null ; } } else { throw new cert path validator exception ( __str__ ) ; },class support
if the current wi is a quantity @$ we add it to the collector . if its the first word in a quantity @$ we record <PLACE_HOLDER> before it,if ( quantifiable . contains ( curr ner tag ) ) { if ( collector . is empty ( ) ) { before index = i - __num__ ; } collector . add ( wi ) ; } prev ner tag = curr ner tag ;,word record
server and cluster has the same <PLACE_HOLDER>,cluster props . clear ( ) ; server props . clear ( ) ; cluster props . set property ( __str__ @$ __str__ ) ; server props . set property ( __str__ @$ __str__ ) ; assert that ( gem fire cache impl . is mis configured ( cluster props @$ server props @$ __str__ ) ) . is false ( ) ; cluster props . set property ( __str__ @$ __str__ ) ; server props . set property ( __str__ @$ __str__ ) ; assert that ( gem fire cache impl . is mis configured ( cluster props @$ server props @$ __str__ ) ) . is false ( ) ;,server has
make sure the given union <PLACE_HOLDER> has a corresponding tuple <PLACE_HOLDER> in the schema .,int union tag = value . get union tag ( ) ; if ( schema . size ( ) <= union tag ) { throw new illegal state exception ( __str__ + union tag + __str__ ) ; } list < object > value list = ( list < object > ) value map . get ( union tag ) ; value list . add ( value . get value ( ) ) ;,tag has
end playback @$ as we did n't manage to find a valid seek <PLACE_HOLDER> .,if ( period position us == c . time_unset ) { set state ( player . state_ended ) ; reset internal ( false @$ false @$ true @$ false @$ true ) ; } else { long new period position us = period position us ; if ( period id . equals ( playback info . period id ) ) { media period holder playing period holder = queue . get playing period ( ) ; if ( playing period holder != null && playing period holder . prepared && new period position us != __num__ ) { new period position us = playing period holder . media period . get adjusted seek position us ( new period position us @$ seek parameters ) ; } if (,playback seek
repl load during migration @$ commits the explicit <PLACE_HOLDER> and start some internal <PLACE_HOLDER>s . call release locks and commit or rollback to do the clean up .,if ( ! driver context . get txn manager ( ) . is txn open ( ) && driver context . get query state ( ) . get hive operation ( ) == hive operation . replload ) { release locks and commit or rollback ( false ) ; } else { },load commits
other task will now finish the process <PLACE_HOLDER>,org . flowable . task . api . task task = task service . create task query ( ) . process instance id ( pi . get id ( ) ) . task definition key ( __str__ ) . single result ( ) ; map < string @$ object > variables = new hash map < string @$ object > ( ) ; variables . put ( __str__ @$ __num__ ) ; task service . complete ( task . get id ( ) @$ variables ) ; assert equals ( __num__ @$ runtime service . create process instance query ( ) . process instance id ( pi . get id ( ) ) . count ( ) ) ;,task finish
a volt db <PLACE_HOLDER> to support indexed expressions and assume unique attribute .,c . set assume unique ( assume unique ) ; if ( has non column exprs ) { c = c . with expressions ( index exprs . to array ( new expression [ index exprs . size ( ) ] ) ) ; },volt db
end of stream read having not read a <PLACE_HOLDER> .,if ( result == c . result_buffer_read ) { assertions . check state ( flags only buffer . is end of stream ( ) ) ; input stream ended = true ; process end of stream ( ) ; return ; } else { return ; },end read
magic numbers @$ if anybody knows why @$ please tell <PLACE_HOLDER>,datfiles . put ( dat file name @$ new byte [ ] { ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$,numbers tell
and will not trigger fallback <PLACE_HOLDER>,verify ( failover service @$ never ( ) ) . get def user ( ) ;,and trigger
if rm is complaining about response id out of sync @$ force reset next <PLACE_HOLDER>,if ( t instanceof invalid application master request exception ) { int response id = amrm client utils . parse expected response id from exception ( t . get message ( ) ) ; if ( response id != - __num__ ) { this . reset response id = response id ; log . info ( __str__ + response id + __str__ + allocate request . get response id ( ) + __str__ + this . app id + __str__ ) ; } else { log . warn ( __str__ + this . app id ) ; } } throw t ;,rm complaining
special case @$ a collection with only a read <PLACE_HOLDER> we assume we can just add to the connection,if ( collection . class . is assignable from ( i . get property type ( ) ) ) { handled properties . add ( i . get name ( ) ) ; collection property value = ( collection ) i . read ( param ) ; if ( ! property value . is empty ( ) ) { list < deferred parameter > params = new array list < > ( ) ; for ( object c : property value ) { deferred parameter to add = load object instance ( c @$ existing @$ object . class ) ; params . add ( to add ) ; } setup steps . add ( new serialzation step ( ) { @ override public void handle ( method,case read
false when it should be true . therefore @$ if we are not on the shade @$ do n't even bother asking if the keyguard is showing . we still need to check it though because showing the <PLACE_HOLDER> on the keyguard has a state of shade but the keyguard is still showing .,final boolean showing keyguard = m state != status bar state . shade || m keyguard monitor . is showing ( ) ; final boolean device public = showing keyguard && is secure ( get current user id ( ) ) ;,keyguard showing
some binary files have <PLACE_HOLDER>,while ( c != __str__ ) { if ( c != __str__ ) sb . append ( c ) ; c = ( char ) buffer . get ( ) ; },files have
after all class information has been read @$ now we can safely inspect import information for errors . if we did this before all parsing was finished @$ we could get vicious circularities @$ since files can import each others ' <PLACE_HOLDER> .,checked = - __num__ ;,files import
to find the existing room created with op set muc 1 @$ we will use op set muc <PLACE_HOLDER> to be sure the room will not be retrieved from op set muc 1 cache,try { found room = op setmuc2 . find room ( test room name ) ; } catch ( exception ex ) { logger . warn ( ex ) ; } assert not null ( __str__ @$ found room ) ; assert equals ( __str__ @$ op set1 room . get name ( ) @$ found room . get name ( ) ) ;,cache set
error generating x 500 <PLACE_HOLDER> object from the cert 's issuer dn @$ leave <PLACE_HOLDER> as is .,cert issuer names [ i ] = cert issuer name ;,error leave
rename lambda <PLACE_HOLDER> to reflect the new owner . not doing so confuses lambda desugaring if it 's run over this class again . lambda desugaring has already renamed the <PLACE_HOLDER> from its original name to include the interface name at this point .,if ( is lambda ) { return name + dependency collector . interface_companion_suffix ; },lambda renamed
this should typically never happen . cancelling work should remove alarms @$ but if an alarm has already fired @$ then fire a stop work request to remove the pending <PLACE_HOLDER> met command handler .,if ( work spec == null ) { stop work ( ) ; return ; },work remove
this statement can not be reached since the above method always throws an <PLACE_HOLDER> this is only here to silence the compiler and any warnings,return consumer records . empty ( ) ;,method throws
for sake of simplicity transform any <PLACE_HOLDER> in array,if ( result instanceof iterable < ? > ) { final list < object > list = new array list < object > ( ) ; for ( object o : ( iterable < object > ) result ) { list . add ( o ) ; } result = list . to array ( ) ; },sake transform
is this a returning <PLACE_HOLDER> ?,if ( branch target finder . is subroutine ( offset ) && branch target finder . is subroutine returning ( offset ) ) { if ( debug ) { system . out . println ( __str__ + instruction . to string ( offset ) ) ; } code attribute composer . append label ( offset ) ; } else { instruction . accept ( clazz @$ method @$ code attribute @$ offset @$ this ) ; },a returning
note to translators : the stylesheet referred to an <PLACE_HOLDER> to the xsl syntax and indicated that it was defined by xsltc @$ but xstlc does not recognized the particular <PLACE_HOLDER> named . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,xstlc recognized
email specifies a <PLACE_HOLDER>,if ( email2 . index of ( __str__ ) != - __num__ ) { string _sub = email2 . substring ( email1 . index of ( __str__ ) + __num__ ) ; if ( _sub . equals ignore case ( email1 ) ) { union . add ( email1 ) ; } else { union . add ( email1 ) ; union . add ( email2 ) ; } } else if ( email2 . starts with ( __str__ ) ) { if ( within domain ( email1 @$ email2 ) ) { union . add ( email2 ) ; } else { union . add ( email1 ) ; union . add ( email2 ) ; } } else { if ( email1 . equals ignore case,email specifies
which will cause the main <PLACE_HOLDER> to recreate this fragment,( new sign out word press com async ( get activity ( ) ) ) . execute on executor ( async task . thread_pool_executor ) ;,which cause
check if send mono triggers all <PLACE_HOLDER> off,channel . note on ( __num__ @$ __num__ ) ; soft . read ( __num__ ) ; assert true ( soft . find voice ( __num__ @$ __num__ ) != null ) ; smsg . set message ( short message . control_change @$ __num__ @$ __num__ @$ __num__ ) ; receiver . send ( smsg @$ - __num__ ) ; soft . read ( __num__ ) ; assert true ( soft . find voice ( __num__ @$ __num__ ) == null ) ; soft . close ( ) ;,mono triggers
first get the <PLACE_HOLDER> indicating the attribute 's type @$ and map it to the appropriate oid .,while ( true ) { c = read char ( in @$ __str__ ) ; if ( c == __str__ ) { break ; } temp . append ( ( char ) c ) ; } oid = ava keyword . getoid ( temp . to string ( ) @$ format @$ keyword map ) ;,first get
for all the databases query the single <PLACE_HOLDER> to test routing,for ( int server = __num__ ; server < vertices . length ; ++ server ) { o database document g = server instance . get ( server ) . get server instance ( ) . open database ( get database name ( ) @$ __str__ @$ __str__ ) ; system . out . println ( __str__ + server + __str__ ) ; try { for ( int i = __num__ ; i < vertices . length ; ++ i ) { final string node name = server instance . get ( i ) . get server instance ( ) . get distributed manager ( ) . get local node name ( ) ; string cluster name = __str__ ; cluster name += __str__ + node name ;,databases query
these colors should get <PLACE_HOLDER> .,intent intent = new custom tabs intent . builder ( ) . set toolbar color ( __num__ ) . set secondary toolbar color ( __num__ ) . set navigation bar color ( __num__ ) . set color scheme params ( color_scheme_light @$ light params ) . set color scheme params ( color_scheme_dark @$ dark params ) . build ( ) . intent ;,colors get
now reptable has 2000 <PLACE_HOLDER> and parttable has 252 <PLACE_HOLDER>,thread . sleep ( __num__ ) ; save tables with default nonce and path ( client ) ; wait for snapshot to finish ( client ) ;,reptable has
call the existing block @$ which will presumably set the <PLACE_HOLDER> properly,block . add statement ( code ) ;,which set
n 1 has no <PLACE_HOLDER>,string nodes config = __str__ ;,n has
step 2 : optional . dynamic runtime property change option we would like to utilize the benefits of obtaining dynamic property changes initialize the dynamic property <PLACE_HOLDER> with our configuration source,dynamic property factory . init with configuration source ( my configuration ) ;,changes initialize
if the next view returned by on <PLACE_HOLDER> search failed in layout manager has no <PLACE_HOLDER>able views @$ we still scroll to that view in order to make it visible on the screen . if it 's <PLACE_HOLDER>able @$ framework already calls rv 's request child <PLACE_HOLDER> which handles bringing this newly <PLACE_HOLDER>ed item onto the screen .,request child on screen ( result @$ null ) ; return focused ;,framework calls
when we get to here the new cache has a use <PLACE_HOLDER> of 1 and when setting a bunch of values on the same node sequence @$ such as when sorting @$ we will keep setting values in that same copy which has a use <PLACE_HOLDER> of 1 .,vec . set element at ( node @$ index ) ; m_last = vec . size ( ) ;,cache has
if we 're at this point in the method 's execution @$ we could n't reconstitute the original hash . so @$ we need to hash the submitted plaintext using current hash service configuration and then compare the formatted <PLACE_HOLDER> with the saved string . this will correctly compare passwords @$ but does not allow changing the hash service configuration without breaking previously saved,return passwords match ( submitted plaintext @$ saved hash ) ;,then compare
email 2 specifies a <PLACE_HOLDER>,if ( email2 . starts with ( __str__ ) ) { if ( within domain ( email1 @$ email2 ) ) { union . add ( email2 ) ; } else { union . add ( email1 ) ; union . add ( email2 ) ; } } else { if ( email1 . equals ignore case ( email2 ) ) { union . add ( email1 ) ; } else { union . add ( email1 ) ; union . add ( email2 ) ; } },email specifies
if filter.g does date <PLACE_HOLDER> for quoted strings @$ we 'd need to verify there 's no type mismatch when string col is filtered by a string that looks like date .,if ( col type == filter type . date && val type == filter type . string ) { try { node value = meta store utils . partition_date_format . get ( ) . parse ( ( string ) node value ) ; val type = filter type . date ; } catch ( parse exception pe ) { } },filter.g does
make sure no function at current and new location ca n't change <PLACE_HOLDER> if it is there .,if ( is func ) { function manager . remove function ( mem addr ) ; function manager . remove function ( new addr ) ; },function change
if the attr has no <PLACE_HOLDER>space uri attr has no local <PLACE_HOLDER>,if ( local name == null ) { string msg = utils . messages . create message ( msg key . er_null_local_element_name @$ new object [ ] { attr name } ) ; if ( f error handler != null ) { f error handler . handle error ( new dom error impl ( dom error . severity_error @$ msg @$ msg key . er_null_local_element_name @$ null @$ null @$ null ) ) ; } } else { },attr has
prev state <PLACE_HOLDER> state expected global counter exptected <PLACE_HOLDER> proc state seq expected block state expect notify,verify seq counter and interactions ( uid rec @$ process_state_foreground_service @$ process_state_service @$ __num__ @$ __num__ @$ network_state_unblock @$ true ) ;,state expected
if <PLACE_HOLDER> of the bits have been cleared in copy @$ that means bit set 1 had at least <PLACE_HOLDER> of the bits set that were set in bs 2,return copy . is empty ( ) ;,1 had
make sure map only contains correct <PLACE_HOLDER>,this . extensions = collections . checked map ( new hash map < > ( ) @$ string . class @$ extension . class ) ; this . extensions . put all ( extensions ) ;,map contains
legacy herb flavors might hit this <PLACE_HOLDER> before the caching logic corrects it @$ so treat this as disabled .,return false ;,flavors hit
we found a simplification . remove the old <PLACE_HOLDER> and add new ones .,if ( range set . as ranges ( ) . size ( ) < filter list . size ( ) ) { for ( final bound dim filter bound : filter list ) { if ( ! new children . remove ( bound ) ) { throw new ise ( __str__ ) ; } } if ( range set . as ranges ( ) . is empty ( ) ) { new children . add ( filtration . match nothing ( ) ) ; } for ( final range < bound value > range : range set . as ranges ( ) ) { if ( ! range . has lower bound ( ) && ! range . has upper bound ( ) ) { new children .,simplification remove
this algorithm has substantial <PLACE_HOLDER> for large argument streams !,seq buffer < ? extends u > buffer = seq buffer . of ( other ) ; return flat map ( t -> buffer . seq ( ) . filter ( u -> predicate . test ( t @$ u ) ) . map ( u -> tuple . < t @$ u > tuple ( t @$ u ) ) ) . on close ( other :: close ) ;,algorithm has
a single instance of hbase checksum failure causes the <PLACE_HOLDER> to switch off hbase checksum verification for the next 100 read requests . verify that this is correct .,for ( int i = __num__ ; i < h file block . checksum_verification_num_io_threshold + __num__ ; i ++ ) { b = hbr . read block data ( __num__ @$ - __num__ @$ pread @$ false @$ true ) ; assert true ( b . get buffer read only ( ) instanceof single byte buff ) ; assert equals ( __num__ @$ h file . get and reset checksum failures count ( ) ) ; },instance causes
special case : if there were no files to measure @$ use the containing j scroll pane 's <PLACE_HOLDER>,if ( d . width == __num__ && get parent ( ) != null ) { if ( get parent ( ) . get parent ( ) instanceof j scroll pane ) { j scroll pane parent = ( j scroll pane ) get parent ( ) . get parent ( ) ; dimension parent size = parent . get size ( ) ; insets insets = parent . get insets ( ) ; d . width = parent size . width - ( insets != null ? insets . right + insets . left : __num__ ) ; } } else { d . width += default_icon_size + width_padding ; },case use
should print further diagnostic <PLACE_HOLDER> ?,return __num__ ;,print further
data should get <PLACE_HOLDER>,watcher . wait for change ( ) ; assert equals ( __str__ @$ new string ( o . zk . getzk database ( ) . get data ( __str__ @$ stat @$ null ) ) ) ; assert equals ( __str__ @$ new string ( o . zk . getzk database ( ) . get data ( __str__ @$ stat @$ null ) ) ) ;,data get
this code adjusts the chunk size to properly account for the size and gap specified in the xp style . it also does it 's own box placement for the chunk animation . this is required because the inherited algorithm from basic progress bar ui goes back and forth whereas xp only goes in one direction . xp also has ghosted trailing chunks to,if ( ! vertical ) { rect . y = rect . y + ins . top ; rect . height = progress bar . get height ( ) - ins . top - ins . bottom ; int len = progress bar . get width ( ) - ins . left - ins . right ; len += ( rect . width + gap ) * __num__ ; double delta = ( double ) ( len ) / ( double ) framecount ; rect . x = ( int ) ( delta * current frame ) + ins . left ; } else { rect . x = rect . x + ins . left ; rect . width = progress bar . get width ( ),code adjusts
required for mdc based routing appender so that child threads can inherit the mdc <PLACE_HOLDER>,system . set property ( __str__ @$ __str__ ) ; configurator . initialize ( __str__ @$ llap_l4j2 . to string ( ) ) ; long end = system . current time millis ( ) ; log . debug ( __str__ @$ llap_l4j2 @$ ( end - start ) @$ async ) ; throw new runtime exception ( __str__ + __str__ + llap constants . log4j2_properties_file + __str__ ) ;,threads inherit
record elasticsearch version and cluster <PLACE_HOLDER> .,if ( target instanceof cluster info accessor ) { recorder . record attribute ( elasticsearch constants . args_version_annotation_key @$ ( ( cluster info accessor ) target ) . _$pinpoint$_get cluster info ( ) ) ; },record elasticsearch
gesture selection helper provides <PLACE_HOLDER> that interprets a combination of motions and gestures in order to provide gesture driven selection support when used in conjunction with recycler view .,final gesture selection helper gesture helper = gesture selection helper . create ( tracker @$ m selection predicate @$ m recycler view @$ scroller @$ m monitor ) ;,helper provides
verify the widget will now display white <PLACE_HOLDER>,attack plugin . on varbit changed ( new varbit changed ( ) ) ; warned skills = attack plugin . get warned skills ( ) ; assert true ( warned skills . contains ( skill . attack ) ) ; assert false ( attack plugin . is warned skill selected ( ) ) ;,widget display
bits consumed range : 0 to 7 @$ where 0 indicates last <PLACE_HOLDER> fully consumed,if ( bits consumed == __num__ || bits consumed == __num__ ) { bits consumed = __num__ ; ++ offset ; },0 indicates
someone modified the components tree <PLACE_HOLDER> while we were computing this range . we can just bail as another range will be computed .,synchronized ( this ) { if ( tree holders size != m component tree holders . size ( ) ) { return false ; } holder = m component tree holders . get ( index ) ; if ( holder . get render info ( ) . renders view ( ) ) { return true ; } children width spec = get actual children width spec ( holder ) ; children height spec = get actual children height spec ( holder ) ; },someone modified
turn the screen off . a black surface is already hiding the <PLACE_HOLDER> of the screen .,if ( m power state . get color fade level ( ) == __num__ ) { set screen state ( display . state_off ) ; m pending screen off = false ; m power state . dismiss color fade resources ( ) ; } else if ( perform screen off transition && m power state . prepare color fade ( m context @$ m color fade fades config ? color fade . mode_fade : color fade . mode_cool_down ) && m power state . get screen state ( ) != display . state_off ) { m color fade off animator . start ( ) ; } else { m color fade off animator . end ( ) ; },surface hiding
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,scheduler captures
since this link includes object <PLACE_HOLDER> from another library @$ we know that library must be statically linked @$ so we need to look at include link static in lto indexing to decide whether to include its objects in the lto indexing for this target .,if ( include link static in lto indexing ) { for ( linker inputs . library to link lib : unique libraries ) { if ( ! lib . contains object files ( ) ) { continue ; } for ( artifact object file : lib . get object files ( ) ) { if ( compiled . contains ( object file ) ) { all bitcode . put ( object file . get exec path ( ) @$ object file ) ; } } } } for ( linker input input : object files ) { if ( this . lto compilation context . contains bitcode file ( input . get artifact ( ) ) ) { all bitcode . put ( input . get artifact (,link includes
check cm when <PLACE_HOLDER>ifier predicts only one <PLACE_HOLDER>,checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ;,classifier predicts
create one file then delete it to trigger the file not found <PLACE_HOLDER> when closing the file .,file sys . create ( new path ( __str__ ) ) ; file sys . delete ( new path ( __str__ ) @$ true ) ; dfs client dfs client = file sys . get client ( ) ;,file found
first operator should return final operator <PLACE_HOLDER>,assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; operator stats = pipeline operator . get operator context ( ) . get nested operator stats ( ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ;,operator return
verify data object localization <PLACE_HOLDER>,localization = get localization ( data obj ) ; assert equals ( __str__ @$ localization . get resource bundle key for name ( ) ) ; assert equals ( __str__ @$ localization . get resource bundle key for description ( ) ) ; assert equals ( __str__ @$ localization . get labeled entity id for name ( ) ) ; assert equals ( __str__ @$ localization . get labeled entity id for description ( ) ) ;,data object
create one build rule to generate build <PLACE_HOLDER> .,build rule params build config params = params ; optional < build rule > values file rule = values file . flat map ( graph builder :: get rule ) ; if ( values file rule . is present ( ) ) { build config params = build config params . copy appending extra deps ( values file rule . get ( ) ) ; } android build config android build config = new android build config ( build config build target @$ project filesystem @$ build config params @$ java package @$ values @$ values file @$ use constant expressions ) ; graph builder . add to index ( android build config ) ;,one build
if the user provided a <PLACE_HOLDER> then we have to make sure the current realm has an events looper to deliver the results .,if ( ( on success != null || on error != null ) ) { shared realm . capabilities . check can deliver notification ( __str__ ) ; },user provided
should not work : <PLACE_HOLDER> on basic type,long ds . group by ( __num__ ) ;,not work
bind and start to accept incoming <PLACE_HOLDER> .,try { bootstrap server . register shutdown hook ( ) ; bootstrap server . start and block ( ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; } log . info ( __str__ ) ;,bind accept
create and set iam role so that firehose service has <PLACE_HOLDER> to the s 3 buckets to put data . please check the trust policy document.json and permissions policy document.json files for the trust and permissions policies set for the role .,string iam role arn = create iam role ( s3 object prefix ) ; redshifts3 configuration . set rolearn ( iam role arn ) ; copy command copy command = new copy command ( ) ; copy command . with copy options ( copy options ) . with data table name ( data table name ) ; redshift destination configuration redshift destination configuration = new redshift destination configuration ( ) ; redshift destination configuration . with clusterjdbcurl ( clusterjdbc url ) . with rolearn ( iam role arn ) . with username ( username ) . with password ( password ) . with copy command ( copy command ) . withs3 configuration ( redshifts3 configuration ) ; create delivery stream request . set redshift destination configuration ( redshift,service has
test that a match set with no tags applied has no <PLACE_HOLDER> filtered,vt session session = controller . get session ( ) ; list < vt match set > match sets = session . get match sets ( ) ; vt match set match set = match sets . get ( __num__ ) ; collection < vt match > matches = match set . get matches ( ) ; for ( vt match match : matches ) { assert true ( __str__ @$ tag filter . passes filter ( match ) ) ; } filter state filter state = tag filter . get filter state ( ) ; map < string @$ vt match tag > excluded tags = ( map < string @$ vt match tag > ) filter state . get ( tag filter . excluded_tags_key ) ;,match has
each test <PLACE_HOLDER> .,int start = __num__ @$ total = __num__ @$ remain = __num__ ; for ( int i = __num__ ; i < gas . length ; ++ i ) { remain += gas [ i ] - cost [ i ] ; total += gas [ i ] - cost [ i ] ; if ( remain < __num__ ) { start = i + __num__ ; remain = __num__ ; } },each test
call the api 's channels.list method to retrieve the resource that represents the authenticated user 's channel . in the api response @$ only include channel information needed for this use case . the channel 's content details part contains playlist i ds relevant to the channel @$ including the id for the list that contains <PLACE_HOLDER> uploaded to the channel .,you tube . channels . list channel request = youtube . channels ( ) . list ( __str__ ) ; channel request . set mine ( true ) ; channel request . set fields ( __str__ ) ; channel list response channel result = channel request . execute ( ) ; list < channel > channels list = channel result . get items ( ) ; if ( channels list != null ) { string upload playlist id = channels list . get ( __num__ ) . get content details ( ) . get related playlists ( ) . get uploads ( ) ; list < playlist item > playlist item list = new array list < playlist item > ( ) ; you tube . playlist items,part contains
extract character list @$ gold quote speaker and mention <PLACE_HOLDER> from the xml document .,document doc = xml utils . read document from file ( file name ) ; node text = doc . get document element ( ) . get elements by tag name ( __str__ ) . item ( __num__ ) ; string doc text = get just text ( text ) ; annotation document = get annotated file ( doc text @$ file name @$ get processed corenlp properties ( ) ) ; list < core map > quotes = document . get ( core annotations . quotations annotation . class ) ; list < core label > tokens = document . get ( core annotations . tokens annotation . class ) ; list < gold quote info > gold list = new array list < > ( ),gold quote
a new directory also receives a <PLACE_HOLDER> of the parent 's default acl .,list < acl entry > default entries = child . is directory ( ) ? parent default entries : collections . < acl entry > empty list ( ) ; final fs permission new perm ; if ( ! acl util . is minimal acl ( access entries ) || ! default entries . is empty ( ) ) { child . add acl feature ( create acl feature ( access entries @$ default entries ) ) ; new perm = create fs permission for extended acl ( access entries @$ child perm ) ; } else { new perm = create fs permission for minimal acl ( access entries @$ child perm ) ; },directory receives
0 means no <PLACE_HOLDER> on master .,if ( master count == __num__ || master count == system_regions ) { assert equals ( master count @$ m actual count ) ; } else { check count ( master count @$ m actual count ) ; },0 means
compare to make sure the created job has the expected <PLACE_HOLDER> @$ i.e . the <PLACE_HOLDER> resulting from a merge of the json file and cli args .,final job actual job = json . read ( actual job config json @$ job . class ) ; final job . builder actual job builder = actual job . to builder ( ) ; builder . set name ( test job name ) . set version ( test job version ) . set image ( busybox ) . set env ( immutable map . of ( redundant env key @$ __str__ ) ) ; assert job equals ( builder . build ( ) @$ actual job builder . build ( ) ) ;,job has
these values always start non <PLACE_HOLDER> .,while ( callback != null ) { future callback internal callback = this . callback ; exception e = this . e ; object result = this . result ; this . callback = null ; this . e = null ; this . result = null ; callback . on completed ( e @$ result @$ this ) ; },values start
some exceptions do not have a <PLACE_HOLDER> ; fall back to the string value .,if ( message == null ) { message = thrown . to string ( ) ; },exceptions have
d 2 has pkeys <PLACE_HOLDER> to 5<PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; ++ i ) { volt queuesql ( loadd2 @$ i @$ __str__ + string . value of ( i ) ) ; } volt executesql ( ) ;,d pkeys
<PLACE_HOLDER> 6 depends on 5 do not add the config <PLACE_HOLDER> to the dependency list . do not add the feature <PLACE_HOLDER> with no dependency to the dependency list .,test data data = new test data ( ) ; data . info = ai ; data . paths with code = paths with code ; return data ;,split add
stop the inbound bridges when the foreign connection is dropped since the bridge has no <PLACE_HOLDER> and needs to be restarted once a new connection to the foreign side is made .,if ( this . foreign connection . compare and set ( connection @$ null ) ) { for ( destination bridge bridge : inbound bridges ) { try { bridge . stop ( ) ; } catch ( exception e ) { } } this . connection service . execute ( new runnable ( ) { @ override public void run ( ) { try { do initialize connection ( false ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; } } } ) ; } else if ( this . local connection . compare and set ( connection @$ null ) ) { for ( destination bridge bridge : outbound bridges ) { try { bridge . stop,bridge has
parens force <PLACE_HOLDER> as an expression .,node script = parse ( __str__ + js + __str__ ) ;,parens force
force default source if any param has unassigned <PLACE_HOLDER>,if ( ! get return ( ) . is valid ( ) ) { return source type . default ; } for ( parameter param : get parameters ( ) ) { if ( ! param . is valid ( ) ) { return source type . default ; } } return get stored signature source ( ) ; manager . lock . release ( ) ;,param unassigned
mock unregister m bean to throw the instance not found <PLACE_HOLDER> @$ indicating that the m bean has already been unregistered,do throw ( new instance not found exception ( ) ) . when ( mockm bean server ) . unregisterm bean ( object name ) ; m beanjmx adapter m beanjmx adapter = spy ( new m beanjmx adapter ( dist member ) ) ; m beanjmx adapter . mbean server = mockm bean server ; m beanjmx adapter . unregisterm bean ( object name ) ;,m found
if passed integer and in list use numeric form else use original <PLACE_HOLDER>,try { int genre id = integer . parse int ( value ) ; if ( genre id < genre types . get max genre id ( ) ) { return bracket wrap ( string . value of ( genre id ) ) ; } else { return value ; } } catch ( number format exception nfe ) { integer genre id = genre types . get instance of ( ) . get id for name ( value ) ; if ( genre id != null ) { return bracket wrap ( string . value of ( genre id ) ) ; } if ( value . equals ignore case ( id3v2 extended genre types . rx . get description ( ) ) ) { value =,form use
get the set of component <PLACE_HOLDER>s to make sure the config only specifies valid component <PLACE_HOLDER>,set < string > component names = get component parallelism ( topology ) . key set ( ) ;,config specifies
these array types still need to be implemented . the superclass wo n't handle <PLACE_HOLDER> so we return null here until we can code schema implementations for <PLACE_HOLDER> .,return null ; default : if ( oid value == type registry . geometry oid ( ) ) { return geometry . builder ( ) ; } else if ( oid value == type registry . geography oid ( ) ) { return geography . builder ( ) ; } else if ( oid value == type registry . citext oid ( ) ) { return schema builder . string ( ) ; } else if ( oid value == type registry . geometry array oid ( ) ) { return schema builder . array ( geometry . builder ( ) . optional ( ) . build ( ) ) ; } else if ( oid value == type registry . hstore oid ( ) ) { return,superclass handle
create a long from the first 8 bytes of the digest this is fine as md 5 has the avalanche <PLACE_HOLDER> . paranoids could have xor folded the other 8 bytes in too .,long seed = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { seed = ( seed << __num__ ) + ( ( int ) digest [ i ] + __num__ ) ; } return seed ;,md has
up to here @$ an error can give a good <PLACE_HOLDER>,log . error ( __str__ @$ client socket . get inet address ( ) @$ t ) ; try { write error to stream ( output stream @$ t ) ; } catch ( io exception e ) { },error give
a field of executable stage which includes the p <PLACE_HOLDER> goes to worker side .,set < p collection node > executable stage outputs = new hash set < > ( ) ;,which includes
<PLACE_HOLDER> 1 reads all local <PLACE_HOLDER> 2 reads 10 local and 10 remote <PLACE_HOLDER> 3 reads all remote,try { final string [ ] hosts = { __str__ @$ __str__ @$ __str__ } ; final int num_local_host1_splits = __num__ ; final int num_local_host2_splits = __num__ ; final int num_remote_splits = __num__ ; final int num_local_splits = num_local_host1_splits + num_local_host2_splits ; int split cnt = __num__ ; set < locatable input split > splits = new hash set < locatable input split > ( ) ; for ( int i = __num__ ; i < num_local_host1_splits ; i ++ ) { splits . add ( new locatable input split ( split cnt ++ @$ __str__ ) ) ; } for ( int i = __num__ ; i < num_local_host2_splits ; i ++ ) { splits . add ( new locatable input split ( split cnt ++ @$,remote reads
owner can add internal system <PLACE_HOLDER>,return create window ( parent @$ type @$ token @$ name @$ owner id @$ false ) ;,owner add
filter actually did some <PLACE_HOLDER> @$ set the new chunk in and release the old chunk .,if ( ( filtered chunk != null ) && ( filtered chunk != orig chunk ) ) { body chunks . set ( i @$ filtered chunk ) ; final int ref cnt = orig chunk . ref cnt ( ) ; if ( ref cnt > __num__ ) { orig chunk . release ( ref cnt ) ; } },filter did
we only check for incomplete multi statement procedures right now add a mandatory <PLACE_HOLDER>,incomplete stmt offset = i start ; incomplete stmt = string . copy value of ( buf @$ i start @$ i cur - i start ) ;,procedures add
check whether we got group created <PLACE_HOLDER>,assert equals ( __str__ @$ __num__ @$ group change collector . collected events . size ( ) ) ; assert equals ( __str__ @$ test group name2 @$ ( ( server stored group event ) group change collector . collected events . get ( __num__ ) ) . get source group ( ) . get group name ( ) ) ;,group created
make sure that the metadata was refreshed during the rebalance and thus subscriptions now contain two <PLACE_HOLDER> .,final set < string > updated subscription set = new hash set < > ( arrays . as list ( topic1 @$ topic2 ) ) ; assert equals ( updated subscription set @$ subscriptions . subscription ( ) ) ;,subscriptions contain
propagating the amrm client nm token cache <PLACE_HOLDER>,nm client . setnm token cache ( rm client . getnm token cache ( ) ) ; nm client . init ( conf ) ; nm client . start ( ) ; assert not null ( nm client ) ; assert equals ( state . started @$ nm client . get service state ( ) ) ;,client nm
for <PLACE_HOLDER>s @$ the computed <PLACE_HOLDER> equals the symbol 's <PLACE_HOLDER> @$ except for two situations :,owntype = sym . type ; if ( owntype . has tag ( class ) ) { chk . check for bad auxiliary class access ( tree . pos ( ) @$ env @$ ( class symbol ) sym ) ; type own outer = owntype . get enclosing type ( ) ; if ( owntype . tsym . type . get type arguments ( ) . non empty ( ) ) { owntype = types . erasure ( owntype ) ; } else if ( own outer . has tag ( class ) && site != own outer ) { type norm outer = site ; if ( norm outer . has tag ( class ) ) { norm outer = types . as enclosing super (,type equals
create four wheels and add them at their locations note that our fancy car actually goes <PLACE_HOLDER>,vector3f wheel direction = new vector3f ( __num__ @$ - __num__ @$ __num__ ) ; vector3f wheel axle = new vector3f ( - __num__ @$ __num__ @$ __num__ ) ; geometry wheel_fr = find geom ( car node @$ __str__ ) ; wheel_fr . center ( ) ; box = ( bounding box ) wheel_fr . get model bound ( ) ; wheel radius = box . gety extent ( ) ; float back_wheel_h = ( wheel radius * __num__ ) - __num__ ; float front_wheel_h = ( wheel radius * __num__ ) - __num__ ; player . add wheel ( wheel_fr . get parent ( ) @$ box . get center ( ) . add ( __num__ @$ - front_wheel_h @$ __num__ ) @$ wheel direction @$,car goes
omitting sha <PLACE_HOLDER> for the moment ; there seems to be no reason to allow it .,if ( params . get hash ( ) != hash type . sha256 && params . get hash ( ) != hash type . sha512 ) { throw new general security exception ( __str__ ) ; },omitting sha
a corrupt mob file does n't abort the <PLACE_HOLDER> of regions @$ so we can enable the table .,admin . enable table ( table ) ; h base fsck res = hbck testing util . doh file quarantine ( conf @$ table ) ; assert equals ( __num__ @$ res . get ret code ( ) ) ; h file corruption checker hfcc = res . geth filecorruption checker ( ) ; assert equals ( __num__ @$ hfcc . geth files checked ( ) ) ; assert equals ( __num__ @$ hfcc . get corrupted ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get failures ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get quarantined ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get missing,file abort
need the der encoded response <PLACE_HOLDER> to verify the signature later,tbs response data = seq tmp [ __num__ ] . to byte array ( ) ;,der encoded
second try should hit the <PLACE_HOLDER> again,try { metastore . get all databases ( ) ; } catch ( runtime exception ignored ) { } assert equals ( mock client . get access count ( ) @$ __num__ ) ;,try hit
the other one has multiple <PLACE_HOLDER> @$ but we do n't . expand to multiple <PLACE_HOLDER> and copy over .,if ( other src . m durations != null ) { my src . make durations ( ) ; my src . m durations . add durations ( other src . m durations ) ; if ( my src . m active duration != __num__ ) { my src . m durations . add duration ( my src . m active proc state @$ my src . m active duration ) ; my src . m active duration = __num__ ; my src . m active proc state = process stats . state_nothing ; } } else if ( my src . m active duration != __num__ ) { if ( my src . m active proc state == other src . m active proc state ) {,one has
date @$ note : these ms counts all presume pst jan @$ feb @$ mar @$ apr 2014 may @$ jun @$ jul @$ aug 2014 <PLACE_HOLDER> @$ oct @$ nov @$ dec 2014 jan 2016 @$ mar 2017 @$ jun 2018 @$ <PLACE_HOLDER> 2019 @$ dec 2020 jan @$ feb @$ mar @$ apr 2014 may @$ jun @$ jul @$ aug 2014,long [ ] exp = new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ;,date aug
not a comma . stop parsing the keep <PLACE_HOLDER> .,if ( ! configuration constants . argument_separator_keyword . equals ( next word ) ) { break ; },the keep
kick off any lingering app transitions form the move <PLACE_HOLDER> to front operation @$ but only consider the top <PLACE_HOLDER> and stack on that display .,if ( display . is top stack ( stack ) && top running activity . is state ( resumed ) ) { stack . execute app transition ( target options ) ; } else { resumed on display |= top running activity . make active if needed ( target ) ; },transitions form
then do n't add the search <PLACE_HOLDER> to the hint .,if ( ! m iconified by default || m search hint icon == null ) { return hint text ; } final int text size = ( int ) ( m search src text view . get text size ( ) * __num__ ) ; m search hint icon . set bounds ( __num__ @$ __num__ @$ text size @$ text size ) ; final spannable string builder ssb = new spannable string builder ( __str__ ) ; ssb . set span ( new image span ( m search hint icon ) @$ __num__ @$ __num__ @$ spannable . span_exclusive_exclusive ) ; ssb . append ( hint text ) ; return ssb ;,then add
required for mdc based routing appender so that child threads can inherit the mdc <PLACE_HOLDER>,system . set property ( default thread context map . inheritable_map @$ __str__ ) ; configurator . initialize ( null @$ log4j file name ) ; log config location ( conf ) ; return __str__ + log4j config file + __str__ + async ;,threads inherit
serializer your dom @$ where node is an org.w 3 c.dom.node assuming that xalan 's serializer can serialize any <PLACE_HOLDER> of dom node,fdom serializer . serializedom3 ( node arg ) ;,serializer serialize
restarted rm has the failed app <PLACE_HOLDER> too .,rm2 . wait for state ( app1 . get application id ( ) @$ rm app state . failed ) ;,rm has
character functions returning number <PLACE_HOLDER> :,add functions ( arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$,functions returning
process has same <PLACE_HOLDER>,string deployment id3 = deploy process without timers ( ) ;,process has
note : complex structure already has a single undefined <PLACE_HOLDER> .,complex structure . add ( new byte data type ( ) ) ; complex structure . add ( new word data type ( ) ) ; complex structure . add ( new pointer32 data type ( data type . default ) @$ __num__ ) ;,structure has
0 x 1006446 : p 2 stack <PLACE_HOLDER> on op 0 . 0 x 10064 ce : p 1 has stack <PLACE_HOLDER> on op 0 ; p 2 stack <PLACE_HOLDER> on op 1 . 0 x 1006480 : p 1 has mem <PLACE_HOLDER> on op 0 ; p 2 has stack <PLACE_HOLDER> on op 0 .,program builder1 . create stack reference ( __str__ @$ ref type . read @$ - __num__ @$ source type . user_defined @$ __num__ ) ; program builder2 . create stack reference ( __str__ @$ ref type . read @$ - __num__ @$ source type . user_defined @$ __num__ ) ; program builder1 . create stack reference ( __str__ @$ ref type . read @$ - __num__ @$ source type . user_defined @$ __num__ ) ; program builder2 . create stack reference ( __str__ @$ ref type . read @$ - __num__ @$ source type . user_defined @$ __num__ ) ; program builder1 . create memory reference ( __str__ @$ __str__ @$ ref type . read @$ source type . user_defined @$ __num__ ) ; program builder2 . create,ref has
create the build <PLACE_HOLDER> .,caching build engine caching build engine = caching build engine factory ( ) . set dep files ( dep files . cache ) . set rule key factories ( rule key factories . of ( default rule key factory @$ input based rule key factory @$ dep filefactory ) ) . build ( ) ;,the build
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( test update deployment . class ) ;,suite using
consuming segment should have all <PLACE_HOLDER> in consuming state,for ( map . entry < string @$ string > entry : consuming segment instance state map . entry set ( ) ) { assert equals ( entry . get value ( ) @$ realtime segment online offline state model . consuming ) ; entry . set value ( realtime segment online offline state model . offline ) ; },segment have
else can we handle iterator types if context node does n't support event <PLACE_HOLDER> ? ?,add event listener ( ) ;,node support
then : the getters should return properly parsed <PLACE_HOLDER>,assert that ( builder . get bind address ( ) ) . is null ( ) ; assert that ( builder . get command ( ) ) . is equal to ( command . start ) ; assert that ( builder . get debug ( ) ) . is false ( ) ; assert that ( builder . get force ( ) ) . is false ( ) ; assert that ( builder . get help ( ) ) . is false ( ) ; assert that ( builder . get hostname for clients ( ) ) . is null ( ) ; assert that ( builder . get member name ( ) ) . is equal to ( __str__ ) ; assert that ( builder . get,getters return
src inode and its subtree can not contain snapshottable <PLACE_HOLDER> with snapshots,fs dir snapshot op . check snapshot ( fsd @$ srciip @$ snapshottable dirs ) ;,inode contain
the current item has higher <PLACE_HOLDER>,final int current index = play queue . get index ( ) ; final play queue item current item = play queue . get item ( current index ) ; if ( current item == null ) return null ;,item has
if spell <PLACE_HOLDER> is disabled @$ just return . the user should explicitly enable the spell <PLACE_HOLDER> .,if ( ! tsd . is spell checker enabled ( ) ) return ; if ( sci == null ) { sci = find avail system spell checker locked ( null @$ tsd ) ; set current spell checker locked ( sci @$ tsd ) ; } else { final string package name = sci . get package name ( ) ; final int change = is package disappearing ( package name ) ; if ( dbg ) slog . d ( tag @$ __str__ + package name ) ; if ( change == package_permanent_change || change == package_temporary_change ) { spell checker info avail sci = find avail system spell checker locked ( package name @$ tsd ) ; if ( avail sci == null || (,user enable
first make sure that app did n't install <PLACE_HOLDER>,assert false ( m enterprise config . is app installed device key and cert ( ) ) ; assert false ( m enterprise config . is app installed ca cert ( ) ) ;,app install
add a test . this test runs the select <PLACE_HOLDER> and expects the result to be order by output .,dbc . add test ( new test config ( __str__ @$ __str__ @$ false @$ order by output ) ) ;,test runs
we do n't yet know the next <PLACE_HOLDER>imum value so get the <PLACE_HOLDER> of the address ranges .,if ( max == null ) { for ( int i = __num__ ; i < addr ranges . length ; i ++ ) { if ( addr ranges [ i ] != null ) { address check max addr = addr ranges [ i ] . get max address ( ) ; if ( ( max == null ) || ( max . compare to ( check max addr ) < __num__ ) ) { max = check max addr ; } } } },value get
someone really wants this <PLACE_HOLDER> to die off .,thread . current thread ( ) . interrupt ( ) ;,someone wants
construct a case expression to handle the null indicator . this also covers the case where a left correlated subquery projects fields from outer relation . since loj can not produce nulls on the lhs @$ the projection now need to make a nullable lhs reference using a nullability indicator . if this this indicator is null @$ it means the subquery does not,rex node [ ] case operands = new rex node [ __num__ ] ;,subquery produce
the last one should have gotten a stream <PLACE_HOLDER>,for ( int i = __num__ ; i < single partmime reader callbacks . size ( ) - __num__ ; i ++ ) { final single partmime exception reader callback impl current callback = single partmime reader callbacks . get ( i ) ; final body part current expected part = multi part mime body . get body part ( i ) ; final map < string @$ string > expected headers = new hash map < string @$ string > ( ) ; @ suppress warnings ( __str__ ) final enumeration < header > all headers = current expected part . get all headers ( ) ; while ( all headers . has more elements ( ) ) { final header header = all headers . next,one gotten
check that the backgrounds have the same <PLACE_HOLDER> of the components to which they are associated,assert that ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) . is equal to ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) ; assert that ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) . is equal to ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) ; final rect text layout bounds = layout state . get mountable output at ( __num__ ) . get bounds ( ) ; final rect text background bounds = layout state . get mountable output at ( __num__ ) . get bounds ( ) ; assert that,backgrounds have
null means get the first <PLACE_HOLDER> .,return get next oid ( null @$ user data ) ;,means get
note down that the process has finished an <PLACE_HOLDER> and is in background <PLACE_HOLDER> starts grace period,if ( r . app != null ) { r . app . set last activity finish time if needed ( system clock . uptime millis ( ) ) ; } final long orig id = binder . clear calling identity ( ) ; try { boolean res ; final boolean finish with root activity = finish task == activity . finish_task_with_root_activity ; if ( finish task == activity . finish_task_with_activity || ( finish with root activity && r == rootr ) ) { res = m stack supervisor . remove task by id locked ( tr . task id @$ false @$ finish with root activity @$ __str__ ) ; if ( ! res ) { slog . i ( tag @$ __str__ ) ; } r,process finished
use print writer.println @$ which uses correct platform <PLACE_HOLDER> ending .,byte array output stream baos = new byte array output stream ( ) ; print writer node report str = new print writer ( new output stream writer ( baos @$ charset . for name ( __str__ ) ) ) ; node report node report = null ; for ( node report report : nodes report ) { if ( ! report . get node id ( ) . equals ( node id ) ) { continue ; } node report = report ; node report str . println ( __str__ ) ; node report str . print ( __str__ ) ; node report str . println ( node report . get node id ( ) ) ; node report str . print ( __str__ ) ; node,which uses
initialize stats publishing table for noscan which has only stats <PLACE_HOLDER> the rest of mr <PLACE_HOLDER> following stats <PLACE_HOLDER> initializes it in exec driver.java,stats publisher stats publisher = factory . get stats publisher ( ) ; if ( ! stats publisher . init ( scc ) ) { throw new hive exception ( error msg . statspublisher_initialization_error . get error coded msg ( ) ) ; },which has
generate for the get <PLACE_HOLDER>,super . abstract method ( symbol table @$ a @$ stream ) ;,the get
if the user clicked the ok <PLACE_HOLDER> @$ update the selected element .,if ( event . get action command ( ) . equals ( __str__ ) ) { set selected element ( ) ; },user clicked
indent:9 exp:12 warn indent:8 <PLACE_HOLDER>,try ( buffered writer writer = files . new buffered writer ( file path @$ charset ) ) { writer . close ( ) ; },exp:12 warn
check if query 2 see the latest optimistic <PLACE_HOLDER>,utils . instance . assert response ( apollo client . query ( query2 ) . response fetcher ( cache_only ) @$ new predicate < response < hero name with id query . data > > ( ) { @ override public boolean test ( response < hero name with id query . data > response ) throws exception { assert that ( response . data ( ) . hero ( ) . id ( ) ) . is equal to ( __str__ ) ; assert that ( response . data ( ) . hero ( ) . name ( ) ) . is equal to ( __str__ ) ; return true ; } } ) ;,query see
shortened string should not have <PLACE_HOLDER>,assert that ( info ) . is equal to ( __str__ ) ;,string have
a correct run should n't deliver any <PLACE_HOLDER>,var counter = __num__ ; for ( var ex : res . get exception list ( ) ) { counter ++ ; logger . info ( ex ) ; } return counter ;,run deliver
buck config should allow nonexistent <PLACE_HOLDER> without throwing .,buck config test utils . create with default filesystem ( temporary folder @$ reader ) ;,config allow
both threads should have returned <PLACE_HOLDER>,assert equals ( __num__ @$ results . size ( ) ) ;,threads returned
variants only contain <PLACE_HOLDER> .,if ( variants contain audio codecs ) { format [ ] audio formats = new format [ selected variants . size ( ) ] ; for ( int i = __num__ ; i < audio formats . length ; i ++ ) { format variant format = variants [ i ] . format ; audio formats [ i ] = derive audio format ( variant format @$ master playlist . muxed audio format @$ true ) ; } muxed track groups . add ( new track group ( audio formats ) ) ; } else { throw new illegal argument exception ( __str__ + codecs ) ; },variants contain
catch the original exception which sounds like : java.lang.illegal argument exception : comparison method violates its general <PLACE_HOLDER> !,swing utilities . invoke later ( ( ) -> { thread . current thread ( ) . set uncaught exception handler ( new thread . uncaught exception handler ( ) { public void uncaught exception ( thread t @$ throwable e ) { e . print stack trace ( ) ; if ( e instanceof illegal argument exception ) { passed = false ; latch . count down ( ) ; } } } ) ; test dialog d = new test dialog ( ) ; d . add window focus listener ( new window adapter ( ) { public void window gained focus ( window event e ) { latch . count down ( ) ; } } ) ; d . set visible ( true ),method violates
byte argument in gram key breaks <PLACE_HOLDER> between equal grams,byte [ ] empty = new byte [ __num__ ] ; gram key [ ] input = { new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ empty ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ empty ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ foo ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . ngram ) @$ foo ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . ngram ) @$ empty ) @$ new gram key ( new,argument breaks
health checker <PLACE_HOLDER> .,if ( is health checker configured ( ) ) { int sleep time = this . conf . get int ( h constants . health_chore_wake_freq @$ h constants . default_thread_wake_frequency ) ; health check chore = new health check chore ( sleep time @$ this @$ get configuration ( ) ) ; },health checker
lazily initialized since most selectors do n't survive module <PLACE_HOLDER> .,if ( set name == null ) { set name = annotations . name of ( set key ) ; } return set name ;,selectors survive
in case when remote tx has updated the current <PLACE_HOLDER> before .,ignore ( ) ;,tx updated
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( testtpcc suite . class ) ;,suite using
the view has now all <PLACE_HOLDER> and can be drawn,m is initializing = false ;,view has
we did not demand online loading @$ therefore a failure does not mean that the missing snippet causes a <PLACE_HOLDER> of this result this may happen during a remote search @$ because snippet loading is omitted to retrieve results faster,if ( cache strategy . must be offline ( ) ) { return page . make result entry ( this . query . get segment ( ) @$ this . peers @$ null ) ; } else { if ( this . snippet fetch words . contains ( segment . catchall string ) ) { return page . make result entry ( this . query . get segment ( ) @$ this . peers @$ null ) ; } final string reason = __str__ + snippet . get error code ( ) ; if ( this . delete if snippet fail ) { this . work tables . failur ls register missing word ( this . query . get segment ( ) . term index ( ) @$,snippet causes
the output stream of this fs does n't support <PLACE_HOLDER> @$ so the below test will fail,conf . set var ( hive conf . conf vars . hive_blobstore_supported_schemes @$ __str__ ) ; strict delimited input writer writer = strict delimited input writer . new builder ( ) . with field delimiter ( __str__ ) . build ( ) ; try { hive streaming connection . new builder ( ) . with database ( db name ) . with table ( tbl name ) . with agent info ( __str__ + thread . current thread ( ) . get name ( ) ) . with record writer ( writer ) . with transaction batch size ( __num__ ) . with hive conf ( conf ) . connect ( ) ; assert . fail ( ) ; } catch ( connection error e ) { assert,stream support
should not trigger and block <PLACE_HOLDER>,partial blocks < test element > partial blocks = block . get partial blocks ( ) ; assert not null ( partial blocks ) ; assert not null ( partial blocks . get block ranges ( ) ) ; assert equals ( __num__ @$ partial blocks . get block ranges ( ) . length ) ; assert equals ( group size @$ partial blocks . get block ranges ( ) [ __num__ ] ) ; assert not null ( partial blocks . get block targets ( ) ) ; assert equals ( __num__ @$ partial blocks . get block targets ( ) . length ) ; assert true ( partial blocks . get block targets ( ) [ __num__ ] . is valid ( ) ) ; assert,not trigger
corresponding method <PLACE_HOLDER> with the current stack may exceeds the stack trace . trims the <PLACE_HOLDER>,if ( method count + stack offset > trace . length ) { method count = trace . length - stack offset - __num__ ; } for ( int i = method count ; i > __num__ ; i -- ) { int stack index = i + stack offset ; if ( stack index >= trace . length ) { continue ; } string builder builder = new string builder ( ) ; builder . append ( horizontal_line ) . append ( __str__ ) . append ( level ) . append ( get simple class name ( trace [ stack index ] . get class name ( ) ) ) . append ( __str__ ) . append ( trace [ stack index ] . get method name,trace trims
check choose server <PLACE_HOLDER> for rsa keys .,results rsa = km . choose server alias ( __str__ @$ null @$ null ) ; if ( results rsa == null ) { throw new exception ( __str__ ) ; } system . out . println ( __str__ ) ;,check choose
no real charset @$ so let the <PLACE_HOLDER>,if ( code . equals ( http post body util . transfer encoding mechanism . binary . value ( ) ) ) { mechanism = transfer encoding mechanism . binary ; } else { throw new error data decoder exception ( __str__ + code ) ; },charset let
new children added @$ qualified name needs <PLACE_HOLDER>,update child categories ( store object @$ new children . values ( ) @$ impacted categories @$ false ) ; break ;,name needs
contract created by create 2 @$ does n't have <PLACE_HOLDER>,assert . assert equals ( __num__ @$ smart contract . get abi ( ) . get entrys count ( ) ) ;,contract have
see if the file matches the regular <PLACE_HOLDER> !,try { if ( pattern != null ) { matcher matcher = pattern . matcher ( file . get name ( ) ) ; get it = matcher . matches ( ) ; } if ( get it ) { string local filename = return target filename ( file . get name ( ) ) ; if ( ( ! only getting new files ) || ( only getting new files && needs download ( local filename ) ) ) { if ( is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ file . get name ( ) @$ target directory ) ) ; } connection . download file ( file @$ return target filename ( file .,file matches
define the do <PLACE_HOLDER> that logs the value provider value .,p . apply ( create . of ( __num__ ) ) . apply ( par do . of ( new do fn < integer @$ integer > ( ) { @ process element public void process ( process context c ) { my options ops = c . get pipeline options ( ) . as ( my options . class ) ; log . info ( __str__ @$ ops . get string value ( ) ) ; } } ) ) ;,the do
this method being used indicates a <PLACE_HOLDER> of this class,throw security logger . root_logger . unsupported operation exception use resource desc ( ) ;,method indicates
d tx 2 <PLACE_HOLDER> @$ but t xs with lower ids are still open,buffer . offer ( __num__ ) ; tx opened . increment and get ( ) ;,d tx
note that this class does not support forward <PLACE_HOLDER> .,if ( ! forward ) { if ( trusted pub key != null ) { prev pub key = trusted pub key ; } else { prev pub key = null ; } } else { throw new cert path validator exception ( __str__ ) ; },class support
note that we do not need to update requires old <PLACE_HOLDER> in events because that flag is only used during region initialization . otherwise we would need to,if ( ! ( this . region instanceof partitioned region ) ) { return ; },note requires
noinspection object <PLACE_HOLDER> in loop,if ( is composite ( current index ) ) { new keys = new tree set < > ( ) ; for ( comparable current key : current keys ) { final list < orid > current result = get from composite index ( current key @$ current index ) ; new keys . add all ( prepare keys ( next index @$ current result ) ) ; } } else { final list < o identifiable > keys ; try ( stream < o raw pair < object @$ orid > > stream = current index . get internal ( ) . stream entries ( current keys @$ true ) ) { keys = stream . map ( ( pair ) -> pair . second ) .,noinspection object
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; rule key input rule key = new rule key ( __str__ ) ; path output = paths . get ( __str__ ) ; build rule rule = new input rule key build rule ( target @$ filesystem @$ params ) { @ override public immutable list < step > get build steps ( build context context @$ buildable context buildable context ) { buildable context . record artifact ( output ) ; return immutable list . of ( new write file step ( filesystem @$ __str__ @$ output @$ false ) ) ; } @ override public source path get,which writes
add ejb suspend handler <PLACE_HOLDER>,boolean enable graceful shutdown = ejb3 subsystem root resource definition . enable_graceful_txn_shutdown . resolve model attribute ( context @$ model ) . as boolean ( ) ; final ejb suspend handler service ejb suspend handler service = new ejb suspend handler service ( enable graceful shutdown ) ; context . get service target ( ) . add service ( ejb suspend handler service . service_name @$ ejb suspend handler service ) . add dependency ( suspend controller service name @$ suspend controller . class @$ ejb suspend handler service . get suspend controller injected value ( ) ) . add dependency ( txn services . jboss_txn_local_transaction_context @$ local transaction context . class @$ ejb suspend handler service . get local transaction context injected value ( ) ) .,ejb suspend
we need to keep the admin <PLACE_HOLDER> instance separate in each callable @$ so that a refresh of the <PLACE_HOLDER> in one callable does not refresh the admin <PLACE_HOLDER> used by another callable .,admin client current admin client = admin store swapper . this . admin client ; int attempt = __num__ ; while ( attempt <= max_fetch_attempts ) { if ( attempt > __num__ ) { logger . info ( __str__ + attempt + __str__ + max_fetch_attempts + __str__ + node . brief to string ( ) + __str__ + wait_time_between_fetch_attempts + __str__ ) ; try { thread . sleep ( wait_time_between_fetch_attempts ) ; } catch ( interrupted exception e ) { throw new voldemort exception ( e ) ; } } logger . info ( __str__ + node . brief to string ( ) + __str__ + hadoop store dir to fetch ) ; try { return current admin client . readonly ops . fetch store ( node .,refresh refresh
16 byte md 5 <PLACE_HOLDER> of the testing secret string encoded in utf 8 .,message digest md5 = message digest . get instance ( __str__ ) ; byte [ ] digest = md5 . digest ( testing secret . get bytes ( standard charsets . utf_8 ) ) ; assert equals ( __num__ @$ digest . length ) ; return byte buffer . allocate ( __num__ ) . put ( digest ) . put ( digest ) . put ( digest ) . array ( ) ;,byte md
nested view has scrollable <PLACE_HOLDER> under this point . let it be handled there .,if ( dx != __num__ && ! is gutter drag ( m last motionx @$ dx ) && can scroll ( this @$ false @$ ( int ) dx @$ ( int ) x @$ ( int ) y ) ) { m last motionx = x ; m last motiony = y ; m is unable to drag = true ; return false ; },view has
holds an <PLACE_HOLDER> in a long register ? the callee has no clue whether the register holds an <PLACE_HOLDER> @$ long or is unused . he always saves a long . here we know a long was saved @$ but we only want an <PLACE_HOLDER> back . narrow the saved long to the <PLACE_HOLDER> that the jvm wants .,if ( loc . holds int ( ) ) { if ( assert . asserts_enabled ) { assert . that ( loc . is register ( ) @$ __str__ ) ; } return new stack value ( value addr . getj long at ( __num__ ) & __num__ ) ; } else if ( loc . holds narrow oop ( ) ) { if ( loc . is register ( ) && vm . getvm ( ) . is big endian ( ) ) { return new stack value ( value addr . get comp oop handle at ( vm . getvm ( ) . get int size ( ) ) @$ __num__ ) ; } else { return new stack value ( value addr . get comp,register holds
log the input value which caused <PLACE_HOLDER> so that it 's available for debugging . but when exposed through an error message it can leak sensitive information @$ even to the client application .,log . trace ( __str__ + tag + __str__ + utilities . format binary string ( value writable . get bytes ( ) @$ __num__ @$ value writable . get length ( ) ) + __str__ + value table desc [ tag ] . get properties ( ) ) ; throw new hive exception ( __str__ @$ e ) ;,which caused
then @$ already tried this credential . remove any rfc 2617 credential since presence of a rfc 2617 credential serves as flag to frontier to requeue this curi and let the curi die a natural <PLACE_HOLDER> .,if ( extant != null ) { extant . detach all ( curi ) ; logger . warning ( __str__ + realm + __str__ + curi . to string ( ) ) ; } else { string server key = get server key ( curi ) ; crawl server server = server cache . get server for ( server key ) ; set < credential > store rfc2617 credentials = get credential store ( ) . subset ( curi @$ http authentication credential . class @$ server . get name ( ) ) ; if ( store rfc2617 credentials == null || store rfc2617 credentials . size ( ) <= __num__ ) { logger . fine ( __str__ + curi ) ; } else { http authentication credential,curi die
make sure that the nested custom layouts do not render <PLACE_HOLDER>,for ( element child : root node . children ( ) ) { assert that ( child . children ( ) . size ( ) @$ is ( __num__ ) ) ; },layouts render
remote node has newer im<PLACE_HOLDER> <PLACE_HOLDER>,if ( bn reg . get layout version ( ) < storage . get layout version ( ) || ( bn reg . get layout version ( ) == storage . get layout version ( ) && bn reg . getc time ( ) > storage . getc time ( ) ) ) msg = __str__ + bn reg . get address ( ) + __str__ + bn reg . get layout version ( ) + __str__ + bn reg . getc time ( ) + __str__ + storage . get layout version ( ) + __str__ + storage . getc time ( ) ;,node has
lower half of the threads insert keys 0 @$ 1 @$2 ... uper half of the threads insert <PLACE_HOLDER> 0 @$ 1 @$ 2 @$ ...,for ( int thread no = __num__ ; thread no < threads ; thread no ++ ) { graql insert query = thread no < threads / __num__ ? graql . parse ( __str__ + thread no + __str__ ) . as insert ( ) : graql . parse ( __str__ + ( thread no - threads / __num__ ) + __str__ ) . as insert ( ) ; completable future < void > async insert = completable future . supply async ( ( ) -> { transaction impl tx = ( transaction impl ) session . write transaction ( ) ; tx . execute ( query ) ; try { barrier . await ( ) ; } catch ( exception e ) { e . print stack,half keys
node only has left <PLACE_HOLDER> which will be the minimum .,if ( right child index >= size ) { min index = left child index ; } else { t left child value = _values . get ( left child index ) ; t right child value = _values . get ( right child index ) ; if ( compare ( left child value @$ right child value ) <= __num__ ) { min index = left child index ; } else { min index = right child index ; } },node left
mem sql uses <PLACE_HOLDER> instead of schemas,return result set . get string ( __str__ ) ;,sql uses
if we do not update content of the record we should keep version of the record the same otherwise we would have <PLACE_HOLDER> when two records may have the same version but different content,final int new record version ; if ( update content ) { new record version = ppos . record version ; } else { new record version = version ; } if ( callback != null ) { callback . call ( rid @$ new record version ) ; } if ( o log manager . instance ( ) . is debug enabled ( ) ) { o log manager . instance ( ) . debug ( this @$ __str__ @$ rid @$ new record version @$ content . length ) ; } record updated . increment ( ) ; if ( content modified ) { return new o storage operation result < > ( new record version @$ content @$ false ) ; } else { return,records have
sak : not calling this means that mouse and mouse motion listeners do n't get <PLACE_HOLDER> . not a problem because the menu manager handles tracking for us .,( ( screen menu itemui ) ui ) . update listeners for screen menu item ( ) ;,listeners get
make sure two messages have different hash <PLACE_HOLDER>,assert not equal ( message . hash code ( ) @$ lazy field . hash code ( ) ) ;,messages have
k.o . criteria @$ start tag is not svg @$ fail <PLACE_HOLDER> on none svg,default :,criteria fail
downgrade priority as user is disconnecting the <PLACE_HOLDER> .,if ( m service . get priority ( device ) > bluetooth profile . priority_on ) { m service . set priority ( device @$ bluetooth profile . priority_on ) ; } return m service . disconnect ( device ) ;,priority disconnecting
be more forgiving of not finding the get listener <PLACE_HOLDER> .,method method = introspector . find method ( source class @$ get listener method name @$ __num__ ) ; if ( method != null ) { set get listener method ( method ) ; },the get
send enough coin to the apply account to make that account has <PLACE_HOLDER> to apply become witness .,grpcapi . witness list witnesslist = blocking stub full . list witnesses ( grpcapi . empty message . new builder ( ) . build ( ) ) ; optional < witness list > result = optional . of nullable ( witnesslist ) ; grpcapi . witness list witness list = result . get ( ) ; if ( result . get ( ) . get witnesses count ( ) < __num__ ) { assert . assert true ( public methed . sendcoin ( low bal address @$ cost for create witness @$ from address @$ test key002 @$ blocking stub full ) ) ; assert . assert false ( create witness not broadcast ( low bal address @$ wrong url @$ test update witness key ) ) ;,account has
does lfs see the correct file <PLACE_HOLDER> ?,assert equals ( lfs . get file status ( pathtotestfile1 ) . get len ( ) @$ testfile1 . length ( ) ) ;,lfs see
eof scanline write the <PLACE_HOLDER>,if ( j == scanline bytes - __num__ ) { if ( abs val == - __num__ ) { stream . write byte ( run count ) ; stream . write byte ( run val ) ; inc comp image size ( __num__ ) ; run count = __num__ ; } else { if ( abs val >= __num__ ) { stream . write byte ( __num__ ) ; stream . write byte ( abs val + __num__ ) ; inc comp image size ( __num__ ) ; for ( int a = __num__ ; a <= abs val ; a ++ ) { stream . write byte ( abs buf [ a ] ) ; inc comp image size ( __num__ ) ; } if ( !,scanline write
by the a 2 dp spec @$ srcs must indicate the capture <PLACE_HOLDER> . however if some device that do not @$ we try to match on some other class bits .,switch ( get device class ( ) ) { case device . audio_video_hifi_audio : case device . audio_video_set_top_box : case device . audio_video_vcr : return true ; default : return false ; },srcs indicate
handle negatives @$ which means last n <PLACE_HOLDER>,if ( start < __num__ ) { start = str . length ( ) + start ; },which means
immediately return the bytes back to the flow controller . consumed bytes converter will convert from the decompressed amount <PLACE_HOLDER> the user knows about to the compressed amount <PLACE_HOLDER> flow control knows about .,flow controller . consume bytes ( stream @$ listener . on data read ( ctx @$ stream id @$ buf @$ padding @$ decompressed end of stream ) ) ; if ( next buf == null ) { break ; } padding = __num__ ;,user knows
nodes must have the same <PLACE_HOLDER>,if ( node1 . get class ( ) != node2 . get class ( ) ) { return false ; } string image1 = node1 . get image ( ) ; string image2 = node2 . get image ( ) ;,nodes have
jdk 11 added the <PLACE_HOLDER> to the error message @$ we can do that for all java versions to be consistent .,return new array index out of bounds exception ( __str__ + index + __str__ + length ) ;,jdk added
check this ps is the master ps for this location @$ only master ps can accept the <PLACE_HOLDER>,if ( ! request . is come from ps ( ) && ! is part master ps ( part loc ) ) { string log = __str__ + request + __str__ + request . get part key ( ) ; log . error ( log ) ; return new updater response ( response type . server_handle_failed @$ log ) ; } else { try { class < ? extends update func > func class = ( class < ? extends update func > ) class . for name ( request . get updater func class ( ) ) ; constructor < ? extends update func > constructor = func class . get constructor ( ) ; constructor . set accessible ( true ) ; update func func =,ps accept
stop the datanode contains b <PLACE_HOLDER>,data node dn = cluster . get data node ( dn list [ decommission nodes num ] . get ipc port ( ) ) ; cluster . stop data node ( dn list [ decommission nodes num ] . get xfer addr ( ) ) ; cluster . set data node dead ( dn . get datanode id ( ) ) ;,datanode contains
to work around the fact that pmd does not yet do full type <PLACE_HOLDER> when it does @$ delete this,set < method name declaration > unique = new hash set < > ( ) ; set < string > sigs = new hash set < > ( ) ; for ( method name declaration mnd : methods . key set ( ) ) { string sig = mnd . get image ( ) + mnd . get parameter count ( ) + mnd . is varargs ( ) ; if ( ! sigs . contains ( sig ) ) { unique . add ( mnd ) ; } sigs . add ( sig ) ; } return unique ;,pmd do
it is expected that every map processes map sleep count <PLACE_HOLDER> of records .,try { context . set status ( __str__ + ( map sleep duration * ( map sleep count - count ) ) + __str__ ) ; long sleep time = sleep calc . calc sleep duration ( context . get task attemptid ( ) @$ count @$ map sleep count @$ map sleep duration ) ; thread . sleep ( sleep time ) ; } catch ( interrupted exception ex ) { throw ( io exception ) new io exception ( __str__ ) . init cause ( ex ) ; } ++ count ;,processes map
android resources @$ views and extras require special <PLACE_HOLDER>,if ( has injection points for annotation ( inject resource . class ) ) { bind listener ( matchers . any ( ) @$ resource listener ) ; } if ( has injection points for annotation ( inject extra . class ) ) { final extras listener extras listener = new extras listener ( context provider ) ; bind listener ( matchers . any ( ) @$ extras listener ) ; },resources require
this creates lock contention for the delay met command <PLACE_HOLDER> inside the command handler . so move the actual execution of the post completion callbacks on the command executor thread .,post on main thread ( new add runnable ( this @$ command handler . create execution completed intent ( m context @$ work spec id @$ needs reschedule ) @$ default_start_id ) ) ;,contention met
password match <PLACE_HOLDER>,inputerror = __num__ ;,password match
provenance events are generated only by connectable components @$ with the exception of download events @$ which have the root process group 's <PLACE_HOLDER> assigned as the component id @$ and drop events @$ which could have the connection <PLACE_HOLDER> assigned as the component id . so @$ we check if the component id is set to the root group and otherwise assume that,final data authorizable authorizable ; if ( root group id . equals ( component id ) ) { authorizable = new data authorizable ( flow manager . get root group ( ) ) ; } else { final connectable connectable = flow manager . find connectable ( component id ) ; if ( connectable == null ) { final connection connection = flow manager . get root group ( ) . find connection ( component id ) ; if ( connection == null ) { throw new resource not found exception ( __str__ ) ; } else { authorizable = new data authorizable ( connection . get source ( ) ) ; } } else { authorizable = new data authorizable ( connectable ) ; } },which have
only add to lexer if the function actually contains <PLACE_HOLDER>,if ( ! raf . actions . is empty ( ) && ! lexer . action funcs . contains key ( r ) ) { lexer . action funcs . put ( r @$ raf ) ; },function contains
just return @$ stats gathering should not block the main <PLACE_HOLDER>,if ( stats publisher == null ) { log . error ( __str__ ) ; if ( is stats reliable ) { throw new hive exception ( error msg . statspublisher_not_obtained . get error coded msg ( ) ) ; } return ; },return block
note : token only copied if the recipient does n't already have <PLACE_HOLDER> .,if ( token == null ) { token = o . token ; },recipient have
the two columns have different data <PLACE_HOLDER>,if ( ! expected data type . get class ( ) . is instance ( actual data type ) ) { if ( expected data type instanceof unknown data type ) { return actual data type ; } if ( actual data type instanceof unknown data type ) { return expected data type ; } if ( hack ignore int big int mismatch ) { if ( expected data type instanceof integer data type && actual data type instanceof big integer data type ) return actual data type ; } string msg = __str__ + table name + __str__ + expected column . get column name ( ) + __str__ ; throw failure handler . create failure ( msg @$ string . value of ( expected data type,columns have
apk does not contain merkle tree root <PLACE_HOLDER> .,return false ;,apk contain
the set of <PLACE_HOLDER> dirs includes device protected <PLACE_HOLDER> dir and credential protected <PLACE_HOLDER> dir which might be null for shared libraries . currently we do n't track these but be lenient and check in case we ever decide to store their usage <PLACE_HOLDER> .,for ( string data dir : data dirs ) { if ( data dir != null ) { pcl . merge app data dirs ( data dir @$ user id ) ; } },dir protected
nodeset comparisons @$ we always call the nodeset <PLACE_HOLDER> . because the arguments are backwards @$ we call the opposite comparison <PLACE_HOLDER> .,if ( obj2 . get type ( ) == x object . class_nodeset ) return obj2 . less than or equal ( this ) ; return this . num ( ) >= obj2 . num ( ) ;,comparisons call
if no partition <PLACE_HOLDER> registered for the cluster @$ simply use the default <PLACE_HOLDER> this can happen when the customized <PLACE_HOLDER> implementation library has not been deployed to the client,if ( partition accessors == null || partition accessors . is empty ( ) ) { _log . error ( __str__ + cluster name + __str__ ) ; return default partition accessor . get instance ( ) ; },accessor use
redis has three chunks <PLACE_HOLDER> last chunk only has entries,navigable map < byte [ ] @$ byte [ ] > chunk map = get binary tree map ( ) ; put encoded key value to map ( chunk map @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) ; put encoded key value to map ( chunk map @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) ; scan result < map . entry < byte [ ] @$ byte [ ] > > scan result first = new scan result < > ( __str__ . get bytes ( ) @$ new array list < map . entry < byte [ ] @$ byte [ ] > > ( ) ) ; scan result <,redis has
if the client acked an index <PLACE_HOLDER> than the current event sequence number since we know the client must have received it from another server .,if ( complete index > context . current index ( ) ) { return ; },client acked
initializes bluetooth <PLACE_HOLDER> .,if ( m bluetooth adapter == null ) { final bluetooth manager bluetooth manager = ( bluetooth manager ) m context . get application context ( ) . get system service ( context . bluetooth_service ) ; m bluetooth adapter = bluetooth manager . get adapter ( ) ; if ( m bluetooth adapter == null ) { log manager . w ( tag @$ __str__ ) ; } },initializes bluetooth
decode data frame common <PLACE_HOLDER>,version = spdy version ;,data frame
first deployment should only have two available xml <PLACE_HOLDER>,validate job xml names ( deployment_name_1 @$ __str__ @$ __str__ ) ;,deployment have
the immersive mode confirmation should never affect the system bar <PLACE_HOLDER> @$ otherwise it will unhide the navigation bar and hide itself .,if ( win candidate . get attrs ( ) . token == m immersive mode confirmation . get window token ( ) ) { final boolean last focus can receive keys = ( m last focused window != null && m last focused window . can receive keys ( ) ) ; win candidate = is status bar keyguard ( ) ? m status bar : last focus can receive keys ? m last focused window : m top fullscreen opaque window state ; if ( win candidate == null ) { return __num__ ; } },confirmation affect
we know lo should n't have a hardware <PLACE_HOLDER> or an i pv 4 broadcast <PLACE_HOLDER> .,network interface lo = network interface . get by name ( lo ) ; assert null ( lo . get hardware address ( ) ) ; for ( interface address ia : lo . get interface addresses ( ) ) { assert null ( ia . get broadcast ( ) ) ; },address have
javax mail incorrectly adds the <PLACE_HOLDER> for the first boundary to the end of the preamble @$ so we trim,assert . assert equals ( javax mail multi partmime reader . _preamble != null ? javax mail multi partmime reader . _preamble . trim ( ) : null @$ expected preamble ) ;,mail adds
currently @$ we get one join <PLACE_HOLDER> @$ but it is easy to change the hard coded number to get more join <PLACE_HOLDER>s for large table joins .,assert ( join order queue . size ( ) == __num__ ) ; assert ( m_join order list . size ( ) == __num__ ) ; m_join order list . add all ( join order queue ) ;,one join
no way to push the current token <PLACE_HOLDER> ...,tokens = new string tokenizer ( l ) ;,way push
verify that balancer runs <PLACE_HOLDER> .,ugi . do as ( new privileged exception action < void > ( ) { @ override public void run ( ) throws exception { test unknown datanode ( conf ) ; assert true ( user group information . is login keytab based ( ) ) ; return null ; } } ) ;,balancer runs
check if the zone actually uses daylight saving <PLACE_HOLDER> around the <PLACE_HOLDER>,if ( tz instanceof basic time zone ) { basic time zone btz = ( basic time zone ) tz ; time zone transition before = btz . get previous transition ( date @$ true ) ; if ( before != null && ( date - before . get time ( ) < dst_check_range ) && before . get from ( ) . getdst savings ( ) != __num__ ) { use standard = false ; } else { time zone transition after = btz . get next transition ( date @$ false ) ; if ( after != null && ( after . get time ( ) - date < dst_check_range ) && after . get to ( ) . getdst savings ( ) != __num__ ),zone uses
one that 's not ended yet has <PLACE_HOLDER>,plan item instance = p ;,one has
if keys exist : update <PLACE_HOLDER>,if ( index >= __num__ ) { values . set ( index @$ values . get ( index ) + val ) ; } else { set ( val @$ keys ) ; },keys exist
not currently used @$ as lengths can extend over more than one <PLACE_HOLDER> i think,int section three length ; int section four length ; int section five length ; int section six length ;,lengths extend
try to insert element that causes file <PLACE_HOLDER> @$ but fail,long file length before expansion = file . length ( ) ; broken random access file braf = new broken random access file ( file @$ __str__ ) ; queue = new queue file ( braf ) ; try { queue . add ( values [ max ] ) ; fail ( ) ; } catch ( io exception e ) { },causes file
send a bluetooth restart <PLACE_HOLDER>,message restart msg = m handler . obtain message ( message_restart_bluetooth_service ) ; m handler . send message delayed ( restart msg @$ get service restart ms ( ) ) ;,bluetooth restart
read nomad heron executor start up <PLACE_HOLDER> from file,string heron nomad script = get heron nomad script ( this . local config ) ; task . set name ( task name ) ;,executor start
the output dot <PLACE_HOLDER> for visualization,set < atn state > marked states = new hash set < atn state > ( ) ; st dot = stlib . get instance of ( __str__ ) ; dot . add ( __str__ @$ start state . state number ) ; dot . add ( __str__ @$ rankdir ) ; list < atn state > work = new linked list < atn state > ( ) ; work . add ( start state ) ; while ( ! work . is empty ( ) ) { atn state s = work . get ( __num__ ) ; if ( marked states . contains ( s ) ) { work . remove ( __num__ ) ; continue ; } marked states . add ( s ) ;,output dot
adders should add all <PLACE_HOLDER>,set < value meta interface > metas = new hash set < value meta interface > ( row meta . get value meta list ( ) ) ; for ( adder adder : adders ) { execution result < list < value meta interface > > result = ( execution result < list < value meta interface > > ) results . get ( adder ) ; for ( value meta interface meta : result . get result ( ) ) { assert true ( meta . get name ( ) @$ metas . remove ( meta ) ) ; } } assert equals ( searchers amount @$ metas . size ( ) ) ;,adders add
python 2 has <PLACE_HOLDER> and python 3 do n't have <PLACE_HOLDER>,assert true ( result . message ( ) . get ( __num__ ) . get data ( ) . contains ( __str__ ) || result . message ( ) . get ( __num__ ) . get data ( ) . contains ( __str__ ) ) ;,python has
no rules fired this <PLACE_HOLDER> @$ so we know this is 0,returned fire count = __num__ ;,rules fired
generate dates for selecting <PLACE_HOLDER>s by date @$ making sure the period will not contain the sample <PLACE_HOLDER>,string from date = sample_timestamp . minus days ( __num__ ) . format ( formatter ) ; string to date = sample_timestamp . minus days ( __num__ ) . format ( formatter ) ;,period contain
the subscription should receive a <PLACE_HOLDER> with the reply to property set .,stomp frame received = responder . receive ( ) ; assert not null ( received ) ; string remote reply to = received . get headers ( ) . get ( stomp . headers . send . reply_to ) ; assert not null ( remote reply to ) ; assert true ( remote reply to . starts with ( string . format ( __str__ @$ type ) ) ) ; log . info ( string . format ( __str__ @$ received . get action ( ) @$ stomp . headers . send . reply_to @$ remote reply to ) ) ;,subscription receive
jsr has no rop <PLACE_HOLDER>,if ( opcode == byte ops . jsr ) { has jsr = true ; return ; } else if ( opcode == byte ops . ret ) { try { return address = ( return address ) arg ( __num__ ) ; } catch ( class cast exception ex ) { throw new runtime exception ( __str__ @$ ex ) ; } return ; },jsr has
in order to avoid going over max <PLACE_HOLDER> i may need to steal from myself even though other pools have free <PLACE_HOLDER> . so figure out how much each group should provide,int nodes needed from others = math . min ( math . min ( max nodes - used nodes @$ nodes from others available ) @$ nodes needed ) ; int nodes needed from us = nodes needed - nodes needed from others ; log . debug ( __str__ @$ nodes needed from us @$ nodes needed from others ) ; if ( nodes needed from us > nodes from us available ) { cluster . set status ( top id @$ __str__ ) ; return __num__ ; },pools have
code browser expands <PLACE_HOLDER> to the code unit .,expected addresses . add ( addr ( __num__ ) @$ addr ( __num__ ) ) ; expected addresses . add ( addr ( __num__ ) @$ addr ( __num__ ) ) ; program selection expected selection = new program selection ( expected addresses ) ; perform action ( select all flows from action @$ get action context ( ) @$ true ) ; program selection current selection = code browser plugin . get current selection ( ) ; assert equals ( new my selection ( expected selection ) @$ new my selection ( current selection ) ) ;,browser expands
append the work unit file <PLACE_HOLDER> to the job input file,throw closer . rethrow ( t ) ;,unit file
descend from the solution set delta . check that it depends on both the workset and the solution set . if it does depend on both @$ this descend should create both <PLACE_HOLDER>,iter . get solution set delta ( ) . accept ( recursive creator ) ; final workset node workset node = ( workset node ) recursive creator . con2node . get ( iter . get workset ( ) ) ; if ( workset node == null ) { throw new compiler exception ( __str__ + __str__ ) ; } iter . get next workset ( ) . accept ( recursive creator ) ; solution set node solution set node = ( solution set node ) recursive creator . con2node . get ( iter . get solution set ( ) ) ; if ( solution set node == null || solution set node . get outgoing connections ( ) == null || solution set node . get outgoing connections,descend create
check the <PLACE_HOLDER> of the first buffer and record it . all further buffers must have the same <PLACE_HOLDER> . the <PLACE_HOLDER> must also be a power of 2,this . total num buffers = memory . size ( ) ; if ( this . total num buffers < min_required_buffers ) { throw new illegal argument exception ( __str__ + min_required_buffers + __str__ ) ; } this . segment size = memory . get ( __num__ ) . size ( ) ; this . record size = serializer . get length ( ) ; this . num key bytes = this . comparator . get normalize key len ( ) ;,buffers have
the set contains the incorrect <PLACE_HOLDER> @$ i.e . the ones without hyphen,line = line . replace ( __str__ @$ __str__ ) ;,set contains
this line inserts the <PLACE_HOLDER> .,volt queuesql ( insert @$ expect_scalar_match ( __num__ ) @$ uuid @$ val @$ update_ts ) ;,line inserts
allocate the trie 2 index array . if the data width is 16 bits @$ the array also includes the <PLACE_HOLDER> for the data .,int index array size = this . index length ; if ( width == value width . bits_16 ) { index array size += this . data length ; },array includes
triggering the evaluation twice will satisfy the entry <PLACE_HOLDER> for b,assert equals ( __num__ @$ runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state active ( ) . count ( ) ) ; string url = build url ( cmmn rest urls . url_case_instance @$ case instance . get id ( ) ) ; http put http put = new http put ( url ) ; http put . set entity ( new string entity ( __str__ ) ) ; execute request ( http put @$ http status . sc_ok ) ; assert equals ( __num__ @$ runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ),evaluation satisfy
withdrawing <PLACE_HOLDER> 1 's suggestion should leave <PLACE_HOLDER> 2 as the new winner . since the zone id is different @$ the time zone setting should be updated if the score is high enough .,script . suggest phone time zone ( empty phone1 suggestion ) ; if ( test case . expected score >= phone_score_usage_threshold ) { script . verify time zone set and reset ( zone phone2 suggestion ) ; } else { script . verify time zone not set ( ) ; },suggestion leave
insert core <PLACE_HOLDER> @$ a @$ b @$ thing . update the <PLACE_HOLDER> on don b,assert equals ( __num__ @$ cwm . getdeletes ( ) ) ; assert equals ( __num__ @$ cwm . get inserts ( ) ) ; assert equals ( __num__ @$ cwm . get updates ( ) ) ; cwm . reset ( ) ; fact handle handle = ksession . insert ( __str__ ) ; ksession . fire all rules ( ) ;,bean update
bob and david both should have a single <PLACE_HOLDER> .,org . flowable . task . api . task bob task = task service . create task query ( ) . task candidate user ( __str__ ) . single result ( ) ; assert that ( __str__ @$ bob task @$ is ( not null value ( ) ) ) ; org . flowable . task . api . task david task = task service . create task query ( ) . task candidate user ( __str__ ) . single result ( ) ; assert that ( __str__ @$ david task @$ is ( not ( null value ( ) ) ) ) ;,bob have
this is an open id connect authentication request so return <PLACE_HOLDER> and let oidc authorization code authentication provider handle it instead,if ( authorization code authentication . get authorization exchange ( ) . get authorization request ( ) . get scopes ( ) . contains ( __str__ ) ) { return null ; },request return
attention : jdk create <PLACE_HOLDER> only from collection @$ but guava and gs can create <PLACE_HOLDER> from iterable and collection,system . out . println ( from iterablegs ) ;,jdk create
res file p ctx root tasks fetch task analyzer explain config cbo <PLACE_HOLDER> @$,explain work work = new explain work ( null @$ null @$ plan . get root tasks ( ) @$ plan . get fetch task ( ) @$ null @$ null @$ config @$ plan . get cbo info ( ) @$ plan . get optimized query string ( ) @$ plan . get optimizedcbo plan ( ) ) ;,analyzer explain
we make sure the main fragment observes the <PLACE_HOLDER> . this will also trigger the <PLACE_HOLDER> to update itself .,usernames repository . add updatable ( this ) ;,fragment observes
we need to copy before iterating because <PLACE_HOLDER> can add other <PLACE_HOLDER>,array list < lifecycle > lifecycles = new array list < > ( deployment . get lifecycle objects ( ) ) ; for ( lifecycle object : lifecycles ) { object . start ( ) ; } http handler root = deployment . get handler ( ) ; final tree map < integer @$ list < managed servlet > > load on startup = new tree map < > ( ) ; for ( map . entry < string @$ servlet handler > entry : deployment . get servlets ( ) . get servlet handlers ( ) . entry set ( ) ) { managed servlet servlet = entry . get value ( ) . get managed servlet ( ) ; integer load on startup number = servlet,listeners add
if the source node and result node are the same use the dom result augmentor . otherwise use the dom result <PLACE_HOLDER> .,if ( source . get node ( ) == node result ) { fdom validator handler = fdom result augmentor ; fdom result augmentor . setdom result ( result ) ; f schema validator . set document handler ( fdom result augmentor ) ; return ; } if ( result . get node ( ) == null ) { try { document builder factory factory = jdk xml utils . getdom factory ( f component manager . get feature ( jdk xml utils . override_parser ) ) ; document builder builder = factory . new document builder ( ) ; result . set node ( builder . new document ( ) ) ; } catch ( parser configuration exception e ) { throw new sax exception ( e,node use
in this case first call to end offsets returns correct <PLACE_HOLDER> @$ but a second thread has updated the source topic but since it 's a source topic @$ the second check should not fire hence no exception,consumer . add end offsets ( collections . singleton map ( topic partition @$ __num__ ) ) ; changelog reader . register ( new state restorer ( topic partition @$ restore listener @$ null @$ __num__ @$ true @$ __str__ @$ identity ( ) ) ) ; expect ( active . restoring task for ( topic partition ) ) . and return ( task ) ; replay ( active ) ; changelog reader . restore ( active ) ;,offsets returns
find all configurations where the key contains any <PLACE_HOLDER> from hidden set,iterable < map . entry < string @$ string > > matching = iterables . filter ( conf @$ conf entry -> { for ( string name : hidden set ) { if ( conf entry . get key ( ) . starts with ( name ) ) { return true ; } } return false ; } ) ;,key contains
hregion calls this <PLACE_HOLDER>,verify ( wal @$ times ( __num__ ) ) . sync ( any long ( ) ) ;,hregion calls
check if the dataset version contains the correct modified <PLACE_HOLDER> of the underlying data location,assert . assert true ( dataset version . get date time ( ) . get millis ( ) == long . value of ( timestamp ) ) ; system . out . println ( dataset version ) ;,version contains
force verify that the destination fs matches the <PLACE_HOLDER>,fs . make qualified ( output ) ; sequence file . sorter sorter = new sequence file . sorter ( fs @$ text . class @$ copy listing file status . class @$ conf ) ; fs . delete ( output @$ false ) ; sorter . sort ( source listing @$ output ) ;,fs matches
a document with canonical tag should not get a webgraph <PLACE_HOLDER> @$ because that belongs to the canonical document,if ( webgraph != null && ( ! contains canonical || ( canonical_equal_sku != null && ( canonical_equal_sku . boolean value ( ) ) ) ) ) { list < solr input document > edges = webgraph . get edges ( subgraph @$ digesturl @$ response header @$ collections @$ crawldepth @$ process types @$ document . get hyperlinks ( ) . key set ( ) @$ source name ) ; doc . webgraph documents . add all ( edges ) ; } else { if ( all attr || contains ( collection schema . inboundlinks_protocol_sxt ) || contains ( collection schema . inboundlinks_urlstub_sxt ) || contains ( collection schema . inboundlinks_anchortext_txt ) || contains ( collection schema . outboundlinks_protocol_sxt ) || contains ( collection schema . outboundlinks_urlstub_sxt,document get
all honest parties should detect <PLACE_HOLDER>,boolean thrown = false ; try { run application ( test application ) ; } catch ( exception e ) { assert true ( e . get cause ( ) instanceof malicious exception ) ; thrown = true ; } assert true ( __str__ @$ thrown ) ;,parties detect
the second byte buffer set to null will throw an <PLACE_HOLDER>,client . new request ( __str__ @$ connector . get local port ( ) ) . scheme ( scenario . get scheme ( ) ) . content ( new content provider ( ) { @ override public long get length ( ) { return - __num__ ; } @ override public iterator < byte buffer > iterator ( ) { return new iterator < byte buffer > ( ) { @ override public boolean has next ( ) { return true ; } @ override public byte buffer next ( ) { throw new no such element exception ( __str__ ) ; } @ override public void remove ( ) { throw new unsupported operation exception ( ) ; } } ; } } ) . send (,buffer throw
compute the new escaped value if the new property value does n't match the previous <PLACE_HOLDER>,if ( g user diruri != null && user dir . equals ( g user dir ) ) { return g user diruri ; },value match
get drop location will throw an illegal state <PLACE_HOLDER> .,if ( ! support . is drop ( ) || ! ( support . get component ( ) instanceof types tree ) ) { return true ; } final j tree . drop location location = ( j tree . drop location ) support . get drop location ( ) ;,location throw
whether the epoch is still valid . note that the runtime guarantees sufficient <PLACE_HOLDER> of java thread pointers to allow age to be placed into low bits .,final word biasable lock bits = mark . and ( biased lock mask in place ( injected_vmconfig ) ) ;,runtime guarantees
the retransform native code that called this method does not propagate <PLACE_HOLDER> . instead of getting an uninformative generic error @$ catch problems here and print it @$ then exit .,e . print stack trace ( ) ; system . exit ( __num__ ) ;,method propagate
reopen ec file @$ get the new <PLACE_HOLDER> .,lb = dfs . get client ( ) . get located blocks ( ec file . to string ( ) @$ __num__ ) . get ( __num__ ) ; lsb = ( located striped block ) lb ; datanode info [ ] new dn locs = lsb . get locations ( ) ;,ec file
all tiers have <PLACE_HOLDER> so the evictor will evict till low watermark for all tiers,long unreserved tier1 = math . round ( capacity_bytes * low_watermark ) ; long unreserved tier2 = math . round ( capacity_bytes * low_watermark ) ; long unreserved tier3 = math . round ( capacity_bytes * low_watermark ) ; for ( int i = __num__ ; i < files_per_tier ; i ++ ) { write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ i * file_size @$ i * file_size ) ; write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ ( i + __num__ ) * file_size @$ i * file_size ) ; write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ ( i + __num__,tiers have
the query returns 4 @$ index 1 has the revision <PLACE_HOLDER> which we do n't test here,assert equals ( __num__ @$ row . length ) ;,4 has
these return a json <PLACE_HOLDER> which describes if and where the query was found . this api may break or disappear at any time in the future . since this is an api call rather than a website @$ we do n't use locale manager to change the tld .,try { string the query = args [ __num__ ] ; string the isbn = args [ __num__ ] ; string uri ; if ( locale manager . is book search url ( the isbn ) ) { int equals = the isbn . index of ( __str__ ) ; string volume id = the isbn . substring ( equals + __num__ ) ; uri = __str__ + volume id + __str__ + the query ; } else { uri = __str__ + the isbn + __str__ + the query ; } char sequence content = http helper . download via http ( uri @$ http helper . content type . json ) ; return new json object ( content . to string ( ) ) ; },these return
<PLACE_HOLDER> sa <PLACE_HOLDER> si <PLACE_HOLDER> su <PLACE_HOLDER> se <PLACE_HOLDER> so,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,kana si
might go out of sync with queue here @$ but should be minor slippage . will not accumulate <PLACE_HOLDER> either @$ but reset on every clear .,size . set ( __num__ ) ;,slippage accumulate
if the remote object implements the unreferenced <PLACE_HOLDER> @$ invoke its unreferenced callback in a separate thread .,remote obj = get impl ( ) ; if ( obj instanceof unreferenced ) { final unreferenced unref obj = ( unreferenced ) obj ; access controller . do privileged ( new new thread action ( ( ) -> { thread . current thread ( ) . set context class loader ( ccl ) ; access controller . do privileged ( ( privileged action < void > ) ( ) -> { unref obj . unreferenced ( ) ; return null ; } @$ acc ) ; } @$ __str__ + next thread num ++ @$ false @$ true ) ) . start ( ) ; },object implements
the execution context should collect the expected request <PLACE_HOLDER>,execution context context = new execution context ( true ) ;,context collect
expected exception . we used to expect that there would be un<PLACE_HOLDER>ed appends but this not reliable now that <PLACE_HOLDER> plays a roll in wall rolling . the above puts also now call <PLACE_HOLDER> .,log . error ( h base markers . fatal @$ __str__ @$ t ) ;,puts call
alice can discover <PLACE_HOLDER>,jenkins rule . web client wc = j . create web client ( ) . login ( __str__ ) ; try { wc . go to ( __str__ ) ; assert . fail ( __str__ ) ; } catch ( failing http status code exception e ) { assert . assert equals ( __str__ @$ __num__ @$ e . get status code ( ) ) ; },alice discover
write out the keep field and keep method <PLACE_HOLDER> @$ if any .,if ( class specification . field specifications != null || class specification . method specifications != null ) { writer . print ( __str__ ) ; writer . println ( configuration constants . open_keyword ) ; write field specification ( class specification . field specifications ) ; write method specification ( class specification . method specifications ) ; writer . println ( configuration constants . close_keyword ) ; } else { writer . println ( ) ; },the keep
note to translators : the substitution text is the name of a function . a literal string here means a constant string <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note means
the listener returns the new set of <PLACE_HOLDER>s to listen to . because 0 means no <PLACE_HOLDER> @$ the listener gets unregistered .,return __num__ ;,0 means
expression consumed <PLACE_HOLDER>,if ( state . backtracking == __num__ && input . index ( ) > first ) { int last = input . lt ( - __num__ ) . get token index ( ) ; string expr = to expression ( prefix @$ first @$ last ) ; pattern . constraint ( expr @$ positional ) ; base descr constr descr = pattern . get descr ( ) . get descrs ( ) . get ( pattern . get descr ( ) . get descrs ( ) . size ( ) - __num__ ) ; constr descr . set location ( input . get ( first ) . get line ( ) @$ input . get ( first ) . get char position in line ( ) ) ;,expression consumed
make sure eviction thread has entered run <PLACE_HOLDER>,while ( ! eviction thread . is entering run ( ) ) { thread . sleep ( __num__ ) ; },thread entered
we should not check the hash code of asset index <PLACE_HOLDER> since this <PLACE_HOLDER> is not consistent and mojang will modify this <PLACE_HOLDER> anytime . so asset index.hash might be outdated .,dependencies . add ( new file download task ( network utils . tourl ( dependency manager . get download provider ( ) . injecturl ( asset index info . get url ( ) ) ) @$ asset index file ) . set cache repository ( dependency manager . get cache repository ( ) ) ) ;,mojang modify
use the caller passed <PLACE_HOLDER> in erased indexes positions,for ( int output idx = __num__ @$ i = __num__ ; i < decoding state . erased indexes . length ; i ++ ) { boolean found = false ; for ( int j = __num__ ; j < erased or not to read indexes . length ; j ++ ) { if ( decoding state . erased indexes [ i ] == erased or not to read indexes [ j ] ) { found = true ; adjusted direct buffer outputs parameter [ j ] = coder util . reset buffer ( decoding state . outputs [ output idx ++ ] @$ data len ) ; } } if ( ! found ) { throw new hadoop illegal argument exception ( __str__ ) ; } },caller passed
only return true if the list items cover the entire <PLACE_HOLDER> of the view,if ( ret value ) { final int list top = m list padding != null ? m list padding . top : m padding top ; view first = get child at ( __num__ ) ; if ( first == null || first . get top ( ) > list top ) { return false ; } final int list bottom = get height ( ) - ( m list padding != null ? m list padding . bottom : m padding bottom ) ; view last = get child at ( get child count ( ) - __num__ ) ; if ( last == null || last . get bottom ( ) < list bottom ) { return false ; } },items cover
interceptor is null @$ call super <PLACE_HOLDER> .,code . mark ( interceptor null case ) ; code . invoke super ( super method @$ null @$ local this @$ local canvas ) ; code . return void ( ) ; final method id < g @$ void > calls super method = generated type . get method ( void_type @$ view method . get invoke name ( ) @$ canvas_type ) ; final code super code = dex maker . declare ( calls super method @$ public ) ; final local < g > super this = super code . get this ( generated type ) ; final local < canvas > super local canvas = super code . get parameter ( __num__ @$ canvas_type ) ; super code . invoke super ( super method,interceptor call
these nodes have no interesting type behavior . these nodes require data flow <PLACE_HOLDER> .,case param_list : case string_key : case member_function_def : case computed_prop : case label : case label_name : case switch : case break : case catch : case try : case script : case module_body : case export : case export_spec : case export_specs : case import : case import_spec : case import_specs : case import_star : case expr_result : case block : case root : case empty : case default_case : case continue : case debugger : case throw : case do : case if : case while : case for : case templatelit_sub : case iter_rest : case object_rest : case destructuring_lhs : typeable = false ; break ; case array_pattern : ensure typed ( n ) ; validator . expect autoboxes to iterable ( n,nodes require
test that an applied filter will include only those <PLACE_HOLDER> chosen to pass the filter,vt match tag foo tag = new test match tag ( __str__ ) ; vt match tag bar tag = new test match tag ( __str__ ) ; vt match tag baz tag = new test match tag ( __str__ ) ; vt session session = controller . get session ( ) ; list < vt match set > match sets = session . get match sets ( ) ; vt match set match set = match sets . get ( __num__ ) ; collection < vt match > matches = match set . get matches ( ) ; list < vt match > matches list = new array list < > ( matches ) ; vt match foo tag match1 = matches list . get ( __num__,filter include
this account type does n't have <PLACE_HOLDER> lock provider . lock resources by current account,if ( resource lock key provider == null ) { lock key = account id ; } else { lock key = resource lock key provider . get lock key ( account id ) ; },type have
all fields may not be comparable so only compare the ones that can be compared completion time is set when a node is finalized @$ so that can not be compared if the node is inprogress @$ do n't compare the last tx <PLACE_HOLDER> either,if ( this . get log segment sequence number ( ) != other . get log segment sequence number ( ) || this . log segment id != other . log segment id || this . first tx id != other . first tx id ) { ret val = false ; } else if ( this . inprogress ) { ret val = other . inprogress ; } else { ret val = ( ! other . inprogress && ( this . last tx id == other . last tx id ) ) ; } if ( ! ret val ) { log . warn ( __str__ @$ this @$ other ) ; } return ret val ;,ones compare
if we have <PLACE_HOLDER> regexes @$ the first one has 1 match @$ second has 12 matches @$ third has <PLACE_HOLDER> matches @$ then we have <PLACE_HOLDER>6 combinations of matches @$ thus <PLACE_HOLDER>6 outlinks to extracted .,int num outlinks = __num__ ; for ( match list match list : match lists . values ( ) ) { num outlinks *= match list . size ( ) ; } string [ ] regex names = match lists . key set ( ) . to array ( new string [ __num__ ] ) ; for ( int i = __num__ ; i < num outlinks ; i ++ ) { map < string @$ object > bindings = make bindings ( match lists @$ regex names @$ i ) ; build and add outlink ( curi @$ bindings ) ; },one has
no sub<PLACE_HOLDER> is open @$ so click only once to open the file <PLACE_HOLDER> .,menu bar . click item ( __str__ ) ; assert true ( is item visible ( __str__ ) ) ;,submenu open
explicit support for auto 'this ' parameter must inject pointer <PLACE_HOLDER> to obtain storage assignment,if ( add auto params && has this ) { inject auto this param = true ; data type [ ] ammended types = new data type [ data types . length + __num__ ] ; ammended types [ __num__ ] = data types [ __num__ ] ; ammended types [ __num__ ] = new pointer data type ( program . get data type manager ( ) ) ; if ( data types . length > __num__ ) { system . arraycopy ( data types @$ __num__ @$ ammended types @$ __num__ @$ data types . length - __num__ ) ; } data types = ammended types ; },support inject
note : just calling this method may trigger the awt <PLACE_HOLDER> to get created,return swing utilities . is event dispatch thread ( ) ;,method trigger
these might trigger <PLACE_HOLDER> .,if ( ! activemq client header . get sampled ( message @$ true ) ) { return trace context . disable sampling ( ) ; } final trace id trace id = populate trace id from request ( message ) ; final trace trace = trace id == null ? trace context . new trace object ( ) : trace context . continue trace object ( trace id ) ; if ( trace . can sampled ( ) ) { span recorder recorder = trace . get span recorder ( ) ; record root span ( recorder @$ target @$ args ) ; } return trace ;,these trigger
since the calling code will push the <PLACE_HOLDER> again @$ we better remove it here,os . remove ( __num__ ) ;,code push
if no <PLACE_HOLDER> is set yet @$ then generate a unique <PLACE_HOLDER>,if ( null == get mutable config ( ) . retrieve option ( option_target_name @$ null ) ) { string target name = target class . get canonical name ( ) + __str__ + uuid . randomuuid ( ) ; set target name ( target name ) ; } return this ;,then generate
we may have changed size @$ so let 's constrain the <PLACE_HOLDER> and bottom offset correctly @$ just in case we 're out of the bounds,set top and bottom offset ( math utils . clamp ( get top and bottom offset ( ) @$ - abl . get total scroll range ( ) @$ __num__ ) ) ;,"s" constrain
override the reporter with our own which collates the allocation <PLACE_HOLDER> .,close guard . set reporter ( new reporter ( ) { @ override public void report ( string message @$ throwable allocation site ) { close guard allocation sites . add ( allocation site ) ; } } ) ;,which collates
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,object [ ] [ ] contents = new object [ ] [ ] { { msg key . bad_msgkey @$ __str__ } @$ { msg key . bad_msgformat @$ __str__ } @$ { msg key . er_serializer_not_contenthandler @$ __str__ } @$ { msg key . er_resource_could_not_find @$ __str__ } @$ { msg key . er_resource_could_not_load @$ __str__ } @$ { msg key . er_buffer_size_lessthan_zero @$ __str__ } @$ { msg key . er_invalid_utf16_surrogate @$ __str__ } @$ { msg key . er_oierror @$ __str__ } @$ { msg key . er_illegal_attribute_position @$ __str__ } @$ { msg key . er_namespace_prefix @$ __str__ } @$ { msg key . er_stray_attribute @$ __str__ } @$ { msg key . er_stray_namespace @$ __str__ } @$ { msg key . er_could_not_load_resource @$,text specifies
when opearting with groups . the group must have entries so changes to take <PLACE_HOLDER> . otherwise group will be lost after loggingout,try { this . op set pers presence1 . subscribe ( group @$ this . fixture . userid2 ) ; synchronized ( o ) { o . wait ( __num__ ) ; } } catch ( exception ex ) { fail ( __str__ + group . get group name ( ) + __str__ + ex . get message ( ) ) ; },changes take
nb . this is always encoded with the implicit tag the checks only make <PLACE_HOLDER> if we assume implicit tagging @$ with explicit tagging the form is always constructed .,while ( val . data . available ( ) != __num__ ) { der value opt = val . data . get der value ( ) ; if ( opt . is context specific ( tag_min ) && ! opt . is constructed ( ) ) { opt . reset tag ( der value . tag_ integer ) ; minimum = opt . get integer ( ) ; } else if ( opt . is context specific ( tag_max ) && ! opt . is constructed ( ) ) { opt . reset tag ( der value . tag_ integer ) ; maximum = opt . get integer ( ) ; } else throw new io exception ( __str__ ) ; },checks make
data does n't allow null @$ eg concurrent hash <PLACE_HOLDER>,return result ;,data allow
get new display metrics based on the display adjustments given to the resources impl . update a <PLACE_HOLDER> if the compatibility info changed @$ because the resources impl object will handle the update internally .,display adjustments daj = r . get display adjustments ( ) ; if ( compat != null ) { daj = new display adjustments ( daj ) ; daj . set compatibility info ( compat ) ; } dm = get display metrics ( display id @$ daj ) ; if ( ! is default display ) { apply non default display metrics to configuration ( dm @$ tmp config ) ; } if ( has override configuration ) { tmp config . update from ( key . m override configuration ) ; } r . update configuration ( tmp config @$ dm @$ compat ) ; r . update configuration ( config @$ dm @$ compat ) ;,metrics update
if we get an exception here @$ then the current class does not declare the <PLACE_HOLDER> @$ but its parent may,try { method = containing class . get declared method ( method name @$ parameter types ) ; } catch ( no such method exception nsme ) { class < ? > parent class = containing class . get superclass ( ) ; if ( parent class != null ) { method = locate method object on class ( method name @$ parent class @$ parameter types ) ; } },class declare
spec recommendation : if cursor was at col 32 @$ move cursor back to col <PLACE_HOLDER> and erase both col <PLACE_HOLDER> & 32,if ( m col == max_cols - __num__ ) { m lines [ m row ] . set char at ( max_cols @$ ts ) ; },cursor col
calling traverse node here would create infinite <PLACE_HOLDER> for a function declaration,if ( node . is function ( ) ) { traverse function ( node @$ scope ) ; } else { traverse node ( node @$ scope ) ; },node create
the typed setters for these can not accept <PLACE_HOLDER> as input .,realm . cancel transaction ( ) ;,setters accept
this weirdness of setting it in our conf and then reading back does two <PLACE_HOLDER> . one @$ it handles the conversion of the time unit . two @$ it keeps the value around for later in case we need it again .,if ( key . equals ( conf vars . event_db_listener_clean_interval . to string ( ) ) || key . equals ( conf vars . event_db_listener_clean_interval . get hive name ( ) ) ) { long time = metastore conf . convert time str ( table event . get new value ( ) @$ time unit . seconds @$ time unit . seconds ) ; metastore conf . set time var ( get conf ( ) @$ metastore conf . conf vars . event_db_listener_clean_interval @$ time @$ time unit . seconds ) ; cleaner . set cleanup interval ( metastore conf . get time var ( get conf ( ) @$ metastore conf . conf vars . event_db_listener_clean_interval @$ time unit . milliseconds ) ) ; },weirdness does
user can specify a <PLACE_HOLDER> to be used to ex<PLACE_HOLDER> rmi object @$ in order to simplify firewall rules if <PLACE_HOLDER> is not specified random one will be allocated .,int rmi port = __num__ ; string rmi port str = props . get property ( property names . rmi_port ) ; try { if ( rmi port str != null ) { rmi port = integer . parse int ( rmi port str ) ; } } catch ( number format exception x ) { throw new agent configuration error ( invalid_jmxremote_rmi_port @$ x @$ rmi port str ) ; } if ( rmi port < __num__ ) { throw new agent configuration error ( invalid_jmxremote_rmi_port @$ rmi port str ) ; },user specify
all location settings are satisfied . the client can initialize location <PLACE_HOLDER> here .,task . add on success listener ( this @$ new on success listener < location settings response > ( ) { @ override public void on success ( location settings response location settings response ) { if ( check self permission ( android . manifest . permission . access_fine_location ) == package manager . permission_granted ) { m location callback = get location callback ( ) ; m location client . request location updates ( m location request @$ m location callback @$ null ) ; executors . new scheduled thread pool ( __num__ ) . schedule ( new runnable ( ) { @ override public void run ( ) { m location client . remove location updates ( m location callback ) ; } } @$ measure_time,client initialize
only one web node should do the <PLACE_HOLDER> in db to avoid any collision,if ( ! lock manager . try lock ( lock_name @$ lock_duration_in_second ) ) { return ; } db client . internal properties dao ( ) . save ( db session @$ projects_in_warning_internal_property @$ long . to string ( nb projects in warning ) ) ; db session . commit ( ) ;,node do
index name <PLACE_HOLDER> .,final int len = page utils . get unsigned byte ( page addr @$ off ) & __num__ ; off ++ ;,index name
the wallpaper has real ultimate <PLACE_HOLDER> @$ but we want to tell it about the overscan area .,df . set ( display frames . m overscan ) ; pf . set ( display frames . m overscan ) ; cf . set ( display frames . m unrestricted ) ; of . set ( display frames . m unrestricted ) ;,wallpaper has
create one build <PLACE_HOLDER> to generate build config.java .,build rule params build config params = params ; optional < build rule > values file rule = values file . flat map ( graph builder :: get rule ) ; if ( values file rule . is present ( ) ) { build config params = build config params . copy appending extra deps ( values file rule . get ( ) ) ; } android build config android build config = new android build config ( build config build target @$ project filesystem @$ build config params @$ java package @$ values @$ values file @$ use constant expressions ) ; graph builder . add to index ( android build config ) ;,one build
even if some members have fallen behind . the config offset used to generate the <PLACE_HOLDER> is included in the response so members that have fallen behind will not use the <PLACE_HOLDER> until they have caught up .,long max offset = null ; for ( map . entry < string @$ extended worker state > state entry : member configs . entry set ( ) ) { long member root offset = state entry . get value ( ) . offset ( ) ; if ( max offset == null ) max offset = member root offset ; else max offset = math . max ( max offset @$ member root offset ) ; } log . debug ( __str__ @$ max offset @$ coordinator . config snapshot ( ) . offset ( ) ) ; return max offset ;,members use
test compatibility with old uri properties bytes : client can understand <PLACE_HOLDER> published by old servers,string old uri json = __str__ ; uri properties from old bytes = json serializer . from bytes ( old uri json . get bytes ( ) ) ; uri properties created new = get instance with old arguments ( __str__ @$ uri weights ) ; assert equals ( from old bytes @$ created new ) ;,client understand
used when the parent view intercepts <PLACE_HOLDER> for things like scrolling,case motion event . action_cancel :,view intercepts
arrays.as list does not accept <PLACE_HOLDER> as parameter,if ( list == null ) { return feel fn result . of error ( new invalid parameters event ( severity . error @$ __str__ @$ __str__ ) ) ; },list accept
incremental publish should happen every 3 <PLACE_HOLDER>,max rows per segment = integer . max_value ; max total rows = __num__ ;,publish happen
sure we need to expand collection only if collection size more than <PLACE_HOLDER> @$ otherwise collection of composite keys already contains original composite key,if ( ! contains collection ) for ( int i = __num__ ; i < collection size ; i ++ ) { final o composite key composite key = new o composite key ( first key . get keys ( ) ) ; composite keys . add ( composite key ) ; } else throw new o index exception ( __str__ ) ;,collection size
checkstyle counts line <PLACE_HOLDER> from 0 but ide from 1,for ( integer line no : empty lines to log ) { log ( line no + __num__ @$ msg_multiple_lines_inside ) ; },checkstyle counts
only recreate the context if the sampler made a <PLACE_HOLDER>,if ( sampled == null && ( sampled = server sampler . try sample ( request ) ) != null ) { extracted = extracted . sampled ( sampled . boolean value ( ) ) ; } return extracted . context ( ) != null ? tracer . join span ( extracted . context ( ) ) : tracer . next span ( extracted ) ;,sampler made
add singleton getter adds a get instance <PLACE_HOLDER> to a class .,test same ( __str__ ) ;,a get
no messages @$ do n't continue <PLACE_HOLDER>,return false ;,messages continue
increment a counter again and check that the existing view was not modified @$ but a new view shows the updated <PLACE_HOLDER> .,increment counter ( startup progress @$ loading_edits @$ loading edits file @$ __num__ ) ; startup progress . end step ( loading_edits @$ loading edits file ) ; startup progress . end phase ( loading_edits ) ; assert equals ( __num__ @$ view . get count ( loading_edits @$ loading edits file ) ) ; view = startup progress . create view ( ) ; assert not null ( view ) ; assert equals ( __num__ @$ view . get count ( loading_edits @$ loading edits file ) ) ;,view shows
downgrade priority as user is disconnecting the hearing <PLACE_HOLDER> .,if ( m service . get priority ( device ) > bluetooth profile . priority_on ) { m service . set priority ( device @$ bluetooth profile . priority_on ) ; } return m service . disconnect ( device ) ;,priority disconnecting
one host matches this name <PLACE_HOLDER>,final string name pattern = __str__ ; when ( model . list hosts ( name pattern ) ) . then return ( immutable list . of ( __str__ ) ) ; final job id job id1 = job id . parse ( __str__ ) ; final job id job id2 = job id . parse ( __str__ ) ; final job job1 = job . new builder ( ) . build ( ) ; final job job2 = job . new builder ( ) . build ( ) ;,host matches
if the final number of <PLACE_HOLDER>s to scroll ends up being 0 @$ the view should still scroll at least one <PLACE_HOLDER> .,return capped scroll step != __num__ ? capped scroll step : direction ;,view scroll
this maybe a bit strange so let me explain . we allow acquiring shared <PLACE_HOLDER> while the parent proc or we have already held the x<PLACE_HOLDER> @$ and also allow releasing the <PLACE_HOLDER>s in any order @$ so it could happen that the x<PLACE_HOLDER> is released but there are still some procs holding the shared <PLACE_HOLDER> . in h base @$ this could happen,return shared lock == __num__ ;,which acquire
we always convert back to byte array @$ since we store it and field only supports <PLACE_HOLDER> so @$ we might as well do it here @$ and improve the performance of working with direct byte arrays,this . source = new bytes array ( objects . require non null ( source ) . to bytes ref ( ) ) ; this . x content type = objects . require non null ( x content type ) ; this . routing = routing ;,field supports
starting the process that fires the signal should start three process <PLACE_HOLDER> that are listening on that signal,runtime service . start process instance by key ( __str__ ) ;,signal start
case 1 : ensure that workers that are backed off are only executed when they are supposed to . greedy scheduler can schedule work <PLACE_HOLDER> that have already been backed off because it is holding on to snapshots of work <PLACE_HOLDER> . so worker wrapper needs to determine if the listenable worker is actually eligible to execute at this point in time .,return ;,case schedule
no <PLACE_HOLDER>s between the focused <PLACE_HOLDER> and the window is actually interested by the key event . let 's try the other j <PLACE_HOLDER> in this window .,if ( parent != null ) { return j component . process key bindings for all components ( e @$ parent @$ pressed ) ; } return false ;,"s" try
values of lists must be accumulated as object <PLACE_HOLDER> objects under the value key . will return as a array <PLACE_HOLDER> . called recursively to traverse the entire object graph of each item in the array .,if ( type . equals ( graphson tokens . type_list ) ) { array node list = ( array node ) value ; array node value array = value and type . put array ( graphson tokens . value ) ; for ( int ix = __num__ ; ix < list . size ( ) ; ix ++ ) { add object ( value array @$ get value ( get typed value from json node ( list . get ( ix ) ) @$ include type ) ) ; } } else if ( type . equals ( graphson tokens . type_map ) ) { object node converted map = json node factory . object node ( ) ; object node json object = ( object node ),values return
using property would change <PLACE_HOLDER> :,composite drawable . child drawable . bottom_absolute . set ( parent drawable . get child at ( __num__ ) @$ __num__ ) ; composite drawable . child drawable . bottom_fraction . set ( parent drawable . get child at ( __num__ ) @$ __num__ ) ; parent drawable . update bounds ( bounds ) ; adjusted bounds = drawable . get bounds ( ) ; expected bounds = new rect ( bounds ) ; expected bounds . top = - __num__ ; expected bounds . bottom = ( int ) ( __num__ * height ) ; assert equals ( expected bounds @$ adjusted bounds ) ;,property change
same document content . leave the data as is in tracking <PLACE_HOLDER> @$ and continue tracking previous attempts .,if ( array utils . slice equals ( document . bytes ( ) @$ document . offset ( ) @$ document . length ( ) @$ retry data buffer @$ __num__ @$ retry data buffer . length ) ) { retries . add ( previous attempt ) ; tracking bytes position ++ ; } else { bytes ref new entry = validate edited entry ( retry data buffer ) ; data . remove ( tracking bytes position ) ; data . copy from ( new entry ) ; if ( ba . available ( ) < new entry . length ( ) ) { tracking array expanded = true ; } previous attempt . attempt number = __num__ ; new document retries . add ( previous attempt ),content tracking
indentation should probably be dealt with before because an indentation has <PLACE_HOLDER> also on the following lines,if ( ! ( element instanceof csm indent ) && ! ( element instanceof csm unindent ) ) { throw new unsupported operation exception ( element . get class ( ) . get simple name ( ) ) ; },indentation has
notification payload <PLACE_HOLDER>,path scheme path = scheme generator . write scheme ( ) ; string scheme xml = project filesystem . read file if it exists ( scheme path ) . get ( ) ; system . out . println ( scheme xml ) ; document builder factory db factory = document builder factory . new instance ( ) ; document builder d builder = db factory . new document builder ( ) ; document scheme = d builder . parse ( project filesystem . new file input stream ( scheme path ) ) ; x path factory xpath factory = x path factory . new instance ( ) ; x path xpath = xpath factory . newx path ( ) ; x path expression expr = xpath . compile,notification payload
at least one pattern includes a path definition : we must use the inet path access handler as inet access handler does n't support path <PLACE_HOLDER>,if ( white . contains ( __str__ ) ) { white list handler = new inet path access handler ( ) ; } else { white list handler = new inet access handler ( ) ; },handler support
some more horrible functions . drop the passed function and return a <PLACE_HOLDER>,tree = __str__ ; check tree ( tree ) ;,functions function
check the buffer has an <PLACE_HOLDER> of the right size .,assert true ( b . has array ( ) ) ; byte [ ] array = b . array ( ) ; assert true ( array . length >= b . capacity ( ) ) ; assert equals ( __num__ @$ b . capacity ( ) ) ;,buffer has
only at least one metrics exporter implement had imported in pom then need register metrics <PLACE_HOLDER>,if ( exporters . size ( ) != __num__ ) { exporters . for each ( exporter -> exporter . set registry ( registry ) ) ; event bus manager . get ( ) . register ( new metrics subscriber ( registry ) ) ; },metrics register
sleep for a while so the user can inspect the <PLACE_HOLDER> . once the sleep is finished @$ tool and <PLACE_HOLDER> will be closed .,long minutes in millis = __num__ * __num__ * duration ; sleep ( minutes in millis ) ; throw new assertion failed error ( __str__ ) ; env . dispose ( ) ;,user inspect
verify also the blocking states dao adds <PLACE_HOLDER> not on disk,check blocking statesdao ( changed base entitlement @$ add on entitlement @$ base effective cancellation or change date @$ false ) ;,dao adds
make sure that the tab strips fills this <PLACE_HOLDER>,set fill viewport ( ! tab strip . is indicator always in center ( ) ) ; add view ( tab strip @$ layout params . match_parent @$ layout params . match_parent ) ;,strips fills
client sends an eds <PLACE_HOLDER> for the only cluster being watched to management server .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_eds @$ __str__ ) ) ) ;,client sends
use object equality to see if this status is the root placeholder . see the <PLACE_HOLDER> for root placeholder above for more information .,for ( file status status : candidates ) { if ( status == root placeholder ) { status = get file status ( root placeholder . get path ( ) ) ; if ( status == null ) continue ; } if ( filter . accept ( status . get path ( ) ) ) { results . add ( status ) ; } },equality see
this call does the right <PLACE_HOLDER> with a null transaction task queue,if ( ! m_complete msg . is restart ( ) ) { do commonspi complete actions ( ) ; log todr ( site connection . getdr gateway ( ) ) ; } else { m_txn state . set begin undo token ( site . k invalid undo token ) ; },call does
verify residual : should contain <PLACE_HOLDER> the current tracker did n't claim .,assert null ( residual . get poll watermark ( ) ) ; assert equals ( __num__ @$ residual . get completed ( ) . size ( ) ) ; assert equals ( __num__ @$ ( int ) residual . get termination state ( ) ) ;,tracker claim
we are not able to retrieve the exact number of cells <PLACE_HOLDER> result cell meta says us . we have to scan for the same results again . throwing dnrioe as a client retry on the same scanner will result in out of order scanner next exception,if ( cell scanner . advance ( ) == false ) { string msg = __str__ + no of results + __str__ + i + __str__ ; log . error ( msg ) ; throw new do not retryio exception ( msg ) ; },meta says
null should not impact <PLACE_HOLDER>,current min = double min kudaf . aggregate ( null @$ current min ) ; assert that ( __num__ @$ equal to ( current min ) ) ;,null impact
if the processed currency is different we return it ; otherwise we return null so that template does not print <PLACE_HOLDER> special,return ( processed currency != get currency ( ) ) ? processed currency : null ;,template print
create a stream in which to serialize the <PLACE_HOLDER> .,try { byte array output stream ostream = new byte array output stream ( ) ; object output stream p = new object output stream ( ostream ) ; p . write object ( old obj ) ; byte [ ] byte array = ostream . to byte array ( ) ; byte array input stream istream = new byte array input stream ( byte array ) ; object input stream q = new object input stream ( istream ) ; new obj = q . read object ( ) ; } catch ( exception ex ) { ex . print stack trace ( ) ; },which serialize
even after the security realm deleted the <PLACE_HOLDER> @$ they can still connect @$ until session invalidation,assert user connected ( wc @$ alice ) ; try { request renew seed for user ( alice ) ; fail ( __str__ ) ; } catch ( failing http status code exception e ) { },realm deleted
mix in various ways to specify no catalog . use java <PLACE_HOLDER> here .,string dep bytes = new string ( client utils . file to bytes ( new file ( deploymenturl ) ) @$ constants . utf8encoding ) ; volt table [ ] results = client . call procedure ( __str__ @$ null @$ dep bytes ) . get results ( ) ;,mix use
only the system can access this <PLACE_HOLDER> .,enforce system only ( ) ; return m network scorer app manager . get all valid scorers ( ) ;,system access
this flag will relax the <PLACE_HOLDER> that the scoped named in this version pragma directive can not resolve to a module .,parser . is module legal type ( true ) ; symtab entry an error occurred = new symtab entry ( ) ; symtab entry entry = parser . scoped name ( parser . current module @$ an error occurred ) ;,flag relax
note : this next bit changes the tree <PLACE_HOLDER> @$ rather than creating a new tree node . beware !,ht . set label ( lf . new label ( ht . value ( ) + __str__ ) ) ;,bit changes
user 2 reject the <PLACE_HOLDER> as planned,op setmuc2 . reject invitation ( invitation @$ invitation . get reason ( ) ) ; op set1 collector . wait for event ( __num__ ) ;,user reject
catching throwable here due to the fact that google app engine raises no class <PLACE_HOLDER> found error for unsafe .,return unsafe ;,engine raises
renumber used <PLACE_HOLDER> and keep track of the last line .,int id = __num__ ; int max line = __num__ ; for ( mapping m : mappings ) { if ( m . used ) { m . id = id ++ ; int end position line = m . end position . get line ( ) ; max line = math . max ( max line @$ end position line ) ; } },renumber used
the new calc <PLACE_HOLDER> must match the original sort <PLACE_HOLDER>,result = calc . copy ( calc . get trait set ( ) . replace ( orig sort collation ) @$ index scan @$ calc . get program ( ) @$ calc . get split count ( ) ) ;,collation match
if a user has set <PLACE_HOLDER> in one test @$ and then selects a different test which supports the same <PLACE_HOLDER> @$ those <PLACE_HOLDER> should have the same values that they did in the original test .,if ( curr args map . contains key ( name ) ) { string new val = curr args map . get ( name ) ; if ( new val != null && new val . length ( ) > __num__ ) { value = new val ; } } new args . add argument ( name @$ value ) ;,which supports
caller wants <PLACE_HOLDER> sent back to them .,if ( receiver != null ) { bundle send bundle = new bundle ( ) ; send bundle . put bundle ( assist_key_receiver_extras @$ pae . receiver extras ) ; try { pae . receiver . on handle assist data ( send bundle ) ; } catch ( remote exception e ) { } },caller wants
check that connection 2 recevied 3 <PLACE_HOLDER>,message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert null ( __str__ @$ message1 ) ;,connection recevied
if prev has <PLACE_HOLDER> than curr @$ the remainder will contain the extra current elements,if ( ! curr remainder . is empty ( ) ) { _info map . add rest spec info ( field . get name ( ) @$ compatibility info . type . array_not_equal @$ _info path @$ prev array ) ; return false ; } return true ;,prev has
if coprocessor exposes any <PLACE_HOLDER> @$ register them .,for ( service service : instance . get services ( ) ) { region . register service ( service ) ; } concurrent map < string @$ object > class data ;,coprocessor exposes
freeze failed when freeze amount less than 1 <PLACE_HOLDER>,ret1 = freeze balance2 ( from address @$ __num__ @$ __num__ @$ test key002 ) ; assert . assert equals ( ret1 . get code ( ) @$ grpcapi . return . response_code . contract_validate_error ) ; assert . assert equals ( ret1 . get message ( ) . to string utf8 ( ) @$ __str__ ) ;,freeze amount
sequence file.block compress <PLACE_HOLDER>,write test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ codec ) ; read test ( fs @$ count @$ seed @$ block compressed file ) ; sort test ( fs @$ count @$ megabytes @$ factor @$ false @$ block compressed file ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; sort test ( fs @$ count @$ megabytes @$ factor @$ true @$ block compressed file ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; merge test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ false @$ factor @$ megabytes ) ; check sort ( fs,file.block compress
the buffer contains the <PLACE_HOLDER> of possible silence .,if ( noise limit == input buffer . position ( ) ) { state = state_maybe_silent ; } else { input buffer . limit ( noise limit ) ; output ( input buffer ) ; },buffer contains
guice will only guarantee this assertion if multibinder ensures the bindings <PLACE_HOLDER> .,injector injector = guice . create injector ( ab @$ modules . override ( ab ) . with ( ab ) ) ; assert equals ( immutable set . of ( __str__ @$ __str__ ) @$ injector . get instance ( key . get ( set of string ) ) ) ;,multibinder ensures
complete task g w<PLACE_HOLDER>ic<PLACE_HOLDER> s<PLACE_HOLDER>ould start task <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name ( plan item instances @$ __str__ ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active @$ waiting_for_repetition ) ;,which start
metadata objects now write their descriptor <PLACE_HOLDER>,if ( cont type != container type . extended_content ) { out . write ( utils . get bytes ( get name ( ) @$ asf header . asf_charset ) ) ; out . write ( asf header . zero_term ) ; },objects write
verify the conflation indexes map equals the <PLACE_HOLDER> of updates,verify conflation indexes size ( __str__ @$ __num__ @$ vm4 @$ vm5 @$ vm6 @$ vm7 ) ; vm4 . invoke ( ( ) -> wan test base . check queue size ( __str__ @$ key values . size ( ) + update key values . size ( ) ) ) ;,map equals
ensure that evaluation succeeds if <PLACE_HOLDER> key does not throw an <PLACE_HOLDER> .,tester . get or create ( error key ) . set builder ( null ) ; tester . set ( error key @$ new string value ( __str__ ) ) ; tester . invalidate ( ) ; assert that ( tester . eval and get ( __str__ ) ) . is equal to ( new string value ( __str__ ) ) ;,key throw
the block we are looking at now has the same <PLACE_HOLDER> as the block that started the currently open catch range @$ and adding it to the currently open range wo n't cause it to be too long .,if ( current handlers . equals ( handlers ) && range is valid ( current start block @$ block @$ addresses ) ) { current end block = block ; continue ; },block has
items that are not skills do not have an experience <PLACE_HOLDER>,long experience = - __num__ ; if ( record . size ( ) == __num__ ) { experience = long . parse long ( record . get ( __num__ ) ) ; } skill skill = new skill ( rank @$ level @$ experience ) ; hiscore builder . set next skill ( skill ) ;,items have
the label does not cross a subpackage <PLACE_HOLDER> .,if ( containing pkg . equals ( label . get package identifier ( ) ) ) { return false ; },label cross
j table does n't allow <PLACE_HOLDER>,return __num__ ;,table allow
note : assumption @$ the decompiler wo n't get the <PLACE_HOLDER> if there is no guard,final code block jump block at = basic block model . get first code block containing ( location @$ monitor ) ;,assumption get
these two exceptions trigger an immediate <PLACE_HOLDER>,throw e ; log . debug ( __str__ @$ iterations @$ e ) ; ex = e ;,exceptions trigger
client has provided a <PLACE_HOLDER>,if ( delegation . get value ( ) != null ) { delegation query = __str__ + delegation ; } else { final token < ? extends token identifier > t = generate delegation token ( ugi @$ null ) ; delegation query = __str__ + new delegation param ( t . encode to url string ( ) ) ; },client provided
if we 're in system server and in a binder transaction we need to clear the calling uid . this works around code in system server that did not call clear calling identity @$ previously this was n't needed because reading settings did not do permission <PLACE_HOLDER> but thats no longer the case . long term this should be removed and callers should properly,if ( settings . is in system server ( ) && binder . get calling uid ( ) != process . my uid ( ) ) { final long token = binder . clear calling identity ( ) ; try { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ) ; } finally { binder . restore calling identity ( token ) ; } } else { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ),settings do
see also regression testing that ensures ee picks up catalog <PLACE_HOLDER> in test sql features new suite,compile limit delete stmt and check catalog ( ddl @$ null @$ __str__ @$ __num__ @$ null ) ;,ee picks
server has defined column <PLACE_HOLDER> explicitly,if ( h cell . is defined width ( ) ) { if ( needs indent && w < hierarchy column indent ) { w = hierarchy column indent ; } total explicit columns widths += w ; } else { if ( h cell . get expand ratio ( ) > __num__ ) { expand ratio divider += h cell . get expand ratio ( ) ; w = __num__ ; if ( needs indent && w < hierarchy column indent ) { hierarchy header with expand ratio = h cell ; } } else { int header width = h cell . get natural column width ( i ) ; int footer width = f cell . get natural column width ( i ) ; w,server defined
let any requests pending a response see an <PLACE_HOLDER>,try { peer . transport listener . on exception ( new transport disposedio exception ( __str__ + this + __str__ ) ) ; } catch ( exception ignore ) { },response see
creating 2 containers for same application which will be requesting same local <PLACE_HOLDER> . container 1 requesting local <PLACE_HOLDER> .,container id c id1 = builder utils . new container id ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; localizer context lc1 = new localizer context ( user @$ c id1 @$ null ) ; resource event req event1 = new resource request event ( lr @$ local resource visibility . private @$ lc1 ) ;,container requesting
image does not have <PLACE_HOLDER> and they are not required . specify that that the texture has no <PLACE_HOLDER> .,gl . gl tex parameteri ( target @$ gl2 . gl_texture_max_level @$ __num__ ) ;,texture has
test server initiated <PLACE_HOLDER>,get server counter start button ( ) . click ( ) ; wait until server counter changes ( ) ;,server initiated
if weighted values are used @$ then the pending operations will adjust the <PLACE_HOLDER> to reflect the correct weight,if ( node == null ) { return ; },operations adjust
first time for this <PLACE_HOLDER> . create <PLACE_HOLDER> local,if ( m == null ) { m = new hash map ( ) ; synchronized ( thread conn maps ) { if ( closed ) { owner . get cancel criterion ( ) . check cancel in progress ( null ) ; throw new distributed system disconnected exception ( __str__ ) ; } for ( iterator it = thread conn maps . iterator ( ) ; it . has next ( ) ; ) { reference r = ( reference ) it . next ( ) ; if ( r . get ( ) == null ) { it . remove ( ) ; } } thread conn maps . add ( new weak reference ( m ) ) ; } thread ordered conn map . set,time create
stop if the last poll produced new no <PLACE_HOLDER>,if ( last poll empty ) { return false ; },poll produced
set up initialization . if no initialization : use a random <PLACE_HOLDER>,if ( initialization == null ) { initialization = string . value of ( iter . get random character ( ) ) ; },initialization use
staxmapper will just output the <PLACE_HOLDER> without adding newlines if this is used,char [ ] chars = value . to char array ( ) ; writer . write characters ( chars @$ __num__ @$ chars . length ) ;,staxmapper output
end if : got four <PLACE_HOLDER>,if ( i == __num__ ) { return - __num__ ; } else { throw new java . io . io exception ( __str__ ) ; },end got
if bolt has not finished <PLACE_HOLDER> or was not exactly once mode @$ just process the tuple immediately,if ( ! init || ( batch cache != null && ! batch cache . is exactly once mode ( ) ) ) { } else { pending batch batch = batch cache . get next pending batch ( last successful batch ) ; if ( batch != null ) { list < byte [ ] > pending msgs = batch . get tuples ( ) ; while ( pending msgs != null ) { for ( byte [ ] msg : pending msgs ) { receiver . deserialize tuple ( deserializer @$ msg @$ queue ) ; } pending msgs = batch . get tuples ( ) ; } } kryo input . set buffer ( data ) ; kryo input . set position ( __num__ ),bolt finished
could define a get outputs <PLACE_HOLDER> instead,double [ ] ret result = new double [ num output ] ;,a get
decrypt temp 1 using trip lede s in cbc mode using the kek and the iv found in the previous step . call the result <PLACE_HOLDER> .,this . param plusiv = new parameters withiv ( this . param @$ this . iv ) ; this . engine . init ( false @$ this . param plusiv ) ; byte [ ] wkcks = new byte [ temp1 . length ] ; for ( int current byte pos = __num__ ; current byte pos != wkcks . length ; current byte pos += block size ) { engine . process block ( temp1 @$ current byte pos @$ wkcks @$ current byte pos ) ; },iv call
disable ssl hostname <PLACE_HOLDER> .,httpsurl connection . set default hostname verifier ( new hostname verifier ( ) { @ override public boolean verify ( string s @$ ssl session ssl ses ) { return true ; } } ) ; super . before test ( ) ;,ssl hostname
attribute value update allows nulls for its <PLACE_HOLDER> @$ since they are semantically meaningful . attribute <PLACE_HOLDER> never have null <PLACE_HOLDER> .,if ( attribute value != null && ! attribute action . delete . to string ( ) . equals ( attribute action ) ) { map . put ( attribute name @$ attribute value ) ; },values have
we ca n't use file.create temp file @$ since it will always create that file no matter what @$ and file.re name to does not allow the <PLACE_HOLDER> to be an existing file,try { stash location = new file ( destination . get parent file ( ) @$ destination . get name ( ) + __str__ + uuid . randomuuid ( ) . to string ( ) ) ; stashed = destination . rename to ( stash location ) ; if ( log . is debug enabled ( ) ) { log . debug ( string . format ( __str__ @$ stash location . get absolute path ( ) ) ) ; } } finally { if ( ! stashed ) { throw new sdk client exception ( __str__ + __str__ ) ; } },name allow
unknown host exception happens if we ca n't resolve host<PLACE_HOLDER> into ip address . unknown host exception 's get message method returns just the host<PLACE_HOLDER> which is a useless message @$ so log the exception class <PLACE_HOLDER> to provide more info .,log . debug ( e . to string ( ) ) ; throw new helios exception ( __str__ + uri @$ e ) ; throw new helios exception ( __str__ + uri @$ e ) ;,hostname log
start spi start <PLACE_HOLDER> .,start stopwatch ( ) ; assert parameter ( cred != null @$ __str__ ) ; if ( log . is debug enabled ( ) ) { log . debug ( config info ( __str__ @$ cred ) ) ; log . debug ( config info ( __str__ @$ cfg ) ) ; log . debug ( config info ( __str__ @$ bucket name suffix ) ) ; log . debug ( config info ( __str__ @$ bucket endpoint ) ) ; log . debug ( config info ( __str__ @$ sse alg ) ) ; } if ( cfg == null ) u . warn ( log @$ __str__ ) ; if ( f . is empty ( bucket name suffix ) ) { u . warn (,spi start
the exception transports the actual return <PLACE_HOLDER> .,return ex . get result ( ) ;,exception transports
check that participant status listener is working <PLACE_HOLDER>,assert equals ( __str__ @$ room + __str__ @$ answer [ __num__ ] ) ;,listener working
do this after the build so that other goals do n't use the <PLACE_HOLDER> if it does n't exist,if ( repository != null ) { write image info ( repository @$ tag ) ; } write metadata ( log ) ; if ( repository == null ) { log . info ( message format . format ( __str__ @$ image id ) ) ; } else { log . info ( message format . format ( __str__ @$ format image name ( repository @$ tag ) ) ) ; },goals use
other protocols could cause <PLACE_HOLDER>,if ( url != null && url . starts with ( __str__ ) ) { this . sb . tables . bookmarks . add bookmark ( this . bmk_user @$ bmk @$ true @$ true ) ; if ( this . autotag ) { if ( ! this . empty ) { this . auto tagging queue . put ( url ) ; } else if ( ! bmk . contains key ( y mark entry . bookmark . tags . key ( ) ) || bmk . get ( y mark entry . bookmark . tags . key ( ) ) . equals ( y mark entry . bookmark . tags . deflt ( ) ) ) { this . auto tagging queue . put ( url,protocols cause
replace the begin <PLACE_HOLDER> with its formatted string and store the old timestamp,long old timestamp = format timestamp ( report generator . begin_date_consumer_name @$ data context ) ;,the begin
was the initial port specified ? if so @$ override this property normally is applied for the client side configuration of resolvers . here we are using it to define the server port that the with <PLACE_HOLDER> the resolvers communicate .,if ( args [ i ] . equals ( __str__ ) && i < args . length - __num__ ) { initial port = java . lang . integer . parse int ( args [ i + __num__ ] ) ; },resolvers communicate
unmanaged realm list does not know actual element <PLACE_HOLDER> .,sb . append ( __str__ ) ;,list know
in order to allow programs to use a single <PLACE_HOLDER> as the display for multiple tabs @$ we will not change the visible compnent if the currently selected tab has a null <PLACE_HOLDER> . this is a bit dicey @$ as we do n't explicitly state we support this in the spec @$ but since programs are now depending on this @$ we 're,if ( selected component != null ) { if ( selected component != visible component && visible component != null ) { if ( swing utilities . find focus owner ( visible component ) != null ) { should change focus = true ; } } set visible component ( selected component ) ; } int tx @$ ty @$ tw @$ th ;,tab has
process fn sets only a single <PLACE_HOLDER> .,if ( iterables . is empty ( kwi . elements iterable ( ) ) ) { timer data timer = iterables . get only element ( kwi . timers iterable ( ) ) ; return ( ( window namespace ) timer . get namespace ( ) ) . get window ( ) ; } else { windowed value < t > value = iterables . get only element ( kwi . elements iterable ( ) ) ; return iterables . get only element ( value . get windows ( ) ) ; },fn sets
if the parent has a default acl @$ copy that default acl an <PLACE_HOLDER> with the umask as the new file 's access acl . if it is a metadata load operation @$ do not consider the umask .,default access control list d acl = current inode directory . get defaultacl ( ) ; short mode = context . is metadata load ( ) ? mode . create full access ( ) . to short ( ) : new file . get mode ( ) ; if ( ! d acl . is empty ( ) ) { access control list acl = d acl . generate child fileacl ( mode ) ; new file . set internal acl ( acl ) ; } if ( file context . is cacheable ( ) ) { new file . set cacheable ( true ) ; } if ( file context . get write type ( ) == write type . async_through ) { new file . set,default acl
create 4 threads @$ each one a daemon thread running the event index <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { final thread t = new thread ( task ) ; t . set daemon ( true ) ; t . start ( ) ; } assert equals ( __num__ @$ commit count . get ( ) ) ;,threads running
bind and start to accept incoming <PLACE_HOLDER> .,channel channel = bootstrap . bind ( new inet socket address ( port ) ) ; all channels . add ( channel ) ; log . info ( __str__ @$ port @$ buffer_size @$ max workers ) ; this . is backpressure enable = config extension . is backpressure enable ( storm conf ) ; if ( is backpressure enable ) { flow ctrl handler = new netty server flow ctrl handler ( storm conf @$ all channels @$ worker tasks ) ; flow ctrl handler . start ( ) ; },bind accept
if this instruction writes ref <PLACE_HOLDER> @$ do n't need to track back since this is the start of the scope,if ( write ref set . contains ( instr . get address ( ) ) ) { write scope . add range ( instr . get min address ( ) @$ instr . get max address ( ) ) ; } else { set scope before instruction ( instr @$ sub set @$ write checker ) ; } set scope after instruction ( instr @$ sub set @$ write checker ) ;,instruction writes
both max and bucket time have 8 <PLACE_HOLDER> left .,assert equals ( __num__ * minute_in_millis @$ m quota controller . get remaining execution time locked ( source_user_id @$ source_package ) ) ;,time left
try to deserialize using deserialize read our writable row <PLACE_HOLDER> created by ser de .,for ( int i = __num__ ; i < row count ; i ++ ) { object [ ] row = rows [ i ] ; lazy binary deserialize read lazy binary deserialize read = new lazy binary deserialize read ( type infos @$ false ) ; bytes writable bytes writable = serde bytes [ i ] ; lazy binary deserialize read . set ( bytes writable . get bytes ( ) @$ __num__ @$ bytes writable . get length ( ) ) ; for ( int index = __num__ ; index < column count ; index ++ ) { if ( use include columns && ! columns to include [ index ] ) { lazy binary deserialize read . skip next field ( ) ; } else,deserialize read
fetch the auto complete text <PLACE_HOLDER> and set an adapter,auto complete text view actv = find view by id ( r . id . widgets_autocompletetextview ) ; actv . set adapter ( new array adapter < > ( this @$ android . r . layout . simple_dropdown_item_1line @$ cheeses . s cheese strings ) ) ;,auto complete
debug level messages are not logged to channelz @$ thus channelz still has the last <PLACE_HOLDER> .,assert that ( stats . channel trace . events ) . contains exactly ( event ) ; assert that ( logs ) . contains ( __str__ + log prefix + __str__ ) ;,channelz has
pointer exceed depth <PLACE_HOLDER> of 2,if ( pointer classification == pointer reference classification . deep ) { return pointer_label_prefix + __str__ + pointer_label_prefix ; },pointer exceed
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
the app master jar <PLACE_HOLDER> .,job . add file to class path ( app_jar ) ;,master jar
verify that username exists and the associated password matches the <PLACE_HOLDER> supplied by the client .,username = a credentials [ __num__ ] ; password = a credentials [ __num__ ] ; if ( username == null || password == null ) { final string message = __str__ ; authentication failure ( __str__ @$ message ) ; },password matches
grouping by size . if group <PLACE_HOLDER> size exceeds specified limit @$ then execute transformation and flush group <PLACE_HOLDER> .,if ( trans executor data . group size > __num__ ) { if ( trans executor data . group buffer . size ( ) >= trans executor data . group size ) { execute transformation ( incoming field values ) ; } } return true ;,then execute
common outer join result <PLACE_HOLDER> .,switch ( save join result ) { case match : equal key series hash map result indices [ equal key series count ] = hash map result count ; equal key series all match indices [ equal key series count ] = all match count ; equal key series is single value [ equal key series count ] = hash map results [ hash map result count ] . is single row ( ) ; equal key series duplicate counts [ equal key series count ] = __num__ ; all matchs [ all match count ++ ] = batch index ; break ; case spill : spills [ spill count ] = batch index ; spill hash map result indices [ spill count ] = hash map result,outer join
copying over empty byte <PLACE_HOLDER> @$ but let 's keep things consistent .,final multi partmime input stream body less body input stream = new multi partmime input stream . builder ( new byte array input stream ( _body less body . get part data ( ) . copy bytes ( ) ) @$ scheduled executor service @$ _body less body . get part headers ( ) ) . with write chunk size ( chunk size ) . build ( ) ; final multi partmime input stream purely empty body input stream = new multi partmime input stream . builder ( new byte array input stream ( _purely empty body . get part data ( ) . copy bytes ( ) ) @$ scheduled executor service @$ _purely empty body . get part headers ( ) ) . with write chunk,copying empty
<PLACE_HOLDER> not found @$ add this <PLACE_HOLDER> and its hfile paths pair to the list,if ( ! found family ) { add family and itsh file path to table in map ( family @$ path to hfile fromns @$ familyh file paths list ) ; },family add
<PLACE_HOLDER> while other threads control the lock grantor future result terminate <PLACE_HOLDER> if other thread has already made us lock grantor terminate <PLACE_HOLDER> if this thread gets control of lock grantor future result,while ( ! own lock grantor future result ) { assert . assert holds lock ( this . destroy lock @$ false ) ; synchronized ( this . lock grantor id lock ) { if ( is currently or is making lock grantor ( ) ) { return ; } else if ( this . lock grantor future result != null ) { lock grantor future result ref = this . lock grantor future result ; } else { own lock grantor future result = true ; lock grantor future result ref = new future result ( this . dm . get cancel criterion ( ) ) ; if ( is debug enabled_dls ) { logger . trace ( log marker . dls_verbose @$ __str__ ) ; },grantor terminate
record driver so other threads add unpartitioned sources can see the driver note : this must be done before reading unpartitioned sources @$ so we see a consistent <PLACE_HOLDER> of the unpartitioned sources,drivers . add ( new weak reference < > ( driver ) ) ; if ( partitioned split != null ) { driver . update source ( new task source ( partitioned split . get plan node id ( ) @$ immutable set . of ( partitioned split ) @$ true ) ) ; },sources see
test named <PLACE_HOLDER> .,message format mf = new message format ( __str__ ) ; if ( ! mf . uses named arguments ( ) ) { errln ( __str__ ) ; } mf = new message format ( __str__ ) ; if ( ! mf . uses named arguments ( ) ) { errln ( __str__ ) ; },test named
verify that we have no <PLACE_HOLDER> in the table because neither file should have been loaded even though one of the files could have .,table table = test_util . get connection ( ) . get table ( tn ) ; result scanner scanner = table . get scanner ( new scan ( ) ) ; try { assert null ( __str__ @$ scanner . next ( ) ) ; } finally { scanner . close ( ) ; },one have
the second time should contain the <PLACE_HOLDER>,has certificate = info . get transport context ( ) instanceof x509 certificate [ ] ;,time contain
<PLACE_HOLDER> wa <PLACE_HOLDER> small wa <PLACE_HOLDER> wi <PLACE_HOLDER> we <PLACE_HOLDER> wo,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,kana wa
note : the subclass will release the 'work finished latch ' early @$ before work is actually finished . this means that the test may proceed and perform <PLACE_HOLDER> earlier than anticipated .,work start latch . count down ( ) ; try { work finished latch . await ( __num__ @$ time unit . seconds ) ; } catch ( interrupted exception e ) { assert . fail ( __str__ ) ; } work finished latch = new count down latch ( __num__ ) ;,test proceed
not sure which thread gets the <PLACE_HOLDER> first so we add them to a map and verify that some thread had 4 threads waiting @$ 3 threads @$ etc .,assert that ( map . size ( ) @$ equal to ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ;,thread gets
m<PLACE_HOLDER>ke sure b will h<PLACE_HOLDER>ve less digits th<PLACE_HOLDER>n <PLACE_HOLDER>,if ( b digits > a digits ) { temp = a digits ; a digits = b digits ; b digits = temp ; } if ( b digits == a digits ) { return ; } assert true ( a digits + b digits == __num__ && a digits > __num__ && b digits > __num__ ) ; a = new decimal128 ( ) ; sa = make numeric string ( a digits ) ; a . update ( sa @$ ( short ) __num__ ) ; b = new decimal128 ( ) ; sb = make numeric string ( b digits ) ; b . update ( sb @$ ( short ) __num__ ) ; if ( b . is zero ( ) ) {,b have
durable topic consumer will consume 10 <PLACE_HOLDER> and disconnect,create consumer ( interest @$ initial max msgs ) ; thread . sleep ( reconnect sleep ) ; create consumer ( interest @$ cleanup msg count ) ; string broker version = ( string ) mbean server . get attribute ( new object name ( __str__ ) @$ __str__ ) ; log . info ( __str__ + broker version ) ; final string the jmx object = __str__ + __str__ + __str__ ; assert true ( __str__ @$ wait . wait for ( new wait . condition ( ) { @ override public boolean is satisified ( ) throws exception { integer pending queue size = ( integer ) mbean server . get attribute ( new object name ( the jmx object ) @$ __str__ ) ; log,consumer consume
x has one right <PLACE_HOLDER>,if ( x . left == null ) { if ( p . left == x ) p . left = x . right ; else p . right = x . right ; x . right . parent = p ; } else { if ( p . left == x ) p . left = x . left ; else p . right = x . left ; x . left . parent = p ; },x has
this pattern has an <PLACE_HOLDER> .,pattern pattern1 = verifier component mock factory . create pattern1 ( ) ; restriction r1 = literal restriction . create restriction ( pattern1 @$ __str__ ) ; restriction r2 = literal restriction . create restriction ( pattern1 @$ __str__ ) ; incompatibility i1 = new incompatibility ( r1 @$ r2 ) ; sub pattern pp1 = new sub pattern ( pattern1 @$ __num__ ) ; pp1 . add ( r1 ) ; pp1 . add ( r2 ) ; restriction r3 = new variable restriction ( pattern1 ) ; restriction r4 = new variable restriction ( pattern1 ) ; incompatibility i2 = new incompatibility ( r1 @$ r2 ) ; sub pattern pp2 = new sub pattern ( pattern1 @$ __num__ ) ; pp2 . add ( r1,pattern has
use cached buffered <PLACE_HOLDER> for now .,return m player . get buffered position ( ) ;,use cached
just let the user enter their <PLACE_HOLDER>,if ( saved instance state != null || m called ) { return ; },user enter
first version @$ using union find data <PLACE_HOLDER>,connect ( i @$ j ) ;,version find
this is a safety net . if the current subtype ca n't be added to the history and the framework could n't find the last <PLACE_HOLDER> @$ we will make the last <PLACE_HOLDER> be the most applicable enabled keyboard subtype of the system <PLACE_HOLDER>s .,if ( text utils . is empty ( target last imi id ) && ! input method utils . can add to last input method ( m current subtype ) ) { final list < input method info > enabled = m settings . get enabled input method list locked ( ) ; if ( enabled != null ) { final int n = enabled . size ( ) ; final string locale = m current subtype == null ? m res . get configuration ( ) . locale . to string ( ) : m current subtype . get locale ( ) ; for ( int i = __num__ ; i < n ; ++ i ) { final input method info imi = enabled . get,framework find
second request should have the same session <PLACE_HOLDER> .,response = client . new request ( __str__ @$ connector . get local port ( ) ) . scheme ( http scheme . https . as string ( ) ) . header ( http header . connection @$ __str__ ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( http status . ok_200 @$ response . get status ( ) ) ; assert true ( server latch . await ( __num__ @$ time unit . seconds ) ) ; assert true ( client latch . await ( __num__ @$ time unit . seconds ) ) ;,request have
in case we 're not on the first page and the <PLACE_HOLDER> exceeds the page <PLACE_HOLDER> @$ we need to do an additional count for the total,if ( page != __num__ || tasks . size ( ) == size ) { long total count = task info query wrapper . get task info query ( ) . count ( ) ; result . set total ( long . value of ( total count . int value ( ) ) ) ; result . set start ( page * size ) ; } return result ;,size exceeds
the dispatcher fires the <PLACE_HOLDER> corresponding to the closest matching rule and passes the context along,dispatcher disp = new default rule dispatcher ( op traits rules proc factory . get default rule ( ) @$ op rules @$ annotate ctx ) ; graph walker ogw = new level order walker ( disp @$ __num__ ) ;,dispatcher fires
null cache does n't actually retain the <PLACE_HOLDER>,assert false ( cache . contains ( __str__ ) ) ;,cache retain
a node throwing the exception because of the can take <PLACE_HOLDER> of the attached faulty node property,dumb slave faulty agent = r . create online slave ( label expression . get ( __str__ ) ) ; faulty agent . get node properties ( ) . add ( new faulty node property ( ) ) ;,node take
shutdown <PLACE_HOLDER> @$ then join <PLACE_HOLDER> @$ hopefully save some shutdown time for tests .,synchronized ( m_all sites ) { for ( mp ro site context site : m_all sites ) { site . shutdown ( ) ; } for ( mp ro site context site : m_all sites ) { site . join thread ( ) ; } },all join
update the position now since the mrp will update it only once the video is playing <PLACE_HOLDER> . in particular @$ if the video is paused @$ the mrp does n't send the command until the video is resumed .,m position extrapolator . on seek ( msec ) ; m seeking = true ; intent intent = new intent ( media control intent . action_seek ) ; intent . add category ( media control intent . category_remote_playback ) ; intent . put extra ( media control intent . extra_session_id @$ m current session id ) ; intent . put extra ( media control intent . extra_item_id @$ m current item id ) ; intent . put extra ( media control intent . extra_item_content_position @$ msec ) ; send intent to route ( intent @$ new result bundle handler ( ) { @ override public void on result ( bundle data ) { if ( get media state listener ( ) != null ) get media state listener,video playing
must look for next delims . simply attempting to <PLACE_HOLDER> the pattern at this point may find a <PLACE_HOLDER> but it might not be the first longest <PLACE_HOLDER> because of missing input @$ or it might <PLACE_HOLDER> a partial token instead of the whole thing .,return null ;,delims find
throws obsolete version exception if another process has created a new <PLACE_HOLDER> already,store client . put ( new key . map value ( ) @$ new node ) ;,process created
next call for get buffered will return the buffered <PLACE_HOLDER> that came after the partial boundary match,if ( buffered byte != boundary [ __num__ ] ) { matched count = __num__ ; } else { matched count = __num__ ; buffered byte = - __num__ ; },call return
start maintenance for 2 nd dn ; still get 3 <PLACE_HOLDER> .,start maintenance ( cluster . get namesystem ( ) @$ dnm @$ __num__ ) ; verify file location ( file index @$ __num__ ) ; data node dn1 = cluster . get data nodes ( ) . get ( __num__ ) ; data node dn2 = cluster . get data nodes ( ) . get ( __num__ ) ;,maintenance get
initialize file <PLACE_HOLDER> as the current user .,return file system . get ( uri @$ hadoop conf ) ;,initialize file
invalidate operations do n't fire cache <PLACE_HOLDER> @$ so do n't assert they were fired .,if ( op != op . invalidate ) { accessor . invoke ( new serializable callable ( ) { @ override public object call ( ) throws exception { region cust = get cache ( ) . get region ( customer ) ; assert false ( ( ( test cache writer ) cust . get attributes ( ) . get cache writer ( ) ) . was fired ) ; region order = get cache ( ) . get region ( order ) ; assert false ( ( ( test cache writer ) order . get attributes ( ) . get cache writer ( ) ) . was fired ) ; region ref = get cache ( ) . get region ( d_reference ) ; assert false (,operations fire
<PLACE_HOLDER> is open @$ should n't close underlying <PLACE_HOLDER>,bris . get input stream ( ) ; verify ( bris @$ times ( bris opens ) ) . open input stream ( mockito . any long ( ) ) ; verify ( bris @$ times ( bris closes ) ) . close ( ) ; verify ( mock stream . in @$ times ( is closes ) ) . close ( ) ;,stream close
create enough element converters note : we have to have a separate element converter for each element @$ because the element converters can reuse the internal <PLACE_HOLDER> . so it 's not safe to use the same element converter to convert multiple elements .,int size = inputoi . get list length ( input ) ; while ( element converters . size ( ) < size ) { element converters . add ( get converter ( input elementoi @$ output elementoi ) ) ; },converters reuse
long iterations should use long key <PLACE_HOLDER> 1,if ( ++ iter cnt > short_iter_threshold ) { key iter = new var key iterator1 ( ( var key iterator2 ) key iter ) ; },iterations use
note to translators : the stylesheet contained an <PLACE_HOLDER> that was not recognized as part of the xsl syntax . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,stylesheet contained
if the entry length field spans a sector <PLACE_HOLDER> @$ write the high order bit of the entry length @$ otherwise write zero for the entry length .,long entry start = log . get file pointer ( ) ; boolean spans boundary = log . check spans boundary ( entry start ) ; write int ( log @$ spans boundary ? __num__ << __num__ : __num__ ) ;,field spans
accumulo connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support
steam reader need copy <PLACE_HOLDER> from pread reader,reader . copy fields ( initial reader ) ;,reader need
the following block of code calculates and updates the max drop rate if the client had been fully degraded in the past and has not received enough requests since being fully degraded . to increase the chances of the client receiving a request @$ we change the max drop rate @$ which influences the maximum <PLACE_HOLDER> of computed drop rate @$ which is used,if ( call count < degrader control . get min call count ( ) ) { if ( strategy == partition degrader load balancer state . strategy . load_balance ) { double old max drop rate = client updater . get max drop rate ( ) ; double transmission rate = __num__ - old max drop rate ; if ( transmission rate <= __num__ ) { transmission rate = initial recovery level ; } else { transmission rate *= ring ramp factor ; transmission rate = math . min ( transmission rate @$ __num__ ) ; } client updater . set max drop rate ( __num__ - transmission rate ) ; } } else if ( ring ramp factor > fast_recovery_threshold && ! degrader control . is high,which influences
user will have an unknown <PLACE_HOLDER> .,password authentication a = privileged request password authentication ( host @$ addr @$ port @$ __str__ @$ realm @$ scheme @$ url @$ requestor type . proxy ) ; if ( a != null ) { ret = new basic authentication ( true @$ host @$ port @$ realm @$ a ) ; } break ; case digest : a = privileged request password authentication ( host @$ null @$ port @$ url . get protocol ( ) @$ realm @$ scheme @$ url @$ requestor type . proxy ) ; if ( a != null ) { digest authentication . parameters params = new digest authentication . parameters ( ) ; ret = new digest authentication ( true @$ host @$ port @$ realm @$ scheme @$,user have
check <PLACE_HOLDER> errors only if automatic <PLACE_HOLDER> is enabled . empty @$ required fields will generate a <PLACE_HOLDER> error containing the required error string . for these fields the exclamation mark will be hidden but the error must still be sent to the client .,validator . invalid value exception validation error = null ; if ( is validation visible ( ) ) { try { validate ( ) ; } catch ( validator . invalid value exception e ) { if ( ! e . is invisible ( ) ) { validation error = e ; } } },fields generate
bottom over scroll might not grab all scrolling <PLACE_HOLDER> @$ we have to scroll as well .,if ( anchor_scrolling ) { float scroll amount = new bottom amount < __num__ ? new bottom amount : __num__ ; expandable view first child = get first child not gone ( ) ; float top = first child . get translationy ( ) ; float distance to top = m scroll anchor view . get translationy ( ) - top - m scroll anchor viewy ; if ( distance to top < - scroll amount ) { float current top pixels = get current over scrolled pixels ( true ) ; set over scrolled pixels ( current top pixels + ( - scroll amount - distance to top ) @$ true @$ false ) ; m scroll anchor view = first child ; m scroll anchor viewy,bottom grab
cancellation of the job should remove <PLACE_HOLDER>,final completable future < job result > job result future = dispatcher gateway . request job result ( job graph . get jobid ( ) @$ testing_timeout ) ; dispatcher gateway . cancel job ( job graph . get jobid ( ) @$ testing_timeout ) . get ( ) ;,cancellation remove
this needs to do a binary search @$ but a binary search is somewhat tough because the sequence test involves two <PLACE_HOLDER> .,if ( test ) { int size = size ( ) @$ i ; for ( i = size - __num__ ; i >= __num__ ; i -- ) { node child = ( node ) element at ( i ) ; if ( child == node ) { i = - __num__ ; break ; } if ( ! dom2 helper . is node after ( node @$ child ) ) { break ; } } if ( i != - __num__ ) { insert index = i + __num__ ; insert element at ( node @$ insert index ) ; } } else { insert index = this . size ( ) ; boolean foundit = false ; for ( int i = __num__ ; i,test involves
clear references to <PLACE_HOLDER> @$ this should orphan the <PLACE_HOLDER> which should in turn trigger orphan removal logic .,team . set one vone player ( null ) ; team2 . set one vone player ( null ) ; tx . commit ( ) ; s . close ( ) ; s = open session ( ) ; tx = s . begin transaction ( ) ; count = ( ( long ) s . create query ( __str__ ) . iterate ( ) . next ( ) ) . int value ( ) ; assert equals ( __str__ + count @$ count @$ __num__ ) ; tx . commit ( ) ; s . close ( ) ;,references orphan
empty selection not allowed @$ keep old <PLACE_HOLDER>,if ( ! is null selection allowed ( ) && s . is empty ( ) ) { mark as dirty ( ) ; return ; },selection keep
failed emit will discard buffer 's <PLACE_HOLDER>,actual . on complete ( ) ;,emit discard
some carriers will download duplicate mms <PLACE_HOLDER> without this ack . when using the system sending method @$ apparently google does not do this for us . not sure why . you might have to have users manually enter their apn settings if you can not get them from the system somehow .,return null ;,carriers download
<PLACE_HOLDER>alidate that ebml read <PLACE_HOLDER>ersion is supported . this extractor only supports <PLACE_HOLDER> 1 .,if ( value != __num__ ) { throw new parser exception ( __str__ + value + __str__ ) ; } break ; case id_doc_type_read_version :,extractor supports
this thread <PLACE_HOLDER>s the <PLACE_HOLDER> in server 1 first @$ then server 2 .,lock the locks ( server1 @$ server2 @$ count down latch ) ;,thread locks
no need for a state sets <PLACE_HOLDER> for the child since it only has two states,if ( indicator != null && indicator . is stateful ( ) ) { final int state set [ ] = pos . position . flat list pos == pos . group metadata . last child fl pos ? child_last_state_set : empty_state_set ; indicator . set state ( state set ) ; },need sets
similar to the test above @$ except the threads will have multiple page <PLACE_HOLDER> opened at a time .,assert timeout preemptively ( of millis ( semi_long_timeout_millis ) @$ ( ) -> { final atomic boolean should stop = new atomic boolean ( ) ; final int cache pages = __num__ ; final int file pages = cache pages * __num__ ; final int thread count = __num__ ; final int page size = thread count * __num__ ; final int max cursors per thread = cache pages / ( __num__ + thread count ) ; assert that ( max cursors per thread * thread count @$ less than ( cache pages ) ) ; get page cache ( fs @$ cache pages @$ page cache tracer . null @$ page cursor tracer supplier . null ) ; try ( paged file paged file = page cache,threads have
service type method automatic recovery deliberately disabled as spring has it 's own recovery <PLACE_HOLDER>,class < ? > channeln class = class . for name ( __str__ ) ; method channeln basic publish = channeln class . get declared method ( __str__ @$ string . class @$ string . class @$ boolean . class @$ boolean . class @$ amqp . basic properties . class @$ byte [ ] . class ) ; expected trace channeln basic publish trace = expectations . event ( rabbitmq test constants . rabbitmq_client @$ channeln basic publish @$ null @$ remote address @$ __str__ + rabbitmq test constants . exchange @$ expectations . annotation ( __str__ @$ rabbitmq test constants . exchange ) @$ expectations . annotation ( __str__ @$ rabbitmq test constants . routing_key_push ) ) ;,recovery has
check stop requested inside the sync to prevent a race in which the wait misses the stopper 's <PLACE_HOLDER> .,if ( stop requested ( ) ) { return ; },wait misses
make sure the home button has an accurate content <PLACE_HOLDER> for accessibility .,update home accessibility ( enable ) ;,button has
if deletion failed then the directory scanner will cleanup the <PLACE_HOLDER> eventually .,cleanup replica ( bpid @$ replica info ) ;,scanner cleanup
classify 1 has <PLACE_HOLDER> a as an allowed type,assert true ( classify type1 . can apply to entity type ( entity typea ) ) ;,1 has
the edge that leaves the do <PLACE_HOLDER> if the condition fails .,if ( ! cond . is true ( ) ) { create edge ( node @$ branch . on_false @$ compute follow node ( node @$ this ) ) ; },edge do
shuffle the list so all clients do n't prefer the same <PLACE_HOLDER>,if ( iter == null ) { list < redis client > clients = new array list < > ( sentinels . values ( ) ) ; collections . shuffle ( clients ) ; iter = clients . iterator ( ) ; },clients prefer
exopackage builds <PLACE_HOLDER> to jar @$ otherwise @$ <PLACE_HOLDER> to raw .,dex store default dex store = exopackage mode . enabled for secondary dexes ( exopackage modes ) ? dex store . jar : dex store . raw ; dex split strategy dex split strategy = args . get minimize primary dex size ( ) ? dex split strategy . minimize_primary_dex_size : dex split strategy . maximize_primary_dex_size ; return new dex split mode ( args . get use split dex ( ) @$ dex split strategy @$ args . get dex compression ( ) . or else ( default dex store ) @$ args . get linear alloc hard limit ( ) @$ args . get dex group lib limit ( ) @$ args . get primary dex patterns ( ) @$ args . get primary dex classes file,exopackage builds
if no entities have <PLACE_HOLDER> just return the first entity .,return get schema ( ) . get entity groups ( ) . iterator ( ) . next ( ) ; return __str__ ;,entities have
test key store only contain key pair <PLACE_HOLDER> .,if ( is cert entry == true ) { throw new runtime exception ( __str__ + __str__ + __str__ + alias ) ; } boolean is key entry = input key store . is key entry ( alias ) ; key key = null ; if ( is key entry ) { key = input key store . get key ( alias @$ in key pass . to char array ( ) ) ; } else { throw new runtime exception ( __str__ + alias ) ; } output key store . set key entry ( alias @$ key @$ out key pass . to char array ( ) @$ certs ) ;,store contain
both threads have terminated and cancel should have cancelled the insert <PLACE_HOLDER> .,result set res = st . execute query ( __str__ ) ; assert false ( res . next ( ) ) ; try { st . close ( ) ; st . cancel ( ) ; fail ( __str__ ) ; } catch ( sql exception e ) { },cancel cancelled
block until the user takes <PLACE_HOLDER>,wait for input ( ) ;,user takes
subsequent same key presses move the keyboard <PLACE_HOLDER> to the next object that starts with the same letter .,if ( ( prefix . length ( ) == __num__ ) && ( c == prefix . char at ( __num__ ) ) ) { starting row ++ ; } else { prefix = typed string ; },presses move
check if application needs more <PLACE_HOLDER> @$ skip if it does n't need more .,if ( reserved container == null ) { if ( ! application . has pending resource request ( candidates . get partition ( ) @$ scheduling mode ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + application . get application attempt id ( ) + __str__ + scheduling mode . name ( ) + __str__ + candidates . get partition ( ) ) ; } activities logger . app . record skipped app activity without allocation ( activities manager @$ node @$ application @$ null @$ activity diagnostic constant . application_do_not_need_resource @$ activity level . app ) ; return cs assignment . skip_assignment ; } for ( scheduler request key scheduler key : application . get scheduler,application needs
iterate over all <PLACE_HOLDER>s found in the string @$ expanding each found <PLACE_HOLDER> .,int last end = __num__ ; macro finder automaton matcher = new macro finder automaton ( blob ) ; while ( matcher . has next ( ) ) { macro match result match result = matcher . next ( ) ; combiner . add string ( blob . substring ( last end @$ match result . get start index ( ) ) ) ; if ( match result . is escaped ( ) ) { combiner . add string ( blob . substring ( match result . get start index ( ) + ( resolve escaping ? __num__ : __num__ ) @$ match result . get end index ( ) ) ) ; } else { macro replacer < t > replacer = replacers . get ( match,iterate found
add listener has thrown an exception ! set future.run ca n't throw any <PLACE_HOLDER> so this must have been caused by add listener itself . the most likely explanation is a misconfigured mock . try to switch to failure .,failure failure ; try { failure = new failure ( t ) ; } catch ( throwable oom most likely ) { failure = failure . fallback_instance ; },future.run throw
... and draw the grid <PLACE_HOLDER> .,super . dispatch draw ( canvas ) ;,and draw
record should have a <PLACE_HOLDER> that indicates that the 'child ' is a choice of 2 different record types,assert true ( first outer schema . get data type ( __str__ ) . get ( ) . get field type ( ) == record field type . choice ) ; final list < data type > first sub types = ( ( choice data type ) first outer schema . get data type ( __str__ ) . get ( ) ) . get possible sub types ( ) ; assert equals ( __num__ @$ first sub types . size ( ) ) ; assert equals ( __num__ @$ first sub types . stream ( ) . filter ( type -> type . get field type ( ) == record field type . record ) . count ( ) ) ;,record have
can throw m bean exception wrapping a role not found exception : throw wrapped exception shall not throw <PLACE_HOLDER> not found exception or reflection exception,try { list < object name > invoke result = cast ( mym bean server . invoke ( ( ( object name ) rel obj ) @$ __str__ @$ params @$ signature ) ) ; if ( invoke result == null || invoke result instanceof array list < ? > ) result = invoke result ; else result = new array list < object name > ( invoke result ) ; } catch ( instance not found exception exc1 ) { throw new runtime exception ( exc1 . get message ( ) ) ; } catch ( reflection exception exc2 ) { throw new runtime exception ( exc2 . get message ( ) ) ; } catch ( m bean exception exc3 ) { exception wrapped exc =,exception throw
this means someone referenced an invalid <PLACE_HOLDER> on a module object . this should be an error @$ so just rewrite and let the type checker complain later . it is n't a super clear error @$ but we 're working on type checking modules soon .,break ;,someone referenced
the partial solution was on the path here . check whether the channel requires certain <PLACE_HOLDER> that are met @$ or whether the channel introduces new <PLACE_HOLDER>,found = true ;,channel introduces
ordinary bmp code point @$ excluding leading surrogates . bmp uses a single level <PLACE_HOLDER> . bmp index starts at offset 0 in the trie 2 index . 32 bit data is stored in the index array itself .,if ( code point < __num__ || ( code point > __num__ && code point <= __num__ ) ) { ix = index [ code point > > utrie2_shift_2 ] ; ix = ( ix << utrie2_index_shift ) + ( code point & utrie2_data_mask ) ; value = data32 [ ix ] ; return value ; },bmp uses
simulate node status updater impl sending c mgr signal <PLACE_HOLDER> event,signal container request signal req = signal container request . new instance ( c id @$ command ) ; list < signal container request > reqs = new array list < > ( ) ; reqs . add ( signal req ) ; container manager . handle ( new c mgr signal containers event ( reqs ) ) ; final argument captor < container signal context > signal context captor = argument captor . for class ( container signal context . class ) ; if ( signal . equals ( signal . null ) ) { verify ( exec @$ never ( ) ) . signal container ( signal context captor . capture ( ) ) ; } else { verify ( exec @$ timeout ( __num__ ),c mgr
handler which do n't send an <PLACE_HOLDER>,sys prop configa . eth62 = new eth62 ( ) { @ override protected void process get block headers ( get block headers message msg ) { if ( msg . get max headers ( ) == __num__ ) { super . process get block headers ( msg ) ; return ; } list < block header > headers = new array list < > ( ) ; for ( int i = __num__ ; i < mainb1b10 . size ( ) ; i ++ ) { headers . add ( mainb1b10 . get ( i ) . get header ( ) ) ; } block headers message response = new block headers message ( headers ) ; send message ( response ) ; } } ;,which send
permission bits from server response will have the <PLACE_HOLDER> for accuracy .,if ( this . permission != null ) { perm arg = this . permission ; },bits have
memory merge may have limited the changed code <PLACE_HOLDER> we are working with .,address set view result set = result pgm . get memory ( ) ; this . latest set = latest changes . get register address set ( ) . intersect ( result set ) ; this . my set = my changes . get register address set ( ) . intersect ( result set ) ; original context = original pgm . get program context ( ) ; latest context = latest pgm . get program context ( ) ; my context = my pgm . get program context ( ) ; result context = result pgm . get program context ( ) ; registers = my context . get registers ( ) ; try { diff original latest = new program diff ( original pgm @$ latest,merge limited
client configuration setting has a <PLACE_HOLDER> over system property,system . set property ( __str__ @$ __str__ ) ; config = new client configuration ( ) . with non proxy hosts ( __str__ ) ; assert equals ( __str__ @$ config . get non proxy hosts ( ) ) ; config . set protocol ( protocol . http ) ; assert equals ( __str__ @$ config . get non proxy hosts ( ) ) ; system . clear property ( __str__ ) ;,setting has
event bus goes here . this method is only called if one of the binding providers provide a binding for the given 'item <PLACE_HOLDER> ' . see the readme.md for the item formatting for the isy link .,this . logger . debug ( __str__ @$ item name @$ command ) ; isy binding config config = get binding config by name ( item name ) ; if ( config != null ) { process command ( config @$ command ) ; } else { this . logger . warn ( __str__ @$ item name ) ; },one provide
secs since midnight now represents <PLACE_HOLDER>,secs since midnight -= minutes ;,secs represents
color <PLACE_HOLDER> always override tint <PLACE_HOLDER> .,final color filter color filter = ( m color filter == null ? m tint filter : m color filter ) ;,filters override
wait until vm 1 has sent the <PLACE_HOLDER>,try { pausibletx . class . wait ( ) ; } catch ( interrupted exception ie ) { fail ( __str__ + ie ) ; },vm sent
analyzer has already handled this <PLACE_HOLDER>,continue ;,analyzer handled
noinspection object <PLACE_HOLDER> in loop,if ( rid . equals ( new o record id ( configuration1 . get schema record id ( ) ) ) && rid . equals ( new o record id ( configuration2 . get schema record id ( ) ) ) ) continue ; if ( rid . get cluster id ( ) == __num__ && rid . get cluster position ( ) == __num__ ) { if ( ! storage type1 . equals ( storage type2 ) ) continue ; },noinspection object
counts can take some <PLACE_HOLDER> to propagate,while ( deadline . has time left ( ) && ( stats . get num successful ( ) != __num__ || stats . get num failed ( ) != __num__ ) ) { thread . sleep ( __num__ ) ; } assert equals ( __num__ @$ stats . get num requests ( ) ) ; assert equals ( __num__ @$ stats . get num successful ( ) ) ; assert equals ( __num__ @$ stats . get num failed ( ) ) ; if ( client != null ) { try { client . shutdown ( ) . get ( __num__ @$ time unit . seconds ) ; } catch ( exception e ) { e . print stack trace ( ) ; } assert . assert true,counts take
return true if child views should not receive this <PLACE_HOLDER> .,if ( should consume event ) { return true ; },views receive
output node local <PLACE_HOLDER>,local properties lp = p . get local properties ( ) ; writer . print ( __str__ ) ; if ( lp . get ordering ( ) != null ) { add property ( writer @$ __str__ @$ lp . get ordering ( ) . to string ( ) @$ true ) ; } else { add property ( writer @$ __str__ @$ __str__ @$ true ) ; } if ( lp . get grouped fields ( ) != null && lp . get grouped fields ( ) . size ( ) > __num__ ) { add property ( writer @$ __str__ @$ lp . get grouped fields ( ) . to string ( ) @$ false ) ; } else { add property ( writer @$ __str__,output node
for enumerations run the <PLACE_HOLDER>,if ( collection utils . is not empty ( types def . get enum defs ( ) ) ) { for ( atlas enum def enum def : types def . get enum defs ( ) ) { atlas enum def created def = enum def store . create ( enum def @$ null ) ; ttr . update guid ( created def . get name ( ) @$ created def . get guid ( ) ) ; ret . get enum defs ( ) . add ( created def ) ; } },enumerations run
use a local copy in case another thread changes <PLACE_HOLDER>,last mapping < configuration property name > last = this . last mapped configuration property name ; if ( last != null && last . is from ( configuration property name ) ) { return last . get mapping ( ) ; } string converted name = configuration property name . to string ( ) ; property mapping [ ] mapping = { new property mapping ( converted name @$ configuration property name ) } ; this . last mapped configuration property name = new last mapping < > ( configuration property name @$ mapping ) ; return mapping ;,thread changes
get the width of a task view so that we know how wide to draw the header <PLACE_HOLDER> .,if ( use grid layout ) { task grid layout algorithm grid layout = m dummy stack view . get grid algorithm ( ) ; grid layout . initialize ( window rect ) ; task view width = ( int ) grid layout . get transform ( __num__ @$ stack . get task count ( ) @$ new task view transform ( ) @$ stack layout ) . rect . width ( ) ; } else { rect task view bounds = stack layout . get untransformed task view bounds ( ) ; if ( ! task view bounds . is empty ( ) ) { task view width = task view bounds . width ( ) ; } },wide draw
set domain file for newly opened domain object note : some domain object implementations may throw runtime <PLACE_HOLDER> so cleanup is required in those cases,try { domain obj . set domain file ( get domain file ( ) ) ; } catch ( exception e ) { domain obj . release ( consumer ) ; file manager . clear domain object ( get pathname ( ) ) ; throwable cause = e . get cause ( ) ; if ( cause instanceof io exception ) { throw ( io exception ) cause ; } else if ( cause instanceof version exception ) { throw ( version exception ) cause ; } throw new io exception ( e . get message ( ) @$ e ) ; },implementations throw
screenshot does not include <PLACE_HOLDER> !,final display display = display content . get display ( ) ; int original rotation = display . get rotation ( ) ; final int original width ; final int original height ; display info display info = display content . get display info ( ) ; if ( fixed to user rotation ) { m force default orientation = true ; original width = display content . m base display width ; original height = display content . m base display height ; } else { original width = display info . logical width ; original height = display info . logical height ; },screenshot include
promote to public the reason for this is that the executor may try and call these things and as it is not in the hierarchy it can not . the necessary knock on effect is that subtypes get their <PLACE_HOLDER> promoted to public too ...,if ( ( modifiers & modifier . protected ) != __num__ ) { new modifiers = modifier . public ; } else if ( ( modifiers & constants . acc_public_private_protected ) == __num__ ) { new modifiers = modifier . public ; },subtypes get
without any read permission the anon have <PLACE_HOLDER> to the user list,jenkins rule . web client wc = j . create web client ( ) ; wc . get options ( ) . set throw exception on failing status code ( false ) ; wc . get options ( ) . set redirect enabled ( false ) ; page page = wc . go to ( __str__ @$ null ) ; check page is redirected to login ( page ) ; assert that ( page . get web response ( ) . get content as string ( ) @$ not ( contains string ( __str__ ) ) ) ; assert request was not blocked ( ) ; page = wc . go to ( __str__ @$ null ) ; assert equals ( __num__ @$ page . get web response,anon have
because this function allocates a totally random <PLACE_HOLDER> @$ it really should n't ever fail to allocate an <PLACE_HOLDER> ; we simply need this because the exception is checked .,throw new resource unavailable exception ( __str__ ) ;,function allocates
no good way to verify that we have an array ... although could i guess check via json parser . so let 's assume <PLACE_HOLDER> is working fine @$ for now .,if ( _injectables != null ) { inject values ( ctxt @$ bean ) ; } final settable bean property [ ] props = _ordered properties ; int i = __num__ ; final int prop count = props . length ; while ( true ) { if ( p . next token ( ) == json token . end_array ) { return bean ; } if ( i == prop count ) { break ; } settable bean property prop = props [ i ] ; if ( prop != null ) { try { prop . deserialize and set ( p @$ ctxt @$ bean ) ; } catch ( exception e ) { throw wrap and throw ( e @$ bean @$ prop . get name,way assume
the internal ci adapters report negative connection <PLACE_HOLDER> and are n't included in public stats .,for ( map . entry < long @$ client interface handle manager > e : m_cihm . entry set ( ) ) { if ( e . get key ( ) > __num__ ) { long admin mode = e . get value ( ) . is admin ? __num__ : __num__ ; long read wait = e . get value ( ) . connection . read stream ( ) . data available ( ) ; long write wait = e . get value ( ) . connection . write stream ( ) . get outstanding message count ( ) ; long outstanding txns = e . get value ( ) . get outstanding txns ( ) ; client_stats . put ( e . get key ( ),adapters report
if we have n't configured the standard dialect @$ preprocessing inlined expressions makes no <PLACE_HOLDER> and in any case we would be in risk of not knowing what we are doing ...,if ( ! this . dialect set configuration . is standard dialect present ( ) ) { return false ; },expressions makes
check to make sure that the runtime defined <PLACE_HOLDER> contain all the template <PLACE_HOLDER> .,set < string > runtime tag keys = new hash set < > ( tags . key set ( ) ) ; runtime tag keys . add all ( config ( ) . tags ( ) . key set ( ) ) ; set < string > template tag keys = template . tags ( ) ; if ( ! runtime tag keys . equals ( template tag keys ) ) { throw new illegal argument exception ( __str__ + template . name ( ) + __str__ + __str__ + runtime tag keys . to string ( ) + __str__ + template tag keys . to string ( ) ) ; } return this . metric name ( template . name ( ) @$ template . group (,tags contain
need to put this here @$ since vanilla assumes this <PLACE_HOLDER> after the vignette was rendered .,if ( pre ( vignette ) ) { gl state manager . enable depth test ( ) ; gl state manager . blend func separate ( gl state manager . source factor . src_alpha @$ gl state manager . dest factor . one_minus_src_alpha @$ gl state manager . source factor . one @$ gl state manager . dest factor . zero ) ; return ; },vanilla assumes
the response object does n't contain any relevant <PLACE_HOLDER> so we have to create a copy of values being sent over the network in case m jp settings is modified while awaiting response,final jetpack settings model sent jp data = new jetpack settings model ( m jp settings ) ; ++ m save request count ; word press . get rest client utilsv1_1 ( ) . set jetpack settings ( m site . get site id ( ) @$ params @$ new rest request . listener ( ) { @ override public void on response ( json object response ) { app log . d ( app log . t . api @$ __str__ ) ; m remote jp settings . monitor active = sent jp data . monitor active ; m remote jp settings . jetpack protect enabled = sent jp data . jetpack protect enabled ; m remote jp settings . jetpack protect whitelist . clear ( ),object contain
this input reference is part of a join expression that refers an expression that comes from either node . to resolve it we need to find its index in the inner node 's expression list using the inner node <PLACE_HOLDER>,if ( resolving outer || resolving inner ) { preconditions . check state ( expr input indx < m_program . get project list ( ) . size ( ) ) ; final rex local ref input local ref = m_program . get project list ( ) . get ( expr input indx ) ; input idx = input local ref . get index ( ) ; rex node expr = m_program . get expr list ( ) . get ( input idx ) ; if ( expr . isa ( sql kind . cast ) ) { expr = ( ( rex call ) expr ) . get operands ( ) . get ( __num__ ) ; } final abstract expression volt expr = expr . accept (,node using
if we 're restarting this transaction @$ and we only have local <PLACE_HOLDER> @$ add some dummy remote <PLACE_HOLDER> so that we can avoid injecting a borrow task into the local buddy site before the complete transaction message with the restart flag reaches it . right now @$ any read on a replicated table which has no distributed <PLACE_HOLDER> will generate these null fragments,boolean used null fragment = false ; if ( m_is restart && m_remote work == null ) { used null fragment = true ; m_remote work = new fragment task message ( m_local work . get initiatorhs id ( ) @$ m_local work . get coordinatorhs id ( ) @$ m_local work . get txn id ( ) @$ m_local work . get unique id ( ) @$ m_local work . is read only ( ) @$ false @$ false @$ m_n part txn @$ m_restart timestamp ) ; m_remote work . set empty for restart ( get next dependency id ( ) ) ; if ( ! m_have distributed init task && ! is for replay ( ) && ! is read only ( ) ) {,which has
if user has specified a stopword <PLACE_HOLDER> other than the template,try { if ( ! conf . get ( __str__ ) . equals ( __str__ ) ) { stop words = new array list < string > ( ) ; string stop word ; buffered reader br = new buffered reader ( conf . get conf resource as reader ( ( conf . get ( __str__ ) ) ) ) ; while ( ( stop word = br . read line ( ) ) != null ) { stop words . add ( stop word ) ; } log . info ( __str__ @$ conf . get ( __str__ ) ) ; } int [ ] ngram arr = retrieve ngrams ( conf ) ; int mingram = ngram arr [ __num__ ] ; int maxgram = ngram,user specified
clients have rw <PLACE_HOLDER> to iud acid tables,table . set access type ( accesstype_readwrite ) ;,clients rw
we need to return the connection back to the jdbc pool . in order to do that we need to close it @$ to keep the remaining code readable one can add a headers end <PLACE_HOLDER> to close the connection .,routing context . add headers end handler ( done -> conn . close ( v -> { } ) ) ; routing context . next ( ) ;,one add
in case when remote tx has updated the current <PLACE_HOLDER> before .,ignore ( ) ;,tx updated
create 100 threads @$ each will do its own <PLACE_HOLDER>,putter [ ] all = new putter [ threads100 ] ;,each do
peek does n't change <PLACE_HOLDER>,assert equals ( __num__ @$ buffer . position ( ) ) ;,peek change
given that a second user has selected the bubble clock <PLACE_HOLDER>,when ( m mock settings wrapper . get lock screen custom clock face ( secondary_user_id ) ) . then return ( bubble_clock ) ;,user selected
fall through to skip since comment has no <PLACE_HOLDER> .,case ls parser filter . filter_skip : default :,comment has
first @$ reduce to account asset balance . else ca n't complete this test <PLACE_HOLDER> .,account capsule to account = db manager . get account store ( ) . get ( byte array . from hex string ( to_address ) ) ; to account . reduce asset amount ( byte string . copy from utf8 ( asset_name ) . to byte array ( ) @$ total_supply - __num__ ) ; db manager . get account store ( ) . put ( to account . get address ( ) . to byte array ( ) @$ to account ) ; participate asset issue actuator actuator = new participate asset issue actuator ( ) ; actuator . set chain base manager ( db manager . get chain base manager ( ) ) . set any ( get contract ( __num__ ) ) ; transaction result,first complete
do n't remove if user specifies <PLACE_HOLDER> or is recorded in previous run,if ( partition . get has user specified high watermark ( ) || this . work unit state . get prop ( configuration keys . work_unit_state_actual_high_water_mark_key ) != null ) { return false ; } return true ;,user specifies
visual index is now the visual <PLACE_HOLDER> minus the bi di controls @$ which should match the <PLACE_HOLDER> of the bidi test.txt ordering .,if ( is ok && ordering count != visual index ) { errln ( __str__ + ordering count + __str__ + visual index ) ; is ok = false ; } if ( ! is ok ) { print error line ( ) ; string builder eord = new string builder ( __str__ ) ; for ( i = __num__ ; i < ordering count ; ++ i ) { eord . append ( __str__ ) . append ( ( char ) ( __str__ + ordering [ i ] ) ) ; } string builder aord = new string builder ( __str__ ) ; for ( i = __num__ ; i < result length ; ++ i ) { int logical index = ubidi . get logical index,which match
note to translators : the stylesheet contained an <PLACE_HOLDER> that was not recognized as part of the xsl syntax . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,stylesheet contained
for batch mode @$ the max watermark should force the <PLACE_HOLDER> to close,if ( watermark . is equal ( bounded window . timestamp_max_value ) ) { finish bundle ( emitter ) ; },watermark force
the block would have been decremented for the scan case as it was wrapped before even the post next hook gets <PLACE_HOLDER> . giving some time for the block to be decremented,thread . sleep ( __num__ ) ; iterator < cached block > iterator = cache . iterator ( ) ; boolean used blocks found = false ; int ref count = __num__ ; while ( iterator . has next ( ) ) { cached block next = iterator . next ( ) ; block cache key cache key = new block cache key ( next . get filename ( ) @$ next . get offset ( ) ) ; if ( cache instanceof bucket cache ) { ref count = ( ( bucket cache ) cache ) . get rpc ref count ( cache key ) ; } else if ( cache instanceof combined block cache ) { ref count = ( ( combined block cache ) cache,hook gets
current cluster has 2 <PLACE_HOLDER>,current . add ( __num__ ) ; current . add ( __num__ ) ;,cluster has
user 2 changes his <PLACE_HOLDER> to away,muc2 . change availability status ( __str__ @$ presence . mode . away ) ; thread . sleep ( __num__ ) ;,user changes
if this does n't throw then the clues map does n't have duplicate <PLACE_HOLDER>,new coordinate clue ( __str__ @$ new world point ( __num__ @$ __num__ @$ __num__ ) @$ null ) ;,map have
if no child found a more specific <PLACE_HOLDER> @$ see if we have a value for the property,if ( ! decision found && property map . contains key ( property name ) ) { property value value = property map . get ( property name ) ; list < string > decision path = get decision path ( ) ; decision set . add decision ( new decision ( value . value @$ decision path @$ value . source ) ) ; decision found = true ; } return decision found ;,child found
if an instance of media tray @$ fall thru returning all media printable <PLACE_HOLDER>,if ( ! ( media name instanceof media size name ) ) { media name = null ; },instance printable
should not do <PLACE_HOLDER> when cursor is null,matcher . cursor = null ; matcher . next is ( opcodes . nop ) ;,not do
flushing and minor compaction keep delete <PLACE_HOLDER>,region . flush ( true ) ; region . compact ( false ) ; assert equals ( __num__ @$ count delete markers ( region ) ) ; region . compact ( true ) ;,compaction keep
all partitioned tables get insert crud <PLACE_HOLDER>,add shim procedure ( prefix + __str__ @$ table @$ null @$ true @$ partition index @$ partitioncolumn @$ false ) ;,tables insert
let client retry the same <PLACE_HOLDER> @$ add pending commit to sync later,if ( ! from read ) { commit ctx commit ctx = new commit ctx ( commit offset @$ channel @$ xid @$ pre op attr ) ; pending commits . put ( commit offset @$ commit ctx ) ; },client retry
method reports <PLACE_HOLDER> in nanoseconds across all processors .,cpu time /= __num__ * os . get available processors ( ) ; double cpu = __num__ ; if ( prev cpu time > __num__ ) { long cpu time diff = cpu time - prev cpu time ; cpu = math . min ( __num__ @$ ( double ) cpu time diff / metrics_update_freq ) ; },method reports
if the element has a <PLACE_HOLDER> @$ treat it like xml @$ not html,if ( null != namespaceuri && namespaceuri . length ( ) > __num__ ) { super . end element ( namespaceuri @$ local name @$ name ) ; return ; } try { elem context elem context = m_elem context ; final elem desc elem desc = elem context . m_element desc ; final int elem flags = elem desc . get flags ( ) ; final boolean elem empty = ( elem flags & elem desc . empty ) != __num__ ; if ( m_do indent ) { final boolean is block element = ( elem flags & elem desc . block ) != __num__ ; boolean should indent = false ; if ( m_ispreserve ) { m_ispreserve = false ; } else if ( m_do indent,element has
here we restore our wallet from a seed with no passphrase . also have a <PLACE_HOLDER> at the backup to mnemonic seed.java example that shows how to backup a wallet by creating a mnemonic sentence .,string seed code = __str__ ; string passphrase = __str__ ; long creationtime = __num__ ; deterministic seed seed = new deterministic seed ( seed code @$ null @$ passphrase @$ creationtime ) ;,here have
robolectric does n't like this <PLACE_HOLDER>,if ( ! test util . are robolectric tests running ( ) ) { ok http builder . add interceptor ( new chuck interceptor ( application . get application context ( ) ) ) ; },robolectric like
client establishes the <PLACE_HOLDER>,socket s1 = new socket ( ) ; s1 . connect ( isa ) ;,client establishes
proper close of attribute template . evaluate the <PLACE_HOLDER> .,buffer . set length ( __num__ ) ; x path xpath = handler . createx path ( expr buffer . to string ( ) @$ owner ) ; m_parts . add element ( new avt partx path ( xpath ) ) ; lookahead = null ;,close evaluate
moving a day the notification q calls the commit <PLACE_HOLDER> . payment is expected .,bus handler . push expected events ( next event . invoice @$ next event . payment @$ next event . invoice_payment ) ; clock . add days ( __num__ ) ; assert listener status ( ) ; parent invoice = invoice user api . get invoice ( parent invoice . get id ( ) @$ call context ) ; assert equals ( parent invoice . get status ( ) @$ invoice status . committed ) ;,q calls
check if the caller has the <PLACE_HOLDER> to invoke 'query m beans ',if ( sm != null ) { checkm bean permission ( ( string ) null @$ null @$ null @$ __str__ ) ; set < object instance > list = querym beans impl ( name @$ null ) ; set < object instance > allowed list = new hash set < object instance > ( list . size ( ) ) ; for ( object instance oi : list ) { try { checkm bean permission ( oi . get class name ( ) @$ null @$ oi . get object name ( ) @$ __str__ ) ; allowed list . add ( oi ) ; } catch ( security exception e ) { } } return filter list of object instances ( allowed list @$ query ),caller has
x sends x crossing to all hierarchy so if the edge of child equals to ancestor and mouse enters child @$ the ancestor will get an <PLACE_HOLDER> too . from java point the <PLACE_HOLDER> is bogus as ancestor is obscured @$ so if the child can get java <PLACE_HOLDER> itself @$ we skip it on ancestor .,long child wnd = xce . get_subwindow ( ) ; if ( child wnd != x constants . none ) { x base window child = x toolkit . window tox window ( child wnd ) ; if ( child != null && child instanceof x window && ! child . is event disabled ( xev ) ) { return ; } },child get
if queue has default label <PLACE_HOLDER> @$ and rr does n't have @$ use the default label <PLACE_HOLDER> of queue,if ( label exp == null && queue info != null && resource request . any . equals ( res req . get resource name ( ) ) ) { log . debug ( __str__ @$ queue info . get default node label expression ( ) ) ; label exp = queue info . get default node label expression ( ) ; },queue has
when the hardware tracks events faster than they are delivered @$ the event will contain a <PLACE_HOLDER> of those skipped points .,int history size = event . get history size ( ) ; for ( int i = __num__ ; i < history size ; i ++ ) { float historicalx = event . get historicalx ( i ) ; float historicaly = event . get historicaly ( i ) ; expand dirty rect ( historicalx @$ historicaly ) ; path . line to ( historicalx @$ historicaly ) ; },event contain
response account <PLACE_HOLDER>,m ams . start add account session ( response @$ account manager service test fixtures . account_type_1 @$ __str__ @$ null @$ false @$ options ) ;,response account
4 jobs created @$ <PLACE_HOLDER> per timer job,assert equals ( __num__ @$ event created count ) ;,jobs created
has to be tied for best break if we 've found one in ` best arborescence ` at this point all edges in ` candidates ` have equal <PLACE_HOLDER> @$ and if one of them is in ` best arborescence ` it will be first,final exclusive edge best edge = candidates . remove first ( ) ;,edges have
first renderer handles <PLACE_HOLDER> .,map < string @$ integer > first renderer mapped capabilities = new hash map < > ( ) ; first renderer mapped capabilities . put ( english . id @$ format_handled ) ; first renderer mapped capabilities . put ( german . id @$ format_unsupported_subtype ) ; renderer capabilities first renderer capabilities = new fake mapped renderer capabilities ( c . track_type_text @$ first renderer mapped capabilities ) ;,renderer handles
split 0 times should throw <PLACE_HOLDER>,try { parts = bytes . split ( low @$ high @$ __num__ ) ; assert true ( __str__ @$ false ) ; } catch ( illegal argument exception iae ) { },split throw
higher order <PLACE_HOLDER> return <PLACE_HOLDER> .,return true ; case switch : case case :,functions return
share ur is use the content : <PLACE_HOLDER> when able @$ which looks bad when displayed in the url bar .,if ( service . is download openable in browser ( is off the record @$ mime type ) ) { uri file uri = uri . from file ( file ) ; uri share uri = get uri for item ( file ) ; string normalized mime type = intent . normalize mime type ( mime type ) ; intent intent = get media viewer intent for download item ( file uri @$ share uri @$ normalized mime type ) ; intent handler . start activity for trusted intent ( intent @$ context ) ; return true ; },ur use
default behavior should update component <PLACE_HOLDER> for the name field,assert null ( name field . get component error ( ) ) ;,behavior update
each replica could have own end point factory <PLACE_HOLDER>,string end point factory = sub config . has path ( end_point_factory_class ) ? sub config . get string ( end_point_factory_class ) : default_end_point_factory_class ; end point factory factory = end point factory resolver . resolve class ( end point factory ) . new instance ( ) ; this . replicas . add ( factory . build replica ( sub config @$ replica name @$ this . selection config ) ) ;,replica have
moved region remove <PLACE_HOLDER> to a guarded block . if a region is getting created it wo n't allow it to destroy any region .,synchronized ( region op lock ) { object name object name = m beanjmx adapter . get regionm bean name ( internal cache . get distributed system ( ) . get distributed member ( ) @$ region . get full path ( ) ) ; try { regionm bean regionm bean = ( regionm bean ) service . get local regionm bean ( region . get full path ( ) ) ; if ( regionm bean != null ) { regionm bean . stop monitor ( ) ; } } catch ( management exception e ) { if ( logger . is debug enabled ( ) ) { logger . debug ( e . get message ( ) @$ e ) ; } return ; } service .,region remove
make all cache elements for this guy go <PLACE_HOLDER> .,remove stale entries ( class value ) ;,elements go
card array is cloned in deck task @$ which pays <PLACE_HOLDER> to memory pressure,m undo . add ( new object [ ] { type @$ o [ __num__ ] } ) ; break ; default : timber . e ( __str__ @$ type ) ; break ;,which pays
only the main task stack change notification requires a <PLACE_HOLDER> .,m handler . send message delayed ( msg @$ notify_task_stack_change_listeners_delay ) ;,notification requires
java does not do this right <PLACE_HOLDER> . the number of bytes in a file is a long @$ but the length of a string is an int . why ?,line guess = ( int ) ( f . length ( ) / ( long ) __str__ . length ( ) ) ; line guess += ( line guess / __num__ ) ;,java do
use a flag bc we need to handle is encap nonce get either <PLACE_HOLDER>,boolean failed = false ;,nonce get
no content <PLACE_HOLDER> check is performed when the md 5 check is enabled @$ since a correct md 5 check would imply a correct content <PLACE_HOLDER> .,try { message digest digest = message digest . get instance ( __str__ ) ; is = new digest validation input stream ( is @$ digest @$ server side hash ) ; } catch ( no such algorithm exception e ) { log . warn ( __str__ + __str__ @$ e ) ; },check imply
create a view coming from the locator that <PLACE_HOLDER> with the installed view,member identifier locator member id = new internal distributed member ( __str__ @$ mock members [ mock members . length - __num__ ] . get membership port ( ) + __num__ ) ; locator member id . set vm kind ( cluster distribution manager . locator_dm_type ) ; list < member identifier > new member list = new array list < > ( members ) ; new member list . add ( locator member id ) ; gms membership view locator view = new gms membership view ( locator member id @$ installed view . get view id ( ) + __num__ @$ new member list ) ;,view coming
client 1 put <PLACE_HOLDER>,client1 . invoke ( ( ) -> { region < string @$ ticker data > region = get cache ( ) . get region ( region name ) ; do put all ( region @$ __str__ @$ one_hundred * __num__ ) ; assert that ( region . size ( ) ) . is equal to ( one_hundred * __num__ ) ; } ) ;,client put
third volume @$ again with 3 mb free <PLACE_HOLDER> .,volumes . add ( mockito . mock ( fs volume spi . class ) ) ; mockito . when ( volumes . get ( __num__ ) . get available ( ) ) . then return ( __num__ * __num__ * __num__ ) ;,volume mb
msc.sync all will trigger a <PLACE_HOLDER>,if ( spt == null ) break ;,all trigger
1 complete frame @$ 1 partial frame : should not trigger a <PLACE_HOLDER>,channel . write inbound ( wrapped buffer ( new byte [ ] { __num__ @$ __num__ } ) @$ wrapped buffer ( new byte [ ] { __num__ } ) ) ; assert equals ( __num__ @$ interceptor . reads triggered ) ;,frame trigger
force eof if a read takes <PLACE_HOLDER> at this position,curr buf idx -- ; buf position = buffer_size ; curr buf = file . get buffer ( curr buf idx ) ; buf position = __num__ ; long buflen = length - buf start ; buf length = buflen > buffer_size ? buffer_size : ( int ) buflen ;,read takes
note : <PLACE_HOLDER> format used by date format only uses int <PLACE_HOLDER>s . remainder operation on 32 bit platform using long is significantly slower than int . so @$ this method casts long <PLACE_HOLDER> into int .,int number = ( int ) numberl ; int limit = decimal buf . length < max int digits ? decimal buf . length : max int digits ; int index = limit - __num__ ; while ( true ) { decimal buf [ index ] = digits [ ( number % __num__ ) ] ; number /= __num__ ; if ( index == __num__ || number == __num__ ) { break ; } index -- ; } int padding = min int digits - ( limit - index ) ; for ( ; padding > __num__ ; padding -- ) { decimal buf [ -- index ] = digits [ __num__ ] ; } int length = limit - index ; to append to . append (,method casts
change text view <PLACE_HOLDER> which should be saved in states .,instrumentation registry . get instrumentation ( ) . run on main sync ( new runnable ( ) { @ override public void run ( ) { text view t = main fragment . get view ( ) . find view by id ( r . id . tv2 ) ; t . set text ( __str__ ) ; } } ) ;,text view
insert the video . the command sends three <PLACE_HOLDER>s . the first specifies which information the api request is setting and which information the api response should return . the second <PLACE_HOLDER> is the video resource that contains metadata about the new video . the third <PLACE_HOLDER> is the actual video content .,you tube . videos . insert video insert = youtube . videos ( ) . insert ( __str__ @$ video object defining metadata @$ media content ) ;,command sends
create a client . its credentials come from a ca that the server does not trust . the client trusts both test c as @$ so we can be sure that the handshake failure is due to the server rejecting the client 's <PLACE_HOLDER> @$ not the client rejecting the server 's <PLACE_HOLDER> .,file client cert chain file = test utils . load cert ( __str__ ) ; file client private key file = test utils . load cert ( __str__ ) ; x509 certificate [ ] client trusted ca certs = { test utils . loadx509 cert ( __str__ ) } ; channel = client channel ( server . get port ( ) @$ client context builder . key manager ( client cert chain file @$ client private key file ) . trust manager ( client trusted ca certs ) . build ( ) ) ; simple service grpc . simple service blocking stub client = simple service grpc . new blocking stub ( channel ) ;,client rejecting
\u 00 a <PLACE_HOLDER> and \uffe <PLACE_HOLDER> are actually the same symbol @$ just different code points . but the ri returns the \uffe <PLACE_HOLDER> and android returns those with \u 00 a <PLACE_HOLDER>,string [ ] yen = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; string [ ] dollar = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } ;,ri returns
if error while generating pojo do not try compile <PLACE_HOLDER> as they very likely depends hence fail too .,if ( has errors ( ) ) { return ; },pojo try
for a perfect match return the single <PLACE_HOLDER> only,if ( long opts . key set ( ) . contains ( opt ) ) { return collections . singleton list ( opt ) ; } for ( string long opt : long opts . key set ( ) ) { if ( long opt . starts with ( opt ) ) { matching opts . add ( long opt ) ; } } return matching opts ;,match return
currently acid requires <PLACE_HOLDER> to be bucketed,execute statement on driver ( __str__ + tbl name + __str__ + __str__ + __str__ + __str__ @$ driver ) ;,acid requires
! drained if dispatch queued new <PLACE_HOLDER> on this dispatcher,synchronized ( mutex ) { drained = queue . is empty ( ) ; },dispatch queued
we only care about it if the image <PLACE_HOLDER> matches the current base <PLACE_HOLDER>,if ( variable declarator id . has image equal to ( base name ) ) { boolean allocation found = declarator . get first descendant of type ( ast allocation expression . class ) != null ; boolean iterator = is iterator ( ) || is factory ( declarator ) ; boolean for loop = is for loop ( declarator ) ; assignments . add ( new assignment ( declarator . get begin line ( ) @$ allocation found @$ iterator @$ for loop ) ) ; },name matches
check notification has css <PLACE_HOLDER> which describes notification type,if ( get attribute ( __str__ ) . contains ( notif type ) ) { return entry . get value ( ) ; },notification has
the old request should have no <PLACE_HOLDER> on new tracker,request . record miss ( ) ; assert equals ( __num__ @$ tracker . get total hit count ( ) ) ; assert equals ( __num__ @$ tracker . get total miss count ( ) ) ; assert equals ( __num__ @$ tracker . get total miss match count ( ) ) ; assert equals ( __num__ @$ tracker . get total eviction count ( ) ) ; assert equals ( __num__ @$ tracker . get total invalidation count ( ) ) ; assert equals ( __num__ @$ tracker . get total load success count ( ) ) ; assert equals ( __num__ @$ tracker . get total load exception count ( ) ) ; assert equals ( __num__ @$ tracker . get total retrieval time ( ),request have
test the same data and <PLACE_HOLDER> with prior @$ should get the same <PLACE_HOLDER> except for the intercept,glm = new glm ( params ) ; model4 = glm . train model ( ) . get ( ) ; assert equals ( __str__ + model3 . _output . _training_metrics . _mse + __str__ + model4 . _output . _training_metrics . _mse @$ model3 . _output . _training_metrics . _mse @$ model4 . _output . _training_metrics . _mse @$ __num__ ) ; assert equals ( __str__ + ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev + __str__ + ( ( model metrics binomialglm ) model4 . _output . _training_metrics ) . _res dev @$ ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev @$ ( ( model metrics binomialglm ) model4 . _output .,data get
cancel currently executing tasks wait a <PLACE_HOLDER> for tasks to respond to being cancelled,if ( ! executor . await termination ( __num__ @$ time unit . seconds ) ) system . out . println ( __str__ ) ;,tasks wait
more complicated race ! ! some client managed to acquire the provider and release it before the <PLACE_HOLDER> was completed . continue the <PLACE_HOLDER> @$ and abort the next remove message .,prc . remove pending = false ; final i binder j binder = prc . holder . provider . as binder ( ) ; provider ref count existing prc = m provider ref count map . get ( j binder ) ; if ( existing prc == prc ) { m provider ref count map . remove ( j binder ) ; } for ( int i = m provider map . size ( ) - __num__ ; i >= __num__ ; i -- ) { provider client record pr = m provider map . value at ( i ) ; i binder my binder = pr . m provider . as binder ( ) ; if ( my binder == j binder ) { m provider map,race continue
verify resource request sent for map have appropriate node label <PLACE_HOLDER> as per the configuration,validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ; validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ; validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ;,request have
britis<PLACE_HOLDER> pronounce <PLACE_HOLDER> in t<PLACE_HOLDER>is word americans give it ' <PLACE_HOLDER> ' for t<PLACE_HOLDER>e name @$ no ' <PLACE_HOLDER> ' for t<PLACE_HOLDER>e plant,if ( string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) || string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ ) || string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) { if ( ( m_current == __num__ ) && string at ( m_current @$ __num__ @$ __str__ @$ __str__ ) ) { if ( m_encode vowels ) { metaph add ( __str__ @$ __str__ ) ; } else { metaph add ( __str__ @$ __str__ ) ; } } else if ( ( m_current == __num__ ) || m_encode vowels ) { metaph add ( __str__ ) ; } m_current ++ ;,h give
all project related messages which changes the <PLACE_HOLDER>,string current etag = string . format ( __str__ @$ objects . hash ( get last modified ( ) @$ get project messages ( ) @$ get env ( ) . get date for last index run ( ) != null ? get env ( ) . get date for last index run ( ) . get time ( ) : __num__ @$ info . get version ( ) ) ) ;,which changes
same for min sdk version @$ if the library is using a code <PLACE_HOLDER> @$ the application must also be using the same code <PLACE_HOLDER> .,string library min sdk version = lower priority document . get raw min sdk version ( string . value of ( this target sdk ) ) ; if ( ! character . is digit ( library min sdk version . char at ( __num__ ) ) ) { if ( ! library min sdk version . equals ( get min sdk version ( default_sdk_version ) ) ) { merging report . add message ( get source file ( ) @$ merging report . record . severity . error @$ string . format ( __str__ + __str__ @$ get min sdk version ( default_sdk_version ) @$ library min sdk version @$ lower priority document . get source file ( ) . print ( false ) ) ) ; return,library using
assert received flow <PLACE_HOLDER> .,process session . assert all flow files transferred ( relationship . anonymous ) ; final list < mock flow file > flow files = process session . get flow files for relationship ( relationship . anonymous ) ; assert equals ( __num__ @$ flow files . size ( ) ) ; final mock flow file flow file = flow files . get ( __num__ ) ; flow file . assert attribute equals ( site to site attributes . s2s_host . key ( ) @$ peer . get host ( ) ) ; flow file . assert attribute equals ( site to site attributes . s2s_address . key ( ) @$ peer . get host ( ) + __str__ + peer . get port ( ) ) ; flow,assert received
mix in various ways to specify no catalog . use empty <PLACE_HOLDER> here .,dep bytes = new string ( client utils . file to bytes ( new file ( deploymenturl ) ) @$ constants . utf8encoding ) ; results = client . call procedure ( __str__ @$ __str__ @$ dep bytes ) . get results ( ) ; assert true ( results . length == __num__ ) ; thread . sleep ( __num__ ) ;,mix use
assert that the disk has seen <PLACE_HOLDER> of the updates,region version vector rvv = getrvv ( vm1 ) ; region version vector diskrvv = get diskrvv ( vm1 ) ; assert samervv ( rvv @$ diskrvv ) ;,disk seen
the new tree root node id must match the original <PLACE_HOLDER> to be able to reconnect the subtrees,new tree . set id ( sub tree . get id ( ) ) ; tree permutations . add ( new tree ) ;,id match
when we 're disabled @$ set the layer <PLACE_HOLDER> to hardware so we can clear the track out from behind the thumb . when enabled set the layer <PLACE_HOLDER> to none so that the halo can be drawn outside the bounds of the slider . after lollipop we use ripple for the halo @$ and an overlay for the marker so we do n't,if ( version . sdk_int < version_codes . lollipop ) { set layer type ( enabled ? layer_type_none : layer_type_hardware @$ null ) ; },enabled set
we want to keep track of the last 1000 events in the files so that we can add them to 'ring buffer ' . however @$ we do n't want to add them directly to ring buffer @$ because once they are added to ring buffer @$ they are available in query results . as a result @$ we can have the issue where,final ring buffer < provenance event record > latest records = new ring buffer < > ( __num__ ) ;,records copy
if already setup return the <PLACE_HOLDER>,if ( null == this . crouton view ) { initialize crouton view ( ) ; } return crouton view ;,setup return
all done @$ no <PLACE_HOLDER> or rollback from tm,if ( xa resource . xa_rdonly == response . get result ( ) ) { super . on response ( command ) ; },all done
conversions on parent would have <PLACE_HOLDER>,attribute conversion info conversion = locate attribute conversion info ( property name ) ; if ( conversion != null ) { return conversion ; } return null ;,conversions have
m total length contains the <PLACE_HOLDER> already,child top = m padding top + bottom - top - m total length ; break ;,length contains
lie data <PLACE_HOLDER> invaid data <PLACE_HOLDER> .,byte i71 [ ] = { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ;,type invaid
check if the .job file has an underlying job <PLACE_HOLDER>,if ( job config . has path ( gobblin_job_template_key ) ) { uri job template relative uri = new uri ( job config . get string ( gobblin_job_template_key ) ) ; if ( ! job template relative uri . get scheme ( ) . equals ( fs_scheme ) ) { throw new runtime exception ( __str__ + fs_scheme + __str__ + flow template diruri . get scheme ( ) ) ; } path full job template path = path utils . merge paths ( new path ( template catalog dir ) @$ new path ( job template relative uri ) ) ; job config = job config . with fallback ( load hocon file at path ( full job template path ) ) ; } job templates .,file has
we do n't care as long as nobody calls the <PLACE_HOLDER> :,compilation helper . set args ( arrays . as list ( __str__ @$ temporary folder . get root ( ) . get absolute path ( ) @$ __str__ ) ) . add source lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . do test ( ) ;,nobody calls
join to next if parent of p paragraph has another <PLACE_HOLDER> after p paragraph @$ and it is n't a leaf .,element parent = p paragraph . get parent element ( ) ; int p paragraph index = parent . get element index ( offset ) ; if ( ( p paragraph index + __num__ ) < parent . get element count ( ) && ! parent . get element ( p paragraph index + __num__ ) . is leaf ( ) ) { last start spec . set direction ( element spec . join next direction ) ; },parent has
but they are not . however @$ i bet someone will try and treat it like a target @$ so find the owning cell if necessary @$ and then fully resolve the <PLACE_HOLDER> against the owning cell 's root .,matcher matcher = include_path_pattern . matcher ( include ) ; preconditions . check state ( matcher . matches ( ) ) ; optional < string > cell name = optional . of nullable ( matcher . group ( __num__ ) ) ; string include path = matcher . group ( __num__ ) ; return cell path resolver . get cell path ( cell name ) . map ( cell path -> cell path . resolve ( include path ) ) . or else get ( ( ) -> cell . get filesystem ( ) . resolve ( include path ) ) ;,then resolve
the query should return all <PLACE_HOLDER> in region .,assert equals ( region . size ( ) @$ results . size ( ) ) ; query observer holder . reset ( ) ;,query return
according to david ackerman @$ the compression type can change the <PLACE_HOLDER> of the document .,if ( compression type . equals ( __str__ ) ) { aiff header . set endian ( aiff audio header . endian . little_endian ) ; } bytes left -= __num__ ; compression name = aiff util . read pascal string ( raf ) ; bytes left -= compression name . length ( ) + __num__ ;,type change
hope the next local variable would have a smaller <PLACE_HOLDER> .,continue ;,hope have
if the structure size indicates there are more fields @$ we are dealing with a newer <PLACE_HOLDER> of the structure . each size check represents a new <PLACE_HOLDER> of the structure .,if ( reader . get pointer index ( ) - index < size ) { security cookie = read pointer ( reader ) ; se handler table = read pointer ( reader ) ; se handler count = read pointer ( reader ) ; } if ( reader . get pointer index ( ) - index < size ) { guard cfc check function pointer = read pointer ( reader ) ; guard cf dispatch function pointer = read pointer ( reader ) ; guard cf function table = read pointer ( reader ) ; guard cf function count = read pointer ( reader ) ; guard flags = new guard flags ( reader . read next int ( ) ) ; } if ( reader . get,check represents
push queries only return value <PLACE_HOLDER> @$ but query metadata schema includes key and meta :,final logical schema stored schema = query metadata . get logical schema ( ) ; final builder actual schema builder = logical schema . builder ( ) . no implicit columns ( ) ; stored schema . value ( ) . for each ( actual schema builder :: value column ) ; return streamed row . header ( no_query_id @$ actual schema builder . build ( ) ) ;,queries return
block types other than <PLACE_HOLDER> blocks always have <PLACE_HOLDER> block encoding.none . to avoid false negative cache misses @$ only perform this check if cached block is a <PLACE_HOLDER> block .,if ( cached block . get block type ( ) . is data ( ) && ! actual data block encoding . equals ( expected data block encoding ) ) { if ( ! expected data block encoding . equals ( data block encoding . none ) && ! actual data block encoding . equals ( data block encoding . none ) ) { log . info ( __str__ + cache key + __str__ + __str__ + expected data block encoding + __str__ + actual data block encoding + __str__ + path ) ; return and evict block ( cache @$ cache key @$ cached block ) ; } return null ; },types have
completing first task will set <PLACE_HOLDER> on process instance,task task = task service . create task query ( ) . single result ( ) ; task service . complete ( task . get id ( ) ) ; assert equals ( __str__ @$ runtime service . get variable ( process instance . get id ( ) @$ __str__ ) ) ;,task set
when amrmp roxy ha is enabled @$ nmss should not have the uam <PLACE_HOLDER> @$ it should be in registry,assert . assert false ( recovered data map . contains key ( sc entry ) ) ;,nmss have
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default autocrawl shallow profile = new crawl profile ( crawl_profile_autocrawl_shallow @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ integer . parse int ( sb . get config ( switchboard constants . autocrawl_shallow_depth @$ __str__ ) ) @$ true @$ crawl profile . get recrawl date ( integer . parse int ( sb . get config ( switchboard constants . autocrawl_days @$ __str__ ) ) * __num__ ) @$ - __num__ @$ true @$ true @$ true @$ false @$ sb . get config,url match
browse destination file browse <PLACE_HOLDER> ...,wb destination file folder = new button ( w general comp @$ swt . push | swt . center ) ; props . set look ( wb destination file folder ) ; wb destination file folder . set text ( base messages . get string ( pkg @$ __str__ ) ) ; form data fdb destination file folder = new form data ( ) ; fdb destination file folder . right = new form attachment ( wb destination directory @$ - margin ) ; fdb destination file folder . top = new form attachment ( w source file folder @$ margin ) ; wb destination file folder . set layout data ( fdb destination file folder ) ; w destination file folder = new text var ( job,destination file
wait for auth failed <PLACE_HOLDER> from client 's <PLACE_HOLDER> thread .,try ( zoo keeper ignored = create client ( new my watcher ( ) @$ host port ) ) { auth failed . await ( ) ; },wait failed
find all aliases matching current <PLACE_HOLDER>,if ( ! common utils . is empty ( request . get active query ( ) . get text ( ) ) && ! common utils . is empty ( word part ) ) { if ( word part . index of ( request . get context ( ) . get syntax manager ( ) . get struct separator ( ) ) != - __num__ || word part . equals ( all_columns_pattern ) ) { return ; } sql dialect sql dialect = sql utils . get dialect from data source ( request . get context ( ) . get data source ( ) ) ; string table name pattern = get table name pattern ( sql dialect ) ; string table alias pattern = get table alias,aliases matching
our protocol does not use an <PLACE_HOLDER> or a preamble .,final multi partmime writer . builder attachments builder = new multi partmime writer . builder ( ) ; for ( final object data source : streaming attachments ) { assert ( data source instanceof rest li attachment data source writer || data source instanceof rest li data source iterator ) ; if ( data source instanceof rest li attachment data source writer ) { attachment utils . append single attachment to builder ( attachments builder @$ ( rest li attachment data source writer ) data source ) ; } else { attachment utils . append multiple attachments to builder ( attachments builder @$ ( rest li data source iterator ) data source ) ; } } final multi partmime writer multi partmime writer = attachment utils . create,protocol use
compiler bug : a strictfp class sets all <PLACE_HOLDER> to strictfp,flags &= ~ code constants . acc_strict ;,class sets
we need to log these independently of cards @$ as one side may have more card <PLACE_HOLDER>,_log rem ( ids @$ consts . rem_note ) ; m db . execute ( __str__ + strids ) ;,side have
only allocate the array if there are enough bytes available . this only works for byte array input stream . the assignment below ensures that buffer has the required <PLACE_HOLDER> .,byte array input stream array input = buffer ; if ( array input . available ( ) < length ) { throw new io exception ( __str__ ) ; } byte [ ] bytes = new byte [ length ] ; array input . read ( bytes ) ; if ( is constructed ( ) ) { der input stream in = new der input stream ( bytes @$ __num__ @$ bytes . length @$ buffer . allowber ) ; bytes = null ; while ( in . available ( ) != __num__ ) { bytes = append ( bytes @$ in . get octet string ( ) ) ; } } return bytes ;,buffer has
hsql erroneously matches an extra <PLACE_HOLDER> ?,if ( ! ishsql ( ) ) { validate table of longs ( client @$ __str__ @$ empty_table ) ; },hsql matches
caller might want different <PLACE_HOLDER>,convert temps ( main @$ units ) ;,caller want
now allocate space to store the chunk using the volt table serialization representation . the chunk will contain an integer row <PLACE_HOLDER> preceding it so it can be sucked straight in . there is a little funny business to overwrite the partition id that is not part of the serialization format,c = get output buffer ( next chunk partition id ) ;,chunk contain
check if user is member of group since api does n't return typed <PLACE_HOLDER>,if ( identity service . create user query ( ) . member of group ( group . get id ( ) ) . user id ( member ship . get user id ( ) ) . count ( ) > __num__ ) { throw new flowable conflict exception ( __str__ + member ship . get user id ( ) + __str__ + group . get id ( ) + __str__ ) ; } identity service . create membership ( member ship . get user id ( ) @$ group . get id ( ) ) ; response . set status ( http status . created . value ( ) ) ; return rest response factory . create membership response ( member ship . get user id ( ),api return
record rpc name @$ client <PLACE_HOLDER> @$ server <PLACE_HOLDER> .,recorder . record rpc name ( invocation . get invoker ( ) . get interface ( ) . get simple name ( ) + __str__ + invocation . get method name ( ) ) ; recorder . record end point ( rpc context . get local address string ( ) ) ; if ( rpc context . get remote host ( ) != null ) { recorder . record remote address ( rpc context . get remote address string ( ) ) ; } else { recorder . record remote address ( __str__ ) ; },name server
n 1 has no <PLACE_HOLDER>,string nodes config = __str__ ;,n has
query should pass and create the <PLACE_HOLDER>,assert equals ( __str__ @$ operation state . finished @$ client . get operation status ( op handle @$ false ) . get state ( ) ) ; client . close operation ( op handle ) ;,query pass
remap duplicate <PLACE_HOLDER> to the token id we created for the first instance of any duplicate token name .,if ( remapping indexes . not empty ( ) ) { remapping indexes . for each key value ( ( index @$ creating index ) -> ids [ index ] = ids [ creating index ] ) ; } return created tokens ;,remap duplicate
the fractional seconds may have <PLACE_HOLDER> through 6 digits but no more and no less .,parse date fails ( __str__ ) ; parse date fails ( __str__ ) ;,seconds have
google camera adds the preview <PLACE_HOLDER> as well as capture <PLACE_HOLDER> @$ for still capture,still builder . add target ( image reader . get surface ( ) ) ; if ( image reader raw != null ) still builder . add target ( image reader raw . get surface ( ) ) ; capture session . stop repeating ( ) ;,camera adds
consider our cached version dirty since app code now has a <PLACE_HOLDER> to it,if ( m drawable == m recycleable bitmap drawable ) { m recycleable bitmap drawable = null ; },version has
verify that mock provider 2 contains a new group named the <PLACE_HOLDER> as the parent meta group of the contact we just moved,contact group new grpp2 = mcl slick fixture . mock pres op setp2 . get server stored contact list root ( ) . get group ( mcl slick fixture . metap1 grp1 . get group name ( ) ) ; assert not null ( __str__ + mcl slick fixture . emilp2 . get display name ( ) + __str__ @$ new grpp2 ) ;,group named
add g 1 and all descendants their <PLACE_HOLDER>,auth entity = auth entity factory . apply ( null ) ; auth entity . set for groups ( new tree set < > ( ) ) ; auth entity . set for groups ( __str__ ) ; auth entity . load ( new tree map < > ( ) ) ; assert equals ( new tree set < > ( arrays . as list ( new string [ ] { __str__ @$ __str__ } ) ) @$ auth entity . for groups ( ) ) ; assert equals ( new tree set < > ( arrays . as list ( new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ) ) @$ auth entity . for projects,g descendants
minimal receive buffer <PLACE_HOLDER> .,final silent server ss = new silent server ( __num__ ) ;,minimal receive
divide space according to expansion <PLACE_HOLDER>s if any span has a <PLACE_HOLDER>,int total expansion = __num__ ; for ( int i = __num__ ; i < span size ; i ++ ) { int item index = span start index + i ; total expansion += expansion ratios [ item index ] ; } for ( int i = __num__ ; i < span size ; i ++ ) { int item index = span start index + i ; int expansion ; if ( total expansion == __num__ ) { expansion = needed extra space / span size ; } else { expansion = ( int ) ( needed extra space * expansion ratios [ item index ] / total expansion ) ; } dimensions [ item index ] += expansion ; allocated extra space += expansion ;,span has
if state machine instance not exists stop <PLACE_HOLDER>,if ( framework error code . state machine instance not exists . equals ( e . get errcode ( ) ) ) { return branch status . phase two_ committed ; },instance exists
create a bitmap of the icon which is <PLACE_HOLDER> the widget 's remoteview requires .,icon . set color filter ( m icon utilities . get disabled color filter ( ) ) ; return m icon utilities . create icon bitmap ( icon ) ;,remoteview requires
in the rare case that more input could cause the <PLACE_HOLDER> to be lost and there is more input coming we must wait for more input . note that hitting the end is okay as long as the <PLACE_HOLDER> can not go away . it is the beginning of the next delims we want to be sure about @$ we do n't care if,if ( found next delim ) { if ( matcher . require end ( ) && ! source closed ) { need input = true ; return null ; } int token end = matcher . start ( ) ; if ( pattern == null ) { pattern = find_any_pattern ; } matcher . use pattern ( pattern ) ; matcher . region ( position @$ token end ) ; if ( matcher . matches ( ) ) { string s = matcher . group ( ) ; position = matcher . end ( ) ; return s ; } else { return null ; } },input cause
user 10 does n't have <PLACE_HOLDER> 3 @$ so no callback .,verify ( c10 @$ times ( __num__ ) ) . on shortcuts changed ( eq ( calling_package_3 ) @$ any ( list . class ) @$ any ( user handle . class ) ) ; assert shortcut ids ( shortcuts . get value ( ) @$ __str__ ) ; assert equals ( start_time + __num__ @$ find shortcut ( shortcuts . get value ( ) @$ __str__ ) . get last changed timestamp ( ) ) ;,user have
regression test @$ used to throw a class cast exception because the job launcher created a query <PLACE_HOLDER> instead of row count <PLACE_HOLDER>,long [ ] row counts = execute ( __str__ @$ $$ ( $ ( __str__ ) ) ) ; assert that ( row counts . length @$ is ( __num__ ) ) ;,launcher created
center inside @$ but bitmap bounds exceed the resize <PLACE_HOLDER> ... so change it to fit center .,if ( scale mode == scale mode . center inside ) { if ( resize width <= b . get width ( ) || resize height <= b . get height ( ) ) scale mode = scale mode . fit center ; },inside exceed
when the trailer contains an orca report @$ callbacks for both listeners will be invoked . both listener will receive the same orca report <PLACE_HOLDER> @$ which means deserialization happens only once .,trailer . put ( orca reporting tracer factory . orca_endpoint_load_metrics_key @$ orca load report . get default instance ( ) ) ; child tracer . inbound trailers ( trailer ) ; argument captor < orca load report > parent report cap = argument captor . for class ( null ) ; argument captor < orca load report > child report cap = argument captor . for class ( null ) ; verify ( orca listener1 ) . on load report ( parent report cap . capture ( ) ) ; verify ( orca listener2 ) . on load report ( child report cap . capture ( ) ) ; assert that ( parent report cap . get value ( ) ) . is equal to ( orca load,listener receive
java serialization only supported <PLACE_HOLDER> if not defaulting to json if java serliazation needed together with json defaulting then add to custom post variable types,if ( ! serializepoj os in variables to json ) { variable types . add type ( new byte array type ( ) ) ; variable types . add type ( new serializable type ( serializable variable type track deserialized objects ) ) ; variable types . add type ( new custom object type ( __str__ @$ item instance . class ) ) ; variable types . add type ( new custom object type ( __str__ @$ message instance . class ) ) ; } if ( custom post variable types != null ) { for ( variable type custom variable type : custom post variable types ) { variable types . add type ( custom variable type ) ; } },serialization supported
take from <PLACE_HOLDER> 1 even though mux said <PLACE_HOLDER> 0 @$ since <PLACE_HOLDER> 0 empty,assert equals ( call @$ fcq . take ( ) ) ; assert equals ( __num__ @$ fcq . size ( ) ) ;,mux said
the summary section displays up to two text views <PLACE_HOLDER> by <PLACE_HOLDER> .,m summary layout = new linear layout ( get context ( ) ) ; m summary layout . add view ( m summary left text view @$ left layout params ) ; m summary layout . add view ( m summary right text view @$ right layout params ) ; main section layout . add view ( m summary layout @$ new linear layout . layout params ( layout params . match_parent @$ layout params . wrap_content ) ) ; set summary text ( null @$ null ) ; create main section content ( main section layout ) ; return main section layout ;,displays views
relaunch the graph @$ which will use the above persisted group <PLACE_HOLDER> ...,graph data = graph function ( __str__ ) ; wait for animation ( ) ;,which use
ensure fc 2 has <PLACE_HOLDER>,assert . assert true ( is dir ( fc2 @$ path ) ) ; assert . assert true ( exists ( fc2 @$ path ) ) ; assert . assert false ( is file ( fc2 @$ path ) ) ;,fc has
catcher also receives the <PLACE_HOLDER> @$ prepended :,catch args . add ( __num__ @$ thrown ) ; helper . assert called ( __str__ @$ catch args ) ; asserteq ( cast . apply ( catch args ) @$ returned ) ;,catcher receives
the acceptor event loop thread needs to be from a different pool otherwise can get <PLACE_HOLDER> in accepted connections under a lot of load,acceptor event loop group = transport . event loop group ( transport . acceptor_event_loop_group @$ __num__ @$ acceptor event loop thread factory @$ __num__ ) ; metrics = initialise metrics ( options ) ; int worker pool size = options . get worker pool size ( ) ; executor service worker exec = new thread pool executor ( worker pool size @$ worker pool size @$ __num__ @$ time unit . milliseconds @$ new linked transfer queue < > ( ) @$ new vertx thread factory ( __str__ @$ checker @$ true @$ options . get max worker execute time ( ) @$ options . get max worker execute time unit ( ) ) ) ; pool metrics worker pool metrics = metrics != null ? metrics .,event get
skip if statements as they have oplog file exists <PLACE_HOLDER> .,if ( line . starts with ( __str__ ) ) { continue ; } else if ( line . contains ( windows script generator . exit_marker ) ) { break ; } else { begin index = line . last index of ( __str__ ) + __num__ ; end index = line . index of ( windows script generator . robocopy_no_job_header @$ begin index ) - __num__ ; oplog name = line . substring ( begin index @$ end index ) . trim ( ) ; add oplog line ( oplog name @$ line ) ; },statements exists
do not overwrite interface <PLACE_HOLDER> with instance <PLACE_HOLDER> do not overwrite private <PLACE_HOLDER> note : private <PLACE_HOLDER> from parent classes are not shown here @$ but when doing the multimethod connection step @$ we overwrite <PLACE_HOLDER> of the parent class with <PLACE_HOLDER> of a subclass and in that case we want to keep the private <PLACE_HOLDER>,if ( match . is private ( ) || ( ! is non real method ( match ) && match . get declaring class ( ) . is interface ( ) && ! method . get declaring class ( ) . is interface ( ) ) ) { } else { cached class methodc = method . get declaring class ( ) ; cached class matchc = match . get declaring class ( ) ; if ( methodc == matchc ) { if ( is non real method ( method ) ) { return method ; } } else if ( ! methodc . is assignable from ( matchc . get the class ( ) ) ) { return method ; } },methods overwrite
this is not necessarily safe @$ but compile time checking on the user side mandates these <PLACE_HOLDER> to be interchangeable,@ suppress warnings ( __str__ ) class < number > unbounded num type = ( class < number > ) num type ; return numeric range ( unbounded num type @$ field @$ min constant == null ? null : min constant . get constant ( ) @$ max constant == null ? null : max constant . get constant ( ) @$ min inc @$ max inc ) ;,checking mandates
disable enter <PLACE_HOLDER> .,override pending transition ( __num__ @$ __num__ ) ;,disable enter
the data matrix will not fit in this size @$ so the matrix should come back <PLACE_HOLDER>,int too small = __num__ ; data matrix writer writer = new data matrix writer ( ) ; bit matrix matrix = writer . encode ( __str__ @$ barcode format . data_matrix @$ too small @$ too small @$ null ) ; assert not null ( matrix ) ; assert true ( too small < matrix . get width ( ) ) ; assert true ( too small < matrix . get height ( ) ) ;,matrix come
if tools not found @$ no point in trying a url class loader so rethrow the original <PLACE_HOLDER> .,if ( ! file . exists ( ) ) throw e ; url [ ] urls = { file . touri ( ) . tourl ( ) } ; trace ( fine @$ urls [ __num__ ] . to string ( ) ) ; cl = url class loader . new instance ( urls ) ; ref tool class loader = new weak reference < class loader > ( cl ) ;,point rethrow
replace values if better probability log <PLACE_HOLDER> @$ if same edit distance or one space difference,if ( ( i == max segmentation word length ) || ( ( ( compositions [ circular index ] . distance sum + top ed == compositions [ destination index ] . distance sum ) || ( compositions [ circular index ] . distance sum + separator length + top ed == compositions [ destination index ] . distance sum ) ) && ( compositions [ destination index ] . probability log sum < compositions [ circular index ] . probability log sum + top probability log ) ) || ( compositions [ circular index ] . distance sum + separator length + top ed < compositions [ destination index ] . distance sum ) ) { compositions [ destination index ] . segmented string = compositions [,probability log
fail verification which should produce an <PLACE_HOLDER> that is send back to the client .,return new trust manager [ ] { new x509 trust manager ( ) { @ override public void check client trusted ( x509 certificate [ ] x509 certificates @$ string s ) throws certificate exception { throw new certificate exception ( ) ; } @ override public void check server trusted ( x509 certificate [ ] x509 certificates @$ string s ) { } @ override public x509 certificate [ ] get accepted issuers ( ) { return empty arrays . empty_x509_certificates ; } } } ;,which produce
this is a code <PLACE_HOLDER> @$ ensure this document uses the same code <PLACE_HOLDER> .,if ( ! character . is digit ( library min sdk version . char at ( __num__ ) ) ) { if ( ! library min sdk version . equals ( get min sdk version ( default_sdk_version ) ) ) { merging report . add message ( get source file ( ) @$ merging report . record . severity . error @$ string . format ( __str__ + __str__ @$ get min sdk version ( default_sdk_version ) @$ library min sdk version @$ lower priority document . get source file ( ) . print ( false ) ) ) ; return ; } },document uses
method name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( operation id ) ) { logger . warn ( operation id + __str__ + camelize ( sanitize name ( __str__ + operation id ) ) ) ; operation id = __str__ + operation id ; } return camelize ( operation id ) ;,name use
sanity check : get only element of a set @$ to ensure all segments have the same data <PLACE_HOLDER> .,final string data source = iterables . get only element ( stream support . stream ( segments . spliterator ( ) @$ false ) . map ( segment id :: get data source ) . collect ( collectors . to set ( ) ) ) ; final multiple specific segment spec query segment spec = new multiple specific segment spec ( stream support . stream ( segments . spliterator ( ) @$ false ) . map ( segment id :: to descriptor ) . collect ( collectors . to list ( ) ) ) ; final segment metadata query segment metadata query = new segment metadata query ( new table data source ( data source ) @$ query segment spec @$ new all column includerator ( ) @$,segments have
do n't stop processing in case a future segment explicitly excludes this <PLACE_HOLDER>,if ( wildcard . equals ( repo ) ) { result = true ; },segment excludes
<PLACE_HOLDER> actually generates the <PLACE_HOLDER>,naming context data store impl = ( naming context data store ) this ; synchronized ( impl ) { impl . list ( how_many @$ bl @$ bi ) ; } if ( debug && bl . value != null ) dprint ( __str__ + how_many + __str__ + bl . value . length + __str__ + bi . value ) ;,list generates
when the node has only one <PLACE_HOLDER>,if ( parent == x ) { x = parent . left == null ? parent . right : parent . left ; } else if ( x . left == null ) { if ( parent . left == x ) parent . left = x . right ; else parent . right = x . right ; } else { if ( parent . left == x ) parent . left = x . left ; else parent . right = x . left ; },node has
parse the output clear the input stream <PLACE_HOLDER>,string line = in reader . read line ( ) ; while ( line != null ) { line = in reader . read line ( ) ; },output clear
if the incoming uri matches a single note <PLACE_HOLDER> @$ does the delete based on the incoming data @$ but modifies the where clause to restrict it to the particular note <PLACE_HOLDER> .,case main_id :,uri matches
an estimate of how much extra memory is needed before we can go ahead and expand the hash table . this includes the new <PLACE_HOLDER> for values @$ group ids @$ and values by group id as well as the size of the current page,preallocated memory in bytes = ( new capacity - hash capacity ) * ( long ) ( long . bytes + integer . bytes ) + ( calculate max fill ( new capacity ) - max fill ) * long . bytes + current page size in bytes ; if ( ! update memory . update ( ) ) { return false ; },estimate includes
direct column <PLACE_HOLDER> on a complex column @$ expressions can not operate on complex columns @$ only postaggs wrap the column <PLACE_HOLDER> in a field <PLACE_HOLDER> postagg so that other postaggs can use it,if ( post aggregator complex direct column is ok ( input row signature @$ post aggregator expression @$ post aggregator rex node ) ) { final post aggregator post aggregator = new field access post aggregator ( post aggregator visitor . get output name prefix ( ) + post aggregator visitor . get and increment counter ( ) @$ post aggregator expression . get direct column ( ) ) ; post aggregator visitor . add post agg ( post aggregator ) ; row order . add ( post aggregator . get name ( ) ) ; } else if ( post aggregator direct column is ok ( input row signature @$ post aggregator expression @$ post aggregator rex node ) ) { row order . add ( post,postaggs wrap
array contains local <PLACE_HOLDER> & rack <PLACE_HOLDER>,test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; cluster . sort by distance ( data nodes [ __num__ ] @$ test nodes @$ test nodes . length ) ; assert true ( test nodes [ __num__ ] == data nodes [ __num__ ] ) ; assert true ( test nodes [ __num__ ] == data nodes [ __num__ ] ) ;,array contains
if the channel could not finish reading in timeout ms @$ the <PLACE_HOLDER> fails . it is used to guarantee the <PLACE_HOLDER> never hangs even if there are bugs of socket channel implementation . for blocking read @$ it possibly returns 0 in some cases .,assert timeout ( start time @$ timeout ) ;,channel finish
if the user configured their <PLACE_HOLDER> we use it even if persistence is disabled since we do n't know anything about their implementation .,if ( job scheduler store == null && ! has start exception ( ) ) { if ( ! is persistent ( ) ) { this . job scheduler store = new in memory job scheduler store ( ) ; configure service ( job scheduler store ) ; return this . job scheduler store ; } try { persistence adapter pa = get persistence adapter ( ) ; if ( pa != null ) { this . job scheduler store = pa . create job scheduler store ( ) ; job scheduler store . set directory ( get scheduler directory file ( ) ) ; configure service ( job scheduler store ) ; return this . job scheduler store ; } } catch ( io exception e ),user configured
setup object <PLACE_HOLDER> .,message receiver s ; if ( sponge . get game ( ) . is server available ( ) ) { s = sponge . get server ( ) . get console ( ) ; } else { s = new client message reciever ( ) ; },setup object
check proj rel only <PLACE_HOLDER> one expression check this project only <PLACE_HOLDER> one expression @$ i.e . scalar subqueries .,list < rex node > proj exprs = project . get projects ( ) ; if ( proj exprs . size ( ) != __num__ ) { return ; },expression check
we do not resolve a vanilla name starting with a lower case letter try to resolve against a default import @$ because we know that the default packages do not contain <PLACE_HOLDER> like these,test default imports &= ! ( type instanceof lower case class ) ; if ( test default imports ) { for ( int i = __num__ @$ size = default_imports . length ; i < size ; i ++ ) { string package prefix = default_imports [ i ] ; string name = type . get name ( ) ; constructed class with package tmp = new constructed class with package ( package prefix @$ name ) ; if ( resolve ( tmp @$ false @$ false @$ false ) ) { type . set redirect ( tmp . redirect ( ) ) ; return true ; } } string name = type . get name ( ) ; if ( name . equals ( __str__ ) ),packages contain
unusual code alert ! it is a bit odd to check the field when you are looking for the <PLACE_HOLDER> that contains the field . but @$ in the decompiler @$ sometimes the 'field ' happens to have the data <PLACE_HOLDER> of the thing that contains it . so @$ if you have : foo.bar then the 'bar ' field will have a data,high variable high variable = var . variable . get high variable ( ) ; if ( high variable instanceof high global ) { if ( matches parent type ( potential field @$ dt ) ) { return potential field ; } } return null ;,field have
put the band into the correct location on the <PLACE_HOLDER> . once the band is moved we translate the device transform so that the band will move down the <PLACE_HOLDER> on the next iteration of the loop .,band graphics . set transform ( uniform transform ) ; band graphics . transform ( device transform ) ; device transform . translate ( __num__ @$ - band height ) ;,band move
this test will read <PLACE_HOLDER> 0 and 1 from the parent before calling split @$ so we expect the primary read to start at offset 2 .,when ( fake storage client . read rows ( read rows request . new builder ( ) . set read position ( stream position . new builder ( ) . set stream ( stream . new builder ( ) . set name ( __str__ ) ) . set offset ( __num__ ) ) . build ( ) ) ) . then return ( new fake big query server stream < > ( parent responses . sub list ( __num__ @$ __num__ ) ) ) ;,primary read
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized . get ( ) ) ;,scheduler captures
set to un<PLACE_HOLDER>ed if global memstore size already exceeds lower <PLACE_HOLDER>,if ( flush pressure >= __num__ ) { max throughput to set = double . max_value ; } else { max throughput to set = max throughput lower bound + ( max throughput upper bound - max throughput lower bound ) * flush pressure ; },size exceeds
while partition has been spilled @$ relocation bloom filter <PLACE_HOLDER> for current bucket @$ and build bloom filter with hashcode .,if ( status == bucket_status_in_filter ) { this . bloom filter . set bits location ( bucket @$ bucket in segment pos + bucket_header_length ) ; this . bloom filter . add hash ( hash code ) ; },relocation bloom
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized . get ( ) ) ;,scheduler captures
see if any alternative name matches <PLACE_HOLDER>,if ( alt names != null ) { for ( int j = __num__ @$ n = alt names . size ( ) ; j < n ; j ++ ) { general name interface alt name = alt names . get ( j ) . get name ( ) ; if ( alt name . equals ( target ) ) { return __num__ ; } } },name matches
svn repo has <PLACE_HOLDER> . if there is a cheap test @$ then this code can be refined @$ boosting performance .,return true ;,repo has
the tables should have the same <PLACE_HOLDER> and be dividable by 3,for ( int i = __num__ ; i < vertex table . length ; i += __num__ ) { vector3f vert = new vector3f ( vertex table [ i ] @$ vertex table [ i + __num__ ] @$ vertex table [ i + __num__ ] ) ; vector3f norm = vert to normal map . get ( vert ) ; if ( norm == null ) { norm = new vector3f ( normal table [ i ] @$ normal table [ i + __num__ ] @$ normal table [ i + __num__ ] ) ; vert to normal map . put ( vert @$ norm ) ; } else { norm . add local ( normal table [ i ] @$ normal table [ i + __num__,tables have
constant get constructor <PLACE_HOLDER> 2 @$,match get member ( clazz @$ method @$ code attribute @$ offset @$ instruction @$ null @$ get constructor matcher2 @$ false @$ false @$ class constants . method_name_init @$ null ) ;,constant get
newton iteration doubles correct <PLACE_HOLDER> at each step .,inverse *= __num__ - x * inverse ; inverse *= __num__ - x * inverse ; inverse *= __num__ - x * inverse ; return inverse ;,iteration doubles
if we have n't got too many attributes let the super <PLACE_HOLDER> look it up,if ( super . get length ( ) < max ) { index = super . get index ( uri @$ local name ) ; return index ; },attributes let
populate the new build rule graph <PLACE_HOLDER> with all of the usable rules from the last build rule graph <PLACE_HOLDER> for incremental action graph generation .,action graph cache . populate action graph builder with cached rules ( event bus @$ target graph @$ graph builder ) ;,new build
the object that we 're modifying here is a copy of the original ! so let 's change the <PLACE_HOLDER> from relative to absolute by grabbing the file object ... in case the name of the file comes from previous steps @$ forget about this !,try { list < string > new filenames = new array list < string > ( ) ; if ( ! is in fields ( ) ) { file input list file list = get files ( space ) ; if ( file list . get files ( ) . size ( ) > __num__ ) { for ( file object file object : file list . get files ( ) ) { if ( file object . exists ( ) ) { new filenames . add ( file object . get name ( ) . get path ( ) ) ; } } set file name ( new filenames . to array ( new string [ new filenames . size ( ) ] ) ) ; set,let change
fire the presence changed <PLACE_HOLDER>,while ( presences . has next ( ) ) { listener . presence changed ( presences . next ( ) ) ; },presence changed
if the <PLACE_HOLDER> sequence number is greater than the next sequence number @$ that indicates a command is missing . register the command <PLACE_HOLDER> and return a future to be completed once commands are properly sequenced . if the session 's current sequence number is too far beyond the last known sequence number @$ reject the command to force it to be resent by,if ( sequence number > session . next request sequence ( ) ) { if ( session . get commands ( ) . size ( ) < max_pending_commands ) { log . trace ( __str__ @$ sequence number @$ session . next request sequence ( ) ) ; session . register command ( request . sequence number ( ) @$ new pending command ( request @$ future ) ) ; return future ; } else { return completable future . completed future ( log response ( command response . builder ( ) . with status ( raft response . status . error ) . with error ( raft error . type . command_failure ) . with last sequence ( session . get request sequence ( ) ) .,future register
copy resource into a byte array . this is necessary because several browsers consider class.get resource a security risk since it can be used to load additional classes . class.get resource as stream just returns raw <PLACE_HOLDER> @$ which we can convert to a sound .,byte [ ] buffer = access controller . do privileged ( new privileged action < byte [ ] > ( ) { public byte [ ] run ( ) { try { input stream resource = basic look and feel . this . get class ( ) . get resource as stream ( sound file ) ; if ( resource == null ) { return null ; } buffered input stream in = new buffered input stream ( resource ) ; byte array output stream out = new byte array output stream ( __num__ ) ; byte [ ] buffer = new byte [ __num__ ] ; int n ; while ( ( n = in . read ( buffer ) ) > __num__ ) { out .,stream returns
only fill in <PLACE_HOLDER> if the argument has a <PLACE_HOLDER> .,code . begin control flow ( __str__ @$ i ) ;,argument has
send user leave <PLACE_HOLDER>,m callbacks . on animation finished ( reorder mode @$ run synchronously @$ false ) ;,user leave
record the failure persistently @$ and upload to uma when the library successfully loads next <PLACE_HOLDER> .,precacheuma . record ( precacheuma . event . precache_task_load_library_fail ) ; break ; default : break ;,library loads
patch throwing pc into return address so that deoptimization finds the right debug <PLACE_HOLDER>,patch return address ( exception pc ) ; word handler pc = exception handler for pc ( exception_handler_for_pc @$ thread ) ; if ( logging ) { printf ( __str__ @$ word . object to tracked pointer ( exception ) . raw value ( ) @$ exception pc . raw value ( ) @$ handler pc . raw value ( ) ) ; decipher ( handler pc . raw value ( ) ) ; printf ( __str__ ) ; },deoptimization finds
a set has only one <PLACE_HOLDER>,return replace ( current element @$ new element ) ;,set has
throw an unchecked exception so the controller can examine the <PLACE_HOLDER> and see whether or not it 's okay,throw new test exception ( __str__ @$ e ) ; log writer utils . get log writer ( ) . error ( __str__ + e @$ e ) ; throw new test exception ( __str__ @$ e ) ; log writer utils . get log writer ( ) . info ( __str__ @$ rde ) ; log writer utils . get log writer ( ) . info ( __str__ @$ cce ) ;,controller examine
string buffer does not throw io <PLACE_HOLDER>,append padded integer ( ( appendable ) buf @$ value @$ size ) ;,buffer throw
wait until main thread gets <PLACE_HOLDER> of the semaphore,synchronized ( locka ) { while ( handshake . get waiter count ( ) == __num__ ) { utils . go sleep ( __num__ ) ; } handshake . semav ( ) ; try { blocked thread . wait until blocked ( ) ; system . out . println ( __str__ ) ; utils . check thread state ( itself @$ thread . state . runnable ) ; check stack ( itself @$ examiner stack @$ es depth ) ; system . out . println ( __str__ + __str__ ) ; utils . check thread state ( blocked thread @$ thread . state . blocked ) ; check stack ( blocked thread @$ blocked stack @$ bs depth ) ; } catch ( exception e ) { e,thread gets
the index population detects <PLACE_HOLDER> on the fly @$ however for updates coming in we 're in a position where we can not detect <PLACE_HOLDER> while applying @$ but instead afterwards .,if ( descriptor . is unique ( ) && can check conflicts without store access ( ) ) { updater = new deferred conflict checking index updater ( updater @$ this :: new reader @$ descriptor ) ; },population detects
report if multiple devices are matching the <PLACE_HOLDER> .,if ( ! quiet && devices . size ( ) > __num__ ) { print message ( __str__ + devices . size ( ) + __str__ ) ; },devices matching
if the expression does n't contain any <PLACE_HOLDER> @$ we 'll look up the <PLACE_HOLDER> in the imports . if not found there @$ it 's a local <PLACE_HOLDER> .,if ( expression . contains ( dot ) ) { int last dot index = expression . last index of ( __str__ ) ; messages package = expression . substring ( __num__ @$ last dot index ) ; } else { string package name = imported classes . get ( expression ) ; if ( package name == null ) { messages package = class package ; } else { messages package = package name ; } },expression contain
closing the writable log channel @$ then the underlying channel is <PLACE_HOLDER> physical log file does,channel . close ( ) ; store channel . close ( ) ;,file does
we overrode this method here to clear the picked state of edges and vertices if we ever get a released event when the user is clicking <PLACE_HOLDER> that is not an edge or vertex,if ( ! is dragging ( ) && vertex == null && edge == null ) { maybe clear picked state ( e ) ; } super . mouse released ( e ) ;,user clicking
this will correct for the <PLACE_HOLDER> of the first view not touching the <PLACE_HOLDER> of the list,adjust views up or down ( ) ; below = fill down ( position + __num__ @$ next top ) ; int child count = get child count ( ) ; if ( child count > __num__ ) { correct too high ( child count ) ; } if ( temp is selected ) { return temp ; } else if ( above != null ) { return above ; } else { return below ; },top touching
test if gdi can handle the <PLACE_HOLDER>,boolean direct togdi = ( ( transform type != affine transform . type_general_transform ) && ( ( transform type & affine transform . type_flip ) == __num__ ) ) ; if ( ! direct togdi ) { return __num__ ; },gdi handle
verify that we can read in all the transactions that we have written . if there were any corruptions @$ it is likely that the reading in of these transactions will throw an <PLACE_HOLDER> .,for ( storage directory sd : fsimage . get storage ( ) . dir iterable ( name node dir type . edits ) ) { file edit file = new file ( sd . get current dir ( ) @$ log file name ) ; system . out . println ( __str__ + edit file ) ; fs edit log loader loader = new fs edit log loader ( namesystem @$ start tx id ) ; long num edits this log = loader . loadfs edits ( new edit log file input stream ( edit file ) @$ start tx id ) ; system . out . println ( __str__ + num edits this log ) ; assert true ( num edits == - __num__ || num edits,the throw
we only need to create a customized image @$ if we 're running on linux @$ as docker on mac os and windows does n't map <PLACE_HOLDER> from the host into the container anyway .,if ( os . get current ( ) != os . linux ) { return base image ; } return image map . compute if absent ( base image @$ ( image ) -> { reporter . handle ( event . info ( __str__ + image + __str__ ) ) ; string work dir = path fragment . create ( __str__ ) . get relative ( exec root . get base name ( ) ) . get path string ( ) ; string builder dockerfile = new string builder ( ) ; dockerfile . append ( string . format ( __str__ @$ image ) ) ; dockerfile . append ( string . format ( __str__ @$ work dir ) ) ; if ( gid > __num__ ) {,docker map
lazy check that we get 5 rows back @$ put off checking <PLACE_HOLDER>,assert equals ( __num__ @$ results [ __num__ ] . get row count ( ) ) ;,check put
make sure this call happens before check declared application <PLACE_HOLDER> as servlet ! ! !,boolean has boot = has boot classes ( webdata ) ; resteasy deployment data . set boot classes ( has boot ) ; class < ? > declared application class = check declared application class as servlet ( webdata @$ class loader ) ;,check declared
the call to build may also throw unsupported version <PLACE_HOLDER> @$ if there are essential fields that can not be represented in the chosen version .,do send ( client request @$ is internal request @$ now @$ builder . build ( version ) ) ;,call throw
unlike message consumers @$ we try current span before trying extraction . this is the proper order because the span in scope should take <PLACE_HOLDER> over a potentially stale header entry . note : brave instrumentation used properly does not result in stale header entries @$ as we always clear message headers after reading .,span span ; if ( maybe parent == null ) { trace context or sampling flags extracted = kafka tracing . extract and clear headers ( extractor @$ request @$ record . headers ( ) ) ; span = kafka tracing . next messaging span ( sampler @$ request @$ extracted ) ; } else { span = tracer . new child ( maybe parent ) ; },span take
table has these split <PLACE_HOLDER>,byte [ ] [ ] table split keys = new byte [ ] [ ] { bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) } ;,table has
referral authenticated <PLACE_HOLDER>,test deployment ( web appurl @$ __str__ @$ __str__ @$ __str__ ) ;,referral authenticated
this action is typically bound to space . if the table is already in an editing mode @$ space should simply enter a space <PLACE_HOLDER> into the table @$ and not select a cell . likewise @$ if the lead cell is already selected then hitting space should just enter a space <PLACE_HOLDER> into the cell and begin editing . in both of these,if ( key == add_to_selection && sender instanceof j table ) { j table table = ( j table ) sender ; int lead row = get adjusted lead ( table @$ true ) ; int lead col = get adjusted lead ( table @$ false ) ; return ! ( table . is editing ( ) || table . is cell selected ( lead row @$ lead col ) ) ; } else if ( key == focus_header && sender instanceof j table ) { j table table = ( j table ) sender ; return table . get table header ( ) != null ; },space enter
before sending the request @$ check if the request requires a global <PLACE_HOLDER> and what we have is a local <PLACE_HOLDER> . if so do an upgrade .,if ( check for upgrade ) { request upgrade request = null ; try { upgrade request = zks . check upgrade session ( request ) ; } catch ( keeper exception ke ) { if ( request . get hdr ( ) != null ) { request . get hdr ( ) . set type ( op code . error ) ; request . set txn ( new error txn ( ke . code ( ) . int value ( ) ) ) ; } request . set exception ( ke ) ; log . warn ( __str__ @$ ke ) ; } catch ( io exception ie ) { log . error ( __str__ @$ ie ) ; } if ( upgrade request != null ),request requires
hour left <PLACE_HOLDER>,m hour left = m left padding size ;,hour left
test send <PLACE_HOLDER> .,account send account = public methed . query account ( send account key @$ blocking stub full ) ; long send account before balance = send account . get balance ( ) ; assert . assert true ( send account before balance == __num__ ) ; account receipt account = public methed . query account ( receipt account key @$ blocking stub full ) ; long receipt account before balance = receipt account . get balance ( ) ; assert . assert true ( receipt account before balance == __num__ ) ;,test send
copy the existing queue into the new one . first remove the oldest <PLACE_HOLDER> if no new ones exist .,if ( m event records . remaining capacity ( ) == __num__ ) { event record record = m event records . poll ( ) ; if ( record != null ) { m call event record map . remove ( record . get record entry ( ) ) ; } },one remove
close the column family handles <PLACE_HOLDER> @$ then the db last,for ( column family handle cfh : column family handles . values ( ) ) { safe close ( cfh @$ exception reference ) ; } logger . info ( __str__ ) ; safe close ( rocksdb @$ exception reference ) ; rocksdb = null ; closed = true ; if ( exception reference . get ( ) != null ) { throw new io exception ( exception reference . get ( ) ) ; } db write lock . unlock ( ) ;,family handles
open d trace <PLACE_HOLDER>,cons . open ( ) ;,d trace
the implementation of fragments do n't need to be beans <PLACE_HOLDER>,spring data repository creator repository creator = new spring data repository creator ( class output @$ composite index @$ ( n ) -> { additional beans . produce ( additional bean build item . unremovable of ( n ) ) ; } ) ;,implementation need
load <PLACE_HOLDER> removes loaded <PLACE_HOLDER> from the input set @$ so the remaining ones were not on the node,final int iterator properties with no value = additional properties to load . int iterator ( ) ; while ( properties with no value . has next ( ) ) { put ( properties with no value . next ( ) @$ no_value ) ; },properties removes
check if the run has enough extra <PLACE_HOLDER> to fit the last tab from the previous row ...,if ( max - end > prev last len ) { tab runs [ run ] = prev last ; if ( ! vertical tab runs ) { rects [ prev last ] . x = start ; } else { rects [ prev last ] . y = start ; } for ( int i = prev last + __num__ ; i <= last ; i ++ ) { if ( ! vertical tab runs ) { rects [ i ] . x = rects [ i - __num__ ] . x + rects [ i - __num__ ] . width ; } else { rects [ i ] . y = rects [ i - __num__ ] . y + rects [ i - __num__ ],run has
for now @$ let 's see <PLACE_HOLDER> happens if we do a similar thing to <PLACE_HOLDER> the runtime code does . i suspect this may cause us to lose the top frame from the stack .,address sp = context . get register as address ( sparc thread context . r_sp ) ; address pc = context . get register as address ( sparc thread context . r_pc ) ; if ( ( sp == null ) || ( pc == null ) ) { return null ; },code does
all existing types should have a dedicated <PLACE_HOLDER>,for ( scenario simulation model . type value : scenario simulation model . type . values ( ) ) { final scenario runner provider retrieved = abstract scenario runner . get specific runner provider ( value ) ; assert not null ( retrieved ) ; },types have
note : we could just check for the error key by itself @$ since we now set that @$ but since we have to go through the whole hash any<PLACE_HOLDER> @$ do it this <PLACE_HOLDER> @$ which is safer for maintenance,if ( key str . starts with ( error ) ) { errors = true ; } log msg ( key str + __str__ + h . get ( key str ) ) ;,key do
note to translators : an element in the stylesheet requires a particular <PLACE_HOLDER> named by the substitution text @$ but that <PLACE_HOLDER> was not specified in the stylesheet .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note requires
specifically only include relevant bits according to the env var <PLACE_HOLDER> @$ rather than strictly overlaying config so that if the config file has a <PLACE_HOLDER> for master endpoints @$ the file does n't override the env var if it 's a domain : as master endpoints takes precedence over domains .,final uri uri = new uri ( master ) ; config config from env var = config factory . empty ( ) ;,file has
handle possible indirection indirect flow should be to a data pointer which references <PLACE_HOLDER> .,if ( ref type == ref type . indirection ) { instruction dest instr = listing . get instruction containing ( ref . get to address ( ) ) ; int cnt = __num__ ; if ( dest instr == null && follow indirect flows ) { if ( instr == null || ! instr . get min address ( ) . equals ( from addr ) ) { instr = listing . get instruction at ( from addr ) ; } cnt = follow indirection ( block ref queue @$ include externals @$ block @$ ref @$ instr . get flow type ( ) . is call ( ) ? ref type . computed_call : ref type . computed_jump @$ monitor ) ; } if ( cnt,which references
empty error messages indicate <PLACE_HOLDER>,if ( original host id == cluster save file state . error_code ) { long host id = savefile_data [ __num__ ] . get long ( __str__ ) ; string host name = savefile_data [ __num__ ] . get string ( __str__ ) ; string warning msg = savefile_data [ __num__ ] . get string ( __str__ ) ; warnings . add ( __str__ + host id + __str__ + host name + __str__ + warning msg ) ; },messages indicate
second observer will receive only last <PLACE_HOLDER>,second observer . assert value ( __num__ ) ;,observer receive
let <PLACE_HOLDER> 1 proceed and cancel <PLACE_HOLDER> 2,task2 handle queue . add ( handle ) ;,task proceed
the test data is constructed such that the merge join zig zag has an early <PLACE_HOLDER> @$ leaving elements on the dynamic path input unconsumed,data set < path > edges = env . from elements ( new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) ) ; iterative data set < path > current paths = edges . iterate ( __num__ ) ; data set < path > new paths = current paths . join ( edges @$ join hint . repartition_sort_merge,zag has
events should share <PLACE_HOLDER>,assert same ( event @$ events . pop ( ) ) ;,events share
update the per file <PLACE_HOLDER>,if ( file check sum generator != null ) { file check sum generator . update ( buffer @$ __num__ @$ read ) ; },per file
functions within i fs cause syntax <PLACE_HOLDER> on safari .,assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ;,functions fs
if comment does n't have a post <PLACE_HOLDER> @$ set it to the passed one and save to comment table,if ( m comment != null && m comment . get post title ( ) == null ) { m comment . set post title ( post title ) ; m dispatcher . dispatch ( comment action builder . new update comment action ( m comment ) ) ; },comment have
add a listener to know when android has done another measurement pass . the listener automatically removes <PLACE_HOLDER> to prevent triggering the animation multiple times .,m layout . add on layout change listener ( new on layout change listener ( ) { @ override public void on layout change ( view v @$ int left @$ int top @$ int right @$ int bottom @$ int old left @$ int old top @$ int old right @$ int old bottom ) { m layout . remove on layout change listener ( this ) ; start animator ( callback ) ; } } ) ;,listener removes
copy & find decimal <PLACE_HOLDER>,int decimal point = end ; for ( int i = start @$ j = offset ; i < end ; i ++ @$ j ++ ) { buf [ i ] = value [ j ] ; if ( buf [ i ] == __str__ && i < decimal point ) { decimal point = i ; } },& find
test chrome driver is not honoring headless <PLACE_HOLDER> ; using chrome driver instead,super . driver = new chrome driver ( options ) ; driver = ( chrome driver ) super . driver ; driver . get ( pages . clicks page ) ; assert that ( check permission ( driver @$ clipboard_read ) ) . is equal to ( __str__ ) ; assert that ( check permission ( driver @$ clipboard_write ) ) . is equal to ( __str__ ) ; driver . set permission ( clipboard_read @$ __str__ ) ; driver . set permission ( clipboard_write @$ __str__ ) ; assert that ( check permission ( driver @$ clipboard_read ) ) . is equal to ( __str__ ) ; assert that ( check permission ( driver @$ clipboard_write ) ) . is equal to ( __str__ ) ;,driver honoring
add epollrdhup so we are notified once the remote peer close the <PLACE_HOLDER> .,flags |= native . epollrdhup ;,peer close
produce a message to broker 1 's test queue and verify that broker 1 's memory <PLACE_HOLDER> has increased @$ but broker 2 still has no memory <PLACE_HOLDER> .,send messages ( __str__ @$ test queue @$ __num__ ) ; assert true ( broker1 test queue . get memory usage ( ) . get usage ( ) > __num__ ) ; assert equals ( __num__ @$ broker2 test queue . get memory usage ( ) . get usage ( ) ) ;,broker has
do one last placeholder <PLACE_HOLDER> @$ this is useful as we do n't stop the build when a library failed a placeholder <PLACE_HOLDER> @$ but the element might have been overridden so the problem was transient . however @$ with the final document ready @$ all placeholders values must have been provided .,if ( ! m optional features . contains ( invoker . feature . no_placeholder_replacement ) ) { perform place holder substitution ( loaded main manifest info @$ xml document optional . get ( ) @$ merging report builder ) ; if ( merging report builder . has errors ( ) ) { return merging report builder . build ( ) ; } },library failed
the partition must have a current <PLACE_HOLDER>,string consumer = current partition consumer . get ( partition ) ; if ( consumer == null ) log . error ( __str__ @$ partition ) ; if ( prev assignment . contains key ( partition ) && current assignment . get ( consumer ) . size ( ) > current assignment . get ( prev assignment . get ( partition ) . consumer ) . size ( ) + __num__ ) { reassign partition ( partition @$ current assignment @$ sorted current subscriptions @$ current partition consumer @$ prev assignment . get ( partition ) . consumer ) ; reassignment performed = true ; modified = true ; continue ; },partition have
only some of the node memories require special <PLACE_HOLDER> handling so we iterate over all of them and process only those that require it,for ( base node base node : context . sinks . values ( ) ) { memory memory = memories . peek node memory ( base node ) ; if ( memory != null ) { protobuf messages . node memory _node = null ; switch ( memory . get node type ( ) ) { case node type enums . query element node : { _node = write query element node memory ( base node . get id ( ) @$ memory @$ wm ) ; break ; } } if ( _node != null ) { _ksb . add node memory ( _node ) ; } } },some require
the actual tested <PLACE_HOLDER>,secu filter . process authentication ( filter context @$ client builder @$ method security @$ tracing . atn tracing ( ) ) ; assert that ( filter context . is should finish ( ) @$ is ( true ) ) ; assert that ( secu context . user ( ) @$ is ( optional . empty ( ) ) ) ;,actual tested
note : batch update the <PLACE_HOLDER> of dataset if the event traffic spike,dataset config manager . update last refresh time ( dataset @$ event . get high watermark ( ) ) ; thirdeye metrics util . processed trigger event counter . inc ( ) ; log . debug ( __str__ + event . get dataset name ( ) ) ;,batch update
direct call to the <PLACE_HOLDER> service shall not throw a <PLACE_HOLDER> not found exception for an internal <PLACE_HOLDER>,if ( relation serv call flg ) { try { relation serv . send role update notification ( my rel id @$ new role @$ old role value ) ; } catch ( relation not found exception exc ) { throw new runtime exception ( exc . get message ( ) ) ; } } else { object [ ] params = new object [ __num__ ] ; params [ __num__ ] = my rel id ; params [ __num__ ] = new role ; params [ __num__ ] = old role value ; string [ ] signature = new string [ __num__ ] ; signature [ __num__ ] = __str__ ; signature [ __num__ ] = __str__ ; signature [ __num__ ] = __str__ ; try {,call throw
updated golden value since we have a different serial version <PLACE_HOLDER> in open jdk .,string s = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; tree map < string @$ string > map = new tree map < string @$ string > ( string . case_insensitive_order ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; sorted map < string @$ string > sub map = map . sub map ( __str__ @$ __str__ ) ; new serialization tester < sorted map < string @$ string > > ( sub map @$ s ) { @,value have
check number of transactions @$ should only have <PLACE_HOLDER>,log entry reader log entry reader = new version aware log entry reader ( new test command reader factory ( ) ) ; assert that ( log files . get lowest log version ( ) @$ is ( log files . get highest log version ( ) ) ) ; long version = log files . get highest log version ( ) ; try ( log versioned store channel channel = log files . open for version ( version ) ; read ahead log channel read ahead log channel = new read ahead log channel ( channel ) ; log entry cursor cursor = new log entry cursor ( log entry reader @$ read ahead log channel ) ) { log entry entry ; long number of transactions,number have
create the allow option <PLACE_HOLDER> .,final j panel allow option panel = new j panel ( layout ) ; allow option panel . set border ( border factory . create titled border ( etched border @$ msg ( __str__ ) ) ) ; allow option panel . add ( tip ( allow shrinking check box @$ __str__ ) @$ constraints last stretch ) ; allow option panel . add ( tip ( allow optimization check box @$ __str__ ) @$ constraints last stretch ) ; allow option panel . add ( tip ( allow obfuscation check box @$ __str__ ) @$ constraints last stretch ) ;,the allow
the front end will know <PLACE_HOLDER> one is the active one ; just use the first one for the test,final workspace active workspace = workspaces [ __num__ ] ; swing utilities . invoke and wait ( new runnable ( ) { @ override public void run ( ) { running tool = active workspace . run tool ( tool config ) ; } } ) ; assert not null ( running tool ) ; swing utilities . invoke and wait ( new runnable ( ) { @ override public void run ( ) { running tool . close ( ) ; } } ) ;,one one
adapter views contain their <PLACE_HOLDER> in a frame so we need to go one layer deeper here .,if ( parent instanceof adapter view animator ) { vg = ( view group ) vg . get child at ( __num__ ) ; } if ( vg == null ) return ; remote response response = null ; int child count = vg . get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { object tag = vg . get child at ( i ) . get tag ( com . android . internal . r . id . fill in intent ) ; if ( tag instanceof remote response ) { response = ( remote response ) tag ; break ; } } if ( response == null ) return ; response . handle,views contain
call the g rpc echo <PLACE_HOLDER>,echo . echo response response = echo service grpc . new blocking stub ( channel ) . echo ( echo . echo request . new builder ( ) . set message ( __str__ ) . build ( ) ) ; assert that ( response . get message ( ) @$ is ( __str__ ) ) ; ( ( managed channel ) channel ) . shutdown ( ) . await termination ( __num__ @$ time unit . seconds ) ;,rpc echo
use batch scan <PLACE_HOLDER> when performing seeded operation,input configurator . set batch scan ( accumulo input format . class @$ conf @$ true ) ; add iterators ( accumulo store @$ conf @$ context . get user ( ) @$ operation ) ; add ranges ( accumulo store @$ conf @$ operation ) ; final java pairrdd < element @$ null writable > pairrdd = spark context . newapi hadooprdd ( conf @$ element input format . class @$ element . class @$ null writable . class ) ; final javardd < element > rdd = pairrdd . map ( new first element ( ) ) ; return rdd ;,batch scan
transfer send some asset issue to default account @$ to test if this transaction use the transaction free <PLACE_HOLDER> .,assert . assert true ( public methed . transfer asset ( to address @$ asset account id . to byte array ( ) @$ __num__ @$ transfer asset address @$ transfer asset create key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ; asset creator net = public methed . get account net ( asset014 address @$ blocking stub full ) ; asset transfer net = public methed . get account net ( transfer asset address @$ blocking stub full ) ; long creator after net used = asset creator net . get net used ( ) ; long transfer after free net used = asset transfer net . get free net used ( ) ; logger,transaction use
e.g . binary operator extends bi function @$ binary operator contains no abstract <PLACE_HOLDER> @$ but it is really a sam,if ( type . is interface ( ) ) { methods = type . redirect ( ) . get all declared methods ( ) ; } else { methods = type . get methods ( ) ; },operator contains
dw suggested buffer size specifies the suggested buffer size for reading the file . generally @$ this size should be large enough to contain the largest chunk in the file . if set to zero @$ or if it is too small @$ the playback software will have to reallocate memory during playback @$ which will reduce <PLACE_HOLDER> . for an interleaved file @$,video track vt = null ; int width = __num__ @$ height = __num__ ;,which reduce
csv can not differentiate null and empty <PLACE_HOLDER> .,if ( value == null ) { current line . add ( __str__ ) ; } else if ( value instanceof string ) { current line . add ( ( string ) value ) ; } else { current line . add ( value . to string ( ) ) ; },csv differentiate
configuration for publishing jars containing sources for generated <PLACE_HOLDER> to the project artifacts for including in the ivy.xml,if ( _generate sources jar task == null ) { configuration container configurations = project . get configurations ( ) ; configuration generated sources = configurations . maybe create ( __str__ ) ; configuration test generated sources = configurations . maybe create ( __str__ ) ; test generated sources . extends from ( generated sources ) ; _generate sources jar task = project . get tasks ( ) . create ( __str__ @$ jar . class @$ jar task -> { jar task . set group ( java base plugin . documentation_group ) ; jar task . set description ( __str__ ) ; jar task . set classifier ( __str__ ) ; } ) ; project . get artifacts ( ) . add ( __str__ @$ _generate sources,configuration generated
if the op <PLACE_HOLDER> is null then the implementation does n't offer a presence operation <PLACE_HOLDER> which is unacceptable for gibberish .,if ( operation set presence1 == null ) { throw new null pointer exception ( __str__ + __str__ + __str__ ) ; },implementation offer
n 1 has no <PLACE_HOLDER>,string nodes config = __str__ ;,n has
all notes with this template must have at least two <PLACE_HOLDER> @$ or we could end up creating orphaned notes,sql = __str__ + utils . ids2str ( cids ) + __str__ ; if ( m col . get db ( ) . query scalar ( sql ) != __num__ ) { return false ; },notes have
keys which have similar <PLACE_HOLDER> as the desired key,list < string > possible matches = new array list < > ( ) ;,which have
set up things @$ the order does <PLACE_HOLDER> as if it is messed up then the set up should fail because of the precondition checks in the respective set up methods for creating a customized task queue see test realtime index task failure test,task storage = set up task storage ( ) ; handoff notifier factory = set up segment hand off notifier factory ( ) ; data segment pusher = set up data segment pusher ( ) ; mdc = set up metadata storage coordinator ( ) ; tb = set up task toolbox factory ( data segment pusher @$ handoff notifier factory @$ mdc ) ; task runner = set up thread pool task runner ( tb ) ; task queue = set up task queue ( task storage @$ task runner ) ;,order does
this is expected @$ void type options have no <PLACE_HOLDER> .,if ( option definition . get type ( ) . equals ( void . class ) ) { } else if ( next args . has next ( ) ) { unconverted value = next args . next ( ) ; command line form . append ( __str__ ) . append ( unconverted value ) ; } else { throw new options parsing exception ( __str__ + arg ) ; },options have
socket timeout exceptions are interrupted io exceptions ; however they do not signify an external interruption @$ but simply a failed download due to some server timing out . so rethrow <PLACE_HOLDER> as ordinary io exceptions .,throw new io exception ( e ) ;,download rethrow
we 'll cheat here and do n't throw an illegal state exception even if this is n't pinned @$ because we know that the reference never gets <PLACE_HOLDER> .,return row ;,reference gets
do not start any application as super dev mode will refresh the <PLACE_HOLDER> once done compiling,if ( super dev mode . enable based on parameter ( ) ) { return ; },mode refresh
source and destination images may have different <PLACE_HOLDER> requirements @$ therefore may have different strides . copy row by row for such case .,int src offset = src buffer . position ( ) ; int dst offset = dst buffer . position ( ) ; size effective plane size = get effective plane size for image ( src @$ i ) ; int src byte count = effective plane size . get width ( ) * src planes [ i ] . get pixel stride ( ) ; for ( int row = __num__ ; row < effective plane size . get height ( ) ; row ++ ) { if ( row == effective plane size . get height ( ) - __num__ ) { int remaining bytes = src buffer . remaining ( ) - src offset ; if ( src byte count > remaining bytes ) { src,images have
will be null in the get <PLACE_HOLDER> or when we run out opf attempts..,if ( notification time != null ) { try { janitor queue . record future notification ( notification time @$ key @$ user token @$ account record id @$ tenant record id ) ; } catch ( final io exception e ) { log . warn ( __str__ @$ payment transaction id @$ e . get message ( ) ) ; } },the get
first lets check that a fully opaque foreground has sufficient <PLACE_HOLDER>,double test ratio = contrast calculator . calculate contrast ( foreground @$ background @$ __num__ ) ; if ( test ratio < min contrast ratio ) { return - __num__ ; },foreground has
we hide the <PLACE_HOLDER> label until we know if the chat supports <PLACE_HOLDER> .,sms button . set visible ( false ) ; center panel . add ( sms button @$ constraints ) ;,chat supports
we then use the looked up <PLACE_HOLDER> to assert that it behaves as expected @$ just like in any other test .,map < com . oracle . truffle . api . source . source @$ coverage > coverage map = coverage instrument . get coverage map ( ) ; assert . assert equals ( __num__ @$ coverage map . size ( ) ) ; coverage map . for each ( ( com . oracle . truffle . api . source . source s @$ coverage v ) -> { set < integer > not yet covered line numbers = coverage instrument . non covered line numbers ( s ) ; object [ ] expected = new integer [ ] { __num__ @$ __num__ @$ __num__ } ; assert . assert array equals ( expected @$ not yet covered line numbers . stream ( ) . sorted ( ) .,the looked
this can happen when the set method throw a checked <PLACE_HOLDER> or something like that,throw new web service exception ( e ) ;,method throw
this case should pass on <PLACE_HOLDER> 6 and later .,if ( ! system . get property ( __str__ ) . starts with ( __str__ ) ) { fail ( ) ; } assert equals ( __str__ @$ e . get message ( ) ) ; assert equals ( __str__ @$ e . get format name ( ) ) ;,case pass
we know an exists only ever has one <PLACE_HOLDER> @$ and the previous algorithm has confirmed the <PLACE_HOLDER> is an or,final group element or = ( group element ) parent . get children ( ) . get ( __num__ ) ; parent . set type ( group element . not ) ; parent . get children ( ) . clear ( ) ; final group element and = group element factory . new and instance ( ) ; for ( rule condition element rule condition element : or . get children ( ) ) { final group element new not = group element factory . new not instance ( ) ; new not . add child ( rule condition element ) ; and . add child ( new not ) ; } parent . add child ( and ) ; parent . pack ( ) ;,exists has
if t and u have the same <PLACE_HOLDER> and same base kind,if ( ( t & ( dim | base_kind ) ) == ( u & ( dim | base_kind ) ) ) { if ( ( u & base_kind ) == object ) { v = ( t & dim ) | object | cw . get merged type ( t & base_value @$ u & base_value ) ; } else { int vdim = element_of + ( u & dim ) ; v = vdim | object | cw . add type ( __str__ ) ; } } else if ( ( t & base_kind ) == object || ( t & dim ) != __num__ ) { int tdim = ( ( ( t & dim ) == __num__ || ( t & base_kind ) ==,t have
enable long press to <PLACE_HOLDER> items enable <PLACE_HOLDER> using handle view enable swipe items,m adapter . set long press drag enabled ( true ) . set handle drag enabled ( true ) . set swipe enabled ( true ) ;,view enable
the expression must have no <PLACE_HOLDER>,return check parameter count ( expr @$ keywords . none ) ;,expression have
then the last task should be cleared @$ and the system should quit lock task <PLACE_HOLDER>,verify ( tr1 ) . perform clear task locked ( ) ; assert false ( m lock task controller . is task locked ( tr1 ) ) ; assert equals ( lock_task_mode_none @$ m lock task controller . get lock task mode state ( ) ) ; verify lock task stopped ( times ( __num__ ) ) ;,system quit
arc to treats the sweep angle mod 360 @$ so check for that @$ since we think 360 means draw the entire <PLACE_HOLDER>,if ( sweep < __num__ && sweep > - __num__ ) { ring path . set fill type ( path . fill type . even_odd ) ; ring path . move to ( x + radius @$ y ) ; ring path . line to ( x + radius + thickness @$ y ) ; ring path . arc to ( bounds @$ __num__ @$ sweep @$ false ) ; ring path . arc to ( inner bounds @$ sweep @$ - sweep @$ false ) ; ring path . close ( ) ; } else { ring path . add oval ( bounds @$ path . direction . cw ) ; ring path . add oval ( inner bounds @$ path . direction . ccw ) ;,means draw
the wait is necessary to have the poll function complete and propagate the <PLACE_HOLDER> from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one three = database onecode node . get comments ( ) . get global code node comment ( ) ; final list < i comment > two three = database twocode node . get comments ( ) . get global code node comment ( ) ; assert equals ( one two size @$ one three . size ( ) ) ; assert equals ( two two size @$ two three . size ( ) ) ; assert equals ( one three @$ two three ) ; assert equals ( __str__ @$ iterables . get last ( one three ) . get comment ( ),function complete
user 6 has other global <PLACE_HOLDER>,user dto user6 = db . users ( ) . insert user ( with email ( __str__ ) ) ; db . users ( ) . insert permission on user ( organization1 @$ user6 @$ administer_quality_profiles ) ;,user has
create the base list of classes which have possible <PLACE_HOLDER> to be overloaded,this . class list = new linked hash set < class > ( ) ; this . class list . add ( super class ) ; if ( generate delegate field ) { class list . add ( delegate class ) ; collections . add all ( this . class list @$ delegate class . get interfaces ( ) ) ; } if ( interfaces != null ) { collections . add all ( this . class list @$ interfaces ) ; } this . proxy name = proxy name ( ) ; this . empty body = empty body ;,which have
grantor acknowledged <PLACE_HOLDER> of lock ...,if ( is debug enabled_dls ) { if ( my reply . reply code == d lock release reply message . ok ) { logger . trace ( log marker . dls_verbose @$ __str__ @$ this . object name @$ my reply . service name ) ; } else if ( my reply . reply code == d lock release reply message . not_grantor ) { logger . trace ( log marker . dls_verbose @$ __str__ @$ my reply . get sender ( ) @$ my reply . service name ) ; } },grantor acknowledged
indent:0 exp:12 warn indent:8 <PLACE_HOLDER> indent:8 <PLACE_HOLDER> indent:8 <PLACE_HOLDER>,system . get property ( __str__ ) ;,exp:12 warn
the <PLACE_HOLDER> can have two peers at the same time and there is no one is conference focus . this is situation when someone has made an attended transfer and has transfered us . we have one <PLACE_HOLDER> with two peers the one we are talking to and the one we have been transfered to . and the first one is been hanged up,return call peers . size ( ) > __num__ ;,call have
spawn a thread to put on server @$ which will acquire a <PLACE_HOLDER> on entry,set client server observer for before interest recovery ( ) ; server1 . invoke ( ( ) -> start server ( ) ) ; verify dead and live servers ( __num__ @$ __num__ ) ; wait for before interest recovery call back ( ) ;,which acquire
always log the <PLACE_HOLDER> here @$ we might not see the <PLACE_HOLDER> in the caller,log . error ( __str__ @$ db err ) ;,error see
any object literal definition prevents <PLACE_HOLDER> .,used ( lines ( __str__ @$ __str__ ) ) ;,definition prevents
they are in different functions @$ find out which encloses the <PLACE_HOLDER>,if ( has larger parent ( outer2 @$ section length ) ) { outer = outer2 ; } else { },which encloses
if user is scrolling headers support <PLACE_HOLDER> @$ swap to empty <PLACE_HOLDER> and wait scrolling finishes .,if ( is showing headers ( ) && grid view != null && grid view . get scroll state ( ) != recycler view . scroll_state_idle ) { get child fragment manager ( ) . begin transaction ( ) . replace ( r . id . scale_frame @$ new fragment ( ) ) . commit ( ) ; grid view . remove on scroll listener ( m wait scroll finish and commit main fragment ) ; grid view . add on scroll listener ( m wait scroll finish and commit main fragment ) ; } else { commit main fragment ( ) ; },user scrolling
client advertises these <PLACE_HOLDER> .,return protocols ;,client advertises
the input method manager only throws security <PLACE_HOLDER> @$ so let 's log all others .,if ( ! ( e instanceof security exception ) ) { slog . wtf ( tag @$ __str__ @$ e ) ; } throw e ;,manager throws
axes draw vertical <PLACE_HOLDER>,int x = left margin - __num__ ; int y = top margin ; font metrics fm = g . get font metrics ( ) ; g . draw line ( x @$ y @$ x @$ y + h ) ; int n = __num__ ; if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ),axes draw
all stack manipulation @$ comparison @$ conversion and arithmetic operators except for idiv and irem ca n't throw <PLACE_HOLDER> so the do n't need to connect exception edges . monitorexit ca n't throw <PLACE_HOLDER> in the context of compiled code because of the structured locking requirement in the parser .,break ; case wide : case breakpoint : default : throw new graal error ( __str__ ) ;,monitorexit throw
alias can be used in js types . types have node type <PLACE_HOLDER> and not name so we have to use their name as <PLACE_HOLDER> .,string node name = n . is string ( ) || n . is import star ( ) ? n . get string ( ) : preprocessor symbol table . get qualified name ( n ) ;,types have
the <PLACE_HOLDER> was not sent using async send @$ so we should only ack the local broker when we get confirmation that the remote broker has received the <PLACE_HOLDER> .,response callback callback = new response callback ( ) { public void on completion ( future response future ) { try { response response = future . get result ( ) ; if ( response . is exception ( ) ) { exception response er = ( exception response ) response ; service local exception ( er . get exception ( ) ) ; } else { dequeue counter . increment and get ( ) ; local broker . oneway ( new message ack ( md @$ message ack . standard_ack_type @$ __num__ ) ) ; } } catch ( io exception e ) { service local exception ( e ) ; } } } ; remote broker . async request ( message @$ callback ) ;,broker received
starting the job master should have read the <PLACE_HOLDER>,try { final completed checkpoint savepoint checkpoint = completed checkpoint store . get latest checkpoint ( false ) ; assert that ( savepoint checkpoint @$ matchers . not null value ( ) ) ; assert that ( savepoint checkpoint . get checkpointid ( ) @$ is ( savepoint id ) ) ; } finally { rpc utils . terminate rpc endpoint ( job master @$ testing timeout ) ; },master read
note param.always on top will be null if the user has n't specified a <PLACE_HOLDER> yet,get check box always on top ( ) . set selected ( param . get always on top ( ) != boolean . false ) ; get check box in scope only ( ) . set selected ( param . is in scope only ( ) ) ; get button mode ( ) . set selected index ( param . get button mode ( ) - __num__ ) ;,user specified
if we have a callback to call @$ schedule it on a different thread . who knows <PLACE_HOLDER> this thread is doing .,if ( callback != null ) { queue buffer . executor . submit ( new callable < void > ( ) { public void call ( ) throws exception { callback . on success ( result ) ; return null ; } } ) ; },thread doing
notify the target capture <PLACE_HOLDER> .,notify ( event @$ true ) ; if ( event . is stopped ( ) ) return event . is cancelled ( ) ;,target capture
no @$ we do n't have a code <PLACE_HOLDER>,if ( code set == __num__ ) { switch ( new code set ) { case code_code_a : pattern index = code_start_a ; break ; case code_code_b : pattern index = code_start_b ; break ; default : pattern index = code_start_c ; break ; } } else { pattern index = new code set ; },no have
the work spec had <PLACE_HOLDER> . once the execution of the worker is complete @$ we might need to disable constraint proxies which were previously enabled for this work spec . hence @$ trigger a <PLACE_HOLDER> changed command .,if ( m has constraints ) { intent intent = command handler . create constraints changed intent ( m context ) ; m dispatcher . post on main thread ( new system alarm dispatcher . add runnable ( m dispatcher @$ intent @$ m start id ) ) ; },spec had
did some instances lose their <PLACE_HOLDER> ?,if ( last known instance configs . size ( ) != current relevant instance configs . size ( ) ) { logger . info ( __str__ @$ table name @$ last known instance configs . size ( ) @$ current relevant instance configs . size ( ) ) ; return true ; },instances lose
training causes all <PLACE_HOLDER> to observe data,for ( vector . element e : weights . non zeroes ( ) ) { int index = e . index ( ) ; classifier . train ( index @$ vector @$ weights . get ( index ) ) ; },training causes
if there is no container acl but the user can still access the container @$ the only possibility is that the user has the admin role . in this case @$ the user should have 0700 <PLACE_HOLDER> to the swift container .,if ( mode == __num__ && m access . get token ( ) != null ) { mode = ( short ) __num__ ; } m account mode = mode ;,user have
this ensures that incremental compilation only touches the <PLACE_HOLDER> that 's been swapped out .,typed scope scope = t . get typed scope ( ) ; if ( ! scope . is block scope ( ) && ! scope . is module scope ( ) ) { infer scope ( t . get current node ( ) @$ scope ) ; },compilation touches
now set the <PLACE_HOLDER> in the row !,for ( int i = __num__ ; i < row meta . size ( ) ; i ++ ) { value meta interface v = row meta . get value meta ( i ) ; object object = data [ i ] ; try { set value ( ps @$ v @$ object @$ i + __num__ ) ; } catch ( kettle database exception e ) { throw new kettle database exception ( __str__ + row meta @$ e ) ; } },now set
skip if any aggregation contains a <PLACE_HOLDER> by,if ( node . has orderings ( ) ) { return context . default rewrite ( node @$ optional . empty ( ) ) ; },aggregation contains
connectivity service should have changed the <PLACE_HOLDER> on lan supported to true,wifi lp . set wake on lan supported ( true ) ; assert equals ( wifi lp @$ m service . get active link properties ( ) ) ;,service changed
response get gateway <PLACE_HOLDER>,if ( cluster member != null ) { cluster . gateway receiver gateway receiver = cluster member . get gateway receiver ( ) ; boolean is gateway = false ; if ( gateway receiver != null ) { responsejson . put ( __str__ @$ true ) ; responsejson . put ( __str__ @$ gateway receiver . get listening port ( ) ) ; responsejson . put ( __str__ @$ gateway receiver . get link throughput ( ) ) ; responsejson . put ( __str__ @$ gateway receiver . get avg batch processing time ( ) ) ; } else { responsejson . put ( __str__ @$ false ) ; } cluster . gateway sender [ ] gateway senders = cluster member . get member gateway senders ( ),response get
the week crosses a year <PLACE_HOLDER> .,time temp = new time ( this ) ; temp . month day += s thursday offset [ week day ] ; temp . normalize ( true ) ;,week crosses
process schema included <PLACE_HOLDER> first @$ so that unnamed classes will belong to the defining class instead of the current class,final list < named data schema > includes = schema . get include ( ) ; for ( named data schema included schema : includes ) { process schema ( included schema @$ null @$ null ) ; } final map < custom info spec @$ object > custom info map = new identity hash map < custom info spec @$ object > ( schema . get fields ( ) . size ( ) * __num__ ) ; for ( record data schema . field field : schema . get fields ( ) ) { final class template spec field class = process schema ( field . get type ( ) @$ record class @$ field . get name ( ) ) ; final record template spec .,schema included
note that test xml ca n't check if they are same because enabling dynamic <PLACE_HOLDER>s causes a meta <PLACE_HOLDER> to be produced .,test xml ( cache @$ false ) ; assert equals ( true @$ dynamic region factory . get ( ) . is open ( ) ) ; assert equals ( f . get absolute file ( ) @$ dynamic region factory . get ( ) . get config ( ) . get disk dir ( ) ) ; region dr = get cache ( ) . get region ( __str__ ) ; if ( dr != null ) { dr . local destroy region ( ) ; },note causes
` promise & & whatever ` never returns promise <PLACE_HOLDER> @$ so it is safe . ` whatever & & promise ` may return promise @$ so return outer context .,return first ? link ( parent @$ true ) : this ; case or :,returns promise
check can not post <PLACE_HOLDER> that already exist .,response = client . post ( namespace path3 @$ null headers @$ new byte [ ] { } ) ; assert equals ( __num__ @$ response . get code ( ) ) ; response = client . post ( namespace path4 @$ constants . mimetype_protobuf @$ model4 . create protobuf output ( ) ) ; assert equals ( __num__ @$ response . get code ( ) ) ;,check post
existing write <PLACE_HOLDER> against the caller 's val<PLACE_HOLDER> list .,if ( tbl == null ) { return null ; } m database mdb = null ; string cat name = tbl . is set cat name ( ) ? tbl . get cat name ( ) : get default catalog ( conf ) ; try { mdb = getm database ( cat name @$ tbl . get db name ( ) ) ; } catch ( no such object exception e ) { log . error ( __str__ @$ e ) ; throw new invalid object exception ( __str__ + database name . get qualified ( cat name @$ tbl . get db name ( ) ) + __str__ ) ; },existing write
transformation must keep the <PLACE_HOLDER>,assert equals ( a @$ and1 . get children ( ) . get ( __num__ ) ) ; assert equals ( c @$ and1 . get children ( ) . get ( __num__ ) ) ; final group element and2 = ( group element ) parent . get children ( ) . get ( __num__ ) ; assert equals ( b @$ and2 . get children ( ) . get ( __num__ ) ) ; assert equals ( c @$ and2 . get children ( ) . get ( __num__ ) ) ;,transformation keep
do n't send an error response here @$ unlike the base authentication filter implementation . this request did not use <PLACE_HOLDER> auth . instead @$ we will send an error response in pre response authorization check filter to allow other authenticator implementations to check the request .,if ( authentication ex == null ) { filter chain . do filter ( request @$ response ) ; } else { http response . send error ( err code @$ authentication ex . get message ( ) ) ; },request use
change <PLACE_HOLDER>s and check that the selected date is not lost and that the calendar has the correct <PLACE_HOLDER> .,click ( resolution hour ) ; check header and body ( date time resolution . hour @$ false ) ; click ( resolution year ) ; check header and body ( date time resolution . year @$ false ) ; click ( resolution minute ) ; check header and body ( date time resolution . minute @$ false ) ;,calendar has
print outs <PLACE_HOLDER> that are set using the entry processor above .,print cache entries ( cache ) ;,print outs
verify exit status matches exit <PLACE_HOLDER> of script,assert . assert equals ( exit code @$ container status . get exit status ( ) ) ;,status matches
no apps can bypass <PLACE_HOLDER>,assert equals ( __num__ @$ m helper . get apps bypassing dnd count ( user ) ) ;,apps bypass
app has a visible <PLACE_HOLDER> ; only upgrade adjustment .,if ( adj > process list . visible_app_adj ) { adj = process list . visible_app_adj ; app . adj type = __str__ ; if ( debug_oom_adj_reason || log uid == app uid ) { report oom adj message locked ( tag_oom_adj @$ __str__ + app ) ; } } if ( proc state > process state cur top ) { proc state = process state cur top ; app . adj type = __str__ ; if ( debug_oom_adj_reason || log uid == app uid ) { report oom adj message locked ( tag_oom_adj @$ __str__ + app ) ; } } if ( sched group < process list . sched_group_default ) { sched group = process list . sched_group_default ; } app . cached = false ;,app has
this method might modify the file on disk . use segment <PLACE_HOLDER> to prevent race condition,lock segment lock = segment locks . get segment lock ( table name with type @$ segment name ) ; try { segment lock . lock ( ) ; final file segment dir = new file ( _fetcher and loader . get segment local directory ( table name with type @$ segment name ) ) ; if ( segment dir . exists ( ) ) { file utils . delete quietly ( segment dir ) ; _logger . info ( __str__ @$ segment dir ) ; } } catch ( final exception e ) { _logger . error ( __str__ + segment name + __str__ + e . get message ( ) @$ e ) ; utils . rethrow exception ( e ) ; } finally { segment,method modify
verify that the archive rule has the correct <PLACE_HOLDER> : the object files from our sources .,rule . get native linkable input ( cxx platform @$ linker . linkable dep type . static @$ graph builder @$ unconfigured target configuration . instance ) ; build rule static rule = graph builder . get rule ( cxx description enhancer . create static library build target ( target @$ cxx platform . get flavor ( ) @$ pic type . pdc ) ) ; assert not null ( static rule ) ; assert equals ( immutable set . of ( cxx source rule factorypdc . create compile build target ( __str__ ) @$ cxx source rule factorypdc . create compile build target ( gen source name ) ) @$ static rule . get build deps ( ) . stream ( ) . map ( build rule,rule has
node 1 still has node 2 in readers <PLACE_HOLDER> .,assert true ( e1 . readers ( ) . contains ( n2 . id ( ) ) ) ; assert not null ( cache1 . get and put ( __num__ @$ __str__ ) ) ; final grid dht cache entry e1f = e1 ; grid test utils . wait for condition ( new grid abs predicate ( ) { @ override public boolean apply ( ) { try { return ! e1f . readers ( ) . contains ( n2 . id ( ) ) ; } catch ( grid cache entry removed exception ignored ) { return true ; } catch ( exception e ) { throw new runtime exception ( e ) ; } } } @$ __num__ ) ;,node has
update the key.current symlink . first create tmp symlink and do <PLACE_HOLDER> of tmp to current so that the operation is atomic .,path tmp symlink = tmp symlink location ( ) ; path target of symlink = construct blob with version file name ( base dir @$ get key ( ) @$ version ) ; log . debug ( __str__ @$ tmp symlink @$ target of symlink ) ; files . create symbolic link ( tmp symlink @$ target of symlink ) ; path current sym link = get current symlink path ( ) ; files . move ( tmp symlink @$ current sym link @$ atomic_move ) ;,tmp symlink
note when restored from backup @$ it 's no longer dynamic @$ so should n't have a <PLACE_HOLDER> .,si = m service . get package shortcut for test ( calling_package_1 @$ __str__ @$ user_0 ) ; assert equals ( __num__ @$ si . get rank ( ) ) ;,note have
swap in our <PLACE_HOLDER> if the filtered fs is using a different <PLACE_HOLDER>,if ( swap scheme != null ) { try { fq path = new path ( new uri ( swap scheme @$ fq path . to uri ( ) . get scheme specific part ( ) @$ null ) ) ; } catch ( uri syntax exception e ) { throw new illegal argument exception ( e ) ; } },fs using
while still holding the lock cancel the <PLACE_HOLDER> by transitioning . this simulates a race where the callback goes to cancel the <PLACE_HOLDER> while the <PLACE_HOLDER> is trying to run .,ees . become active ( ) ;,lock cancel
otherwise @$ adjust enabled <PLACE_HOLDER> .,cancel caption animator ( ) ; if ( enabled ) { helper text view = new app compat text view ( context ) ; helper text view . set id ( r . id . textinput_helper_text ) ; if ( version . sdk_int >= __num__ ) { helper text view . set text alignment ( view . text_alignment_view_start ) ; } if ( typeface != null ) { helper text view . set typeface ( typeface ) ; } helper text view . set visibility ( view . invisible ) ; view compat . set accessibility live region ( helper text view @$ view compat . accessibility_live_region_polite ) ; set helper text appearance ( helper text text appearance ) ; set helper text view text color ( helper,adjust enabled
it was <PLACE_HOLDER> @$ try again . the assumption is that it must be a word start if the last one had <PLACE_HOLDER> following it .,word position = words . next ( ) ; if ( word position != break iterator . done ) { offs = line start + word position - seg . offset ; if ( offs != line end ) { return offs ; } } segment cache . release shared segment ( seg ) ; return break iterator . done ;,one had
check that method under test throws <PLACE_HOLDER>,try { epki . get key spec ( ( key ) null @$ __str__ ) ; fail ( get name ( ) + __str__ ) ; } catch ( null pointer exception ok ) { },method throws
the id lists must have the same <PLACE_HOLDER>,this . operatori ds . add ( operatorid . from job vertexid ( this . id ) ) ; this . operator ids alternatives . add ( null ) ;,lists have
both use the same client <PLACE_HOLDER> @$ so there 's no lock conflict . not necessarily <PLACE_HOLDER>eal @$ but how the system currently works .,utils . io result ( future reader1 ) ; utils . io result ( future reader2 ) ; dlm0 . close ( ) ; dlm1 . close ( ) ;,both use
instance does not require all other <PLACE_HOLDER> to specify it as well .,test types ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ strict_inexistent_property ) ;,instance require
the user 1 queue should inherit the <PLACE_HOLDER> from the root queue,fs leaf queue user queue = scheduler . get queue manager ( ) . get leaf queue ( __str__ @$ true ) ; assert equals ( __num__ @$ user queue . get num runnable apps ( ) ) ; assert equals ( __num__ @$ user queue . get min share preemption timeout ( ) ) ; assert equals ( __num__ @$ user queue . get fair share preemption timeout ( ) ) ; assert equals ( __num__ @$ user queue . get fair share preemption threshold ( ) @$ __num__ ) ;,queue inherit
make sure the thread saw the correct <PLACE_HOLDER>,assert true ( thread success . get ( ) ) ;,thread saw
scaling preserves aspect <PLACE_HOLDER>,float scale ratio = scaled . width * __num__ / size . width ;,scaling preserves
the 2 i indexes have to be stored in the appositely created strong consistent <PLACE_HOLDER> bucket type : this however has to be done only if the user actually created it ! so @$ if the latter does n't exist @$ then the scan transactions will not be performed at all .,if ( strong consistency && ! strong consistent scans bucket type . is empty ( ) ) { bucket type2i = strong consistent scans bucket type ; perform strong consistent scans = true ; } else { bucket type2i = bucket type ; perform strong consistent scans = false ; },user created
set character offset <PLACE_HOLDER> for this section,curr section core map . set ( core annotations . character offset begin annotation . class @$ section start token . get ( core annotations . character offset begin annotation . class ) ) ;,character offset
time of creation we use a do <PLACE_HOLDER> with the access control context that was in place when this was created .,if ( acc == null && system . get security manager ( ) != null ) { throw new security exception ( __str__ ) ; } return access controller . do privileged ( new privileged action < object > ( ) { public object run ( ) { try { class < ? > c ; object cl ; if ( table == null || ! ( ( cl = table . get ( __str__ ) ) instanceof class loader ) ) { cl = thread . current thread ( ) . get context class loader ( ) ; if ( cl == null ) { cl = class loader . get system class loader ( ) ; } } reflect util . check package access ( class,a do
skeleton throws unmarshal <PLACE_HOLDER> if it does not recognize the operation number ; this is consistent with the case of an unrecognized method hash .,p . pln ( __str__ + id unmarshal exception + __str__ ) ; p . p oln ( __str__ ) ;,skeleton throws
clearing focus forces the <PLACE_HOLDER> to commit any pending changes @$ e.g . typed text in a number picker .,m time picker . clear focus ( ) ; dismiss ( ) ;,focus forces
full recurse into class hierarchy @$ should engage all available <PLACE_HOLDER> @$ so all internal maps and lists are transformed too,return immutable list . of ( pojoizer . convert to pojo ( rclass . prop1 ) ) ;,recurse engage
let 's create full <PLACE_HOLDER> then,array list < object > result = new array list < object > ( total size ) ; buffer . complete and clear buffer ( values @$ ptr @$ result ) ; return result ;,"s" create
if we are currently performing a binary search on the input @$ do n't forward the results currently this <PLACE_HOLDER> is set when a query is optimized using a compact index . the map reduce job responsible for scanning and filtering the index sets this <PLACE_HOLDER> . it remains set throughout the binary search executed by the hive binary search record resder until a,if ( io context . is binary searching ( ) ) { return ; } boolean ret = ( boolean ) condition inspector . get primitive java object ( condition ) ; if ( boolean . true . equals ( ret ) ) { forward ( row @$ row inspector ) ; },job sets
if <PLACE_HOLDER> does not exists defined for this region then create a <PLACE_HOLDER>,if ( null == member ) { member = new cluster . member ( ) ; member . set name ( member name ) ; cluster . get membersh map ( ) . put ( member name @$ member ) ; },region create
client <PLACE_HOLDER> required exception is caught by o auth 2 <PLACE_HOLDER> request redirect web filter which initiates <PLACE_HOLDER>,if ( authorization grant type . authorization_code . equals ( context . get client registration ( ) . get authorization grant type ( ) ) && context . get authorized client ( ) == null ) { return mono . error ( ( ) -> new client authorization required exception ( context . get client registration ( ) . get registration id ( ) ) ) ; },which initiates
foo impl was implicitly bound @$ it is an error to call get instance or get <PLACE_HOLDER> @$ it is ok to call get binding for introspection @$ but an error to get the <PLACE_HOLDER> of the binding,ensure fails ( injector @$ allow_binding @$ foo impl . class ) ;,error get
check whether the broadcast inputs use the same plan <PLACE_HOLDER> at the branching point,for ( int i = __num__ ; i < broadcast channels combination . size ( ) ; i ++ ) { named channel nc = broadcast channels combination . get ( i ) ; plan node bc source = nc . get source ( ) ; if ( ! are branch compatible ( bc source @$ input source ) ) { valid combination = false ; break ; } for ( int k = __num__ ; k < i ; k ++ ) { plan node other bc source = broadcast channels combination . get ( k ) . get source ( ) ; if ( ! are branch compatible ( bc source @$ other bc source ) ) { valid combination = false ; break ; },inputs use
negative ty value means an upward <PLACE_HOLDER> so subtracting ty means expanding the panel .,set clamped panel height ( m initial panel height - ty ) ; request update ( ) ;,ty means
the use of 'flat map ' at notification sender does not preserve actual <PLACE_HOLDER> order . use typed qname map to assert regardless of ordering .,final map < string @$ referenceable > entities = message . get entities ( ) . stream ( ) . collect ( collectors . to map ( ref -> atlas utils . to typed qualified name ( ref . get type name ( ) @$ ( string ) ref . get ( attr_qualified_name ) ) @$ ref -> ref ) ) ; boolean has flow path seen = false ; for ( int i = __num__ ; i < expects . length ; i ++ ) { final referenceable expect = expects [ i ] ; final string type name = expect . get type name ( ) ; final referenceable actual = entities . get ( atlas utils . to typed qualified name ( type name @$,use preserve
user trigger <PLACE_HOLDER> to tr<PLACE_HOLDER>nsfer token to b,byte string asset account dev = public methed . query account ( dev001 address @$ blocking stub full ) . get asset issuedid ( ) ; byte string fake token id = byte string . copy from utf8 ( long . to string ( long . value of ( asset account dev . to string utf8 ( ) ) + __num__ ) ) ; string param = __str__ + fake token id . to string utf8 ( ) + __str__ ; final string trigger txid = public methed . trigger contract ( transfer token contract address @$ __str__ @$ param @$ false @$ __num__ @$ __num__ @$ __str__ @$ __num__ @$ dev001 address @$ dev001 key @$ blocking stub full ) ; public methed . wait produce next,user trigger
we load the url from the tab rather than directly from the content view so the tab has a <PLACE_HOLDER> of using a prerenderer page is any .,int load type = native load url ( m native tab android @$ params . get url ( ) @$ params . get verbatim headers ( ) @$ params . get post data ( ) @$ params . get transition type ( ) @$ params . get referrer ( ) != null ? params . get referrer ( ) . get url ( ) : null @$ params . get referrer ( ) != null ? params . get referrer ( ) . get policy ( ) : __num__ @$ params . get is renderer initiated ( ) @$ params . get should replace current entry ( ) @$ params . get intent received timestamp ( ) @$ params . get has user gesture ( ) ) ;,tab has
synchronization provided by controller <PLACE_HOLDER> to make sure that only one thread updates the <PLACE_HOLDER>,_controller metrics . set value of table gauge ( table name with type @$ controller gauge . number_of_replicas @$ n replicas external ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . percent_of_replicas @$ ( n replicas ideal max > __num__ ) ? ( n replicas external * __num__ / n replicas ideal max ) : __num__ ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . segments_in_error_state @$ n errors ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . percent_segments_available @$ ( n segments > __num__ ) ? ( __num__ - ( n offline * __num__ / n segments,thread updates
this is solely for testing . it checks if the test has set the looped <PLACE_HOLDER> to false @$ and if so remembers that and then sets it to true at the end . we have to check here first to make sure we go through a complete iteration of the loop before resetting it .,do { boolean set looped = ! looped . get ( ) ; txn store . mutexapi . lock handle handle = null ; long started at = - __num__ ; try { handle = txn handler . get mutexapi ( ) . acquire lock ( txn store . mutex_key . cleaner . name ( ) ) ; started at = system . current time millis ( ) ; long min open txn id = txn handler . find min open txn id ( ) ; for ( compaction info compaction info : txn handler . find ready to clean ( ) ) { clean ( compaction info @$ min open txn id ) ; } } catch ( throwable t ) { log . error ( __str__,test set
step 3 : lookup mounted file <PLACE_HOLDER>,byte [ ] dir = path . as byte array ( ) ; for ( unix mount entry entry : fs . get mount entries ( ) ) { if ( arrays . equals ( dir @$ entry . dir ( ) ) ) return entry ; } throw new io exception ( __str__ ) ;,mounted file
unrecognized attributes do not cause an <PLACE_HOLDER>,return ;,attributes cause
if the user did n't specify a ser <PLACE_HOLDER> @$ we use the <PLACE_HOLDER>fault .,string ser de class name ; if ( get ser name ( ) == null ) { if ( storage handler == null ) { ser de class name = plan utils . get default ser de ( ) . get name ( ) ; log . info ( __str__ + ser de class name + __str__ + table name ) ; } else { ser de class name = storage handler . get ser de class ( ) . get name ( ) ; log . info ( __str__ + ser de class name + __str__ + table name ) ; } } else { ser de class name = get ser name ( ) ; ddl utils . validate ser de ( ser de class name,user specify
system arraycopy does the boundary checks anyways @$ no <PLACE_HOLDER> to check extra,system . arraycopy ( this . memory @$ index @$ dst @$ offset @$ length ) ;,checks anyways
open jdk 8 does not support iv parameter <PLACE_HOLDER> for gcm .,system . out . println ( __str__ + ex . to string ( ) ) ; return ;,jdk support
these <PLACE_HOLDER> are usually api methods that are templated in such a way as the <PLACE_HOLDER> become too large @$ really to much so to even read . not sure of the best way to trim these <PLACE_HOLDER> without losing the type specificity provided by the template arguments . for now @$ just trim the name to some length that still leaves us with,string builder buffy = new string builder ( ) ; buffy . append ( name . substring ( __num__ @$ symbol utilities . max_symbol_name_length / __num__ ) ) ; buffy . append ( __str__ ) ; buffy . append ( name . substring ( length - __num__ ) ) ;,read trim
evaluate batch 1 so that temporary arrays in the expression have residual <PLACE_HOLDER> to interfere in later computation,or expr . evaluate ( batch1 ) ;,arrays have
the timestamp given by golden gate does not have nanoseconds <PLACE_HOLDER> needed by oracle timestamp,string corrected timestamp = time stamp . append ( __str__ ) . to string ( ) ;,timestamp have
we check only permission because igfs client adds user<PLACE_HOLDER> and group <PLACE_HOLDER> explicitly .,assert equals ( props . get ( igfs utils . prop_permission ) @$ igfs . info ( dir ) . properties ( ) . get ( igfs utils . prop_permission ) ) ;,client adds
set the values which have no <PLACE_HOLDER> and compare,set annotation member value ( impl1 @$ __str__ @$ annotation . boolean1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . byte1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . short1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . char1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . int1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . long1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . float1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . double1 ( ) ) ;,which have
create an asynchronous request that will occur in the background @$ because this request may result in stopping components @$ which can take an indeterminate <PLACE_HOLDER> of time .,final string request id = uuid . randomuuid ( ) . to string ( ) ; final asynchronous web request < version control information entity @$ version control information entity > request = new standard asynchronous web request < > ( request id @$ request entity @$ group id @$ user @$ get update steps ( ) ) ;,which take
the test is complete when the client sends the other object <PLACE_HOLDER> .,if ( object == server test object . get other object ( ) ) stop end points ( ) ;,client sends
finally @$ we change the lists again so they are once more different @$ and ensure the set contains <PLACE_HOLDER> .,list1 . remove ( __str__ ) ; list2 . remove ( __str__ ) ; set < list < string > > set = injector . get instance ( key . get ( set of list of strings ) ) ; assert equals ( immutable set . of ( immutable list . of ( __str__ ) @$ immutable list . of ( __str__ ) ) @$ set ) ;,set contains
path stack has <PLACE_HOLDER> of stack at front,if ( path stack . is empty ( ) ) throw new config exception . bug or broken ( __str__ ) ; else return new path ( path stack . descending iterator ( ) ) ;,stack has
foo has the <PLACE_HOLDER> and length of ` super `,node super dotg replacement = prototype . get first child ( ) ; assert node ( super dotg replacement ) . matches qualified name ( __str__ ) . has equal source info to ( source super ) ;,foo has
multiply by 2 because each has a <PLACE_HOLDER> @$ and map.entry,int expected size = ( map results . size ( ) + duplicates ) * __num__ ; assert equals ( __str__ + joiner . on ( __str__ ) . join ( other matches ) @$ expected size @$ other matches size ) ; assert true ( entry set match ) ; assert true ( entry set javax match ) ; assert true ( map provider match ) ; assert true ( map javax provider match ) ; assert true ( collection of providers of entry of provider match ) ; assert true ( collection of javax providers of entry of provider match ) ; assert equals ( allow duplicates @$ map set match ) ; assert equals ( allow duplicates @$ map set provider match ) ; assert,each has
the above method will perform the <PLACE_HOLDER> as long as the user does not cancel the request,if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . open_policy_file ) == __num__ ) { tool dialog td = new tool dialog ( policy tool . get message ( __str__ ) @$ tool @$ tw @$ true ) ; td . display user save ( tool dialog . open ) ; } else if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . save_policy_file ) == __num__ ) { string filename = ( ( j text field ) tw . get component ( tool window . mw_filename_textfield ) ) . get text ( ) ; if ( filename == null || filename . length ( ) ==,method perform
create a native linkable dep and have it list the fake build <PLACE_HOLDER> above as a link time dependency .,native linkable input native linkable input = native linkable input . of ( immutable list . of ( source path arg . of ( fake build rule . get source path to output ( ) ) ) @$ immutable set . of ( ) @$ immutable set . of ( ) ) ; fake native linkable group native linkable = create native linkable ( __str__ @$ native linkable input @$ native linkable input ) ;,fake build
check that ot ns ordering is not breaking <PLACE_HOLDER>,ksession = serialization helper . get serialised stateful knowledge session ( ksession @$ true ) ; ksession . fire all rules ( ) ;,ns breaking
no requirement to pull an <PLACE_HOLDER> from the distributed cache anymore .,just elected primary node = false ;,requirement pull
connection acquisition takes more than 0 <PLACE_HOLDER> in a real system,stub data source . set connection acquistion time ( connection acquisition time ms ) ; final executor service thread pool = new fixed thread pool ( thread count ) ; final count down latch all threads done = new count down latch ( iterations ) ; for ( int i = __num__ ; i < iterations ; i ++ ) { thread pool . submit ( ( ) -> { if ( ref . get ( ) == null ) { quietly sleep ( rest time ms ) ; try ( connection c2 = ds . get connection ( ) ) { quietly sleep ( work time ms ) ; } catch ( exception e ) { ref . set ( e ) ; } } all threads,acquisition takes
method does not throw an <PLACE_HOLDER> .,verify read xml returns expected signatures ( __str__ @$ package parser . signing details . signature scheme version . unknown ) ;,method throw
otherwise @$ the user specified specific test <PLACE_HOLDER> to build and run @$ so build a graph around these .,log . debug ( __str__ @$ get arguments ( ) ) ; target graph creation result = params . get parser ( ) . build target graph without top level configuration targets ( parsing context @$ parse arguments as target node specs ( params . get cell ( ) @$ params . get client working dir ( ) @$ get arguments ( ) @$ params . get buck config ( ) ) @$ params . get target configuration ( ) ) ; explicit build targets = target graph creation result . get build targets ( ) ; log . debug ( __str__ @$ explicit build targets ) ; immutable set . builder < build target > test targets builder = immutable set . builder ( ) ; for,user specified
this all could probably be done more elegantly via a group extracted from a more comprehensive regexp . clean up any extra <PLACE_HOLDER> around the remainder of the line @$ which should be a view name .,return statement . substring ( matcher . end ( ) ) . trim ( ) ;,elegantly clean
both columns have same data <PLACE_HOLDER> @$ return any one of them,return expected data type ;,columns have
user has specified a <PLACE_HOLDER> @$ we can continue ...,try { file object file object = kettlevfs . get file object ( vfs filename @$ this ) ; if ( ! ( file object instanceof local file ) ) { throw new kettle exception ( __str__ + vfs filename + __str__ ) ; } string real filename = kettlevfs . get filename ( file object ) ; file file = new file ( real filename ) ; if ( ( file . exists ( ) && file . can read ( ) ) || is local infile ( ) == false ) { if ( log . is detailed ( ) ) { log detailed ( __str__ + real filename + __str__ ) ; } if ( connection != null ) { database db = new database,user specified
the first vertex has a duplicate <PLACE_HOLDER> from a vertex in the graph and should not be added to the new graph,vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; graph = graph . add vertices ( vertices ) ; data set < vertex < long @$ long > > data = graph . get vertices ( ) ; list < vertex < long @$ long > > result = data . collect ( ) ; expected result = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; compare result as tuples ( result @$ expected result ) ;,vertex has
find out if the class which contains this field <PLACE_HOLDER> has access to the class which declares the public or protected field .,if ( source class == ctx class ) { class definition declarer = field . get class definition ( ) ; if ( declarer . is package private ( ) && ! declarer . get name ( ) . get qualifier ( ) . equals ( source class . get name ( ) . get qualifier ( ) ) ) { field = member definition . make proxy member ( field @$ clazz @$ env ) ; } },which contains
if the path does <PLACE_HOLDER> @$ then pass on to the subclass implementation for specific checks :,if ( path matches ( path pattern @$ requesturi ) ) { if ( log . is trace enabled ( ) ) { log . trace ( __str__ + path pattern + __str__ + encode . for html ( requesturi ) + __str__ + __str__ ) ; } return filter chain manager . proxy ( original chain @$ path pattern ) ; },path does
generate dates for selecting <PLACE_HOLDER>s by date @$ making sure the period will not contain the sample <PLACE_HOLDER>,string from date = sample_timestamp . minus seconds ( __num__ * seconds_per_day ) . to string ( ) . substring ( __num__ @$ __num__ ) ; string to date = sample_timestamp . minus seconds ( seconds_per_day ) . to string ( ) . substring ( __num__ @$ __num__ ) ;,period contain
check if the node contains the <PLACE_HOLDER> passed as filter,region function context impl rfc = ( region function context impl ) context ; partitioned region pr = ( partitioned region ) rfc . get data set ( ) ; int [ ] bucketi ds = rfc . get local bucket array ( pr ) ; pr . get gem fire cache ( ) . get logger ( ) . fine ( __str__ + bucketi ds ) ; result sender < integer > rs = context . < integer > get result sender ( ) ; if ( ! pr . get data store ( ) . are all buckets hosted ( bucketi ds ) ) { throw new assertion error ( __str__ + bucketi ds + __str__ ) ; } else { for ( int i =,node contains
base connector config has 13 <PLACE_HOLDER> @$ connector 's configs add 2 @$ and 2 producer overrides,assert equals ( __num__ @$ result . values ( ) . size ( ) ) ; assert true ( result . values ( ) . stream ( ) . any match ( config info -> ack config key . equals ( config info . config value ( ) . name ( ) ) && ! config info . config value ( ) . errors ( ) . is empty ( ) ) ) ; assert true ( result . values ( ) . stream ( ) . any match ( config info -> sasl config key . equals ( config info . config value ( ) . name ( ) ) && config info . config value ( ) . errors ( ) . is empty ( ),config has
check that large slices do something <PLACE_HOLDER>,string tree = __str__ ; check tree ( tree ) ;,slices do
clear the current suggestions as the server response always includes the new ones . exception is when filtering @$ then we need to retain the value if the user does not select <PLACE_HOLDER> of the options matching the filter .,if ( ! widget . waiting for filtering response ) { widget . current suggestion = null ; widget . suggestion popup . menu . clear items ( ) ; popup open and cleared = widget . suggestion popup . is attached ( ) ; },user select
the specified network connection is not available . displays error <PLACE_HOLDER> .,web view my web view = find view by id ( r . id . webview ) ; my web view . load data ( get resources ( ) . get string ( r . string . connection_error ) @$ __str__ @$ null ) ;,network displays
free hash map <PLACE_HOLDER> @$ but not release back to <PLACE_HOLDER> manager,mutable object iterator < tuple2 < binary row @$ binary row > > iterator = sorter . getkv iterator ( ) ; tuple2 < binary row @$ binary row > kv ; while ( ( kv = iterator . next ( ) ) != null ) { binary row key = kv . f0 ; binary row value = kv . f1 ; fallback input . replace ( key @$ value ) ; if ( last key == null ) { last key = key . copy ( ) ; agg sum is null = true ; agg sum = - __num__ ; } else if ( key . get size in bytes ( ) != last key . get size in bytes ( ) || ! (,hash map
if this change in state is going to cause the action bar to be hidden @$ defer the on destroy callback until the animation is finished and associated relayout is about to happen . this lets apps better anticipate visibility and layout <PLACE_HOLDER> .,if ( ! check showing flags ( m hidden by app @$ m hidden by system @$ false ) ) { m deferred destroy action mode = this ; m deferred mode destroy callback = m callback ; } else { m callback . on destroy action mode ( this ) ; },apps anticipate
a match with both pos and word labeled should have both <PLACE_HOLDER> on the same node,string pattern = __str__ ;,match have
dexopts have to match exactly since aspect only creates <PLACE_HOLDER> for listed ones,return normalize dexopts ( iterables . filter ( tokenized dexopts @$ predicates . in ( get android config ( rule context ) . get dexopts supported in incremental dexing ( ) ) ) ) ;,aspect creates
it will show all its tiles . in this case @$ the tiles have to be entered before the container is measured . any change in the tiles @$ should trigger a <PLACE_HOLDER> .,final int num tiles = m records . size ( ) ; final int width = measure spec . get size ( width measure spec ) ; final int available width = width - get padding start ( ) - get padding end ( ) ; final int height mode = measure spec . get mode ( height measure spec ) ; if ( height mode == measure spec . unspecified ) { m rows = ( num tiles + m columns - __num__ ) / m columns ; } m cell width = ( available width - m side padding * __num__ - ( m cell margin horizontal * m columns ) ) / m columns ;,case trigger
scroll means relative float view <PLACE_HOLDER>,do drag float view ( move pos @$ move item @$ false ) ; m prev time = m curr time ;,means float
simple lv hangul simple lvt hangul lvtt @$ last jam<PLACE_HOLDER> expands f<PLACE_HOLDER>r search llvvvtt @$ every jam<PLACE_HOLDER> expands f<PLACE_HOLDER>r search 0 x ac 01 as c<PLACE_HOLDER>nj<PLACE_HOLDER>ining jam<PLACE_HOLDER> 0 x ac 01 as c<PLACE_HOLDER>mpatibility jam<PLACE_HOLDER> 0 x ac 0 f as c<PLACE_HOLDER>nj<PLACE_HOLDER>ining jam<PLACE_HOLDER> ; last expands f<PLACE_HOLDER>r search 0 x afff as c<PLACE_HOLDER>nj<PLACE_HOLDER>ining jam<PLACE_HOLDER> ; all expand f<PLACE_HOLDER>r search small letter ae @$ expands small,string tsce text = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ;,lvtt expands
is true if either the resources has increased or execution <PLACE_HOLDER> updated from opportunistic to guaranteed,boolean is increase = false ; if ( ! current resource . equals ( target resource ) ) { is resource change = true ; is increase = resources . fits in ( current resource @$ target resource ) && ! resources . fits in ( target resource @$ current resource ) ; } else if ( ! current exec type . equals ( target exec type ) ) { is exec type update = true ; is increase = current exec type == execution type . opportunistic && target exec type == execution type . guaranteed ; } if ( is increase ) { org . apache . hadoop . yarn . api . records . container increased container = null ; if ( is resource change,resources has
note that this might miss collisions @$ use internal tick <PLACE_HOLDER> to check for collision on every tick . see internal tick test on how to implement it .,contacts . clear ( ) ;,collision use
do n't send the parsed db name @$ as this method will parse <PLACE_HOLDER> .,ret = get_partitions_ps_with_auth ( db_name @$ tbl_name @$ part_vals @$ max_parts @$ null @$ null ) ; ret = filter utils . filter partitions if enabled ( is server filter enabled @$ filter hook @$ ret ) ; end function ( __str__ @$ ret != null @$ ex @$ tbl_name ) ;,method parse
otherwise @$ we return the generic placeholder of this library @$ that dependents can use get the real build <PLACE_HOLDER> via querying the action graph .,prebuilt cxx library paths paths = get paths ( build target @$ args ) ; return new prebuilt cxx library ( build target @$ project filesystem @$ params ) { private final transitive cxx preprocessor input cache transitive cxx preprocessor input cache = new transitive cxx preprocessor input cache ( this ) ; cxx deps all exported deps = cxx deps . builder ( ) . add deps ( args . get exported deps ( ) ) . add platform deps ( args . get exported platform deps ( ) ) . build ( ) ; private boolean has headers ( cxx platform cxx platform ) { if ( ! args . get exported headers ( ) . is empty ( ) ) { return true ; },real build
magnitude of a <PLACE_HOLDER> of two so large that scaling a finite nonzero value by it would be guaranteed to over or underflow ; due to rounding @$ scaling down takes takes an additional <PLACE_HOLDER> of two which is reflected here,final int max_scale = double consts . max_exponent + - double consts . min_exponent + double consts . significand_width + __num__ ; int exp_adjust = __num__ ; int scale_increment = __num__ ; double exp_delta = double . nan ;,magnitude takes
note to translators : xsltc could not find the stylesheet <PLACE_HOLDER> with the name specified by the substitution text .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note find
pure symbol reference : the data contains the symbol 's <PLACE_HOLDER> @$ load it,if ( data info . is symbol reference ( ) ) { register result register = as register ( result ) ; a arch64 address address = a arch64 address . create scaled immediate address ( result register @$ __num__ ) ; masm . adrp ldr ( __num__ @$ result register @$ address ) ; crb . compilation result . record data patch ( before @$ new c global data reference ( data info ) ) ; } else { a arch64 address address = masm . get placeholder ( before ) ; masm . load address ( as register ( result ) @$ address @$ __num__ ) ; crb . compilation result . record data patch ( before @$ new c global data reference ( data info,data contains
previous call to read <PLACE_HOLDER> file has already set the <PLACE_HOLDER> to draw,if ( text to use != file_text ) { set text to draw ( text @$ range @$ user @$ null ) ; } set font params ( name @$ size @$ style @$ transform ) ; set transformg2 ( g2transform ) ;,call set
a special native server to check basic <PLACE_HOLDER>,if ( mode != - __num__ ) { if ( ns ( - __num__ ) . wait for ( ) != __num__ ) { proc . d ( __str__ ) ; mode = - __num__ ; } },server check
cache size is 3 @$ so 4 th access should remove first <PLACE_HOLDER>,assert equals ( __num__ @$ evicted from cache . size ( ) ) ;,access remove
fake stream 1 had 3 <PLACE_HOLDER> before & 1 new shard after restore,stream to shard count . put ( __str__ @$ __num__ + __num__ ) ;,stream had
set default enable <PLACE_HOLDER> to cluster only if it is true .,if ( dflt enable val ) baseline auto adjust enabled . propagate ( dflt enable val ) ;,default enable
if we are in the middle of connecting process do not fire events @$ will do it later when the method connect and login <PLACE_HOLDER> its work,if ( err != null && err . get condition ( ) == stream error . condition . conflict ) { synchronized ( connect and login lock ) { if ( in connect and login ) { event during login = new registration state change event ( protocol provider service jabber impl . this @$ get registration state ( ) @$ registration state . unregistered @$ registration state change event . reason_multiple_logins @$ __str__ ) ; return ; } } fire registration state changed ( get registration state ( ) @$ registration state . unregistered @$ registration state change event . reason_multiple_logins @$ __str__ ) ; disconnect and clean connection ( ) ; return ; },method connect
the handler created a <PLACE_HOLDER> of the <PLACE_HOLDER> and is now done with it .,dup2 . release ( ) ;,handler created
black color @$ show a <PLACE_HOLDER> off :,if ( color . equals ( color . black ) ) { set icon ( image utilities . load image icon ( __str__ @$ false ) ) ; } else { set icon ( new color icon ( ) ) ; },color show
no averagers returned <PLACE_HOLDER> . all buckets must be empty . skip this row .,return null ;,averagers returned
the used <PLACE_HOLDER> should still 4 gb and negative available <PLACE_HOLDER>,wait memory ( scheduler @$ nm id @$ __num__ * gb @$ - __num__ * gb @$ __num__ @$ __num__ * __num__ ) ;,resource and
any exceptions thrown inside an event listener will not stop <PLACE_HOLDER> of the event,timber . w ( e @$ __str__ ) ;,exceptions stop
we need to make sure start is inserted before start delay ended event @$ because otherwise inserting start delay ended events first would change the start event <PLACE_HOLDER> .,if ( need to swap start ) { animation event start event = m events . remove ( start event id ) ; m events . add ( i @$ start event ) ; i ++ ; } animation event start delay end event = m events . remove ( start delay end id ) ; m events . add ( i @$ start delay end event ) ; i += __num__ ; i ++ ;,events change
legacy application listeners only understood one <PLACE_HOLDER> at a time,for ( final file file : files ) { final application event ae = new application event ( toolkit . get default toolkit ( ) @$ file . get absolute path ( ) ) ; send event to each listener until handled ( ae @$ new event dispatcher ( ) { public void dispatch event ( final application listener listener ) { listener . handle open file ( ae ) ; } } ) ; },listeners understood
else : mtp deleted the <PLACE_HOLDER> as part of cleanup . do n't send an event .,break ; default : return false ;,mtp deleted
class.for name and class.get constructor are supposed to never return null though a broken class loader could break the <PLACE_HOLDER> . just in case we introduce this specific catch to avoid polluting the logs with np es .,logger . log ( warning @$ string . format ( __str__ @$ pm class name ) ) ; logger . log ( warning @$ string . format ( __str__ @$ pm class name ) ) ; logger . log ( warning @$ string . format ( __str__ @$ pm class name ) @$ e ) ;,loader break
second cancellation should n't trigger additional <PLACE_HOLDER>,server stream . cancel ( status ) ; do ping pong ( server listener ) ;,cancellation trigger
create a display which only contains 2 <PLACE_HOLDER> .,final activity display display = add new activity display at ( activity display . position_top ) ; final activity stack stack1 = create fullscreen stack with simple activity at ( display ) ; final activity stack stack2 = create fullscreen stack with simple activity at ( display ) ;,which contains
assume that this zone stopped <PLACE_HOLDER>,if ( tz . getn transitions ( ) > __num__ ) { tz . setdst type ( timezone . x_dst ) ; long time = time . get local time ( max year @$ month . january @$ __num__ @$ __num__ ) ; time -= zrec . get gmt offset ( ) ; tz . add transition ( time @$ tz . get offset index ( gmt offset ) @$ tz . get dst offset index ( __num__ ) ) ; used zone = true ; } else { tz . setdst type ( timezone . no_dst ) ; },zone stopped
only apps that target older than o sdk can add window without a token @$ after that we require a token so apps can not add <PLACE_HOLDER> directly as the token is added by the notification system . window manager does the checking for this .,out app op [ __num__ ] = op_toast_window ; return add_okay ; case type_dream : case type_input_method : case type_wallpaper : case type_presentation : case type_private_presentation : case type_voice_interaction : case type_accessibility_overlay : case type_qs_dialog :,apps add
add to all instant messaging operation sets the message listener which handles all received <PLACE_HOLDER> .,im . add message listener ( this ) ;,all received
user changed the <PLACE_HOLDER>,if ( new data != data ) { is object = get context . is object ( ) ; data = new data ; },user changed
have the nn redundancy monitor compute the reconstruction and invalidation commands to send d ns every <PLACE_HOLDER> .,conf . set int ( dfs config keys . dfs_namenode_redundancy_interval_seconds_key @$ __num__ ) ;,d ns
if no identity was found then the user does not have a reserved room <PLACE_HOLDER>,return null ;,user have
serialized version 0 progressive actions did not contain <PLACE_HOLDER> .,boolean is legacy progressive = version == __num__ && download request . type_progressive . equals ( type ) ; list < stream key > keys = new array list < > ( ) ; if ( ! is legacy progressive ) { int key count = input . read int ( ) ; for ( int i = __num__ ; i < key count ; i ++ ) { keys . add ( read key ( type @$ version @$ input ) ) ; } },version contain
update once at end of iteration to reduce heap write <PLACE_HOLDER>,cursor = i ; last ret = i - __num__ ; if ( mod count != expected mod count ) throw new concurrent modification exception ( ) ;,heap write
add key as int here as to have the importer use the token <PLACE_HOLDER>,while ( store property cursor . next ( ) ) { visitor . property ( store property cursor . property key ( ) @$ store property cursor . property value ( ) . as object ( ) ) ; },importer use
do n't include auto remove <PLACE_HOLDER> that are finished or finishing .,if ( tr . auto remove recents && tr . get top activity ( ) == null ) { if ( debug_recents ) { slog . d ( tag_recents @$ __str__ + tr ) ; } continue ; },auto remove
this loop may throw an io <PLACE_HOLDER>,while ( true ) { int bytes read = as . read ( loaded audio @$ loaded audio byte length @$ int len - loaded audio byte length ) ; if ( bytes read <= __num__ ) { as . close ( ) ; break ; } loaded audio byte length += bytes read ; },loop throw
randomly choose the app directory the chance of picking a directory is proportional to the available space on the directory . firstly calculate the <PLACE_HOLDER> of all available space on these directories,for ( string local dir : local dirs ) { path cur base = get application dir ( new path ( local dir ) @$ user @$ app id ) ; long space = __num__ ; try { space = get disk free space ( cur base ) ; } catch ( io exception e ) { log . warn ( __str__ @$ cur base @$ e ) ; } available on disk [ i ++ ] = space ; total available += space ; },chance calculate
for binary set the response op <PLACE_HOLDER>,if ( this . protocol == protocol . binary ) { reply . rewind ( ) ; reply . put ( position_opcode @$ buffer . get ( position_opcode ) ) ; reply . put int ( position_opaque @$ buffer . get int ( position_opaque ) ) ; if ( connection handler . get logger ( ) . finer enabled ( ) ) { connection handler . get logger ( ) . finer ( __str__ + reply + __str__ + command . bufferto string ( reply ) ) ; } } socket channel channel = this . socket . get channel ( ) ; if ( channel == null || ! channel . is open ( ) ) { throw new illegal state exception ( __str__ ) ; },binary set
now reload the reloadable top class and introduce that field i which will shadow the <PLACE_HOLDER> in the supertype,r top . load new version ( __str__ @$ retrieve rename ( top @$ top + __str__ ) ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; run on instance ( c top @$ i top @$ __str__ @$ __num__ ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; is mgr fa = get field accessor ( i top ) ; assert contains ( __str__ @$ fa . to string ( ) ) ; assert does,which shadow
right now use no automatic transition so that the app can supply <PLACE_HOLDER> if desired .,m decor toolbar . set visibility ( view . gone ) ;,app supply
in the absence of sorted by clause @$ the sorted dynamic partition insert should honor the <PLACE_HOLDER> of records provided by order by in select statement,reduce sink operator parentrs op = operator utils . find single operator upstream ( parent @$ reduce sink operator . class ) ; if ( parentrs op != null && parse ctx . get query properties ( ) . has outer order by ( ) ) { string parentrs op order = parentrs op . get conf ( ) . get order ( ) ; string parentrs op null order = parentrs op . get conf ( ) . get null order ( ) ; if ( parentrs op order != null && ! parentrs op order . is empty ( ) && sort positions . is empty ( ) ) { key cols . add all ( parentrs op . get conf ( ) . get key,insert honor
b is maxed out on capacity @$ so this move should fail the <PLACE_HOLDER>,future = wm . apply move session async ( sessiona3 @$ __str__ ) ; assert not null ( future . get ( ) ) ; assert false ( future . get ( ) ) ; wm . add test event ( ) . get ( ) ; while ( sessiona3 . is open ( ) ) { thread . sleep ( __num__ ) ; } assert null ( sessiona3 . get pool name ( ) ) ; assert equals ( __str__ @$ sessiona3 . get reason for kill ( ) ) ; assert equals ( __num__ @$ all session providers . get ( __str__ ) . get sessions ( ) . size ( ) ) ; assert equals ( __num__ @$ all session providers . get ( __str__,move fail
does this observer match the target <PLACE_HOLDER> ?,if ( target user handle == user handle . user_all || entry . user handle == user handle . user_all || target user handle == entry . user handle ) { if ( leaf ) { if ( ( flags & content resolver . notify_skip_notify_for_descendants ) != __num__ && entry . notify for descendants ) { if ( debug ) slog . d ( tag @$ __str__ + entry . observer + __str__ ) ; continue ; } } else { if ( ! entry . notify for descendants ) { if ( debug ) slog . d ( tag @$ __str__ + entry . observer + __str__ ) ; continue ; } } if ( debug ) slog . d ( tag @$ __str__ + entry .,observer match
make data node <PLACE_HOLDER> to be not qualified to choose,update heartbeat with usage ( data nodes [ __num__ ] @$ __num__ * hdfs server constants . min_blocks_for_write * block_size @$ __num__ @$ ( hdfs server constants . min_blocks_for_write - __num__ ) * block_size @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ;,data node
if neither are present then store empty <PLACE_HOLDER> in parameter slot,final constant pool gen cpg = class gen . get constant pool ( ) ; final instruction list il = method gen . get instruction list ( ) ; il . append ( new push ( cpg @$ constants . emptystring ) ) ;,neither store
the second stream will see the first <PLACE_HOLDER>,verify ( picker @$ timeout ( __num__ ) ) . pick subchannel ( args2 ) ;,stream see
check if this string does not contain any uppercase <PLACE_HOLDER> .,if ( lowercased ) { return this ; } final byte [ ] new value = platform dependent . allocate uninitialized array ( length ( ) ) ; for ( i = __num__ @$ j = array offset ( ) ; i < new value . length ; ++ i @$ ++ j ) { new value [ i ] = to lower case ( value [ j ] ) ; } return new ascii string ( new value @$ false ) ;,string contain
if the decl<PLACE_HOLDER>ring cl<PLACE_HOLDER>ss of the field references missing cl<PLACE_HOLDER>sses <PLACE_HOLDER> ` no cl<PLACE_HOLDER>ss def found error ` c<PLACE_HOLDER>n be thrown . we intrinsify ` it here .,method intrinsic = get intrinsic ( analysis @$ hosted @$ b @$ exception synthesizer . throw no class def found error method ) ; if ( intrinsic == null ) { return false ; } throw no class def found error ( b @$ target method @$ e . get message ( ) ) ;,class references
stack allocation needs an allocation <PLACE_HOLDER> that is a compile time constant @$ so we split the byte array up in multiple chunks and write them separately .,final int chunk size = __num__ ; final c char pointer bytes = stack value . get ( chunk size ) ; int chunk offset = offset ; int input length = length ; while ( input length > __num__ ) { int chunk length = math . min ( input length @$ chunk size ) ; for ( int i = __num__ ; i < chunk length ; i ++ ) { int index = chunk offset + i ; byte b ; if ( value instanceof string ) { b = ( byte ) char at ( ( string ) value @$ index ) ; } else if ( value instanceof char [ ] ) { b = ( byte ) ( ( char [ ],allocation needs
skip if this consumer already has all the topic <PLACE_HOLDER> it can get,if ( consumer partition count == all subscriptions . get ( consumer ) . size ( ) ) continue ;,consumer has
<PLACE_HOLDER> migration has updated the <PLACE_HOLDER> @$ send the request to the new <PLACE_HOLDER>,m_mailbox . send ( new leader @$ counter . get open message ( ) ) ;,migration updated
other topics should not throw <PLACE_HOLDER> @$ but they should clear existing <PLACE_HOLDER>,metadata . update ( metadata response @$ time . milliseconds ( ) ) ; metadata . maybe throw exception for topic ( __str__ ) ; metadata . maybe throw any exception ( ) ;,topics throw
update recognition locked can possibly update the <PLACE_HOLDER> of models,array list < model data > model datas = new array list < model data > ( m model data map . values ( ) ) ; for ( model data model data : model datas ) { update recognition locked ( model data @$ is allowed @$ notify ) ; },recognition update
create a dummy select to select all <PLACE_HOLDER>,gen select plan ( pctx @$ map join op ) ; return map join op ;,dummy select
32 mb default keep <PLACE_HOLDER> in memory,kahadb persistence adapter . set index cache size ( __num__ ) ; kahadb persistence adapter . set index write batch size ( __num__ ) ; kahadb persistence adapter . set enable index recovery file ( false ) ; kahadb persistence adapter . set enable index disk syncs ( false ) ; broker . add connector ( __str__ ) ; broker . start ( ) ; string options = __str__ ; connection factory = new activemq connection factory ( broker . get transport connectors ( ) . get ( __num__ ) . get connect uri ( ) + options ) ;,default keep
note that @$ by construction in this method @$ a jsr block has at least two successors in the control flow graph : the first one leads the next <PLACE_HOLDER> after the jsr @$ while the second one leads to the jsr target .,next insn = new label ( ) ;,one leads
note to translators : the stylesheet tried to create an element with a <PLACE_HOLDER> that was not a valid xml <PLACE_HOLDER> . the substitution text contains the <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text contains
if the user mouses around the neutral zone @$ then start the close timer . the timer will be reset if the user enters the <PLACE_HOLDER> .,close timer . start ( ) ;,user enters
hash aggregates and partial aggregates invalidate the <PLACE_HOLDER> ordering . so @$ we will need an orderby node .,if ( number hash aggregates > __num__ ) { return true ; } else if ( number window functions == __num__ ) { if ( index use . get window function uses index ( ) == window function scoreboard . no_index_use ) { return true ; } else { assert ( index use . get window function uses index ( ) == window function scoreboard . statement_level_order_by_index ) ; return number receive nodes > __num__ ; } } else if ( number window functions == __num__ ) { return ! ( index use . get window function uses index ( ) == __num__ && index use . is window function compatible with order by ( ) ) ; } else { return true ; },aggregates invalidate
check if graphics card does n't support depth <PLACE_HOLDER>,if ( img . get format ( ) . is depth format ( ) && ! caps . contains ( caps . depth texture ) ) { throw new renderer exception ( __str__ ) ; } if ( target == gl . gl_texture_cube_map ) { int cube size = limits . get ( limits . cubemap size ) ; if ( img . get width ( ) > cube size || img . get height ( ) > cube size ) { throw new renderer exception ( __str__ + img + __str__ + cube size ) ; } if ( img . get width ( ) != img . get height ( ) ) { throw new renderer exception ( __str__ ) ; } } else { int,card support
call super vector map join outer filtered <PLACE_HOLDER> @$ which calls super map join <PLACE_HOLDER> with new input inspector .,super . initialize op ( hconf ) ; first batch = true ;,map join
pattern has <PLACE_HOLDER> and match,if ( pp < plen && ( s . char at ( ps ) == p . char at ( pp ) || p . char at ( pp ) == __str__ ) ) { ++ pp ; ++ ps ; } else if ( pp < plen && p . char at ( pp ) == __str__ ) { old ps = ps ; old pp = pp ; met star = true ; ++ pp ; } else { if ( met star ) { ++ old ps ; ps = old ps ; pp = old pp ; ++ pp ; } else { return false ; } },pattern has
no actions @$ hide the row @$ clear out the <PLACE_HOLDER>,if ( m actions == null ) { m action row . set visibility ( view . gone ) ; m current view . set slice actions ( null ) ; m current view . set insets ( get padding start ( ) @$ get padding top ( ) @$ get padding end ( ) @$ get padding bottom ( ) ) ; return ; },actions clear
browse source files <PLACE_HOLDER> ...,wbb gpg exe = new button ( wgpg group @$ swt . push | swt . center ) ; props . set look ( wbb gpg exe ) ; wbb gpg exe . set text ( base messages . get string ( pkg @$ __str__ ) ) ; fdbb gpg exe = new form data ( ) ; fdbb gpg exe . right = new form attachment ( __num__ @$ - margin ) ; fdbb gpg exe . top = new form attachment ( w stepname @$ margin ) ; wbb gpg exe . set layout data ( fdbb gpg exe ) ; wbb gpg exe . add selection listener ( new selection adapter ( ) { public void widget selected ( selection event e ) { file,source files
when animation or scrolling removes a focused <PLACE_HOLDER> @$ focus to grid view itself to avoid losing focus .,if ( retain focus for child ) { m private flag |= pflag_retain_focus_for_child ; request focus ( ) ; },animation removes
vector should contain rawname value <PLACE_HOLDER>,int i = __num__ ; while ( i < annotation local attrs . size ( ) ) { string rawname = ( string ) annotation local attrs . element at ( i ++ ) ; int colon index = rawname . index of ( __str__ ) ; string prefix @$ localpart ; if ( colon index == - __num__ ) { prefix = __str__ ; localpart = rawname ; } else { prefix = rawname . substring ( __num__ @$ colon index ) ; localpart = rawname . substring ( colon index + __num__ ) ; } string uri = schema doc . f namespace support . geturi ( f symbol table . add symbol ( prefix ) ) ; local str buffer . append ( rawname ),vector contain
validate that every inherited interface must extend pipeline <PLACE_HOLDER> except for pipeline <PLACE_HOLDER> itself .,validate inherited interfaces extend pipeline options ( iface ) ; @ suppress warnings ( __str__ ) set < class < ? extends pipeline options > > combined pipeline options interfaces = fluent iterable . from ( validated pipeline options interfaces ) . append ( iface ) . to set ( ) ;,interface extend
request did n't succeed because the token was revoked so we invalidate the token stored in the session and render the index page so that the user can start the o auth <PLACE_HOLDER> again,if ( res2 . failed ( ) ) { ctx . session ( ) . destroy ( ) ; ctx . fail ( res2 . cause ( ) ) ; } else { user info . put ( __str__ @$ res2 . result ( ) . json array ( ) ) ; json object data = new json object ( ) . put ( __str__ @$ user info ) ; engine . render ( data @$ __str__ @$ res3 -> { if ( res3 . succeeded ( ) ) { ctx . response ( ) . put header ( __str__ @$ __str__ ) . end ( res3 . result ( ) ) ; } else { ctx . fail ( res3 . cause ( ) ) ; },user start
run the timing <PLACE_HOLDER> first warm up the <PLACE_HOLDER> to make sure it gets compiled,if ( argv [ __num__ ] . equals ( __str__ ) ) { for ( int i = __num__ ; i < warmup_loops ; i ++ ) { timeit ( __num__ @$ __num__ @$ __num__ ) ; } system . out . println ( __str__ ) ; timeit ( timing_trials @$ get_timing_loops @$ set_timing_loops ) ; } else if ( argv [ __num__ ] . equals ( __str__ ) ) { max_milliseconds = stress_milliseconds ; } else { throw new runtime exception ( __str__ + argv [ __num__ ] + __str__ ) ; },method warm
test suite uses standalone wal <PLACE_HOLDER> to verify pds content .,grid test utils . add test if needed ( suite @$ ignite wal reader test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds exchange during checkpoint test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds reserve wal segments test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds reserve wal segments with compaction test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite wal replaying after restart test . class @$ ignored tests ) ;,suite uses
while visit the unmatched build <PLACE_HOLDER> @$ the probe element is null @$ and the unmatched build iterator would iterate all the unmatched build <PLACE_HOLDER> @$ so we return false during the second calling of this method .,this . unmatched build visited = true ; return true ;,unmatched build
check that active primary has a newer <PLACE_HOLDER> so that peer recovery works,if ( primary != null ) { return is version compatible allocating replica ( allocation . routing nodes ( ) @$ primary . current node id ( ) @$ node @$ allocation ) ; } else { return allocation . decision ( decision . yes @$ name @$ __str__ ) ; },primary has
create data with 2 chunks : the 2 nd chunk has <PLACE_HOLDER> of the block size,long first chunk length = block_size * blocks per chunk ; long second chunk length = block_size / __num__ ; dfs test util . create file ( fs @$ src data @$ buffer len @$ first chunk length @$ block_size @$ repl factor @$ src seed ) ; dfs test util . append file new block ( ( distributed file system ) fs @$ src data @$ ( int ) second chunk length ) ; dfs test util . create file ( fs @$ new path ( target base + filename + __str__ + first chunk length ) @$ buffer len @$ first chunk length @$ block_size @$ repl factor @$ dst seed ) ; dfs test util . create file ( fs @$ new path ( target,chunk has
if a component specifies the file with a bad <PLACE_HOLDER> @$ the corresponding slot will be initialized by default physical <PLACE_HOLDER> . in such case find <PLACE_HOLDER> 2 d may return composite <PLACE_HOLDER> which can not be casted to physical <PLACE_HOLDER> .,if ( ! component names [ slot ] . equals ignore case ( name ) ) { try { components [ slot ] = ( physical font ) fm . find font2d ( component names [ slot ] @$ style @$ font manager . physical_fallback ) ; } catch ( class cast exception cce ) { components [ slot ] = fm . get default physical font ( ) ; } },slot find
permission controller hosts default permission <PLACE_HOLDER> and role management @$ so it 's a critical part of the core system .,m required permission controller package = get required permission controllerl pr ( ) ;,controller hosts
verify that both consumers have a <PLACE_HOLDER> of 10,assert equals ( __num__ @$ brokerb . get destination ( advisory topic ) . get consumers ( ) . get ( __num__ ) . get prefetch size ( ) ) ; assert equals ( __num__ @$ brokerb . get destination ( topic1 ) . get consumers ( ) . get ( __num__ ) . get prefetch size ( ) ) ; assert deq inflight ( __num__ @$ __num__ ) ;,consumers have
important : first add factory methods ; then constructors @$ so latter can override <PLACE_HOLDER> !,_add factory creators ( ctxt @$ bean desc @$ vchecker @$ intr @$ creators @$ creator defs ) ;,latter override
bail now if we are waiting to populate . this is to hold off on creating views from the time the user releases their <PLACE_HOLDER> to fling to a new position until we have finished the scroll to that position @$ avoiding glitches from happening at that point .,if ( m populate pending ) { if ( debug ) log . i ( tag @$ __str__ ) ; sort child drawing order ( ) ; return ; },user releases
each thread gets the <PLACE_HOLDER> from the existing pool . if no <PLACE_HOLDER> is available @$ it requests one and waits for it to be created . but a thread that requests the <PLACE_HOLDER> and before it could wait @$ other thread could steal that <PLACE_HOLDER> . the <PLACE_HOLDER>s in progress tries to avoid these edge cases by not requesting a <PLACE_HOLDER> from the,if ( connections in progress . get ( ) == __num__ ) { resource = non blocking get ( key @$ pool ) ; } if ( resource == null ) { connections in progress . increment and get ( ) ; try { attempt grow ( key @$ this . object factory @$ pool ) ; resource = non blocking get ( key @$ pool ) ; } finally { connections in progress . decrement and get ( ) ; } } return resource ;,thread gets
the playlist contains a byte order <PLACE_HOLDER> @$ which gets discarded .,last = reader . read ( ) ;,playlist contains
let the fragment handle the back <PLACE_HOLDER> if it implements our on parent back pressed listener,fragment fragment = m bottom nav . get active fragment ( ) ; if ( fragment instanceof on activity back pressed listener ) { boolean handled = ( ( on activity back pressed listener ) fragment ) . on activity back pressed ( ) ; if ( handled ) { return ; } } if ( is task root ( ) && device utils . get instance ( ) . is chromebook ( this ) ) { return ; },fragment handle
write <PLACE_HOLDER> to disk so that configuration.get password will find <PLACE_HOLDER>,provider . flush ( ) ;,password find
this maybe not take effect @$ when not every consume queue has extend <PLACE_HOLDER> .,if ( is ext addr ( tags code ) ) { max ext addr = tags code ; },queue extend
the tests are built on assuming 8191 max command line <PLACE_HOLDER>,assert equals ( __num__ @$ shell . windows_max_shell_length ) ; shell script builder builder = shell script builder . create ( ) ;,max command
default display should show system <PLACE_HOLDER>,assert true ( m target . should show system decors locked ( m primary display ) ) ;,display show
this is the last split in the list @$ the filters define the <PLACE_HOLDER> from the previous split to the current split and also the current split to the end,if ( i == split keys . size ( ) - __num__ ) { range filter = string . format ( __str__ + __str__ @$ lowest bound @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; range filter = string . format ( __str__ @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; } else { range filter = string . format ( __str__ + __str__ @$ lowest bound @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; },filters define
if we did n't find an existing policy create a new <PLACE_HOLDER>,if ( found policy == null ) { final string uuid seed = resource + action ; final access policy . builder builder = new access policy . builder ( ) . identifier generate from seed ( uuid seed ) . resource ( resource ) . add user ( user identifier ) ; if ( action . equals ( read_code ) ) { builder . action ( request action . read ) ; } else if ( action . equals ( write_code ) ) { builder . action ( request action . write ) ; } else { throw new illegal state exception ( __str__ + action ) ; } final access policy access policy = builder . build ( ) ; final policy jaxb policy = createjaxb,policy create
the current group has produced all its <PLACE_HOLDER>,if ( current group position == current group size ) { memory size in bytes -= current group size in bytes ; current group position = __num__ ; current rows = next grouped rows ( ) ; continue ; },group produced
now resource should have 0 <PLACE_HOLDER> .,assert . assert equals ( __num__ @$ lr . sem . available permits ( ) ) ;,resource have
region destroy will destroy dls and manual free <PLACE_HOLDER> only,if ( dlock service == null ) { dlock service = d lock service . create ( get full path ( ) @$ get system ( ) @$ true @$ false @$ false ) ; },destroy destroy
let 's also read back the result <PLACE_HOLDER> ...,node result rows node = xml handler . get sub node ( node @$ xml_rows_tag ) ; list < node > result nodes = xml handler . get nodes ( result rows node @$ row meta . xml_data_tag ) ; if ( ! result nodes . is empty ( ) ) { row meta row meta = new row meta ( xml handler . get sub node ( result rows node @$ row meta . xml_meta_tag ) ) ; for ( node result node : result nodes ) { object [ ] row data = row meta . get row ( result node ) ; rows . add ( new row meta and data ( row meta @$ row data ) ) ; } },"s" read
the number is just to debug the <PLACE_HOLDER> of the fieldname,set < string > key set = fields . key set ( ) ; list < string > entries = new array list < string > ( key set ) ; string [ ] field names = entries . to array ( new string [ entries . size ( ) ] ) ; const . sort strings ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ;,number debug
check that the jstl bundle is not already included in the <PLACE_HOLDER> @$ and include it if it is not because subsequent classes such as os gi web inf configuration use this <PLACE_HOLDER> to determine which jars are considered to be on the container classpath,if ( jstl bundle != null ) { if ( pattern == null ) { pattern = pattern . compile ( jstl bundle . get symbolic name ( ) ) ; deployment manager . set context attribute ( os gi web inf configuration . container_bundle_pattern @$ jstl bundle . get symbolic name ( ) ) ; } else if ( ! ( pattern . matcher ( jstl bundle . get symbolic name ( ) ) . matches ( ) ) ) { string s = tmp + __str__ + jstl bundle . get symbolic name ( ) ; pattern = pattern . compile ( s ) ; deployment manager . set context attribute ( os gi web inf configuration . container_bundle_pattern @$ s ) ; } } for,classes use
create a new item list which contain a new adapter item object the id of the new item is changed @$ and will be treated as a new item according to the rule we set in the callback . this test case is to verify the get change payload <PLACE_HOLDER> still honor the standard we set up to judge new item,m items . clear ( ) ; adapter item new item = new adapter item ( __num__ @$ __str__ @$ __str__ ) ; m items . add ( new item ) ;,the honor
unexpected node <PLACE_HOLDER> : ' . ' found when expecting <PLACE_HOLDER> : an identifier,assert not null ( compile ( __str__ ) ) ;,unexpected node
the object that we 're modifying here is a copy of the original ! so let 's change the <PLACE_HOLDER> from relative to absolute by grabbing the file object ... in case the name of the file comes from previous steps @$ forget about this !,try { if ( ! accepting filenames ) { for ( int i = __num__ ; i < file name . length ; i ++ ) { file object file object = kettlevfs . get file object ( space . environment substitute ( file name [ i ] ) @$ space ) ; file name [ i ] = resource naming interface . name resource ( file object @$ space @$ utils . is empty ( file mask [ i ] ) ) ; } } return null ; } catch ( exception e ) { throw new kettle exception ( e ) ; },let change
check whether the path matches an existing leaf <PLACE_HOLDER> .,for ( string part : parts ) { if ( ! create new branch && node != root && node . children . is empty ( ) ) { return this ; } if ( node . children . contains key ( part ) ) { node = node . children . get ( part ) ; } else { create new branch = true ; node tmp = new node ( ) ; node . children . put ( part @$ tmp ) ; node = tmp ; } },path matches
grouping shuffle reader produces <PLACE_HOLDER> in empty windows . for now @$ we count the element at least once to keep the current counter behavior .,if ( windows size == __num__ ) { element count . add value ( __num__ ) ; } else { element count . add value ( windows size ) ; },reader produces
deploy a new version of the child process in which the user task has an updated <PLACE_HOLDER>,process engine . get repository service ( ) . create deployment ( ) . name ( __str__ ) . add bpmn model ( __str__ @$ childv2 bpmn model ) . deploy ( ) ; runtime service . start process instance by key ( __str__ ) ; list < task > list = task service . create task query ( ) . list ( ) ; assert equals ( __str__ @$ __num__ @$ list . size ( ) ) ; task task = list . get ( __num__ ) ; assert equals ( __str__ @$ __str__ @$ task . get name ( ) ) ;,task has
length protocol version authentication result code server host <PLACE_HOLDER> connection <PLACE_HOLDER>,int offset = __num__ + __num__ + __num__ + __num__ + __num__ ;,host id
unlock managed <PLACE_HOLDER> with unified lock,for ( user info profile : m user manager . get profiles ( user id ) ) { if ( tied managed profile ready to unlock ( profile ) ) { try { unlock child profile ( profile . id @$ false @$ challenge type @$ challenge @$ reset lockouts ) ; } catch ( remote exception e ) { log . d ( tag @$ __str__ @$ e ) ; } } if ( ! already unlocked ) { long ident = clear calling identity ( ) ; try { maybe show encryption notification for user ( profile . id ) ; } finally { restore calling identity ( ident ) ; } } },unlock managed
let 's load <PLACE_HOLDER> that we just have written and use it for prediction .,easy predict model wrapper te model wrapper = null ; target encoder mojo model loaded mojo model = ( target encoder mojo model ) mojo model . load ( mojo file . get path ( ) ) ; te model wrapper = new easy predict model wrapper ( loaded mojo model ) ;,"s" load
2 nd . case : message two received cancels the <PLACE_HOLDER>,runtime service . start process instance by key ( __str__ ) ; execution execution message two = runtime service . create execution query ( ) . message event subscription name ( __str__ ) . single result ( ) ; assert not null ( execution message two ) ; runtime service . message event received ( __str__ @$ execution message two . get id ( ) ) ; user task = task service . create task query ( ) . single result ( ) ; assert not null ( user task ) ; assert equals ( __str__ @$ user task . get task definition key ( ) ) ; task service . complete ( user task . get id ( ) ) ; assert equals ( __num__ @$ runtime,2 cancels
success ! do n't release the <PLACE_HOLDER> when exiting @$ let parcel file descriptor inner do that when it is closed .,stable provider = null ; return new asset file descriptor ( pfd @$ fd . get start offset ( ) @$ fd . get declared length ( ) ) ;,! release
file <PLACE_HOLDER>.is shared depends on whether the underlying platform support shared <PLACE_HOLDER> @$ but it works on windows & linux .,assert true ( file lock . is shared ( ) ) ; assert same ( read only file channel @$ file lock . channel ( ) ) ; assert equals ( position @$ file lock . position ( ) ) ; assert equals ( size @$ file lock . size ( ) ) ;,support shared
every language equals <PLACE_HOLDER> :,assert true ( new german ( ) . equals consider variants if specified ( new german ( ) ) ) ; assert true ( new germany german ( ) . equals consider variants if specified ( new germany german ( ) ) ) ; assert true ( new english ( ) . equals consider variants if specified ( new english ( ) ) ) ; assert true ( new american english ( ) . equals consider variants if specified ( new american english ( ) ) ) ;,language equals
mimic the normal kdc behavior . when a server is not allowed to send s 4 u 2 self @$ do not send an <PLACE_HOLDER> . instead @$ send a ticket which is useless later .,if ( ! names . contains ( cname . to string ( ) ) ) { allow forwardable = false ; },behavior send
if there are no tangents use the classic <PLACE_HOLDER>,if ( tb == null ) { apply skinning ( mesh @$ offset matrices ) ; } else { apply skinning tangents ( mesh @$ offset matrices @$ tb ) ; },tangents use
now we have a <PLACE_HOLDER> . let ' register this <PLACE_HOLDER> .,m upload store . register post model ( post @$ media list ) ;,"" register
actual and values have the same <PLACE_HOLDER> but are they in the same order ?,if ( ! diff . differences found ( ) ) { int i = __num__ ; for ( object element from actual : actual ) { if ( ! are equal ( element from actual @$ values [ i ] ) ) { throw failures . failure ( info @$ elements differ at index ( element from actual @$ values [ i ] @$ i @$ comparison strategy ) ) ; } i ++ ; } return ; },actual have
a polygon should have more than 2 <PLACE_HOLDER>,preconditions . check argument ( abscissa . length > __num__ ) ;,polygon have
noinspection android lint clickable <PLACE_HOLDER> accessibility,note block holder . m avatar image view . set on touch listener ( m on gravatar touch listener ) ; if ( site id == user id ) { note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_no ) ; } else { note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_yes ) ; } note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_no ) ; note block holder . m avatar image view . set content description ( null ) ; note block holder . m avatar image view . set on click listener ( null ) ;,noinspection android
we are unable to recover . silently eat the <PLACE_HOLDER> .,return null_output_stream ;,silently eat
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default text snippet local profile = new crawl profile ( crawl_profile_snippet_local_text @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_snippet_local_text_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ true @$ false @$ false @$ false @$ true @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . ifexist @$ __str__ + crawl_profile_snippet_local_text @$ client identification . yacy intranet crawler agent name @$ null,url match
balancer sends initial <PLACE_HOLDER> .,lb response observer . on next ( build initial response ( ) ) ;,balancer sends
if we have no record of this environment @$ that means the current rule implicitly uses the <PLACE_HOLDER> for this group . so explicitly opt that group 's <PLACE_HOLDER> into the refined set before trying to remove specific items .,if ( env to prune == null ) { for ( environment with group default env : get defaults ( refined environment to prune @$ dep environments . get refined environments ( ) ) ) { refined environments so far . add ( default env ) ; labels to environments . put ( default env . environment ( ) @$ default env ) ; } env to prune = verify . verify not null ( labels to environments . get ( refined environment to prune ) ) ; },rule opt
no space for eo cd <PLACE_HOLDER> in the file .,if ( file size < zip_eocd_rec_min_size ) { return null ; },space cd
hive returns table.column for column <PLACE_HOLDER> . grab the column <PLACE_HOLDER> as the string after the last period,int column name delimiter = column name from meta . last index of ( __str__ ) ; string column name = column name from meta . substring ( column name delimiter + __num__ ) ; switch ( meta . get column type ( i ) ) { case char : case longnvarchar : case longvarchar : case nchar : case nvarchar : case varchar : case array : case struct : case java_object : case other : case sqlxml : builder . name ( column name ) . type ( ) . union of ( ) . null builder ( ) . end null ( ) . and ( ) . string type ( ) . end union ( ) . no default ( ) ; break ;,returns grab
closing window should focus tex <PLACE_HOLDER>,openw . click ( ) ; text field element text field = $ ( text field element . class ) . first ( ) ; window element window = $ ( window element . class ) . first ( ) ; window . close ( ) ; assert equals ( text field . get wrapped element ( ) @$ get focused element ( ) ) ;,window focus
now lost a finishable <PLACE_HOLDER> .,r2 . set can update finishable ( ) ; task wrapper . finishable state updated ( false ) ; task wrapper2 = task executor service . preemption queue . peek ( ) ; assert not null ( task wrapper2 ) ; assert true ( task wrapper . is in preemption queue ( ) ) ; r2 . complete ( ) ; r2 . await end ( ) ; task executor service . shut down ( false ) ;,now lost
all plan item instances are created . now activate <PLACE_HOLDER> .,command context util . get agenda ( command context ) . plan activate plan item instance operation ( entry dependent plan item instance @$ satisfied criterion . get id ( ) ) ; for ( int i = parent plan item instances to activate . size ( ) - __num__ ; i >= __num__ ; i -- ) { plan item instance entity parent plan item instance = parent plan item instances to activate . get ( i ) ; if ( parent plan item instance == null ) { command context util . get agenda ( command context ) . plan create plan item instance operation ( parent plan item instance ) ; } command context util . get agenda ( command context ) . plan activate,plan activate
on windows the system <PLACE_HOLDER> may be different than the user <PLACE_HOLDER> . this is an unsupported configuration @$ but in that case we want to return a dummy <PLACE_HOLDER> that will never cause a match in the usage of this api . this is important because windows documents that the family names of fonts are enumerated using the language of the system <PLACE_HOLDER>,system locale = ( locale ) java . security . access controller . do privileged ( new java . security . privileged action ( ) { public object run ( ) { string file encoding = system . get property ( __str__ @$ __str__ ) ; string sys encoding = system . get property ( __str__ ) ; if ( sys encoding != null && ! sys encoding . equals ( file encoding ) ) { return locale . root ; } string language = system . get property ( __str__ @$ __str__ ) ; string country = system . get property ( __str__ @$ __str__ ) ; string variant = system . get property ( __str__ @$ __str__ ) ; return new locale ( language @$ country,which return
since we send the empty <PLACE_HOLDER> to the client @$ if the client has not modified the <PLACE_HOLDER> then we do change it,if ( ! string utils . is empty ( user info . get password ( ) ) ) { user . set password ( user info . get password ( ) ) ; } user . set description ( user info . get description ( ) ) ; return user ;,client modified
this hook is for testing <PLACE_HOLDER> only .,if ( boolean . get boolean ( cli strings . ignore_interceptors ) ) { return result model . create info ( cli strings . shutdown__msg__shutdown_entire_ds ) ; } response response = read yes no ( cli strings . shutdown__msg__warn_user @$ response . yes ) ; if ( response == response . no ) { return result model . create error ( cli strings . shutdown__msg__aborting_shutdown ) ; } else { return result model . create info ( cli strings . shutdown__msg__shutdown_entire_ds ) ; },hook testing
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
after looping 3 times @$ the process should end . cycle should not repeat <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { job timer = management service . create timer job query ( ) . single result ( ) ; management service . move timer to executable job ( timer . get id ( ) ) ; management service . execute job ( timer . get id ( ) ) ; } assert process ended ( process instance . get id ( ) ) ;,cycle repeat
so let 's create pipe <PLACE_HOLDER> to make that possible,return new pipe cloner impl ( ) . copy ( p ) ;,"s" create
note that regular ser de does n't tolerate fewer <PLACE_HOLDER> .,list < object > deserialized row ; if ( do write fewer columns ) { deserialized row = ( list < object > ) serde_fewer . deserialize ( bytes writable ) ; } else { deserialized row = ( list < object > ) serde . deserialize ( bytes writable ) ; } object [ ] row = rows [ i ] ; for ( int index = __num__ ; index < write column count ; index ++ ) { object expected = row [ index ] ; object object = deserialized row . get ( index ) ; if ( expected == null || object == null ) { if ( expected != null || object != null ) { fail ( __str__ ) ; } },de tolerate
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default proxy profile = new crawl profile ( crawl_profile_proxy @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ integer . parse int ( sb . get config ( switchboard constants . proxy_prefetch_depth @$ __str__ ) ) @$ true @$ crawl profile . get recrawl date ( crawl_profile_proxy_recrawl_cycle ) @$ - __num__ @$ false @$ true @$ true @$ false @$ sb . get config bool ( switchboard constants . proxy_indexing_local_text @$ true ) @$ sb . get config bool ( switchboard constants . proxy_indexing_local_media,ip match
the address should have <PLACE_HOLDER>,result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; file name = null ;,address have
test when a user single clicks a directory name and clicks the action <PLACE_HOLDER> when the filename text field is empty,set mode ( files_only ) ; file dir = new file ( get test directory path ( ) ) ; set dir ( dir . get parent file ( ) ) ; assert equals ( dir . get parent file ( ) @$ get current directory ( ) ) ; set filename field text ( null ) ; set file ( dir ) ; press ok ( ) ; wait for chooser ( ) ; assert true ( __str__ + __str__ + last selected file @$ chooser . is showing ( ) ) ; assert equals ( dir @$ get current directory ( ) ) ;,user clicks
i found that crimson does n't show the proper stack trace when a runtime <PLACE_HOLDER> happens inside a schema builder . the following code shows the actual <PLACE_HOLDER> that happened .,if ( e . get cause ( ) instanceof sax exception ) { sax exception se = ( sax exception ) e . get cause ( ) ; if ( se . get exception ( ) != null ) se . get exception ( ) . print stack trace ( ) ; } throw e ;,code shows
scale fractional <PLACE_HOLDER> @$ dot @$ integer <PLACE_HOLDER> .,final int scale = fast scale ; final boolean is zero fast1 and fast2 = ( fast1 == __num__ && fast2 == __num__ ) ; final boolean is zero fast2 = ( fast2 == __num__ ) ; int lower longword scale = __num__ ; int middle longword scale = __num__ ; int high longword scale = __num__ ; long long word = fast0 ; if ( scale > __num__ ) { lower longword scale = math . min ( scale @$ longword_decimal_digits ) ; for ( int i = __num__ ; i < lower longword scale ; i ++ ) { scratch buffer [ index -- ] = ( byte ) ( byte_digit_zero + long word % __num__ ) ; long word /= __num__ ; } if (,digits integer
make sure nongreedy mech cut off does n't kill this <PLACE_HOLDER>,lexer grammar lg = new lexer grammar ( __str__ + __str__ + __str__ + __str__ ) ;,mech kill
test renaming a source to a path which uses a <PLACE_HOLDER> as a directory .,source file = new file ( test_dir @$ __str__ ) ; assert . assert true ( source file . create new file ( ) ) ; file bad target = new file ( target file @$ __str__ ) ; try { nativeio . rename to ( source file @$ bad target ) ; assert . fail ( ) ; } catch ( nativeio exception e ) { if ( path . windows ) { assert . assert equals ( string . format ( __str__ ) @$ e . get message ( ) ) ; } else { assert . assert equals ( errno . enotdir @$ e . get errno ( ) ) ; } },which uses
starting the case instance starts the process task . the process has an async <PLACE_HOLDER> at the beginning,cmmn engine . get cmmn runtime service ( ) . create case instance builder ( ) . case definition key ( __str__ ) . start ( ) ; job job = process engine . get management service ( ) . create job query ( ) . single result ( ) ; assert null ( job . get scope type ( ) ) ; job test helper . wait for job executor to process all jobs ( process engine . get process engine configuration ( ) @$ process engine . get management service ( ) @$ __num__ @$ __num__ ) ;,process has
oops ! we better let ognl take <PLACE_HOLDER> of this its own way ...,throw new ognl shortcut expression not applicable exception ( ) ;,ognl take
total overlap do n't add either <PLACE_HOLDER> @$ get next range 1 and range 2,range1 = range1 it . has next ( ) ? range1 it . next ( ) : null ; range2 = range2 it . has next ( ) ? range2 it . next ( ) : null ; break ; case range1_starts_at_range2_ends_after_range2 :,overlap add
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized . get ( ) ) ;,scheduler captures
nested view has scrollable <PLACE_HOLDER> under this point . let it be handled there .,if ( dy != __num__ && ! is gutter drag vertically ( m last motiony @$ dy ) && can scroll vertically ( this @$ false @$ ( int ) dy @$ ( int ) x @$ ( int ) y ) ) { m last motionx = x ; m last motiony = y ; m is unable to drag = true ; return false ; },view has
already exist . only update allowed <PLACE_HOLDER> .,connected controller record record = m controller records . get ( saved info ) ; record . allowed commands = commands ;,update allowed
the overflow will require scrolling to get to all the items . extend the <PLACE_HOLDER> so that part of the hidden items is displayed .,if ( actual size < m overflow panel . get count ( ) ) { extension = ( int ) ( m line height * __num__ ) ; },overflow extend
break @$ next read will support larger <PLACE_HOLDER> .,break ;,read support
make sure test invoice plugin api will return an additional tax <PLACE_HOLDER>,test invoice plugin api . add tax item ( new tax invoice item ( uuid . randomuuid ( ) @$ null @$ null @$ account . get id ( ) @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ new local date ( __num__ @$ __num__ @$ __num__ ) @$ new local date ( __num__ @$ __num__ @$ __num__ ) @$ __str__ @$ big decimal . one @$ account . get currency ( ) @$ null @$ null ) ) ; test invoice plugin api . add tax item ( new tax invoice item ( uuid . randomuuid ( ) @$ null @$ null @$ account . get id ( ) @$ null @$ null @$,api return
the touchable region should not exceed the <PLACE_HOLDER> of its container .,apply op to region by bounds ( touchable region @$ this @$ region . op . intersect ) ; final view parent parent = get parent ( ) ; if ( parent != null ) { parent . subtract obscured touchable region ( touchable region @$ this ) ; },region exceed
we know that the parameters have the same name @$ since this is what the descriptor 's hash code & equality are based on . the only thing that may be different is the <PLACE_HOLDER> . and since the proposed parameter does not have a <PLACE_HOLDER> @$ we want to use whatever is currently set .,return old parameter == null ? descriptor : old parameter . get descriptor ( ) ;,parameter have
if the comment is lacking <PLACE_HOLDER> @$ offer moderation actions,if ( note . get comment status ( ) == comment status . unapproved ) { if ( note . can moderate ( ) ) { add comment approve action for comment notification ( context @$ builder @$ note id ) ; } } else { if ( note . can like ( ) ) { add comment like action for comment notification ( context @$ builder @$ note id ) ; } },comment lacking
error getting active data rethrows <PLACE_HOLDER>,try { mockito . when ( mockzk . get data ( mockito . eq ( zk_lock_name ) @$ mockito . eq ( false ) @$ any ( ) ) ) . then throw ( new keeper exception . auth failed exception ( ) ) ; elector . get active data ( ) ; assert . fail ( __str__ ) ; } catch ( keeper exception . auth failed exception ke ) { mockito . verify ( mockzk @$ mockito . times ( __num__ ) ) . get data ( mockito . eq ( zk_lock_name ) @$ mockito . eq ( false ) @$ any ( ) ) ; },error rethrows
generating portfolio object <PLACE_HOLDER> to be populated across the pr 's & local regions,portfolio data [ ] portfolio data = create portfolio data ( start_portfolio_data_index @$ total_data_size ) ;,portfolio object
count all nodes @$ how many groups <PLACE_HOLDER> node has <PLACE_HOLDER>,configuration group config = configuration . with batch size ( config @$ neo store . get relationship group store ( ) . get records per page ( ) ) ; stats provider memory usage = new memory usage stats provider ( neo store @$ group cache ) ; execute stage ( new count groups stage ( group config @$ from store @$ group cache @$ memory usage ) ) ; long from node id = __num__ ; while ( from node id < high node id ) { long to node id = group cache . prepare ( from node id ) ; monitor . defragmenting node range ( from node id @$ to node id ) ; execute stage ( new scan and cache groups stage (,groups has
owner sets <PLACE_HOLDER> and permission,verify set acl ( test_user_1 @$ test_dir_uri @$ null @$ test_user_2 . get group ( ) @$ ( short ) __num__ @$ true ) ; file info file info = m file system master . get file info ( m file system master . get file id ( new alluxiouri ( test_dir_file_uri ) ) ) ; assert equals ( test_user_2 . get group ( ) @$ file info . get group ( ) ) ; assert equals ( ( short ) __num__ @$ file info . get mode ( ) ) ;,owner sets
handle the case where the last call to write actually caused an <PLACE_HOLDER> in the log,if ( ( ledger writer . is log segment in error ( ) || force recovery ) && reset on error ) { completable future < void > close future ; if ( ledger writer . is log segment in error ( ) ) { close future = ledger writer . async abort ( ) ; } else { close future = ledger writer . async close ( ) ; } return close future . then compose ( new function < void @$ completion stage < bk log segment writer > > ( ) { @ override public completable future < bk log segment writer > apply ( void result ) { remove cached log writer ( ) ; if ( ledger writer . is log segment in,call caused
inner query has a single group by and outer query has grouping <PLACE_HOLDER>,assert query ( __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ) ;,query has
make sure the state we just read in for columns @$ rows @$ and scrollbar visibility has legal <PLACE_HOLDER>,if ( columns < __num__ ) { columns = __num__ ; } if ( rows < __num__ ) { rows = __num__ ; } if ( ( scrollbar visibility < scrollbars_both ) || ( scrollbar visibility > scrollbars_none ) ) { this . scrollbar visibility = scrollbars_both ; } if ( text area serialized data version < __num__ ) { set focus traversal keys ( keyboard focus manager . forward_traversal_keys @$ forward traversal keys ) ; set focus traversal keys ( keyboard focus manager . backward_traversal_keys @$ backward traversal keys ) ; },state has
then : above command should not throw any <PLACE_HOLDER>,authorization validator . check authorization ( service context @$ meta store @$ statement ) ;,command throw
the line crosses a paragraph <PLACE_HOLDER>,if ( get paragraph index ( start ) != get paragraph index ( limit - __num__ ) ) { throw new illegal argument exception ( ) ; },line crosses
check that union has only a single <PLACE_HOLDER>,if ( this . get outgoing connections ( ) . size ( ) > __num__ ) { throw new compiler exception ( __str__ ) ; } boolean children skipped due to replicated input = false ;,union has
we compute <PLACE_HOLDER> the distance between each cluster and its closest neighbor is to set a proportional distance threshold for points that should be involved in calculating the centroid .,closest cluster distances . clear ( ) ; for ( vector center : centroids ) { vector closest other cluster = centroids . search first ( center @$ true ) . get value ( ) ; closest cluster distances . add ( distance measure . distance ( center @$ closest other cluster ) ) ; },distance is
create a media file <PLACE_HOLDER>,string file name = string . format ( __str__ @$ new simple date format ( __str__ ) . format ( new date ( ) ) ) ; if ( type == media_type_image ) { file name = string . format ( __str__ @$ file name ) ; } else if ( type == media_type_video ) { file name = string . format ( __str__ @$ file name ) ; } else { log . e ( tag @$ __str__ + type ) ; return null ; } return new file ( string . format ( __str__ @$ storage dir . get path ( ) @$ file . separator @$ file name ) ) ;,media file
reduce to subinterfaces this does not need to be recursive since we make a copy @$ and that copy contains all super <PLACE_HOLDER> for the whole hierarchy,for ( ct class intf : alter map . values ( ) ) { ct class [ ] interfaces ; try { interfaces = intf . get interfaces ( ) ; } catch ( not found exception e ) { throw new runtime exception ( e ) ; } for ( ct class c : interfaces ) alter map . remove ( c . get name ( ) ) ; } return alter map ;,copy contains
exception will set <PLACE_HOLDER>,new certificate manager ( key @$ cert ) ;,exception set
ok found a file we support and have a driver for the format @$ for this demo @$ we 'll just use the first <PLACE_HOLDER>,try { offline tile provider tile provider = new offline tile provider ( new simple register receiver ( get activity ( ) ) @$ new file [ ] { list [ i ] } ) ; m map view . set tile provider ( tile provider ) ; string source = __str__ ; i archive file [ ] archives = tile provider . get archives ( ) ; if ( archives . length > __num__ ) { set < string > tile sources = archives [ __num__ ] . get tile sources ( ) ; if ( ! tile sources . is empty ( ) ) { source = tile sources . iterator ( ) . next ( ) ; this . m map view . set tile,ok use
check that both reduce and combiner have the same <PLACE_HOLDER>,assert equals ( driver strategy . sorted_group_reduce @$ reduce node . get driver strategy ( ) ) ;,combiner have
servicetask in process 'update jpa <PLACE_HOLDER>s process ' should have set <PLACE_HOLDER> on entity to update .,object updated entity = runtime service . get variable ( process instance . get id ( ) @$ __str__ ) ; assert true ( updated entity instanceof field accessjpa entity ) ; assert equals ( __str__ @$ ( ( field accessjpa entity ) updated entity ) . get value ( ) ) ;,servicetask set
the glow depends <PLACE_HOLDER> on the velocity @$ and therefore starts out nearly invisible .,m glow alpha start = __num__ ; m glow scaley start = math . max ( m glow scaley @$ __num__ ) ;,glow depends
find the actual first which matches the <PLACE_HOLDER> .,boolean bool = super . internal first ( ) ; if ( p == null ) { return bool ; } while ( bool ) { if ( p . evaluate ( this ) ) { break ; } bool = super . internal next ( ) ; } return bool ;,which matches
initial board has a <PLACE_HOLDER> like the following in the center : wb bw,int middle row = board . length / __num__ ; int middle column = board [ middle row ] . length / __num__ ; board [ middle row ] [ middle column ] = new piece ( color . white ) ; board [ middle row + __num__ ] [ middle column ] = new piece ( color . black ) ; board [ middle row + __num__ ] [ middle column + __num__ ] = new piece ( color . white ) ; board [ middle row ] [ middle column + __num__ ] = new piece ( color . black ) ; black count = __num__ ; white count = __num__ ;,board has
this case should pass on java 6 and later @$ as those jr es have a gif <PLACE_HOLDER> .,if ( system . get property ( __str__ ) . starts with ( __str__ ) ) { fail ( ) ; },es have
start definitions root <PLACE_HOLDER>,xtw . write start element ( bpmn2_prefix @$ element_definitions @$ bpmn2_namespace ) ; xtw . set default namespace ( bpmn2_namespace ) ; xtw . write default namespace ( bpmn2_namespace ) ; xtw . write namespace ( bpmn2_prefix @$ bpmn2_namespace ) ; xtw . write namespace ( xsi_prefix @$ xsi_namespace ) ; xtw . write namespace ( xsd_prefix @$ schema_namespace ) ; xtw . write namespace ( activiti_extensions_prefix @$ activiti_extensions_namespace ) ; xtw . write namespace ( bpmndi_prefix @$ bpmndi_namespace ) ; xtw . write namespace ( omgdc_prefix @$ omgdc_namespace ) ; xtw . write namespace ( omgdi_prefix @$ omgdi_namespace ) ; for ( string prefix : model . get namespaces ( ) . key set ( ) ) { if ( ! default namespaces . contains ( prefix,definitions root
let 's give the principal the administration <PLACE_HOLDER> @$ without granting access,mutable acl acl first deny = new acl impl ( identity @$ __num__ @$ acl authorization strategy @$ new console audit logger ( ) ) ; acl first deny . insert ace ( __num__ @$ base permission . administration @$ new principal sid ( auth ) @$ false ) ;,"s" give
workaround : if the abstract value type does not have any <PLACE_HOLDER> @$ a vector containing value base should be returned instead of an empty vector,if ( v . derived from ( ) . size ( ) == __num__ ) stream . print ( __str__ ) ; else { symtab entry parent ; for ( int i = __num__ ; i < v . derived from ( ) . size ( ) ; i ++ ) { if ( i == __num__ ) stream . print ( __str__ ) ; else stream . print ( __str__ ) ; parent = ( symtab entry ) v . derived from ( ) . element at ( i ) ; stream . print ( util . java name ( parent ) ) ; } },type have
an event passes multiple <PLACE_HOLDER>,event filter dimension map . put ( __str__ @$ lists . new array list ( __str__ ) ) ; filtered events = event filter . apply dimension filter ( all events @$ event filter dimension map ) ; assert . assert equals ( filtered events . size ( ) @$ __num__ ) ; assert . assert equals ( filtered events . get ( __num__ ) . get name ( ) @$ __str__ ) ;,event passes
asif : do the <PLACE_HOLDER> of the different group junction results .,compiled value iter operands to send = null ; if ( ! iter operands . is empty ( ) ) { int size = iter operands . size ( ) ; compiled value cv [ ] = new compiled value [ size ] ; for ( int k = __num__ ; k < size ; ++ k ) { cv [ k ] = ( compiled value ) this . iter operands . get ( k ) ; } if ( cv . length == __num__ ) { iter operands to send = cv [ __num__ ] ; } else { iter operands to send = new compiled junction ( cv @$ this . operator ) ; } },asif do
get <PLACE_HOLDER> in this we should try 5 count get the <PLACE_HOLDER>,int count = __num__ ; exception error = null ; while ( count > __num__ ) { if ( command . is cancel ) { if ( command . m listener != null ) command . m listener . on cancel ( ) ; break ; } try { command . m result = i_command . command ( command . m id @$ command . m timeout @$ command . m parameters ) ; if ( command . m listener != null ) command . m listener . on completed ( command . m result ) ; break ; } catch ( exception e ) { error = e ; count -- ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e1,count get
if no resources in the holder @$ or if the holder has different resources loaded @$ then load the <PLACE_HOLDER> and set the new resources in the holder,if ( resources == null || ! config files . equals ( resources . get config resources ( ) ) ) { log . debug ( __str__ ) ; resources = new validation resources ( config files @$ get configuration from files ( config files ) ) ; validation resource holder . set ( resources ) ; } final configuration hive config = resources . get configuration ( ) ; problems . add all ( kerberos properties . validate principal and keytab ( this . get class ( ) . get simple name ( ) @$ hive config @$ principal @$ key tab @$ log ) ) ; return problems ;,resources load
like many fully successful operations @$ a single row fetch <PLACE_HOLDER> as 2 logical row operations @$ one for locating the row and one for retrieving it .,assert equals ( __num__ @$ m_ee . m_calls fromee ) ; assert equals ( long opthreshold @$ m_ee . m_last tuples accessed ) ; assert true ( __num__ < m_ee . m_curr memory in bytes ) ; assert true ( __num__ > m_ee . m_curr memory in bytes ) ; assert true ( __num__ < m_ee . m_peak memory in bytes ) ; assert true ( __num__ > m_ee . m_peak memory in bytes ) ; assert true ( m_ee . m_peak memory in bytes >= m_ee . m_curr memory in bytes ) ;,row fetch
jdbc should return <PLACE_HOLDER> ...,if ( data instanceof string ) { r . deliver ( data ) ; } else if ( data instanceof long ) { long indexes = ( ( long ) data ) . long value ( ) ; r . deliver ( convert set value ( column @$ indexes @$ options ) ) ; },jdbc return
another call to close should not cause an <PLACE_HOLDER>,zis . close ( ) ;,call cause
after the delay the lfo should start <PLACE_HOLDER> let make sure output is accurate enough,double p_step = ( __num__ / control_rate ) * math . exp ( ( freq - __num__ ) * ( math . log ( __num__ ) / __num__ ) ) ; double p = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { p += p_step ; double predicted_output = __num__ + math . sin ( p * __num__ * math . pi ) * __num__ ; if ( math . abs ( predicted_output - lfo_output [ __num__ ] ) > __num__ ) throw new exception ( __str__ + predicted_output + __str__ + lfo_output [ __num__ ] + __str__ ) ; lfo . process control logic ( ) ; },lfo start
7900040069 ea 030000000000 8300 c 20003006 aea 03005 c 003800223 aab 5107 a <PLACE_HOLDER> b 41 a 0030 d 506 e 3000414010250 bf 630007000000000000000000000000000000 c 3107209000089050000000000006 bea 03005 c 003800403 aab 5107 a <PLACE_HOLDER> b 41 a 0030 d 506 e 3000414010250 bf 630007000000000000000000000000000000 c 3107209000089050000000000006 cea 03005 c 0038005 e 3 aab 5107 a <PLACE_HOLDER> b 41 a 0030 d 506 e 3000414010250,apel protocol decoder decoder = new apel protocol decoder ( null ) ;,aab 5107
app : released api 10 dev : released api <PLACE_HOLDER>,verify compute target sdk version ( older_version @$ released @$ true @$ older_version ) ;,app released
complete task c which shoul<PLACE_HOLDER> start task <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name ( plan item instances @$ __str__ ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active @$ waiting_for_repetition ) ;,which start
we used to destroy the buffers on exiting fs mode @$ this is no longer needed since fs change will cause a surface data <PLACE_HOLDER>,synchronized ( peer ) { exit full screen exclusive ( screen @$ peer ) ; },change cause
the value of call count can exceed <PLACE_HOLDER> only if the callback thread survives the exception thrown by the first callback .,assert true ( call count . get ( ) > __num__ ) ; if ( watcher != null ) { watcher . stop ( ) ; watcher . wait for state ( file change watcher . state . stopped ) ; },value exceed
if still null then the implementation does n't offer a presence operation <PLACE_HOLDER> which is unacceptable for jabber .,if ( op set pers presence2 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ;,implementation offer
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,scheduler captures
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
first preference for <PLACE_HOLDER> goes to the task id set in the activity options . use the <PLACE_HOLDER> associated with that if possible .,if ( task id != invalid_task_id ) { options . set launch task id ( invalid_task_id ) ; final task record task = any task for id ( task id @$ match_task_in_stacks_or_recent_tasks_and_restore @$ options @$ on top ) ; options . set launch task id ( task id ) ; if ( task != null ) { return task . get stack ( ) ; } },preference use
start from relation type go to a role player check the role player 's type go to the other role player check two <PLACE_HOLDER> are different check the role player 's type check the subtypes,assert that ( plan @$ contains in any order ( instance of ( label fragment . class ) @$ instance of ( label fragment . class ) @$ instance of ( label fragment . class ) @$ instance of ( in isa fragment . class ) @$ instance of ( out role player fragment . class ) @$ instance of ( out isa fragment . class ) @$ instance of ( out role player fragment . class ) @$ instance of ( neq fragment . class ) @$ instance of ( out isa fragment . class ) @$ instance of ( out sub fragment . class ) ) ) ;,start go
this is the actual creation of the file system . return true indicating a <PLACE_HOLDER>,return su . execute ( listener @$ root username @$ root password @$ new create ( listener @$ home @$ uid @$ gid @$ user name ) ) ;,system indicating
for some reason in some corner cases nodes are n't having sentence <PLACE_HOLDER> set do a pass and make sure all nodes have sentence <PLACE_HOLDER> set,semantic graph sg = sentence . get ( semantic graph core annotations . collapsed dependencies annotation . class ) ; if ( sg != null ) { for ( indexed word iw : sg . vertex set ( ) ) { if ( iw . get ( core annotations . sentence index annotation . class ) == null && sentence . get ( core annotations . sentence index annotation . class ) != null ) { iw . set sent index ( sentence . get ( core annotations . sentence index annotation . class ) ) ; } } },nodes have
some adjustments to not crash the <PLACE_HOLDER> with emtpy data,if ( m max elements == __num__ ) { m max elements = __num__ ; } if ( m mcount == __num__ ) { m mcount = __num__ ; } if ( m first element == m last element ) { m first element = __num__ ; m last element = __num__ ; } if ( m max cards == __num__ ) { m max cards = __num__ ; } return list . size ( ) > __num__ ;,adjustments crash
index used stats <PLACE_HOLDER>,string query str = __str__ ; query query = qs . new query ( query str ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { query . execute ( ) ; } assert equals ( __num__ @$ key index1 stats . get total uses ( ) ) ;,index used
latvian does not use <PLACE_HOLDER> first,string [ ] data = { __str__ @$ __str__ @$ __str__ @$ __str__ } ; generic locale starter ( new locale ( __str__ @$ __str__ ) @$ data ) ;,latvian use
exclude if this bulletin does n't have a source <PLACE_HOLDER> or if it does n't match,if ( bulletin query . get source id pattern ( ) != null ) { if ( bulletin . get source id ( ) == null || ! bulletin query . get source id pattern ( ) . matcher ( bulletin . get source id ( ) ) . find ( ) ) { return false ; } },bulletin have
ensure a timer throws an illegal state <PLACE_HOLDER> after cancelled,try { t = new timer ( ) ; timer test task test task = new timer test task ( ) ; date d = new date ( system . current time millis ( ) + __num__ ) ; t . cancel ( ) ; try { t . schedule ( test task @$ d ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } t = new timer ( ) ; test task = new timer test task ( ) ; d = new date ( system . current time millis ( ) + __num__ ) ; test task . cancel ( ) ; try { t . schedule ( test task @$ d ) ; fail ( __str__ ),timer throws
if 1 st sibling @$ return <PLACE_HOLDER>,if ( result == null ) { result = node . get parent node ( ) ; return result ; },1 st
assert that all threads got back the same <PLACE_HOLDER>,for ( future < map < bounded window @$ long > > result : results ) { assert equals ( value @$ result . get ( ) ) ; for ( map . entry < bounded window @$ long > entry : result . get ( ) . entry set ( ) ) { assert same ( value . get ( entry . get key ( ) ) @$ entry . get value ( ) ) ; } },threads got
subnormal values have a 0 implicit <PLACE_HOLDER> ; normal values have a 1 implicit <PLACE_HOLDER> .,answer . append ( subnormal ? __str__ : __str__ ) ;,values have
rejoin does not do paused <PLACE_HOLDER> .,list < string > rejoin cmd ln str = rejoin cmd ln . create command line ( ) ; string cmd line full = __str__ ; for ( string element : rejoin cmd ln str ) { cmd line full += __str__ + element ; } log . info ( cmd line full ) ; m_proc builder . command ( ) . clear ( ) ; m_proc builder . command ( ) . add all ( rejoin cmd ln str ) ; process proc = m_proc builder . start ( ) ; start = system . current time millis ( ) ;,rejoin do
make sure we do n't think anyone else is holding the <PLACE_HOLDER>,if ( got ) { if ( blackboard . get is locked ( ) ) { string msg = __str__ + service name + __str__ + object name + __str__ + ( ( d lock service ) service ) . get lock grantor id ( ) + __str__ + service . is lock grantor ( ) ; system . out . println ( __str__ + msg ) ; fail ( msg ) ; } blackboard . set is locked ( true ) ; long count = blackboard . get count ( ) ; system . out . println ( __str__ + count + __str__ + service name + __str__ + object name ) ; thread . sleep ( hold time ) ; blackboard . inc count ( ),anyone holding
no service for this class @$ must choose the <PLACE_HOLDER> by selection,p conf . as node list ( ) . get ( ) . stream ( ) . filter ( this :: not reserved provider key ) . for each ( provider specific conf -> { if ( ! provider specific . compare and set ( null @$ provider specific conf ) ) { throw new security exception ( __str__ + __str__ + provider specific . get ( ) . key ( ) + __str__ + provider specific conf . key ( ) ) ; } } ) ;,service choose
wait to make sure new deployment file takes <PLACE_HOLDER>,thread . sleep ( __num__ ) ;,file takes
note to translators : the stylesheet referred to an <PLACE_HOLDER> to the xsl syntax and indicated that it was defined by xsltc @$ but xstlc does not recognized the particular <PLACE_HOLDER> named . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,xstlc recognized
the user has selected a new <PLACE_HOLDER>,if ( node != null ) { this . target = node ; if ( node . get start node ( ) != null ) { session session = model . get singleton ( ) . get session ( ) ; list < context > contexts = session . get contexts for node ( node . get start node ( ) ) ; for ( context context : contexts ) { ctx names . add ( context . get name ( ) ) ; } } else if ( node . get context ( ) != null ) { ctx names . add ( node . get context ( ) . get name ( ) ) ; } },user selected
invalid <PLACE_HOLDER> is now since the session straddles the window cutoff <PLACE_HOLDER> .,expected stats . expiration time elapsed = now ; expected stats . execution time in window ms = __num__ * minute_in_millis ; expected stats . bg job count in window = __num__ ; expected stats . execution time in max period ms = __num__ * minute_in_millis ; expected stats . bg job count in max period = __num__ ; expected stats . session count in window = __num__ ; m quota controller . update execution stats locked ( __num__ @$ __str__ @$ input stats ) ; assert equals ( expected stats @$ input stats ) ; input stats . window size ms = expected stats . window size ms = __num__ * minute_in_millis ;,session straddles
exopackage builds <PLACE_HOLDER> to jar @$ otherwise @$ <PLACE_HOLDER> to raw .,dex store default dex store = exopackage mode . enabled for secondary dexes ( exopackage modes ) ? dex store . jar : dex store . raw ; dex split strategy dex split strategy = args . get minimize primary dex size ( ) ? dex split strategy . minimize_primary_dex_size : dex split strategy . maximize_primary_dex_size ; return new dex split mode ( args . get use split dex ( ) @$ dex split strategy @$ args . get dex compression ( ) . or else ( default dex store ) @$ args . get linear alloc hard limit ( ) @$ args . get dex group lib limit ( ) @$ args . get primary dex patterns ( ) @$ args . get primary dex classes file,exopackage builds
will be null when the feature to use minimized <PLACE_HOLDER> is disabled .,if ( minimized bitcode == null ) { return full bitcode ; } return minimized bitcode ;,feature use
proto source root . this ensures that protos can reference either the full <PLACE_HOLDER> or the short <PLACE_HOLDER> when including other protos .,command line . add all ( vector arg . of ( transitive imports ) . mapped ( new expand import args fn ( output directory @$ direct proto source roots ) ) ) ; if ( protos in direct dependencies != null ) { if ( ! protos in direct dependencies . is empty ( ) ) { command line . add all ( __str__ @$ vector arg . join ( __str__ ) . each ( protos in direct dependencies ) . mapped ( new expand to path fn with imports ( output directory @$ direct proto source roots ) ) ) ; } else { command line . add ( __str__ ) ; } },protos reference
start <PLACE_HOLDER> create <PLACE_HOLDER>,event = ( activiti entity event ) listener . get events received ( ) . get ( __num__ ) ; assert equals ( activiti event type . entity_created @$ event . get type ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get process instance id ( ) ) ; assert not equals ( process instance . get id ( ) @$ event . get execution id ( ) ) ; assert equals ( process instance . get process definition id ( ) @$ event . get process definition id ( ) ) ;,event create
if aod is showing @$ the ime should be hidden . however @$ sometimes the aod is considered hidden because it 's in the process of hiding @$ but it 's still being shown on screen . in that case @$ we want to continue hiding the ime until the windows have completed <PLACE_HOLDER> . this way @$ we know that the ime can,final boolean hide ime = win . is input method window ( ) && ( m aod showing || ! m default display policy . is window manager draw complete ( ) ) ; if ( hide ime ) { return true ; } final boolean show ime over keyguard = ime target != null && ime target . is visible lw ( ) && ( ime target . can show when locked ( ) || ! can be hidden by keyguard lw ( ime target ) ) ;,windows completed
so if the authorities equal the library internal <PLACE_HOLDER> @$ the developer forgot to set his application id,if ( __str__ . equals ( provider info . authority ) ) { throw new illegal state exception ( __str__ + __str__ ) ; } super . attach info ( context @$ provider info ) ;,authorities equal
sets the peer supported signature <PLACE_HOLDER> to use in km temporarily .,session . set peer supported signature algorithms ( supported sign algs ) ;,sets supported
if we 're in a chunk and the item spanned <PLACE_HOLDER>,if ( in block && special chunk ) { in block = false ; block size index = - __num__ ; block size position = - __num__ ; start_block ( ) ; special chunk = false ; },item spanned
target <PLACE_HOLDER> may not match connected <PLACE_HOLDER>,dc . send ( bb @$ p . get socket address ( ) ) ;,address match
for ssl by default @$ which automatically generates <PLACE_HOLDER> . this could be removed if all hosts were configured to contain a security domain with ssl enabled .,final path address https listener = undertow . append ( __str__ @$ __str__ ) . append ( __str__ @$ __str__ ) ; ops . add ( util . get empty operation ( model description constants . remove @$ https listener . to model node ( ) ) ) ;,which generates
if we have operation set persistent presence skip sending initial presence while login is executed @$ the operation set will take <PLACE_HOLDER> of it,if ( get operation set ( operation set persistent presence . class ) != null ) conf conn . set send presence ( false ) ;,set take
weird case : someone has given us their own custom i intent <PLACE_HOLDER> @$ and now they have someone else trying to send to it but of course this is n't really a pending intent @$ so there is no base intent @$ and the caller is n't supplying an intent ... but we never want to dispatch a null intent to a receiver,if ( intent == null ) { slog . wtf ( tag @$ __str__ ) ; intent = new intent ( intent . action_main ) ; },someone given
sep ; ascii ' 4 ' single quotes check header <PLACE_HOLDER>,parse setup setup = new parse setup ( xls_info @$ ( byte ) __num__ @$ true @$ parse setup . no_header @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ ctypes @$ null @$ null @$ null ) ;,quotes check
second call must not log another <PLACE_HOLDER>,assert that ( under test . is operational ( ) ) . is false ( ) ; assert that ( memory appender . events ) . extracting ( i logging event :: get level @$ i logging event :: get message ) . contains only once ( tuple ( level . info @$ __str__ ) ) ;,call log
fuzzy k means <PLACE_HOLDER>,throw new unsupported operation exception ( __str__ ) ;,k means
one shot <PLACE_HOLDER>,if ( timeto measure iterations == __num__ ) { long start = system . current time millis ( ) ; ksession . execute ( command factory . new insert elements ( arrays . as list ( data ) ) ) ; system . out . println ( __str__ + message + __str__ + ( system . current time millis ( ) - start ) ) ; assert true ( list . size ( ) > __num__ ) ; } else { long start = system . current time millis ( ) ; long end = start + timeto measure iterations ; int count = __num__ ; while ( system . current time millis ( ) < end ) { stateless kie session sess2 = create stateless knowledge session,one shot
tree indexes have a 40 byte <PLACE_HOLDER> per row .,isize . width min += tree_map_entry_overhead + tuple_ptr_size ; isize . width max += tree_map_entry_overhead + tuple_ptr_size ;,indexes have
just an arbitrary string which must uniquely identify the verticle <PLACE_HOLDER>,return __str__ ;,which identify
if root then we can ony create <PLACE_HOLDER> at home or public <PLACE_HOLDER>,when ( tree mocked . is root ( ) ) . then return ( true ) ; when ( tree mocked . get path ( ) ) . then return ( new directory ) ; when ( repository . find directory ( any string ( ) ) ) . then return ( lazy ) ; when ( repository . create repository directory ( tree mocked @$ new directory ) ) . then call real method ( ) ; assert null ( repository . create repository directory ( tree mocked @$ new directory ) . get path ( ) ) ;,root create
then it is an attribute or its not an attribute name nor operation name and the below invocation will throw a attribute not found <PLACE_HOLDER> .,result = do attribute operation ( mbsc @$ instance @$ sub command @$ attribute info ) ;,invocation throw
policy qualifiers must be rejected @$ since we do n't have any way to convey them back to the application . that 's the default @$ so no need to write <PLACE_HOLDER> .,builder params . set date ( params . date ( ) ) ;,need write
the following should have the same <PLACE_HOLDER> as the previous @$ since it has the same restrictions applied in reverse order .,s = open session ( ) ; s . get transaction ( ) . begin ( ) ; root criteria = s . create criteria ( order . class @$ __str__ ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; root criteria . create alias ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; result = ( order ) root criteria . unique result ( ) ; assert equals ( order1 . get order id ( ) @$ result . get order id ( ) ) ; assert equals ( __num__ @$ result,following have
res stores the first found abstract <PLACE_HOLDER>,method res = null ; for ( method mi : methods ) { if ( ! modifier . is abstract ( mi . get modifiers ( ) ) ) continue ; if ( mi . get annotation ( traits . implemented . class ) != null ) continue ; try { object . class . get method ( mi . get name ( ) @$ mi . get parameter types ( ) ) ; continue ; } catch ( no such method exception e ) { } if ( res != null ) return null ; res = mi ; },stores found
this format has no <PLACE_HOLDER> .,return __str__ ;,format has
only the exception case would have destroyed the proxy <PLACE_HOLDER> . we can safely proceed here .,repo . put entry in monitoring region map ( member @$ proxy monitoring region ) ; repo . put entry in notif region map ( member @$ proxy notification region ) ; try { if ( ! running ) { return ; } proxy factory . create all proxies ( member @$ proxy monitoring region ) ; management cache listener . mark ready ( ) ; notif listener . mark ready ( ) ; } catch ( exception e ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ e ) ; } throw new management exception ( e ) ; },case destroyed
have one cycle of successful calls to verify valid tracker clients returned . try <PLACE_HOLDER> balancing on this update state @$ need to update state before forcing the strategy .,tracker client resulttc = get tracker client ( strategy @$ request @$ new request context ( ) @$ __num__ @$ clients ) ; strategy . set strategy ( default_partition_id @$ partition degrader load balancer state . strategy . load_balance ) ; resulttc = get tracker client ( strategy @$ request @$ new request context ( ) @$ __num__ @$ clients ) ; assert not null ( resulttc @$ __str__ ) ; for ( int j = __num__ ; j < num_checks ; j ++ ) { cc list . add ( client1 . get call tracker ( ) . start call ( ) ) ; cc list . add ( client2 . get call tracker ( ) . start call ( ) ) ; } clock . add,returned try
completing the task will end the process <PLACE_HOLDER>,task service . complete ( task . get id ( ) ) ; assert process ended ( proc id ) ;,task end
drag & drop <PLACE_HOLDER>,recycler view drag drop manager drag drop manager = new recycler view drag drop manager ( ) ; drag drop manager . set dragging item shadow drawable ( ( nine patch drawable ) context . get resources ( ) . get drawable ( r . drawable . shadow_8dp ) ) ; recycler view . adapter adapter = new quick search adapter ( ) ; adapter . set has stable ids ( true ) ; adapter = drag drop manager . create wrapped adapter ( adapter ) ;,& drop
the line we 're about to add should not be added anymore @$ append three dots to previous one instead to indicate more <PLACE_HOLDER> is truncated,if ( current height + height > box height ) { if ( ! layouts . is empty ( ) ) { layouts . remove ( layouts . size ( ) - __num__ ) ; if ( last line . length ( ) >= __num__ ) { last line = last line . substring ( __num__ @$ last line . length ( ) - __num__ ) + __str__ ; } layouts . add ( new text layout ( last line @$ g . get font ( ) @$ g . get font render context ( ) ) ) ; } else { layouts . add ( layout ) ; current height += height ; } break ; } else { layouts . add ( layout ) ; last,dots indicate
as the annotation is legal on fields and methods only @$ javac itself will take <PLACE_HOLDER> of printing an error message for this .,return ;,itself take
wait @$ the client no longer has <PLACE_HOLDER> to the display .,if ( ! m window manager internal . is uid allowed on display ( cs . self reported display id @$ cs . uid ) ) { return input bind result . invalid_display_id ; },client has
... find <PLACE_HOLDER> that fit,if ( ( piece = pieces [ ix ] ) == null ) continue ; else list = pieces [ ix ] . shapes ( row @$ col ) ; for ( shape shape : list ) { if ( ( shape . bitmap & puzzle ) != __num__ ) continue ; long clone = puzzle | shape . bitmap ; int irow = row ; int icol = col / __num__ + __num__ ; next : while ( irow < __num__ ) { while ( icol < __num__ ) { if ( ( clone & mask [ irow ] [ icol ] ) == __num__ ) break next ; icol ++ ; } irow ++ ; icol = __num__ ; } entry entry ; pieces [ ix,... find
caution : fist delete tasks then jobs @$ as task has a foreign <PLACE_HOLDER> .,try { int job retention days = monitor task info . get default retention days ( ) ; int deleted tasks = dao_registry . get taskdao ( ) . delete records older than days ( job retention days ) ; int deleted jobs = dao_registry . get jobdao ( ) . delete records older than days ( job retention days ) ; log . info ( __str__ @$ deleted tasks @$ deleted jobs @$ job retention days ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; },tasks has
if we are using osvr get the eye <PLACE_HOLDER> here,if ( environment . getvr hardware ( ) instanceof osvr ) { ( ( osvr ) environment . getvr hardware ( ) ) . get eye info ( ) ; },osvr get
see if the qualifier expression is a type or <PLACE_HOLDER> name . javac does not provide the exact method required @$ so we first check if qualifier expression identifies a type @$ and if not @$ then we check to see if it identifies a <PLACE_HOLDER> .,type t = attr . attrib type ( ref . qualifier expression @$ env ) ; if ( t . is erroneous ( ) ) { if ( ref . member name == null ) { package symbol pck = elements . get package element ( ref . qualifier expression . to string ( ) ) ; if ( pck != null ) { return pck ; } else if ( ref . qualifier expression . has tag ( jc tree . tag . ident ) ) { tsym = env . encl class . sym ; member name = ( ( jc ident ) ref . qualifier expression ) . name ; } else return null ; } else { return null ; } } else {,expression identifies
so that other persistence context can also see new <PLACE_HOLDER>,employee = sfsb1 . get employee notx ( __num__ ) ;,context see
verify user from a group which has column family level <PLACE_HOLDER> can read all the data belonging to that family and group which has no <PLACE_HOLDER> ca n't read any data .,grant on table ( test_util @$ testgroup_1_name @$ table name @$ test_family @$ null @$ permission . action . read ) ; verify allowed ( testgroup1_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup1_user1 @$ scan family action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan family action for group with family level access ) ;,which has
function invoked 2 <PLACE_HOLDER> @$ cancel never,verify ( on next function @$ times ( __num__ ) ) . apply ( any ( ) ) ; verify ( subscription @$ never ( ) ) . cancel ( ) ;,function invoked
do n't emit anything .. allow configured spout wait <PLACE_HOLDER> to kick in,return ; log . error ( __str__ + get file progress ( reader ) + __str__ @$ e ) ; mark file as bad ( reader . get file path ( ) ) ;,spout wait
then the repository has a success <PLACE_HOLDER> and usernames are available,assert false ( usernames repository . is error ( ) ) ; assert true ( usernames repository . get ( ) . length == usernames . length ) ;,repository has
find the old generation which supports low memory <PLACE_HOLDER>,list iterator iter = pools . list iterator ( ) ; while ( iter . has next ( ) ) { memory poolmx bean p = ( memory poolmx bean ) iter . next ( ) ; if ( p . get type ( ) == memory type . heap && p . is usage threshold supported ( ) ) { mpool = p ; if ( trace ) { system . out . println ( __str__ + __str__ ) ; memory util . print memory pool ( mpool ) ; } break ; } } test listener listener = new test listener ( ) ; sensor listener l2 = new sensor listener ( ) ; notification emitter emitter = ( notification emitter ) mm ; emitter .,which supports
register the chat window menu bar <PLACE_HOLDER> .,container filter . put ( container . container_id @$ container . container_chat_menu_bar . getid ( ) ) ; bundle context . register service ( plugin component factory . class . get name ( ) @$ new otr plugin component factory ( container . container_chat_menu_bar ) @$ container filter ) ;,menu bar
but if statements and loops get <PLACE_HOLDER> automagically .,assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ; assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ; assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ;,statements get
last task to exit when shutdown release <PLACE_HOLDER>,int remaining = thread exit ( this @$ replace me ) ; if ( remaining == __num__ && is shutdown ( ) ) impl close ( ) ;,shutdown release
release compute engine startup method which will throw startup <PLACE_HOLDER>,compute engine . release startup ( ) ; while ( ce server . get status ( ) == monitored . status . down ) { },which throw
deque contains a <PLACE_HOLDER> that when true indicates that the value in that position of the window is missing .,return new aggregate function < t @$ integer > ( ) { private final array deque < boolean > queue = new array deque < > ( ) ; private int missing count = __num__ ; @ override public void remove left most ( ) { boolean removed missing value = queue . remove ( ) ; if ( removed missing value ) { missing count -- ; } } @ override public void add right most ( t new value ) { queue . add ( false ) ; } @ override public void add right most missing ( ) { queue . add ( true ) ; missing count ++ ; } @ override public integer get value ( ) { return queue . size (,deque contains
if this is a transactional remove <PLACE_HOLDER> @$ we will not have version information as it is only generated at commit so treat transactional remove <PLACE_HOLDER> as if the server is not versioned . if we have no storage then act as if the server is not versioned .,final boolean server is versioned = proxy result != null && proxy result . region is versioned ( ) && ! istx ( ) && get data policy ( ) . with storage ( ) ; if ( ! server is versioned && ! partial result ) { proxy result = null ; },transactional remove
if user typed the 'quit ' command @$ wait until the server closes the <PLACE_HOLDER> .,while ( ( line = command input stream . read line ( ) ) != null ) { if ( __str__ . equals ( line . to lower case ( ) ) ) { log . info ( __str__ ) ; channel . close ( ) ; break ; } message base msg = produce message base on user input ( line @$ ( short ) random . next int ( short . max_value ) ) ; if ( msg == null ) { continue ; } send ( msg ) ; },server closes
if the job is python shell job @$ the entry point class name is python gate <PLACE_HOLDER> server . otherwise @$ the entry point class of python job is python driver,if ( entry point class == null ) { entry point class = __str__ ; } if ( jar file path == null ) { throw new illegal argument exception ( __str__ ) ; } jar file = get jar file ( jar file path ) ;,name python
<PLACE_HOLDER>ut that is not known until after a 's creation . a 's creation occurs within <PLACE_HOLDER> @$ which requires <PLACE_HOLDER> to have an outer field in order to access a 's capturing field for i. <PLACE_HOLDER> 's creation therefore requires the outer field to <PLACE_HOLDER>e passed as an outer argument . <PLACE_HOLDER>ecause of the cascading effects of the statements in this test and,resolve source ( __str__ @$ __str__ + __str__ + __str__ ) ; class instance creation b create = ( class instance creation ) nodes by type . get ( kind . class_instance_creation ) . get ( __num__ ) ; expression outer arg = b create . get expression ( ) ; assert true ( outer arg instanceof simple name ) ; variable element var = tree util . get variable element ( outer arg ) ; assert not null ( var ) ; assert equals ( __str__ @$ element util . get name ( var ) ) ;,creation requires
for remote output port @$ it 's possible that multiple processors are connected . in that case @$ the received flow file is cloned and passed to each connection . so we need to create multiple data set <PLACE_HOLDER> .,final list < connection status > connections = nifi flow . get outgoing connections ( port process id ) ; if ( connections == null || connections . is empty ( ) ) { logger . warn ( __str__ @$ new object [ ] { event } ) ; return ; },data set
owid with the given value found <PLACE_HOLDER> searching now for row id ...,if ( pos >= __num__ ) { key = compressed owids [ pos ] ; if ( row id < row ids [ key . from index ] || row id > row ids [ key . to index - __num__ ] ) { return false ; } if ( arrays . binary search ( row ids @$ key . from index @$ key . to index @$ row id ) >= __num__ ) { return true ; } },owid found
rather than fight it @$ let root have an <PLACE_HOLDER>,nodes . put ( __str__ @$ root ) ; nodes . put ( root zookeeper @$ root ) ; root . add child ( proc child zookeeper ) ; nodes . put ( proc zookeeper @$ proc data node ) ; proc data node . add child ( quota child zookeeper ) ; nodes . put ( quota zookeeper @$ quota data node ) ;,root have
compressed streams do not update <PLACE_HOLDER> until after sync marker @$ so we can hit eof here .,try { close ( ) ; } catch ( final exception e2 ) { log . warn ( e . get message ( ) @$ e2 ) ; } return false ;,streams update
means get all <PLACE_HOLDER> @$ so no need to add filter,if ( db name != null && ! __str__ . equals ( db name ) ) { append simple condition ( filter builder @$ __str__ @$ new string [ ] { db name } @$ parameter vals ) ; },means get
the client must not send the <PLACE_HOLDER> .,assert false ( trailers latch . await ( __num__ @$ time unit . seconds ) ) ;,client send
comparing with itself returns <PLACE_HOLDER>,assert true ( comp . compare ( p1 @$ p1 ) == __num__ ) ;,itself returns
if the flushed requests has <PLACE_HOLDER> @$ we should propagate it also and fail the checkpoint,check and propagate async error ( ) ;,requests has
functions that are n't ctors or interfaces have no instance <PLACE_HOLDER> .,return null ;,functions have
3 regions 3 replicas write some <PLACE_HOLDER> to the table,table ht = test_util . get connection ( ) . get table ( table name ) ; list < put > puts = new array list < > ( ) ; byte [ ] qualifier = bytes . to bytes ( __str__ ) ; put put = new put ( new byte [ ] { ( byte ) __str__ } ) ; put . add column ( cf @$ qualifier @$ bytes . to bytes ( __str__ ) ) ; puts . add ( put ) ; put = new put ( new byte [ ] { ( byte ) __str__ } ) ; put . add column ( cf @$ qualifier @$ bytes . to bytes ( __str__ ) ) ; puts . add ( put ),regions write
do init transactions calls sender.do once three <PLACE_HOLDER> @$ only two requests are sent @$ so we should only poll twice,verify ( client @$ times ( __num__ ) ) . poll ( eq ( retry_backoff_ms ) @$ any long ( ) ) ;,transactions calls
the user might cancel the file <PLACE_HOLDER> dialog in this case we should not close the test plan,if ( gui package . is dirty ( ) ) { return false ; },user cancel
remove env var that would default child jvm to use parent 's <PLACE_HOLDER> as default . child jvm would use default <PLACE_HOLDER> for a hadoop client,if ( hadoop mem == __num__ ) { variables . remove ( hadoop_mem_key ) ; } else { console . print info ( __str__ + hadoop mem + __str__ ) ; variables . put ( hadoop_mem_key @$ string . value of ( hadoop mem ) ) ; },jvm use
note : the above limiter could block and delay the caller 's <PLACE_HOLDER>,callback . on error ( t ) ; latch . count down ( ) ;,limiter block
seek with no offset epoch requires no validation no matter <PLACE_HOLDER> the current leader is,state . seek unvalidated ( tp0 @$ new subscription state . fetch position ( __num__ @$ optional . of ( __num__ ) @$ new metadata . leader and epoch ( broker1 @$ optional . of ( __num__ ) ) ) ) ; assert false ( state . has valid position ( tp0 ) ) ; assert true ( state . awaiting validation ( tp0 ) ) ; state . seek unvalidated ( tp0 @$ new subscription state . fetch position ( __num__ @$ optional . empty ( ) @$ new metadata . leader and epoch ( broker1 @$ optional . of ( __num__ ) ) ) ) ; assert true ( state . has valid position ( tp0 ) ) ; assert false ( state . awaiting validation,leader is
this rename never overwrites the <PLACE_HOLDER> so files deleted and collected are irrelevant .,return create rename result ( fsd @$ renameiip @$ false @$ null ) ;,rename overwrites
the empty class has a default <PLACE_HOLDER> .,assert that ( class info . get ( ) . declared members ( ) ) . has size ( __num__ ) ; assert that ( class info . get ( ) . declared members ( ) . iterator ( ) . next ( ) . member name ( ) ) . is equal to ( __str__ ) ;,class has
if text field equals only one <PLACE_HOLDER> @$ use that <PLACE_HOLDER> .,equate row object match = get match from table ( equate from filter ) ; if ( match != null ) { return match ; } return null ;,field equals
measure the component with undefined width spec @$ as the contents of the hscroll have unlimited horizontal <PLACE_HOLDER> .,child component tree . set root and size spec ( content props @$ size spec . make size spec ( __num__ @$ unspecified ) @$ height spec @$ content size ) ; content props . measure ( context @$ size spec . make size spec ( __num__ @$ unspecified ) @$ height spec @$ content size ) ; measured width = content size . width ; measured height = content size . height ; measured component width . set ( measured width ) ; measured component height . set ( measured height ) ;,contents have
when errors are not stored alongside values @$ <PLACE_HOLDER> errors that are recovered from do not make the parent <PLACE_HOLDER>,if ( supports transient exceptions ) { assert that error info ( error info ) . is transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is not null ( ) ; } else { assert that error info ( error info ) . is not transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is null ( ) ; },errors make
comment must immeiately precede the parameter <PLACE_HOLDER>,parm entry . comment ( token . comment ) ; param attribute ( parm entry ) ; parm entry . type ( param type spec ( entry ) ) ; parm entry . name ( token . name ) ; match ( token . identifier ) ; if ( isnt in list ( entry . parameters ( ) @$ parm entry . name ( ) ) ) entry . add parameter ( parm entry ) ;,comment precede
when generating transform <PLACE_HOLDER> @$ the authority of the output path will be determined based on this hostname configuration .,server configuration . set ( property key . master_hostname @$ __str__ ) ; list < transform plan > plans = m catalog . get transform plan ( db name @$ table name @$ transform_definition ) ; map < string @$ layout > transformed layouts = maps . new hash map with expected size ( plans . size ( ) ) ; plans . for each ( plan -> transformed layouts . put ( plan . get base layout ( ) . get spec ( ) @$ plan . get transformed layout ( ) ) ) ; m catalog . complete transform table ( noop journal context . instance @$ db name @$ table name @$ transform_definition . get definition ( ) @$ transformed layouts ) ; table .,generating transform
reduce ipc client connection retry <PLACE_HOLDER> and interval time,configuration client conf = new configuration ( false ) ; client conf . set int ( common configuration keys . ipc_client_connect_max_retries_key @$ __num__ ) ; client conf . set int ( common configuration keys . ipc_client_connect_retry_interval_key @$ __num__ ) ;,connection retry
ok now we are sure to have both a valid post and show gutenberg flag @$ let 's start the editing session <PLACE_HOLDER>,create post editor analytics session tracker ( m show gutenberg editor @$ m edit post repository . get post ( ) @$ m site @$ m is new post ) ;,"s" start
a function parameter can not be replaced with a direct inlined value if it is referred to by an inner function . the inner function can out live the call we are replacing @$ so inner function must capture a unique <PLACE_HOLDER> . this approach does not work within loop bodies so those are forbidden elsewhere .,if ( n . is function ( ) ) { in inner function = true ; },function capture
case 2 : on api 23 @$ we double scheduler workers because <PLACE_HOLDER> scheduler prefers batching . so is the work is periodic @$ we only need to execute it once per interval . also potential bugs in the platform may cause a <PLACE_HOLDER> to run more than once .,if ( m work spec . is periodic ( ) || m work spec . is backed off ( ) ) { long now = system . current time millis ( ) ; boolean is first run = m work spec . period start time == __num__ ; if ( ! is first run && now < m work spec . calculate next run time ( ) ) { logger . get ( ) . debug ( tag @$ string . format ( __str__ + __str__ @$ m work spec . worker class name ) ) ; resolve ( true ) ; return ; } },bugs cause
this region does not overlap the <PLACE_HOLDER> @$ so skip it .,if ( ! range . overlaps ( byte key range . of ( last end key @$ response end key ) ) ) { last offset = response offset ; last end key = response end key ; continue ; },region overlap
reuse test generic writable inner classes to test <PLACE_HOLDER> that also implement configurable .,foo generic writable generic = new foo generic writable ( ) ; generic . set conf ( conf ) ; baz baz = new baz ( ) ; generic . set ( baz ) ; baz result = serialization test util . test serialization ( conf @$ baz ) ; assert equals ( baz @$ result ) ; assert not null ( result . get conf ( ) ) ;,reuse test
after emitting each pane @$ it will continue accumulating the elements so that each approximation includes <PLACE_HOLDER> of the previous data in addition to the newly arrived data .,p collection < table row > speculative results = flow info . apply ( __str__ @$ window . < kv < string @$ integer > > into ( fixed windows . of ( duration . standard minutes ( window duration ) ) ) . triggering ( repeatedly . forever ( after processing time . past first element in pane ( ) . plus delay of ( one_minute ) ) ) . accumulating fired panes ( ) . with allowed lateness ( one_day ) ) . apply ( new total flow ( __str__ ) ) ;,approximation includes
there is an existing user who only has unsupported <PLACE_HOLDER> in methods,if ( task . is successful ( ) && last signed in providers . is empty ( ) && ! methods . is empty ( ) ) { return tasks . for exception ( new firebase ui exception ( error codes . developer_error ) ) ; },who has
agree on the consensus if this node does not see the <PLACE_HOLDER> at all or if this node sees the <PLACE_HOLDER> however the timeout is out,if ( found client == null || is timeout exceeded ( found client @$ watchdog client retry timeout ) ) { client disconnected consensus = true ; },node sees
the master deletes the <PLACE_HOLDER> when it receives this exception .,throw new not serving region exception ( __str__ + encoded name + __str__ ) ;,master deletes
poi produced the infinity <PLACE_HOLDER> when it should have produced the degrees <PLACE_HOLDER> .,return __num__ ; case __num__ :,poi produced
file has stopped <PLACE_HOLDER> ...,if ( ! properties file exists ) { f last modified = - __num__ ; f xalan properties = null ; },file stopped
first copy the children as the call to copy.add will modify the <PLACE_HOLDER> we 're iterating on,enumeration < ? > enum from = node . children ( ) ; list < j meter tree node > tmp = new array list < > ( ) ; while ( enum from . has more elements ( ) ) { j meter tree node child = ( j meter tree node ) enum from . next element ( ) ; tmp . add ( child ) ; } for ( j meter tree node j meter tree node : tmp ) { copy . add ( j meter tree node ) ; } tree model . insert node into ( copy @$ target @$ index ++ ) ; nodes for removal . add ( node ) ; paths to select [ path position ++ ] =,call modify
now @$ both the close action and a grab for an write page <PLACE_HOLDER> is waiting for our first thread . when we release that <PLACE_HOLDER> @$ we should see that either close completes and our second thread @$ the one b<PLACE_HOLDER>ed on the write <PLACE_HOLDER> @$ gets an exception @$ or we should see that the second thread gets the <PLACE_HOLDER> @$ and,unlock latch . count down ( ) ;,thread gets
now do get <PLACE_HOLDER> on created mapfile .,map file . reader reader = new map file . reader ( qualified dir name @$ conf ) ; try { assert equals ( null @$ reader . mid key ( ) ) ; } finally { reader . close ( ) ; },now get
not an array of primitives throws a class cast <PLACE_HOLDER> if rhs is not an array,append ( ( object [ ] ) lhs @$ ( object [ ] ) rhs @$ comparator ) ;,array throws
update processor config : note that we want to run a generation update even if the processor config has no <PLACE_HOLDER> ; we still need to handle <PLACE_HOLDER> in the exported tables,update processor config ( connectors ) ; if ( ! requires new export generation ) { if ( m_generation . get ( ) != null ) { m_generation . get ( ) . update generation id ( catalog context . m_gen id ) ; } export log . info ( __str__ ) ; return ; },config has
as it turns out @$ starting a new media player on the completion of a previous player ends up slightly overlapping the two <PLACE_HOLDER> @$ so slightly delaying the start of the next player gives a better user experience,if ( m next player != null ) { system clock . sleep ( __num__ ) ; m next player . start ( ) ; },player ends
make cdi work in containers with implicit archive scanning <PLACE_HOLDER>,write optional ( output folder @$ new supporting file ( __str__ @$ __str__ @$ __str__ ) ) ;,containers scanning
fallback if file not a valid <PLACE_HOLDER> .,changes . put ( file @$ file ) ;,fallback file
the file should contain just 1 <PLACE_HOLDER> :,assert equals ( __num__ @$ straus . get len ( ) ) ; har file system . close ( ) ; local fs . delete ( tmp path @$ true ) ;,file contain
check if the sequence at the current position matches locale <PLACE_HOLDER> .,for ( int i = __num__ ; i < __num__ ; i ++ ) { int digit str len = locale digits [ i ] . length ( ) ; if ( str . region matches ( start @$ locale digits [ i ] @$ __num__ @$ digit str len ) ) { dec val [ __num__ ] = i ; return digit str len ; } },sequence matches
inflater likes a <PLACE_HOLDER> of slack,long size = get entry size ( jzentry ) + __num__ ;,inflater likes
user should not see anything so give unsatisfiable <PLACE_HOLDER>,return process payload builder . process definitions ( ) . with process definition key ( __str__ + uuid . randomuuid ( ) . to string ( ) ) . build ( ) ;,anything give
check for region destroyed <PLACE_HOLDER> from server .,client1 . invoke ( new cache serializable runnable ( __str__ ) { @ override public void run2 ( ) throws cache exception { region local region = get cache ( ) . get region ( __str__ + regions [ __num__ ] ) ; if ( local region != null ) { wait . pause ( __num__ * __num__ ) ; cq query [ ] cqs = get cache ( ) . get query service ( ) . get cqs ( ) ; if ( cqs != null && cqs . length > __num__ ) { assert true ( cqs [ __num__ ] . is closed ( ) ) ; } assert null ( __str__ + __str__ @$ get cache ( ) . get region ( __str__ + regions,check destroyed
some legacy mode devices do not support <PLACE_HOLDER> off,if ( is hardware level limited or better ( ) ) { check true for key ( key @$ __str__ @$ modes list . contains ( camera metadata . control_af_mode_off ) ) ; },devices support
no cluster in cfg : get the default <PLACE_HOLDER>,if ( ! clusters . contains field ( i cluster name ) ) cfg = clusters . field ( all_wildcard ) ; else cfg = clusters . field ( i cluster name ) ;,cluster get
for job props @$ deletion in local folder means <PLACE_HOLDER> from ancestor folder and reschedule .,load new common config and handle new job ( path @$ job scheduler . action . reschedule ) ; return ;,deletion means
spellcheck dictionary contains <PLACE_HOLDER> and general accentuation,assert equals ( __num__ @$ rule . match ( lang tool . get analyzed sentence ( __str__ ) ) . length ) ;,dictionary contains
see if the alias has a lateral <PLACE_HOLDER> . if so @$ chain the lateral <PLACE_HOLDER> operator on,list < ast node > lateral views = alias to lateral views . get ( alias ) ; if ( lateral views != null ) { operator op = e . get value ( ) ; for ( ast node lateral view tree : alias to lateral views . get ( alias ) ) { op = gen lateral view plan ( qb @$ op @$ lateral view tree ) ; } e . set value ( op ) ; },alias has
remove any provided <PLACE_HOLDER> that do not exist in this segment .,for ( string column name : immutable list . copy of ( column names ) ) { if ( index . get column holder ( column name ) == null ) { column names . remove ( column name ) ; } },any provided
propagate tracing headers @$ so remote service can use current <PLACE_HOLDER> as its parent,map < string @$ list < string > > tracing headers = tracing headers ( tracer @$ current span ) ; map < string @$ list < string > > outbound headers = tracer provider . map ( provider -> provider . update outbound headers ( current span @$ tracer @$ parent span . or else ( null ) @$ tracing headers @$ inbound headers ) ) . or else ( tracing headers ) ;,service use
service config returns a different health check <PLACE_HOLDER> .,resolution attrs = attrs with health check service ( __str__ ) ; resolved addresses result2 = resolved addresses . new builder ( ) . set addresses ( resolved address list ) . set attributes ( resolution attrs ) . build ( ) ; hc lb event delivery . handle resolved addresses ( result2 ) ; in order . verify ( orig lb ) . handle resolved addresses ( result2 ) ;,config returns
since <PLACE_HOLDER> are always running on a single host keep the resolution timeout low as otherwise people running with strange network configurations will see very slow <PLACE_HOLDER>,conf . put ( property key . network_host_resolution_timeout_ms @$ __str__ ) ;,people see
hint arrow has no <PLACE_HOLDER> @$ and always returns the current <PLACE_HOLDER>,if ( is in agility arena ( ) ) { world point new ticket position = client . get hint arrow point ( ) ; world point old tick position = last arena ticket position ; last arena ticket position = new ticket position ; if ( old tick position != null && new ticket position != null && ( old tick position . getx ( ) != new ticket position . getx ( ) || old tick position . gety ( ) != new ticket position . gety ( ) ) ) { log . debug ( __str__ @$ old tick position @$ new ticket position ) ; if ( config . notify agility arena ( ) ) { notifier . notify ( __str__ ) ; },arrow has
okay @$ we have the <PLACE_HOLDER> . now have the agent do the restore .,stage . close ( ) ; m backup data = parcel file descriptor . open ( m backup data name @$ parcel file descriptor . mode_read_only ) ; m new state = parcel file descriptor . open ( m new state name @$ parcel file descriptor . mode_read_write | parcel file descriptor . mode_create | parcel file descriptor . mode_truncate ) ;,now have
0 : incorrect stats since hbm.xml ca n't enable natural id <PLACE_HOLDER>,assert equals ( __num__ @$ session factory ( ) . get statistics ( ) . get natural id query execution count ( ) ) ;,stats enable
same dead node <PLACE_HOLDER> of the time .,int max try = config . get node repetitions ( ) + __num__ ; if ( max try < __num__ ) { max try = __num__ ; } return new ketama iterator ( k @$ max try @$ get ketama nodes ( ) @$ hash alg ) ;,dead node
make a copy of the graph to avoid concurrency problems . graph manipulations are not thread safe @$ and another thread can concurrently inline this <PLACE_HOLDER> .,final structured graph graph = ( structured graph ) method . compilation info . get graph ( ) . copy ( debug ) ; try ( debug context . scope s = debug . scope ( __str__ @$ graph @$ method @$ this ) ) { try { try ( indent in = debug . log and indent ( __str__ @$ method ) ) { boolean inlined = false ; for ( invoke invoke : graph . get invokes ( ) ) { if ( invoke instanceof invoke node ) { throw vm error . should not reach here ( __str__ + invoke . call target ( ) . target method ( ) . format ( __str__ ) + __str__ + ( graph . method ( ) ==,thread inline
first heartbeat which schedules first <PLACE_HOLDER> .,localizer heartbeat response response = spy service . heartbeat ( stat ) ; assert equals ( __str__ @$ localizer action . live @$ response . get localizer action ( ) ) ;,which schedules
reset the flow scope 's syntactic scope to the function block @$ rather than the function node <PLACE_HOLDER> . this allows pulling out local vars from the function by name to verify their types .,if ( cfg root . is function ( ) ) { return scope = rtn state . get in ( ) . with syntactic scope ( scope creator . create scope ( cfg root . get last child ( ) ) ) ; } else { return scope = rtn state . get in ( ) ; },function node
the write <PLACE_HOLDER> to use in the states .,write options write options = null ; linked hash map < string @$ rocksdb keyed state backend . rocks db kv state info > kv state information = new linked hash map < > ( ) ; rocksdb db = null ; abstract rocksdb restore operation restore operation = null ; rocks db ttl compact filters manager ttl compact filters manager = new rocks db ttl compact filters manager ( enable ttl compaction filter @$ ttl time provider ) ; resource guard rocksdb resource guard = new resource guard ( ) ; snapshot strategy < k > snapshot strategy ; priority queue set factory priority queue factory ; rocksdb serialized composite key builder < k > shared rocks key builder ;,the write
instance klass array is sorted by name . do binary <PLACE_HOLDER>,int low = __num__ ; int high = tmp klasses . length - __num__ ; int mid = - __num__ ; while ( low <= high ) { mid = ( low + high ) > > __num__ ; instance klass mid val = tmp klasses [ mid ] ; int cmp = mid val . get name ( ) . as string ( ) . compare to ( class name ) ; if ( cmp < __num__ ) { low = mid + __num__ ; } else if ( cmp > __num__ ) { high = mid - __num__ ; } else { return tmp klasses [ mid ] ; } },klass do
currently @$ this will always be null because the manifest does n't have any useful <PLACE_HOLDER>,if ( jersey version == null ) jersey version = __str__ ;,manifest have
stop commit <PLACE_HOLDER> .,return true ;,stop commit
style rules should always have the lowest <PLACE_HOLDER> .,return - __num__ ;,rules have
note : for plugins that depend on other plugin artifacts the plugin realm contains more than one plugin <PLACE_HOLDER> . however @$ only the first <PLACE_HOLDER> is of interest .,if ( ! first descriptor ) { return ; } first descriptor = false ; if ( ! plugin artifact . get group id ( ) . equals ( plugin descriptor . get group id ( ) ) ) { errors . add ( __str__ + plugin descriptor . get group id ( ) ) ; } if ( ! plugin artifact . get artifact id ( ) . equals ( plugin descriptor . get artifact id ( ) ) ) { errors . add ( __str__ + plugin descriptor . get artifact id ( ) ) ; } if ( ! plugin artifact . get base version ( ) . equals ( plugin descriptor . get version ( ) ) ) { errors . add ( __str__,realm contains
this is a direct call to a method that the static analysis did not see as invoked . this can happen when the receiver is always null . in most cases @$ the method profile also has a length of 0 and the below code to kill the invoke would trigger . but not all methods have <PLACE_HOLDER> @$ for example methods with manually,if ( call target . invoke kind ( ) . is direct ( ) && ! ( ( hosted method ) call target . target method ( ) ) . get wrapped ( ) . is simply implementation invoked ( ) ) { unreachable invoke ( graph @$ invoke @$ call target ) ; continue ; },methods have
synchronously end the animation @$ jumping to the end state . animator set has synchronous listener <PLACE_HOLDER> on all supported ap is .,if ( ! animated ) { current animation . end ( ) ; },set has
null implies a 'thunk ' <PLACE_HOLDER>,return instruction != null ;,null implies
let super do its <PLACE_HOLDER> first,super . setup view interceptors ( view ) ; add view serialization interceptor ( view ) ;,super do
make sure link properties represents the latest private dns status . this does not need to be done before update dnses because the link properties are not the source of the private dns <PLACE_HOLDER> . update dnses will fetch the private dns <PLACE_HOLDER> from dns manager .,m dns manager . update private dns status ( net id @$ new lp ) ; if ( is default network ( network agent ) ) { handle apply default proxy ( new lp . get http proxy ( ) ) ; } else { update proxy ( new lp @$ old lp ) ; } update wake on lan ( new lp ) ;,dnses fetch
did not find a child to receive the event . assign the pointer to the least recently added <PLACE_HOLDER> .,if ( new touch target == null && m first touch target != null ) { new touch target = m first touch target ; while ( new touch target . next != null ) { new touch target = new touch target . next ; } new touch target . pointer id bits |= id bits to assign ; },pointer added
now do a <PLACE_HOLDER> @$ but cancel it in the middle,vm0 . invoke ( ( ) -> { count down latch rebalancing cancelled = new count down latch ( __num__ ) ; count down latch rebalancing finished = new count down latch ( __num__ ) ; internal resource manager manager = get cache ( ) . get internal resource manager ( ) ; internal resource manager . set resource observer ( new resource observer adapter ( ) { @ override public void rebalancing or recovery started ( region region ) { try { rebalancing cancelled . await ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; } } @ override public void rebalancing or recovery finished ( region region ) { rebalancing finished . count,now do
this function releases one sync <PLACE_HOLDER> only .,return __num__ ;,function releases
existing write <PLACE_HOLDER> against the caller 's val<PLACE_HOLDER> list .,if ( part == null ) { return null ; } if ( mt == null ) { throw new invalid object exception ( __str__ ) ; },existing write
should not throw <PLACE_HOLDER>,compaction latch . count down ( ) ; latch . count down ( ) ; for ( scan thread thread : scan threads ) { thread . join ( ) ; },not throw
note that we are always delivering a small <PLACE_HOLDER> to the transport here which may incur transport framing overhead as it may be sent separately to the contents of the grpc frame . the final <PLACE_HOLDER> may not be completely written because we do not flush the last buffer . do not report the last <PLACE_HOLDER> as sent .,sink . deliver frame ( writeable header @$ false @$ false @$ messages buffered - __num__ ) ; messages buffered = __num__ ;,note report
end enabled <PLACE_HOLDER>,break ;,end enabled
counting source.unbounded has very good splitting <PLACE_HOLDER>,assert that ( initial inputs @$ has size ( num splits ) ) ; int read per split = __num__ ; int total size = num splits * read per split ; set < long > expected outputs = contiguous set . create ( range . closed open ( __num__ @$ ( long ) total size ) @$ discrete domain . longs ( ) ) ; collection < long > read items = new array list < > ( total size ) ; for ( committed bundle < ? > initial input : initial inputs ) { committed bundle < unbounded source shard < long @$ ? > > shard bundle = ( committed bundle < unbounded source shard < long @$ ? > > ) initial input,source.unbounded has
this method always throws activation <PLACE_HOLDER>,user service . activate user ( __str__ ) ;,method throws
verify that the vertices are all in the same slot sharing <PLACE_HOLDER>,slot sharing group group1 ; slot sharing group group2 ;,vertices sharing
cache the label field @$ so that if it contains inlined <PLACE_HOLDER> then it 's free . otherwise cache the dynamic <PLACE_HOLDER> in another data structure and point into it .,long label field = node cursor . get label field ( ) ; boolean has inlined labels = ! node labels field . field points to dynamic record of labels ( node cursor . get label field ( ) ) ; if ( labels == null ) { has inlined labels = true ; label field = no_labels_field . long value ( ) ; },cache cache
if its completed no further <PLACE_HOLDER>,if ( stage steps collection completed ) { return ; },its completed
call resource monitor <PLACE_HOLDER> so that it does all validations .,new health monitor ( deployment . get systemsettings ( ) @$ new dummy snmp trap sender ( ) ) ;,resource monitor
0 xc 1 a 5 f <PLACE_HOLDER> e @$ 0 x 40358874 @$ 0 x 64 d 4 ef 0 d @$ 0 xc 0089309,masm . subsd ( xmm5 @$ xmm2 ) ; masm . andl ( rdx @$ __num__ ) ; masm . shrl ( rdx @$ __num__ ) ; masm . movdqu ( xmm0 @$ new amd64 address ( r11 @$ rdx @$ amd64 address . scale . times1 @$ - __num__ ) ) ; masm . movdqu ( xmm4 @$ record external address ( crb @$ coeff16 ) ) ;,5 f
we want to be compatible back to gingerbread @$ but surface <PLACE_HOLDER> was n't introduced until honeycomb . since the interface can not use a surface <PLACE_HOLDER> @$ if the developer wants to display a preview we must use a surface holder . if the developer does n't want to display a preview we use a surface <PLACE_HOLDER> if we are running at least,try { if ( build . version . sdk_int >= build . version_codes . honeycomb ) { m camera . set preview texture ( null ) ; } else { m camera . set preview display ( null ) ; } } catch ( exception e ) { log . e ( tag @$ __str__ + e ) ; },interface use
client will always see 'initiating ' <PLACE_HOLDER> first .,if ( request . get options ( ) . get run async ( ) ) { return new backup status ( backup id @$ backup state . initiating ) ; },client see
check if its instance of <PLACE_HOLDER> @$ in that the case throw <PLACE_HOLDER> exists exception .,if ( ind instanceof index ) { if ( remotely originated ) { return ( index ) ind ; } throw new index name conflict exception ( string . format ( __str__ @$ index name ) ) ; } future task < index > old index future task = ( future task < index > ) ind ; index index = null ; boolean interrupted = false ; try { if ( old index future task == null ) { index future task . run ( ) ; index = index future task . get ( ) ; if ( index != null ) { this . indexes . put ( index task @$ index ) ; partitioned index pr index = ( partitioned index ) index ;,case throw
stream 1 receives another <PLACE_HOLDER>,buffer = create message frame ( fake message ) ; frame handler ( ) . data ( false @$ __num__ @$ buffer @$ message frame length ) ; verify ( frame writer @$ timeout ( time_out_ms ) ) . window update ( eq ( __num__ ) @$ eq ( ( long ) __num__ * message frame length ) ) ;,stream receives
the anonymous click listener since that changes the model state @$ instead our anonymous click listener should use the hash <PLACE_HOLDER> of the user 's click listener,model click listener model click listener = new model click listener ( ) ; view click listener view click listener = new view click listener ( ) ; test controller controller = new test controller ( ) ; adapter data observer observer mock = mock ( adapter data observer . class ) ; controller . get adapter ( ) . register adapter data observer ( observer mock ) ; model with click listener_ model = new model with click listener_ ( ) ; controller . set model ( model ) ; controller . request model build ( ) ; verify ( observer mock ) . on item range inserted ( eq ( __num__ ) @$ eq ( __num__ ) ) ; model = new model with click listener_,listener use
transaction log <PLACE_HOLDER>,log provider . raw message matcher ( ) . assert contains ( __str__ ) ; log provider . raw message matcher ( ) . assert contains ( __str__ ) ; management service . shutdown ( ) ;,transaction log
do an update @$ which will pick up the large <PLACE_HOLDER>,region . put ( __num__ @$ __num__ ) ;,which pick
for tabs above the focused tab move <PLACE_HOLDER> up to 0 .,if ( i < focus index ) { add animation ( set @$ tab @$ scroll_offset @$ tab . get scroll offset ( ) @$ tab . get scroll offset ( ) - m height - spacing @$ tab_focused_animation_duration @$ __num__ ) ; } else if ( i > focus index ) { float covering tab position = layout tab . gety ( ) ; float distance to border = math utils . clamp ( m height - covering tab position @$ __num__ @$ m height ) ; float delay = tab_focused_max_delay * distance to border / m height ; add animation ( set @$ tab @$ y_in_stack_offset @$ tab . gety in stack offset ( ) @$ tab . gety in stack offset ( ) + m,tabs move
verify that server 1 's event queue has the default <PLACE_HOLDER>,server1 . invoke ( ( ) -> { internal cache cache = cluster startup rule . get cache ( ) ; async event queue queue = cache . get async event queue ( __str__ ) ; assert that ( queue . get batch size ( ) ) . is equal to ( gateway sender . default_batch_size ) ; assert that ( queue . get batch time interval ( ) ) . is equal to ( default_batch_time_interval ) ; assert that ( queue . get maximum queue memory ( ) ) . is equal to ( gateway sender . default_maximum_queue_memory ) ; } ) ; gfsh . execute and assert that ( __str__ + altered_batch_size + __str__ + altered_batch_time_interval + __str__ + altered_maximum_queue_memory ) . status is success (,queue has
if needed @$ update the maximum stack <PLACE_HOLDER> and number of locals @$ and stack map frames .,if ( current basic block != null ) { if ( compute == compute_all_frames || compute == compute_inserted_frames ) { current basic block . frame . execute ( opcode @$ __num__ @$ fieldref symbol @$ symbol table ) ; } else { int size ; char first desc char = descriptor . char at ( __num__ ) ; switch ( opcode ) { case opcodes . getstatic : size = relative stack size + ( first desc char == __str__ || first desc char == __str__ ? __num__ : __num__ ) ; break ; case opcodes . putstatic : size = relative stack size + ( first desc char == __str__ || first desc char == __str__ ? - __num__ : - __num__ ) ; break ; case,maximum stack
add <PLACE_HOLDER>@$<PLACE_HOLDER> should returns <PLACE_HOLDER>@$<PLACE_HOLDER>,try { inc ( usage @$ suffix @$ resource . new instance ( __num__ @$ __num__ ) @$ label ) ; check ( __num__ @$ __num__ @$ get ( usage @$ suffix @$ label ) ) ; } catch ( no such method exception e ) { },1 returns
creation lock <PLACE_HOLDER>,post destroy action ( ) ;,creation lock
zk peek lock interrupt <PLACE_HOLDER> expected,assert true ( e instanceof zk release lock interrupt exception ) ;,peek lock
each interpreter group has one <PLACE_HOLDER>,for ( interpreter group interpreter group : interpreter setting . get all interpreter groups ( ) ) { assert equals ( __num__ @$ interpreter group . get session num ( ) ) ; },group has
do the carry out <PLACE_HOLDER>,instructions . add ( reil helpers . create and ( base offset ++ @$ byte size @$ __str__ @$ byte size @$ is zero condition seven @$ byte size @$ shifter carry out tmp1 ) ) ; instructions . add ( reil helpers . create bsh ( base offset ++ @$ d word size @$ register node value1 @$ d word size @$ minus thirty one set @$ byte size @$ tmp var5 ) ) ; instructions . add ( reil helpers . create and ( base offset ++ @$ byte size @$ tmp var5 @$ byte size @$ is zero condition four @$ byte size @$ shifter carry out tmp2 ) ) ; instructions . add ( reil helpers . create sub ( base offset ++ @$,the carry
rehash the table if the <PLACE_HOLDER> of entries would exceed the <PLACE_HOLDER> of buckets .,if ( f num >= f table size ) { rehash ( ) ; bucket = hash % f table size ; } else if ( collision count >= max_hash_collisions && key instanceof string ) { rebalance ( ) ; bucket = hash ( key ) % f table size ; },number exceed
after terminate suppressed <PLACE_HOLDER> which itself suppressed original err,assert equals ( __num__ @$ after terminate . get suppressed ( ) . length ) ; assert equals ( error @$ after terminate . get suppressed ( ) [ __num__ ] ) ; assert equals ( __num__ @$ error . get suppressed ( ) . length ) ; assert equals ( err @$ error . get suppressed ( ) [ __num__ ] ) ;,itself suppressed
<PLACE_HOLDER> 6 : dest content changed @$ source content changed <PLACE_HOLDER> 7 : dest name change & content changed @$ source name changed & content changed <PLACE_HOLDER> 8 : dest name changed & content changed @$ source content changed <PLACE_HOLDER> 9 : dest content changed @$ source name changed & content changed,if ( tree structure changed ( orig root @$ latest root ) && tree structure changed ( orig root @$ my root ) ) { keep other or create tree ( orig root @$ my root @$ result root @$ i + __num__ ) ; } else if ( name changed ( orig root @$ latest tree name ) && tree structure changed ( orig root @$ latest root ) && name changed ( orig root @$ my tree name ) ) { names content changed ( my root @$ my tree name @$ result tree name @$ orig root @$ i + __num__ ) ; } else if ( name changed ( orig root @$ latest tree name ) && name changed ( orig root @$ my,name changed
most likely client browser closed <PLACE_HOLDER>,if ( t instanceof socket exception ) { get logger ( ) . info ( __str__ + __str__ ) ; return ; },browser closed
add modules files as inputs . instead they rely on input discovery to recognize the needed ones . however @$ orphan <PLACE_HOLDER> runs before input discovery and thus module files would be discarded as orphans . this is strictly better than marking all transitive modules as inputs @$ which would also effectively disable orphan <PLACE_HOLDER> for .pcm files .,if ( output file . is file type ( cpp file types . cpp_module ) ) { return immutable set . of ( output file ) ; } return super . get mandatory outputs ( ) ;,which disable
let system take <PLACE_HOLDER> for date change events .,final work source work source = null ;,system take
no suitable <PLACE_HOLDER> in the cache so we need to allocate a new one . to avoid the cache growing then we remove the first <PLACE_HOLDER> from the cache and free it .,if ( ! cache . is empty ( ) ) { buf = cache . remove first ( ) ; free ( buf ) ; } return byte buffer . allocate direct ( size ) ;,one remove
add all spanning <PLACE_HOLDER> that need to be inserted after this one .,add spanning cells ( ) ;,all spanning
trigger a gc that checks that the verification code allows humongous <PLACE_HOLDER> with code cache roots ; <PLACE_HOLDER> should be all live here .,system . gc ( ) ;,code allows
ensure moving task between two stacks updates resumed <PLACE_HOLDER>,r . set state ( resumed @$ __str__ ) ; assert equals ( r @$ m stack . get resumed activity ( ) ) ; final activity stack dest stack = m root activity container . get default display ( ) . create stack ( windowing_mode_fullscreen @$ activity_type_standard @$ true ) ;,updates resumed
a list which will hold the <PLACE_HOLDER> for the column families once the db is opened,list < column family handle > columns = new array list < > ( ) ; m db = rocksdb . open ( m db opts @$ m db path @$ cf descriptors @$ columns ) ; m checkpoint = checkpoint . create ( m db ) ; for ( int i = __num__ ; i < columns . size ( ) - __num__ ; i ++ ) { m column handles . get ( i ) . set ( columns . get ( i + __num__ ) ) ; },which hold
for whatever reason this activity is being launched into a new task ... yet the caller has requested a result <PLACE_HOLDER> . well @$ that is pretty messed up @$ so instead immediately send <PLACE_HOLDER> a cancel and let the new task continue launched as normal without a dependency on its originator .,if ( source stack != null && ( m launch flags & flag_activity_new_task ) != __num__ ) { slog . w ( tag @$ __str__ ) ; source stack . send activity result locked ( - __num__ @$ m start activity . result to @$ m start activity . result who @$ m start activity . request code @$ result_canceled @$ null ) ; m start activity . result to = null ; },caller requested
null pkg indicates user <PLACE_HOLDER>,cancel all notifications by list locked ( m notification list @$ calling uid @$ calling pid @$ pkg @$ true @$ channel id @$ flag checker @$ false @$ user id @$ false @$ reason @$ listener name @$ true ) ;,pkg indicates
the file name should contain an extension <PLACE_HOLDER> .,if ( file name . last index of ( extension_separator ) == - __num__ ) { return false ; },name contain
pod does not have any <PLACE_HOLDER> that references pvc,if ( volume to che volume name . is empty ( ) ) { continue ; },pod have
someone asked build <PLACE_HOLDER> @$ let stop the build before trying to run another build step,if ( executor != null && executor . is interrupted ( ) ) { throw new interrupted exception ( ) ; },someone build
we check that access control exceptions contain absolute <PLACE_HOLDER> .,path parent = p . get parent ( ) ; assert true ( parent . is uri path absolute ( ) ) ; assert true ( e . get message ( ) . contains ( parent . to string ( ) ) ) ; return false ;,exceptions contain
client & server have the same <PLACE_HOLDER>,if ( server methods == null ) { return true ; },client have
because users do n't have a backup of media @$ it 's safer to import new data and rely on them running a media db <PLACE_HOLDER> to get rid of any unwanted media . in the future we might also want to duplicate this step import media,hash map < string @$ string > name to num = new hash map < > ( ) ; hash map < string @$ string > num to name = new hash map < > ( ) ; file media map file = new file ( dir . get absolute path ( ) @$ __str__ ) ; if ( media map file . exists ( ) ) { json reader jr = new json reader ( new file reader ( media map file ) ) ; jr . begin object ( ) ; string name ; string num ; while ( jr . has next ( ) ) { num = jr . next name ( ) ; name = jr . next string ( ) ; name,them running
test 2 : test that the correct number of <PLACE_HOLDER> read is returned if the source has less <PLACE_HOLDER> available than fit in the buffer .,b = new byte [ ref . length ] ; bytes read = is . read ( b ) ; assert equals ( __str__ @$ bytes read @$ ref . length - __num__ ) ; for ( i = __num__ ; i < bytes read ; i ++ ) { equal &= ( b [ i ] == ref [ i + __num__ ] ) ; } assert true ( __str__ @$ equal ) ;,source has
the first part of xor <PLACE_HOLDER> contains the flushed data @$ which we copy out . the remainder contains the <PLACE_HOLDER> that will be needed for manual transformation in a subsequent call .,for ( int i = __num__ ; i < bytes to flush ; i ++ ) { out [ out offset ++ ] = flushed block [ i ] ; },part contains
new settings for set install location <PLACE_HOLDER> no longer initiated here .,if ( upgrade version == __num__ ) { upgrade version = __num__ ; },settings install
given : thread which awaited <PLACE_HOLDER> .,segment aware aware = new segment aware ( __num__ @$ false ) ; aware . check can read archive or reserve work segment ( __num__ ) ; try { aware . release work segment ( __num__ ) ; } catch ( assertion error e ) { return ; } fail ( __str__ ) ;,which awaited
coalesce can handle this more <PLACE_HOLDER>,list < string > nick names = entity manager . create query ( __str__ + __str__ @$ string . class ) . get result list ( ) ;,coalesce handle
if more input could extend the <PLACE_HOLDER> then we must wait for more input,if ( matcher . looking at ( ) ) { if ( matcher . hit end ( ) && ! source closed ) { need input = true ; return null ; } skipped = true ; position = matcher . end ( ) ; },input extend
tree related <PLACE_HOLDER> .,menu = new j menu ( __str__ ) ; menu bar . add ( menu ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new add action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new insert action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new reload action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action,tree related
normal operation blocks the site <PLACE_HOLDER> on the sitetasker queue .,while ( m_should continue ) { m_txn state = null ; site tasker task = m_scheduler . take ( ) ; task . run ( get site procedure connection ( ) ) ; },operation blocks
this thread is being killed @$ do not print <PLACE_HOLDER>,if ( t instanceof thread death ) { } else { system . err . println ( __str__ + thread . current thread ( ) . get name ( ) + __str__ + t . get class ( ) . get canonical name ( ) ) ; try { t . print stack trace ( ) ; } catch ( throwable ignored ) { } },thread print
all <PLACE_HOLDER> input columns are repeating . generate <PLACE_HOLDER> once . lookup once . since the <PLACE_HOLDER> is repeated @$ we must use entry 0 regardless of selected in use .,if ( all key input columns repeating ) { key vector serialize write . set output ( current key output ) ; key vector serialize write . serialize write ( batch @$ __num__ ) ; join util . join result join result ; if ( key vector serialize write . get has any nulls ( ) ) { join result = join util . join result . nomatch ; } else { byte [ ] key bytes = current key output . get data ( ) ; int key length = current key output . get length ( ) ; join result = hash multi set . contains ( key bytes @$ __num__ @$ key length @$ hash multi set results [ __num__ ] ) ; } if,input generate
errors signal fatal <PLACE_HOLDER> @$ so unregister and assume connection lost .,logger . debug ( __str__ + __str__ ) ; irc connection . this . irc . delete listener ( this ) ;,errors signal
no good way to resolve ambiguous <PLACE_HOLDER> at transition @$ but following code work in most case .,tz . get offset ( local millis @$ true @$ offsets ) ; if ( tztype == time type . standard && offsets [ __num__ ] != __num__ || tztype == time type . daylight && offsets [ __num__ ] == __num__ ) { tz . get offset ( local millis - ( __num__ * __num__ * __num__ * __num__ ) @$ true @$ offsets ) ; },way resolve
set a small time unit as cookie max age so that the server sends a <PLACE_HOLDER>,hive conf . set time var ( conf vars . hive_server2_thrift_http_cookie_max_age @$ __num__ @$ time unit . seconds ) ; hive conf . set bool var ( conf vars . hive_support_concurrency @$ false ) ; minihs2 = mini hive kdc . get minihs2 with kerb ( mini hive kdc @$ hive conf ) ; minihs2 . start ( new hash map < string @$ string > ( ) ) ;,server sends
if vertex low <PLACE_HOLDER> is same as visited <PLACE_HOLDER> then this is start vertex for strongly connected component . keep popping vertices out of stack still you find current vertex . they are all part of one strongly connected component .,if ( visited time . get ( vertex ) == low time . get ( vertex ) ) { set < vertex < integer > > strongly connected componenet = new hash set < > ( ) ; vertex v ; do { v = stack . poll first ( ) ; on stack . remove ( v ) ; strongly connected componenet . add ( v ) ; } while ( ! vertex . equals ( v ) ) ; result . add ( strongly connected componenet ) ; },then visited
if the evaluate yields true then pass all <PLACE_HOLDER> else pass 0 <PLACE_HOLDER>,if ( pred instanceof expr node generic func desc ) { expr node generic func desc gen func = ( expr node generic func desc ) pred ; for ( expr node desc leaf : gen func . get children ( ) ) { if ( leaf instanceof expr node generic func desc ) { long new num rows = __num__ ; for ( expr node desc child : gen func . get children ( ) ) { new num rows = evaluate child expr ( stats @$ child @$ asp ctx @$ needed cols @$ op @$ num rows ) ; } return num rows - new num rows ; } else if ( leaf instanceof expr node constant desc ) { expr node constant desc encd,true pass
no need to call create all <PLACE_HOLDER> @$ since future drawables will change layout direction when they are prepared .,final int count = m num children ; final drawable [ ] drawables = m drawables ; for ( int i = __num__ ; i < count ; i ++ ) { if ( drawables [ i ] != null ) { boolean child changed = false ; if ( android . os . build . version . sdk_int >= android . os . build . version_codes . m ) { child changed = drawables [ i ] . set layout direction ( layout direction ) ; } if ( i == current index ) { changed = child changed ; } } } m layout direction = layout direction ; return changed ;,need create
let 's write an invalid <PLACE_HOLDER>,handshake ( client @$ sock -> { buffer buff = buffer . buffer ( ) ; buff . append byte ( ( byte ) ( __num__ ) ) . append byte ( ( byte ) __num__ ) ; sock . write ( buff ) ; } ) ;,"s" write
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; path output = paths . get ( __str__ ) ; immutable set < source path > inputs before = immutable set . of ( ) ; dep file build rule rule = new dep file build rule ( target @$ filesystem @$ params ) { @ add to rule key private final source path path = path source path . of ( filesystem @$ input file ) ; @ override public immutable list < step > get build steps ( build context context @$ buildable context buildable context ) { return immutable list . of ( new write file step,which writes
some variables do n't have <PLACE_HOLDER> @$ but use the name of their datatype,if ( datatype != null ) { return datatype . get name ( ) ; } string signature = get signature ( true ) ; string fixed = symbol utilities . replace invalid chars ( signature @$ true ) ; return fixed ;,variables have
expected exception because the boundary timer event created a timer <PLACE_HOLDER> to be executed after 10 minutes,wait for job executor to process all jobs and executable timer jobs ( __num__ @$ __num__ ) ;,event created
the packing plan consists of two containers . the first one contains 2 <PLACE_HOLDER> and 2 bolts the second one contains 2 <PLACE_HOLDER> and 1 bolt . during scaling we remove 3 <PLACE_HOLDER> and 3 bolts and thus the first container is removed .,map < string @$ integer > component changes = new hash map < > ( ) ; component changes . put ( spout_name @$ spout scaling down ) ;,one contains
recalculate wait <PLACE_HOLDER> .,woke = ( woke == - __num__ ) ? system . current time millis ( ) : woke ; wait time = this . period - ( woke - start time ) ;,recalculate wait
execute asynchronous discard <PLACE_HOLDER>,executor . run queued commands ( ) ; verify ( state @$ times ( __num__ ) ) . discard state ( ) ;,asynchronous discard
and compare the <PLACE_HOLDER> ...,try { assert source record match ( actual record @$ expected record @$ ignorable fields :: contains @$ comparators by field name @$ comparators by schema name ) ; } catch ( assertion error e ) { result . error ( ) ; string msg = __str__ + schema util . as string ( actual record . key ( ) ) + __str__ + e . get message ( ) ; testing . debug ( msg ) ; throw new mismatch record exception ( e @$ msg @$ actual record history @$ expected record history ) ; },and compare
our internal version does not have this <PLACE_HOLDER>,if ( ! analysis mock . is this bazel ( ) ) { return ; },version have
no class found <PLACE_HOLDER> to the system property passed in from bazel,if ( suite == null ) { if ( args . length == __num__ && suite class name != null ) { system . err . printf ( __str__ @$ suite class name ) ; return __num__ ; } },class found
sp plans which have an index which can provide the window <PLACE_HOLDER> ordering do n't create an order by node .,if ( ! ( child instanceof order by plan node ) ) { return plan ; } order by plan node onode = ( order by plan node ) child ; child = onode . get child ( __num__ ) ;,which provide
make sure the developer is following the <PLACE_HOLDER> .,if ( ( node . get text ( ) == null ) && ( node . get content description ( ) == null ) ) { throw new runtime exception ( __str__ + __str__ ) ; } node . get bounds in parent ( temp parent rect ) ; if ( temp parent rect . equals ( invalid_parent_bounds ) ) { throw new runtime exception ( __str__ + __str__ ) ; } final int actions = node . get actions ( ) ; if ( ( actions & accessibility node info . action_accessibility_focus ) != __num__ ) { throw new runtime exception ( __str__ + __str__ ) ; } if ( ( actions & accessibility node info . action_clear_accessibility_focus ) != __num__ ) { throw new runtime exception,developer following
assume the path did not match <PLACE_HOLDER> in the document,return empty_result ;,path match
most subclasses will need <PLACE_HOLDER> :,sampler sampler = jmctx . get current sampler ( ) ; bindings . put ( __str__ @$ sampler ) ;,subclasses need
process named query i <PLACE_HOLDER>,list < string > named query ids = list named queries result . get named query ids ( ) ;,process named
the plugin <PLACE_HOLDER> has been changed while a configuration network call is going on @$ we need to dispatch another configure plugin action since we do n't allow multiple configure actions to happen at the same time this might happen either because user changed the <PLACE_HOLDER> or a remove plugin action has started,if ( is plugin state changed since last configuration dispatch ( ) ) { dispatch configure plugin action ( false ) ; } else if ( m is removing plugin && ! m plugin . is active ( ) ) { dispatch remove plugin action ( ) ; m is active = m plugin . is active ( ) ; m switch active . set checked ( m is active ) ; },user changed
if byte array output stream.write has <PLACE_HOLDER> @$ that is a very bad thing,throw new runtime exception ( ex ) ;,stream.write has
0 means requery the <PLACE_HOLDER> for this @$ it does n't have a valid width .,if ( ! lp . is decor && lp . height factor == __num__ ) { final item info ii = info for child ( child ) ; if ( ii != null ) { lp . height factor = ii . height factor ; lp . position = ii . position ; } },0 means
ensure that server time increments every time we do an <PLACE_HOLDER> @$ otherwise successive puts having the same timestamp will override each other,environment edge manager test helper . inject edge ( new incrementing environment edge ( ) ) ;,time do
test once using the current correct hash function @$ expect no mispartitioned <PLACE_HOLDER>,client response cr = client . call procedure ( __str__ @$ ( object ) null ) ; volt table hashinator matches = cr . get results ( ) [ __num__ ] ; hashinator matches . advance row ( ) ; while ( hashinator matches . advance row ( ) ) { assert equals ( __num__ @$ hashinator matches . get long ( __str__ ) ) ; } volt table validate result = cr . get results ( ) [ __num__ ] ;,test expect
if the current component contains an <PLACE_HOLDER> @$ run that one,if ( component . has ( __str__ ) ) { json object action = component . getjson object ( __str__ ) ; ( ( jason view activity ) context ) . call ( action . to string ( ) @$ new json object ( ) . to string ( ) @$ __str__ @$ v . get context ( ) ) ; } else { view cursor = v ; while ( cursor . get parent ( ) != null ) { json object item = ( json object ) ( ( ( view ) cursor . get parent ( ) ) . get tag ( ) ) ; if ( item != null && ( item . has ( __str__ ) || item . has ( __str__ ),component contains
let the local destroy or processing of transfer do the <PLACE_HOLDER>,return ;,destroy do
check that the mapping matches the <PLACE_HOLDER> .,for ( int i = __num__ ; i < values . length ; i ++ ) { assert equals ( values [ i ] @$ buf . get ( mapper . map ( i ) ) ) ; },mapping matches
the new area tiles contains a <PLACE_HOLDER>,if ( ( collision data flags [ x ] [ checky ] & y wall flags east ) != __num__ ) { return false ; },tiles contains
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; source path input = path source path . of ( filesystem @$ filesystem . get root path ( ) . get file system ( ) . get path ( __str__ ) ) ; filesystem . touch ( path resolver . get relative path ( input ) ) ; path output = build target paths . get gen path ( filesystem @$ target @$ __str__ ) ; dep file build rule rule = new dep file build rule ( target @$ filesystem @$ params ) { @ add to rule key private final source path path = input ; @ override,which writes
lets take a look at the operator memory <PLACE_HOLDER> .,dispatcher disp = null ; final set < map join operator > map joins = new linked hash set < map join operator > ( ) ; linked hash map < rule @$ node processor > rules = new linked hash map < rule @$ node processor > ( ) ; rules . put ( new rule reg exp ( __str__ @$ map join operator . get operator name ( ) + __str__ ) @$ new node processor ( ) { @ override public object process ( node nd @$ stack < node > stack @$ node processor ctx proc ctx @$ object ... node outputs ) { map joins . add ( ( map join operator ) nd ) ; return null ; } } ) ;,lets take
if the reparent <PLACE_HOLDER> token contains previous display 's last focus <PLACE_HOLDER> @$ means it will end up to gain <PLACE_HOLDER> focus on the target display @$ so it should not be notified that it lost focus from the previous display .,if ( token . has child ( prev dc . m last focus ) ) { prev dc . m last focus = null ; },token contains
run the counter job @$ outputs to a single reduce <PLACE_HOLDER> and file,log . info ( __str__ ) ; try { boolean success = counter . wait for completion ( true ) ; if ( ! success ) { string message = __str__ + counter . get status ( ) . get state ( ) + __str__ + counter . get status ( ) . get failure info ( ) ; log . error ( message ) ; throw new runtime exception ( message ) ; } } catch ( io exception | interrupted exception | class not found exception e ) { log . error ( __str__ @$ e ) ; throw e ; } log . info ( __str__ ) ;,job reduce
if no restrictions were saved @$ dpm method should return an empty <PLACE_HOLDER> as per java doc .,return bundle != null ? new bundle ( bundle ) : new bundle ( ) ;,method return
get the start index ; the assertions below will fail if the assignment logic does not meet correct <PLACE_HOLDER>,int num consumers = kafka topic partition assigner . assign ( mock get all partitions for topics return . get ( __num__ ) @$ num subtasks ) ; for ( int subtask index = __num__ ; subtask index < mock get all partitions for topics return . size ( ) ; subtask index ++ ) { test partition discoverer partition discoverer = new test partition discoverer ( topics descriptor @$ subtask index @$ mock get all partitions for topics return . size ( ) @$ test partition discoverer . create mock get all topics sequence from fixed return ( collections . singleton list ( test_topic ) ) @$ test partition discoverer . create mock get all partitions from topics sequence from fixed return ( mock get all partitions,logic meet
change the mob compaction merge <PLACE_HOLDER>,conf . set long ( mob constants . mob_compaction_mergeable_threshold @$ merge size ) ; common policy test logic ( __str__ @$ mob compact partition policy . weekly @$ false @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ true ) ;,compaction merge
otherwise we saw a transaction spend our <PLACE_HOLDER> @$ but we did n't try and spend them ourselves yet . the outputs are already marked as spent by the connect call above @$ so check if there are any more for us to use . move if not .,if ( result == transaction input . connection result . success ) { transaction connected = check not null ( input . get connected transaction ( ) ) ; log . info ( __str__ @$ input . get outpoint ( ) @$ tx . get tx id ( ) ) ; maybe move pool ( connected @$ __str__ ) ; if ( output . is mine or watched ( this ) ) { check state ( my unspents . remove ( output ) ) ; } },transaction spend
allocation of array may have caused gc @$ which may have caused additional <PLACE_HOLDER> to go stale . removing these <PLACE_HOLDER> from the reference queue will make them eligible for reclamation .,while ( queue . poll ( ) != null ) ;,which caused
end if found better end for left <PLACE_HOLDER> end for left state do right restricted <PLACE_HOLDER>,for ( int right state = __num__ ; right state < num states ; right state ++ ) { int narrowl = narrowl extent_end [ right state ] ; if ( narrowl <= start ) { continue ; } binary rule [ ] right rules = bg . split rules withrc ( right state ) ; for ( binary rule rule : right rules ) { int left child = rule . left child ; int narrowr = narrowr extent_start [ left child ] ; if ( narrowr > narrowl ) { continue ; } int min2 = widel extent_end [ right state ] ; int min = ( narrowr > min2 ? narrowr : min2 ) ; int max1 = wider extent_start [ left child ] ;,end do
this will trigger end callback which should set <PLACE_HOLDER> to their target values .,view compat . animate ( view ) . cancel ( ) ;,which set
check that obj literal have only <PLACE_HOLDER> that are present in the message text,for ( string ph name : ph names ) { if ( ! used placeholders . contains ( ph name ) ) { throw new malformed exception ( __str__ + ph name @$ node ) ; } },literal have
current node already performs <PLACE_HOLDER> .,return ;,node performs
if the proxy directly proxy the <PLACE_HOLDER> or extends it @$ return true,if ( iface . is assignable from ( get class ( ) ) ) { return true ; } else if ( iface . is assignable from ( delegate . get class ( ) ) ) { return true ; } else if ( wrapper . class . is assignable from ( delegate . get class ( ) ) ) { return ( ( wrapper ) unwrapp6 spy proxy ( ) ) . is wrapper for ( iface ) ; },proxy proxy
instance info in response from peer will trigger local registry <PLACE_HOLDER>,create peer eureka node ( ) . heartbeat ( instance info . get app name ( ) @$ instance info . get id ( ) @$ instance info @$ null @$ false ) ; expect request type ( request type . batch ) ;,info trigger
key gets <PLACE_HOLDER> here,byte [ ] bytes = sortk1 . to byte array ( ) ; do assert ( bytes . length == __num__ && bytes [ __num__ ] == __num__ && bytes [ __num__ ] == __num__ && bytes [ __num__ ] == __num__ @$ __str__ ) ;,key gets
the one join <PLACE_HOLDER> for this specialized class .,bytes column vector join col vector = ( bytes column vector ) batch . cols [ single join column ] ; byte [ ] [ ] vector = join col vector . vector ; int [ ] start = join col vector . start ; int [ ] length = join col vector . length ;,one join
initialize map local <PLACE_HOLDER>,local work = mrwork . get map red local work ( ) ; exec context . set local work ( local work ) ; mapred context . init ( true @$ new job conf ( jc ) ) ; mo . pass exec context ( exec context ) ; mo . initialize local work ( jc ) ; mo . initialize map operator ( jc ) ; if ( local work == null ) { return ; },initialize map
check whether surface flinger spontaneously changed <PLACE_HOLDER> out from under us . schedule traversals to ensure that the correct state is reapplied if necessary .,if ( m active mode id != __num__ && m active mode id != active record . m mode . get mode id ( ) ) { m active mode invalid = true ; send traversal request locked ( ) ; } boolean records changed = records . size ( ) != m supported modes . size ( ) || modes added ;,flinger changed
this is the case when we 've encountered a decimal separator . the fractional part will not change the <PLACE_HOLDER> @$ but we will verify that the fractional part is well formed .,while ( i < size in bytes ) { byte current byte = tmp bytes [ i ] ; if ( current byte < __str__ || current byte > __str__ ) { return null ; } i ++ ; } if ( ! negative ) { result = - result ; if ( result < __num__ ) { return null ; } } return result ;,part change
if this path has a root <PLACE_HOLDER> the given path 's root must match,if ( ! this . root . equals ignore case ( other . root ) ) { return false ; },path has
arbitrary and not worth documenting @$ as activity manager will kill this <PLACE_HOLDER> shortly anyway .,return __num__ ;,manager kill
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized . get ( ) ) ;,scheduler captures
force a 100 continue <PLACE_HOLDER> .,jetty request . get http channel ( ) . send response ( http generator . continue_100_info @$ null @$ false ) ;,100 continue
create a web hdfs file system <PLACE_HOLDER>,uri uri = new uri ( __str__ + string . value of ( port ) ) ; configuration conf = new configuration ( ) ; web hdfs file system webfs = ( web hdfs file system ) file system . get ( uri @$ conf ) ; content summary summary = webfs . get content summary ( new path ( __str__ ) ) ; verify content summary ( sym link summary for dir contains fromdfs @$ summary ) ;,hdfs file
the class in which create <PLACE_HOLDER> is implemented,if ( connector bundle . is connected component connector ( type ) ) { j class type create widget class = connector bundle . find inherited method ( type @$ __str__ ) . get enclosing type ( ) ; j method get widget = connector bundle . find inherited method ( type @$ __str__ ) ; j class type widget type = get widget . get return type ( ) . is class ( ) ; if ( create widget class . get qualified source name ( ) . equals ( abstract component connector . class . get canonical name ( ) ) ) { if ( get widget . get enclosing type ( ) . get qualified source name ( ) . equals ( abstract component connector,which create
if the user set a thread stack <PLACE_HOLDER> at thread creation @$ then use that .,if ( stack size != __num__ ) { chosen stack size = stack size ; } else { final int default thread stack size = ( int ) x options . get xss ( ) . get value ( ) ; if ( default thread stack size != __num__ ) { chosen stack size = default thread stack size ; } },thread stack
for dependencies that are missing we canonicalize and remap the target so we do n't suggest private build <PLACE_HOLDER> .,set < jar owner > canonicalized missing = missing targets . stream ( ) . filter ( owner -> owner . label ( ) . is present ( ) ) . sorted ( comparator . comparing ( ( jar owner owner ) -> owner . label ( ) . get ( ) ) ) . map ( owner -> owner . with label ( owner . label ( ) . map ( label -> canonicalize target ( label ) ) ) ) . collect ( to immutable set ( ) ) ;,private build
if the millisecond date value contains time <PLACE_HOLDER> @$ mask it out .,super ( date ) ;,value contains
dw suggested buffer size specifies how large a buffer should be used to read this stream . typically @$ this contains a value corresponding to the largest chunk present in the stream . using the correct buffer size makes playback more efficient . use <PLACE_HOLDER> if you do not know the correct buffer size .,d . write int ( tr . quality ) ;,present use
processor has an input <PLACE_HOLDER> and fails,runner . set non loop connection ( true ) ; runner . enqueue ( new byte [ __num__ ] ) ; runner . run ( __num__ @$ false ) ; runner . assert all flow files transferred ( query solr . failure @$ __num__ ) ; flow file = runner . get flow files for relationship ( query solr . failure ) . get ( __num__ ) ; flow file . assert attribute exists ( query solr . exception ) ; flow file . assert attribute exists ( query solr . exception_message ) ; runner . clear transfer state ( ) ;,processor has
call matrix changed <PLACE_HOLDER> if needed,if ( m matrix change listener != null ) { rectf display rect = get display rect ( matrix ) ; if ( display rect != null ) { m matrix change listener . on matrix changed ( display rect ) ; } },matrix changed
the second line has a ' <PLACE_HOLDER> ' @$ so it needs more ascent and descent .,if ( m enabled ) { assert equals ( - __num__ * em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; } else { assert equals ( - em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; },line has
current time should trigger <PLACE_HOLDER> of older active file,rotate . maybe rotate ( current time ) ;,time trigger
mock the second split read stream <PLACE_HOLDER> .,when ( fake storage client . split read stream ( split read stream request . new builder ( ) . set original stream ( streams . get ( __num__ ) ) . set fraction ( __num__ ) . build ( ) ) ) . then return ( split read stream response . new builder ( ) . set primary stream ( streams . get ( __num__ ) ) . set remainder stream ( stream . new builder ( ) . set name ( __str__ ) ) . build ( ) ) ;,split read
admin should have all <PLACE_HOLDER>,user group information admin user = user group information . create remote user ( admin_user ) ; assert true ( acl manager . check access ( admin user @$ application access type . view_app @$ app_owner @$ app id ) ) ; assert true ( acl manager . check access ( admin user @$ application access type . modify_app @$ app_owner @$ app id ) ) ;,admin have
create the module provider ; this class provides a tile <PLACE_HOLDER> that actually loads the tile from the map file .,module provider = new maps forge tile module provider ( simple register receiver @$ from files @$ tile writer ) ;,class provides
instant apps need <PLACE_HOLDER> to create foreground services .,if ( r . app info . is instant app ( ) ) { final int mode = m am . m app ops service . check operation ( app ops manager . op_instant_app_start_foreground @$ r . app info . uid @$ r . app info . package name ) ; switch ( mode ) { case app ops manager . mode_allowed : break ; case app ops manager . mode_ignored : slog . w ( tag @$ __str__ + r . app info . package name + __str__ + __str__ ) ; return ; case app ops manager . mode_errored : throw new security exception ( __str__ + r . app info . package name + __str__ ) ; default : m am . enforce permission (,apps need
clear the underlying finished <PLACE_HOLDER> .,clear finished bits ( entry . get value ( ) ) ;,the finished
do not asser equals here since one of the addresses may contain a display <PLACE_HOLDER> or something of the kind,assert true ( __str__ + __str__ @$ expected peer1 address . index of ( peer atp1 . get address ( ) ) != - __num__ || peer atp1 . get address ( ) . index of ( expected peer1 address ) != - __num__ ) ; assert equals ( __str__ + __str__ @$ expected peer1 display name @$ peer atp1 . get display name ( ) ) ;,one contain
set create wal <PLACE_HOLDER> to true and use default values for other options .,util . start mini cluster ( start mini cluster option . builder ( ) . createwal dir ( true ) . build ( ) ) ;,set create
fake ftp server has timestamp <PLACE_HOLDER> in minutes . specify milliseconds <PLACE_HOLDER> so that test does not need to wait for minutes .,runner . set property ( list file . target_system_timestamp_precision @$ list file . precision_millis ) ; runner . assert valid ( ) ;,server has
super call writes the connection <PLACE_HOLDER> we need to flush to send it this is called only on the client,ctx . flush ( ) ;,call writes
heartbeat has no <PLACE_HOLDER>,if ( message type != protocol constants . msgtype_heartbeat_request && message type != protocol constants . msgtype_heartbeat_response ) { codec codec = codec factory . get codec ( rpc message . get codec ( ) ) ; body bytes = codec . encode ( rpc message . get body ( ) ) ; compressor compressor = compressor factory . get compressor ( rpc message . get compressor ( ) ) ; body bytes = compressor . compress ( body bytes ) ; full length += body bytes . length ; },heartbeat has
if the user specified a custom view for the tab indicators @$ then do not draw the bottom <PLACE_HOLDER> .,if ( ! m draw bottom strips ) { return ; },then draw
the dispatcher fires the <PLACE_HOLDER> corresponding to the closest matching rule and passes the context along,dispatcher disp = new default rule dispatcher ( null @$ op rules @$ proc ctx ) ; graph walker ogw = new default graph walker ( disp ) ;,dispatcher fires
gets set once @$ so doc and descendants have first <PLACE_HOLDER>,assert equals ( __str__ @$ doc . base uri ( ) ) ;,doc have
concurrent hash map does not need <PLACE_HOLDER> . here,for ( abstract thread group thread group : groups ) { thread group . wait threads stopped ( ) ; },map need
1 st root <PLACE_HOLDER>,if ( parent == null ) { long next = ( ( long ) phase << phase_shift ) | adjust ; if ( unsafe . compare and swap long ( this @$ state offset @$ s @$ next ) ) break ; } else { synchronized ( this ) { if ( state == s ) { phase = parent . do register ( __num__ ) ; if ( phase < __num__ ) break ; while ( ! unsafe . compare and swap long ( this @$ state offset @$ s @$ ( ( long ) phase << phase_shift ) | adjust ) ) { s = state ; phase = ( int ) ( root . state > > > phase_shift ) ; } break ; },st root
we accept property with targets type to be compatible with old jdks see <PLACE_HOLDER>,if ( targets getter . is executed ( ) && ! targets getter . is disposed ( ) && ( targets getter . get actual type ( ) == x atom . xa_atom || targets getter . get actual type ( ) == x data transferer . targets_atom . get atom ( ) ) && targets getter . get actual format ( ) == __num__ ) { int count = targets getter . get number of items ( ) ; if ( count > __num__ ) { long atoms = targets getter . get data ( ) ; formats = new long [ count ] ; for ( int index = __num__ ; index < count ; index ++ ) { formats [ index ] = native .,jdks see
if the first argument is const then just set the <PLACE_HOLDER> and continue,if ( col name == null ) { is const = true ; prev const = ( ( expr node constant desc ) leaf ) . get value ( ) ; continue ; },argument set
chrome has been killed @$ reconstruct a download <PLACE_HOLDER> .,if ( m download info == null ) { m download info = new download info . builder ( ) . set file name ( title ) . set description ( c . get string ( c . get column index ( download manager . column_description ) ) ) . set mime type ( c . get string ( c . get column index ( download manager . column_media_type ) ) ) . set content length ( long . parse long ( c . get string ( c . get column index ( download manager . column_total_size_bytes ) ) ) ) . build ( ) ; },chrome reconstruct
last parameter to copy from referenced method @$ exclude final var <PLACE_HOLDER>,int last = local context . needs var args conversion ( ) ? impl size - __num__ : impl size ;,parameter exclude
reset the stream @$ but set a lower limit . writing beyond the limit should throw an <PLACE_HOLDER>,stream . reset ( size - __num__ ) ; assert true ( __str__ @$ ( stream . get limit ( ) == size - __num__ ) ) ; caught exception = false ; try { stream . write ( input @$ __num__ @$ size ) ; } catch ( exception e ) { caught exception = true ; } assert true ( __str__ @$ caught exception ) ;,limit throw
other errors need a full <PLACE_HOLDER> .,log . error ( __str__ @$ service != null ? ( service . to string ( ) + __str__ + service . get service state ( ) ) : __str__ @$ thrown ) ; exit exception = convert to exit exception ( thrown ) ;,errors need
selection triggers <PLACE_HOLDER>,change selection to navigate ( table @$ __num__ @$ __num__ ) ; run swing ( ( ) -> get providers ( ) [ __num__ ] . close component ( ) ) ; assert equals ( addr ( __str__ ) @$ cb . get current address ( ) ) ;,selection triggers
wait until the <PLACE_HOLDER> used by res 1 is returned to the pool @$ so that the next request reuses the <PLACE_HOLDER> .,while ( ! connection returned to pool ) { condition . await ( ) ; } lock . unlock ( ) ;,request reuses
mimic a task failure ; setting up the task for cleanup simulates the abort protocol to be played . without checks in the framework @$ this will fail as the <PLACE_HOLDER>ter will cause a <PLACE_HOLDER> to happen for the cleanup task .,task . set task cleanup task ( ) ; my umbilical umbilical = new my umbilical ( ) ; task . run ( job @$ umbilical ) ; assert true ( __str__ @$ umbilical . task done ) ;,committer cause
indirect type <PLACE_HOLDER> should give same type <PLACE_HOLDER> as for parent class,assert that ( type mapping finder . find type mapping ( descendant class . class ) ) . is same as ( type mapping ) ;,mapping give
if only implicit defaults exist @$ have a null default <PLACE_HOLDER> default primitives . this makes it so if there is a nullable object and a primitive in a group @$ the default value will be to null out the object .,if ( default attribute == null || has explicit default ( attribute ) || attribute . has set nullability ( ) ) { default attribute = attribute ; },primitives have
at this point @$ the configuration 's size needs to be taken into account as not all configurations have all <PLACE_HOLDER> .,int screen layout = __num__ ; int ui mode = __num__ ; int smallest screen width dp = __num__ ; int screen width dp = __num__ ; int screen height dp = __num__ ; byte [ ] locale script = new byte [ __num__ ] ; byte [ ] locale variant = new byte [ __num__ ] ; byte screen layout2 = __num__ ; byte screen config pad1 = __num__ ; short screen config pad2 = __num__ ; if ( size >= screen_config_min_size ) { screen layout = unsigned bytes . to int ( buffer . get ( ) ) ; ui mode = unsigned bytes . to int ( buffer . get ( ) ) ; smallest screen width dp = buffer . get short ( ),configurations have
view enters low padding <PLACE_HOLDER> :,if ( view min < padding min ) { first view = view ; if ( m focus scroll strategy == base grid view . focus_scroll_page ) { while ( prepend one column visible items ( ) ) { circular int array positions = m grid . get item positions in rows ( m grid . get first visible index ( ) @$ pos ) [ row ] ; first view = find view by position ( positions . get ( __num__ ) ) ; if ( view max - get view min ( first view ) > client size ) { if ( positions . size ( ) > __num__ ) { first view = find view by position ( positions . get ( __num__ ) ),view enters
check that resource exhaustion triggers an <PLACE_HOLDER>,try { m ip sec service . reserve net id ( ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { },exhaustion triggers
completing the human task a should mark the <PLACE_HOLDER> as completable,cmmn task service . complete ( taska . get id ( ) ) ; plan item instance stage1 plan item instance = cmmn runtime service . create plan item instance query ( ) . plan item instance name ( __str__ ) . single result ( ) ; assert that ( stage1 plan item instance . is completable ( ) ) . is true ( ) ; assert that ( cmmn runtime service . create plan item instance query ( ) . stage instance id ( stage1 plan item instance . get id ( ) ) . list ( ) ) . is not empty ( ) ;,a mark
if this is a parent pr @$ create the bucket @$ possibly going over redundancy . we need to do this so that we can create the child region in this member . this member may have the latest <PLACE_HOLDER> for the child region .,if ( has persistent child region ( ) ) { result = partitioned region . get data store ( ) . grab bucket ( bid @$ get distribution manager ( ) . get distribution manager id ( ) @$ true @$ true @$ false @$ null @$ true ) ; } else { if ( this . partitioned region . is shadowpr ( ) && this . partitioned region . get colocated with ( ) != null ) { partitioned region colocated region = colocation helper . get colocated region ( this . partitioned region ) ; if ( this . partitioned region . get data policy ( ) . with persistence ( ) && ! objects . require non null ( colocated region ) . get data,member have
currently there is no way to stop the meta store service . it will be stopped when the test jvm <PLACE_HOLDER> . this is how other tests are also using meta store server .,hive server2 . stop ( ) ; set started ( false ) ; try { if ( llap cluster != null ) { llap cluster . stop ( ) ; } if ( mr != null ) { mr . shutdown ( ) ; mr = null ; } if ( dfs != null ) { dfs . shutdown ( ) ; dfs = null ; } } catch ( io exception e ) { },test jvm
some will have <PLACE_HOLDER> more than everything else .,int num high = ( int ) ( total - ( math . floor ( mean ) * count ) ) ; int num low = ( int ) ( count - num high ) ; min = ( num high * ( math . ceil ( mean ) - mean ) ) + ( num low * ( mean - math . floor ( mean ) ) ) ;,some have
do n't let the parent intercept our <PLACE_HOLDER> .,src . get parent ( ) . request disallow intercept touch event ( true ) ; return true ;,parent intercept
make sure our set text call did not trigger the <PLACE_HOLDER> to be created,matching window = text field . get active matching window ( ) ; assert null ( __str__ @$ matching window ) ;,call trigger
build up the state index . the bg & ug both expect a set <PLACE_HOLDER> of states .,build state index ( ) ; build grammars ( ) ;,bg expect
check the looking up <PLACE_HOLDER> having only one memory segment,check argument ( key . get segments ( ) . length == __num__ ) ; final int hash code1 = key . hash code ( ) ; int new pos = hash code1 & num buckets mask ;,the looking
if the layer does n't have a drawable or unresolved theme <PLACE_HOLDER> for a drawable @$ attempt to parse one from the child element . if multiple child elements exist @$ we 'll only use the first one .,if ( layer . m drawable == null && ( layer . m theme attrs == null ) ) { while ( ( type = parser . next ( ) ) == xml pull parser . text ) { } if ( type != xml pull parser . start_tag ) { throw new xml pull parser exception ( parser . get position description ( ) + __str__ + __str__ ) ; } layer . m drawable = drawable . create from xml inner for density ( r @$ parser @$ attrs @$ m layer state . m src density override @$ theme ) ; layer . m drawable . set callback ( this ) ; state . m children changing configurations |= layer . m drawable . get,layer have
to be . note however that in the context of this function @$ the block can claim to be as difficult as it wants to be ... . if somebody was able to take control of our network connection and fork us onto a different chain @$ they could send us valid blocks with ridiculously easy difficulty and this function would accept <PLACE_HOLDER> .,big integer target = get difficulty target as integer ( ) ; big integer h = get hash ( ) . to big integer ( ) ; if ( h . compare to ( target ) > __num__ ) { if ( throw exception ) throw new verification exception ( __str__ + get hash as string ( ) + __str__ + target . to string ( __num__ ) ) ; else return false ; },function accept
if this is a dead notification @$ then ask the <PLACE_HOLDER> if it 's really failed,if ( status event . get state ( ) == z wave node state . dead ) { z controller . request is failed node ( node . get node id ( ) ) ; },then ask
saves the closed capture sessions if device is not closed yet . we need to notify camera device closed <PLACE_HOLDER> to these capture sessions .,if ( m camera device != null ) { synchronized ( m closed capture sessions ) { m closed capture sessions . add ( m capture session ) ; } } m capture session = new capture session ( m handler ) ; m capture session . set session config ( previous session config ) ;,device closed
at this point @$ the connector has performed the initial <PLACE_HOLDER> and awaits changes ...,assert that ( found last . get ( ) ) . is true ( ) ;,connector performed
previous diff should add <PLACE_HOLDER> to the view,invoke later ( prev diff ) ; view set . add range ( addr ( __str__ ) @$ addr ( __str__ ) ) ; assert equals ( addr ( __str__ ) @$ cb . get current address ( ) ) ; assert equals ( addr ( __str__ ) @$ get diff address ( ) ) ; assert equals ( cb . get current selection ( ) @$ new program selection ( addr ( __str__ ) @$ addr ( __str__ ) ) ) ; assert equals ( view set @$ cb . get view ( ) ) ;,diff add
wer expect a successful <PLACE_HOLDER>,c . connect blocking ( ) ; c . close blocking ( ) ; http headers . put ( __str__ @$ __str__ ) ; c = new example client ( new uri ( __str__ ) @$ http headers ) ;,wer expect
bind include <PLACE_HOLDER>,c = new method closure ( this @$ __str__ ) ; super . set variable ( __str__ @$ c ) ;,bind include
var has constant value @$ return a <PLACE_HOLDER> .,if ( var . get constant value ( ) != null ) { return tree util . new literal ( var . get constant value ( ) @$ type util ) ; },var return
generating cpu quota before the query finishes . this assertion verifies cpu <PLACE_HOLDER> during quota generation .,root . generate cpu quota ( __num__ ) ; stream . of ( root @$ child ) . for each ( group -> assert within cpu limit ( group @$ __num__ ) ) ;,assertion verifies
enable table @$ use retain <PLACE_HOLDER> to assign regions .,admin . enable table ( table name ) . join ( ) ; list < h region location > regions2 = async meta table accessor . get tableh region locations ( meta table @$ table name ) . get ( ) ;,use retain
loaded key must equal stored <PLACE_HOLDER>,byte [ ] bytesa2 = store . key util ( ) . identity key to bytes ( keya2 ) ; assert true ( __str__ @$ arrays . equals ( bytesa1 @$ bytesa2 ) ) ;,key equal
iteration of inodes needs exclusive <PLACE_HOLDER>,begin write ( ) ;,iteration needs
the service group had an unknown <PLACE_HOLDER> !,if ( candidate services == null ) { _log . error ( __str__ + colo cluster variant ) ; return exception_exit_code ; },group had
if put if absent fails @$ opportunistically use its return <PLACE_HOLDER>,for ( ; ; ) { v new value = remapping function . apply ( key @$ old value ) ; if ( new value != null ) { if ( old value != null ) { if ( replace ( key @$ old value @$ new value ) ) return new value ; } else if ( ( old value = put if absent ( key @$ new value ) ) == null ) return new value ; else continue have old value ; } else if ( old value == null || remove ( key @$ old value ) ) { return null ; } continue retry ; },opportunistically use
same stream @$ merge <PLACE_HOLDER> .,if ( stream . equals ( other start . stream ) ) { final map < partition id type @$ sequence offset type > new map = new hash map < > ( partition sequence number map ) ; new map . put all ( other start . partition sequence number map ) ; final set < partition id type > new exclusive partitions = new hash set < > ( ) ; partition sequence number map . for each ( ( partition id @$ sequence offset ) -> { if ( exclusive partitions . contains ( partition id ) && ! other start . partition sequence number map . contains key ( partition id ) ) { new exclusive partitions . add ( partition id ) ;,stream merge
only the legacy token have <PLACE_HOLDER> on that capability,generate new token ( wc @$ __str__ @$ __str__ ) ; check combination with config and method for legacy token creation ( config @$ wc @$ user ) ;,token have
a serializable function can only return an object type @$ so if the do <PLACE_HOLDER> parameter is a primitive type @$ then box it for the return . the return type will be unboxed before being forwarded to the do <PLACE_HOLDER> parameter .,if ( output type . get raw type ( ) . is primitive ( ) ) { output type = type descriptor . of ( primitives . wrap ( output type . get raw type ( ) ) ) ; },the do
if the length value is wrong or not all data made it to disk this read will not complete correctly . there could be overflow @$ underflow etc . so use a try finally block to indicate that all partitions are now corrupt . the enclosing exception handlers will do the right thing <PLACE_HOLDER> to propagating the error and closing the file .,boolean completed read = false ; int checksum start position = __num__ ; int row count = __num__ ; try { c . b ( ) . clear ( ) ; if ( is compressed ( ) ) { c . b ( ) . limit ( next chunk length + m_table header . capacity ( ) + __num__ ) ; } else { c . b ( ) . limit ( ( next chunk length - __num__ ) + m_table header . capacity ( ) ) ; } m_table header . position ( __num__ ) ; c . b ( ) . put ( m_table header ) ; c . b ( ) . position ( c . b ( ) . position ( ) + __num__,handlers do
client endpoint reads eof and shutdown <PLACE_HOLDER> as result,assert equals ( - __num__ @$ client . get input stream ( ) . read ( ) ) ; client . shutdown input ( ) ;,endpoint reads
seek to the end just before the last page of stream to get the <PLACE_HOLDER> .,long last page search position = end position - ogg page header . max_page_size ; if ( last page search position > position before seek to end ) { return last page search position ; },page get
open tab may throw an <PLACE_HOLDER> when the tab is not found .,outer . open tab ( __str__ ) ;,tab throw
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default autocrawl shallow profile = new crawl profile ( crawl_profile_autocrawl_shallow @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ integer . parse int ( sb . get config ( switchboard constants . autocrawl_shallow_depth @$ __str__ ) ) @$ true @$ crawl profile . get recrawl date ( integer . parse int ( sb . get config ( switchboard constants . autocrawl_days @$ __str__ ) ) * __num__ ) @$ - __num__ @$ true @$ true @$ true @$ false @$ sb . get config,url match
longs need 2 <PLACE_HOLDER>,var ++ ;,longs need
add parameters from the configuration in the job trace the reason why the job configuration parameters @$ as seen in the jobconf file @$ are added first because the specialized <PLACE_HOLDER> obtained from rumen should override the job conf <PLACE_HOLDER> .,for ( map . entry < object @$ object > entry : job . get job properties ( ) . get value ( ) . entry set ( ) ) { job conf . set ( entry . get key ( ) . to string ( ) @$ entry . get value ( ) . to string ( ) ) ; },values override
if needed @$ update the maximum stack <PLACE_HOLDER> and number of locals @$ and stack map frames .,if ( current basic block != null ) { if ( compute == compute_all_frames || compute == compute_inserted_frames ) { current basic block . frame . execute ( opcodes . ldc @$ __num__ @$ constant symbol @$ symbol table ) ; } else { int size = relative stack size + ( is long or double ? __num__ : __num__ ) ; if ( size > max relative stack size ) { max relative stack size = size ; } relative stack size = size ; } },maximum stack
scroll down and verify that the old elements do n't have the <PLACE_HOLDER> any more,get grid element ( ) . get row ( __num__ ) ; assert false ( has css class ( row2 @$ __str__ ) ) ; assert false ( has css class ( cell4_2 @$ __str__ ) ) ;,elements have
start definitions root <PLACE_HOLDER>,xtw . write start element ( element_definitions ) ; xtw . set default namespace ( cmmn_namespace ) ; xtw . write default namespace ( cmmn_namespace ) ; xtw . write namespace ( xsi_prefix @$ xsi_namespace ) ; xtw . write namespace ( flowable_extensions_prefix @$ flowable_extensions_namespace ) ; xtw . write namespace ( cmmndi_prefix @$ cmmndi_namespace ) ; xtw . write namespace ( omgdc_prefix @$ omgdc_namespace ) ; xtw . write namespace ( omgdi_prefix @$ omgdi_namespace ) ; for ( string prefix : model . get namespaces ( ) . key set ( ) ) { if ( ! default namespaces . contains ( prefix ) && string utils . is not empty ( prefix ) ) xtw . write namespace ( prefix @$ model . get namespaces (,definitions root
in addition to changing the containing <PLACE_HOLDER> @$ inlining function declarations also changes the function name <PLACE_HOLDER> from the containing <PLACE_HOLDER> to the inner <PLACE_HOLDER> .,if ( is function declaration ) { compiler . report change to change scope ( value ) ; compiler . report change to enclosing scope ( value . get parent ( ) ) ; },declarations changes
do n't create future with sequenced future manager . otherwise session would receive discontinued sequence number @$ and it would make future work item 'keeping call sequence when session execute <PLACE_HOLDER> ' impossible .,return session result . create future with result ( result_error_permission_denied ) ;,session execute
target <PLACE_HOLDER> may not match connected <PLACE_HOLDER>,dc . send ( bb @$ p . get socket address ( ) ) ;,address match
l l e s t j <PLACE_HOLDER> 0000 m i,byte [ ] expected = { ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__,s t
search <PLACE_HOLDER> and give the <PLACE_HOLDER> to the appropriate child which has become visible .,get view ( ) . request focus ( ) ;,search focus
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default media snippet global profile = new crawl profile ( crawl_profile_snippet_global_media @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_snippet_global_media_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ true @$ false @$ false @$ true @$ true @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . ifexist @$ __str__ + crawl_profile_snippet_global_media @$ client identification . yacy intranet crawler agent name @$ null,ip match
we do n't know <PLACE_HOLDER> the last varbit is @$ so we just hit the end @$ then set it for future iterations,log . debug ( __str__ @$ i ) ; num varbits = i ; break ;,varbit is
running the 'reset expired ' logic should have no <PLACE_HOLDER> @$ the lock time is not yet passed,expired jobs = management service . execute command ( new find expired jobs cmd ( expired jobs pages size @$ job service configuration . get job entity manager ( ) ) ) ; assert equals ( __num__ @$ expired jobs . size ( ) ) ; assert job details ( true ) ;,logic have
id @$ input <PLACE_HOLDER> @$ transliterated <PLACE_HOLDER>,string data [ ] = { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ;,id transliterated
avoid some images do not have a <PLACE_HOLDER> .,if ( size > __num__ ) { point . x = __num__ ; point . y = __num__ ; },images have
unbuffered . will not play naughty <PLACE_HOLDER> with the file position .,data input stream metadata = new data input stream ( stream ) ;,unbuffered play
store the checksum for thw write <PLACE_HOLDER> so that on recovery we know if we have a consistent write <PLACE_HOLDER> on disk .,recovery file . write long ( checksum . get value ( ) ) ;,checksum write
the processor does n't taint the consumer <PLACE_HOLDER> which has already finished,span processor span = take span ( consumer spans ) ; assert that ( processor span . id ( ) ) . is not equal to ( consumer span . id ( ) ) ;,processor taint
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default media snippet local profile = new crawl profile ( crawl_profile_snippet_local_media @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_snippet_local_media_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ true @$ false @$ false @$ false @$ true @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . ifexist @$ __str__ + crawl_profile_snippet_local_media @$ client identification . yacy intranet crawler agent name @$ null,content match
the parsed string has a relative <PLACE_HOLDER>,if ( ! relative contributors . is empty ( ) ) { array memory relative = array memory . create hashed ( relative_contributor_names . length ) ; array . ref of index ( __str__ ) . assign ( relative ) ; arrays . stream ( relative_contributor_names ) . for each ( name -> { long value = relative contributors . get or default ( name @$ __num__ ) ; relative . ref of index ( name ) . assign ( value ) ; } ) ; arrays . stream ( relative_bool_contributor_names ) . filter ( relative contributors :: contains key ) . for each ( name -> relative . ref of index ( name ) . assign ( memory . true ) ) ; } return array ;,string has
<PLACE_HOLDER> 6 : dest content changed @$ source content changed <PLACE_HOLDER> 7 : dest name change & content changed @$ source name changed & content changed <PLACE_HOLDER> 8 : dest name changed & content changed @$ source content changed <PLACE_HOLDER> 9 : dest content changed @$ source name changed & content changed,if ( tree structure changed ( orig root @$ latest root ) && tree structure changed ( orig root @$ my root ) ) { keep other or create tree ( orig root @$ my root @$ result root @$ i + __num__ ) ; } else if ( name changed ( orig root @$ latest tree name ) && tree structure changed ( orig root @$ latest root ) && name changed ( orig root @$ my tree name ) ) { names content changed ( my root @$ my tree name @$ result tree name @$ orig root @$ i + __num__ ) ; } else if ( name changed ( orig root @$ latest tree name ) && name changed ( orig root @$ my,name changed
problem when shutdown is called on the cloud eureka client where the application info manager bean is requested but wont be allowed because we are shutting down . to avoid this we use the <PLACE_HOLDER> directly .,application info manager app manager ; if ( aop utils . is aop proxy ( manager ) ) { app manager = proxy utils . get target object ( manager ) ; } else { app manager = manager ; } cloud eureka client cloud eureka client = new cloud eureka client ( app manager @$ config @$ this . optional args @$ this . context ) ; cloud eureka client . register health check ( health check handler ) ; return cloud eureka client ;,problem use
if prehash make same <PLACE_HOLDER> with sha 1 to check expected code .,if ( pre hash && ! proc name . equals ( __str__ ) ) { params . put ( __str__ @$ get hashed password forhttp var ( password @$ client auth scheme . hash_sha1 ) ) ; call proc overjson raw ( params @$ http port @$ expected code @$ session id ) ; },prehash make
if this is a property on an object literal @$ always expect an <PLACE_HOLDER> somewhere,if ( name . get parent ( ) . is object literal ( ) ) { return true ; },somewhere expect
read default write object <PLACE_HOLDER>,boolean called default write object = read boolean ( ) ; read object state . begin unmarshal custom value ( this @$ called default write object @$ ( current class desc . read object method != null ) ) ; if ( current class desc . has read object ( ) ) set state ( in_read_object_remote_not_custom_marshaled ) ;,default write
try to connect client 2 with no credentials verify that the creation of region throws security <PLACE_HOLDER>,if ( gen . class code ( ) . equals ( credential generator . class code . ssl ) ) { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ noforce_authreq_exception ) ) ; client2 . invoke ( ( ) -> do puts ( __num__ @$ other_exception ) ) ; } else { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ authreq_exception ) ) ; },creation throws
if the user revoked the api <PLACE_HOLDER> @$ we can delete it,if ( revoked . is legacy ( ) ) { p . api token = null ; },user revoked
the cache limits the <PLACE_HOLDER> ordering to available memory,add combination values ( __str__ @$ new object [ ] { new boolean ( false ) } ) ;,cache limits
volatile image we immediately punt in subclasses . if this poses a problem we 'll need a more sophisticated detection <PLACE_HOLDER> @$ or api .,this ( buffer_strategy_specified_off ) ;,image need
note to translators : the stylesheet contained an element that was not recognized as part of the xsl syntax . the substitution text gives the element <PLACE_HOLDER> .,return new object [ ] [ ] { { basis library . run_time_internal_err @$ __str__ } @$ { basis library . run_time_copy_err @$ __str__ } @$ { basis library . data_conversion_err @$ __str__ } @$ { basis library . external_func_err @$ __str__ } @$ { basis library . equality_expr_err @$ __str__ } @$ { basis library . invalid_argument_err @$ __str__ } @$ { basis library . format_number_err @$ __str__ } @$ { basis library . iterator_clone_err @$ __str__ } @$ { basis library . axis_support_err @$ __str__ } @$ { basis library . typed_axis_support_err @$ __str__ } @$ { basis library . stray_attribute_err @$ __str__ } @$ { basis library . stray_namespace_err @$ __str__ } @$ { basis library . namespace_prefix_err @$ __str__ } @$ { basis library,text gives
clear memory which corresponds to all code blocks no <PLACE_HOLDER> to remove context since this will happen when instruction is added,if ( overwrite ) { for ( address range range : instruction set . get address set ( ) ) { clear code units ( range . get min address ( ) @$ range . get max address ( ) @$ false @$ task monitor adapter . dummy_monitor ) ; } } else { check instruction set ( instruction set @$ skip delay slots ) ; },memory blocks
collect all metadata @$ newer <PLACE_HOLDER> override older <PLACE_HOLDER>,if ( is newer ( res @$ val ) ) { meta = merge meta ( val . get meta data ( ) @$ meta ) ; res . set ( val ) ; } else { meta = merge meta ( meta @$ val . get meta data ( ) ) ; },values override
start a group even if the group has only one <PLACE_HOLDER> or is empty .,helper . start group ( ) ;,group has
after setting the playback speed @$ reset m is rewinding <PLACE_HOLDER> .,m is rewinding = false ;,m rewinding
check if all implementation methods already have a common <PLACE_HOLDER> assigned . each implementation method can have multiple <PLACE_HOLDER>s because of interfaces . we compute the intersection of the <PLACE_HOLDER> sets for all implementation methods .,set < integer > result slots = vtables slots . get ( method . implementations [ __num__ ] ) ; for ( hosted method impl : method . implementations ) { set < integer > impl slots = vtables slots . get ( impl ) ; if ( impl slots == null ) { result slots = null ; break ; } result slots . retain all ( impl slots ) ; } if ( result slots != null && ! result slots . is empty ( ) ) { int result slot = integer . max_value ; for ( int slot : result slots ) { result slot = math . min ( result slot @$ slot ) ; } return result slot ; },methods have
selection and tap can move <PLACE_HOLDER> from this tap position .,final float eventx = event . getx ( ) ; final float eventy = event . gety ( ) ; final boolean is mouse = event . is from source ( input device . source_mouse ) ; switch ( event . get action masked ( ) ) { case motion event . action_down : if ( extracted text mode will be started ( ) ) { hide ( ) ; } else { m min touch offset = m max touch offset = m text view . get offset for position ( eventx @$ eventy ) ; if ( m gesture stayed in tap region ) { if ( m tap state == tap_state_double_tap || m tap state == tap_state_triple_click ) { final float deltax = eventx -,selection move
make the consumer return a single <PLACE_HOLDER> for each partition,when ( consumer mock . poll ( any long ( ) ) ) . then return ( new consumer records < > ( collections . singleton map ( partition that will be revoked @$ spout with mocked consumer setup helper . create records ( partition that will be revoked @$ __num__ @$ __num__ ) ) ) ) . then return ( new consumer records < > ( collections . singleton map ( assigned partition @$ spout with mocked consumer setup helper . create records ( assigned partition @$ __num__ @$ __num__ ) ) ) ) . then return ( new consumer records < > ( collections . empty map ( ) ) ) ;,consumer return
this conditional is for test acl submit <PLACE_HOLDER> where app is rejected and no app is added .,if ( scheduler . get scheduler applications ( ) . contains key ( id . get application id ( ) ) ) { scheduler . add application attempt ( id @$ false @$ false ) ; } list < resource request > ask = new array list < > ( requests ) ; rm app rm app = mock ( rm app . class ) ; rm app attempt rm app attempt = mock ( rm app attempt . class ) ; when ( rm app . get current app attempt ( ) ) . then return ( rm app attempt ) ; when ( rm app attempt . getrm app attempt metrics ( ) ) . then return ( new rm app attempt metrics ( id @$,acl submit
test that it works when the <PLACE_HOLDER> timeout kills the outstanding <PLACE_HOLDER> ...,test shutdown request outstanding ( __num__ @$ __num__ @$ remote invocation exception . class @$ exception . class ) ;,timeout kills
1 : no stats since hbm.xml ca n't enable natural id <PLACE_HOLDER>,assert equals ( __num__ @$ session factory ( ) . get statistics ( ) . get natural id cache put count ( ) ) ;,stats enable
take the julian <PLACE_HOLDER> for compatibility @$ which will produce a gregorian <PLACE_HOLDER> .,fixed date = jfd ;,which produce
the following test uses a query that returns a group and a user entry . the ldap atn should use the group membership <PLACE_HOLDER> to identify the users for the returned group and the authentication should succeed for the users of that group as well as the lone user 4 in this case,test case = default builder ( ) . basedn ( __str__ ) . userdn patterns ( __str__ @$ __str__ ) . custom query ( string . format ( __str__ @$ group1_name @$ user4 . get id ( ) ) ) . build ( ) ; test case . assert authenticate passes ( user1 . credentials with id ( ) ) ; test case . assert authenticate passes ( user1 . credentials with dn ( ) ) ; test case . assert authenticate passes ( user4 . credentials with id ( ) ) ; test case . assert authenticate passes ( user4 . credentials with dn ( ) ) ; test case = default builder ( ) . basedn ( __str__ ) . userdn patterns ( __str__ @$ __str__,atn use
instances @$ one class at a time @$ in sorted order @$ printing each <PLACE_HOLDER>,if ( true ) { system . out . println ( __str__ + __str__ ) ; list < reference type > rt list = new array list ( __num__ ) ; rt list . add ( null ) ; long start1 = system . current time millis ( ) ; size = __num__ ; long count = __num__ ; for ( int ii = sorted . size ( ) - __num__ ; ii >= __num__ ; ii -- ) { to sort xxx = sorted . get ( ii ) ; if ( xxx . count <= cut_off ) { break ; } rt list . set ( __num__ @$ xxx . rt ) ; start = system . current time millis ( ) ; list < object,instances printing
infinite loop to mimic heavy processing thread 't 1 ' wo n't leave this method when thread 't <PLACE_HOLDER> ' enters this,while ( true ) { },thread "t"
closing region with colocated regions will throw an <PLACE_HOLDER> and the region will not be closed .,accessor . invoke ( ( ) -> pr colocationd unit test . close region with colocated regions ( customer partitioned region name @$ false ) ) ;,region throw
this is to test that the background thread created by the adapter is retaining the stream <PLACE_HOLDER> properly . if it were not @$ the program would crash after this method exits and its outer autorelease pool drains .,assert not null ( stream ) ;,thread retaining
middle word gets integer <PLACE_HOLDER> ; lower longword is cleared .,if ( abs round power < two_x_longword_decimal_digits ) { final int adjusted abs power = abs round power - longword_decimal_digits ; final long round factor = power of ten table [ adjusted abs power ] ; result0 = __num__ ; result1 = ( ( fast1 / round factor ) * round factor ) ; result2 = fast2 ; } else { final int adjusted abs power = abs round power - two_x_longword_decimal_digits ; final long round factor = power of ten table [ adjusted abs power ] ; result0 = __num__ ; result1 = __num__ ; result2 = ( ( fast2 / round factor ) * round factor ) ; },word gets
the user asked for stats to be collected . some stats like number of rows require a <PLACE_HOLDER> of the data however @$ some other stats @$ like number of files @$ do not require a complete <PLACE_HOLDER> update the stats which do not require a complete <PLACE_HOLDER> .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },which require
get the point which are near the cell covering the <PLACE_HOLDER> and outside of the <PLACE_HOLDER>,m_points for polygons . add ( geography point value . normalize lng lat ( center longitude + radius in degrees @$ center latitude + radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude - radius in degrees @$ center latitude + radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude - radius in degrees @$ center latitude - radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude + radius in degrees @$ center latitude - radius in degrees ) ) ;,which covering
value that has been set is new or modified bring the binary in sync so that the deserialization gives the correct <PLACE_HOLDER>,if ( offset == modified_indicator_offset ) { update binary represenation ( ) ; offset = this . offsets [ field num ] ; },deserialization gives
suppress all application not found <PLACE_HOLDER> for now .,continue ;,suppress found
if java did atomic <PLACE_HOLDER> @$ this would be one,synchronized ( this ) { if ( auto registration running || auto registration complete ) { return ; } },java did
normalize host name and pattern by turning them into absolute domain names if they are not yet absolute . this is needed because server certificates do not normally contain absolute names or patterns @$ but they should be treated as absolute . at the same time @$ any host name presented to this method should also be treated as absolute for the purposes of,if ( ! host name . ends with ( __str__ ) ) { host name += __str__ ; } if ( ! pattern . ends with ( __str__ ) ) { pattern += __str__ ; },absolute matches
if the qualifier expression contains <PLACE_HOLDER> @$ give up attribution of method reference,if ( expr type . is erroneous ( ) ) { result = that . type = expr type ; return ; },expression contains
check that corrupting our array does n't affect other <PLACE_HOLDER> .,exceptions [ __num__ ] = null pointer exception . class ; exceptions = method . get exception types ( ) ; assert equals ( __num__ @$ exceptions . length ) ; assert equals ( index out of bounds exception . class @$ exceptions [ __num__ ] ) ;,check affect
total dirs includes root <PLACE_HOLDER>,assert equals ( dir count + __num__ @$ total dirs ) ; file status max file = collections . max ( written files . values ( ) @$ new comparator < file status > ( ) { @ override public int compare ( file status first @$ file status second ) { return first . get len ( ) < second . get len ( ) ? - __num__ : ( ( first . get len ( ) == second . get len ( ) ) ? __num__ : __num__ ) ; } } ) ; p = pattern . compile ( __str__ ) ; matcher = p . matcher ( output . to string ( __str__ ) ) ; assert true ( matcher . find ( ),dirs includes
make the title fragment occupy the entire <PLACE_HOLDER>,if ( null == m fragment manager . find fragment by id ( r . id . quote_fragment_container ) ) { m title frame layout . set layout params ( new linear layout . layout params ( match_parent @$ match_parent ) ) ; m quotes frame layout . set layout params ( new linear layout . layout params ( __num__ @$ match_parent ) ) ; } else { m title frame layout . set layout params ( new linear layout . layout params ( __num__ @$ match_parent @$ __num__ ) ) ; m quotes frame layout . set layout params ( new linear layout . layout params ( __num__ @$ match_parent @$ __num__ ) ) ; },fragment occupy
sometimes safari does not render <PLACE_HOLDER> correctly when attaching . setting the visibility to hidden and a bit later restoring will make everything just fine .,if ( browser info . get ( ) . is safari ( ) ) { get element ( ) . get style ( ) . set visibility ( style . visibility . hidden ) ; scheduler . get ( ) . schedule finally ( ( ) -> { get element ( ) . get style ( ) . set visibility ( style . visibility . visible ) ; } ) ; },safari render
add <PLACE_HOLDER> & null objects add <PLACE_HOLDER> & null objects,bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection,integer add
only unzip zip <PLACE_HOLDER> .,reader = new filtered data entry reader ( new data entry name filter ( new extension matcher ( __str__ ) ) @$ zip reader @$ reader ) ;,unzip zip
iterate over the instructions @$ adding information for each place that the active variable set <PLACE_HOLDER> .,for ( int i = __num__ ; i < insn sz ; i ++ ) { if ( can throw during last insn && ( i == freeze secondary state at ) ) { primary state . set immutable ( ) ; primary state = primary state . mutable copy ( ) ; } ssa insn insn = insns . get ( i ) ; register spec result ; result = insn . get local assignment ( ) ; if ( result == null ) { result = insn . get result ( ) ; if ( result != null && primary state . get ( result . get reg ( ) ) != null ) { primary state . remove ( primary state . get ( result,variable set
if mode is add <PLACE_HOLDER> check if the repository name does not exist in the repository list <PLACE_HOLDER> close this dialog if mode is edit <PLACE_HOLDER> check if the repository name is the same as before if not check if the new name does not exist in the repository . otherwise return true to this method @$ which will mean that repository already exist,if ( input . get description ( ) != null && input . get description ( ) . length ( ) > __num__ ) { if ( mode == mode . add ) { if ( master repositories meta . search repository ( input . get name ( ) ) == null ) { dispose ( ) ; } else { display repository already exist message ( input . get name ( ) ) ; } } else { if ( master repository name . equals ( input . get name ( ) ) ) { dispose ( ) ; } else if ( master repositories meta . search repository ( input . get name ( ) ) == null ) { dispose ( ) ; } else,name exist
methods with 1 implementations do not need a <PLACE_HOLDER> because invokes can be done as direct calls without the need for a <PLACE_HOLDER> . methods with 0 implementations are unreachable .,if ( method . wrapped . is invoked ( ) || method . wrapped . is implementation invoked ( ) ) { if ( method . implementations . length > __num__ ) { int slot = find slot ( method @$ vtables map @$ used slots map @$ vtables slots ) ; method . vtable index = slot ; assign implementations ( method . get declaring class ( ) @$ method @$ slot @$ vtables map ) ; } },methods need
phreak must also remove the <PLACE_HOLDER> from the rule network evaluator,if ( match . get rule agenda item ( ) != null ) { if ( left tuple . get memory ( ) != null ) { left tuple . get memory ( ) . remove ( left tuple ) ; } },phreak remove
first see if we need to filter @$ and if so <PLACE_HOLDER> the new size will be,sz = insns . size ( ) ; for ( int i = __num__ ; i < sz ; i ++ ) { if ( insns . get ( i ) . get opcode ( ) != rops . move_return_address ) { new sz ++ ; } } if ( new sz == sz ) { return insns ; },size be
if the request capacity does not require <PLACE_HOLDER> @$ just update the length of the memory .,if ( ! chunk . unpooled ) { if ( new capacity > length ) { if ( new capacity <= max length ) { length = new capacity ; return this ; } } else if ( new capacity > max length > > > __num__ && ( max length > __num__ || new capacity > max length - __num__ ) ) { length = new capacity ; trim indices to capacity ( new capacity ) ; return this ; } },capacity require
change the mob compaction merge <PLACE_HOLDER>,conf . set long ( mob constants . mob_compaction_mergeable_threshold @$ merge size ) ; common policy test logic ( __str__ @$ mob compact partition policy . monthly @$ false @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } @$ true ) ;,compaction merge
note : another sanitize because camelize can create an invalid <PLACE_HOLDER>,return sanitize kotlin specific names ( modified ) ;,camelize create
some adjustments to not crash the <PLACE_HOLDER> with emtpy data,if ( m max elements == __num__ ) { m max elements = __num__ ; } if ( m mcount == __num__ ) { m mcount = __num__ ; } if ( m first element == m last element ) { m first element = __num__ ; m last element = __num__ ; } if ( m max cards == __num__ ) { m max cards = __num__ ; } return list . size ( ) > __num__ ;,adjustments crash
current logic overflows at 30 @$ so set <PLACE_HOLDER> to max,if ( count >= __num__ ) { return m max sleep ms ; } else { int sleep ms = m base sleep time ms * ( thread local random . current ( ) . next int ( __num__ << count @$ __num__ << ( count + __num__ ) ) ) ; return math . min ( abs ( sleep ms @$ m max sleep ms ) @$ m max sleep ms ) ; },logic overflows
store it because need elements of it to print <PLACE_HOLDER> in output,stored header = line ;,elements print
should only clear this if the last schema had identity <PLACE_HOLDER> .,if ( ! f may match field map . is empty ( ) ) { f may match field map . clear ( ) ; },schema had
ensure the invariant that either all main accounts have a <PLACE_HOLDER> set @$ or none .,if ( account one password set != string utils . is not empty ( account two password ) || account one password set != string utils . is not empty ( account three password ) ) { throw new illegal argument exception ( ) ; },accounts have
| | ft instanceof g fx define compacted <PLACE_HOLDER>,if ( main panel . is internal flash viewer selected ( ) ) { image panel . set timelined ( main panel . make timelined ( font tag @$ font page num ) @$ font tag . get swf ( ) @$ __num__ ) ; },fx define
now the real test ! set up <PLACE_HOLDER> to be able to pick up correct tokens .,security util . set security info providers ( new custom security info ( ) ) ; token < client toam token identifier > token = converter utils . convert from yarn ( original client toam token @$ am . address ) ;,! set
host must have real keyboard <PLACE_HOLDER> .,if ( ! m host . is focused ( ) && ! m host . request focus ( ) ) { return false ; },host have
since this was sorted in reverse order @$ <PLACE_HOLDER> ' 0 ' contains the highest ai @$ and <PLACE_HOLDER> ' 1 ' tells us the <PLACE_HOLDER> of the corresponding sentence in its array .,ay eye ray [ sentence number ] = all anonymity indices . get ( sentence number ) [ quality rank ] [ __num__ ] ;,ai tells
verify that new job requests have no <PLACE_HOLDER> .,job runnable = submit concurrent jobs ( __num__ @$ config @$ false @$ false @$ submit job helper . get delayed resonse answer ( __num__ @$ __num__ ) @$ kill job helper . get delayed resonse answer ( __num__ @$ status bean ) @$ __str__ ) ; assert true ( job runnable . exception == null ) ;,requests have
aggregates with <PLACE_HOLDER> aggregate functions cost a bit <PLACE_HOLDER>,float multiplier = __num__ + ( float ) agg calls . size ( ) * __num__ ; for ( aggregate call agg call : agg calls ) { if ( agg call . get aggregation ( ) . get name ( ) . equals ( __str__ ) ) { multiplier += __num__ ; } },aggregates cost
following should throw null pointer <PLACE_HOLDER> .,select results sr = ( select results ) q . execute ( ) ; assert true ( __str__ @$ ( ( range index test hook ) hook ) . is hooked ( __num__ ) ) ; range index . set test hook ( null ) ;,following throw
make sure <PLACE_HOLDER> cache is filled so that initial allocations wo n't sl<PLACE_HOLDER>e high <PLACE_HOLDER> unnecessarily .,try ( indexed id generator freelist = new indexed id generator ( page cache @$ directory . file ( __str__ ) @$ immediate ( ) @$ id type . node @$ false @$ ( ) -> __num__ @$ long . max_value @$ false ) ) { freelist . maintenance ( ) ; race race = new race ( ) ; work sync < indexed id generator @$ ids > work sync = new work sync < > ( freelist ) ; for ( int t = __num__ ; t < threads ; t ++ ) { int thread = t ; race . add contestant ( throwing ( ( ) -> { int cursor = __num__ ; for ( int i = __num__ ; i < transactions_per_thread ;,allocations slide
the system bars are visible . make any desired <PLACE_HOLDER> to your ui @$ such as showing the action bar or other navigational controls .,mainui . set immersive mode ( false ) ; set immersive timer ( ) ; if ( my debug . log ) log . d ( tag @$ __str__ ) ;,system make
create a second instance @$ which will pick up the same <PLACE_HOLDER>,try ( s3a file system second delegate = news3a instance ( uri @$ conf ) ) { assert bound todt ( second delegate @$ token kind ) ; if ( encryption test enabled ( ) ) { assert not null ( __str__ @$ second delegate . get server side encryption algorithm ( ) ) ; assert equals ( __str__ @$ fs . get server side encryption algorithm ( ) @$ second delegate . get server side encryption algorithm ( ) ) ; } contract test utils . assert deleted ( second delegate @$ test path @$ true ) ; assert not null ( __str__ @$ second delegate . get delegation token ( __str__ ) ) ; },which pick
callee of the constructor guarantees dynamic bucket node map will only be used when the stage has no remote <PLACE_HOLDER> . when the stage has no remote <PLACE_HOLDER> @$ any scan is grouped execution guarantees all scan is grouped execution .,if ( bucket node map . is dynamic ( ) ) { lifespan scheduler = new dynamic lifespan scheduler ( bucket node map @$ nodes @$ partition handles @$ concurrent lifespans per task ) ; } else { lifespan scheduler = new fixed lifespan scheduler ( bucket node map @$ partition handles @$ concurrent lifespans per task ) ; },stage has
read field offset <PLACE_HOLDER>,access_flags_offset = db . lookup int constant ( __str__ ) . int value ( ) ; name_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; signature_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; initval_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; low_offset = db . lookup int constant ( __str__ ) . int value ( ) ; high_offset = db . lookup int constant ( __str__ ) . int value ( ) ; field_slots = db . lookup int constant ( __str__ ) . int value ( ) ; fieldinfo_tag_size = db . lookup int constant ( __str__ ) . short value ( ) ;,field offset
important : first add factory <PLACE_HOLDER> ; then constructors @$ so latter can override former !,_add factory creators ( ctxt @$ bean desc @$ vchecker @$ intr @$ creators @$ creator defs ) ;,first add
instead @$ the calling code is responsible for the synchronization . see <PLACE_HOLDER> for details .,return get components_ no client code ( ) ;,synchronization see
the address should have <PLACE_HOLDER>,deque < property > result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; model node handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; string file name = null ;,address have
dont do <PLACE_HOLDER>,;,dont do
now we know the bytes contained <PLACE_HOLDER> we need to rewrite :,class reader file reader = new class reader ( bytes ) ; rewrite class adaptor class adaptor = new rewrite class adaptor ( type registry ) ; try { file reader . accept ( class adaptor @$ class reader . skip_frames ) ; } catch ( dont rewrite exception drex ) { return bytes ; },bytes contained
remove the placeholder seek <PLACE_HOLDER> and replace it with the range seek <PLACE_HOLDER> .,seek bar = ( seek bar ) view . find view by id ( r . id . repeat_song_range_placeholder_seekbar ) ; relative layout . layout params params = ( relative layout . layout params ) seek bar . get layout params ( ) ; view group = ( view group ) seek bar . get parent ( ) ; view group . remove view ( seek bar ) ; range seek bar = new range seek bar < integer > ( __num__ @$ current song duration secs @$ get activity ( ) ) ; range seek bar . set layout params ( params ) ; view group . add view ( range seek bar ) ; if ( shared preferences . get int ( common . repeat_mode @$,placeholder seek
we found the id . deleting the item via the content provider will also remove the <PLACE_HOLDER>,if ( c . move to first ( ) ) { long id = c . get long ( c . get column index or throw ( media store . images . media . _id ) ) ; uri delete uri = content uris . with appended id ( media store . images . media . external_content_uri @$ id ) ; content resolver . delete ( delete uri @$ null @$ null ) ; } else { },id remove
must use raw local because the checksummer does n't honor <PLACE_HOLDER> .,file system fs = file system . get local ( conf ) . get raw ( ) ; object inspector inspector ; synchronized ( test orc file . class ) { inspector = object inspector factory . get reflection object inspector ( my row . class @$ object inspector factory . object inspector options . java ) ; } properties tbl props = new properties ( ) ; tbl props . set property ( __str__ @$ __str__ ) ; tbl props . set property ( __str__ @$ __str__ ) ; hive conf . set int var ( conf @$ hive conf . conf vars . hive_orc_base_delta_ratio @$ __num__ ) ; acid output format . options options = new acid output format . options ( conf ) . filesystem,checksummer honor
when entity expansion limit is set by the application @$ we need to check for the entity expansion limit set by the parser @$ if number of entity expansions exceeds the entity expansion limit @$ parser will throw fatal <PLACE_HOLDER> . note that this represents the nesting level of open entities .,f entity expansion count ++ ; if ( f limit analyzer != null ) { f limit analyzer . add value ( entity expansion index @$ name @$ __num__ ) ; } if ( f security manager != null && f security manager . is over limit ( entity expansion index @$ f limit analyzer ) ) { f security manager . debug print ( f limit analyzer ) ; f error reporter . report error ( xml message formatter . xml_domain @$ __str__ @$ new object [ ] { f security manager . get limit value by index ( entity expansion index ) } @$ xml error reporter . severity_fatal_error ) ; f entity expansion count = __num__ ; },parser throw
the main thread will send a <PLACE_HOLDER>,work unit wu = e . get current work unit ( ) ; if ( wu . is main work ( ) ) { if ( problems == null ) { future . set ( executable ) ; e . get owner ( ) . task completed ( e @$ task @$ duration ) ; } else { future . set ( problems ) ; e . get owner ( ) . task completed with problems ( e @$ task @$ duration @$ problems ) ; } },thread send
if event has this <PLACE_HOLDER> to filter on,if ( event dimension transformed . contains ( filter dimension transformed ) || filter dimension transformed . contains ( event dimension transformed ) ) { set < string > event dimension values set = new hash set < > ( event dimension values transformed ) ; event dimension values set . retain all ( filtered dimension values transformed ) ; if ( ! event dimension values set . is empty ( ) ) { filtered events . add ( event ) ; event added = true ; break ; } },event has
client sends an eds <PLACE_HOLDER> containing all clusters being watched to management server .,verify ( request observer ) . on next ( arg that ( new discovery request matcher ( __str__ @$ immutable list . of ( __str__ @$ __str__ ) @$ xds client impl . ads_type_url_eds @$ __str__ ) ) ) ;,client sends
the automation service can suppress other <PLACE_HOLDER> .,final user state user state = get user state locked ( resolved user id ) ; if ( m ui automation manager . suppressing accessibility services locked ( ) ) { return collections . empty list ( ) ; } final list < accessibility service connection > services = user state . m bound services ; final int service count = services . size ( ) ; final list < accessibility service info > result = new array list < > ( service count ) ; for ( int i = __num__ ; i < service count ; ++ i ) { final accessibility service connection service = services . get ( i ) ; if ( ( service . m feedback type & feedback type ) !=,service suppress
create a build <PLACE_HOLDER> that gives 3 million pairs with 3 values sharing the same key @$ plus 400 k pairs with two colliding keys,mutable object iterator < binary row > build1 = new uniform binary row generator ( num keys @$ build vals per key @$ false ) ; mutable object iterator < binary row > build2 = new binary hash table test . constants key value pairs iterator ( repeated value1 @$ __num__ @$ repeated value count build ) ; mutable object iterator < binary row > build3 = new binary hash table test . constants key value pairs iterator ( repeated value2 @$ __num__ @$ repeated value count build ) ; list < mutable object iterator < binary row > > builds = new array list < > ( ) ; builds . add ( build1 ) ; builds . add ( build2 ) ; builds . add (,a build
remove any that are constants @$ or expressions where all params exactly match a <PLACE_HOLDER> by expression :,if ( ! unmatched selects . is empty ( ) ) { throw new ksql exception ( __str__ + unmatched selects ) ; } final set view < column reference exp > unmatched selects agg = sets . difference ( aggregate analysis . get aggregate select fields ( ) @$ group by exprs ) ; if ( ! unmatched selects agg . is empty ( ) ) { throw new ksql exception ( __str__ + __str__ + unmatched selects agg ) ; } final set < column reference exp > having columns = aggregate analysis . get non aggregate having fields ( ) ; final set < column reference exp > having only = sets . difference ( having columns @$ group by exprs ) ; if (,params match
the keys of probe and build <PLACE_HOLDER> are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer join works well in this case .,final int probe vals per key = __num__ ;,keys build
there should be at least two iterations of this loop because reset thread loopers calls <PLACE_HOLDER> ' on background loopers once @$ which also resets the scheduler .,for ( int i = __num__ ; i < __num__ ; i ++ ) { assert that ( shadow of ( background looper ) . get scheduler ( ) . size ( ) ) . is equal to ( __num__ ) ; assert that ( shadow of ( background looper ) . get scheduler ( ) . get current time ( ) ) . is equal to ( __num__ ) ; handler . post ( empty ) ; handler . post delayed ( empty @$ __num__ ) ; shadow of ( background looper ) . run to end of tasks ( ) ; assert that ( shadow of ( background looper ) . get scheduler ( ) . get current time ( ) ) . is equal to,loopers calls
second input parameter but 3 rd <PLACE_HOLDER> .,return get column param string ( __num__ @$ arg1 column ) + __str__ + get column param string ( __num__ @$ arg3 column ) ;,parameter rd
constructor is not visible and config seems to be null at this point @$ hence we can not use the build <PLACE_HOLDER> here,constructor < transfer manager builder > tmb constructor = transfer manager builder . class . get declared constructor ( config . class @$ transfer manager . class ) ; tmb constructor . set accessible ( true ) ; return tmb constructor . new instance ( config @$ transfer manager ) . with feature ( read after write consistent . class ) . as default ( ) ;,the build
only my parameter signature changed so auto merge my <PLACE_HOLDER> .,get merge my ( ) . replace function parameters ( entry @$ monitor ) ;,auto merge
add a new feed all new feeds will have the most recent <PLACE_HOLDER> marked as unplayed,feed item most recent = new feed . get most recent item ( ) ; if ( most recent != null ) { most recent . set new ( ) ; } new feeds list . add ( new feed ) ; result feeds [ feed idx ] = new feed ; log . d ( tag @$ __str__ + new feed . get title ( ) + __str__ ) ; collections . sort ( new feed . get items ( ) @$ new feed item pubdate comparator ( ) ) ; if ( new feed . get page nr ( ) == saved feed . get page nr ( ) ) { if ( saved feed . compare with other ( new feed ) ) { log,feeds marked
note : parser not working properly here @$ use url note : raw live format could crash <PLACE_HOLDER>,m listener . on dash url ( uri . parse ( m dashmpd url . to string ( ) ) ) ;,note crash
short cache ttl should trigger state <PLACE_HOLDER> if source is not never,mono cache time < integer > cached = new mono cache time < > ( source . mono ( ) @$ duration . of millis ( __num__ ) @$ schedulers . parallel ( ) ) ;,ttl trigger
n.b . missing format width exception is the only kind of illegal format exception whose constructor can take and display arbitrary error <PLACE_HOLDER> @$ hence its use below .,int length = pattern . length ( ) ; int arg length = arguments . size ( ) ; int i = __num__ ;,constructor take
bypass pe <PLACE_HOLDER> @$ but permit p es to complete ... valid docs wo n't care .,try { for ( ; ; ) { int c = getc ( ) ; if ( c == __str__ ) { c = getc ( ) ; if ( c != __str__ ) { if ( save comment text ) str tmp . append ( __str__ ) ; ungetc ( ) ; continue ; } next char ( __str__ @$ __str__ @$ null ) ; break one comment ; } if ( save comment text ) str tmp . append ( ( char ) c ) ; } } catch ( end of input exception e ) { if ( in . is internal ( ) ) { error ( __str__ @$ null ) ; } fatal ( __str__ ) ; },bypass pe
if the peer has an initializing <PLACE_HOLDER> @$ they are also not online .,if ( initializingid != null ) { remove newer persistentid ( offline members @$ initializingid ) ; } handle partially destroyed region ( offline members @$ members to wait for @$ persistentid @$ initializingid @$ disk storeid ) ;,peer has
two slashes means generic uri <PLACE_HOLDER> @$ so we get the authority,if ( ( ( index + __num__ ) < uri spec len ) && ( uri spec . substring ( index ) . starts with ( __str__ ) ) ) { index += __num__ ; int start pos = index ; char test char = __str__ ; while ( index < uri spec len ) { test char = uri spec . char at ( index ) ; if ( test char == __str__ || test char == __str__ || test char == __str__ ) { break ; } index ++ ; } if ( index > start pos ) { initialize authority ( uri spec . substring ( start pos @$ index ) ) ; } else { m_host = __str__ ; } },slashes means
the handler created a <PLACE_HOLDER> of the <PLACE_HOLDER> and is now done with it .,dup2 . release ( ) ;,handler created
cancel currently executing tasks wait a <PLACE_HOLDER> for tasks to respond to being cancelled,if ( ! executor . await termination ( __num__ @$ time unit . seconds ) ) { log . warn ( __str__ ) ; },tasks wait
the slow loading resource on that page takes 6 <PLACE_HOLDER> to return @$ but with 'none ' page loading strategy 'refresh ' operation should not wait .,assert that ( duration ) . as ( __str__ ) . is less than ( __num__ ) ;,resource takes
without limiting to be inside the parent <PLACE_HOLDER> @$ the out screen size should keep relative to the input <PLACE_HOLDER> .,final activity record . compat display insets compat intsets = new activity record . compat display insets ( display content ) ; task . compute config resource overrides ( in out config @$ parent config @$ compat intsets ) ; assert equals ( ( short side - status bar height ) * density_default / parent config . density dpi @$ in out config . screen height dp ) ; assert equals ( long side * density_default / parent config . density dpi @$ in out config . screen width dp ) ; assert equals ( configuration . orientation_landscape @$ in out config . orientation ) ;,size keep
note : the python version uses <PLACE_HOLDER> which return an empty list when indexed beyond what the list contains . since we ca n't slice out an empty sublist in java @$ we must check if we 've reached the end and clear the fnames list manually .,if ( cnt == fnames . size ( ) ) { fnames . clear ( ) ; } else { fnames = fnames . sub list ( cnt @$ fnames . size ( ) ) ; } m con . publish progress ( string . format ( anki droid app . get app resources ( ) . get string ( r . string . sync_media_downloaded_count ) @$ m download count ) ) ;,version uses
now get the operator class which drives the <PLACE_HOLDER>,final class < ? extends driver < s @$ ot > > driver class = this . config . get driver ( ) ; this . driver = instantiation util . instantiate ( driver class @$ driver . class ) ; string head name = get environment ( ) . get task info ( ) . get task name ( ) . split ( __str__ ) [ __num__ ] . trim ( ) ; this . metrics = get environment ( ) . get metric group ( ) . get or add operator ( head name . starts with ( __str__ ) ? head name . substring ( __num__ ) : head name ) ; this . metrics . getio metric group ( ) . reuse input metrics,which drives
we want to be compatible back to gingerbread @$ but surface texture was n't introduced until honeycomb . since the interface can not use a surface texture @$ if the developer wants to display a preview we must use a surface <PLACE_HOLDER> . if the developer does n't want to display a preview we use a surface texture if we are running at least,try { if ( build . version . sdk_int >= build . version_codes . honeycomb ) { m camera . set preview texture ( null ) ; } else { m camera . set preview display ( null ) ; } } catch ( exception e ) { log . e ( tag @$ __str__ + e ) ; },interface use
jersey does n't close the output stream when there is no entity as such the publisher needs to be closed from here ... it is assumed it 's possible to close the publisher @$ the output stream @$ multiple <PLACE_HOLDER>,try { publisher . close ( ) ; } catch ( io exception e ) { throw new illegal state exception ( __str__ @$ e ) ; },jersey close
raw types take this <PLACE_HOLDER> as well,if ( type instanceof class ) { return for class ( ( class < ? > ) type ) ; } else if ( type instanceof parameterized type ) { final parameterized type parameterized type = ( parameterized type ) type ; final type [ ] type arguments = parameterized type . get actual type arguments ( ) ; final java type definition [ ] generic bounds = new java type definition [ type arguments . length ] ; for ( int i = __num__ ; i < type arguments . length ; i ++ ) { generic bounds [ i ] = resolve type definition ( type arguments [ i ] @$ method @$ method type args ) ; } return for class ( ( class <,types take
rack 1 has file 1 @$ file <PLACE_HOLDER> and file 3 and file 4 rack <PLACE_HOLDER> has file <PLACE_HOLDER> and file 3 and file 4 rack 3 has file 3 and file 4 setup a filter so that only file 1 and file <PLACE_HOLDER> can be combined,in format = new dummy input format ( ) ; file input format . add input path ( job @$ in dir ) ; in format . set min split size rack ( __num__ ) ;,rack file
create the reveal animations that will run when the retreat passes <PLACE_HOLDER>,for ( int i = __num__ ; i < steps ; i ++ ) { reveal animations [ i ] = new pending reveal animator ( was - i @$ new leftward start predicate ( dot centerx [ was - i ] ) ) ; dots to hide [ i ] = was - i ; } add update listener ( new animator update listener ( ) { @ override public void on animation update ( value animator value animator ) { retreating joinx2 = ( float ) value animator . get animated value ( ) ; if ( build . version . sdk_int >= build . version_codes . jelly_bean ) { post invalidate on animation ( ) ; } else { post invalidate ( ) ; },retreat passes
completing them should just end the process <PLACE_HOLDER>,task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; assert equals ( __num__ @$ runtime service . create execution query ( ) . count ( ) ) ;,them end
check the base znodes <PLACE_HOLDER> first . only do the recursion if base znode 's perms are not correct .,try { list < acl > actual acls = recoverable zoo keeper . get acl ( znode paths . basez node @$ new stat ( ) ) ; if ( ! is base znode acl setup ( actual acls ) ) { log . info ( __str__ ) ; set znode acls recursive ( znode paths . basez node ) ; } } catch ( keeper exception . no node exception nne ) { return ; } catch ( interrupted exception ie ) { interrupted exception no throw ( ie @$ false ) ; } catch ( io exception | keeper exception e ) { log . warn ( __str__ @$ e ) ; },base znodes
first record contains header <PLACE_HOLDER>,if ( use headers ) { for ( int i = __num__ ; i < record . size ( ) && i < serieses . length ; i ++ ) { serieses [ i ] . set name ( record . get ( i ) ) ; } record = it . next ( ) ; },record contains
compute the missing segments if there are at least two segments and the table has time <PLACE_HOLDER>,int num missing segments = __num__ ; int num segments = offline segmentzk metadata list . size ( ) ; segments validation and retention config validation config = table config . get validation config ( ) ; if ( segment interval utils . eligible for missing segment check ( num segments @$ validation config ) ) { list < interval > segment intervals = new array list < > ( num segments ) ; int num segments with invalid intervals = __num__ ; for ( offline segmentzk metadata offline segmentzk metadata : offline segmentzk metadata list ) { interval time interval = offline segmentzk metadata . get time interval ( ) ; if ( time interval != null && time utils . is valid time interval ( time,table has
do abc tag <PLACE_HOLDER> does not have name,continue ;,abc tag
should we change the current <PLACE_HOLDER> ? do we have a <PLACE_HOLDER> set ?,if ( code set == __num__ ) { switch ( new code set ) { case code_code_a : pattern index = code_start_a ; break ; case code_code_b : pattern index = code_start_b ; break ; default : pattern index = code_start_c ; break ; } } else { pattern index = new code set ; },code have
if none of the arguments have a <PLACE_HOLDER> specified @$ we just send all the values as the entity body,if ( get send parameter values as post body ( ) ) { has entity body = true ; arguments arguments = get arguments ( ) ; string builder entity body content = new string builder ( arguments . get argument count ( ) * __num__ ) ; for ( j meter property j meter property : arguments ) { http argument arg = ( http argument ) j meter property . get object value ( ) ; if ( charset != null ) { entity body content . append ( arg . get encoded value ( charset ) ) ; } else { entity body content . append ( arg . get encoded value ( ) ) ; } } string entity request entity = new string,none have
java needs the obsolete <PLACE_HOLDER> @$ icu needs the modern <PLACE_HOLDER> @$ but we let icu know about both so that we never need to convert back when talking to it .,linked hash set < locale > set = new linked hash set < locale > ( ) ; for ( string locale name : locale names ) { set . add ( locale from string ( locale name ) ) ; } return set . to array ( new locale [ set . size ( ) ] ) ;,java needs
last name should have no <PLACE_HOLDER>,check tooltip ( $ ( text field element . class ) . get ( __num__ ) @$ null ) ;,name have
p 12 key<PLACE_HOLDER> currently does not support separate <PLACE_HOLDER> and entry passwords,if ( p12keystore . equals ignore case ( ks . get type ( ) ) ) { throw une2 ; } else { pkey = get key passwd ( alias @$ null @$ null ) ; pp = new password protection ( pkey ) ; entry = ks . get entry ( alias @$ pp ) ; },keystore support
if actual dimensions do n't match the declared <PLACE_HOLDER> @$ reset everything .,if ( this . s width > __num__ && this . s height > __num__ && ( this . s width != s width || this . s height != s height ) ) { reset ( false ) ; if ( bitmap != null ) { if ( ! bitmap is cached ) { bitmap . recycle ( ) ; } bitmap = null ; if ( on image event listener != null && bitmap is cached ) { on image event listener . on preview released ( ) ; } bitmap is preview = false ; bitmap is cached = false ; } } this . decoder = decoder ; this . s width = s width ; this . s height = s height ;,dimensions match
verify escaped partition names do n't return <PLACE_HOLDER>,exception thrown = false ; try { string bad part name = __str__ ; client . get partition ( db name @$ tbl name @$ bad part name ) ; } catch ( no such object exception e ) { exception thrown = true ; } assert true ( __str__ @$ exception thrown ) ; path part path = new path ( part . get sd ( ) . get location ( ) ) ; assert true ( fs . exists ( part path ) ) ; client . drop partition ( db name @$ tbl name @$ part . get values ( ) @$ true ) ; assert false ( fs . exists ( part path ) ) ;,names return
pairs now contains a uniquified list of the sorted inputs @$ with counts for how often that item appeared . now sort by how frequently they occur @$ and pick the most frequent . if the first place is tied between two @$ do n't pick <PLACE_HOLDER> .,collections . sort ( pairs ) ; final pair first pair = pairs . get ( __num__ ) ; if ( pairs . size ( ) == __num__ ) return first pair . item ; final pair second pair = pairs . get ( __num__ ) ; if ( first pair . count > second pair . count ) return first pair . item ; check state ( first pair . count == second pair . count ) ; return __num__ ;,list pick
dw <PLACE_HOLDER> specifies the <PLACE_HOLDER> of the data chunk @$ in bytes .,idx1 chunk . finish ( ) ;,size specifies
do bind implements all four <PLACE_HOLDER> of binding,naming context data store impl = ( naming context data store ) this ; do bind ( impl @$ n @$ nc @$ false @$ binding type . ncontext ) ;,bind implements
a task normally slowly accrues scheduled <PLACE_HOLDER> in a level and then moves to the next @$ but if the split had a particularly long quanta @$ accrue <PLACE_HOLDER> to each level as if it had run in that level up to the level limit .,for ( int current level = old level ; current level < new level ; current level ++ ) { long time accrued to level = math . min ( seconds . to nanos ( level_threshold_seconds [ current level + __num__ ] - level_threshold_seconds [ current level ] ) @$ remaining level contribution ) ; add level time ( current level @$ time accrued to level ) ; remaining level contribution -= time accrued to level ; remaining task time -= time accrued to level ; } add level time ( new level @$ remaining level contribution ) ; long new level min priority = get level min priority ( new level @$ scheduled nanos ) ; return new priority ( new level @$ new level min priority,task accrues
x attr exists @$ and replace it using create| replace <PLACE_HOLDER> .,fs . setx attr ( path @$ name1 @$ value1 @$ enum set . of ( x attr set flag . create ) ) ; fs . setx attr ( path @$ name1 @$ new value1 @$ enum set . of ( x attr set flag . create @$ x attr set flag . replace ) ) ; xattrs = fs . getx attrs ( path ) ; assert . assert equals ( xattrs . size ( ) @$ __num__ ) ; assert . assert array equals ( new value1 @$ xattrs . get ( name1 ) ) ; fs . removex attr ( path @$ name1 ) ;,create| replace
its important that we not store the stack <PLACE_HOLDER> in the weak order queue as the stack also is used in the weak hash map as key . so just store the enclosed atomic integer which should allow to have the stack <PLACE_HOLDER> g ced .,head = new head ( stack . available shared capacity ) ; head . link = tail ; interval = stack . interval ; handle recycle count = interval ;,which allow
both sides contain only one <PLACE_HOLDER>,join function . join ( firstv1 @$ firstv2 @$ collector ) ;,sides contain
creating the file makes the link <PLACE_HOLDER>,create and write file ( file abs ) ; wrapper . get file status ( link ) ;,file makes
circular reveal library uses absolute coordinates setup <PLACE_HOLDER>,support animator anim = io . codetail . animation . view animation utils . create circular reveal ( view @$ centerx @$ centery @$ start radius @$ end radius ) ; anim . set duration ( ( int ) duration ) ; anim . set interpolator ( interpolator ) ;,library uses
convert text <PLACE_HOLDER> to table <PLACE_HOLDER> . we assume the text <PLACE_HOLDER> must be the sql output . this assumption is correct for now . ideally livy should return table <PLACE_HOLDER> . we may do it in the future release of livy .,for ( interpreter result message message : result . message ( ) ) { if ( message . get type ( ) == interpreter result . type . text ) { list < string > rows = parsesql output ( message . get data ( ) ) ; result2 . add ( interpreter result . type . table @$ string utils . join ( rows @$ __str__ ) ) ; if ( rows . size ( ) >= ( max result + __num__ ) ) { result2 . add ( result messages . get exceeds limit rows message ( max result @$ zeppelin_livy_spark_sql_max_result ) ) ; } } else { result2 . add ( message . get type ( ) @$ message . get data ( ) ),livy return
lcn type need connection <PLACE_HOLDER>,dtx local context . make proxy ( ) ;,type need
two characters form the hex <PLACE_HOLDER> .,for ( int i = __num__ @$ j = __num__ ; j < len ; i ++ ) { int f = character . digit ( data [ j ] @$ __num__ ) << __num__ ; j ++ ; f = f | character . digit ( data [ j ] @$ __num__ ) ; j ++ ; out [ i ] = ( byte ) ( f & __num__ ) ; } return out ;,characters form
bugzilla 1133 : generate attribute as well as namespace event . sax does expect <PLACE_HOLDER> .,if ( get should outputns attr ( ) ) { this . add attribute always ( __str__ @$ no_prefix ? __str__ : prefix @$ no_prefix ? __str__ : ( __str__ + prefix ) @$ __str__ @$ ns @$ false ) ; },sax expect
now to test the case when rm already gave <PLACE_HOLDER> @$ and nm suddenly realizes that the container is running .,log . info ( __str__ + __str__ ) ; container statuses . clear ( ) ; container status list . clear ( ) ; container status list . add ( builder utils . new container status ( conts . get ( __num__ ) . get id ( ) @$ container state . running @$ __str__ @$ __num__ @$ conts . get ( __num__ ) . get resource ( ) ) ) ; container statuses . put ( app . get application id ( ) @$ container status list ) ; resp = nm1 . node heartbeat ( container statuses @$ true ) ;,rm gave
make sure the object outlives the <PLACE_HOLDER>,if ( ( flags & ( call_type_callback | call_type_global_value | call_type_struct_member ) ) > __num__ ) { o . retain ( ) ; },object outlives
if the last table does not contain any <PLACE_HOLDER> we still need to mark the last processed event as the last one,if ( snapshot context . last table ) { snapshot context . offset . mark last snapshot record ( ) ; },table contain
a volt db extension to customize the sql function set <PLACE_HOLDER>,volt disabled = disabled_in_functioncustom_constructor ;,extension customize
ensure that the notifier updated the <PLACE_HOLDER>,assert equals ( v1 @$ r . get entry ( k3 ) . get value ( ) ) ;,notifier updated
this call is checking the private field <PLACE_HOLDER> in the reloaded method has been changed to use the <PLACE_HOLDER>ors into the type that can <PLACE_HOLDER> the field from outside,method = st . get clazz ( ) . get method ( __str__ ) ; string = ( string ) method . invoke ( object ) ; assert equals ( __str__ @$ string ) ;,call checking
tab 2 has a custom <PLACE_HOLDER> @$ but no text or icon,tab = tab layout . get tab at ( __num__ ) ; assert null ( tab . get text ( ) ) ; assert null ( tab . get icon ( ) ) ; assert not null ( tab . get custom view ( ) ) ; assert equals ( r . id . my_custom_tab @$ tab . get custom view ( ) . get id ( ) ) ;,tab has
client sent an ack rds <PLACE_HOLDER> .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_rds @$ __str__ ) ) ) ;,client sent
have server write <PLACE_HOLDER> .,try ( blockhead connection server conn = server conn fut . get ( timeouts . connect @$ timeouts . connect_unit ) ) { byte [ ] payload = new byte [ buffer size / __num__ ] ; arrays . fill ( payload @$ ( byte ) __str__ ) ; byte buffer server frame = buffer util . allocate ( buffer size ) ; buffer util . flip to fill ( server frame ) ; server frame . put ( ( byte ) ( __num__ | __num__ ) ) ; server frame . put ( ( byte ) __num__ ) ; server frame . put ( ( byte ) ( payload . length > > __num__ ) ) ; server frame . put ( ( byte ) ( payload,server write
restrictions to match all valid <PLACE_HOLDER> .,final parameters valid = disjoint . add sub parameters ( __str__ ) ;,restrictions match
no need to sync because noone has <PLACE_HOLDER> to new info yet,new info . policy entries . add ( pe ) ;,noone has
while statement always has a <PLACE_HOLDER> of at least 1,bool comp while ++ ; entry stack . peek ( ) . bump decision points ( bool comp while ) ; super . visit ( node @$ data ) ; logger . exiting ( class_name @$ __str__ ) ; return data ;,statement has
normal passes use syntactic scope creator @$ so that 's <PLACE_HOLDER> we use here .,redeclaration handler redeclaration handler = ( scope s @$ string name @$ node n @$ compiler input input ) -> { } ; syntactic scope creator scope creator = new syntactic scope creator ( compiler @$ redeclaration handler ) ; return scope creator . create scope ( root @$ null ) ;,passes use
first callback fetches <PLACE_HOLDER> based on a query,data provider < person @$ ? > data provider = data provider . from callbacks ( query -> { int offset = query . get offset ( ) ; int limit = query . get limit ( ) ; list < person > persons = get person service ( ) . fetch persons ( offset @$ limit ) ; return persons . stream ( ) ; } @$ query -> get person service ( ) . get person count ( ) ) ;,callback fetches
these exist just to support the 'old ' lombok.experimental.builder @$ which had these properties . <PLACE_HOLDER> no longer has them .,boolean fluent = to boolean ( annotation . get actual expression ( __str__ ) @$ true ) ; boolean chain = to boolean ( annotation . get actual expression ( __str__ ) @$ true ) ; string builder method name = builder instance . builder method name ( ) ; string build method name = builder instance . build method name ( ) ; string builder class name = builder instance . builder class name ( ) ; string to builder method name = __str__ ; boolean to builder = builder instance . to builder ( ) ; java . util . list < name > type args for to builder = null ; if ( builder method name == null ) builder method name = __str__ ;,which had
if our locale does n't have a <PLACE_HOLDER> for some reason @$ then we do n't really have a reasonable default .,if ( text utils . is empty ( system locale . get language ( ) ) ) { return null ; } final list < keyboard layout > layouts = new array list < > ( ) ; visit all keyboard layouts ( new keyboard layout visitor ( ) { @ override public void visit keyboard layout ( resources resources @$ int keyboard layout res id @$ keyboard layout layout ) { if ( layout . get vendor id ( ) != d . get vendor id ( ) || layout . get product id ( ) != d . get product id ( ) ) { return ; } final locale list locales = layout . get locales ( ) ; final int num locales = locales,locale have
we want to differentiate between io <PLACE_HOLDER>s thrown by the repository and io <PLACE_HOLDER>s thrown from processor code . as a result @$ as have the flow file access input stream that catches io <PLACE_HOLDER> from the repository and translates into either flow file access <PLACE_HOLDER> or content not found <PLACE_HOLDER> . we keep track of any content not found <PLACE_HOLDER> because if it,final flow file access input stream ffais = new flow file access input stream ( counting in @$ source @$ curr claim ) ; final flow file access output stream ffaos = new flow file access output stream ( counting out @$ source ) ; boolean cnfe thrown = false ; try { writer . process ( create task termination stream ( ffais ) @$ create task termination stream ( ffaos ) ) ; } catch ( final content not found exception cnfe ) { cnfe thrown = true ; throw cnfe ; } finally { written to flow file = counting out . get bytes written ( ) ; this . bytes written += written to flow file ; this . bytes read += counting in .,code catches
put the call args back on stack so the method call can find <PLACE_HOLDER>,if ( should backup ) { restore stack ( backup args indices @$ arg types @$ is static call ) ; },call find
if another thread concurrently removes the only remaining <PLACE_HOLDER> from the entry @$ this put and get will return null @$ since the entry is about to be removed from the map . in that case @$ we obtain a fresh entry from the map and do the put on it .,while ( result == null ) { result = get or create entry ( key ) . put and get ( val ) ; },thread removes
note that this insertion method is worthwhile since the vast majority mark group keys will have only one <PLACE_HOLDER> .,if ( o == null ) { mark groups . put ( token @$ token ) ; } else if ( o instanceof token entry ) { list < token entry > l = new array list < > ( ) ; l . add ( ( token entry ) o ) ; l . add ( token ) ; mark groups . put ( token @$ l ) ; } else { @ suppress warnings ( __str__ ) list < token entry > l = ( list < token entry > ) o ; l . add ( token ) ; } last hash = __num__ ; for ( int end = math . max ( __num__ @$ i - min + __num__ ) ; i >,keys have
schedule monitor only if the job wants auto <PLACE_HOLDER> functionality,try { if ( context . get job detail ( ) . get job data map ( ) . get boolean ( auto_interruptible ) ) { job interrupt monitor plugin monitor plugin = ( job interrupt monitor plugin ) context . get scheduler ( ) . get context ( ) . get ( job_interrupt_monitor_key ) ; long job data delay = default_max_runtime ; if ( context . get job detail ( ) . get job data map ( ) . get ( max_run_time ) != null ) { job data delay = context . get job detail ( ) . get job data map ( ) . get long ( max_run_time ) ; } future = monitor plugin . schedule job interrupt monitor ( context . get job,job wants
dummy out passed target <PLACE_HOLDER> @$ since we do n't care about target .,if ( arg . equals ( __str__ ) ) { get arg value ( args @$ arg ) ; } else if ( platform_module_system_options . contains ( arg ) ) { add platform module system options ( arg @$ get arg value ( args @$ arg ) ) ; } else if ( arg . starts with ( batch_processing_max_flag ) ) { } else if ( obsolete flags . contains ( arg ) ) { } else if ( arg . starts with ( __str__ ) ) { usage ( __str__ + arg ) ; } else if ( name table . is valid class name ( arg ) && ! has known file suffix ( arg ) ) { entry classes . add ( arg ) ;,dummy passed
synchronize so default <PLACE_HOLDER> does n't override other default <PLACE_HOLDER>,synchronized ( session ) { object result = session . get attribute ( name ) ; if ( result == null && default value != null ) { session . set attribute ( name @$ default value ) ; result = default value ; } return result ; },value override
action bar drawer toggle ties together the the proper <PLACE_HOLDER> between the navigation drawer and the action bar app icon .,m drawer toggle = new action bar drawer toggle ( get activity ( ) @$ m drawer layout @$ r . drawable . ic_drawer @$ r . string . drawer_open @$ r . string . drawer_close ) { @ override public void on drawer closed ( view drawer view ) { super . on drawer closed ( drawer view ) ; if ( ! is added ( ) ) { return ; } get activity ( ) . invalidate options menu ( ) ; } @ override public void on drawer opened ( view drawer view ) { super . on drawer opened ( drawer view ) ; if ( ! is added ( ) ) { return ; } if ( ! m user learned drawer ),drawer ties
when hive.metastore.transactional.event.listeners is set @$ a failed event should not create a new <PLACE_HOLDER>,dummy raw store fail event . set event succeed ( false ) ; try { ms client . alter_partition ( default db name @$ tbl name @$ new part @$ null ) ; fail ( __str__ ) ; } catch ( exception ex ) { },event create
wait for all non cancelled <PLACE_HOLDER> to be completed,try { boolean acquired = sem . try acquire ( max_waiting_time @$ time unit . milliseconds ) ; assert . assert true ( __str__ @$ acquired ) ; background executor . cancel all ( __str__ @$ true ) ; assert . assert equals ( __str__ @$ list . size ( ) @$ nb_add ) ; for ( int i = __num__ ; i < nb_add ; i ++ ) { assert . assert true ( __str__ @$ i < nb_add ) ; } } catch ( interrupted exception e ) { assert . assert false ( __str__ @$ true ) ; },non cancelled
different properties means different <PLACE_HOLDER>,assert false ( objects . equals ( empty @$ finger ) ) ; assert false ( objects . equals ( empty @$ finger brand ) ) ; assert false ( objects . equals ( finger @$ finger brand ) ) ;,properties means
optionally check the byte after this frame matches sync <PLACE_HOLDER> .,if ( ! try read ( pes buffer @$ adts scratch . data @$ __num__ ) ) { return true ; } adts scratch . set position ( __num__ ) ; int frame size = adts scratch . read bits ( __num__ ) ; if ( frame size <= __num__ ) { return false ; },frame matches
lower android versions have a reference <PLACE_HOLDER> with 1024 entries only,for ( int i = __num__ ; i < __num__ ; i ++ ) { int [ ] ints = jni test . return int array ( ) ; assert not null ( ints ) ; },versions have
assert true here to make test tool take this test <PLACE_HOLDER> into account,assert . assert true ( true ) ;,tool take
setting <PLACE_HOLDER> interval to 1 hour to prevent bp service actor sends <PLACE_HOLDER> periodically to nn during running test case @$ and bp service actor only sends <PLACE_HOLDER> once after startup,conf . set time duration ( dfs_heartbeat_interval_key @$ __num__ @$ time unit . hours ) ; minidfs cluster cluster = new minidfs cluster . builder ( conf ) . nn topology ( minidfsnn topology . simpleha federated topology ( __num__ ) ) . build ( ) ; cluster . wait active ( ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; metrics record builder rb = get metrics ( dn . get metrics ( ) . name ( ) ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$,actor sends
note to translators : this message is produced if a reference to a function has too many or too few <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,reference has
already have demand @$ so do n't need to do anything @$ the current demand will complete the <PLACE_HOLDER> .,if ( requested ( ) != __num__ ) { return ; },demand complete
defaults to cs <PLACE_HOLDER>,fla version = fla version . cs6 ;,defaults cs
if it is a dynamic <PLACE_HOLDER> mapping @$ we can safely assume leaf <PLACE_HOLDER> name does not have ' . ' in it validate if parent <PLACE_HOLDER> is specified @$ then parent <PLACE_HOLDER> exists and an instance of auto create enabled parent <PLACE_HOLDER>,queue mapping entity new mapping = validate and get auto created queue mapping ( queue manager @$ mapping @$ queue path ) ; if ( new mapping != null ) { new mappings . add ( new mapping ) ; } else { new mappings . add ( mapping ) ; },instance create
when table could not be fetched from metastore @$ it is not known whether the table was added . deleting the table when aborting commit has the <PLACE_HOLDER> of deleting table not added in this transaction . not deleting the table may leave garbage behind . the former is much more dangerous than the latter . therefore @$ the table is not considered added,if ( ! done ) { throw e ; },commit has
dispatching to empty statement will not call back <PLACE_HOLDER> @$ must call our visit empty statement explicitly,if ( finally statement instanceof empty statement ) { visit empty statement ( ( empty statement ) finally statement ) ; } else { finally statement . visit ( this ) ; },statement call
the error messages should contain meaningful <PLACE_HOLDER> .,assert that ( ex ) . has message that ( ) . contains ( __str__ ) ;,messages contain
base 64 encode the <PLACE_HOLDER> and then convert to a string for the header .,encoded buf = base64 . encode ( buf @$ url_safe ) ; return encoded buf . to string ( utf_8 ) ; release ( buf ) ; release ( encoded buf ) ;,base encode
batch 2 does a <PLACE_HOLDER> based on the read,volt queuesql ( write @$ expect_scalar_match ( __num__ ) @$ current + __num__ @$ pkey ) ; volt executesql ( ) ;,batch does
disable reorder of columns . allowing this would cause the <PLACE_HOLDER> to become uns<PLACE_HOLDER> ; we rely on knowing that mnemonics are always in the first column @$ and that operand columns are in a particular order .,this . get table header ( ) . set reordering allowed ( false ) ;,reorder cause
replication scope allows <PLACE_HOLDER> @$ and does not require empty directories,if ( replication spec . is in replication scope ( ) ) { return ; },scope allows
client 1 did not register <PLACE_HOLDER>,await ( ) . until asserted ( ( ) -> { assert that ( client1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ - __num__ ) ; assert that ( client2 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ - __num__ ) ; assert that ( server1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ - __num__ ) ; assert that,client register
flush those recovered buffered <PLACE_HOLDER> out .,produce synchronously to partition zero ( input @$ as list ( new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) ) ) ; verify output ( output raw @$ new hash set < > ( as list ( new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__,those recovered
remove <PLACE_HOLDER> should also be clearing the last modified <PLACE_HOLDER>,m exif . remove timestamp ( ) ; assert that ( m exif . get last modified timestamp ( ) ) . is equal to ( exif . invalid_timestamp ) ;,timestamp clearing
inbuilt assumption that the testdir has only one <PLACE_HOLDER> file .,path di test = new path ( tmppath @$ testdir ) ; if ( ! fs . exists ( di test ) ) { throw new runtime exception ( tmpdir + file . separator + testdir + __str__ ) ; } if ( ! shim loader . get hadoop shims ( ) . is directory ( fs . get file status ( di test ) ) ) { throw new runtime exception ( tmpdir + file . separator + testdir + __str__ ) ; } fs data input stream fi test = fs . open ( ( fs . list status ( di test ) ) [ __num__ ] . get path ( ) ) ; file input stream fi gold = new file input stream ( new,testdir has
bind include <PLACE_HOLDER>,c = new method closure ( this @$ __str__ ) ; super . set variable ( __str__ @$ c ) ;,bind include
another producer has moved the <PLACE_HOLDER> failed to increment,so element ( buffer @$ calc element offset ( p index @$ mask ) @$ e ) ; so sequence ( s buffer @$ seq offset @$ p index + __num__ ) ; return true ;,producer moved
step 3 a : if the widest name of this type in other narrows this <PLACE_HOLDER> @$ remove this <PLACE_HOLDER> and add widest other to new this . simultaneously @$ check for situation where there is a name of this type in other @$ but no name in other matches @$ narrows @$ or widens this <PLACE_HOLDER> .,boolean same type = false ; for ( int j = __num__ ; j < other . size ( ) ; j ++ ) { general subtree other entrygs = other . get ( j ) ; general name interface other entry = get general name interface ( other entrygs ) ; switch ( this entry . constrains ( other entry ) ) { case name_narrows : remove ( i ) ; i -- ; new this . add ( other entrygs ) ; same type = false ; break ; case name_same_type : same type = true ; continue ; case name_match : case name_widens : same type = false ; break ; case name_diff_type : default : continue ; } break ; },name narrows
the number of got <PLACE_HOLDER>,int got = cis . read ( result ) ;,number got
end if : j component end finally end <PLACE_HOLDER>,log ( out @$ __str__ ) ;,end end
configure a virtual destination that forwards messages from topic test queue <PLACE_HOLDER>,remote session . create consumer ( included ) ; message producer included producer = local session . create producer ( included ) ; message test = local session . create text message ( __str__ ) ; final destination statistics destination statistics = local broker . get destination ( included ) . get destination statistics ( ) ;,messages queue
first @$ test accepted <PLACE_HOLDER>,final string json = mapper . write value as string ( base value wrapper . withb ( __num__ ) ) ; base value wrapper w = mapper . read value ( json @$ base value wrapper . class ) ; assert equals ( __num__ @$ w . value . x ) ;,test accepted
if a partition has multiple partition keys @$ we make the assumption that make part name with one key will return a <PLACE_HOLDER> of the name made with both all the keys .,string escaped name fragment = warehouse . make part name ( part key to val @$ false ) ; if ( key count == __num__ ) { params . put ( param name @$ escaped name fragment ) ; fltr . append ( __str__ ) . append ( is eq ? __str__ : __str__ ) . append ( param name ) ; } else if ( key pos + __num__ == key count ) { params . put ( param name @$ __str__ + escaped name fragment ) ; fltr . append ( is eq ? __str__ : __str__ ) . append ( __str__ ) . append ( param name ) . append ( __str__ ) ; } else if ( key pos == __num__ ) { params,name return
copy from does not set properties that have been already set @$ so this must be called after @$ which is a bit in the reverse from <PLACE_HOLDER> one might think .,m_output format . copy from ( m_stylesheet root . get output properties ( ) ) ;,one think
until the scan manager apply a new <PLACE_HOLDER> @$ we 're going to work with a default result log config object .,config = new result log config ( ) ; config . set memory max records ( mem capacity ) ; config . set log file name ( get log file name ( false ) ) ; config . set log file max records ( file capacity ) ;,manager apply
impl in filter base might do unnecessary <PLACE_HOLDER> for off heap backed cells .,return false ;,impl do
a tree on the left @$ an editor on the right : put <PLACE_HOLDER> in a sash form ... right below the editor we display the error messages ...,sash form = new sash form ( shell @$ swt . horizontal ) ; sash form . set layout ( new fill layout ( ) ) ; form data fd sash form = new form data ( ) ; fd sash form . left = new form attachment ( __num__ @$ __num__ ) ; fd sash form . right = new form attachment ( __num__ @$ __num__ ) ; fd sash form . top = new form attachment ( __num__ @$ __num__ ) ; fd sash form . bottom = new form attachment ( buttons composite @$ - __num__ ) ; sash form . set layout data ( fd sash form ) ; form data fdbc = new form data ( ) ; fdbc . left = new,tree put
roll back so next write buffer can do its <PLACE_HOLDER> on the next call but at the first write we 're at 0 .,if ( m write index == __num__ && m write buf index != __num__ ) { m write index = m chunk size ; m write buf index -- ; },buffer do
remove the filter popup if the user has cleared all <PLACE_HOLDER>,if ( showing && length == __num__ ) { dismiss popup ( ) ; m filtered = false ; },user cleared
always cleanup any created <PLACE_HOLDER> @$ even if the test failed,list < app deployment > deployments = repository service . create deployment query ( ) . list ( ) ; for ( app deployment deployment : deployments ) { repository service . delete deployment ( deployment . get id ( ) @$ true ) ; },any created
the current txn is either in open or aborted <PLACE_HOLDER> . mark the write ids <PLACE_HOLDER> as per the txn <PLACE_HOLDER> .,invalid write id list . add ( write id ) ; if ( valid txn list . is txn aborted ( txn id ) ) { aborted bits . set ( invalid write id list . size ( ) - __num__ ) ; } else { min open write id = math . min ( min open write id @$ write id ) ; },either mark
note to translators : the message indicates that the encoding requested for the output document was on that requires <PLACE_HOLDER> that is not available from the java virtual machine being used to execute the program .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,document requires
unconditionally reduce the amount of memory required for flow control because there is no object allocation costs associated with doing so and the stream will not have any more local flow control <PLACE_HOLDER> to keep track of anymore .,stream . set property ( state key @$ reduced_flow_state ) ;,stream have
all types must have the same <PLACE_HOLDER>,for ( logical type type : normalized types ) { if ( type . get type root ( ) != type root ) { return null ; } if ( type . get children ( ) . size ( ) != number of children ) { return null ; } },types have
if a run instances request does n't specify a client <PLACE_HOLDER> @$ fill one in @$ otherwise retries could result in unwanted instances being launched in the customer 's account .,if ( original request instanceof run instances request ) { run instances request run instances request = ( run instances request ) original request ; if ( run instances request . get client token ( ) == null ) { request . add parameter ( __str__ @$ uuid . randomuuid ( ) . to string ( ) ) ; } } else if ( original request instanceof modify reserved instances request ) { modify reserved instances request modify reserved instances request = ( modify reserved instances request ) original request ; if ( modify reserved instances request . get client token ( ) == null ) { request . add parameter ( __str__ @$ uuid . randomuuid ( ) . to string ( ) ) ; } },request specify
invalidate both edits <PLACE_HOLDER> .,invalidate edits dir at index ( __num__ @$ true @$ false ) ; invalidate edits dir at index ( __num__ @$ true @$ false ) ;,invalidate edits
it makes some <PLACE_HOLDER> to return true @$ as no filter implies all shall pass the filter @$ but if this returns true @$ then any other filters can be used as the source of the data to filter @$ which does n't make <PLACE_HOLDER> if this is meant to only be used by itself .,return false ;,which make
let 's skip these strange <PLACE_HOLDER> @$ as shortening the text might leave us behind with invalid markup etc,if ( context . length ( ) > max_context_length ) { continue ; },"s" skip
verify that the second job used the <PLACE_HOLDER> that was passed to its builder .,assert that ( override prober . probed ( ) @$ is ( true ) ) ;,job used
note : adjust the column size after loading the initial <PLACE_HOLDER> so the table has the column <PLACE_HOLDER> generated,this . get column model ( ) . get column ( __num__ ) . set max width ( __num__ ) ;,table has
start the enable procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new enable table procedure ( proc exec . get environment ( ) @$ table name ) ) ; int last step = __num__ ;,procedure kill
should n't ever throw cancelled since this method uses a dummy <PLACE_HOLDER> .,msg . error ( this @$ __str__ + e . get message ( ) @$ e ) ;,method uses
splits must be stable @$ and can not change during consecutive executions for example : kafka should not add <PLACE_HOLDER> if more then one topic is read .,for ( int i = __num__ ; i < num splits ; i ++ ) { result . add ( new microbatch source < > ( splits . get ( i ) @$ max read time @$ __num__ @$ num records [ i ] @$ i @$ source id @$ reader cache interval ) ) ; },kafka add
as we only support limited concurrent checkpoints @$ after checkpoint triggered more than the <PLACE_HOLDER> @$ the current periodic trigger would been assigned as null .,checkpoint coordinator . trigger checkpoint ( system . current time millis ( ) @$ false ) ; assert false ( checkpoint coordinator . is current periodic trigger available ( ) ) ; assert equals ( max concurrent checkpoints @$ checkpoint coordinator . get number of pending checkpoints ( ) ) ; checkpoint coordinator . abort pending checkpoints ( new checkpoint exception ( checkpoint failure reason . job_failover_region ) ) ;,checkpoint triggered
remove env var that would default child jvm to use parent 's <PLACE_HOLDER> as default . child jvm would use default <PLACE_HOLDER> for a hadoop client,if ( hadoop mem == __num__ ) { variables . remove ( hadoop_mem_key ) ; } else { variables . put ( hadoop_mem_key @$ string . value of ( hadoop mem ) ) ; },jvm use
dmn engine does n't generate the whole <PLACE_HOLDER> when no entry of the decision table match,if ( result raw != null ) { for ( expression element expression element : elements without class ) { if ( ! ( result raw instanceof map ) ) { throw new scenario exception ( __str__ ) ; } map < string @$ object > result = ( map < string @$ object > ) result raw ; result raw = result . get ( expression element . get step ( ) ) ; } } class < ? > result class = result raw != null ? result raw . get class ( ) : null ; object expected result raw = expected result . get raw value ( ) ; return get result wrapper ( fact mapping . get class name ( ) @$ expected,engine generate
no state changed @$ check when we should remove the delay flag from the shards the next time . if cluster state changed @$ we can leave the scheduling of the next delay up to the cluster changed <PLACE_HOLDER> this should not be needed @$ but we want to be extra safe here,if ( old state == new state ) { schedule if needed ( current nano time ( ) @$ new state ) ; },state changed
we use the default backend <PLACE_HOLDER>,if ( build time config . default backend . version . is present ( ) ) { add config ( property collector @$ engine settings . default_backend @$ hibernate search elasticsearch recorder . default_backend ) ; },default backend
a buffer underflow will cause the <PLACE_HOLDER> to stop .,al source play ( sourceid ) ;,underflow cause
strip the noisy stack trace <PLACE_HOLDER> .,string [ ] exclusions = excluded methods . get ( ) ; for ( int k = __num__ ; k < exclusions . length ; k += __num__ ) { if ( exclusions [ k ] . equals ( element . get class name ( ) ) && exclusions [ k + __num__ ] . equals ( element . get method name ( ) ) ) { continue out ; } } buf . append ( __str__ ) ; buf . append ( element . to string ( ) ) ; buf . append ( newline ) ;,noisy stack
have we overflowed a dtm identity 's addressing <PLACE_HOLDER> ?,if ( m_dtm ident . size ( ) == ( node index > > > dtm manager . ident_dtm_node_bits ) ) { add newdtmid ( node index ) ; } m_firstch . add element ( can have first child ? notprocessed : dtm . null ) ; m_nextsib . add element ( notprocessed ) ; m_parent . add element ( parent index ) ; m_exptype . add element ( expanded typeid ) ; m_data orq name . add element ( data or prefix ) ; if ( m_prevsib != null ) { m_prevsib . add element ( previous sibling ) ; } if ( dtm . null != previous sibling ) { m_nextsib . set element at ( node index @$ previous sibling ) ; } if (,identity addressing
ok. now i have my <PLACE_HOLDER> up server & region server services and dodgy wal @$ go ahead with test .,file system fs = file system . get ( conf ) ; path root dir = new path ( dir + get name ( ) ) ; dodgyfs log dodgywal = new dodgyfs log ( fs @$ root dir @$ get name ( ) @$ conf ) ; dodgywal . init ( ) ; path originalwal = dodgywal . get current file name ( ) ;,ok. have
others should not receive <PLACE_HOLDER>,for ( int i = __num__ ; i < m test adapter . get item count ( ) ; i ++ ) { if ( i == __num__ ) { continue ; } recycler view . view holder other = m recycler view . find view holder for adapter position ( i ) ; assert equals ( __num__ @$ m animator . pre layout info map . get ( other ) . change flags ) ; } check for main thread exception ( ) ;,others receive
check if there have been any changes if we have n't built the media db yet @$ do so on this sync . see <PLACE_HOLDER> at the top of this class about this difference to the original .,try { if ( m col . get media ( ) . need scan ( ) ) { m con . publish progress ( r . string . sync_media_find ) ; m col . log ( __str__ ) ; m col . get media ( ) . find changes ( ) ; } int last usn = m col . get media ( ) . last usn ( ) ; json object ret = m server . begin ( ) ; int srv usn = ret . get int ( __str__ ) ; if ( ( last usn == srv usn ) && ! ( m col . get media ( ) . have dirty ( ) ) ) { return __str__ ; } m col . log,check see
the header does n't match the <PLACE_HOLDER> header or is invalid . try the next byte offset .,if ( ( candidate synchronized header data != __num__ && ! headers match ( header data @$ candidate synchronized header data ) ) || ( frame size = mpeg audio header . get frame size ( header data ) ) == c . length_unset ) { if ( searched bytes ++ == search limit bytes ) { if ( ! sniffing ) { throw new parser exception ( __str__ ) ; } return false ; } valid frame count = __num__ ; candidate synchronized header data = __num__ ; if ( sniffing ) { input . reset peek position ( ) ; input . advance peek position ( peeked id3 bytes + searched bytes ) ; } else { input . skip fully ( __num__ ) ; },header match
a new call will trigger a new <PLACE_HOLDER>,client call < string @$ integer > call2 = oob1 . new call ( method @$ call options . default ) ; call2 . start ( mock call listener2 @$ headers ) ; client call < string @$ integer > call3 = oob1 . new call ( method @$ call options . default . with wait for ready ( ) ) ; call3 . start ( mock call listener3 @$ headers ) ; verify ( mock transport factory @$ times ( __num__ ) ) . new client transport ( eq ( socket address ) @$ eq ( new client transport options ( ) . set authority ( __str__ ) . set user agent ( user_agent ) ) @$ isa ( channel logger . class ) ) ; transport,call trigger
while this is more general than <PLACE_HOLDER> stapler can invoke on these types @$ the above is the only criterion for stapler to attempt dispatch . therefore prohibit this as a regular getter .,if ( parameter types . length > __num__ && parameter types [ __num__ ] == string . class ) { return false ; },stapler invoke
use a short 100 ms sleep since this could be done inline with a rs startup even if we fail @$ other region servers can take <PLACE_HOLDER> of it,this . executor = new thread pool executor ( nb workers @$ nb workers @$ __num__ @$ time unit . milliseconds @$ new linked blocking queue < > ( ) ) ; thread factory builder tfb = new thread factory builder ( ) ; tfb . set name format ( __str__ ) ; tfb . set daemon ( true ) ; this . executor . set thread factory ( tfb . build ( ) ) ; this . latest paths = new hash map < > ( ) ; this . replication for bulk load data enabled = conf . get boolean ( h constants . replication_bulkload_enable_key @$ h constants . replication_bulkload_enable_default ) ; this . sleep for retries = this . conf . get long ( __str__,servers take
matching legacy i pv 4 <PLACE_HOLDER> and path matching legacy i pv 4 <PLACE_HOLDER> and path matching second <PLACE_HOLDER> of i pv 4 addresses and path matching second <PLACE_HOLDER> of i pv 4 addresses and path matching i pv 6 <PLACE_HOLDER> and path matching i pv 6 <PLACE_HOLDER> and path matching i pv 4 cidr notation <PLACE_HOLDER> and path matching i pv 4 cidr,final string [ ] [ ] allowed = { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } ;,addresses matching
test that a click on an item invokes the registered <PLACE_HOLDER>,int index to click = expected count - __num__ ; on data ( all of ( is ( instance of ( string . class ) ) @$ is ( expected content [ index to click ] ) ) ) . in root ( is dialog ( ) ) . perform ( click ( ) ) ; verify ( on click listener @$ times ( __num__ ) ) . on click ( m alert dialog @$ index to click ) ;,click invokes
close created <PLACE_HOLDER> .,list < completable future < void > > connection close futures = new array list < > ( m connections . size ( ) ) ; for ( connection connection : m connections ) { connection close futures . add ( connection . close ( ) ) ; } m connections . clear ( ) ; return completable future . all of ( connection close futures . to array ( new completable future [ __num__ ] ) ) ;,close created
first char is note <PLACE_HOLDER> next : are rest <PLACE_HOLDER> @$ pitch @$ note <PLACE_HOLDER>,int channel = __num__ ; int velocity = __num__ ; int track = __num__ ; final int resolution = __num__ ; final int resolution delta = resolution / __num__ ; int index = __num__ ;,char note
end of string @$ process last <PLACE_HOLDER> .,run end = end ;,end process
check job config for overrides @$ otherwise use the default server <PLACE_HOLDER> .,long job val = job conf . get long ( var . varname @$ - __num__ ) ; return ( job val != - __num__ ) ? job val : hive conf . get long var ( daemon conf @$ var ) ;,job use
when application explicitly truncates <PLACE_HOLDER> ; timestamp based purge is only used to cleanup log <PLACE_HOLDER> that have been marked for truncation,if ( ( l . is truncated ( ) || ! conf . get explicit truncation by application ( ) ) && ! l . is in progress ( ) && ( l . get completion time ( ) < min timestamp to keep ) ) { purge list . add ( l ) ; } else { break ; },application truncates
session 1 : insert a user with email addresses but no email addresses <PLACE_HOLDER>,session session = open session ( ) ; session . begin transaction ( ) ; user user = new user ( ) ; user . set name ( __str__ ) ; contact contact = new contact ( ) ; contact . set name ( __str__ ) ; contact . set email addresses ( email addresses ) ; contact = ( contact ) session . merge ( contact ) ; user . set contact ( contact ) ; user = ( user ) session . merge ( user ) ; session . get transaction ( ) . commit ( ) ; session . close ( ) ;,email addresses
outside of srtm covered <PLACE_HOLDER>,assert equals ( __num__ @$ instance . get ele ( __num__ @$ __num__ ) @$ precision ) ; assert equals ( __num__ @$ instance . get ele ( __num__ @$ __num__ ) @$ precision ) ; assert equals ( __num__ @$ instance . get ele ( __num__ @$ __num__ ) @$ precision ) ; assert equals ( __num__ @$ instance . get ele ( __num__ @$ __num__ ) @$ precision ) ;,outside covered
jjt add child <PLACE_HOLDER> it 's child node list according to known indexes . going backwards makes sure the first time it gets the right size avoiding copies .,for ( int i = parameter types . length - __num__ ; i >= __num__ ; i -- ) { ast formal parameter formal parameter = new ast formal parameter ( java parser tree constants . jjtformalparameter ) ; formal parameters . jjt add child ( formal parameter @$ i ) ; formal parameter . jjt set parent ( formal parameters ) ; ast variable declarator id variable declarator id = new ast variable declarator id ( java parser tree constants . jjtvariabledeclaratorid ) ; variable declarator id . set image ( __str__ + i ) ; formal parameter . jjt add child ( variable declarator id @$ __num__ ) ; variable declarator id . jjt set parent ( formal parameter ) ; ast type type = new,jjt add
note : we do n't break because other branches may have useful state <PLACE_HOLDER>,if ( m ) { match = true ; },branches have
create a text file output <PLACE_HOLDER>,string test file output name = name ; text file output meta text file output meta = new text file output meta ( ) ; string text file input pid = registry . get plugin id ( step plugin type . class @$ text file output meta ) ; step meta text file output step = new step meta ( text file input pid @$ test file output name @$ text file output meta ) ;,text file
trigger scheduling @$ will allocate a <PLACE_HOLDER> with priority 0,cs . handle ( new node update scheduler event ( rm . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ) ) ;,trigger allocate
update the final routing look up <PLACE_HOLDER> .,if ( ! replica to server map . is empty ( ) ) { segment to replica to server map . put ( segment name @$ replica to server map ) ; } else { handle no serving host ( segment name ) ; },routing look
r dot java <PLACE_HOLDER>,android resource android resource = new android resource ( build target . with appended flavors ( aar_android_resource_flavor ) @$ project filesystem @$ android resource params @$ graph builder @$ immutable sorted set . < build rule > natural order ( ) . add ( assemble assets directories ) . add ( assemble resource directories ) . add all ( original build rule params . get declared deps ( ) . get ( ) ) . build ( ) @$ assemble resource directories . get source path to output ( ) @$ immutable sorted map . of ( ) @$ null @$ assemble assets directories . get source path to output ( ) @$ immutable sorted map . of ( ) @$ manifest . get source path to output,r dot
app hit the rate <PLACE_HOLDER> .,if ( ! is under timing session count quota && stats . session count in window < stats . session count limit ) { in quota time elapsed = math . max ( in quota time elapsed @$ stats . session rate limit expiration time elapsed ) ; },app hit
management server sends back a cds <PLACE_HOLDER> without cluster for the requested resource .,list < any > clusters = immutable list . of ( any . pack ( build cluster ( __str__ @$ null @$ false ) ) @$ any . pack ( build cluster ( __str__ @$ null @$ false ) ) ) ; discovery response response = build discovery response ( __str__ @$ clusters @$ xds client impl . ads_type_url_cds @$ __str__ ) ; response observer . on next ( response ) ;,server sends
sadly @$ motion event does n't implement <PLACE_HOLDER> @$ so we compare references .,assert true ( expected == m last context event ) ;,event implement
assert that the correct dir does not contain the <PLACE_HOLDER> .,assert true ( pairs correct . stream ( ) . none match ( p -> p . get path ( ) . equals ( cwd correct ) ) ) ;,dir contain
if a shape has 2 or less <PLACE_HOLDER> it can not be reduced,if ( tolerance <= __num__ || n < __num__ ) { return shape ; } boolean [ ] marked = new boolean [ n ] ;,shape has
next tag does n't correctly handle dt <PLACE_HOLDER>,if ( event == xml stream constants . start_document ) { while ( ! fast infoset stream reader . is start element ( ) ) event = fast infoset stream reader . next ( ) ; },tag handle
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default proxy profile = new crawl profile ( crawl_profile_proxy @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ integer . parse int ( sb . get config ( switchboard constants . proxy_prefetch_depth @$ __str__ ) ) @$ true @$ crawl profile . get recrawl date ( crawl_profile_proxy_recrawl_cycle ) @$ - __num__ @$ false @$ true @$ true @$ false @$ sb . get config bool ( switchboard constants . proxy_indexing_local_text @$ true ) @$ sb . get config bool ( switchboard constants . proxy_indexing_local_media,url match
let the sfsb invoke an <PLACE_HOLDER> on a remote server,final string message = __str__ ; final string first echo = sfsb on local server . get echo by invoking on remote server bean ( message ) ; assert . assert equals ( __str__ @$ message @$ first echo ) ;,sfsb invoke
table should not contain any <PLACE_HOLDER> at initialization,assert null ( table . get column footer ( __str__ ) ) ; assert null ( table . get column footer ( __str__ ) ) ; assert null ( table . get column footer ( __str__ ) ) ;,table contain
for this simple example quickstart @$ make the security manager accessible as a jvm singleton . most applications would n't do this and instead rely on their container configuration or web.xml for webapps . that is outside the scope of this simple quickstart @$ so we 'll just do the bare <PLACE_HOLDER> so you can continue to get a feel for things .,security utils . set security manager ( security manager ) ;,manager do
wait a minute and you should get other 6 <PLACE_HOLDER> executed,wait minute quota ( ) ; assert equals ( __num__ @$ do puts ( __num__ @$ family @$ qualifier @$ tables [ __num__ ] ) ) ;,minute get
note on quoting : it would be wrong here @$ since argv will be passed to runtime.exec @$ which should not parse <PLACE_HOLDER> or split on whitespace .,argv . add ( __str__ + name + __str__ + props . get property ( name ) ) ;,which parse
mandatory server hello done <PLACE_HOLDER>,upcoming states . add ( hs_server_hello_done ) ;,hello done
the main wf and the sub wf should be in running <PLACE_HOLDER>,workflow = workflow execution service . get execution status ( workflow id @$ true ) ; assert not null ( workflow ) ; assert equals ( running @$ workflow . get status ( ) ) ; assert equals ( __num__ @$ workflow . get tasks ( ) . size ( ) ) ; assert equals ( correlation id @$ workflow . get correlation id ( ) ) ; assert equals ( __str__ @$ workflow . get input ( ) . get ( __str__ ) ) ; assert equals ( __str__ @$ workflow . get input ( ) . get ( __str__ ) ) ; sub workflow = workflow execution service . get execution status ( sub workflow id @$ true ) ; assert not null ( sub workflow,wf running
any application must override <PLACE_HOLDER> of the package declaration methods .,package declaration ( off @$ nm . id ) ;,application override
update scripts using the now complete goog module namespaces global state and unspool the script <PLACE_HOLDER> that were queued up by all the recording .,for ( node c : script nodes ) { push script ( script descriptions . remove first ( ) ) ; if ( ! c . is from externs ( ) || node util . is from type summary ( c ) ) { node traversal . traverse ( compiler @$ c @$ new script updater ( ) ) ; } pop script ( ) ; } declare synthetic externs ( ) ;,scripts namespaces
bitwise or combines the sign bits so any negative value fails the <PLACE_HOLDER> .,if ( ( index | size | bytes . length - index - size ) < __num__ ) { throw new array index out of bounds exception ( string . format ( __str__ @$ bytes . length @$ index @$ size ) ) ; } int offset = index ; final int limit = offset + size ;,value fails
the following flags reduce startup <PLACE_HOLDER> and are acceptable only for dev purposes,args . add ( __str__ ) ; if ( ! is preventnoverify ( ) ) { args . add ( __str__ ) ; },flags reduce
certificate is good . if cert completes the <PLACE_HOLDER> @$ process user checkers that do n't support forward checking and process policies over whole <PLACE_HOLDER> and backtrack appropriately if there is a failure else if cert does not complete the <PLACE_HOLDER> @$ add it to the <PLACE_HOLDER>,if ( builder . is path completed ( cert ) ) { if ( debug != null ) debug . println ( __str__ + __str__ ) ; list < x509 certificate > appended certs = new array list < > ( cp list ) ; if ( builder . trust anchor . get trusted cert ( ) == null ) { appended certs . add ( __num__ @$ cert ) ; } set < string > init exp pol set = collections . singleton ( policy checker . any_policy ) ; policy node impl root node = new policy node impl ( null @$ policy checker . any_policy @$ null @$ false @$ init exp pol set @$ false ) ; list < pkix cert path checker >,failure complete
simple check to make sure things lo<PLACE_HOLDER> <PLACE_HOLDER> ...,for ( table catalog_tbl : catalog_db . get tables ( ) ) { string builder sb = new string builder ( ) ; catalog schema tools . to schema ( sb @$ catalog_tbl @$ null @$ false @$ null @$ null ) ; string sql = sb . to string ( ) ; assert true ( sql . starts with ( __str__ + catalog_tbl . get type name ( ) ) ) ; for ( column catalog_col : catalog_tbl . get columns ( ) ) { assert true ( sql . index of ( catalog_col . get type name ( ) ) != - __num__ ) ; } for ( constraint catalog_const : catalog_tbl . get constraints ( ) ) { constraint type const_type = constraint type .,things look
wild card ipv 6 single broadcast ipv 6 <PLACE_HOLDER> : fe 80 : xx : xx ... loopback ipv 6 <PLACE_HOLDER> site local ipv 6 <PLACE_HOLDER> : fec 0 : xx : xx ...,if ( inet addr . is any local address ( ) || inet addr . is link local address ( ) || inet addr . is loopback address ( ) || inet addr . is site local address ( ) ) { return true ; },broadcast ipv
and calculate the difference . expandable will have <PLACE_HOLDER> .,if ( child instanceof i sectionable && has header ( child ) ) { i header header = get header of ( child ) ; if ( ! ( header instanceof i expandable ) ) { return get global position of ( child ) - get global position of ( header ) - __num__ ; } } return get siblings of ( child ) . index of ( child ) ;,expandable have
we deal with xml <PLACE_HOLDER> get xml <PLACE_HOLDER>,if ( meta . getxml source file ( ) ) { file object xmlfile validator = kettlevfs . get file object ( xml fieldvalue ) ; if ( xmlfile validator == null || ! xmlfile validator . exists ( ) ) { log error ( base messages . get string ( pkg @$ __str__ @$ xml fieldvalue ) ) ; throw new kettle step exception ( base messages . get string ( pkg @$ __str__ @$ xml fieldvalue ) ) ; } sourcexml = new stream source ( xmlfile validator . get content ( ) . get input stream ( ) ) ; },file get
since the overlay panel manager can show <PLACE_HOLDER> without request panel show being called @$ the flag for the panel being shown should be set to true here .,m panel shown = true ; super . peek panel ( reason ) ;,manager show
create child creates the entire component <PLACE_HOLDER>,component root = element == null ? null : design context . read design ( element ) ;,child creates
caller must call strip html <PLACE_HOLDER> on passed val,string [ ] split = val . split ( __str__ @$ __num__ ) ; if ( split . length != __num__ ) { return null ; } string mid = split [ __num__ ] ; val = split [ __num__ ] ; string csum = long . to string ( utils . field checksum ( val ) ) ; list < long > nids = new array list < > ( ) ; cursor cur = null ; try { cur = m col . get db ( ) . get database ( ) . query ( __str__ @$ new string [ ] { mid @$ csum } ) ; long nid = cur . get long ( __num__ ) ; string flds = cur . get string,caller call
do n't let cp initialization errors kill the <PLACE_HOLDER>,if ( this . cp host != null ) { try { this . cp host . post start master ( ) ; } catch ( io exception ioe ) { log . error ( __str__ @$ ioe ) ; } },errors kill
the system apk may have been updated with an older version of the one on the data partition @$ but which granted a new system <PLACE_HOLDER> that it did n't have before . in this case we do want to allow the app to now get the new <PLACE_HOLDER> if the ancestral apk is privileged to get it .,if ( disabled ps != null && disabled pkg != null && is package requesting permission ( disabled pkg @$ perm ) && ( ( privileged permission && disabled ps . is privileged ( ) ) || ( oem permission && disabled ps . is oem ( ) && can grant oem permission ( disabled ps @$ perm ) ) ) ) { allowed = true ; },which granted
we catch any kind of problem here . some linux distros have <PLACE_HOLDER> accessing the clipboard .,return new data flavor [ __num__ ] ;,distros have
construct the build <PLACE_HOLDER> to build the binary jar .,immutable set < java library > transitive classpath deps = java library classpath provider . get classpath deps ( params . get build deps ( ) ) ; immutable set < source path > transitive classpaths = java library classpath provider . get classpaths from libraries ( transitive classpath deps ) ; java binary java binary = new java binary ( binary build target @$ project filesystem @$ params . copy appending extra deps ( transitive classpath deps ) @$ java options . apply ( build target . get target configuration ( ) ) . get java runtime launcher ( graph builder @$ build target . get target configuration ( ) ) @$ args . get main class ( ) . or else ( null ) @$ args,the build
mpi is tracking <PLACE_HOLDER> per partition hsid . we need to make sure we write ours into the message getting sent to the mpi,resp . set executor site id ( m_mailbox . geths id ( ) ) ; m_mailbox . send ( counter . m_destination id @$ resp ) ; voltdb . crash global voltdb ( __str__ @$ true @$ null ) ;,mpi tracking
we go through this loop until we achieve a match or until the maximum loop count is reached . we record the points at <PLACE_HOLDER> the date and the string starts to match . once matching starts @$ it should continue .,int loop ; int dmatch = __num__ ;,date starts
no parameter @$ just send <PLACE_HOLDER>,if ( params == null || params . size ( ) == __num__ ) return o common const . empty_byte_array ;,parameter send
the long press on the original item should not have triggered a long press on the item touch helper . this check is a stand in for a better way to see if the item touch helper 's internal long press callback method was called . m callback.m has drag <PLACE_HOLDER> will only be 0 if the item touch helper 's long press call,assert equals ( __num__ @$ m callback . m has drag flag . size ( ) ) ;,callback.m has
by some reason many hive errors have this sql <PLACE_HOLDER>,if ( sql state . sql_08s01 . get code ( ) . equals ( sql state ) ) { return error type . normal ; },errors have
we 're letting the window listener take <PLACE_HOLDER> of this,this . set default close operation ( j frame . do_nothing_on_close ) ;,listener take
mock get launch <PLACE_HOLDER>,final activity starter starter = prepare starter ( flag_activity_new_task @$ false ) ;,mock get
the first entry provides turn <PLACE_HOLDER> for car and foot only,assert equals ( __num__ @$ cartc enc . get decimal ( false @$ tc flags ) @$ __num__ ) ; assert equals ( __num__ @$ trucktc enc . get decimal ( false @$ tc flags ) @$ __num__ ) ; assert true ( double . is infinite ( biketc enc . get decimal ( false @$ tc flags ) ) ) ; tc storage . read flags ( tc flags @$ gh utility . get edge ( graph @$ __num__ @$ __num__ ) . get edge ( ) @$ __num__ @$ gh utility . get edge ( graph @$ __num__ @$ __num__ ) . get edge ( ) ) ; assert equals ( __num__ @$ cartc enc . get decimal ( false @$ tc flags ) @$ __num__,entry provides
the user asked for <PLACE_HOLDER> to be collected . some <PLACE_HOLDER> like number of rows require a scan of the data however @$ some other <PLACE_HOLDER> @$ like number of files @$ do not require a complete scan update the <PLACE_HOLDER> which do not require a complete scan .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },scan update
empty point raises <PLACE_HOLDER>,assert invalid multi point ( __str__ @$ __str__ ) ; assert invalid multi point ( __str__ @$ __str__ @$ __str__ ) ;,point raises
service type method automatic recovery deliberately disabled as spring has it 's own recovery <PLACE_HOLDER>,class < ? > channeln class = class . for name ( __str__ ) ; method channeln basic publish = channeln class . get declared method ( __str__ @$ string . class @$ string . class @$ boolean . class @$ boolean . class @$ amqp . basic properties . class @$ byte [ ] . class ) ; expected trace channeln basic publish trace = expectations . event ( rabbitmq test constants . rabbitmq_client @$ channeln basic publish @$ null @$ remote address @$ __str__ + rabbitmq test constants . exchange @$ expectations . annotation ( __str__ @$ rabbitmq test constants . exchange ) @$ expectations . annotation ( __str__ @$ rabbitmq test constants . routing_key_pull ) ) ;,recovery has
a new occupant has joined the <PLACE_HOLDER>,if ( ! is user status modification ) { for ( participant status listener listener : participant status listeners ) { listener . joined ( from ) ; } },occupant joined
now do the 1 big <PLACE_HOLDER> .,file . seek ( wb . offset ) ; if ( max stat > __num__ ) { if ( stat idx < max stat ) { stats [ stat idx ++ ] = sequence . get length ( ) ; } else { long all = __num__ ; for ( ; stat idx > __num__ ; ) { all += stats [ -- stat idx ] ; } system . err . println ( __str__ + all / max stat ) ; } } file . write ( sequence . get data ( ) @$ sequence . get offset ( ) @$ sequence . get length ( ) ) ; replication target replication target = journal . get replication target ( ) ; if ( replication target !=,now do
the wait is necessary to have the poll function complete and propagate the <PLACE_HOLDER> from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one two = database one function . get global comment ( ) ; final list < i comment > two two = database two function . get global comment ( ) ; assert not null ( one two ) ; assert not null ( two two ) ; assert equals ( one one . size ( ) + __num__ @$ one two . size ( ) ) ; assert equals ( two one . size ( ) + __num__ @$ two two . size ( ) ) ; assert equals ( one two @$ two two ) ; final int one two size =,function complete
ze time expect the <PLACE_HOLDER> in 100 increment,int steps = activity user . get steps goal ( ) / __num__ ;,time expect
check am 2 get the nm <PLACE_HOLDER> from am 1 .,assert . assert equals ( expectednm tokens . size ( ) @$ register response . getnm tokens from previous attempts ( ) . size ( ) ) ; for ( int i = __num__ ; i < expectednm tokens . size ( ) ; i ++ ) { assert . assert true ( expectednm tokens . get ( i ) . equals ( register response . getnm tokens from previous attempts ( ) . get ( i ) ) ) ; },2 get
consumer ca n't see the odd producer <PLACE_HOLDER>,if ( e == null ) { if ( index != lv producer index ( ) ) { e = spin wait for element ( buffer @$ offset ) ; } else { return null ; } },consumer see
each test must set the schema access <PLACE_HOLDER> and schema @$ and enable the writer cs,runner . set property ( jolt transform record . record_writer @$ __str__ ) ;,test set
load should also clear <PLACE_HOLDER>,binder . read bean ( person ) ; assert null ( name field . get component error ( ) ) ;,load clear
the final thread that should n't execute releases the latch once it has run so it is deterministic that the other two fill the thread <PLACE_HOLDER> until this one rejects,if ( ! should execute ) { success latch . count down ( ) ; },two fill
once the repl load is successful @$ the this config should be unset or else @$ the subsequent repl load will also drop those <PLACE_HOLDER> which will cause data loss .,load with clause = collections . empty list ( ) ;,load drop
we currently do not validate the elements on the ancestors @$ assuming they 've already been validated . this also means some checks such as unique ids might not be check all <PLACE_HOLDER> .,for ( annotation handler annotation handler : environment . get handlers ( ) ) { if ( ! annotation handler . is enabled ( ) ) { continue ; } string validator simple name = annotation handler . get class ( ) . get simple name ( ) ; string annotation name = annotation handler . get target ( ) ; set < ? extends element > annotated elements = extracted model . get root annotated elements ( annotation name ) ; set < element > validated annotated elements = new linked hash set < > ( ) ; validating holder . put root annotated elements ( annotation name @$ validated annotated elements ) ; if ( ! annotated elements . is empty ( ) ) { logger,checks check
skip children which already have their <PLACE_HOLDER> assigned,if ( current child . get type ( ) == null ) { if ( current child . jjt get last token ( ) . to string ( ) . equals ( __str__ ) ) { if ( previous child != null ) { current child . set type definition ( previous child . get type definition ( ) ) ; } else { ast class or interface declaration type declaration = current child . get first parent of type ( ast class or interface declaration . class ) ; if ( type declaration != null ) { current child . set type definition ( type declaration . get type definition ( ) ) ; } } } else if ( current child . jjt get last token,which have
this branch coordinates fragment task or completed transaction task @$ holds the <PLACE_HOLDER> until all the sites on the node receive the task . task with newer sp handle will,if ( task . need coordination ( ) && m_scoreboard enabled ) { coordinated task queue offer ( task ) ; } else { task queue offer ( task ) ; },task holds
extract the resource context that contains projection information for root object <PLACE_HOLDER> @$ metadata and paging .,final resource context resource context = routing result . get context ( ) ;,information object
update the keys in vm 0 until the entry <PLACE_HOLDER> rolls over . this means that if we did a conflict check @$ vm 0 's key will have a lower entry <PLACE_HOLDER> than vm 1 @$ which would cause us to prefer vm 1 's value,vm0 . invoke ( ( ) -> { internal region region = ( internal region ) get cache ( ) . get region ( region name ) ; region . put ( __num__ @$ __str__ ) ; region entry region entry = region . get region entry ( __num__ ) ; version tag tag = region entry . get version stamp ( ) . as version tag ( ) ; tag . set entry version ( tag . get entry version ( ) - __num__ ) ; region entry . get version stamp ( ) . set versions ( tag ) ; } ) ;,key have
produced properties match relevant <PLACE_HOLDER>,global properties gp = new global properties ( ) ; gp . set hash partitioned ( new field list ( __num__ ) ) ; local properties lp = local properties . for grouping ( new field list ( __num__ @$ __num__ ) ) ; requested global properties rgp = new requested global properties ( ) ; rgp . set hash partitioned ( new field list ( __num__ ) ) ; requested local properties rlp = new requested local properties ( ) ; rlp . set grouped fields ( new field list ( __num__ ) ) ; to join1 . set required global props ( rgp ) ; to join1 . set required local props ( rlp ) ; to join1 . set ship strategy ( ship strategy type,properties match
register used <PLACE_HOLDER> and fields before canonicalization can optimize them .,register used elements ( ) ; canonicalizer phase . create ( ) . apply ( graph @$ bb . get providers ( ) ) ;,register used
perform a topological sort which returns the state descriptor <PLACE_HOLDER> in their priority .,list < graph vertex < class < ? extends state descriptor > > > sorted successors = topological sort ( preference graph . values ( ) ) ;,which returns
all symbols that came from goog.module are collected separately because they will have to be processed first . see <PLACE_HOLDER> below .,list < symbol > types = new array list < > ( ) ; list < symbol > goog module export types = new array list < > ( ) ; list < symbol > module types = new array list < > ( ) ;,symbols see
provide compatibility with legacy applications which may pass boolean <PLACE_HOLDER> in bind args .,if ( arg instanceof boolean ) { native bind long ( m connection ptr @$ statement ptr @$ i + __num__ @$ ( ( boolean ) arg ) . boolean value ( ) ? __num__ : __num__ ) ; } else { native bind string ( m connection ptr @$ statement ptr @$ i + __num__ @$ arg . to string ( ) ) ; },which pass
pdx @$ alias @$ nested <PLACE_HOLDER>,return new object [ ] { new object [ ] { __str__ @$ true @$ true @$ true } @$ new object [ ] { __str__ @$ true @$ true @$ false } @$ new object [ ] { __str__ @$ true @$ false @$ true } @$ new object [ ] { __str__ @$ true @$ false @$ false } @$ new object [ ] { __str__ @$ false @$ true @$ true } @$ new object [ ] { __str__ @$ false @$ true @$ false } @$ new object [ ] { __str__ @$ false @$ false @$ true } @$ new object [ ] { __str__ @$ false @$ false @$ false } @$ new object [ ] { __str__ @$ true @$ true,pdx nested
call the g rpc echo service <PLACE_HOLDER>,echo . echo response response = echo service grpc . new blocking stub ( channel ) . echo ( echo . echo request . new builder ( ) . set message ( __str__ ) . build ( ) ) ; assert that ( response . get message ( ) @$ is ( __str__ ) ) ; ( ( managed channel ) channel ) . shutdown ( ) . await termination ( __num__ @$ time unit . seconds ) ;,rpc echo
transfer the location <PLACE_HOLDER> from the incoming event to the event factory so that the event we ask it to generate for us has the same location <PLACE_HOLDER>,xml event factory . set location ( start element . get location ( ) ) ; return xml event factory . create start element ( new q name ( mapping xsd support . instance . hbm xsd ( ) . get namespace uri ( ) @$ start element . get name ( ) . get local part ( ) ) @$ start element . get attributes ( ) @$ target namespaces . iterator ( ) ) ;,event has
second dfs client does renew <PLACE_HOLDER>,final dfs client mock client2 = create mock client ( ) ; mockito . do return ( true ) . when ( mock client2 ) . renew lease ( ) ; assert same ( renewer @$ lease renewer . get instance ( fake_authority @$ fake_ugi_a @$ mock client2 ) ) ; renewer . put ( mock client2 ) ;,client renew
note : m result handler uses main <PLACE_HOLDER> @$ so this must not be blocked .,m result handler = new handler ( context . get main looper ( ) ) ; m next seq number = __num__ ; m pending commands = new array map < > ( ) ; m requested command seq numbers = new array set < > ( ) ; boolean connect requested ; if ( token . get type ( ) == type_session ) { m service connection = null ; connect requested = request connect to session ( connection hints ) ; } else { m service connection = new session service connection ( connection hints ) ; connect requested = request connect to service ( ) ; } if ( ! connect requested ) { close ( ) ; },handler uses
orc file does n't have <PLACE_HOLDER> by linkedin 's convention .,return compare iterators ( keys1 @$ keys2 @$ ( key1 @$ key2 ) -> { if ( ! remove extension ( key1 ) . equals ( key2 ) ) { log . error ( string . format ( __str__ @$ key1 @$ key2 ) ) ; return false ; } orc row iterator it1 = expected . get ( key1 ) ; orc row iterator it2 = observed . get ( key2 ) ; if ( ! it1 . get type info ( ) . equals ( it2 . get type info ( ) ) ) { log . error ( string . format ( __str__ @$ key1 @$ key2 ) ) ; return false ; } boolean result = true ; while ( it1 . has next,file have
someone else won the <PLACE_HOLDER> and created the file,throw x ;,someone won
ui 1 has not yet been added in ui.init where logging takes <PLACE_HOLDER>,assert equals ( __str__ @$ get log row ( __num__ ) ) ; string url = get testurl ( getui class ( ) ) . replace ( __str__ @$ __str__ ) ; driver . get ( url ) ;,logging takes
<PLACE_HOLDER> dot <PLACE_HOLDER>,for ( int j = __num__ ; j < dimension ; j ++ ) { gg += g [ j ] * g [ j ] ; dgg += ( xi [ j ] + g [ j ] ) * xi [ j ] ; },g dot
client 1 has two <PLACE_HOLDER> to secret 1,acldao . enroll client ( jooq context . configuration ( ) @$ client1 . get id ( ) @$ group1 . get id ( ) ) ; acldao . enroll client ( jooq context . configuration ( ) @$ client1 . get id ( ) @$ group2 . get id ( ) ) ; acldao . allow access ( jooq context . configuration ( ) @$ secret1 . get id ( ) @$ group1 . get id ( ) ) ; acldao . allow access ( jooq context . configuration ( ) @$ secret1 . get id ( ) @$ group2 . get id ( ) ) ; set < sanitized secret > secret = acldao . get sanitized secrets for ( client1 ) ; assert that (,client has
in case this worker has no <PLACE_HOLDER> on this node,if ( left > right ) { log . debug ( __str__ + nid ) ; this . node pos start [ __num__ * nid + __num__ ] = left ; this . node pos end [ __num__ * nid + __num__ ] = right ; log . debug ( string . format ( __str__ @$ __num__ * nid + __num__ @$ left @$ right ) ) ; this . node pos start [ __num__ * nid + __num__ ] = left ; this . node pos end [ __num__ * nid + __num__ ] = right ; log . debug ( string . format ( __str__ @$ __num__ * nid + __num__ @$ left @$ right ) ) ; return ; },worker has
a unique compound index on the partitioning key and another column gets no <PLACE_HOLDER> .,schema = __str__ + __str__ + __str__ ; check valid unique and assume unique ( schema @$ null @$ msgpr ) ;,index gets
<PLACE_HOLDER> these elements using insertion <PLACE_HOLDER>,a [ e1 ] = t ; a [ e2 ] = a [ e1 ] ;,elements using
first time store has one <PLACE_HOLDER> file @$ next id will be 2,mockito . do answer ( ans ) . when ( m store ) . roll writer ( __num__ ) ;,store has
since nm 's and rm 's token sequence no is different @$ response should contain system <PLACE_HOLDER> for apps,assert equals ( __num__ @$ response1 . get token sequence no ( ) ) ; assert equals ( __num__ @$ response1 . get system credentials for apps ( ) . size ( ) ) ; resource tracker service . close ( ) ;,response contain
this will fail for all users currently has <PLACE_HOLDER> verified,if ( check valid string ( signal username ) && signal username . equals ( preferences . get verified signal username ( ) ) ) { find preference ( preference manager . verify_signal ) . set summary ( r . string . verification_dialog_summary_verified ) ; } else { find preference ( preference manager . verify_signal ) . set summary ( r . string . verification_dialog_summary ) ; },users has
user 1 creates the <PLACE_HOLDER>,try { muc = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc . create ( __str__ ) ; form form = new form ( form . type_submit ) ; form field field = new form field ( __str__ ) ; field . set type ( __str__ ) ; form . add field ( field ) ; form . set answer ( __str__ @$ arrays . as list ( __str__ ) ) ; muc . send configuration form ( form ) ; } catch ( exception e ) { e . print stack trace ( ) ; fail ( e . get message ( ) ) ; },user creates
this method is racy since it 's possible someone undeployed the <PLACE_HOLDER> after we called get deployment and checked the <PLACE_HOLDER> exists . if we now discover the <PLACE_HOLDER> is undeployed @$ throw an exception and handle it the same as if we discovered this earlier .,throw new job not deployed exception ( host @$ job id ) ; throw new helios runtime exception ( __str__ @$ e ) ;,someone undeployed
kill zoo keeper 0 @$ removing monitor <PLACE_HOLDER> .,try { fail site ( __num__ ) ; } catch ( exception e ) { fail ( ) ; } while ( monitor for manager4 . m_members . size ( ) != __num__ && ! monitor for manager4 . has identical membership ( monitor for manager2 ) ) { thread . yield ( ) ; },removing monitor
add some test data now add 5 main objects . <PLACE_HOLDER> will contain key <PLACE_HOLDER> @$ 2 will contain key <PLACE_HOLDER> & key 2 and so on,for ( ; id <= __num__ ; ++ id ) { map key index data mkid = new map key index data ( id ) ; for ( int j = __num__ ; j <= id ; ++ j ) { mkid . maap . put ( __str__ + j @$ __str__ + j ) ; } test rgn . put ( id @$ mkid ) ; } -- id ;,1 contain
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( test index scan count suite . class ) ;,suite using
any application must override <PLACE_HOLDER> of the define field methods .,identifier arg ids [ ] = null ; identifier exp ids [ ] = null ; if ( args != null ) { arg ids = new identifier [ args . length ] ; for ( int i = __num__ ; i < args . length ; i ++ ) { arg ids [ i ] = args [ i ] . id ; } } if ( exp != null ) { exp ids = new identifier [ exp . length ] ; for ( int i = __num__ ; i < exp . length ; i ++ ) { exp ids [ i ] = exp [ i ] . id ; } } define field ( where @$ doc @$ mod @$ t @$ nm,application override
at normal playback speed @$ we stop buffering when the buffer reaches the <PLACE_HOLDER> .,assert that ( load control . should continue loading ( min_buffer_us @$ speed ) ) . is false ( ) ;,buffer reaches
bypass get transaction <PLACE_HOLDER> .,set transaction type ( cardio2e transaction types . get ) ; super . set object type ( cardio2e object types . zones_bypass ) ; set object number ( object number ) ;,bypass get
add column families build a <PLACE_HOLDER> of keys,list < column family descriptor builder > family builders = new array list < > ( ) ; sorted map < byte [ ] @$ integer > map = new tree map < > ( bytes . bytes_comparator ) ; visit bulkh files ( fs @$ hfof dir @$ new bulkh file visitor < column family descriptor builder > ( ) { @ override public column family descriptor builder bulk family ( byte [ ] family name ) { column family descriptor builder builder = column family descriptor builder . new builder ( family name ) ; family builders . add ( builder ) ; return builder ; } @ override public void bulkh file ( column family descriptor builder builder @$ file status hfile status ) throws,families build
always cleanup any created <PLACE_HOLDER> @$ even if the test failed,list < app deployment > deployments = repository service . create deployment query ( ) . list ( ) ; for ( app deployment deployment : deployments ) { repository service . delete deployment ( deployment . get id ( ) @$ true ) ; },any created
check for offline members @$ if the region has <PLACE_HOLDER> even if we intend to replace offline data @$ we still need to make sure the bucket is n't completely offline,if ( ! replace offline data || redundancy == - __num__ ) { bucket persistence advisor persist advisor = get persistence advisor ( ) ; if ( persist advisor != null ) { if ( ! persist advisor . was hosting ( ) && advisor . get had primary ( ) ) { final persistent membership view membership view = persist advisor . get membership view ( ) ; if ( membership view == null ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . partitioned region . getpr id ( ) @$ partitioned region . bucket_id_separator @$ bid ) ; } return false ; } set < persistent memberid > offline members = membership view .,region has
per javadoc log exceptions but still go online . note : this does not include errors @$ which indicate a fatal <PLACE_HOLDER>,logger . log ( warning @$ string . format ( __str__ @$ cl . get class ( ) ) @$ e ) ;,which indicate
get the app log aggregation impl <PLACE_HOLDER> to crash,local dirs handler service mocked dir svc = mock ( local dirs handler service . class ) ; log aggregation service log aggregation service = new log aggregation service ( dispatcher @$ this . context @$ del srvc @$ mocked dir svc ) ; log aggregation service . init ( this . conf ) ; log aggregation service . start ( ) ; application id application1 = builder utils . new application id ( __num__ @$ __num__ ) ; log aggregation service . handle ( new log handler app started event ( application1 @$ this . user @$ null @$ this . acls ) ) ; log aggregation service . handle ( new log handler app finished event ( application1 ) ) ; dispatcher . await ( ),app log
peers need different <PLACE_HOLDER>,super ( __str__ ) ;,peers need
invoke the method to have the subclass write <PLACE_HOLDER>,try { write entities ( tl conf @$ manager @$ context ) ; } finally { manager . close ( ) ; },subclass write
this code hides a <PLACE_HOLDER> of complexity which can be reduced if one thinks about it .,if ( breakpoint . get type ( ) == breakpoint type . regular ) { c breakpoint painter . paint breakpoints ( m_manager @$ m_graph @$ breakpoint . get address ( ) ) ; },code hides
the module list given as the parameter contains a list of <PLACE_HOLDER> . in case the context menu of a module node is created @$ this list contains exactly one module . in case the context menu of a table is created @$ the list contains the corresponding <PLACE_HOLDER> of the selected rows of the table .,final list < j component > menus = new array list < j component > ( ) ; if ( modules . size ( ) == __num__ ) { final module target module = modules . get ( __num__ ) ; menus . add ( new j menu item ( new pathfinding action ( target module ) ) ) ; },list contains
depth first search from root directory for all application log <PLACE_HOLDER>,remote iterator < file status > iter = list ( dirpath ) ; while ( iter . has next ( ) ) { file status stat = iter . next ( ) ; path child path = stat . get path ( ) ; if ( stat . is directory ( ) ) { application id app id = parse application id ( child path . get name ( ) ) ; if ( app id != null ) { app log dir present . set true ( ) ; if ( should clean app log dir ( child path @$ now @$ fs @$ retain millis ) ) { delete dir ( child path ) ; } } else { clean app log dir ( child path,search log
nested stage has 3 plan <PLACE_HOLDER> @$ and one of them refereces the root took from the plan model,assert equals ( __num__ @$ nested stage . get plan items ( ) . size ( ) ) ; stage nested nested stage = null ; for ( plan item plan item : nested stage . get plan items ( ) ) { assert not null ( plan item . get plan item definition ( ) ) ; if ( plan item . get plan item definition ( ) instanceof stage ) { nested nested stage = ( stage ) plan item . get plan item definition ( ) ; } } assert not null ( nested nested stage ) ; assert equals ( __str__ @$ nested nested stage . get name ( ) ) ; assert equals ( __num__ @$ nested nested stage . get plan items,stage has
double precision only has 52 <PLACE_HOLDER> of mantissa,if ( level > __num__ ) return point crossings for line ( px @$ py @$ x0 @$ y0 @$ x1 @$ y1 ) ; double xmid = ( xc0 + xc1 ) / __num__ ; double ymid = ( yc0 + yc1 ) / __num__ ; xc0 = ( x0 + xc0 ) / __num__ ; yc0 = ( y0 + yc0 ) / __num__ ; xc1 = ( xc1 + x1 ) / __num__ ; yc1 = ( yc1 + y1 ) / __num__ ; double xc0m = ( xc0 + xmid ) / __num__ ; double yc0m = ( yc0 + ymid ) / __num__ ; double xmc1 = ( xmid + xc1 ) / __num__ ; double ymc1 = ( ymid + yc1 ),precision has
in theory node name ca n't be null but better be careful who knows <PLACE_HOLDER> other implementations may be doing ? ...,if ( get node name ( ) == null ) { if ( arg . get node name ( ) != null ) { return false ; } } else if ( ! get node name ( ) . equals ( arg . get node name ( ) ) ) { return false ; } if ( get local name ( ) == null ) { if ( arg . get local name ( ) != null ) { return false ; } } else if ( ! get local name ( ) . equals ( arg . get local name ( ) ) ) { return false ; } if ( get namespaceuri ( ) == null ) { if ( arg . get namespaceuri ( ),implementations doing
godin : note that other channels use method <PLACE_HOLDER> in order to do the same thing,tmp builder . set length ( __num__ ) ; return true ;,channels use
as a heuristic to avoid indefinite writer <PLACE_HOLDER> @$ block if the thread that momentarily appears to be head of queue @$ if one exists @$ is a waiting writer . this is only a probabilistic effect since a new reader will not block if there is a waiting writer behind other enabled readers that have not yet drained from the queue .,return apparently first queued is exclusive ( ) ;,heuristic avoid
item is offscreen . get child <PLACE_HOLDER> and calculate item <PLACE_HOLDER> based on current shuffle state,return calc item height ( position @$ get child height ( position ) ) ;,offscreen get
test case 8 : 2 components @$ 2 instances for each comp 2 already <PLACE_HOLDER>ed . comp 1 has a new instance <PLACE_HOLDER> @$ we should terminate the service,comp = create component ( service scheduler @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component1 instances = comp . get all component instances ( ) ; container status . set exit status ( - __num__ ) ; component comp2 = create component ( component instance . get component ( ) . get scheduler ( ) @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component2 instances = comp2 . get all component instances ( ) ; map <,components has
check this tree supports this <PLACE_HOLDER> by having properly populated core label 's,list < tree > leaves = this . get leaves ( ) ; if ( ! ( leaves . get ( __num__ ) . label ( ) instanceof core label ) ) { throw new illegal argument exception ( __str__ ) ; } else if ( ( ( core label ) leaves . get ( __num__ ) . label ( ) ) . word ( ) == null ) { throw new illegal argument exception ( __str__ ) ; } else if ( ( ( core label ) leaves . get ( __num__ ) . label ( ) ) . after ( ) == null ) { throw new illegal argument exception ( __str__ ) ; } list < core label > core labels = this . get,tree supports
inflater likes a <PLACE_HOLDER> of slack,long size = get entry size ( jzentry ) + __num__ ;,inflater likes
check whether the buffer contains the <PLACE_HOLDER> to be read,if ( cur < bar ) { pos ++ ; return buff [ cur ] ; },buffer contains
first time ! let the framework create a cpp asset <PLACE_HOLDER> 2 and an asset <PLACE_HOLDER> @$ which we 'll hang on to .,if ( system cpp asset manager2 == null ) { directly on ( asset manager . class @$ __str__ ) ; cached system apk assets = _asset manager static_ . get system apk assets ( ) ; cached system apk assets set = _asset manager static_ . get system apk assets set ( ) ; } else { _asset manager static_ . set system apk assets ( cached system apk assets ) ; _asset manager static_ . set system apk assets set ( cached system apk assets set ) ; s system = reflection helpers . call constructor ( asset manager . class @$ class parameter . from ( boolean . class @$ true ) ) ; s system . set apk assets ( cached system apk assets,framework create
assert valid line cnt specified equals the successfully inserted line <PLACE_HOLDER>,assert equals ( valid line cnt @$ line count ) ; assert equals ( invalid line cnt @$ invalidlinecnt ) ;,cnt equals
has <PLACE_HOLDER> collection that we get back is immutable so we keep track of which <PLACE_HOLDER>s need to be deleted after they 've been folded into the janus graph step and then remove them from their step using has <PLACE_HOLDER>.remove has <PLACE_HOLDER>,final list < has container > removable has containers = new array list < > ( ) ; final set < string > step labels = current step . get labels ( ) ; has container holder . get has containers ( ) . for each ( has container -> { if ( graph step . process has container ids ( graph step @$ has container ) ) { step labels . for each ( janusgraph step :: add label ) ; removable has containers . add ( has container ) ; } } ) ;,collection has
do n't calculate cost if the server does n't have enough <PLACE_HOLDER> or is loading the segment,if ( proposal segment size > server . get available size ( ) || server . is loading segment ( proposal segment ) ) { return double . positive_infinity ; },server have
<PLACE_HOLDER> 6 depends on 5 do not add the config <PLACE_HOLDER> to the dependency list . do not add the feature <PLACE_HOLDER> with no dependency to the dependency list .,test data data = new test data ( ) ; data . info = ai ; data . paths with code = paths with code ; return data ;,depends add
do we need to pad the file so the stripe does n't straddle a block <PLACE_HOLDER> ?,long start = raw writer . get bytes written ( ) ; final long current stripe size = index size + data size + footer . get serialized size ( ) ; final long available = block size - ( start % block size ) ; final long overflow = current stripe size - adjusted stripe size ; final float avail ratio = ( float ) available / ( float ) default stripe size ; if ( avail ratio > __num__ && avail ratio < __num__ && avail ratio > padding tolerance ) { float correction = overflow > __num__ ? ( float ) overflow / ( float ) adjusted stripe size : __num__ ; correction = correction > padding tolerance ? padding tolerance : correction ; adjusted,stripe straddle
no alias to stage.. no local <PLACE_HOLDER>,if ( localwork . get alias to fetch work ( ) . is empty ( ) ) { new local work . set has staged alias ( false ) ; curr task . set backup task ( local task . get backup task ( ) ) ; curr task . set backup children tasks ( local task . get backup children tasks ( ) ) ; return ; },alias stage..
unless an accept <PLACE_HOLDER> parameter is explicitly set to false @$ respond that this server accepts <PLACE_HOLDER>,if ( ! boolean . to string ( false ) . equals ignore case ( request . get parameter ( __str__ ) ) ) { response . set header ( __str__ @$ __str__ ) ; },server accepts
we need to check if the class has a custom timer <PLACE_HOLDER>,return class timed == null || class timed . name ( ) . is empty ( ) ? metric registry . name ( clazz @$ method name ) : metric registry . name ( class timed . name ( ) @$ method name ) ;,class has
element consumed <PLACE_HOLDER> if policy element found,return false ;,element consumed
we need to run the subprocedure even if we have no relevant regions . the coordinator expects <PLACE_HOLDER> in the procedure and without sending message the snapshot attempt will hang and fail .,log . debug ( __str__ + snapshot . get name ( ) + __str__ + snapshot . get table ( ) + __str__ + snapshot . get type ( ) ) ; foreign exception dispatcher exn dispatcher = new foreign exception dispatcher ( snapshot . get name ( ) ) ; configuration conf = rss . get configuration ( ) ; long timeout millis = conf . get long ( snapshot_timeout_millis_key @$ snapshot_timeout_millis_default ) ; long wake millis = conf . get long ( snapshot_request_wake_millis_key @$ snapshot_request_wake_millis_default ) ; switch ( snapshot . get type ( ) ) { case flush : snapshot subprocedure pool task manager = new snapshot subprocedure pool ( rss . get server name ( ) . to string ( ) @$ conf @$,coordinator expects
this method will only return full resource <PLACE_HOLDER> when activating one @$ to give the caller the result atomically with the activation .,wm full resource plan full plan after alter = getms ( ) . alter resource plan ( request . get resource plan name ( ) @$ request . get ns ( ) @$ request . get resource plan ( ) @$ request . is is enable and activate ( ) @$ request . is is force deactivate ( ) @$ request . is is replace ( ) ) ; if ( full plan after alter != null ) { response . set full resource plan ( full plan after alter ) ; } return response ;,method return
if the filter does not need bound <PLACE_HOLDER> @$ execute query using full engine,if ( ! needs bound value ( filter expression ) ) { materialized result result = runner . execute ( __str__ + filter ) ; assert equals ( result . get types ( ) . size ( ) @$ __num__ ) ; boolean query result ; if ( result . get materialized rows ( ) . is empty ( ) ) { query result = false ; } else { assert equals ( result . get materialized rows ( ) . size ( ) @$ __num__ ) ; query result = ( boolean ) iterables . get only element ( result . get materialized rows ( ) ) . get field ( __num__ ) ; } results . add ( query result ) ; } return results ;,filter need
is ok when considering two zero duration inervals this is the simplest case @$ as the two intervals either have a <PLACE_HOLDER> or not if not @$ then they are equal and abut,interval test0808 = new interval ( __num__ @$ __num__ ) ; interval test1010 = new interval ( __num__ @$ __num__ ) ;,intervals have
this intentionally does disconnect without <PLACE_HOLDER>ing the vaadin session to avoid dead<PLACE_HOLDER>s where the server uses a <PLACE_HOLDER> for the websocket connection,new thread ( ( ) -> { set push connection ( null ) ; } ) . start ( ) ;,server uses
if the decision point list contains <PLACE_HOLDER> of the parent rows @$ update it to include the new row as well,if ( ( decision point list . contains ( new integer ( old row num ) ) || decision point list . contains ( new integer ( new row num ) ) ) && ! decision point list . contains ( new integer ( combined row num ) ) ) { decision point list . add element ( new integer ( combined row num ) ) ; },list contains
we know that the parameters have the same <PLACE_HOLDER> @$ since this is what the descriptor 's hash code & equality are based on . the only thing that may be different is the description . and since the proposed parameter does not have a description @$ we want to use whatever is currently set .,return old parameter == null ? descriptor : old parameter . get descriptor ( ) ;,parameter have
when tag compression is been used in this file @$ tag compression context will have a not null <PLACE_HOLDER> passed .,if ( tag compression context != null ) { tag compression context . uncompress tags ( source @$ dest @$ tags length ) ; } else { byte buffer utils . copy from stream to buffer ( dest @$ source @$ tags length ) ; },context passed
we need to pass the meta from the pod in the deployment as that is what matches machine <PLACE_HOLDER>,final object meta template meta = to create . get spec ( ) . get template ( ) . get metadata ( ) ; store starting machine ( created pod @$ template meta @$ machine configs @$ server resolver ) ;,what matches
first describe what is taking a long <PLACE_HOLDER> .,switch ( m_fragment context ) { default : case ro_batch : case rw_batch : sb . append ( __str__ + m_current procedure name ) ; break ; case catalog_update : sb . append ( __str__ ) ; break ; case catalog_load : sb . append ( __str__ ) ; break ; },what taking
make sure that both listeners have received their <PLACE_HOLDER> .,assert equals ( __str__ + __str__ @$ __num__ @$ call1 listener . collected events . size ( ) ) ;,listeners received
attention : only update the exist <PLACE_HOLDER> for sorted storage method,double [ ] values ; if ( vector storage utils . use int key ( vector ) ) { values = get int double vector ( ) . get storage ( ) . get values ( ) ; } else { values = get long double vector ( ) . get storage ( ) . get values ( ) ; } for ( int i = __num__ ; i < values . length ; i ++ ) { values [ i ] = func . update ( ) ; },attention exist
map types attribute as key enum no <PLACE_HOLDER> as value,ret . add ( get attribute def ( attr name prefix @$ atlas base type def . get map type name ( attribute type @$ enum_def_with_no_default . get name ( ) ) ) ) ;,types attribute
get transform record ; if no transform is found @$ will throw illegal argument <PLACE_HOLDER>,transform record info = user record . m transform records . get resource or throw ( resource id ) ;,record throw
remove all the elements from the <PLACE_HOLDER> which match the first element pf the <PLACE_HOLDER> and also get the number of occurnce of first element of <PLACE_HOLDER> @$ in the bag,int occurrence = bag . occurrences ( as list . get ( __num__ ) ) ;,which match
the status from error response overwrites the one in update response . however @$ results and errors are not expected to have overlapping key . see batch update response <PLACE_HOLDER> .,if ( error data != null ) { update data . put ( __str__ @$ error data . get ( __str__ ) ) ; update data . put ( __str__ @$ error data ) ; },batch update
this is when no seek <PLACE_HOLDER> is set during start delay . if developers change the seek <PLACE_HOLDER> during the delay @$ animation will start from the seeked position right away .,if ( m start time > frame time && m seek fraction == - __num__ ) { return false ; } else { m running = true ; start animation ( ) ; },the seek
remember that these elements got <PLACE_HOLDER>,for ( element element : instance . non zeroes ( ) ) { int j = element . index ( ) ; update steps . set quick ( j @$ get step ( ) ) ; update counts . increment quick ( j @$ __num__ ) ; } next step ( ) ;,elements got
remove the linkstamp objects from inputs so that create linkstamp compile <PLACE_HOLDER> does n't cause a circular dependency .,immutable set < artifact > expanded linker artifacts no linkstamps = sets . difference ( expanded linker artifacts @$ linkstamp object artifacts ) . immutable copy ( ) ; cc toolchain variables variables ; try { immutable list . builder < string > user link flags = immutable list . < string > builder ( ) . add all ( linkopts ) . add all ( cpp configuration . get linkopts ( ) ) ; if ( is lto indexing && cpp configuration . use standalone lto indexing command lines ( ) ) { user link flags . add all ( cpp configuration . get lto index options ( ) ) ; } variables = link build variables . setup variables ( get link type ( ) .,linkstamp compile
file tree file name iterator returns all paths in the directory tree which file name is build file name <PLACE_HOLDER> out build file name to get a path pointing to package root folder all paths are relative,package roots = immutable sorted set . copy of ( iterators . transform ( file tree file name iterator . of ( file tree @$ build file name ) @$ path -> more paths . get parent or empty ( path ) ) ) ; break ; default : throw new illegal state exception ( ) ;,which file
the window manager will give us a valid window <PLACE_HOLDER>,if ( m show dialog for submenu ) { m sub menu helper = new menu dialog helper ( sub menu ) ; m sub menu helper . show ( null ) ; return true ; },manager give
the following should take over a <PLACE_HOLDER>,secure random rand = new secure random ( ) ; rand . next bytes ( new byte [ __num__ ] ) ;,following take
note : a valid authorization response contains either a <PLACE_HOLDER> ' or 'error ' parameter .,http servlet response response = mock ( http servlet response . class ) ; filter chain filter chain = mock ( filter chain . class ) ; this . filter . do filter ( request @$ response @$ filter chain ) ; verify ( filter chain ) . do filter ( any ( http servlet request . class ) @$ any ( http servlet response . class ) ) ;,response contains
anonymous access . when we add the o auth 2 <PLACE_HOLDER> we do n't know about boot endpoints @$ so the default has to be a 401 .,assert equals ( http status . unauthorized @$ http . get status code ( __str__ ) ) ;,o auth
the caller specified a data <PLACE_HOLDER>,for ( string datanode uuid : data node uuids ) { boolean found = false ; for ( int index = __num__ ; index < info . length ; index ++ ) { if ( info [ index ] . get datanode uuid ( ) . equals ( datanode uuid ) ) { data node names . add ( info [ index ] . get xfer addr ( ) ) ; datanode infos . add ( name node adapter . get datanode ( cluster . get namesystem ( nn index ) @$ info [ index ] ) ) ; found = true ; break ; } } if ( ! found ) { throw new io exception ( __str__ + datanode uuid ) ; } },caller specified
note that this might miss collisions @$ use internal tick callback to check for collision on every tick . see internal tick <PLACE_HOLDER> on how to implement it .,contacts . clear ( ) ;,note see
known previous uid @$ so we know <PLACE_HOLDER> package set to check,for ( string pkg : package names ) { hash set < string > set = m backup participants . get ( old uid ) ; if ( set != null && set . contains ( pkg ) ) { remove package from set locked ( set @$ pkg ) ; if ( set . is empty ( ) ) { if ( more_debug ) slog . v ( tag @$ __str__ ) ; m backup participants . remove ( old uid ) ; } } },package set
for child cuboid @$ some measures do n't need <PLACE_HOLDER> .,if ( parent id != cuboid id ) { boolean [ ] aggr mask = new boolean [ measure descs . length ] ; for ( int i = __num__ ; i < measure descs . length ; i ++ ) { aggr mask [ i ] = ! measure descs [ i ] . get function ( ) . get measure type ( ) . only aggr in base cuboid ( ) ; if ( ! aggr mask [ i ] ) { logger . info ( __str__ @$ measure descs [ i ] ) ; } } scanner . set aggr mask ( aggr mask ) ; } return scanner ;,measures need
make sure dn 's jmx sees the failed <PLACE_HOLDER>,final string [ ] expected failed volumes = { dn1 vol1 . get absolute path ( ) } ; data node test utils . trigger heartbeat ( dn ) ; fs dataset spi < ? > fsd = dn . getfs dataset ( ) ; assert equals ( expected failed volumes . length @$ fsd . get num failed volumes ( ) ) ; assert array equals ( expected failed volumes @$ convert to absolute paths ( fsd . get failed storage locations ( ) ) ) ;,jmx sees
this filter allows all <PLACE_HOLDER> to stay in the view,set filter text ( __str__ ) ; tree path first visible path = scroll to ( __num__ ) ; set filter text ( __str__ ) ; tree path new first visible path = get last visible path ( ) ; assert close enough ( first visible path @$ new first visible path ) ;,filter allows
if all the others require <PLACE_HOLDER> @$ this is likely a bug,assert equals ( __str__ @$ native meta . get modify column statement ( __str__ @$ new value meta string ( __str__ @$ __num__ @$ __num__ ) @$ __str__ @$ false @$ __str__ @$ false ) ) ;,others require
if any provider has been disabled @$ clear all last <PLACE_HOLDER>s for all providers . this is to be on the safe side in case a provider has <PLACE_HOLDER> derived from this disabled provider .,if ( ! m useable ) { m last location . clear ( ) ; m last location coarse interval . clear ( ) ; },provider has
client 1 add <PLACE_HOLDER> and put all,async invocation < void > register interest and put all in client1 = client1 . invoke async ( ( ) -> { region < string @$ ticker data > region = get cache ( ) . get region ( region name ) ; region . get attributes mutator ( ) . add cache listener ( new counting cache listener < > ( ) ) ; region . register interest ( __str__ ) ; do put all ( region @$ title @$ one_hundred ) ; } ) ;,client add
same as above @$ except this is a bit of stress testing . attach 5 database <PLACE_HOLDER> and make sure they are all removed .,int n = __num__ ; array list < string > attached db files = new array list < string > ( n ) ; for ( int i = __num__ ; i < n ; i ++ ) { attached db files . add ( m database . get path ( ) + i ) ; } db obj = sq lite database . open or create database ( m database . get path ( ) @$ null ) ; db obj . execsql ( __str__ ) ; for ( int i = __num__ ; i < n ; i ++ ) { db obj . execsql ( __str__ + attached db files . get ( i ) + __str__ + i ) ; } assert true (,same attach
release only tickets that have been flushed . fail the <PLACE_HOLDER> .,iterator < flush ticket > ticket iterator = m ticket set . iterator ( ) ; while ( ticket iterator . has next ( ) ) { flush ticket ticket = ticket iterator . next ( ) ; ticket iterator . remove ( ) ; if ( ticket . get target counter ( ) <= m flush counter . get ( ) ) { ticket . set completed ( ) ; } else { ticket . set error ( exc ) ; } },release fail
since the 200 was committed @$ the 500 did not get the <PLACE_HOLDER> to be written,assert that ( __str__ @$ response . get status ( ) @$ is ( __num__ ) ) ; assert that ( __str__ @$ response . get content ( ) @$ is ( __str__ ) ) ; assuming that ( http version == http version . http_1_1 @$ ( ) -> assert that ( response @$ contains header value ( __str__ @$ __str__ ) ) ) ;,500 get
only append if target does not contain the <PLACE_HOLDER>,if ( ! target . contains key ( key ) ) { target . put ( key @$ entry . get value ( ) . get value ( profiles ) ) ; },target contain
at the moment @$ web assembly only supports one memory <PLACE_HOLDER> @$ thus the only valid memory index is 0 .,assert . assert int equal ( mem index @$ __num__ @$ __str__ ) ; long data offset = __num__ ; byte instruction ; do { instruction = read1 ( ) ; switch ( instruction ) { case instructions . i32_const : data offset = read signed int32 ( ) ; break ; case instructions . global_get : read global index ( ) ; throw new wasm exception ( __str__ ) ; case instructions . end : break ; default : assert . fail ( string . format ( __str__ @$ instruction ) ) ; } } while ( instruction != instructions . end ) ;,assembly supports
some adjustments to not crash the <PLACE_HOLDER> with empty data,if ( m max elements == __num__ ) { m max elements = __num__ ; } if ( m mcount == __num__ ) { m mcount = __num__ ; } if ( m first element == m last element ) { m first element = __num__ ; m last element = __num__ ; } if ( m max cards == __num__ ) m max cards = __num__ ; meta info . setm dynamic axis ( true ) ; meta info . setm has colored cumulative ( true ) ; meta info . setm type ( type ) ; meta info . setm title ( r . string . stats_forecast ) ; meta info . setm backwards ( true ) ; meta info . setm value labels ( m,adjustments crash
replica set <PLACE_HOLDER> with no address ...,host and port = server address . default host ( ) ;,replica set
if any of the declared <PLACE_HOLDER>s match the package <PLACE_HOLDER> @$ it 's valid .,for ( signature signature : provider . signatures ) { if ( signature . equals ( package info . signatures [ __num__ ] ) ) return true ; } return false ;,any match
negative transition @$ which makes a duplicated local time <PLACE_HOLDER>,if ( ( ( duplicated time opt & std_dst_mask ) == local_std && dst to std ) || ( ( duplicated time opt & std_dst_mask ) == local_dst && std to dst ) ) { transition += offset after ; } else if ( ( ( duplicated time opt & std_dst_mask ) == local_std && std to dst ) || ( ( duplicated time opt & std_dst_mask ) == local_dst && dst to std ) ) { transition += offset before ; } else if ( ( duplicated time opt & former_latter_mask ) == local_former ) { transition += offset before ; } else { transition += offset after ; },which makes
so 'source ' and 'original source ' are null otherwise declaring source has some <PLACE_HOLDER>,if ( declaring source == null ) { if ( stack trace option == include stack trace option . complete || stack trace option == include stack trace option . only_for_declaring_source ) { declaring source = source provider . get ( call stack ) ; } else { declaring source = source provider . get from class names ( module source . get module class names ( ) ) ; } },source has
join keys have difference <PLACE_HOLDER> ?,if ( k1 . size ( ) != k2 . size ( ) ) { return k1 . size ( ) - k2 . size ( ) ; } if ( comparators . length == __num__ ) { return __num__ ; },keys have
last chance @$ look in the old hive config value . still avoiding <PLACE_HOLDER> .,if ( reporters to start == null ) { reporters to start = conf . get ( metastore conf . conf vars . hive_metrics_reporter . get hive name ( ) ) ; if ( reporters to start == null ) { reporters to start = metastore conf . get var ( conf @$ metastore conf . conf vars . metrics_reporters ) ; } },chance avoiding
iterator should have an <PLACE_HOLDER> when dir is not empty,it = fs . list status iterator ( d ) ; assert true ( it . has next ( ) ) ; it . next ( ) ; assert false ( it . has next ( ) ) ; for ( int i = __num__ ; i < list limit * __num__ ; i ++ ) { p = new path ( d @$ __str__ + i ) ; assert . assert true ( fs . create new file ( p ) ) ; },iterator have
this will be picked up by janitor to figure out what really happened and correct the state if needed note that the default case includes the null <PLACE_HOLDER>,case undefined : default : return transaction status . unknown ;,case includes
service type method automatic recovery deliberately disabled as spring has it 's own recovery <PLACE_HOLDER>,class < ? > channeln class = class . for name ( __str__ ) ; method channeln basic publish = channeln class . get declared method ( __str__ @$ string . class @$ string . class @$ boolean . class @$ boolean . class @$ amqp . basic properties . class @$ byte [ ] . class ) ; expected trace channeln basic publish trace = expectations . event ( rabbitmq test constants . rabbitmq_client @$ channeln basic publish @$ null @$ remote address @$ __str__ + rabbitmq test constants . exchange @$ expectations . annotation ( __str__ @$ rabbitmq test constants . exchange ) @$ expectations . annotation ( __str__ @$ rabbitmq test constants . routing_key_pull ) ) ;,recovery has
move over a bit so the oval does n't overwrite the <PLACE_HOLDER>,int label padding = __num__ ; location . x -= label padding * __num__ ; location . y -= label padding * __num__ ;,oval overwrite
for the current purpose @$ both blocks in question must either both have a <PLACE_HOLDER> or both not have a <PLACE_HOLDER> to be considered equal @$ and it turns out here that that 's not the case .,if ( ( ( primary1 == - __num__ ) || ( primary2 == - __num__ ) ) && ( primary1 != primary2 ) ) { return false ; },blocks have
treat the parameter & return types of the function as 'declared ' if the function has js <PLACE_HOLDER> with type annotations @$ or a parameter has inline js <PLACE_HOLDER> . note that this does not distinguish between cases where all parameters have js <PLACE_HOLDER> vs only one parameter has js <PLACE_HOLDER> .,boolean declared = ( arg jsdoc != null && arg jsdoc . contains declaration ( ) ) || node util . function has inline jsdocs ( i argument ) ; i argument . setjs type ( match function ( restricted parameter @$ arg fn type @$ declared ) ) ;,parameter has
make this loop forever in case a hang in the first client prevents the listener from ever starting . this loop should make the test <PLACE_HOLDER> and thus we 'll get a thread dump,while ( true ) { try { s = new socket ( __str__ @$ node1 port ) ; break ; } catch ( connect exception ce ) { thread . sleep ( __num__ ) ; } } input stream in = s . get input stream ( ) ; final long end = system . current time millis ( ) + __num__ ; while ( system . current time millis ( ) < end ) { log ( __str__ ) ; thread . sleep ( __num__ ) ; try { if ( in . read ( ) < __num__ ) { break ; } } catch ( io exception ioe ) { break ; } } if ( system . current time millis ( ) > end ),loop make
wait til the sources have emitted number <PLACE_HOLDER> for each key and completed a checkpoint,subtask index flat mapper . work completed latch . await ( deadline . time left ( ) . to millis ( ) @$ time unit . milliseconds ) ;,sources emitted
all set ; now set up the <PLACE_HOLDER> and launch the agent,set up pipes ( ) ; m agent = m backup manager service . bind to agent synchronous ( m target app @$ full backup . key_value_data_token . equals ( info . domain ) ? application thread constants . backup_mode_incremental : application thread constants . backup_mode_restore_full ) ; m agent package = pkg ;,all set
all types must have the same <PLACE_HOLDER> of children,if ( type . get children ( ) . size ( ) != number of children ) { return null ; },types have
constuctor can handle null <PLACE_HOLDER>,docs = new document [ ] { new document ( location @$ mime type @$ standard charsets . utf_8 . name ( ) @$ this @$ null @$ keywlist @$ single list ( title ) @$ extractor . get summary information ( ) . get author ( ) @$ extractor . get doc summary information ( ) . get company ( ) @$ null @$ descriptions @$ __num__ @$ __num__ @$ contents . to string ( ) @$ null @$ null @$ null @$ false @$ extractor . get summary information ( ) . get last save date time ( ) ) } ;,constuctor handle
not already . the previous call will activate the right <PLACE_HOLDER> in the context of the <PLACE_HOLDER> model but will only show the <PLACE_HOLDER> to the user if chrome was already in the foreground . the intent is getting the <PLACE_HOLDER> id mostly because it does not cost much to do so . when receiving the intent @$ the <PLACE_HOLDER> associated with the <PLACE_HOLDER>,intent new intent = tab . create bring tab to front intent ( m tab . get id ( ) ) ; if ( new intent != null ) { new intent . add flags ( intent . flag_activity_new_task ) ; m tab . get application context ( ) . start activity ( new intent ) ; },call activate
n.b . do n't set regexp <PLACE_HOLDER> to empty to keep regexp,this . regexp data field . set text ( __str__ ) ;,n.b set
if the text is not of adequate size @$ then show an error <PLACE_HOLDER>,if ( text . length <= __num__ ) { throw new annotation exception ( invalid_symbol_text ) ; } if ( program == null ) { return create undecorated string ( prototype string @$ text ) ; },then show
check file content and bucket <PLACE_HOLDER> .,map < file @$ string > contents = test utils . get file content by path ( out dir ) ; for ( map . entry < file @$ string > file contents : contents . entry set ( ) ) { integer bucket id = integer . parse int ( file contents . get key ( ) . get parent file ( ) . get name ( ) ) ; assert . assert true ( bucket id >= __num__ && bucket id <= __num__ ) ; assert . assert equals ( string . format ( __str__ @$ bucket id @$ bucket id ) @$ file contents . get value ( ) ) ; },check file
another listener should not have received any <PLACE_HOLDER>,assert that ( another queue . is empty ( ) @$ is ( true ) ) ; kv state registry . unregister kv state ( job id @$ job vertex id @$ key group range @$ registration name @$ kv stateid ) ; assert that ( state deregistration notifications . poll ( ) @$ equal to ( job id ) ) ;,listener received
data output.write utf will only accept 65535 <PLACE_HOLDER>,int max buffer = __num__ ;,utf accept
same type @$ request code and intent action implies <PLACE_HOLDER> .,assert that ( pending intent . get activity ( context @$ __num__ @$ new intent ( __str__ ) @$ flag_no_create ) ) . is same instance as ( pending intent ) ;,code implies
for each context set remote <PLACE_HOLDER> .,for ( entry < cluster node @$ grid spi test context > e : ctxs . entry set ( ) ) { for ( cluster node n : nodes ) { if ( ! n . equals ( e . get key ( ) ) ) e . get value ( ) . remote nodes ( ) . add ( n ) ; } },context set
priority 4 : find all supported java <PLACE_HOLDER> @$ and use the newest .,list < file > java home dirs = java finder . find supported java home from installations ( java config @$ java filter ) ; if ( ! java home dirs . is empty ( ) ) { java home dir = java home dirs . iterator ( ) . next ( ) ; if ( save ) { java config . save java home ( java home dir ) ; } system . out . println ( java home dir ) ; return exit_success ; } return exit_failure ;,all supported
get the annotation associated witht the local attr <PLACE_HOLDER>,if ( attr decl . get attribute node ( schema symbols . att_ref ) == null ) { attr use . f annotations = attribute . get annotations ( ) ; } else { xs object list annotations ; if ( annotation != null ) { annotations = new xs object list impl ( ) ; ( ( xs object list impl ) annotations ) . addxs object ( annotation ) ; } else { annotations = xs object list impl . empty_list ; } attr use . f annotations = annotations ; },annotation associated
buffers are only discarded after they are acked . discarding them here would cause the <PLACE_HOLDER> to generate too much work for the receiver .,m_future . set ( true ) ;,them cause
use a new redirect each time as different clients could be requesting the <PLACE_HOLDER> with different host names,final redirect handler redirect handler = new redirect handler ( location ) ; redirect handler . handle request ( exchange ) ;,time requesting
initialize the default edit <PLACE_HOLDER> .,get view ( ) . add view mode ( m_edit mode ) ; m_edit mode listener = new internal edit mode listener < node type @$ edge type > ( m_graph listeners ) ; m_edit mode . add listener ( m_edit mode listener ) ;,default edit
a volt db extension to support indexed <PLACE_HOLDER> and partial indexes,expression predicate = ( expression ) arguments [ __num__ ] ; @ suppress warnings ( __str__ ) java . util . list < expression > index exprs = ( java . util . list < expression > ) arguments [ __num__ ] ; boolean assume unique = ( ( boolean ) arguments [ __num__ ] ) . boolean value ( ) ; if ( index exprs != null ) { table works . add expr index ( index columns @$ index exprs . to array ( new expression [ index exprs . size ( ) ] ) @$ name @$ unique @$ migrating @$ predicate ) . set assume unique ( assume unique ) ; break ; } org . hsqldb_voltpatches . index . index added index =,extension support
check access in case the principal has no authorization <PLACE_HOLDER>,try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_general ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { } try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_auditing ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { } try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_ownership ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { },principal has
add the auto expand <PLACE_HOLDER>,m entry listener . on pending entry added ( m row . get entry ( ) ) ; m bubble controller . update bubble ( m row . get entry ( ) ) ;,auto expand
caller did not specify any root <PLACE_HOLDER> @$ so just use our own .,if ( throwable == null ) { return new cache closed exception ( reason @$ disconnect cause ) ; },caller specify
delegate to the old behavior @$ allowing <PLACE_HOLDER> to override .,map to map from id ( data @$ obj ) ;,delegate allowing
on finish : when function returns something : store register 0 ; return value <PLACE_HOLDER> store register 2 <PLACE_HOLDER> store register 1 <PLACE_HOLDER> push register 0 return when function does not return anything : store register 2 <PLACE_HOLDER> store register 1 <PLACE_HOLDER> when original function has some returns @$ but no return at the end of function : push undefined locjump : store register,int return reg = - __num__ ; pos = start index + count - __num__ ; if ( code . get ( pos ) instanceof action return ) { pos -- ; if ( pos < start index ) { return false ; } if ( ! ( code . get ( pos ) instanceof action push ) ) { return false ; } action push pu = ( action push ) code . get ( pos ) ; if ( pu . values . size ( ) != __num__ ) { return false ; } if ( ! ( pu . values . get ( __num__ ) instanceof register number ) ) { return false ; } register number rn = ( register number ) pu .,store register
compensate for the dropped <PLACE_HOLDER>,s . request ( __num__ ) ; g . on next ( t ) ;,the dropped
this function also starts new <PLACE_HOLDER>,scan entity reference ( f content buffer ) ;,function starts
snapshot directory @$ as some file system implementations do not modify the parent directory 's mod <PLACE_HOLDER> when there are new sub items @$ for example @$ s 3 .,file status [ ] snapshot dirs = fs utils . list status ( fs @$ snapshot dir @$ p -> ! p . get name ( ) . equals ( snapshot description utils . snapshot_tmp_dir_name ) ) ;,implementations modify
ensure that committed watermarks match exactly the input <PLACE_HOLDER> because we shutdown in an orderly manner .,assert . assert true ( one record extractor . validate watermarks ( true @$ external watermark storage ) ) ;,watermarks match
make sure the client processes the <PLACE_HOLDER> correctly,test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( __str__ + client conn . get data events buffer ( ) . last written scn ( ) ) ; return client conn . get data events buffer ( ) . last written scn ( ) == __num__ ; } } @$ __str__ + consumer . get sequences ( ) @$ __num__ @$ log ) ; test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( __str__ + consumer . get event num ( ) ) ; return stats . get total stats ( ) . get num data,client processes
insert fake <PLACE_HOLDER> with the correct modification time . the call should return the fake <PLACE_HOLDER>,format = new dummy file input format ( ) ; format . set file path ( temp dir ) ; format . configure ( new configuration ( ) ) ; file base statistics out dated fake stats = new file base statistics ( math . min ( math . min ( mod time1 @$ mod time2 ) @$ mod time3 ) - __num__ @$ fake_size @$ base statistics . avg_record_bytes_unknown ) ; base statistics re gathered = format . get statistics ( out dated fake stats ) ; assert . assert equals ( __str__ @$ total @$ re gathered . get total input size ( ) ) ;,call return
bean should retain weak <PLACE_HOLDER> for this node,assert . assert equals ( failover target @$ failback target ) ; result = bean . increment ( ) ;,bean retain
animated image drawable calls post <PLACE_HOLDER> and release only if m post <PLACE_HOLDER>or exists .,if ( decoder . m animated ) { image decoder post process ptr = decoder . m post processor == null ? null : decoder ; decoder . check state ( true ) ; drawable d = new animated image drawable ( decoder . m native ptr @$ post process ptr @$ decoder . m desired width @$ decoder . m desired height @$ decoder . get color space ptr ( ) @$ decoder . check for extended ( ) @$ src density @$ src . compute dst density ( ) @$ decoder . m crop rect @$ decoder . m input stream @$ decoder . m asset fd ) ; decoder . m input stream = null ; decoder . m asset fd = null ; return,drawable calls
the data is locked in the <PLACE_HOLDER> @$ or we 're ignoring the <PLACE_HOLDER> . bypass the <PLACE_HOLDER> and read from upstream .,if ( next span == null ) { next data source = upstream data source ; next data spec = new data spec ( uri @$ http method @$ null @$ read position @$ read position @$ bytes remaining @$ key @$ flags ) ; } else if ( next span . is cached ) { uri file uri = uri . from file ( next span . file ) ; long file position = read position - next span . position ; long length = next span . length - file position ; if ( bytes remaining != c . length_unset ) { length = math . min ( length @$ bytes remaining ) ; } next data spec = new data spec ( file uri @$,cache bypass
eap 7 does not have the clone <PLACE_HOLDER>,profile clone eap7x ( ) ;,eap have
we should ideally only get back the <PLACE_HOLDER> field in our metadata @$ but the resource method here intentionally sends back the <PLACE_HOLDER> and the id to verify that restli does n't interfere with a manual metadata projection .,assert greeting ( metadata greeting @$ false @$ true @$ true ) ;,method sends
check the next check respected <PLACE_HOLDER> @$ because there 's no point of sending a request immediately only to get a 'connection refused ' error .,final stopwatch stopwatch = stopwatch . create started ( ) ; health check request logs . take ( ) ; assert that ( stopwatch . elapsed ( time unit . milliseconds ) ) . is greater than ( retry_interval . to millis ( ) * __num__ / __num__ ) ;,check respected
use a custom merge <PLACE_HOLDER> for put values .,super ( ( old item @$ new item ) -> new git hub repository ( old item ) . overwrite ( new item ) @$ git hub repository :: get id @$ repository -> repository != null ? repository : git hub repository . none ( ) @$ git hub repository :: none ) ;,custom merge
set <PLACE_HOLDER> to holder first because asynchronous fork : :put record can pull the <PLACE_HOLDER> when it detects fork state.failed status .,fork throwable holder holder = task . get fork throwable holder ( this . broker ) ; holder . set throwable ( this . get index ( ) @$ t ) ; this . fork state . set ( fork state . failed ) ; this . logger . error ( string . format ( __str__ @$ this . index @$ this . task id @$ holder ) @$ t ) ;,record pull
this test can be very liberal . too liberal will just do some <PLACE_HOLDER> refreshes . too conservative will display obsolete info .,if ( upper . index of ( __str__ ) > - __num__ || upper . index of ( __str__ ) > - __num__ || upper . index of ( __str__ ) > - __num__ ) { direct refresh tree ( ) ; },liberal do
if the proxy host requires authentication then add the host <PLACE_HOLDER> to the <PLACE_HOLDER> provider,final auth configuration auth = proxy . get auth ( ) ; if ( auth != null ) { if ( credentials provider == null ) { credentials provider = new basic credentials provider ( ) ; } auth scope auth scope = new auth scope ( http host @$ auth . get realm ( ) @$ auth . get auth scheme ( ) ) ; credentials credentials = configure credentials ( auth ) ; credentials provider . set credentials ( auth scope @$ credentials ) ; },authentication add
let 's add a <PLACE_HOLDER> & setter,expression field exp = new field expression ( param field ) ; answer . add method ( __str__ + method name @$ acc_public @$ real type . get plain node reference ( ) @$ parameter . empty_array @$ class node . empty_array @$ new return statement ( field exp ) ) ;,"s" add
notification payload <PLACE_HOLDER>,path scheme path = scheme generator . write scheme ( ) ; string scheme xml = project filesystem . read file if it exists ( scheme path ) . get ( ) ; system . out . println ( scheme xml ) ; document builder factory db factory = document builder factory . new instance ( ) ; document builder d builder = db factory . new document builder ( ) ; document scheme = d builder . parse ( project filesystem . new file input stream ( scheme path ) ) ; x path factory xpath factory = x path factory . new instance ( ) ; x path build action xpath = xpath factory . newx path ( ) ; x path expression build action expr,notification payload
if another engine has set the async history session <PLACE_HOLDER> already @$ there 's no need to do it again .,if ( ! session factories . contains key ( async history session . class ) ) { async history session factory async history session factory = new async history session factory ( ) ; if ( async history listener == null ) { init default async history listener ( ) ; } async history session factory . set async history listener ( async history listener ) ; session factories . put ( async history session . class @$ async history session factory ) ; } ( ( async history session factory ) session factories . get ( async history session . class ) ) . register job data types ( cmmn async history constants . ordered_types ) ;,engine set
the super implementation does not handle the following <PLACE_HOLDER>,params . set endpoint identification algorithm ( identification protocol ) ; params . set algorithm constraints ( algorithm constraints ) ; if ( sni matchers . is empty ( ) && ! no sni matcher ) { params . setsni matchers ( null ) ; } else { params . setsni matchers ( sni matchers ) ; },implementation handle
the system ui checks the <PLACE_HOLDER> of app ops and updates the location icon accordingly .,intent intent = new intent ( location manager . high_power_request_change_action ) ; m context . send broadcast as user ( intent @$ user handle . all ) ;,ui checks
ensure removing with the iterator does n't corrupt the <PLACE_HOLDER>,if ( ( size = ht . size ( ) ) < __num__ ) { fail ( __str__ + size ) ; },iterator corrupt
its not unique @$ add a suitable <PLACE_HOLDER> ...,string stub = name . substring ( __num__ @$ name . length ( ) - ext . length ( ) - __num__ ) ; int index = __num__ ; do { index ++ ; name = stub + __str__ + index + __str__ + ext ; } while ( this . get script impl ( name ) != null ) ; return name ;,its add
this call generally deletes any files at locations that are declared <PLACE_HOLDER> of the action @$ although some actions perform additional work @$ while others intentionally keep previous <PLACE_HOLDER> in place .,try ( silent closeable d = profiler . profile ( profiler task . info @$ __str__ ) ) { action . prepare ( action execution context . get exec root ( ) ) ; } catch ( io exception e ) { throw to action execution exception ( __str__ @$ e @$ action @$ null ) ; },others keep
ensure wrapper view child has a <PLACE_HOLDER>,wrapper view wrapper view child = ( wrapper view ) child ; if ( ! wrapper view child . has header ( ) ) { continue ; },child has
each volume has 2 <PLACE_HOLDER>,int initial block count = num volumes * __num__ ; create file ( test file @$ initial block count ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; final fs dataset spi < ? extends fs volume spi > data = dn . data ; dn . data = mockito . spy ( data ) ; final int new volume count = __num__ ; list < thread > add volume delayed threads = collections . synchronized list ( new array list < > ( ) ) ; atomic boolean add volume error = new atomic boolean ( false ) ; atomic boolean list storage error = new atomic boolean ( false ) ; count down latch add volume,volume has
first let 's remove all <PLACE_HOLDER> which do n't belong in the parents,array list < expandable notification row > to remove = new array list < > ( ) ; for ( int i = __num__ ; i < m list container . get container child count ( ) ; i ++ ) { view view = m list container . get container child at ( i ) ; if ( ! ( view instanceof expandable notification row ) ) { continue ; } expandable notification row parent = ( expandable notification row ) view ; list < expandable notification row > children = parent . get notification children ( ) ; list < expandable notification row > ordered children = m tmp child order map . get ( parent ) ; if ( children != null ) { to,"s" remove
the return value of 0 indicates success @$ anything else indicates <PLACE_HOLDER>,return ret ;,value indicates
everything has a <PLACE_HOLDER> even arrays and null,size += __num__ ;,everything has
the list of remote <PLACE_HOLDER> got updated @$ so update the cached list <PLACE_HOLDER> which includes both local and network <PLACE_HOLDER>,if ( ! arrays . equals ( prev remote printers @$ current remote printers ) ) { refresh services ( ) ; prev remote printers = current remote printers ; },which includes
call the get <PLACE_HOLDER> to recursively fetch the resource,if ( obj != null ) { obj = obj . get ( a key @$ aliases visited @$ requested ) ; },the get
package manager must return a package <PLACE_HOLDER> and application <PLACE_HOLDER> .,final package info package info = new package info ( ) ; package info . package name = test_package_name ; when ( m mock package manager . get package info ( eq ( test_package_name ) @$ any int ( ) ) ) . then return ( package info ) ; final application info application info = new application info ( ) ; application info . uid = test_uid ;,manager return
make sure that in the working dir abc.jar has the new <PLACE_HOLDER>,assert that ( file utils . read file to string ( jar1 . to file ( ) @$ __str__ ) ) . is equal to ( __str__ ) ; verify ( config region @$ times ( __num__ ) ) . put ( eq ( __str__ ) @$ argument captor . capture ( ) @$ eq ( __str__ ) ) ; configuration = argument captor . get value ( ) ; assert that ( configuration . get jar names ( ) ) . contains exactly in any order ( __str__ @$ __str__ ) ;,abc.jar has
if both match @$ take <PLACE_HOLDER>,if ( got positive && got negative ) { if ( positive suffix . length ( ) > negative suffix . length ( ) ) { got negative = false ; } else if ( positive suffix . length ( ) < negative suffix . length ( ) ) { got positive = false ; } },both match
1 because a send causes the <PLACE_HOLDER> to be recreated again which sends a new demand advisory,assert remote advisory count ( advisory consumer @$ __num__ ) ; assert advisory broker counts ( __num__ @$ __num__ @$ __num__ ) ;,send causes
identifier <PLACE_HOLDER> with keyword in another release . identifier <PLACE_HOLDER> with keyword in letter @$ but not in case .,if ( ( token . equals ( token . identifier ) || token . equals ( token . macro identifier ) ) && ! token . is escaped ( ) ) { if ( token . collides with keyword ( ) ) parse exception . warning ( scanner @$ util . get message ( __str__ @$ token . name ) ) ; },identifier identifier
some jdk throws <PLACE_HOLDER> here @$ but should n't .,es . shutdown ( ) ;,jdk throws
unregister should notify dropwizard <PLACE_HOLDER>,registry . remove ( metric name . name ( __str__ ) . build ( ) ) ; mockito . verify ( listener ) . on gauge removed ( mockito . eq ( __str__ ) ) ; assert equals ( __num__ @$ registry . get gauges ( ) . size ( ) ) ; assert equals ( __num__ @$ registry . get metric registry ( ) . get gauges ( ) . size ( ) ) ;,unregister notify
if someone has made a <PLACE_HOLDER> and added this subscriber multiple times @$ let 's handle it gracefully,if ( subscription != null ) { try { s . cancel ( ) ; } catch ( final throwable t ) { ( new illegal state exception ( s + __str__ @$ t ) ) . print stack trace ( system . err ) ; } } else { subscription = s ; try { s . request ( __num__ ) ; } catch ( final throwable t ) { ( new illegal state exception ( s + __str__ @$ t ) ) . print stack trace ( system . err ) ; } },someone made
test that both spawns have the local <PLACE_HOLDER> attached as a execution info,assert that ( spawn . get execution info ( ) ) . contains key ( __str__ ) ; action execution context context = invocation . get argument ( __num__ ) ; file out err out err = context . get file out err ( ) ; called . add ( out err ) ; if ( spawn . get output files ( ) . size ( ) != __num__ ) { try ( output stream stream = out err . get output stream ( ) ) { stream . write ( __str__ . get bytes ( utf_8 ) ) ; stream . write ( ( test log helper . header_delimiter + __str__ ) . get bytes ( utf_8 ) ) ; stream . write ( __str__ . get,spawns have
' ! ' represents an interior node that represents an icann <PLACE_HOLDER> in the map . ' ? ' represents a leaf node @$ which represents an icann <PLACE_HOLDER> in map . ' : ' represents an interior node that represents a private <PLACE_HOLDER> in the map ' @$ ' represents a leaf node @$ which represents a private <PLACE_HOLDER> in the map .,if ( c == __str__ || c == __str__ || c == __str__ || c == __str__ ) { string domain = prefix_joiner . join ( stack ) ; if ( domain . length ( ) > __num__ ) { builder . put ( domain @$ public suffix type . from code ( c ) ) ; } },"" represents
the error handler built into the rest template should handle 400 and 500 series <PLACE_HOLDER> .,throw new rest client exception ( __str__ + response entity . get status code ( ) + __str__ ) ;,handler handle
we are dragging directly over a card @$ make sure that we also catch the gesture even if nobody else wants the touch <PLACE_HOLDER> .,if ( m callback . get child at position ( ev ) != null ) { on intercept touch event ( ev ) ; return true ; } else { cancel long press ( ) ; return false ; },nobody wants
intercept the record @$ which can be potentially modified ; this method does not throw <PLACE_HOLDER>,producer record < k @$ v > intercepted record = this . interceptors . on send ( record ) ; return do send ( intercepted record @$ callback ) ;,method throw
for any <PLACE_HOLDER>s in the controller callback @$ test browser callback should have overridden the <PLACE_HOLDER> and call matching api in the callback proxy .,for ( int i = __num__ ; i < methods . length ; i ++ ) { assert not equals ( __str__ + methods [ i ] + __str__ @$ browser callback . class @$ methods [ i ] . get declaring class ( ) ) ; assert not equals ( __str__ + methods [ i ] + __str__ @$ controller callback . class @$ methods [ i ] . get declaring class ( ) ) ; },callback overridden
aaa entity only gets <PLACE_HOLDER> when a persistent property is accessed,assert equals ( ( short ) __num__ @$ aaa entity . get field ina mapped superclass ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert true ( hibernate . is initialized ( aaa entity ) ) ; assert equals ( true @$ aaa entity . get field ina entity ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert equals ( __str__ @$ aaa entity . get field inaa entity ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert equals ( __num__ @$ aaa entity . get field inaaa entity ( ) ) ; assert equals,entity gets
cleanup allocated <PLACE_HOLDER> .,for ( long ptr : ptrs ) grid unsafe . free memory ( ptr ) ; ptrs . clear ( ) ;,cleanup allocated
could n't find an external in latest that we think matches this <PLACE_HOLDER> in my . so just add this <PLACE_HOLDER> and give it a conflict name if necessary .,if ( latest external location == null ) { try { external location result external location = add external ( my external location @$ monitor ) ; external location [ ] external locations = new external location [ ] { result external location @$ latest external location @$ my external location @$ null } ; adjustid maps for add ( external locations @$ result external location @$ my ) ; } catch ( duplicate name exception e ) { msg . error ( this @$ __str__ + my external location . get symbol ( ) . get name ( true ) + __str__ + e . get message ( ) ) ; } catch ( invalid input exception e ) { msg . error ( this @$ __str__,external add
copy stored explicit <PLACE_HOLDER> to the beginning of the array .,system . arraycopy ( state @$ __num__ @$ new state @$ __num__ @$ n ) ; state = new state ;,copy stored
create three link tree objects . one will change the dependencies @$ and one just changes destination <PLACE_HOLDER> to make sure that 's taken into account,python symlink tree first sym link tree build rule = new python symlink tree ( __str__ @$ build target @$ project filesystem @$ output path @$ immutable map . of ( paths . get ( __str__ ) @$ path source path . of ( project filesystem @$ more paths . relativize ( tmp dir . get root ( ) @$ file1 ) ) ) @$ first merge directories @$ graph builder ) ; python symlink tree second sym link tree build rule = new python symlink tree ( __str__ @$ build target @$ project filesystem @$ output path @$ immutable map . of ( paths . get ( __str__ ) @$ path source path . of ( project filesystem @$ more paths . relativize ( tmp dir .,one changes
the order of the next two <PLACE_HOLDER>s is important @$ as a change listener of the transport combo sets the proxy port <PLACE_HOLDER> to its default,connection panel . set selected transport ( preferred transport ) ; connection panel . set proxy port ( proxy port ) ; security panel . load account ( sip acc reg . get security registration ( ) ) ; presence panel . reinit ( ) ; presence panel . set presence enabled ( enable presence ) ; presence panel . set force peer to peer mode ( forcep2p ) ; presence panel . set poll period ( polling period ) ; presence panel . set subscription expiration ( subscription period ) ; if ( ! enable presence ) { presence panel . set presence options enabled ( enable presence ) ; } connection panel . set keep alive method ( keep alive method ) ; connection panel .,listener sets
and that local files always takes <PLACE_HOLDER> over remote files .,action lookup key action lookup key = new action lookup key ( ) { @ override public sky function name function name ( ) { return sky function name . for_testing ; } } ; sky key action key1 = action lookup data . create ( action lookup key @$ __num__ ) ; sky key action key2 = action lookup data . create ( action lookup key @$ __num__ ) ; artifact out1 = create derived artifact ( __str__ ) ; artifact out2 = create derived artifact ( __str__ ) ; map < sky key @$ sky value > metadata to inject = new hash map < > ( ) ; metadata to inject . put ( action key1 @$ action value with remote artifact ( out1 @$,files takes
check for overflow @$ if overflow is detected set the current <PLACE_HOLDER> to the max <PLACE_HOLDER> .,if ( current interval millis >= max interval millis / multiplier ) { current interval millis = max interval millis ; } else { current interval millis *= multiplier ; },check set
the return value of 0 indicates success @$ anything else indicates <PLACE_HOLDER>,return ret ;,anything indicates
notification payload <PLACE_HOLDER>,path scheme path = scheme generator . write scheme ( ) ; string scheme xml = project filesystem . read file if it exists ( scheme path ) . get ( ) ; system . out . println ( scheme xml ) ; document builder factory db factory = document builder factory . new instance ( ) ; document builder d builder = db factory . new document builder ( ) ; document scheme = d builder . parse ( project filesystem . new file input stream ( scheme path ) ) ; x path factory xpath factory = x path factory . new instance ( ) ; x path build xpath = xpath factory . newx path ( ) ; x path expression build expr = build,notification payload
if element has <PLACE_HOLDER> @$ then serialize them @$ otherwise serialize en empty tag .,if ( elem . has child nodes ( ) ) { state = enter element state ( null @$ null @$ tag name @$ preserve space ) ; child = elem . get first child ( ) ; while ( child != null ) { serialize node ( child ) ; child = child . get next sibling ( ) ; } end elementio ( tag name ) ; } else { if ( ! is document state ( ) ) { state . after element = true ; state . empty = false ; } },element has
1 is excluded via property @$ 3 already has <PLACE_HOLDER> @$ so we only expect two updates .,drain work queue ( su @$ nonstat_part_count - __num__ ) ; for ( int i = __num__ ; i < nonstat_part_count ; ++ i ) { verify stats up to date ( __str__ @$ __str__ + i @$ lists . new array list ( __str__ ) @$ ms client @$ i != excluded_part ) ; } verify stats up to date ( __str__ @$ __str__ + excluded_part @$ lists . new array list ( __str__ ) @$ ms client @$ false ) ; ms client . close ( ) ;,3 has
result will contain the <PLACE_HOLDER> which have to be output,final sorted set < attr > result = this . result ; result . clear ( ) ; if ( element . has attributes ( ) ) { named node map attrs = element . get attributes ( ) ; int attrs length = attrs . get length ( ) ; for ( int i = __num__ ; i < attrs length ; i ++ ) { attr attribute = ( attr ) attrs . item ( i ) ; string n uri = attribute . get namespaceuri ( ) ; string n name = attribute . get local name ( ) ; string n value = attribute . get value ( ) ; if ( ! xmlns_uri . equals ( n uri ) ) { result .,result contain
no standard add key <PLACE_HOLDER> as we want to catch ctrl globally no matter of focus,keyboard focus manager . get current keyboard focus manager ( ) . add key event post processor ( new key event post processor ( ) { @ override public boolean post process key event ( key event e ) { if ( e . getid ( ) == key event . key_pressed ) { la . key pressed ( e ) ; } if ( e . getid ( ) == key event . key_released ) { la . key released ( e ) ; } if ( e . getid ( ) == key event . key_typed ) { la . key typed ( e ) ; } return false ; } } ) ;,standard add
the host has partition <PLACE_HOLDER> @$ go ahead to move them,if ( m_cartographer . get master count ( ihid ) > __num__ ) { migrate partition leader message message = new migrate partition leader message ( ihid @$ integer . min_value ) ; for ( integer integer : live hids ) { m_mailbox . send ( core utils . geths id from host and site ( integer @$ host messenger . client_interface_site_id ) @$ message ) ; } message = new migrate partition leader message ( ihid @$ integer . min_value ) ; message . set start task ( ) ; message . set stop node service ( ) ; m_mailbox . send ( core utils . geths id from host and site ( ihid @$ host messenger . client_interface_site_id ) @$ message ) ; },host partition
testcases include enough ident <PLACE_HOLDER> for now .,assert equals ( __str__ @$ pretty ( __str__ ) ) ;,testcases include
chrome <PLACE_HOLDER> does not necessarily match the desired <PLACE_HOLDER> because of auto updates ...,if ( browser util . is chrome ( get desired capabilities ( ) ) ) { browser identifier = get expected user agent string ( get desired capabilities ( ) ) + __str__ ; } else if ( browser util . is firefox ( get desired capabilities ( ) ) ) { browser identifier = get expected user agent string ( get desired capabilities ( ) ) + __str__ ; } else { browser identifier = get expected user agent string ( desired capabilities ) + desired capabilities . get version ( ) ; },version match
we assume here that programs do n't change the <PLACE_HOLDER> of the keyword undefined to something other than the <PLACE_HOLDER> undefined .,if ( __str__ . equals ( name ) || __str__ . equals ( name ) ) { return ternary value . false ; } else if ( __str__ . equals ( name ) ) { return ternary value . true ; } else { return ternary value . unknown ; },programs change
indicate that the active request has performed the <PLACE_HOLDER>,active request . update performed ( ) ; return generate ok response ( response entity ) . build ( ) ;,request performed
if this project has correlated <PLACE_HOLDER> @$ create value generator and produce the correlated variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel @$ frame ) ; },project correlated
if not websocket @$ then just return the <PLACE_HOLDER>,return msg ;,then return
add view start add view <PLACE_HOLDER>,skeleton models . add ( new skeleton model builder ( ) . set start view ( btn3 ) . set end view ( btn4 ) . build ( ) ) ;,start add
now we look if user 2 received an <PLACE_HOLDER>,assert equals ( __str__ @$ __num__ @$ op set2 collector . collected events . size ( ) ) ;,user received
short circuit the no time bound <PLACE_HOLDER> .,if ( timeout < __num__ ) { try { processor . process ( now @$ region @$ mutations @$ wal edit ) ; } catch ( io exception e ) { string row = processor . get rows to lock ( ) . is empty ( ) ? __str__ : __str__ + bytes . to string binary ( processor . get rows to lock ( ) . iterator ( ) . next ( ) ) + __str__ ; log . warn ( __str__ + processor . get class ( ) . get name ( ) + __str__ + row @$ e ) ; throw e ; } return ; },circuit bound
web view inside browser does n't want initial <PLACE_HOLDER> to be set .,settings . set need initial focus ( false ) ;,view want
the kerberos based <PLACE_HOLDER> .,token token = logged in user . do as ( new privileged exception action < token > ( ) { @ override public token run ( ) throws io exception { get delegation token request request = records . new record ( get delegation token request . class ) ; request . set renewer ( renewer string ) ; return hs service . get delegation token ( request ) . get delegation token ( ) ; } } ) ; return token ;,kerberos based
if no service has the capability to introspect screen @$ we do not register callback in the window manager for window changes @$ so we have to ask the window manager what the focused window is to update the active one . the active window also determined <PLACE_HOLDER> from which windows are delivered .,synchronized ( m lock ) { if ( m windows for accessibility callback == null ) { m focused window id = get focused window id ( ) ; if ( window id == m focused window id ) { m active window id = window id ; } } },window determined
all trackers should have the node available with data <PLACE_HOLDER>,assert not null ( local tracker . get data ( false ) ) ; assert not null ( local tracker . block until available ( ) ) ; assert true ( bytes . equals ( local tracker . get data ( false ) @$ data one ) ) ; assert not null ( second tracker . get data ( false ) ) ; assert not null ( second tracker . block until available ( ) ) ; assert true ( bytes . equals ( second tracker . get data ( false ) @$ data one ) ) ; assert true ( thread . has data ) ; assert true ( bytes . equals ( thread . tracker . get data ( false ) @$ data one ) ),trackers have
basic key chain can mix output script <PLACE_HOLDER> .,if ( ( result = basic . find key from pub hash ( pub key hash ) ) != null ) return result ; if ( chains != null ) { for ( deterministic key chain chain : chains ) { if ( script type != null && script type != chain . get output script type ( ) ) continue ; if ( ( result = chain . find key from pub hash ( pub key hash ) ) != null ) return result ; } },chain mix
one write <PLACE_HOLDER> for session 1,request req1 = create write request ( __num__ @$ __num__ ) ; process request with wait ( req1 ) ;,one write
check if ping command does not encounter any <PLACE_HOLDER>,if ( process . exit value ( ) == __num__ ) { shell_result = parse ( process ) ; } else { shell_result = su_busyboox_ping ( params [ __num__ ] . address ) ; },command encounter
3 sentries @$ each completion should trigger the <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { case instance case instance = cmmn runtime service . create case instance builder ( ) . case definition key ( __str__ ) . start ( ) ; list < plan item instance > plan item instances = cmmn runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state ( plan item instance state . active ) . order by name ( ) . asc ( ) . list ( ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; cmmn runtime service . trigger plan item instance ( plan,sentries trigger
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
we want to be compatible back to gingerbread @$ but surface <PLACE_HOLDER> was n't introduced until honeycomb . since the interface can not use a surface <PLACE_HOLDER> @$ if the developer wants to display a preview we must use a surface holder . if the developer does n't want to display a preview we use a surface <PLACE_HOLDER> if we are running at least,try { if ( build . version . sdk_int >= build . version_codes . honeycomb ) { camera . set preview texture ( null ) ; } else { camera . set preview display ( null ) ; } } catch ( exception e ) { log . e ( tag @$ __str__ + e ) ; },interface use
need to convert value to number this happens because json treats <PLACE_HOLDER> as an integer even if the field is supposed to be a long,if ( value instanceof number ) { number number value = ( number ) value ; class < ? > clazz = field . get type ( ) ; if ( clazz == integer . class || clazz == int . class ) { return number value . int value ( ) ; } else if ( clazz == long . class || clazz == long . class ) { return number value . long value ( ) ; } else if ( clazz == double . class || clazz == double . class ) { return number value . double value ( ) ; } else if ( clazz == float . class || clazz == float . class ) { return number value . float value,json treats
when we get this far @$ we 're exiting @$ so no need to reset the <PLACE_HOLDER> .,system . set property ( __str__ @$ factory class ) ; transformer factory transformer factory = transformer factory . new instance ( ) ; transformer trans = transformer factory . new transformer ( ) ; trans . set output property ( output keys . omit_xml_declaration @$ __str__ ) ; trans . set output property ( output keys . indent @$ __str__ ) ; trans . set output property ( __str__ @$ __str__ ) ;,need reset
we have a bucket so assign the frist left <PLACE_HOLDER> and return,if ( list != null ) { tuple = list . get first ( ) ; return tuple ; },bucket assign
get object can return <PLACE_HOLDER> if constraints were specified but not met,if ( s3 object == null ) return null ; output stream output stream = null ; try { output stream = new buffered output stream ( new file output stream ( destination file ) ) ; byte [ ] buffer = new byte [ __num__ * __num__ ] ; int bytes read ; while ( ( bytes read = s3 object . get object content ( ) . read ( buffer ) ) > - __num__ ) { output stream . write ( buffer @$ __num__ @$ bytes read ) ; } } catch ( io exception e ) { throw new sdk client exception ( __str__ + e . get message ( ) @$ e ) ; } finally { close quietly ( output stream @$,object return
make sure we can tell that the buckets have low <PLACE_HOLDER>,vm0 . invoke ( ( ) -> validate redundancy ( __str__ @$ __num__ @$ __num__ @$ __num__ ) ) ;,buckets have
configure how content assist <PLACE_HOLDER> will appear .,assistant . enable auto activation ( store . get boolean ( sql preference constants . enable_auto_activation ) ) ; assistant . set auto activation delay ( store . get int ( sql preference constants . auto_activation_delay ) ) ; assistant . set proposal popup orientation ( i content assistant . proposal_overlay ) ; assistant . set sorter ( new sql completion sorter ( ) ) ; assistant . set information control creator ( get information control creator ( source viewer ) ) ;,content assist
since the api request specified a unique playlist id @$ the api response should return exactly <PLACE_HOLDER> playlist . if the response does not contain a playlist @$ then the specified playlist id was not found .,list < playlist > playlist list = playlist list response . get items ( ) ; if ( playlist list . is empty ( ) ) { system . out . println ( __str__ + playlist id ) ; return ; } playlist playlist = playlist list . get ( __num__ ) ;,response return
the caller expects <PLACE_HOLDER> @$ we have to use an intermediary .,return new id enumeration ( collections . enumeration ( availablei ds ) ) ;,caller expects
test server can see <PLACE_HOLDER> written by client,server . invoke ( client server host name verification distributed test :: do server region test ) ;,server see
if not active or if the flow catalog is not up yet then ca n't process config <PLACE_HOLDER>,if ( ! is active || ! this . flow catalog . is running ( ) ) { log . warn ( __str__ @$ this . is active ) ; return false ; } return true ;,then process
existing completed <PLACE_HOLDER> for servlet name,return null ;,existing completed
since the api request specified a unique <PLACE_HOLDER> id @$ the api response should return exactly one <PLACE_HOLDER> . if the response does not contain a <PLACE_HOLDER> @$ then the specified <PLACE_HOLDER> id was not found .,list < playlist > playlist list = playlist list response . get items ( ) ; if ( playlist list . is empty ( ) ) { system . out . println ( __str__ + playlist id ) ; return ; } playlist playlist = playlist list . get ( __num__ ) ;,response contain
matching legacy i pv 4 <PLACE_HOLDER> matching legacy i pv 4 <PLACE_HOLDER> matching second <PLACE_HOLDER> of i pv 4 addresses matching second <PLACE_HOLDER> of i pv 4 addresses matching i pv 6 <PLACE_HOLDER> matching i pv 6 <PLACE_HOLDER> matching i pv 4 cidr notation <PLACE_HOLDER> matching i pv 4 cidr notation <PLACE_HOLDER> matching i pv 6 cidr notation <PLACE_HOLDER> matching i pv 6 cidr,final string [ ] [ ] allowed = { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } ;,range pv
if needed @$ update the maximum stack <PLACE_HOLDER> and number of locals @$ and stack map frames .,if ( current basic block != null ) { if ( compute == compute_all_frames || compute == compute_inserted_frames ) { current basic block . frame . execute ( opcode @$ __num__ @$ null @$ null ) ; } else { int size = relative stack size + stack_size_delta [ opcode ] ; if ( size > max relative stack size ) { max relative stack size = size ; } relative stack size = size ; } if ( ( opcode >= opcodes . ireturn && opcode <= opcodes . return ) || opcode == opcodes . athrow ) { end current basic block with no successor ( ) ; } },maximum stack
a volt db extension to make the third parameter <PLACE_HOLDER>,parse list alt = new short [ ] { tokens . openbracket @$ tokens . question @$ tokens . comma @$ tokens . question @$ tokens . x_option @$ __num__ @$ tokens . comma @$ tokens . question @$ tokens . closebracket } ;,extension make
and adding an old one will not trigger any <PLACE_HOLDER>,cache . check and store ( now @$ a1 ) ;,one trigger
the <PLACE_HOLDER> in the linear axis will not have <PLACE_HOLDER> after the decimal point .,vertical axis . set label format ( __str__ ) ; linear axis horizontal axis = new linear axis ( ) ; horizontal axis . set label format ( __str__ ) ; scatter point series . set vertical axis ( vertical axis ) ; scatter point series . set horizontal axis ( horizontal axis ) ;,values have
pipeline extends pipeline <PLACE_HOLDER>,add pipeline base extends ( __str__ @$ pipeline transform . class ) ;,pipeline extends
the first column contains the labels @$ and the second column contains the <PLACE_HOLDER> .,box row = box . create horizontal box ( ) ; box col = box . create vertical box ( ) ; col . add ( label1 != null ? label1 : box . create vertical glue ( ) ) ; col . add ( label2 != null ? label2 : box . create vertical glue ( ) ) ; row . add ( col ) ; row . add ( box . create horizontal strut ( __num__ ) ) ; col = box . create vertical box ( ) ; col . add ( field1 != null ? field1 : box . create vertical glue ( ) ) ; col . add ( field2 != null ? field2 : box . create vertical glue ( ) ),column contains
check if the array is really too big . this is an optimistic check because the heap probably has other <PLACE_HOLDER> in it @$ so the next collection will throw an out of memory error if this object is allocated and survives .,if ( size . above or equal ( heap policy . get large array threshold ( ) ) ) { if ( size . above or equal ( heap policy . get maximum heap size ( ) ) ) { throw array allocation too large ; } unaligned heap chunk . unaligned header u chunk = heap chunk provider . get ( ) . produce unaligned chunk ( size ) ; result = allocate large array ( hub @$ length @$ size @$ u chunk @$ tlab @$ remembered set ) ; } else { aligned header new chunk = prepare new allocation chunk ( tlab ) ; result = allocate small array ( hub @$ length @$ size @$ tlab @$ remembered set @$ new chunk ),heap has
to avoid duplication only first subtask keeps track of next transactional id hint . otherwise all of the subtasks would write exactly same <PLACE_HOLDER> .,if ( get runtime context ( ) . get index of this subtask ( ) == __num__ && semantic == semantic . exactly_once ) { check state ( next transactional id hint != null @$ __str__ ) ; long next free transactional id = next transactional id hint . next free transactional id ; if ( get runtime context ( ) . get number of parallel subtasks ( ) > next transactional id hint . last parallelism ) { next free transactional id += get runtime context ( ) . get number of parallel subtasks ( ) * kafka producers pool size ; } next transactional id hint state . add ( new next transactional id hint ( get runtime context ( ) . get number of parallel,track write
check that net flow into a vertex equals <PLACE_HOLDER> @$ except at source and sink,if ( math . abs ( value + excess ( g @$ s ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ s ) ) ; system . err . println ( __str__ + value ) ; return false ; } if ( math . abs ( value - excess ( g @$ t ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ t ) ) ; system . err . println ( __str__ + value ) ; return false ; } for ( int v = __num__ ; v < g . v ( ) ; v ++ ) { if ( v == s || v == t,flow equals
remove all elements . should set array <PLACE_HOLDER> to null,partitions . clear ( ) ; table entity . set attribute ( __str__ @$ partitions ) ; init ( ) ; response = entity store . create or update ( new atlas entity stream ( entities info ) @$ false ) ; updated table = response . get first updated entity by type name ( table_type ) ; validate entity ( entities info @$ get entity from store ( updated table ) ) ;,elements set
okay @$ it 's showtime . let 's add the <PLACE_HOLDER> through live ddl,try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; } try { admin client . update application catalog ( null @$ new file ( path to deployment ) ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; },"s" add
test no balance to send <PLACE_HOLDER> . query balance before send <PLACE_HOLDER> .,long deplay account before balance = public methed . query account ( delay account3 address @$ blocking stub full ) . get balance ( ) ; long recevier account before balance = public methed . query account ( receiver account4 address @$ blocking stub full ) . get balance ( ) ; logger . info ( __str__ + deplay account before balance ) ; logger . info ( __str__ + recevier account before balance ) ; assert . assert false ( public methed . sendcoin delayed ( receiver account4 address @$ send coin amount @$ delay second @$ delay account3 address @$ delay account3 key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ;,balance send
they may have changed the document and the variables via instrumentation so update the <PLACE_HOLDER> to it,execution input = execution input . transform ( builder -> builder . variables ( parse result . get variables ( ) ) ) ; execution input ref . set ( execution input ) ; log not safe . debug ( __str__ @$ query ) ; final list < validation error > errors = validate ( execution input @$ document @$ graphql schema @$ instrumentation state ) ; if ( ! errors . is empty ( ) ) { log not safe . warn ( __str__ @$ query ) ; return new preparsed document entry ( errors ) ; } return new preparsed document entry ( document ) ;,document update
check request size will throw io <PLACE_HOLDER> if request is rejected,if ( is large request ( length ) ) { check request size when message received ( length ) ; si . set large request size ( length ) ; },size throw
at this point @$ suggestion <PLACE_HOLDER> matches the new selection <PLACE_HOLDER>,if ( update prompt and selection if match found ) { if ( ! suggestion key . equals ( selected option key ) || suggestion . get replacement string ( ) . equals ( tb . get text ( ) ) || force update text ) { set text ( suggestion . get replacement string ( ) ) ; selected option key = suggestion key ; } },key matches
script output contains ambiguous node <PLACE_HOLDER>,string script content = __str__ + __str__ + __str__ ; write node attribute script file ( script content @$ true ) ; node attributes provider . init ( get conf for node attribute script ( ) ) ; node attributes provider . start ( ) ;,output contains
impl in filter base might do unnecessary <PLACE_HOLDER> for off heap backed cells .,return false ;,impl do
this case represents only the polymer <PLACE_HOLDER> that are inside a function which is an arg to goog.load module,check state ( is function arg in goog load module ( enclosing node ) ) ; node enclosing script = node util . get enclosing script ( enclosing node ) ; node insertion point = get node for insertion ( enclosing script ) ; insertion point . add child to front ( declaration code ) ; compiler . report change to change scope ( insertion point ) ;,case represents
eliminate loops in case of the boot class loader returning <PLACE_HOLDER> as a parent,return ( parent == cl ) ? null : parent ;,case returning
if timestamps are equal then match images did not correctly match up the image <PLACE_HOLDER> and image proxy,preconditions . check argument ( ! min image info timestamp . equals ( min image proxy timestamp ) ) ; if ( min image info timestamp > min image proxy timestamp ) { for ( int i = m pending images . size ( ) - __num__ ; i >= __num__ ; i -- ) { if ( m pending images . key at ( i ) < min image info timestamp ) { image proxy image proxy = m pending images . value at ( i ) ; image proxy . close ( ) ; m pending images . remove at ( i ) ; } } } else { for ( int i = m pending image infos . size ( ) - __num__ ; i,images match
clear annotation database finder restore hierarchy <PLACE_HOLDER>,guice . set annotation database package names ( null ) ; guice . set hierarchy traversal filter factory ( new hierarchy traversal filter factory ( ) ) ;,finder restore
start the prepared thread so that it is writing znodes while the follower is restarting . on the first restart @$ the follow should use txnlog to catchup . for subsequent restart @$ the follower should use a <PLACE_HOLDER> to catchup .,if ( i == __num__ ) { mytestfoo thread . start ( ) ; log . info ( __str__ @$ index ) ; qu . restart ( index ) ; thread . sleep ( __num__ ) ; log . info ( __str__ @$ index ) ; qu . shutdown ( index ) ; thread . sleep ( __num__ ) ; log . info ( __str__ @$ index ) ; qu . restart ( index ) ; log . info ( __str__ @$ index ) ; },follower use
validate report <PLACE_HOLDER> .,file report file = new file ( location ) ; assert . assert true ( __str__ + location @$ report file . exists ( ) ) ; validate jdr report contents ( report file ) ;,validate report
do n't publish directly @$ ensure that the user explicitly reviews and approves <PLACE_HOLDER> first .,idempotent executor . execute and swallowio exceptions ( title @$ title @$ ( ) -> get or create blogger service ( auth data ) . posts ( ) . insert ( blog id @$ post ) . set is draft ( true ) . execute ( ) . get id ( ) ) ;,user reviews
load balancer receives all <PLACE_HOLDER> of callbacks,transport info1 . listener . transport ready ( ) ; verify ( state listener1 @$ times ( __num__ ) ) . on subchannel state ( state info captor . capture ( ) ) ; assert same ( connecting @$ state info captor . get all values ( ) . get ( __num__ ) . get state ( ) ) ; assert same ( ready @$ state info captor . get all values ( ) . get ( __num__ ) . get state ( ) ) ; verify ( state listener2 ) . on subchannel state ( state info captor . capture ( ) ) ; assert same ( connecting @$ state info captor . get value ( ) . get state ( ) ) ; resolver . listener,balancer receives
namespace declarations parameter has no <PLACE_HOLDER> if namespaces is false .,if ( ! f namespace declarations && f namespace aware ) { int len = attributes . get length ( ) ; for ( int i = len - __num__ ; i >= __num__ ; -- i ) { if ( xml symbols . prefix_xmlns == attributes . get prefix ( i ) || xml symbols . prefix_xmlns == attributes . getq name ( i ) ) { attributes . remove attribute at ( i ) ; } } } super . start element ( element @$ attributes @$ augs ) ;,declarations has
the candidate name did n't return any <PLACE_HOLDER> so the name is unique,if ( null == item ) { return true ; } else if ( item . get name ( ) . equals ( current job name ) ) { return true ; } else { return false ; },name return
the super implementation does not handle the following <PLACE_HOLDER>,identification protocol = params . get endpoint identification algorithm ( ) ; algorithm constraints = params . get algorithm constraints ( ) ; prefer local cipher suites = params . get use cipher suites order ( ) ; list < sni server name > sni names = params . get server names ( ) ; if ( sni names != null ) { no sni extension = sni names . is empty ( ) ; server names = sni names ; } collection < sni matcher > matchers = params . getsni matchers ( ) ; if ( matchers != null ) { no sni matcher = matchers . is empty ( ) ; sni matchers = matchers ; } if ( ( handshaker != null ) &&,implementation handle
the key'algo ' contains the <PLACE_HOLDER> @$ 'algorithm ' is the long version,final string algo full name = readkv ( __str__ ) ;,"keyalgo" contains
we do n't validate the string but let the parser do the <PLACE_HOLDER> for us it throws a validation exception,validate optional ( key @$ is optional @$ v -> { logical type t = logical type parser . parse ( v ) ; if ( t . get type root ( ) == logical type root . unresolved ) { throw new validation exception ( __str__ + v + __str__ ) ; } } ) ;,parser do
server confirmed that it 's possible to use stream compression . start stream compression <PLACE_HOLDER> the reader and writer with the new compressed version,init reader and writer ( ) ;,reader start
set background @$ if your root layout does n't have <PLACE_HOLDER>,final drawable window background = get window ( ) . get decor view ( ) . get background ( ) ; top blur view . setup with ( root ) . set frame clear drawable ( window background ) . set blur algorithm ( new support render script blur ( this ) ) . set blur radius ( radius ) . set has fixed transformation matrix ( true ) ; bottom blur view . setup with ( root ) . set frame clear drawable ( window background ) . set blur algorithm ( new support render script blur ( this ) ) . set blur radius ( radius ) . set has fixed transformation matrix ( true ) ; int initial progress = ( int ) ( radius,layout have
component has a <PLACE_HOLDER>,if ( constraints . ascent >= __num__ ) { int baseline = constraints . ascent ; constraints . descent = h - constraints . ascent + constraints . insets . bottom ; constraints . ascent += constraints . insets . top ; constraints . baseline resize behavior = c . get baseline resize behavior ( ) ; constraints . center padding = __num__ ; if ( constraints . baseline resize behavior == component . baseline resize behavior . center_offset ) { int next baseline = c . get baseline ( w @$ h + __num__ ) ; constraints . center offset = baseline - h / __num__ ; if ( h % __num__ == __num__ ) { if ( baseline != next baseline ) { constraints . center,component has
jvms : if the <PLACE_HOLDER> is finite and the divisor is an infinity @$ the result equals the <PLACE_HOLDER> . jvms : if the <PLACE_HOLDER> is a zero and the divisor is finite @$ the result equals the <PLACE_HOLDER> .,if ( x == __num__ || double . is infinite ( y ) ) { return x ; } double result = safe rem ( x @$ y ) ;,result equals
raptor connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support
new width is <PLACE_HOLDER> . make it smaller to match height .,if ( bound aspect > intrinsic aspect ) { final int width = ( int ) ( h * intrinsic aspect ) ; left = ( w - width ) / __num__ ; right = left + width ; } else { final int height = ( int ) ( w * ( __num__ / intrinsic aspect ) ) ; top = ( h - height ) / __num__ ; bottom = top + height ; },width is
why do we have a separate field for the singular form ? assoc <PLACE_HOLDER> and assoc key .,if ( finder schema . has assoc key ( ) ) { string part key = finder schema . get assoc key ( ) ; compound key key = ( compound key ) generate key ( ) ; finder . assoc key ( part key @$ key . get part ( part key ) ) ; },key assoc
the user d<PLACE_HOLDER> not prov<PLACE_HOLDER>e a uri resolver @$ or it d<PLACE_HOLDER> not return a source for the included stylesheet module @$ or the source has no system <PLACE_HOLDER> set @$ so we fall back to using the system <PLACE_HOLDER> resolver to take the href and base to generate the base uri of the included stylesheet .,baseuri = systemid resolver . get absoluteuri ( get href ( ) @$ handler . get base identifier ( ) ) ;,source has
not a list @$ but not a primtive @$ return the nested group <PLACE_HOLDER>,return g . get group ( field index @$ __num__ ) ;,list return
version 2 layout draws outer <PLACE_HOLDER> bigger than inner,if ( controller . get version ( ) == time picker dialog . version . version_1 ) { m text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_outer ) ) ; m inner text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_inner ) ) ; } else { m text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_outer_v2 ) ) ; m inner text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_inner_v2 ) ) ; } m inner text grid heights = new float [ __num__ ] ;,version draws
because new plan has an evergreen phase <PLACE_HOLDER> we end up directly on that phase <PLACE_HOLDER>,final timed phase current phase = plan aligner . get current timed phase on change ( default subscription base @$ new plan @$ effective change date @$ null @$ catalog @$ internal call context ) ; assert . assert equals ( current phase . get start phase ( ) @$ align start date ) ; assert . assert equals ( current phase . get phase ( ) . get phase type ( ) @$ phase type . evergreen ) ;,plan has
size of metadata has increased @$ the most complex <PLACE_HOLDER> @$ more atoms affected,int additional space required for metadata = new ilst size - old ilst size ;,size increased
o auth encodes some <PLACE_HOLDER> differently :,return url decoder . decode ( url @$ __str__ ) . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) ;,auth encodes
the service that detects lost standby master <PLACE_HOLDER>,get executor service ( ) . submit ( new heartbeat thread ( heartbeat context . master_lost_master_detection @$ new lost master detection heartbeat executor ( ) @$ ( int ) server configuration . get ms ( property key . master_standby_heartbeat_interval ) @$ server configuration . global ( ) @$ m master context . get user state ( ) ) ) ; get executor service ( ) . submit ( new heartbeat thread ( heartbeat context . master_log_config_report_scheduling @$ new log config report heartbeat executor ( ) @$ ( int ) server configuration . get ms ( property key . master_log_config_report_heartbeat_interval ) @$ server configuration . global ( ) @$ m master context . get user state ( ) ) ) ; if ( server configuration . get boolean (,detects lost
size of buffer that is deep in buffer stack you can only find the <PLACE_HOLDER> of the buffer that is on top,if ( current stack size != m stack . stack size ) { throw new runtime exception ( __str__ ) ; } return m position - c_pos ;,size find
management server sends back another rds <PLACE_HOLDER> containing the route configuration for the requested resource .,route configs = immutable list . of ( any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) @$ build virtual host ( immutable list . of ( __str__ @$ __str__ ) @$ __str__ ) ) ) ) ) ; response = build discovery response ( __str__ @$ route configs @$ xds client impl . ads_type_url_rds @$ __str__ ) ; response observer . on next ( response ) ;,server sends
this is dangerous : new instance can throw checked <PLACE_HOLDER> . this is dangerous : config is mutable .,if ( ! runnable before . is interface ( ) ) { runnable before . new instance ( ) . run ( get configuration ( ) ) ; },instance throw
invalidate throws entry not found exception then repopulate <PLACE_HOLDER>,try ( ignored exception ignored1 = add ignored exception ( entry not found exception . class . get name ( ) ) ; ignored exception ignored2 = add ignored exception ( reply exception . class . get name ( ) ) ) { for ( int i = invalidate range_2 start ; i <= invalidate range_2 end ; i ++ ) { string key = integer . to string ( i ) ; string value = integer . to string ( i ) ; assert that thrown by ( ( ) -> region . invalidate ( key ) ) . is instance of ( entry not found exception . class ) ; region . create ( key @$ value ) ; } },exception repopulate
properties javadoc discourages <PLACE_HOLDER> of put so we <PLACE_HOLDER> set property,if ( name instanceof string && value instanceof string ) { props . set property ( ( string ) name @$ ( string ) value ) ; },javadoc discourages
always use the smallest radius to make sure the rounded corners will completely cover the <PLACE_HOLDER> .,return math . min ( top radius @$ bottom radius ) ;,corners cover
the two methods follow different code paths to determine <PLACE_HOLDER> the build target for the result should be and we want to test both of them .,try ( fixture fixture = create synchronous execution fixture ( __str__ ) ) { cell cell = fixture . get cell ( ) ; path root build file path = cell . get filesystem ( ) . resolve ( __str__ ) ; path a build file path = cell . get filesystem ( ) . resolve ( __str__ ) ; fixture . get target node parse pipeline ( ) . get all requested target nodes ( cell @$ root build file path @$ optional . empty ( ) ) ; optional < build file manifest > root raw nodes = fixture . get raw node parse pipeline cache ( ) . lookup computed node ( cell @$ root build file path @$ event bus ) ; fixture .,the build
unlike list popup window @$ popup menu does n't have an api to check whether it is showing . use a custom <PLACE_HOLDER> to check the visibility of the drop down list view instead .,on view ( with class name ( matchers . is ( drop_down_class_name ) ) ) . in root ( is platform popup ( ) ) . check ( matches ( is displayed ( ) ) ) ;,api use
expected exception as the function is shutting down the target <PLACE_HOLDER> and the result collector will get member departed exception,return __str__ ;,function shutting
transition to fatal error since we have unresolved <PLACE_HOLDER> .,sender . run once ( ) ;,transition have
asserts object <PLACE_HOLDER> on the object graph .,assert true ( copya . get object ( ) == copyc . get object ( ) ) ; assert true ( copya . get objects ( ) . get ( __num__ ) == copyc . get objects ( ) . get ( __num__ ) ) ; assert true ( copya == copyc . get objects ( ) . get ( __num__ ) ) ; assert true ( copyc == copya . get objects ( ) . get ( __num__ ) ) ;,asserts object
we do a remote call to have an io exception if the connection is broken . see the <PLACE_HOLDER> 4939578,return connection . get connection id ( ) ;,4939578 see
the list of relational values should contain 2 or more values : the first represents the <PLACE_HOLDER> the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ jaxb any mapping . get name ( ) ) @$ origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source type source = new hibernate type source impl ( jaxb any mapping . get meta type ( ) ) ; private final relational value source relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > value mappings = new hash map < string @$ string > ( ) ; { for ( jaxb hbm any value,first represents
creation of an isa will force a <PLACE_HOLDER> .,inet socket address initial isa = new inet socket address ( hostname @$ port ) ; if ( initial isa . get address ( ) == null ) { throw new illegal argument exception ( __str__ + initial isa ) ; } final list < blocking service and interface > sai = new array list < > ( __num__ ) ;,creation force
the keys of probe and build sides are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer join <PLACE_HOLDER> well in this case .,final int probe_vals_per_key = __num__ ;,side join
javac does not allow final or static in interface methods order annotation <PLACE_HOLDER> hence no need to check that this is not a method or annotation field,while ( modifier != null ) { final int type = modifier . get type ( ) ; if ( type == token types . literal_public || type == token types . literal_static && ast . get type ( ) != token types . method_def || type == token types . abstract && ast . get type ( ) != token types . class_def || type == token types . final && ast . get type ( ) != token types . class_def ) { log ( modifier @$ msg_key @$ modifier . get text ( ) ) ; break ; } modifier = modifier . get next sibling ( ) ; },javac allow
set capacity will change <PLACE_HOLDER> of underlying byte array,buf . set capacity ( start size ) ; assert equals ( start size @$ buf . capacity ( ) ) ; check buffer ( buf @$ empty ) ;,capacity change
find out @$ which output number the <PLACE_HOLDER> to the parent,int num ; for ( num = __num__ ; num < this . outgoing connections . size ( ) ; num ++ ) { if ( this . outgoing connections . get ( num ) == to parent ) { break ; } } if ( num >= this . outgoing connections . size ( ) ) { throw new compiler exception ( __str__ + __str__ ) ; },which output
the spy throws an io <PLACE_HOLDER> when writing to the second directory,do answer ( new faulty save image ( false ) ) . when ( spy image ) . savefs image ( any ( ) @$ any ( ) @$ any ( ) ) ; should fail = false ; break ; case save_all_fsimages :,spy throws
fetch the cr ls via ldap . ldap cert store has its own caching <PLACE_HOLDER> @$ see the class description for more info . safe cast since xsel is an x 509 certificate selector .,try { return ( collection < x509crl > ) ldap cert store . getcr ls ( xsel ) ; } catch ( cert store exception cse ) { throw new pkix . cert store type exception ( __str__ @$ cse ) ; },store has
buck modules do not support <PLACE_HOLDER>,return ( __ @$ ___ ) -> true ;,modules support
server sent <PLACE_HOLDER>,assert equals ( ( long ) expected settings . get max header list size ( ) @$ new settings . max header list size ( ) ) ; assert equals ( ( integer ) expected settings . get max frame size ( ) @$ new settings . max frame size ( ) ) ; assert equals ( ( integer ) expected settings . get initial window size ( ) @$ new settings . initial window size ( ) ) ; assert equals ( ( long ) expected settings . get max concurrent streams ( ) @$ new settings . max concurrent streams ( ) ) ; assert equals ( null @$ new settings . header table size ( ) ) ; complete ( ) ; break ; default,server sent
in the second rebalance the new member gets its assignment and this member has no <PLACE_HOLDER> or revocations,expect rebalance ( __num__ @$ collections . empty list ( ) @$ collections . empty list ( ) ) ; member . poll ( easy mock . any int ( ) ) ; power mock . expect last call ( ) ; power mock . replay all ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ double . positive_infinity ) ; herder . tick ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; herder . tick ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; power mock . verify all ( ),assignment has
we use a runnable to make sure this work . because if the list view is handling <PLACE_HOLDER> @$ this might not work .,m view files . post ( new runnable ( ) { @ override public void run ( ) { if ( should be selected idx >= __num__ && should be selected idx < m file adapter . get count ( ) ) { m view files . set selection ( should be selected idx ) ; } else if ( ! m file adapter . is empty ( ) ) m view files . set selection ( __num__ ) ; } } ) ;,view handling
the first two tasks have the same <PLACE_HOLDER>,final task task = noop task . create ( math . min ( __num__ @$ ( i - __num__ ) * __num__ ) ) ;,tasks have
we should show the promo view only when the panel has reached the exact expanded <PLACE_HOLDER> .,if ( percentage == __num__ ) { show promo view ( ) ; } else { hide promo view ( ) ; },panel reached
no fallback should take more than 2 <PLACE_HOLDER> .,if ( new style2 != null ) { if ( fallback cache [ new style2 . ordinal ( ) ] != null ) { throw new illegal state exception ( __str__ ) ; } },fallback take
strip auth <PLACE_HOLDER> from result .,result . remove ( account manager . key_authtoken ) ; if ( log . is loggable ( tag @$ log . verbose ) ) { log . v ( tag @$ get class ( ) . get simple name ( ) + __str__ + response ) ; },strip auth
only the first request ever generated should contain an install <PLACE_HOLDER> .,if ( succeeded && sending install request ) { m send install event = false ; register new request ( current timestamp ) ; generate and post request ( current timestamp @$ sessionid ) ; },request contain
lazily initialized <PLACE_HOLDER> of the notification .,status bar notification sbn = r . sbn ; status bar notification old sbn = ( old != null ) ? old . sbn : null ; trim cache trim cache = new trim cache ( sbn ) ; for ( final managed service info info : get services ( ) ) { boolean sbn visible = is visible to listener ( sbn @$ info ) ; boolean old sbn visible = old sbn != null ? is visible to listener ( old sbn @$ info ) : false ; if ( ! old sbn visible && ! sbn visible ) { continue ; } if ( r . is hidden ( ) && info . target sdk version < build . version_codes . p ) { continue,lazily initialized
wait for a full second to let the inner view drag helper complete the <PLACE_HOLDER>,ui controller . loop main thread for at least ( __num__ ) ;,helper complete
configure the atomic <PLACE_HOLDER> directories key so every folder will have atomic <PLACE_HOLDER> applied .,conf . set ( azure native file system store . key_atomic_rename_directories @$ __str__ ) ; azure blob storage test account test account = azure blob storage test account . create ( conf ) ; assume not null ( test account ) ; test statistics with account ( test account ) ;,folder have
otherwise we can get into an infinite loop because the object type hash code method calls this <PLACE_HOLDER> .,return objects . hash code ( properties . key set ( ) ) ;,method calls
filter projects only to groups which match project 's <PLACE_HOLDER>,set < group > copy = group . matching ( project @$ groups ) ;,which match
check mapping a different element returns the expected <PLACE_HOLDER> .,bounded window input window2 = new interval window ( instant . now ( ) @$ duration . standard minutes ( __num__ ) ) ; assert equals ( global window . instance @$ window mapping fn . get side input window ( input window2 ) ) ; assert equals ( input window2 @$ ( ( kv ) test sdk harness . get input values ( ) . get ( __num__ ) . get value ( ) ) . get value ( ) ) ;,check returns
update <PLACE_HOLDER> visibility only triggers when a scroll is completed . so a user might click the <PLACE_HOLDER> when the animation is still ongoing potentially pushing the target position outside of the bounds of the day picker view,if ( position >= __num__ && position < day picker view . get count ( ) ) { day picker view . smooth scroll to position ( position ) ; update button visibility ( position ) ; },user click
since value may exceed integer <PLACE_HOLDER> @$ use stock parser which checks for this .,if ( length >= __num__ ) { value = integer . parse int ( text . sub sequence ( position @$ position += length ) . to string ( ) ) ; } else { int i = position ; if ( negative ) { i ++ ; } try { value = text . char at ( i ++ ) - __str__ ; } catch ( string index out of bounds exception e ) { return ~ position ; } position += length ; while ( i < position ) { value = ( ( value << __num__ ) + ( value << __num__ ) ) + text . char at ( i ++ ) - __str__ ; } if ( negative ) { value = -,value exceed
if system time has gone backwards increase <PLACE_HOLDER> by 1 ms to maintain uniqueness,last time = ( now < last time ) ? last time + __num__ : now ; last count = short . min_value ; done = true ;,time gone
query the data using the <PLACE_HOLDER>,query data ( ) ;,data using
test show create <PLACE_HOLDER>,string expected sql = format sql text ( format ( __str__ @$ get session ( ) . get catalog ( ) . get ( ) @$ get session ( ) . get schema ( ) . get ( ) @$ __str__ @$ query ) ) . trim ( ) ; actual = compute actual ( __str__ ) ; assert equals ( get only element ( actual . get only column as set ( ) ) @$ expected sql ) ; actual = compute actual ( format ( __str__ @$ get session ( ) . get catalog ( ) . get ( ) @$ get session ( ) . get schema ( ) . get ( ) ) ) ; assert equals ( get only element ( actual .,show create
let children has one chance to catch the touch @$ and request the swipe not intercept useful when swipe <PLACE_HOLDER> wrap a swipe <PLACE_HOLDER> or other gestural <PLACE_HOLDER>,if ( ! before check && m is being dragged ) { return false ; },layout wrap
pattern order is used downstream so that we can know what order the text is in instead of encoding it in the string @$ which would require more <PLACE_HOLDER> later to remove it pre feature selection .,map < string @$ integer > pattern order = new hash map < > ( ) ; int order = __num__ ; if ( has option ( from_option [ __num__ ] ) ) { patterns . add ( mail processor . from_prefix ) ; pattern order . put ( mail options . from @$ order ++ ) ; } if ( has option ( to_option [ __num__ ] ) ) { patterns . add ( mail processor . to_prefix ) ; pattern order . put ( mail options . to @$ order ++ ) ; } if ( has option ( references_option [ __num__ ] ) ) { patterns . add ( mail processor . refs_prefix ) ; pattern order . put ( mail options . refs @$,which require
total jobs number is threads num <PLACE_HOLDER>,assert equals ( __str__ @$ threads num * jobs per task @$ stolen . get ( ) + none stolen . get ( ) ) ; assert false ( __str__ @$ stolen . get ( ) == __num__ ) ; for ( ignite g : g . all grids ( ) ) assert true ( __str__ @$ nodes . contains ( g . name ( ) ) ) ; assert true ( __str__ + stolen + __str__ + none stolen + __str__ @$ math . abs ( stolen . get ( ) - __num__ * none stolen . get ( ) ) <= __num__ ) ;,jobs threads
request a list of upload ur ls for the dropped <PLACE_HOLDER>,if ( ! file params . is empty ( ) ) { get rpc proxy ( file drop target rpc . class ) . drop ( file params ) ; } event . prevent default ( ) ; event . stop propagation ( ) ;,the dropped
wrong ur ls missing <PLACE_HOLDER> @$ however accepted by digest url and multi protocol url constructors,test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; digesturl java url = new digesturl ( java url str ) ; string java hash result = ascii . string ( java url . hash ( ) ) ;,ur ls
locked cleanup may generate <PLACE_HOLDER> we can send unlocked,if ( ! is held by current thread ( ) ) { map . process pending notifications ( ) ; },cleanup generate
in theory @$ the subclasses of stream source may implement the bounded one input <PLACE_HOLDER> @$ so we still need the following call to end the input,if ( ! is canceled or stopped ( ) ) { synchronized ( locking object ) { operator chain . end head operator input ( __num__ ) ; } },subclasses implement
log any changes to what is currently driving the brightness <PLACE_HOLDER> .,if ( ! m brightness reason temp . equals ( m brightness reason ) || brightness adjustment flags != __num__ ) { slog . v ( tag @$ __str__ + brightness + __str__ + m brightness reason temp . to string ( brightness adjustment flags ) + __str__ + m brightness reason + __str__ ) ; m brightness reason . set ( m brightness reason temp ) ; },what driving
the entry was not in the cache @$ make a new one . get the data <PLACE_HOLDER> .,snmp table handler handler = get handler ( m ) ;,one get
presto only uses get reflection object inspector here @$ in a test method . therefore @$ we choose to work around this issue by synchronizing this method . before synchronizing this method @$ test in this class fails approximately <PLACE_HOLDER> out of <PLACE_HOLDER>0 runs on travis .,return get reflection object inspector ( type @$ object inspector options . java ) ;,test fails
if the thread context classloader is set @$ then try its <PLACE_HOLDER> to find a matching context,class loader tccl = thread . current thread ( ) . get context class loader ( ) ; loader = tccl ; if ( loader != null ) { if ( log . is debug enabled ( ) ) log . debug ( __str__ ) ; synchronized ( __context map ) { while ( ctx == null && loader != null ) { ctx = get context for class loader ( loader ) ; if ( ctx == null && loader != null ) loader = loader . get parent ( ) ; } if ( ctx == null ) { ctx = new naming context ( obj @$ tccl @$ env @$ name @$ name ctx ) ; __context map . put ( tccl @$ ctx ),then try
each time the version <PLACE_HOLDER> as we walk the list @$ that counts as a successful operation,long version = - __num__ ; for ( int i = __num__ ; i < number of operations ; i ++ ) { if ( operations . get ( i ) . version ( ) >= version ) { count ++ ; version = operations . get ( i ) . version ( ) ; } },each time
each transform also gets its own <PLACE_HOLDER>,list < string > expected groups = arrays . as list ( connector config . common_group @$ connector config . transforms_group @$ connector config . error_group @$ __str__ @$ __str__ ) ; assert equals ( expected groups @$ result . groups ( ) ) ; assert equals ( __num__ @$ result . error count ( ) ) ;,transform gets
create some destroyed <PLACE_HOLDER> so the gc service is populated,serializable callable create = new serializable callable ( __str__ ) { @ override public object call ( ) { region factory f = get cache ( ) . create region factory ( get region attributes ( ) ) ; cc region = ( local region ) f . create ( name ) ; return cc region . get distribution manager ( ) . get distribution manager id ( ) ; } } ;,some destroyed
now based on that floating point percentage compute the real scroller <PLACE_HOLDER> .,final int visible amt = f scroll bar . get visible amount ( ) ; final int max = f scroll bar . get maximum ( ) ; final int min = f scroll bar . get minimum ( ) ; final int extent = max - min ;,now compute
check if the line has at least one <PLACE_HOLDER> or digit,for ( string line : lines ) { if ( character_digits_pattern . matcher ( line ) . matches ( ) ) { stderr . append ( line ) ; } },line has
dep 1 should have only dep 1 abi <PLACE_HOLDER>,dex produced from java library java dep1 dex rule = ( dex produced from java library ) graph builder . get rule ( build target factory . new instance ( __str__ ) ) ; assert not null ( java dep1 dex rule ) ; assert that ( java dep1 dex rule . get desugar deps ( ) @$ has size ( __num__ ) ) ; assert that ( java dep1 dex rule . get desugar deps ( ) @$ has item ( java dep2 abi . get source path to output ( ) ) ) ; assert that ( java dep1 dex rule . get build deps ( ) @$ all of ( has item ( java dep2 abi ) @$ not ( has item ( java dep1,dep have
make the client use client cache <PLACE_HOLDER> so it will have a default pool,this . durable clientvm . invoke ( ( ) -> cache server test util . create client cache ( get client pool ( get server host name ( ) @$ server1 port @$ true ) @$ region name @$ get client distributed system properties ( durable client id ) ) ) ;,client use
and will not trigger fallback <PLACE_HOLDER>,verify ( failover service @$ never ( ) ) . get def user ( ) ;,and trigger
error while invoking found <PLACE_HOLDER> @$ use empty <PLACE_HOLDER>,result = ( i persistable ) the class . new instance ( ) ; result . deserialize ( in ) ;,invoking found
finishing task 2 should complete the <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( plan item instances . get ( __num__ ) . get id ( ) ) ; plan item instances = cmmn runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state ( plan item instance state . active ) . order by name ( ) . asc ( ) . list ( ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; expected names = new string [ ] { __str__ } ; for ( int i = __num__ ; i < plan item instances . size ( ) ; i ++ ) { assert equals,task complete
validate inode tree <PLACE_HOLDER> match given <PLACE_HOLDER> .,for ( mutable inode < ? > node : journaled ) { assert true ( tree entries . contains ( node . to journal entry ( ) ) ) ; },entries match
fastpath : do not construct a new <PLACE_HOLDER> if the src is a <PLACE_HOLDER> and is already normalized .,if ( src instanceof string ) { int span length = span quick check yes ( src ) ; if ( span length == src . length ( ) ) { return ( string ) src ; } string builder sb = new string builder ( src . length ( ) ) . append ( src @$ __num__ @$ span length ) ; return normalize second and append ( sb @$ src . sub sequence ( span length @$ src . length ( ) ) ) . to string ( ) ; },fastpath construct
the original dsl test returns a double while the exec model returns an <PLACE_HOLDER>,assert equals ( ( ( number ) results . iterator ( ) . next ( ) . get value ( ) ) . int value ( ) @$ __num__ ) ;,model returns
second combine should replace <PLACE_HOLDER> ; should read existing data @$ and write final data to disk .,current time += second_in_millis ; reader . reset ( ) ; rotate . combine active ( reader @$ writer ( __str__ ) @$ current time ) ; reader . assert read ( __str__ ) ; assert read all ( rotate @$ __str__ ) ;,combine replace
do bind implements all four <PLACE_HOLDER> of binding,naming context data store impl = ( naming context data store ) this ; do bind ( impl @$ n @$ nc @$ true @$ binding type . ncontext ) ;,bind implements
one <PLACE_HOLDER> reader closes @$ the other still receives the <PLACE_HOLDER>,output consumer binding . dispose ( ) ; consumed out . reset ( ) ; consumed out2 . reset ( ) ; context . eval ( source ) ; engine output = engine output + full output ; assert equals ( engine output @$ to unix string ( engine out ) ) ; assert equals ( __num__ @$ consumed out . size ( ) ) ; assert true ( consumed out2 . size ( ) > __num__ ) ; from out reader2 = new buffered reader ( new input stream reader ( new byte array input stream ( consumed out2 . to byte array ( ) ) ) ) ; assert equals ( full lines @$ read lines list ( from out reader2 ) ) ;,other receives
if right becomes root parent means rotation happened at lower level . so just return right so that nodes at upper level can set their <PLACE_HOLDER> correctly,if ( right == root . parent ) { return right ; },nodes set
if child is not updated yet call <PLACE_HOLDER>,if ( child != null && child . get visibility ( ) != view . gone ) { if ( child . get measured height ( ) == __num__ || child . get measured width ( ) == __num__ ) child . measure ( measure spec . make measure spec ( width @$ measure spec . at_most ) @$ measure spec . make measure spec ( height @$ measure spec . at_most ) ) ; layout params lp = ( layout params ) child . get layout params ( ) ; final int child width = child . get measured width ( ) ; final int child height = child . get measured height ( ) ; if ( child top + child height + lp . top,child updated
for now the call bellow will return type <PLACE_HOLDER>,type = compiler . resolve type ref ( model @$ bkm @$ bkm @$ null ) ;,bellow return
table row specified <PLACE_HOLDER> @$ match anything character class .,if ( table el . f char class == __num__ ) { break ; },row specified
very simple format that shows millisecond <PLACE_HOLDER>,try { return long . to string ( d . get time ( ) ) ; } catch ( exception ignore ) { return __str__ ; },shows millisecond
the one join <PLACE_HOLDER> for this specialized class .,long column vector join col vector = ( long column vector ) batch . cols [ single join column ] ; long [ ] vector = join col vector . vector ;,one join
ensure the work handler has processed the <PLACE_HOLDER> inside the cache listener of wifi tracker,wait for handlers to process currently enqueued messages ( tracker ) ;,handler processed
0 x 100231 d : p 1 and p 2 have same note <PLACE_HOLDER> .,program builder1 . create bookmark ( __str__ @$ bookmark type . note @$ __str__ @$ __str__ ) ; program builder2 . create bookmark ( __str__ @$ bookmark type . note @$ __str__ @$ __str__ ) ;,1 have
by default @$ parse validates the <PLACE_HOLDER> @$ which is what we want .,schema schema = new schema . parser ( ) . parse ( metadata . get schema string ( ) ) ; assert equals ( __num__ @$ schema . get fields ( ) . size ( ) ) ;,parse validates
a media period may report a discontinuity at the current playback position to ensure the renderers are flushed . only report the discontinuity <PLACE_HOLDER> if the position changed .,if ( discontinuity position us != playback info . position us ) { playback info = copy with new position ( playback info . period id @$ discontinuity position us @$ playback info . content position us ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( playing period holder != queue . get reading period ( ) ) ;,period report
camera device should call configure <PLACE_HOLDER> and have it finish before constructing us,if ( configure success ) { m state callback . on configured ( this ) ; if ( debug ) log . v ( tag @$ m id string + __str__ ) ; m configure success = true ; } else { m state callback . on configure failed ( this ) ; m closed = true ; log . e ( tag @$ m id string + __str__ ) ; m configure success = false ; },device call
only the stream which was just added will change <PLACE_HOLDER> . so we only need an array of size 1 .,list < parent changed event > events = new array list < parent changed event > ( __num__ ) ; connection state . take child ( new parent @$ false @$ events ) ; notify parent changed ( events ) ;,stream change
does the list already contain that <PLACE_HOLDER> ?,return ! cached device list . contains ( id ) ;,list contain
optimization : the special case where we 're a <PLACE_HOLDER> of size 1 @$ and the other factor is receiving the <PLACE_HOLDER> @$ and of size 2,if ( domain . size ( ) == __num__ && ( result domain . size ( ) == other domain . size ( ) ) && result domain . size ( ) == __num__ ) { return other . multiply ( this ) ; } else { int [ ] mapping = new int [ result . neighbor indices . length ] ; int [ ] other mapping = new int [ result . neighbor indices . length ] ; for ( int i = __num__ ; i < result . neighbor indices . length ; i ++ ) { mapping [ i ] = domain . index of ( result . neighbor indices [ i ] ) ; other mapping [ i ] = other domain .,factor receiving
should use provided <PLACE_HOLDER> @$ with original bit array size,bit array array3 = new bit array ( __num__ ) ; array3 = matrix . get row ( __num__ @$ array3 ) ; assert equals ( __num__ @$ array3 . get size ( ) ) ; for ( int x = __num__ ; x < __num__ ; x ++ ) { boolean on = ( x & __num__ ) == __num__ ; assert equals ( on @$ array . get ( x ) ) ; assert equals ( on @$ array2 . get ( x ) ) ; assert equals ( on @$ array3 . get ( x ) ) ; },use provided
all symbols must have declaration <PLACE_HOLDER> .,check not null ( decl node ) ; return declare symbol ( sym . get name ( ) @$ get type ( sym ) @$ is type inferred ( sym ) @$ scope @$ decl node @$ sym . getjs doc info ( ) ) ;,symbols have
lines before functions should take <PLACE_HOLDER>,function f = program . get function manager ( ) . get function at ( get addr ( __num__ ) ) ; code unit cu = listing . get code unit at ( f . get entry point ( ) ) ; assert true ( cb . go to field ( cu . get min address ( ) @$ plate field factory . field_name @$ __num__ @$ __num__ ) ) ; listing text field tf = ( listing text field ) cb . get current field ( ) ; assert equals ( __num__ @$ tf . get num rows ( ) ) ;,lines take
assert that the log contains the correct <PLACE_HOLDER> .,html page log = j . create web client ( ) . get page ( proj @$ __str__ ) ; string logastext = log . as text ( ) ; assert true ( logastext . contains ( __str__ + abstract project . workspace offline reason . all_suitable_nodes_are_offline . name ( ) + __str__ ) ) ;,log contains
since the link is relative and the holder is not even being documented @$ this must be an inherited link . redirect it . the current class either overrides the referenced <PLACE_HOLDER> or inherits it automatically .,if ( see . text ( ) . trim ( ) . starts with ( __str__ ) && ! ( containing . is public ( ) || util . is linkable ( containing @$ configuration ) ) ) { if ( this instanceof class writer impl ) { containing = ( ( class writer impl ) this ) . get class doc ( ) ; } else if ( ! containing . is public ( ) ) { configuration . get doclet specific msg ( ) . warning ( see . position ( ) @$ __str__ @$ tag name @$ containing . qualified name ( ) ) ; } else { configuration . get doclet specific msg ( ) . warning ( see . position ( ) @$,class overrides
if no values in this block were set @$ we can just set its <PLACE_HOLDER> to be the same as some other block with no values set @$ assuming we 've seen one yet .,if ( ! block touched [ plane ] [ i ] && i untouched != - __num__ ) { indices [ plane ] [ i ] = i untouched ; } else { int j block start = limit compacted * blockcount ; if ( i > limit compacted ) { system . arraycopy ( values [ plane ] @$ i block start @$ values [ plane ] @$ j block start @$ blockcount ) ; } if ( ! block touched [ plane ] [ i ] ) { i untouched = ( short ) j block start ; } indices [ plane ] [ i ] = ( short ) j block start ; limit compacted ++ ; },block set
after tx state proxy committed @$ get <PLACE_HOLDER> will get the <PLACE_HOLDER> for the oldtx but caller should not perform ops on this tx state proxy,assert true ( tx mgr . get lock ( tx @$ txid ) ) ;,lock get
if this list does not contain our <PLACE_HOLDER> name @$ then its not referencing our <PLACE_HOLDER> this is a cheezy test ... but it errs on the side of less false hits .,if ( meth . get reference names ( ) . is empty ( ) && ! meth . is super ( ) ) { list < string > pack class = meth . get qualifier names ( ) ; if ( ! pack class . is empty ( ) ) { for ( string name : pack class ) { if ( name . equals ( class name ) ) { found = true ; break ; } } } else { found = true ; } },its referencing
all other operands require a <PLACE_HOLDER>,return register priority . must have register ;,operands require
the user allowed the <PLACE_HOLDER> to toggle touch exploration .,m enable touch exploration dialog = new alert dialog . builder ( m context ) . set icon attribute ( android . r . attr . alert dialog icon ) . set positive button ( android . r . string . ok @$ new on click listener ( ) { @ override public void on click ( dialog interface dialog @$ int which ) { user state . m touch exploration granted services . add ( service . m component name ) ; persist component names to setting locked ( settings . secure . touch_exploration_granted_accessibility_services @$ user state . m touch exploration granted services @$ user state . m user id ) ; user state . m is touch exploration enabled = true ; final long identity =,user allowed
local job tracker allows the only <PLACE_HOLDER> per wave @$ but text input format replaces it with the calculated value based on input split size option .,if ( __str__ . equals ( cfg . get ( __str__ @$ __str__ ) ) ) { file input format . set min input split size ( job cfg @$ __num__ * __num__ * __num__ ) ; file input format . set max input split size ( job cfg @$ long . max_value ) ; },tracker allows
if maps @$ recursively compare the key and value <PLACE_HOLDER>,if ( c1 . equals ( category . map ) ) { map object inspector mapoi1 = ( map object inspector ) o1 ; map object inspector mapoi2 = ( map object inspector ) o2 ; object inspector child key1 = mapoi1 . get map key object inspector ( ) ; object inspector child key2 = mapoi2 . get map key object inspector ( ) ; if ( compare types ( child key1 @$ child key2 ) ) { object inspector child val1 = mapoi1 . get map value object inspector ( ) ; object inspector child val2 = mapoi2 . get map value object inspector ( ) ; if ( compare types ( child val1 @$ child val2 ) ) { return true ; } } return,maps compare
json does n't support null <PLACE_HOLDER> @$ so just write the null key value,if ( input == null ) { map . put ( null key value @$ null ) ; } else { map . put ( input @$ input ) ; },json support
do not animate the clock when waking up from a pulse . the height callback will take <PLACE_HOLDER> of pushing the clock to the right position .,if ( ! m pulsing && ! m dozing ) { m animate next position update = false ; } m notification stack scroller . set pulsing ( pulsing @$ animate pulse ) ; m keyguard status view . set pulsing ( pulsing ) ;,callback take
list of need fetch next contains all tables that we have joined <PLACE_HOLDER> in their candidate storage @$ and we need to clear candidate storage and promote their next group storage to candidate storage and fetch <PLACE_HOLDER> until we reach a new group .,if ( ( list of need fetch next . size ( ) > __num__ ) && clear ) { for ( byte b : list of need fetch next ) { try { fetch next group ( b ) ; } catch ( exception e ) { throw new hive exception ( e ) ; } } },list fetch
this method handles row <PLACE_HOLDER> after the first in a multivalued row @$ so just return false,return false ;,method handles
we inserted a keyframe let 's shift the <PLACE_HOLDER> .,j ++ ;,"s" shift
test that generated policy file has included both <PLACE_HOLDER>,assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( class loader perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ; assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( socket perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ;,file included
before we set the value we must flush all pending changes from the entitymanager if we do n't do this @$ in some cases the primary key will not yet be set in the object which will cause <PLACE_HOLDER> down the road .,entity manager session . flush ( ) ;,which cause
metered change should update <PLACE_HOLDER>,m cell network agent . add capability ( network capabilities . net_capability_not_metered ) ; wait for idle ( ) ; verify ( m stats service @$ at least once ( ) ) . force update ifaces ( eq ( only cell ) @$ any ( network state [ ] . class ) @$ eq ( mobile_ifname ) @$ eq ( new vpn info [ __num__ ] ) ) ; reset ( m stats service ) ; m cell network agent . remove capability ( network capabilities . net_capability_not_metered ) ; wait for idle ( ) ; verify ( m stats service @$ at least once ( ) ) . force update ifaces ( eq ( only cell ) @$ any ( network state [ ] . class ),change update
create a new item list which contain a new adapter item object the id of the new item is changed @$ and will be treated as a new item according to the rule we set in the callback . this test case is to verify the get change payload <PLACE_HOLDER> still honor the standard we set up to judge new item,m items . clear ( ) ; adapter item new item = new adapter item ( __num__ @$ __str__ @$ __str__ ) ; m items . add ( new item ) ;,the honor
show a unified format which contains <PLACE_HOLDER> @$ host and port .,string src service name = src fs . get canonical service name ( ) ; string des service name = des fs . get canonical service name ( ) ; if ( src service name == null || des service name == null ) { return false ; } if ( src service name . equals ( des service name ) ) { return true ; } if ( src service name . starts with ( __str__ ) && des service name . starts with ( __str__ ) ) { collection < string > internal name services = conf . get trimmed string collection ( __str__ ) ; if ( ! internal name services . is empty ( ) ) { if ( internal name services . contains,which contains
make cookie secure when using <PLACE_HOLDER>,if ( use secure ) { root context . get session handler ( ) . get session cookie config ( ) . set secure ( use secure ) ; },cookie using
components commonly have conditional <PLACE_HOLDER>s assigned . using the click <PLACE_HOLDER> matcher we can assert whether or not a given component has a <PLACE_HOLDER> attached to them . noinspection unchecked,assert that ( c @$ component ) . extracting sub component at ( __num__ ) . has ( sub component with ( c @$ test footer component . matcher ( c ) . click handler ( is null . < event handler < click event > > null value ( null ) ) . build ( ) ) ) ;,component has
wsdl does n't have any schema <PLACE_HOLDER>,return new source [ __num__ ] ;,wsdl have
memory map does duplicate <PLACE_HOLDER>,return - __num__ ;,map duplicate
rack 1 has file 1 @$ file 2 and file <PLACE_HOLDER> and file 4 rack 2 has file 2 and file <PLACE_HOLDER> and file 4 rack <PLACE_HOLDER> has file <PLACE_HOLDER> and file 4 setup a filter so that only file 1 and file 2 can be combined,in format = new dummy input format ( ) ; file input format . add input path ( job @$ in dir ) ; in format . set min split size rack ( __num__ ) ;,3 file
jar files have only generated <PLACE_HOLDER> @$ not the source toc file,return null ;,files generated
skip bundles that have n't reached resolved <PLACE_HOLDER> ; skip fragments .,if ( b . get state ( ) <= bundle . resolved || b . get headers ( ) . get ( constants . fragment_host ) != null ) continue ; try { cls = b . load class ( name ) ; break ; } catch ( class not found exception ignored ) { },bundles resolved
buffer should contain at least one <PLACE_HOLDER>,if ( height > __num__ || miny - sample model translatey > __num__ ) { if ( scanline stride > data . length ) { throw new raster format exception ( __str__ + scanline stride ) ; } },buffer contain
check if instrument uses general midi 2 default <PLACE_HOLDER> .,int bank = get patch ( ) . get bank ( ) ; if ( bank > > __num__ == __num__ || bank > > __num__ == __num__ ) { boolean [ ] ch = new boolean [ __num__ ] ; for ( int i = __num__ ; i < ch . length ; i ++ ) ch [ i ] = true ; return ch ; } boolean [ ] ch = new boolean [ __num__ ] ; for ( int i = __num__ ; i < ch . length ; i ++ ) ch [ i ] = true ; ch [ __num__ ] = false ; return ch ;,instrument uses
return new byte buffer every <PLACE_HOLDER> to prevent potential side effects,if ( clazz . equals ( byte buffer . class ) ) { byte buffer buff = byte buffer . allocate ( __num__ ) ; rand . next bytes ( buff . array ( ) ) ; return buff ; } else if ( type . equals ( placement constraint . class ) ) { placement constraint . abstract constraint s constraint expr = target in ( node @$ allocation tag ( __str__ ) ) ; ret = placement constraints . build ( s constraint expr ) ; },byte buffer
completing all the tasks ends the case <PLACE_HOLDER>,for ( task t : cmmn task service . create task query ( ) . case instance id ( case instance . get id ( ) ) . list ( ) ) { cmmn task service . complete ( t . get id ( ) ) ; } assert case instance ended ( case instance ) ;,tasks ends
check new server registers <PLACE_HOLDER> for reconnected client .,try ( ignite new srv = start grid ( server count ( ) + __num__ ) ) { await partition map exchange ( ) ; lsnr . latch = new count down latch ( __num__ ) ; ignite cache < object @$ object > new srv cache = new srv . cache ( default_cache_name ) ; for ( integer key : primary keys ( new srv cache @$ __num__ ) ) new srv cache . put ( key @$ key ) ; assert true ( lsnr . latch . await ( __num__ @$ milliseconds ) ) ; } cur . close ( ) ;,server registers
string buffer does not throw io <PLACE_HOLDER>,print to ( ( appendable ) buf @$ instant ) ;,buffer throw
check if list has some <PLACE_HOLDER> that can be rebalanced,while ( iterator . has next ( ) ) { if ( iterator . next ( ) . ds member list . size ( ) > __num__ ) { flag to continue with rebalance = true ; break ; } } if ( ! flag to continue with rebalance ) { rebalance result . set status message ( cli strings . rebalance__msg__no_rebalancing_regions_on_ds ) ; rebalance result . set success ( true ) ; return rebalance result ; } list < rebalance region result > rebalance region results = new array list < > ( ) ; for ( memberpr info memberpr : list member region ) { try { if ( memberpr . ds member list . size ( ) > __num__ ) { for ( int i,list has
protected mode should have deleted the <PLACE_HOLDER>,assert . assert equals ( children . size ( ) @$ __num__ @$ children . to string ( ) ) ;,mode deleted
prefer no default domain @$ as wild fly does not register an mbean <PLACE_HOLDER> with a default domain,if ( null != m bean server . get default domain ( ) ) { for ( int i = __num__ ; i < m bean server list . size ( ) ; i ++ ) { m bean server anm bean server = m bean server list . get ( i ) ; if ( null == anm bean server . get default domain ( ) ) { m bean server = anm bean server ; break ; } } },fly register
executors provide no <PLACE_HOLDER> @$ so make our own .,int queued = queue size . get and increment ( ) ; if ( max queue length > __num__ && queued >= max queue length ) { queue size . decrement and get ( ) ; return false ; } executor . execute ( new fifo call runner ( task ) { @ override public void run ( ) { task . set status ( rpc server . get status ( ) ) ; task . run ( ) ; queue size . decrement and get ( ) ; } } ) ; return true ;,executors provide
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,generic push profile = new crawl profile ( crawl profile . crawl_profile_push_stub + collection @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ null @$ - __num__ @$ true @$ true @$ false @$ false @$ true @$ true @$ false @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . nocache @$ collection @$ client identification . yacy intranet crawler agent name @$ null @$ null @$ __num__ ) ;,content match
does the program class already have a new <PLACE_HOLDER> ?,string new class name = new class name ( program class ) ; if ( new class name != null ) { class names to avoid . add ( mixed case class name ( new class name ) ) ; if ( repackage classes == null || ! allow access modification ) { string class name = program class . get name ( ) ; map package name ( class name @$ new class name @$ repackage classes == null && flatten package hierarchy == null ) ; } },class have
alice can see upstream @$ so <PLACE_HOLDER> gets built @$ but the upstream build can not see <PLACE_HOLDER> :,auth . grant ( item . read ) . on items ( upstream ) . to ( __str__ @$ __str__ ) ; map < string @$ authentication > qia config = new hash map < string @$ authentication > ( ) ; qia config . put ( upstream name @$ user . get ( __str__ ) . impersonate ( ) ) ; qia config . put ( downstream name @$ user . get ( __str__ ) . impersonate ( ) ) ; queue item authenticator configuration . get ( ) . get authenticators ( ) . replace ( new mock queue item authenticator ( qia config ) ) ; b = r . build and assert success ( upstream ) ; r . assert log not contains (,upstream see
run this dummy interpret <PLACE_HOLDER> first to launch the remote interpreter process to avoid the time overhead of launching the process .,interpreter1 . interpret ( __str__ @$ context1 ) ; thread thread1 = new thread ( ) { @ override public void run ( ) { try { assert equals ( code . success @$ interpreter1 . interpret ( __str__ @$ context1 ) . code ( ) ) ; } catch ( interpreter exception e ) { e . print stack trace ( ) ; fail ( ) ; } } } ; thread thread2 = new thread ( ) { @ override public void run ( ) { try { assert equals ( code . success @$ interpreter1 . interpret ( __str__ @$ context1 ) . code ( ) ) ; } catch ( interpreter exception e ) { e . print stack trace ( ) ; fail,dummy interpret
output <PLACE_HOLDER> is activity manager service dump activities <PLACE_HOLDER>,if ( __str__ . equals ( cmd ) || __str__ . equals ( cmd ) ) { m atm internal . write activities to proto ( proto ) ; } else if ( __str__ . equals ( cmd ) || __str__ . equals ( cmd ) ) { synchronized ( this ) { write broadcasts to proto locked ( proto ) ; } } else if ( __str__ . equals ( cmd ) ) { string [ ] new args ; string name ; if ( opti >= args . length ) { name = null ; new args = empty_string_array ; } else { name = args [ opti ] ; opti ++ ; new args = new string [ args . length - opti ] ;,service dump
imply <PLACE_HOLDER> if owner has <PLACE_HOLDER> set .,implicit |= sym . owner . flags_field & strictfp ; break ; default : throw new assertion error ( ) ;,owner has
server closes the health checking <PLACE_HOLDER> without any response,health impl . calls . poll ( ) . response observer . on error ( status . cancelled . as exception ( ) ) ;,server closes
if our constituents have post exported linker flags @$ our dependents should use <PLACE_HOLDER> .,for ( native linkable linkable : constituents . get linkables ( ) ) { args builder . add all ( linkable . get exported post linker flags ( graph builder ) ) ; } return native linkable input . of ( args builder . build ( ) @$ immutable list . of ( ) @$ immutable list . of ( ) ) ;,dependents use
cache ca n't handle null <PLACE_HOLDER>,if ( function == null ) { return false ; },cache handle
inject <PLACE_HOLDER> into current thread ; current thread will have <PLACE_HOLDER> @$ child will also have <PLACE_HOLDER>,threads [ i ] = make thread ( prefix + __str__ + i @$ true @$ true @$ true @$ true @$ __str__ ) ;,child have
update the screen to see changes take <PLACE_HOLDER>,if ( m map view != null ) { this . invalidate compass ( ) ; },changes take
response account features op package <PLACE_HOLDER>,m ams . has features ( m mock account manager response @$ account manager service test fixtures . account_success @$ null @$ __str__ ) ;,features op
nm 5 has no <PLACE_HOLDER> allocated containers .,for ( container moo container : allocated containers ) { assert . assert false ( moo container . get node id ( ) . equals ( nm5 . get node id ( ) ) ) ; },nm has
recompute system ui <PLACE_HOLDER> .,if ( ( m window attributes changes flag & window manager . layout params . translucent_flags_changed ) != __num__ ) { m attach info . m recompute global attributes = true ; },system ui
tccl ca n't see the <PLACE_HOLDER>,is context loader = false ;,tccl see
first @$ reduce to account asset balance . else ca n't complete this test <PLACE_HOLDER> .,account capsule to account = db manager . get account store ( ) . get ( byte array . from hex string ( to_address ) ) ; long id = db manager . get dynamic properties store ( ) . get token id num ( ) ; to account . reduce asset amountv2 ( byte string . copy from utf8 ( string . value of ( id ) ) . to byte array ( ) @$ total_supply - __num__ @$ db manager . get dynamic properties store ( ) @$ db manager . get asset issue store ( ) ) ; db manager . get account store ( ) . put ( to account . get address ( ) . to byte array ( ) @$ to account,first complete
for local queries returning pdx <PLACE_HOLDER> wrap the resultset with results collection pdx deserializer wrapper which deserializes these pdx <PLACE_HOLDER> .,if ( needspdx deserialization wrapper ( false ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ needs copy on read wrapper ) ; } else if ( ! is remote query ( ) && this . cache . get copy on read ( ) && result instanceof select results ) { if ( needs copy on read wrapper ) { result = new results collection copy on read wrapper ( ( select results ) result ) ; } },which deserializes
the count . use has <PLACE_HOLDER>s and is single <PLACE_HOLDER> instead .,throw new unsupported operation exception ( __str__ ) ;,count has
call set <PLACE_HOLDER> with empty bounds to remove the <PLACE_HOLDER> .,set cutout ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ;,call set
assert that the current value in store reflects all <PLACE_HOLDER> being processed,assert that ( new active store . get ( key ) @$ is ( equal to ( total num messages - __num__ ) ) ) ;,value reflects
all fail return <PLACE_HOLDER>,return tc . get random data node ( ) ;,all fail
in future could check current <PLACE_HOLDER> ... for now this should be enough :,return type deserializer . deserialize typed from array ( p @$ ctxt ) ;,future check
verify that scan encounters correct <PLACE_HOLDER>,scan scan = new scan ( ) ; try { result scanner scanner = ht . get scanner ( scan ) ; result res = null ; do { res = scanner . next ( ) ; } while ( res != null ) ; } catch ( table not enabled exception e ) { ok = true ; } assert true ( ok ) ; admin . enable table ( table ) ; assert true ( __str__ @$ test_util . geth base cluster ( ) . get master ( ) . get table state manager ( ) . is table state ( ht . get name ( ) @$ table state . state . enabled ) ) ; assert equals ( table state . state . enabled,scan encounters
the editing stopped event is how the client can use the enter <PLACE_HOLDER> to close the widget,assert equals ( __str__ + __str__ @$ listener . stopped count @$ __num__ ) ;,client use
master holds the <PLACE_HOLDER> @$ so restart the master .,if ( server . equals ( cluster status . get master name ( ) ) ) { restart master ( server @$ sleep time ) ; } else { restart rs ( server @$ sleep time ) ; },master holds
then the produced thread dump should contain that expected <PLACE_HOLDER> at least,assert true ( file contains ( threaddump file @$ __str__ @$ dumpable process . class . get name ( ) ) ) ;,dump contain
if string labeled <PLACE_HOLDER> @$ create aux vec,map < string @$ integer > map = m . get row label bindings ( ) ; if ( map != null ) { labels = make empty str vec ( frame . any vec ( ) ) ; vec . writer writer = labels . open ( ) ; map < integer @$ string > rmap = reverse map ( map ) ; for ( int r = __num__ ; r < m . row size ( ) ; r ++ ) { writer . set ( r @$ rmap . get ( r ) ) ; } writer . close ( closer ) ; },string labeled
call the real pre create table <PLACE_HOLDER>,mockito . do call real method ( ) . when ( storage handler ) . pre create table ( table ) ;,pre create
be sure to let our parent perform any <PLACE_HOLDER> needed,j label renderer = ( j label ) super . get table cell renderer component ( data ) ; object value = data . get value ( ) ; j table table = data . get table ( ) ; boolean is selected = data . is selected ( ) ; set text ( __str__ ) ; set horizontal alignment ( center ) ; vt match match = ( vt match ) value ; vt association association = match . get association ( ) ; vt association status association status = association . get status ( ) ; if ( ! is selected ) { renderer . set background ( match table renderer . get background color ( association @$ table @$ renderer . get background ( ),parent perform
the value specified in xml takes <PLACE_HOLDER> over the environment variable !,if ( config . get object timeout minutes ( ) > __num__ ) { object timeout = config . get object timeout minutes ( ) ; } else if ( ! utils . is empty ( system timeout ) ) { object timeout = const . to int ( system timeout @$ __num__ ) ; } else { object timeout = __num__ * __num__ ; },value takes
pulling this common case out of ` is timer enabled slow ` gives c 1 a better <PLACE_HOLDER> to inline this method .,if ( ! metrics enabled ) { return false ; },slow gives
saved persons will have same <PLACE_HOLDER>,person1 . field ( __str__ @$ __str__ ) . save ( ) ; person2 . field ( __str__ @$ __str__ ) . save ( ) ; person3 . field ( __str__ @$ __str__ ) . save ( ) ;,persons have
check that participant status listener is working <PLACE_HOLDER>,assert equals ( __str__ @$ room + __str__ @$ answer [ __num__ ] ) ; assert equals ( __str__ @$ room + __str__ @$ answer [ __num__ ] ) ;,listener working
default config should not change the <PLACE_HOLDER>,identity transformer identity transformer = get transformer with default identity config ( config ) ; identity transformer . transform acl entries for set request ( acl entries ) ; check acl entries list ( acl entries to be transformed @$ acl entries ) ; reset identity config ( config ) ;,config change
first let 's check the <PLACE_HOLDER> source used in the emf ...,final connection provider connection provider = emf . unwrap ( session factory implementor . class ) . get service registry ( ) . get service ( connection provider . class ) ; assert that ( connection provider @$ instance of ( datasource connection provider impl . class ) ) ; final datasource connection provider impl ds cp = ( datasource connection provider impl ) connection provider ; assert that ( ds cp . get data source ( ) @$ is ( pu data source ) ) ;,"s" check
one clone <PLACE_HOLDER>,list < scanner report . duplication > duplication groups = result . duplications for ( input file ) ; assert that ( duplication groups ) . has size ( __num__ ) ; scanner report . duplication clone group = duplication groups . get ( __num__ ) ; assert that ( clone group . get origin position ( ) . get start line ( ) ) . is equal to ( __num__ ) ; assert that ( clone group . get origin position ( ) . get end line ( ) ) . is equal to ( __num__ ) ; assert that ( clone group . get duplicate list ( ) ) . has size ( __num__ ) ; assert that ( clone group . get duplicate ( __num__,one clone
the filter should filter out all <PLACE_HOLDER> @$ but we still expect to see every row .,filter filter = new row filter ( compare operator . equal @$ new binary comparator ( bytes . to bytes ( __str__ ) ) ) ; scan = new scan ( base scan ) ; scan . set filter ( filter ) ; test metric ( scan @$ server side scan metrics . count_of_rows_scanned_key_metric_name @$ rows . length ) ;,filter filter
lease will not be granted if the time taken so far plus lease time exceeds the max <PLACE_HOLDER> .,while ( segment completion mgr . _seconds + lease time sec <= start time + segment completion manager . get max commit time for all segments seconds ( ) ) { params = new request . params ( ) . with instance id ( s2 ) . with offset ( s2 offset ) . with segment name ( segment name str ) . with extra time sec ( lease time sec ) ; response = segment completion mgr . extend build time ( params ) ; assert . assert equals ( response . get status ( ) @$ controller response status . processed ) ; assert . assert true ( ( fsm map . contains key ( segment name str ) ) ) ; segment completion mgr .,time exceeds
test read last buffer <PLACE_HOLDER> across input data <PLACE_HOLDER>s .,test array . set position ( __num__ ) ; result [ __num__ ] = __num__ ; test array . read bits ( result @$ __num__ @$ __num__ ) ; assert that ( result [ __num__ ] ) . is equal to ( ( byte ) __num__ ) ;,test read
and will not trigger fallback <PLACE_HOLDER>,verify ( failover service @$ never ( ) ) . activate ( ) ;,and trigger
if needed @$ update the maximum stack size and number of locals @$ and stack map <PLACE_HOLDER> .,visit switch insn ( dflt @$ labels ) ;,maximum stack
they do not have 0 x dfd 0 <PLACE_HOLDER> set,if ( ( c & __num__ ) != __num__ ) { return false ; } return c == __str__ || c == __str__ || c == __num__ || c == __num__ ;,x dfd
checks the query buffer for cte queries or simple select statements returns the <PLACE_HOLDER> where the injection should start .,final int offset = get statement index ( sb ) ; if ( ! limit helper . has first row ( selection ) ) { add top expression ( sb @$ offset ) ; } else { final string select clause = fill alias in select clause ( sb @$ offset ) ; if ( shallow index of pattern ( sb @$ order_by_pattern @$ offset ) > __num__ ) { add top expression ( sb @$ offset ) ; } enclose with outer query ( sb @$ offset ) ; sb . insert ( offset @$ ! iscte ? __str__ : __str__ ) ; sb . append ( __str__ ) . append ( select clause ) . append ( __str__ ) ; sb . append ( __str__ ),checks returns
to test that the function correctly puts <PLACE_HOLDER> in the resulting template .,string key uri = __str__ ; key template template = aead key templates . create kms aead key template ( key uri ) ; assert equals ( new kms aead key manager ( ) . get key type ( ) @$ template . get type url ( ) ) ; assert equals ( output prefix type . tink @$ template . get output prefix type ( ) ) ; kms aead key format format = kms aead key format . parse from ( template . get value ( ) @$ extension registry lite . get empty registry ( ) ) ; assert equals ( key uri @$ format . get key uri ( ) ) ;,function puts
either node attribute constraint or source allocation <PLACE_HOLDER> alone,if ( splitted . length == __num__ ) { node constraint parser np = new node constraint parser ( spec str ) ; optional < abstract constraint > constraint optional = optional . of nullable ( np . try parse ( ) ) ; if ( constraint optional . is present ( ) ) { st = source tags . empty source tags ( ) ; constraint = constraint optional . get ( ) . build ( ) ; } else { st = source tags . parse from ( spec str ) ; constraint = null ; } } else { throw new placement constraint parse exception ( __str__ + spec str ) ; },node attribute
since we ca n't see the request object on the remote side @$ we ca n't <PLACE_HOLDER> whether the remote side actually performed an authorization <PLACE_HOLDER> here @$ so always set this to true for the proxy servlet . if the remote node failed to perform an authorization <PLACE_HOLDER> @$ pre response authorization <PLACE_HOLDER> filter will log that on the remote node .,client request . set attribute ( auth config . druid_authorization_checked @$ true ) ;,side performed
revoke table read <PLACE_HOLDER> to test grant revoke .,try { revoke from table using access control client ( test_util @$ system user connection @$ test grant revoke . get short name ( ) @$ test_table @$ null @$ null @$ permission . action . read ) ; } catch ( throwable e ) { log . error ( __str__ @$ e ) ; },table read
provide an empty key store since trust manager impl does n't support null key <PLACE_HOLDER> . trust manager impl will use cert store to lookup certificates .,key store store = key store . get instance ( key store . get default type ( ) ) ; store . load ( null ) ; m delegate = new trust manager impl ( store @$ null @$ cert store ) ;,impl support
make the user agent tester change its <PLACE_HOLDER> and make sure we are notified,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = supported status set2 . get ( jabber status enum . free_for_chat ) ;,tester change
now make sure declarative cache ca n't create the same <PLACE_HOLDER>,try { cache creation cache = new cache creation ( ) ; cache . create pool factory ( ) . add locator ( alias2 @$ __num__ ) . create ( __str__ ) ; ignored exception expected exception = add ignored exception ( string . format ( __str__ @$ __str__ ) ) ; try { test xml ( cache ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } finally { expected exception . remove ( ) ; } } finally { pool manager . close ( ) ; },cache create
excepted error will be something like : javax.ejb.ejb exception : wflyjpa 0030 : found extended persistence <PLACE_HOLDER> in sfsb invocation call stack but that can not be used because the transaction already has a transactional <PLACE_HOLDER> associated with it ...,try { top level bean . reference two distinct extended persistence contexts in sametx_fail ( ) ; } catch ( ejb exception caught ) { error = caught ; } assert not null ( __str__ + __str__ @$ error ) ;,transaction has
these should only affect the given <PLACE_HOLDER>,session s = open session ( ) ; transaction t = s . begin transaction ( ) ; int count = s . create query ( __str__ ) . set string ( __str__ @$ __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; count = s . create query ( __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; t . commit ( ) ; s . close ( ) ; data . cleanup ( ) ;,these affect
some point the proxy client class should centralize the connection aliveness <PLACE_HOLDER> and no longer rely on the custom tabs to ping the connections .,try { getm bean server connection ( ) . get default domain ( ) ; } catch ( io exception ex ) { vm panel . get proxy client ( ) . mark as dead ( ) ; },class centralize
only static endpoints @$ return an optimized endpoint <PLACE_HOLDER> .,return new static endpoint group ( static endpoints ) ;,endpoints return
wait 2 election <PLACE_HOLDER>outs so that this master and other masters have <PLACE_HOLDER> to realize they are not leader .,common utils . sleep ms ( __num__ * m conf . get election timeout ms ( ) ) ; if ( state machine . get last applied sequence number ( ) != last appliedsn || state machine . get last primary start sequence number ( ) != gain primacysn ) { continue ; },master have
find all terminal nodes having no incoming <PLACE_HOLDER> .,set < object > all dependents = new hash set < > ( ) ; for ( dependency dep : edges ) { if ( ( dep . depends on instanceof local thread ) ) { if ( dep . depender instanceof message key ) { all dependents . add ( dep . depends on ) ; } } else { all dependents . add ( dep . depends on ) ; } } list < dependency graph > result = new linked list < > ( ) ; for ( object depender : depends on obj ) { if ( ! all dependents . contains ( depender ) ) { result . add ( get sub graph ( depender ) ) ; } } return result ;,nodes having
the result has not all words in it . find another sentence that represents the missing other words and find recursively more <PLACE_HOLDER>,if ( this . remaining terms . size ( ) < query terms . size ( ) ) { max length = max length - this . snippet string . length ( ) ; if ( max length < __num__ ) max length = __num__ ; try { tsr = new snippet extractor ( order . values ( ) @$ this . remaining terms @$ max length ) ; } catch ( final unsupported operation exception e ) { throw e ; } final string next snippet = tsr . snippet string ; if ( next snippet == null ) return ; this . snippet string = this . snippet string + ( __str__ + next snippet ) ; this . remaining terms = tsr . remaining terms,words find
rack 1 has file 1 @$ file 2 and file <PLACE_HOLDER> and file 4 rack 2 has file 2 and file <PLACE_HOLDER> and file 4 rack <PLACE_HOLDER> has file <PLACE_HOLDER> and file 4 setup a filter so that only file 1 and file 2 can be combined,in format = new dummy input format ( ) ; file input format . add input path ( job @$ in dir ) ; in format . set min split size rack ( __num__ ) ;,rack file
called when the action bar search text has changed . update the search <PLACE_HOLDER> @$ and restart the loader to do a new query with this <PLACE_HOLDER> .,search view . set on query text listener ( new search view . on query text listener ( ) { @ override public boolean on query text change ( string new text ) { string new filter = ! text utils . is empty ( new text ) ? new text : null ; if ( m cur filter == null && new filter == null ) { return true ; } if ( m cur filter != null && m cur filter . equals ( new filter ) ) { return true ; } m cur filter = new filter ; get loader manager ( ) . restart loader ( __num__ @$ null @$ cursor loader list fragment . this ) ; return true ; } @,bar update
this must be done to assure the correct vfs file system drivers will process the <PLACE_HOLDER>,string scheme = extract scheme ( full parameter name ) ; try { delegating file system options builder delegatefs options builder = new delegating file system options builder ( kettlevfs . get instance ( ) . get file system manager ( ) ) ; if ( scheme != null ) { delegatefs options builder . set config string ( opts @$ scheme @$ name @$ value ) ; } else { log . log minimal ( __str__ + vfs url ) ; } } catch ( file system exception e ) { if ( ( e . get code ( ) != null ) && ( e . get code ( ) . equals ignore case ( __str__ ) ) ) { log . log minimal ( __str__,drivers process
view enters high padding <PLACE_HOLDER> :,if ( view max > client size + padding min ) { if ( m focus scroll strategy == base grid view . focus_scroll_page ) { first view = view ; do { circular int array positions = m grid . get item positions in rows ( pos @$ m grid . get last visible index ( ) ) [ row ] ; last view = find view by position ( positions . get ( positions . size ( ) - __num__ ) ) ; if ( get view max ( last view ) - view min > client size ) { last view = null ; break ; } } while ( append one column visible items ( ) ) ; if ( last view != null,view enters
if the text wrapper does n't have a start <PLACE_HOLDER> and the new character is a starting one,if ( squigglies . get ( i ) . start index == - __num__ && ! is closing squiggly ( squiggly ) ) { if ( squiggly of interest == - __num__ ) { squiggly of interest = i ; } else { if ( squigglies . get ( i ) . end index < squigglies . get ( squiggly of interest ) . end index ) { squiggly of interest = i ; } } },wrapper have
if the original test turned off sanity <PLACE_HOLDER> @$ make sure our synthesized code passes it .,if ( ! validity check && ! compiler . has errors ( ) ) { new var check ( compiler @$ true ) . process ( externs @$ root ) ; },test turned
exception should cause a <PLACE_HOLDER>,system . err . println ( ) ;,exception cause
this test may not trigger eviction each <PLACE_HOLDER> @$ repeat it 20 <PLACE_HOLDER>s .,for ( int i = __num__ ; i < __num__ ; i ++ ) { delete during eviction ( i ) ; common utils . sleep ms ( __num__ * heartbeat_interval_ms ) ; },test trigger
expiry processing can fill the <PLACE_HOLDER> with a snapshot of the producer priority @$ before producers are complete,add combination values ( __str__ @$ new object [ ] { new integer ( __num__ ) } ) ;,processing fill
since on <PLACE_HOLDER> unselected is triggered before on <PLACE_HOLDER> selected when transferring to another <PLACE_HOLDER> @$ pending update if m is selecting <PLACE_HOLDER> is true to prevent dialog from being dismissed in the process of selecting <PLACE_HOLDER> .,if ( m route for volume updating by user != null || m is selecting route || m is animating volume slider layout ) { return true ; },m selecting
the value of call count can exceed 1 only if the callback thread survives the <PLACE_HOLDER> thrown by the first callback .,assert true ( call count . get ( ) > __num__ ) ; if ( watcher != null ) { watcher . stop ( ) ; watcher . wait for state ( file change watcher . state . stopped ) ; },thread survives
a media period may report a <PLACE_HOLDER> at the current playback position to ensure the renderers are flushed . only report the <PLACE_HOLDER> externally if the position changed .,if ( period position us != playback info . position us ) { playback info = playback info . copy with new position ( playback info . period id @$ period position us @$ playback info . content position us @$ get total buffered duration us ( ) ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( ) ; period position us = playing period holder . to period time ( renderer position us ) ; maybe trigger pending messages ( playback info . position us @$ period position us ) ; playback info . position us = period position us ;,period report
the following check handles <PLACE_HOLDER> of the cutover year before the cutover itself happens .,if ( is gregorian != ( jd >= cutover julian day ) ) { invert gregorian = true ; jd = super . handle compute julian day ( best field ) ; } return jd ;,check handles
host name should be valid @$ but most probably not existing if its not enough @$ then should probably run 'list ' <PLACE_HOLDER> first to be sure ...,final string not existent fake host name = __str__ ; string credentials not found msg = null ; try { run credential program ( not existent fake host name @$ credential helper name ) ; log . warn ( __str__ @$ credential helper name ) ; } catch ( exception e ) { if ( e instanceof invalid result exception ) { credentials not found msg = extract credential provider error message ( ( invalid result exception ) e ) ; } if ( is blank ( credentials not found msg ) ) { log . warn ( __str__ @$ credential helper name @$ e . get message ( ) ) ; } else { log . debug ( __str__ @$ credentials not found msg ) ; },then run
register an ejb channel open <PLACE_HOLDER>,open listener channel open listener = remoteejb service . get open listener ( ) ; try { registration = endpoint . register service ( ejb_channel_name @$ channel open listener @$ this . channel creation options ) ; } catch ( service registration exception e ) { throw new start exception ( e ) ; },ejb channel
decide whether to perform a get or put <PLACE_HOLDER>,while ( benchmark complete . get ( ) == false ) { if ( rand . next double ( ) < config . getputratio ) { try { client response response = client . call procedure ( __str__ @$ processor . generate random key for retrieval ( ) ) ; final volt table pair data = response . get results ( ) [ __num__ ] ; if ( pair data . get row count ( ) == __num__ ) missed gets . increment and get ( ) ; else { final payload processor . pair pair = processor . retrieve from store ( pair data . fetch row ( __num__ ) . get string ( __num__ ) @$ pair data . fetch row ( __num__ ) . get,a get
this node has already set <PLACE_HOLDER> asking a release to signal it @$ so it can safely park .,if ( ws == node . signal ) return true ;,node set
sets handshaker <PLACE_HOLDER> .,if ( handshaker options . get rpc protocol versions ( ) != null ) { start client req . set rpc versions ( handshaker options . get rpc protocol versions ( ) ) ; } if ( handshaker options instanceof alts client options ) { alts client options client options = ( alts client options ) handshaker options ; if ( ! strings . is null or empty ( client options . get target name ( ) ) ) { start client req . set target name ( client options . get target name ( ) ) ; } for ( string service account : client options . get target service accounts ( ) ) { start client req . add target identities builder ( ) . set,sets handshaker
time may be negative which means time milli <PLACE_HOLDER> before 00:00:00 this maybe a bug in calcite avatica,while ( time < __num__ ) { time += millis_per_day ; } int h = time / __num__ ; int time2 = time % __num__ ; int m = time2 / __num__ ; int time3 = time2 % __num__ ; int s = time3 / __num__ ; int ms = time3 % __num__ ; int2 ( buf @$ h ) ; buf . append ( __str__ ) ; int2 ( buf @$ m ) ; buf . append ( __str__ ) ; int2 ( buf @$ s ) ; if ( precision > __num__ ) { buf . append ( __str__ ) ; while ( precision > __num__ ) { buf . append ( ( char ) ( __str__ + ( ms / __num__ ) ) ) ;,which means
if not @$ let 's see <PLACE_HOLDER> factory method to use :,if ( type . is enum type ( ) ) { return factory . create enum deserializer ( ctxt @$ type @$ bean desc ) ; } if ( type . is container type ( ) ) { if ( type . is array type ( ) ) { return factory . create array deserializer ( ctxt @$ ( array type ) type @$ bean desc ) ; } if ( type . is map like type ( ) ) { json format . value format = bean desc . find expected format ( type . get raw class ( ) ) ; if ( format . get shape ( ) != json format . shape . pojo ) { map like type mlt = ( map like,method use
dynamic ser de always writes out bytes <PLACE_HOLDER>,bytes writable bw = ( bytes writable ) r ; out stream . write ( bw . get ( ) @$ __num__ @$ bw . get size ( ) ) ; out stream . write ( final row separator ) ;,ser writes
excess processors will notice that they are not needed right now @$ and will park until they are . the most important thing here is that future units will have a lower <PLACE_HOLDER> of processor as expected max .,return lock ;,units have
schema and relation id pools are tapped @$ schema id pool twice because the renew is triggered . each id acquisition requires 1 <PLACE_HOLDER> and 2 reads,verify store metrics ( get config ( ) . get ( ids_store_name ) @$ system_metrics @$ immutable map . of ( m_mutate @$ __num__ @$ m_get_slice @$ __num__ ) ) ;,acquisition requires
callback might have started an edge <PLACE_HOLDER> .,if ( m drag state == state_dragging ) { break ; },callback started
round screen @$ check boxed @$ do n't use <PLACE_HOLDER> on boxed,if ( m is round ) { if ( ( lp . boxed edges & layout params . box_left ) == __num__ ) { margin left = lp . left margin ; } if ( ( lp . boxed edges & layout params . box_right ) == __num__ ) { margin right = lp . right margin ; } if ( ( lp . boxed edges & layout params . box_top ) == __num__ ) { margin top = lp . top margin ; } if ( ( lp . boxed edges & layout params . box_bottom ) == __num__ ) { margin bottom = lp . bottom margin ; } } else { margin left = lp . left margin ; margin top = lp . top,screen use
a camera preview <PLACE_HOLDER> comes . request a <PLACE_HOLDER> .,byte [ ] picture = new byte [ length ] ; frame frame = manager . get frame ( picture @$ __num__ @$ __num__ ) ;,camera request
3 entities should have been updated : <PLACE_HOLDER> added julius and removed mike from employee.friends mike removed <PLACE_HOLDER> from employee.friends julius added <PLACE_HOLDER> in employee.friends,updated entities = entity store . get by ids ( immutable list . of ( max id . get guid ( ) @$ mike id . get guid ( ) @$ john id . get guid ( ) @$ julius id . get guid ( ) ) ) ; max entity = updated entities . get entity ( max id . get guid ( ) ) ; julius entity = updated entities . get entity ( julius id . get guid ( ) ) ; atlas entity mike entity = updated entities . get entity ( mike id . get guid ( ) ) ; atlas entity john entity = updated entities . get entity ( john id . get guid ( ) ) ; verify relationship attribute,max added
unexpected in bazel logic @$ but skyframe makes no <PLACE_HOLDER> that this package and configuration are actually present .,if ( env . values missing ( ) ) { return null ; },unexpected makes
we update m last frame always rather than in the conditional with the last inset variables @$ because m frame size changed only tracks the <PLACE_HOLDER> and height changing .,m window frames . m last frame . set ( m window frames . m frame ) ; if ( did frame insets change || win animator . m surface resized || config changed || drag resizing changed || m report orientation changed ) { if ( debug_resize || debug_orientation ) { slog . v ( tag_wm @$ __str__ + this + __str__ + __str__ + m window frames . get insets changed info ( ) + __str__ + win animator . m surface resized + __str__ + config changed + __str__ + drag resizing changed + __str__ + m report orientation changed ) ; } if ( m app token != null && m app died ) { m app token . remove dead windows ( ),size tracks
the token was valid @$ but the impersonation failed . this token is clearly not his real password @$ so there 's no point in continuing the request processing . report this <PLACE_HOLDER> and abort .,logger . log ( warning @$ __str__ + username + __str__ @$ x ) ; throw new servlet exception ( x ) ; throw new servlet exception ( x ) ;,processing report
set bucket encryption has no output <PLACE_HOLDER>,return new set bucket encryption result ( ) ;,encryption has
marks both theme configs as changed so main activity restarts <PLACE_HOLDER> on return,find preference ( __str__ ) . set on preference change listener ( new preference . on preference change listener ( ) { @ override public boolean on preference change ( preference preference @$ object new value ) { config . mark changed ( get activity ( ) @$ __str__ ) ; config . mark changed ( get activity ( ) @$ __str__ ) ; get activity ( ) . recreate ( ) ; return true ; } } ) ;,activity restarts
new streams will use the last <PLACE_HOLDER>,delayed stream wfr5 = ( delayed stream ) delayed transport . new stream ( method @$ headers @$ wait for ready call options ) ; assert null ( wfr5 . get real stream ( ) ) ; in order . verify ( picker ) . pick subchannel ( new pick subchannel args impl ( method @$ headers @$ wait for ready call options ) ) ; in order . verify no more interactions ( ) ; assert equals ( __num__ @$ delayed transport . get pending streams count ( ) ) ;,streams use
if records have been read @$ check the has next <PLACE_HOLDER> @$ if not then get the next file to process,if ( this . current file != null && this . current file itr != null ) { this . has next = this . current file itr . has next ( ) ; if ( ! this . has next ) { get next file to read ( ) ; } } else { get next file to read ( ) ; },the has
if the packet has <PLACE_HOLDER> to read @$ we increment its refcount explicitly for every packet writer . so we need to release here . if the packet has no <PLACE_HOLDER> to read @$ then it has to be the last packet . it needs to be released as well .,m current chunk . release ( ) ; m current chunk = null ;,packet has
the below loop creates dump <PLACE_HOLDER> for each table . it reads through the list of write notification events @$ groups the entries per table and creates the lists of files to be replicated . the event <PLACE_HOLDER> in the dump path will have sub<PLACE_HOLDER> for each table . this folder will have metadata for the table and the list of files to be,for ( int idx = __num__ ; idx < num entry ; idx ++ ) { ql md table = new org . apache . hadoop . hive . ql . metadata . table ( event message . get table obj ( idx ) ) ; if ( ql md table prev == null ) { ql md table prev = ql md table ; } if ( ! ql md table prev . get complete name ( ) . equals ( ql md table . get complete name ( ) ) ) { create dump file for table ( within context @$ ql md table prev @$ ql ptns @$ files tobe added ) ; ql ptns = new array list < > ( ) ; files,loop creates
first client that connects completes this <PLACE_HOLDER> .,if ( ! connected client . set ( multiplexer ) ) { additional multiplexers . offer ( multiplexer ) ; } try { return connected client . get ( ) . get inbound observer ( ) ; } catch ( interrupted exception | execution exception e ) { throw new runtime exception ( e ) ; },client completes
balancer should unassign the <PLACE_HOLDER>,assert true ( __str__ @$ admin . balance ( ) ) ; test_util . wait until no regions in transition ( ) ; admin . assign ( region . get encoded name as bytes ( ) ) ; test_util . wait until no regions in transition ( __num__ ) ; currentfn = fnm . get favored nodes ( region ) ; assert not null ( currentfn ) ; assert equals ( __str__ @$ favored node assignment helper . favored_nodes_num @$ currentfn . size ( ) ) ; assert true ( __str__ @$ admin . balance ( ) ) ; test_util . wait until no regions in transition ( __num__ ) ; check favored node assignments ( table name @$ fnm @$ region states ) ;,balancer unassign
the second request should fail with unprocessed request exception which has a <PLACE_HOLDER> of go away received exception .,assert that thrown by ( future2 :: join ) . is instance of ( completion exception . class ) . has cause instance of ( unprocessed request exception . class ) . has root cause instance of ( go away received exception . class ) ;,which has
resetting the session <PLACE_HOLDER> should not reset the poll <PLACE_HOLDER>,time . sleep ( max poll interval ms + __num__ ) ; heartbeat . reset session timeout ( ) ; assert true ( heartbeat . poll timeout expired ( time . milliseconds ( ) ) ) ;,timeout reset
register the result in the session to ensure retries receive the same <PLACE_HOLDER> for the command .,session . register result ( sequence @$ result ) ;,retries receive
not in a. has no <PLACE_HOLDER> in c. not in final result .,client . call procedure ( __str__ @$ __num__ @$ __num__ ) ;,not has
the max and min distance is the total height of the recycler view minus the height of the last child . this ensures that each scroll will never scroll more than a single <PLACE_HOLDER> on the recycler view . that is @$ the max scroll will make the last child the first child and vice versa when scrolling the opposite way .,if ( percentage visible > __num__ ) { max distance -= layout manager . get decorated measured height ( last child ) ; },scroll scroll
those modes never skip <PLACE_HOLDER> @$ there is no need to track unskipped <PLACE_HOLDER> .,if ( build mode == build type . deep || build mode == build type . populate_from_remote_cache ) { return optional . empty ( ) ; },modes skip
we capture and set the context once the user provided observable <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,user provided
right paren encountered matched <PLACE_HOLDER> of expression node @$ or end of expression matched with a left paren node .,if ( n . f precedence != p ) { error ( rbbi rule builder . u_brk_mismatched_paren ) ; },paren encountered
the above method will perform the quit as long as the user does not cancel the <PLACE_HOLDER>,td . display user save ( tool dialog . quit ) ;,user cancel
pipeline should have ssl <PLACE_HOLDER> and server tls <PLACE_HOLDER>,iterator < map . entry < string @$ channel handler > > iterator = pipeline . iterator ( ) ; assert that ( iterator . next ( ) . get value ( ) ) . is instance of ( ssl handler . class ) ;,pipeline have
this file does not actually contain a <PLACE_HOLDER>,if ( box header == null ) { logger . warning ( error message . mp4_file_has_no_metadata . get msg ( ) ) ; return tag ; },file contain
input pattern expected <PLACE_HOLDER>,string data [ ] = { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ;,pattern expected
if the patch <PLACE_HOLDER> does n't equal port <PLACE_HOLDER> @$ return error here in case of mismatch between audio ports and audio patches .,if ( patch generation [ __num__ ] != port generation [ __num__ ] ) { return error ; } for ( int i = __num__ ; i < new patches . size ( ) ; i ++ ) { for ( int j = __num__ ; j < new patches . get ( i ) . sources ( ) . length ; j ++ ) { audio port config port cfg = update port config ( new patches . get ( i ) . sources ( ) [ j ] @$ new ports ) ; new patches . get ( i ) . sources ( ) [ j ] = port cfg ; } for ( int j = __num__ ; j < new patches . get (,generation equal
chunk contains <PLACE_HOLDER> from glmmme run,glm weights fun [ ] glmfun rand = null ; long chk start row idx = chunks [ __num__ ] . start ( ) ;,chunk contains
bits consumed range : 1 to 8 @$ where 8 indicates last <PLACE_HOLDER> fully consumed,bits consumed = __num__ - last bitfield dt . get bit offset ( ) ;,8 indicates
we rebuild the the external <PLACE_HOLDER> using the has of the parent <PLACE_HOLDER> @$ the shared timestamp and the sequence number,string time stamp = string utils . substring after ( internal meeting id @$ __str__ ) ; string external hash = digest utils . sha1 hex ( ( parent meeting id + __str__ + time stamp + __str__ + params . get ( api params . sequence ) ) ) ; external meeting id = external hash + __str__ + time stamp ;,the has
m fake dragging is true when a fake drag is interrupted by an a 11 y command set it to false so end fake drag wo n't fling the recycler <PLACE_HOLDER>,m fake dragging = false ; boolean has new target = m target != target ; m target = target ; dispatch state changed ( scroll_state_settling ) ; if ( has new target ) { dispatch selected ( target ) ; },drag fling
the while loop will create a rotating <PLACE_HOLDER>,int current index = i ; int proper doc id ; while ( actual doc id != ( proper doc id = sorted doc ids [ current index ] ) ) { int current doc id = current index + start doc id offset ; _data buffer . copy to ( proper doc id * _doc size long @$ _data buffer @$ current doc id * _doc size long @$ _doc size long ) ; sorted doc ids [ current index ] = current doc id ; current index = proper doc id - start doc id offset ; },loop create
this condition matches the head <PLACE_HOLDER> but has some inputs those are created by notification .,return inputs == null || ( ( collection ) inputs ) . is empty ( ) || ( ( collection < map < string @$ object > > ) inputs ) . stream ( ) . any match ( m -> ! __str__ . equals ( m . get ( __str__ ) ) ) ;,condition matches
set max queue <PLACE_HOLDER> to 3 so that 2 calls from the test wo n't trigger back off because the queue is full .,rpc . builder builder = new server builder ( conf ) . set queue size per handler ( queue size per handler ) . set num handlers ( __num__ ) . set verbose ( true ) ; return setup test server ( builder ) ;,max queue
we enqueued a pending frame @$ let 's try <PLACE_HOLDER> else next .,break ;,let try
atomically run this function on a clone of the bits from the existing key and install the result as the new value . this function may run multiple <PLACE_HOLDER> if there are collisions .,atomic q = new atomic2 ( ) ; q . invoke ( key ) ;,function run
always has a left <PLACE_HOLDER> .,node left node = nodes [ left index ] ; float left value = left node . value ;,always has
fail rarely ... but allow failure as this one has no <PLACE_HOLDER>,if ( math . random ( ) > __num__ ) { throw new runtime exception ( __str__ ) ; },one has
all elements of each list must have equal <PLACE_HOLDER> so we find the <PLACE_HOLDER> using the first element .,strict bit vector res = new strict bit vector ( alist . get ( __num__ ) . get size ( ) + blist . get ( __num__ ) . get size ( ) ) ; list < strict bit vector > products = int stream . range ( __num__ @$ alist . size ( ) ) . parallel ( ) . map to obj ( i -> multiply without reduction ( alist . get ( i ) @$ blist . get ( i ) ) ) . collect ( collectors . to list ( ) ) ; products . stream ( ) . reduce ( res @$ ( a @$ b ) -> { a . xor ( b ) ; return a ; } ) ; return,elements have
mssql bukl insert can only use local <PLACE_HOLDER> @$ so that 's what we limit ourselves to .,if ( ! ( file object instanceof local file ) ) { throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ vfs filename ) ) ; },insert use
ensure that rtr is doing <PLACE_HOLDER> and still making progress,tasks [ __num__ ] = test tasks . unending ( __str__ ) ; results [ __num__ ] = remote task runner . run ( tasks [ __num__ ] ) ; wait for one worker to have unacked tasks ( ) ; if ( rtr test utils . task announced ( __str__ @$ tasks [ __num__ ] . get id ( ) ) ) { rtr test utils . mock worker running task ( __str__ @$ tasks [ __num__ ] ) ; rtr test utils . mock worker complete successful task ( __str__ @$ tasks [ __num__ ] ) ; } else { rtr test utils . mock worker running task ( __str__ @$ tasks [ __num__ ] ) ; rtr test utils . mock worker complete successful task,rtr doing
this prevents the case where a program represents the entire address <PLACE_HOLDER> in which everything looks like a pointer,if ( program is entire memory space ( program ) ) { return null ; },program represents
for third client check that key <PLACE_HOLDER> @$ key 6 are accessible but others are not,return new operation with action [ ] { new operation with action ( operation code . put @$ __num__ @$ op flags . none @$ __num__ ) @$ new operation with action ( operation code . get @$ __num__ @$ op flags . check_nokey @$ new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) @$ new operation with action ( operation code . get @$ __num__ @$ op flags . check_nokey | op flags . use_oldconn | op flags . check_notauthz @$ new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) @$ new operation with action ( operation code . get @$ __num__ @$ op flags . check_nokey @$ new int [ ] { __num__ @$ __num__ },client check
mark all classes with package visible members . mark all exception catches of methods . count all method <PLACE_HOLDER> . mark super <PLACE_HOLDER> and other access of methods .,stack size computer stack size computer = new stack size computer ( ) ; program class pool . classes accept ( new multi class visitor ( new class visitor [ ] { new package visible member containing class marker ( ) @$ new all constant visitor ( new package visible member invoking class marker ( ) ) @$ new all method visitor ( new optimization info member filter ( new all attribute visitor ( new multi attribute visitor ( new attribute visitor [ ] { stack size computer @$ new catch exception marker ( ) @$ new all instruction visitor ( new multi instruction visitor ( new instruction visitor [ ] { new super invocation marker ( ) @$ new dynamic invocation marker ( ) @$ new backward,mark mark
explicitly setting service rpc for datanode . this because dfs util.get nn service rpc addresses for cluster looks up client facing port and service port at the same time @$ and if no setting for service rpc @$ it would return client port @$ in this case @$ it will be the auxiliary port for data node . which is not <PLACE_HOLDER> auxiliary is,cluster conf . set ( dfs_namenode_service_rpc_address_key @$ __str__ ) ; cluster conf . set ( common configuration keys . hadoop_security_sasl_props_resolver_class @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set boolean ( dfs_namenode_send_qop_enabled @$ true ) ;,auxiliary for
domain attribute must contain at least one embedded <PLACE_HOLDER> @$ or the value must be equal to .local .,int dot index = cookie domain . index of ( __str__ @$ __num__ ) ; if ( ( ( dot index < __num__ ) || ( dot index == cookie domain . length ( ) - __num__ ) ) && ( ! cookie domain . equals ( __str__ ) ) ) { throw new malformed cookie exception ( __str__ + cookie . get domain ( ) + __str__ + __str__ ) ; },attribute contain
start the truncate procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new truncate table procedure ( proc exec . get environment ( ) @$ table name @$ preserve splits ) ) ; test recovery and double execution ( util @$ proc id @$ step ) ; procedure testing utility . set kill and toggle before store update ( proc exec @$ false ) ; util . wait until all regions assigned ( table name ) ;,procedure kill
no container to cleanup . cleanup app level <PLACE_HOLDER> .,if ( app . containers . is empty ( ) ) { app . handle app finish with containers cleanedup ( ) ; return application state . application_resources_cleaningup ; },container app
try all delimiters and choose the delimiter which yields the shortest <PLACE_HOLDER> .,int min frame length = integer . max_value ; channel buffer min delim = null ; for ( channel buffer delim : delimiters ) { int frame length = index of ( buffer @$ delim ) ; if ( frame length >= __num__ && frame length < min frame length ) { min frame length = frame length ; min delim = delim ; } } if ( min delim != null ) { int min delim length = min delim . capacity ( ) ; channel buffer frame ; if ( discarding too long frame ) { discarding too long frame = false ; buffer . skip bytes ( min frame length + min delim length ) ; int too long frame length = this . too,which yields
path is n't supported @$ mimic resolver <PLACE_HOLDER> .,if ( remote locations . is empty ( ) ) { return null ; },path mimic
let system take <PLACE_HOLDER> for time tick events .,final work source work source = null ;,system take
so that future open gl revisions wo n't break jme <PLACE_HOLDER> fall through intentional,case __num__ : caps . add ( caps . glsl450 ) ; case __num__ : caps . add ( caps . glsl440 ) ; case __num__ : caps . add ( caps . glsl430 ) ; case __num__ : caps . add ( caps . glsl420 ) ; case __num__ : caps . add ( caps . glsl410 ) ; case __num__ : caps . add ( caps . glsl400 ) ; case __num__ : caps . add ( caps . glsl330 ) ; case __num__ : caps . add ( caps . glsl150 ) ; case __num__ : caps . add ( caps . glsl140 ) ; case __num__ : caps . add ( caps . glsl130 ) ; case __num__ : caps . add ( caps,revisions break
note : the original implementation had both <PLACE_HOLDER> calling the 3 arg method . this is preserved but perhaps it should pass the args array instead of null .,for ( int i = __num__ ; i < ifcs . length ; i ++ ) { method = internal find method ( ifcs [ i ] @$ method name @$ arg count @$ null ) ; if ( method != null ) { break ; } },implementation had
both the collapsed and full qs panels get this <PLACE_HOLDER> @$ this check determines which one should handle showing the detail .,if ( should show detail ( ) ) { qs panel . this . show detail ( show @$ r ) ; } if ( m detail record == r ) { fire toggle state changed ( state ) ; } r . scan state = state ; if ( m detail record == r ) { fire scan state changed ( r . scan state ) ; } if ( announcement != null ) { m handler . obtain message ( h . announce_for_accessibility @$ announcement ) . send to target ( ) ; },collapsed get
font shapes define no fillstyles per se @$ but do reference fillstyle index 1 @$ which represents the font <PLACE_HOLDER> . we just report null in this case .,begin fill ( null ) ;,which represents
we are interested in this fetch only if the beginning offset matches the current consumed <PLACE_HOLDER>,if ( error == errors . none ) { subscription state . fetch position position = subscriptions . position ( tp ) ; if ( position == null || position . offset != fetch offset ) { log . debug ( __str__ + __str__ @$ tp @$ fetch offset @$ position ) ; return null ; } log . trace ( __str__ @$ partition . records . size in bytes ( ) @$ tp @$ position ) ; iterator < ? extends record batch > batches = partition . records . batches ( ) . iterator ( ) ; completed fetch = next completed fetch ; if ( ! batches . has next ( ) && partition . records . size in bytes ( ) > __num__ ),beginning matches
do n't let the parent intercept our <PLACE_HOLDER> .,src . get parent ( ) . request disallow intercept touch event ( true ) ; return true ;,parent intercept
the first round of scan uses the m iterator left from last eviction . then scan the <PLACE_HOLDER> from a new iterator for at most two round : first round to mark candidate.m is accessed as false @$ second round to remove the candidate from the <PLACE_HOLDER> .,int round to scan = __num__ ; while ( num to evict > __num__ && round to scan > __num__ ) { if ( ! m iterator . has next ( ) ) { m iterator = m pool . entry set ( ) . iterator ( ) ; round to scan -- ; } map . entry < k @$ resource > candidate map entry = m iterator . next ( ) ; resource candidate = candidate map entry . get value ( ) ; if ( candidate . m is accessed ) { candidate . m is accessed = false ; } else { if ( candidate . m ref count . compare and set ( __num__ @$ integer . min_value ) ) { m iterator,iterator scan
negative cols have <PLACE_HOLDER> of cols added,if ( cols [ __num__ ] < __num__ ) for ( int i = __num__ ; i < cols . length ; i ++ ) cols [ i ] += fr . num cols ( ) ;,cols have
intensity steps hr distance <PLACE_HOLDER>,return sample ;,steps hr
add properties from builder configuration @$ after the injected build <PLACE_HOLDER> .,final variable resolver < string > resolver = new union < > ( new by map < > ( env ) @$ vr ) ; args . add key value pairs from property string ( __str__ @$ this . properties @$ resolver @$ sensitive vars ) ; if ( uses private repository ( ) ) args . add ( __str__ + build . get workspace ( ) . child ( __str__ ) ) ; args . add tokenized ( normalized target ) ; wrap up arguments ( args @$ normalized target @$ build @$ launcher @$ listener ) ; build env vars ( env @$ mi ) ; if ( ! launcher . is unix ( ) ) { args = args . to windows command ( ),injected build
inclusive range of i pv 6 <PLACE_HOLDER>,handler . include ( __str__ ) ;,range pv
we need some way to tell <PLACE_HOLDER> the real author is . right now i 'm just going to arbitrarily decide that this is via the document title @$ as i ca n't really think of an easy way to do it otherwise ; no matter what we decide to use @$ the test set will need to be prepped before hand regardless .,for ( string test doc : results . key set ( ) ) { document curr test doc = null ; for ( document d : test docs ) { if ( d . get title ( ) . equals ( test doc ) ) { curr test doc = d ; break ; } } string selected author = __str__ ; double max = __num__ ; for ( string potential author : results . get ( curr test doc . get title ( ) ) . key set ( ) ) { if ( results . get ( curr test doc . get title ( ) ) . get ( potential author ) . double value ( ) > max ) { max = results . get,author is
null params indicate external subject <PLACE_HOLDER> . no <PLACE_HOLDER> context will be attached .,return do subject login ( subject @$ null ) ;,params indicate
no unmap byte <PLACE_HOLDER>,return null ;,unmap byte
v 0 has the smallest <PLACE_HOLDER> @$ stricter checking is done later,if ( record size < legacy record . record_overhead_v0 ) throw new corrupt record exception ( string . format ( __str__ @$ record size @$ legacy record . record_overhead_v0 ) ) ; if ( record size > max message size ) throw new corrupt record exception ( string . format ( __str__ @$ record size @$ max message size ) ) ; if ( remaining < header_size_up_to_magic ) return null ; byte magic = buffer . get ( buffer . position ( ) + magic_offset ) ; if ( magic < __num__ || magic > record batch . current_magic_value ) throw new corrupt record exception ( __str__ + magic ) ; return record size + log_overhead ;,v has
use a condition that folds after floating guards and before guard lowering . after disabling pea and read elimination @$ the following condition does the <PLACE_HOLDER> .,if ( static value == static value ) { value = a . x ; result = foo ( a ) ; },condition does
clone the call to prevent a <PLACE_HOLDER> with another thread stomping on the response while being sent . the original call is effectively discarded since the wait count wo n't hit zero,call = new rpc call ( this ) ; setup response ( call @$ status @$ rpc error code proto . error_rpc_server @$ null @$ t . get class ( ) . get name ( ) @$ string utils . stringify exception ( t ) ) ; setup response ( call @$ call . response params . return status @$ call . response params . detailed err @$ call . rv @$ call . response params . error class @$ call . response params . error ) ;,call prevent
close server 2 and pause so server has a <PLACE_HOLDER> to close,close server ( server2 ) ; wait . pause ( __num__ * __num__ ) ; wait for cqs disconnected ( client @$ __str__ @$ __num__ ) ;,server has
no permissions required or already grunted @$ can start crop image <PLACE_HOLDER>,m crop image view . set image uri async ( m crop image uri ) ;,permissions start
hand reset the big table <PLACE_HOLDER> .,for ( int column : big table retain column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } for ( int column : non outer small table key column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } if ( outer small table key column map != null ) { for ( int column : outer small table key column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } },hand reset
the view root <PLACE_HOLDER>,vertical layout view layout = new vertical layout ( fields ) ; view layout . set size full ( ) ; view layout . set component alignment ( fields @$ alignment . middle_center ) ; view layout . set style name ( reindeer . layout_blue ) ; set composition root ( view layout ) ;,view root
send two task start and two task fail <PLACE_HOLDER> for tasks 0 and 1,mockito . when ( reader . get next event ( ) ) . then answer ( new answer < history event > ( ) { public history event answer ( invocation on mock invocation ) throws io exception { int event id = num events read . get and increment ( ) ; taskid tid = tids [ event id & __num__ ] ; if ( event id < __num__ ) { return new task started event ( tid @$ __num__ @$ task type @$ __str__ ) ; } if ( event id < __num__ ) { task failed event tfe = new task failed event ( tid @$ __num__ @$ task type @$ __str__ @$ __str__ @$ null @$ new counters ( ) ) ; tfe .,start fail
constrain the maximum state version this backup agent can handle in case a newer or corrupt backup <PLACE_HOLDER> existed,if ( state version > state_version ) { state version = state_version ; },agent handle
only privileged apps and updated privileged apps can add child <PLACE_HOLDER> .,if ( pkg . child packages != null && ! pkg . child packages . is empty ( ) ) { if ( ( scan flags & scan_as_privileged ) == __num__ ) { throw new package manager exception ( __str__ + __str__ + pkg . package name ) ; } final int child count = pkg . child packages . size ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { package parser . package child pkg = pkg . child packages . get ( i ) ; if ( m settings . has other disabled system pkg with childl pr ( pkg . package name @$ child pkg . package name ) ) { throw new package manager,apps add
need to extend this test to validate that different users can create <PLACE_HOLDER> of exact same name . so far only create followed by build is tested . need to test create followed by create .,client . action destroy ( same app name ) ;,users create
was this condition here because it can be set by set camera ? set camera will not trigger events for camera listeners ? or other cause ? actually we can say if we are starting fling here if reason is set @$ was it an original condition <PLACE_HOLDER> here ?,if ( m camera change tracker . is empty ( ) ) { m camera change tracker . set reason ( reason ) ; log . d ( __str__ @$ __str__ + reason + __str__ + m camera change tracker . is user interaction ( ) + __str__ + m camera change tracker . is animated ( ) ) ; handle map changed event ( event types . region_will_change ) ; } else { log . d ( __str__ @$ __str__ ) ; },reason set
check that ranks provide a valid topological <PLACE_HOLDER>,for ( int v = __num__ ; v < g . v ( ) ; v ++ ) { for ( int w : g . adj ( v ) ) { if ( rank ( v ) > rank ( w ) ) { system . err . printf ( __str__ @$ v @$ w @$ v @$ rank ( v ) @$ w @$ rank ( w ) ) ; return false ; } } },ranks provide
completing this task should end the <PLACE_HOLDER>,task service . complete ( task after sub process . get id ( ) ) ; assert process ended ( pi . get id ( ) ) ;,task end
set up for the schema factory and then set a global one which overrides the <PLACE_HOLDER> .,conf . set ( committer_factory_class @$ __str__ ) ; create committer factory ( simple committer factory . class @$ http_path @$ conf ) ;,which overrides
wait a bit to make sure compiler had a <PLACE_HOLDER> to do it 's stuff :,try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { } start time = system . nano time ( ) ; record loop with expected interval ( histogram @$ timing loop count @$ expected interval ) ; end time = system . nano time ( ) ; delta usec = ( end time - start time ) / __num__ ; rate = __num__ * timing loop count / delta usec ; system . out . println ( label + __str__ ) ; system . out . println ( label + timing loop count + __str__ + delta usec + __str__ + rate + __str__ ) ; rate = __num__ * histogram . get total count ( ) / delta usec ;,compiler had
swap if page row object <PLACE_HOLDER>,swap = old page row != m page row ;,row object
private method call leaving the invokespecial alone will cause a verify <PLACE_HOLDER>,if ( owner . equals ( classname ) ) { string descriptor = utils . insert extra parameter ( owner @$ desc ) ; super . visit method insn ( invokestatic @$ utils . get executor name ( classname @$ suffix ) @$ name @$ descriptor @$ false ) ; return ; } else { type descriptor supertype descriptor = get type ( owner ) ; method member target = supertype descriptor . get by name and descriptor ( name + desc ) ; if ( target != null && target . is protected ( ) ) { super . visit method insn ( invokespecial @$ classname @$ name + method suffix super dispatcher @$ desc @$ false ) ; } else { super . visit method insn,call cause
use linked hash <PLACE_HOLDER> for predictable iteration order .,this . method to node = new linked hash map < > ( ) ;,use linked
we are not interested in defining a new enter <PLACE_HOLDER> . instead we change default <PLACE_HOLDER> duration,get window ( ) . get enter transition ( ) . set duration ( get resources ( ) . get integer ( r . integer . anim_duration_long ) ) ;,new enter
this is all jealously ; should be refactored to ask tibm if it wants to be filtered for rejoin and eliminate this horrible introspection . this implementation mimics the original live rejoin <PLACE_HOLDER> for execution site ... multi part ad hoc does not need to be checked because its an alias and runs procedure as planned .,if ( ! msg . is sys proc task ( ) ) { return true ; },implementation mimics
sleep to ensure session registry impl will update <PLACE_HOLDER>,thread . sleep ( __num__ ) ;,session update
computation : unconfigured build <PLACE_HOLDER> to raw <PLACE_HOLDER> node computation,default unconfigured target node factory raw target node factory = new default unconfigured target node factory ( params . get known rule types provider ( ) @$ new built target verifier ( ) @$ cell . get cell path resolver ( ) @$ new selector list factory ( new selector factory ( params . get unconfigured build target factory ( ) ) ) ) ; build target to unconfigured target node computation build target to unconfigured target node computation = build target to unconfigured target node computation . of ( raw target node factory @$ cell ) ;,unconfigured build
looks like we 're going to need the buffers ... we know the new string will be shorter . using the old length may overshoot a <PLACE_HOLDER> @$ but it will save us from resizing the buffer .,if ( decoded == null ) { decoded = new string buffer ( old length ) ; out = new byte array output stream ( __num__ ) ; } else { out . reset ( ) ; },looks overshoot
during this time @$ one of the threads should initialize the partition <PLACE_HOLDER> @$ and then all of the threads should get the new ring .,assert equals ( count done ( results ) @$ results . size ( ) ) ; executor . shutdown now ( ) ;,one initialize
workers have some <PLACE_HOLDER> to complete their in progress tasks,long until = system . current time millis ( ) + shutdown timeout in ms ; log . debug ( __str__ @$ shutdown timeout in ms ) ; while ( system . current time millis ( ) < until && ce worker controller . has at least one processing worker ( ) ) { thread . sleep ( __num__ ) ; },workers have
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
the test passed @$ so just <PLACE_HOLDER> from main and harness will interepret this <PLACE_HOLDER> as a pass,return ;,return interepret
missing response gets <PLACE_HOLDER> around,for ( int row = __num__ ; row < y [ __num__ ] . _len ; ++ row ) { if ( y [ __num__ ] . isna ( row ) ) { if ( ( start + row ) % nfolds == test fold ) y [ __num__ ] . set ( row @$ test fold ) ; } else { if ( y [ __num__ ] . at8 ( row ) == ( classes == null ? class label : classes [ class label ] ) ) { if ( test fold == get fold id ( start + row @$ seeds [ class label ] ) ) y [ __num__ ] . set ( row @$ test fold ) ; } } },response gets
remove all declared <PLACE_HOLDER> and find out which modules were used while at it .,cc compilation context . headers and modules headers and modules = cc compilation context . compute declared headers and used modules ( use pic @$ undeclared headers @$ header info ) ; used modules = immutable list . copy of ( headers and modules . modules ) ; undeclared headers . remove all ( headers and modules . headers ) ;,all declared
we optimize a little here : if the name has no <PLACE_HOLDER> @$ which is the common case @$ then it is unnecessarily expensive to call regex split,if ( speaker . index of ( __str__ ) >= __num__ ) { for ( string s : whitespace_pattern . split ( speaker ) ) { if ( ant . head string . equals ignore case ( s ) ) return true ; } } else { if ( ant . head string . equals ignore case ( speaker ) ) return true ; },name has
deserialize record may return a <PLACE_HOLDER> if there is no more data . however @$ when we are deserializing an edit @$ we do so only when we know that we should have data . this is why the java docs for this method on the interface indicate that this method should never return <PLACE_HOLDER> . as a result @$ if there is no,throw new eof exception ( ) ;,record return
weird <PLACE_HOLDER> : does not find <PLACE_HOLDER> @$ because it scans for consistent case only,assert false ( r . contains ignore case ( __str__ ) ) ;,not find
it will be checked inside the put <PLACE_HOLDER> that poor <PLACE_HOLDER> does not overwrite rich <PLACE_HOLDER>,segment . fulltext ( ) . put metadata ( entry ) ;,metadata overwrite
... then fetch the <PLACE_HOLDER> for the specified uv set name ...,list < vector2f > uvs for name = uvs for material . get ( uv set name ) ; if ( uvs for name == null ) { uvs for name = new array list < vector2f > ( ) ; uvs for material . put ( uv set name @$ uvs for name ) ; },then fetch
blk can see <PLACE_HOLDER> back,epsilon ( blk end @$ loop ) ;,blk see
object has default meta <PLACE_HOLDER> @$ so we need to replace it on demand,object = obj ;,object has
set stopped if no more write <PLACE_HOLDER> tp meta tables since last time we went around the loop . any open meta regions will be closed on our way out .,if ( all user regions offline ) { if ( old request count == get write request count ( ) ) { stop ( __str__ ) ; break ; } old request count = get write request count ( ) ; } else { close user regions ( this . abort requested ) ; },tables write
note : these lines center the <PLACE_HOLDER> dialog in the current window .,point p = main frame . get location on screen ( ) ; dimension d1 = main frame . get size ( ) ; dimension d2 = dialog . get size ( ) ; dialog . set location ( p . x + ( d1 . width - d2 . width ) / __num__ @$ p . y + ( d1 . height - d2 . height ) / __num__ ) ; dialog . set visible ( true ) ;,lines center
push view and context related <PLACE_HOLDER> from android to flutter .,send user settings to flutter ( ) ; send locales to flutter ( get resources ( ) . get configuration ( ) ) ; send viewport metrics to flutter ( ) ;,view related
each thread needs some <PLACE_HOLDER> to process it . if a thread needs to send an oob message to the client @$ but blocked on network for long <PLACE_HOLDER> @$ we need to force its termination .,log . info ( __str__ ) ; wait all peers ( __num__ @$ time unit . seconds ) ;,thread needs
check if the range is preceding the <PLACE_HOLDER> or after it .,boolean is before = range . second . equals ( quote begin token index - __num__ ) ;,range preceding
revisit : the following should also update id <PLACE_HOLDER>,attr . set node value ( attr value ) ; index = f element . set xerces attribute node ( attr ) ; f augmentations . insert element at ( new augmentations impl ( ) @$ index ) ; attr . set specified ( false ) ;,the update
update video track count @$ audio & subtitle track <PLACE_HOLDER> .,m video track count = __num__ ; m audio tracks = new array list < > ( ) ; m subtitle tracks = new array list < > ( ) ; m selected audio track index = __num__ ;,audio track
checks for entries that failed to import . get failures provides up to <PLACE_HOLDER> of the first failed entries for the job @$ if any exist .,list < string > failed endpoints = get import job result . get import job response ( ) . get failures ( ) ; if ( failed endpoints != null ) { system . out . println ( __str__ ) ; for ( string failed endpoint : failed endpoints ) { system . out . println ( failed endpoint ) ; } },failures provides
assign playback info immediately such that all getters return the right <PLACE_HOLDER> .,playback info previous playback info = this . playback info ; this . playback info = playback info ; boolean is playing = is playing ( ) ; notify listeners ( new playback info update ( playback info @$ previous playback info @$ listeners @$ track selector @$ position discontinuity @$ position discontinuity reason @$ timeline change reason @$ seek processed @$ play when ready @$ previous is playing != is playing ) ) ;,getters return
user already specified schema <PLACE_HOLDER>,if ( vendor extensions . contains key ( codegen_vendor_extension_key ) ) { logger . info ( __str__ + base name + __str__ ) ; return ; },user specified
durable client registers durable <PLACE_HOLDER> on server,register interest ( durable clientvm @$ region name @$ true @$ interest result policy . keys_values ) ;,client registers
initialize drag <PLACE_HOLDER> .,previous touch point px . set ( e . getx ( ) @$ e . gety ( ) ) ; return true ;,initialize drag
if there is a <PLACE_HOLDER> change @$ but the two values have the same <PLACE_HOLDER> ...,if ( ( instant ^ sum ) < __num__ && ( instant ^ offset ) >= __num__ ) { throw new arithmetic exception ( __str__ ) ; } return offset ;,values have
the app master jar <PLACE_HOLDER> .,job . add file to class path ( app_jar ) ;,master jar
lazy definition of schema : do not write empty <PLACE_HOLDER>,final boolean solrlazy = get config bool ( switchboard constants . federated_service_solr_indexing_lazy @$ true ) ;,definition write
make sure the user now has defaultproc <PLACE_HOLDER>,try { user client . call procedure ( __str__ @$ __num__ @$ __num__ ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; } threw = false ; try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { assert true ( pce . get message ( ) . contains ( __str__ ) ) ; threw = true ; } assert true ( __str__ @$ threw ) ; threw = false ; try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { assert true ( pce . get message ( ),user has
init our two <PLACE_HOLDER> flags . basically the unmarked <PLACE_HOLDER> counter is always chasing the current <PLACE_HOLDER> counter . when it catches up @$ that means we made a pass through that did not add any new <PLACE_HOLDER>s to the lists @$ at which time we are done . we could have used a expanding array of flags which we used to mark off,int unmarked state = __num__ ; int cur state = __num__ ;,counter chasing
the instruction after the jsr starts a new basic <PLACE_HOLDER> .,next basic block = new label ( ) ;,instruction starts
now @$ both the close action and a grab for an write page <PLACE_HOLDER> is waiting for our first thread . when we release that <PLACE_HOLDER> @$ we should see that either close completes and our second thread @$ the one b<PLACE_HOLDER>ed on the write <PLACE_HOLDER> @$ gets an exception @$ or we should see that the second thread gets the <PLACE_HOLDER> @$ and,unlock latch . count down ( ) ;,thread gets
we need two certificates @$ one for the client and one for the server . the client must know the server 's public <PLACE_HOLDER> to make a curve connection .,z cert client cert = new z cert ( ) ; z cert server cert = new z cert ( ) ;,client know
check sm <PLACE_HOLDER> and member access before cracking .,try { check access ( ref kind @$ defc @$ member ) ; check security manager ( defc @$ member ) ; } catch ( illegal access exception ex ) { throw new illegal argument exception ( ex ) ; } if ( allowed modes != trusted && member . is caller sensitive ( ) ) { class < ? > caller class = target . internal caller class ( ) ; if ( ! has private access ( ) || caller class != lookup class ( ) ) throw new illegal argument exception ( __str__ + caller class ) ; },check sm
now the block has 10 internal <PLACE_HOLDER> .,assert equals ( __num__ @$ new dn locs . length ) ;,block has
now let 's try to do an upsert where the outcome relies <PLACE_HOLDER> on doing an update @$ doing an insert and also triggering a delete .,client . call procedure ( __str__ @$ __num__ @$ __num__ @$ __num__ ) ; client . call procedure ( __str__ @$ __num__ @$ __num__ @$ __num__ ) ;,outcome relies
another producer has moved the <PLACE_HOLDER> failed to increment,so element ( buffer @$ calc element offset ( p index @$ mask ) @$ e ) ; so sequence ( s buffer @$ seq offset @$ p index + __num__ ) ; return true ;,producer moved
each follower may have just sent a leader check @$ which receives no <PLACE_HOLDER>,cluster . stabilise ( math . max ( default millis ( leader_check_timeout_setting ) + default millis ( leader_check_interval_setting ) + default_delay_variability + default_election_delay @$ default millis ( follower_check_timeout_setting ) + default millis ( follower_check_interval_setting ) + default_delay_variability ) + default_cluster_state_update_delay + default_cluster_state_update_delay ) ;,which receives
configure preexechooks with disallow transform hook to disallow transform <PLACE_HOLDER>,if ( session ctx . get client type ( ) == hive authz session context . client_type . hiveserver2 && hive conf . get bool var ( hive conf . conf vars . hive_authorization_enabled ) ) { string hooks = hive conf . get var ( hive conf . conf vars . preexechooks ) . trim ( ) ; if ( hooks . is empty ( ) ) { hooks = disallow transform hook . class . get name ( ) ; } else { hooks = hooks + __str__ + disallow transform hook . class . get name ( ) ; } log . debug ( __str__ + hooks ) ; hive conf . set var ( hive conf . conf vars . preexechooks @$ hooks ),preexechooks transform
this object holds its <PLACE_HOLDER> . we want to write a send channel that the other side can use to retrieve that <PLACE_HOLDER> .,if ( m have data ) { if ( m send channel == null ) { m send channel = new send channel ( this ) ; } out . write strong binder ( m send channel ) ; } else { out . write strong binder ( m receive channel ) ; },object holds
true if standard and daylight time use the same <PLACE_HOLDER> .,boolean use same name = false ;,time use
check that slave satisfies min <PLACE_HOLDER> .,if ( cont . get resource ( ) . get virtual cores ( ) < props . cpus per node ( ) || cont . get resource ( ) . get memory ( ) < props . total memory per node ( ) ) { log . log ( level . fine @$ __str__ @$ new object [ ] { cont . get node id ( ) . get host ( ) @$ cont . get resource ( ) . get virtual cores ( ) @$ cont . get resource ( ) . get memory ( ) } ) ; return false ; } return true ;,slave satisfies
verify login user got a new <PLACE_HOLDER> .,login user . relogin from keytab ( ) ; kerberos ticket new login ticket = check ticket and keytab ( login user @$ principal1 @$ true ) ; assert . assert not equals ( login ticket . get auth time ( ) @$ new login ticket . get auth time ( ) ) ;,user got
doesnt matter <PLACE_HOLDER> node.id and voldemort.home values are for this test,props . set property ( __str__ @$ __str__ ) ; props . set property ( __str__ @$ __str__ ) ; voldemort config = new voldemort config ( props ) ; if ( this . prefix partition id ) { voldemort config . set rocksdb prefix keys with partition id ( true ) ; } this . rocks db config = new rocks db storage configuration ( voldemort config ) ; this . rocks db store = ( rocks db storage engine ) rocks db config . get store ( test utils . make store definition ( __str__ ) @$ test utils . make single node routing strategy ( ) ) ; random = new random ( ) ;,values test
instantiate new headless analyzer and parse <PLACE_HOLDER> .,headless analyzer analyzer = headless analyzer . get loggable instance ( log file @$ script log file @$ true ) ; headless options options = analyzer . get options ( ) ; parse options ( options @$ args @$ option start index @$ ghidraurl @$ files to import ) ;,headless analyzer
check if any parameter is referenced . if so @$ user must have read <PLACE_HOLDER> on the parameter context,for ( final string proposed property value : proposed properties . values ( ) ) { parameter token list token list = parameter parser . parse tokens ( proposed property value ) ; if ( ! token list . to reference list ( ) . is empty ( ) ) { references parameter = true ; break ; } },user read
in case we have a pending <PLACE_HOLDER> @$ <PLACE_HOLDER> now . see m pending <PLACE_HOLDER> for more information .,synchronized ( this ) { key = m pending repin . get or default ( uid @$ - __num__ ) ; if ( key == - __num__ ) { return ; } m pending repin . remove ( uid ) ; },repin see
test trace signature uniqueness touch the trace <PLACE_HOLDER>,fs . delete ( test trace file @$ false ) ;,uniqueness touch
fair scheduler does n't support this <PLACE_HOLDER> @$ set capacity scheduler as the scheduler for this <PLACE_HOLDER> .,conf . set ( yarn configuration . rm_scheduler @$ capacity scheduler . class . get name ( ) ) ; m clock = mock ( clock . class ) ; mcs = mock ( capacity scheduler . class ) ; when ( mcs . get resource calculator ( ) ) . then return ( rc ) ; lm = mock ( rm node labels manager . class ) ; try { when ( lm . is exclusive node label ( any string ( ) ) ) . then return ( true ) ; } catch ( io exception e ) { },scheduler support
offset the start so keys do n't overlap and cause constraint <PLACE_HOLDER>,m_rangemin = m_rows * topicnum ;,keys overlap
close the fd via the parent stream 's close <PLACE_HOLDER> . the parent will reinvoke our close <PLACE_HOLDER> @$ which is defined in the superclass abstract interruptible channel @$ but the is open logic in that <PLACE_HOLDER> will prevent this <PLACE_HOLDER> from being reinvoked .,if ( parent != null ) { ( ( java . io . closeable ) parent ) . close ( ) ; } else { nd . close ( fd ) ; },method prevent
if there was more than one @$ see <PLACE_HOLDER> one can take us forward the most words,if ( candidates > __num__ ) { if ( f iter . get index ( ) < range end ) { found best : do { int words matched = __num__ ; if ( words [ ( words found + __num__ ) % thai_lookahead ] . candidates ( f iter @$ f dictionary @$ range end ) > __num__ ) { if ( words matched < __num__ ) { words [ words found % thai_lookahead ] . mark current ( ) ; words matched = __num__ ; } if ( f iter . get index ( ) >= range end ) { break found best ; } do { if ( words [ ( words found + __num__ ) % thai_lookahead ] . candidates ( f iter @$,one take
if root then we can create any <PLACE_HOLDER> at home or public <PLACE_HOLDER>s,when ( tree mocked . is root ( ) ) . then return ( true ) ; when ( tree mocked . get path ( ) ) . then return ( new directory ) ; when ( repository . find directory ( any string ( ) ) ) . then return ( lazy ) ; when ( repository . create repository directory ( tree mocked @$ new directory ) ) . then call real method ( ) ; assert equals ( __str__ @$ repository . create repository directory ( tree mocked @$ new directory ) . get path ( ) ) ;,root create
if the reference does not match any <PLACE_HOLDER> or static page @$ throw an exception .,throw new illegal argument exception ( __str__ + name + __str__ + matcher . group ( ) ) ;,reference match
let 's define the schema of the data that we want to import the <PLACE_HOLDER> in which columns are defined here should match the <PLACE_HOLDER> in which they appear in the input data,schema input data schema = new schema . builder ( ) . add column string ( __str__ ) . add column categorical ( __str__ @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ) . add column string ( __str__ ) . build ( ) ;,order match
only one shard should have all imported <PLACE_HOLDER> @$ since we have the same routing for both <PLACE_HOLDER>,response = execute ( __str__ ) ; assert that ( response . rows ( ) [ __num__ ] [ __num__ ] @$ is ( __num__ ) ) ;,shard have
units can not be new because wdtk represents <PLACE_HOLDER> as strings already,return null ;,wdtk represents
cassandra connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support
striped executor uses a custom <PLACE_HOLDER> .,if ( striped exec svc != null ) { monitor striped pool ( striped exec svc ) ; },executor uses
server provided the first <PLACE_HOLDER>,if ( sasl auth type . has challenge ( ) ) { challenge token = sasl auth type . get challenge ( ) . to byte array ( ) ; sasl auth type = sasl auth . new builder ( sasl auth type ) . clear challenge ( ) . build ( ) ; } else if ( sasl client . has initial response ( ) ) { challenge token = new byte [ __num__ ] ; },server provided
<PLACE_HOLDER> str not str <PLACE_HOLDER> since shows up in intelli j next to <PLACE_HOLDER>,vec [ ] vecs = vecs ( ) ; string s [ ] = new string [ vecs . length ] ; for ( int i = __num__ ; i < vecs . length ; ++ i ) s [ i ] = vecs [ i ] . get_type_str ( ) ; return s ;,types str
null separator means use <PLACE_HOLDER>,if ( separator chars == null ) { while ( i < len ) { if ( character . is whitespace ( str . char at ( i ) ) ) { if ( match || preserve all tokens ) { last match = true ; if ( size plus1 ++ == max ) { i = len ; last match = false ; } list . add ( str . substring ( start @$ i ) ) ; match = false ; } start = ++ i ; continue ; } last match = false ; match = true ; i ++ ; } } else if ( separator chars . length ( ) == __num__ ) { final char sep = separator chars . char at,separator means
remove the register shutdown hook which disconnects the <PLACE_HOLDER> from the distributed system upon jvm shutdown,remove shutdown hook ( ) ; logger . info ( __str__ ) ; logging session . shutdown ( ) ;,which disconnects
now we know that both implementation yielded same <PLACE_HOLDER>,if ( got status == - __num__ ) { if ( ! got . to string ( ) . equals ( exp . to string ( ) ) ) { errln ( __str__ + refidna name + __str__ + uidna name + __str__ + exp + __str__ + got + __str__ + prettify ( src ) + __str__ + options ) ; } } else { logln ( __str__ + refidna name + __str__ + uidna name + __str__ + prettify ( src ) + __str__ + options ) ; },implementation yielded
nm 1 do 50 <PLACE_HOLDER>,donm heartbeat ( rm1 @$ nm1 . get node id ( ) @$ __num__ ) ; check num of containers in an app on given node ( __num__ @$ nm1 . get node id ( ) @$ cs . get application attempt ( am2 . get application attempt id ( ) ) ) ; report nm1 = rm1 . get resource scheduler ( ) . get node report ( nm1 . get node id ( ) ) ; assert . assert equals ( __num__ * gb @$ report nm1 . get used resource ( ) . get memory size ( ) ) ; assert . assert equals ( __num__ * gb @$ report nm1 . get available resource ( ) . get memory size ( ) ) ;,nm do
need to set nodelay or else batches larger than mtu can trigger 40 ms nagling <PLACE_HOLDER> .,conf copy . set boolean ( common configuration keys public . ipc_client_tcpnodelay_key @$ true ) ; rpc . set protocol engine ( conf copy @$ q journal protocolpb . class @$ protobuf rpc engine . class ) ; return security util . do as login user ( new privileged exception action < q journal protocol > ( ) { @ override public q journal protocol run ( ) throws io exception { rpc . set protocol engine ( conf copy @$ q journal protocolpb . class @$ protobuf rpc engine . class ) ; q journal protocolpb pbproxy = rpc . get proxy ( q journal protocolpb . class @$ rpc . get protocol version ( q journal protocolpb . class ) @$ addr @$ conf copy ),need trigger
google analytics uses no grouping <PLACE_HOLDER>,conversion meta . set grouping symbol ( null ) ;,analytics uses
server did not perform the <PLACE_HOLDER> @$ so do n't leave an invalid entry here,if ( owner . get concurrency checks enabled ( ) && event . no version received from server ( ) ) { if ( is debug enabled ) { logger . debug ( __str__ @$ event ) ; } return false ; },server perform
now reptable has 2000 <PLACE_HOLDER> and parttable has 252 <PLACE_HOLDER>,save tables with path ( client @$ testnonce + __str__ @$ snapshots path ) ; client . close ( ) ;,reptable has
check that null pd array throws <PLACE_HOLDER>,try { new access control context ( null ) ; throw new exception ( __str__ ) ; } catch ( exception e ) { if ( ! ( e instanceof null pointer exception ) ) { throw new exception ( __str__ ) ; } },array throws
the activity manager only throws security exceptions @$ so let 's log all <PLACE_HOLDER> .,if ( ! ( e instanceof security exception ) ) { slog . wtf ( tag @$ __str__ @$ e ) ; } throw e ;,"s" log
the user has a preferred <PLACE_HOLDER> set and it just became available @$ thus setting it as active,if ( ( configured handler != null ) && configured handler . equals ( handler . get class ( ) . get name ( ) ) ) { set active popup message handler ( handler ) ; },user has
test fact handle <PLACE_HOLDER>,assert true ( fact handle == connected fact handle ) ; assert true ( ! ( fact handle == disconnected fact handle ) ) ;,fact handle
the container fills its <PLACE_HOLDER> so we can use it orientation if it has one specified ; otherwise we prefer to use the orientation of its topmost child that has one specified and fall back on this container 's unset or unspecified value as a candidate if none of the children have a better candidate for the orientation .,if ( m orientation != screen_orientation_unset && m orientation != screen_orientation_unspecified ) { return m orientation ; } for ( int i = m children . size ( ) - __num__ ; i >= __num__ ; -- i ) { final window container wc = m children . get ( i ) ; final int orientation = wc . get orientation ( candidate == screen_orientation_behind ? screen_orientation_behind : screen_orientation_unset ) ; if ( orientation == screen_orientation_behind ) { candidate = orientation ; continue ; } if ( orientation == screen_orientation_unset ) { continue ; } if ( wc . fills parent ( ) || orientation != screen_orientation_unspecified ) { return orientation ; } },container fills
the mediacodec process has changed @$ clean up the old <PLACE_HOLDER> and client before we boost the new process @$ so that the state is left clean if things go wrong .,m boosted pid = - __num__ ; if ( m client != null ) { try { m client . unlink to death ( m death recipient @$ __num__ ) ; } catch ( exception e ) { } finally { m client = null ; } } try { client . link to death ( m death recipient @$ __num__ ) ; log . i ( tag @$ __str__ + pid + __str__ + process . thread_group_top_app ) ; process . set process group ( pid @$ process . thread_group_top_app ) ; m boosted pid = pid ; m client = client ; return package manager . permission_granted ; } catch ( exception e ) { log . e ( tag @$ __str__ + e ) ;,state clean
loose signatures allow a return <PLACE_HOLDER> of object ...,return impl method . return type . equals ( __str__ ) || ( impl method . return type . equals ( __str__ ) && sdk method . return type . ends with ( __str__ ) ) ;,signatures allow
simple counter does n't receive the <PLACE_HOLDER> after simple is removed from the composite,assert that ( simple counter . count ( ) ) . is equal to ( __num__ ) ; composite . add ( simple ) ; composite counter . increment ( ) ;,counter receive
does offset exceed buffer <PLACE_HOLDER> or negative ?,if ( offset > data . limit ( ) || offset < __num__ ) { throw new assertion error ( ) ; },offset exceed
history guru constructor needs environment <PLACE_HOLDER> so no locking is done here .,history guru hist guru = history guru . get instance ( ) ;,constructor needs
check that the memcache contains the <PLACE_HOLDER> @$ and the <PLACE_HOLDER> data store contains the <PLACE_HOLDER>,caching session data store ds = ( caching session data store ) context . get session handler ( ) . get session cache ( ) . get session data store ( ) ; assert not null ( ds ) ; session data store persistent store = ds . get session store ( ) ; session data map data map = ds . get session data map ( ) ;,memcache contains
the context could hold a <PLACE_HOLDER> to a wgl surface data @$ which in turn has a <PLACE_HOLDER> back to this wgl graphics config @$ so in order for this instance to be disposed we need to break the connection,ogl render queue rq = ogl render queue . get instance ( ) ; rq . lock ( ) ; try { ogl context . invalidate current context ( ) ; } finally { rq . unlock ( ) ; },context hold
someone else wants to subscribe to our presence . ask <PLACE_HOLDER> for approval,swing utilities . invoke later ( ( ) -> { try { subscription request ( presence . get from ( ) . as bare jid ( ) ) ; } catch ( smack exception . not connected exception | interrupted exception e ) { log . warning ( __str__ + presence . get from ( ) @$ e ) ; } } ) ; break ; case unsubscribe :,someone ask
do null byte buffer test <PLACE_HOLDER>,if ( ! null byte buffer test ( the mac ) ) { system . out . println ( __str__ ) ; return false ; } return true ;,byte buffer
only play this sound if the feet hit the <PLACE_HOLDER>,if ( event . get character relative position ( ) . y == __num__ && character sounds . last sound time + min_time < time . get game time in ms ( ) ) { boolean old block is liquid = event . get old block ( ) . is liquid ( ) ; boolean new block is liquid = event . get new block ( ) . is liquid ( ) ; static sound sound = null ; if ( ! old block is liquid && new block is liquid ) { sound = random . next item ( character sounds . enter water sounds ) ; } else if ( old block is liquid && ! new block is liquid ) { sound = random .,feet hit
use the derived mean total <PLACE_HOLDER> as a side input,p collection < kv < string @$ integer > > filtered = sum scores . apply ( __str__ @$ par do . of ( new do fn < kv < string @$ integer > @$ kv < string @$ integer > > ( ) { private final counter num spammer users = metrics . counter ( __str__ @$ __str__ ) ; @ process element public void process element ( process context c ) { integer score = c . element ( ) . get value ( ) ; double gmc = c . side input ( global mean score ) ; if ( score > ( gmc * score_weight ) ) { log . info ( __str__ + c . element ( ) . get key ( ),derived mean
begin pulse . note that it 's very important that the pulse finished callback be invoked when we 're done so that the caller can drop the pulse <PLACE_HOLDER> .,m pulse callback = callback ; m pulse reason = reason ;,caller drop
check if listener @$ defined by classname @$ received all <PLACE_HOLDER>,list < activiti event > events = static test activiti event listener . get events received ( ) ; assert false ( events . is empty ( ) ) ; boolean insert found = false ; boolean delete found = false ; for ( activiti event e : events ) { if ( activiti event type . entity_created == e . get type ( ) ) { insert found = true ; } else if ( activiti event type . entity_deleted == e . get type ( ) ) { delete found = true ; } } assert true ( insert found ) ; assert true ( delete found ) ;,listener received
first iteration will not have any <PLACE_HOLDER> or messages every other iteration may have <PLACE_HOLDER> and messages for each partition of vertices emit a view and their outgoing messages,final java pairrdd < object @$ view outgoing payload < m > > view outgoingrdd = ( ( null == view incomingrdd ) ? graphrdd . map values ( vertex writable -> new tuple2 < > ( vertex writable @$ optional . < view incoming payload < m > > absent ( ) ) ) : graphrdd . left outer join ( view incomingrdd ) ) . map partitions to pair ( partition iterator -> { kryo shim service loader . apply configuration ( graph computer configuration ) ; if ( ! partition iterator . has next ( ) ) { return collections . empty iterator ( ) ; } final vertex program < m > worker vertex program = vertex program . create vertex program ( hadoop,iteration have
exception should cause a <PLACE_HOLDER>,system . err . println ( ) ;,exception cause
which otherwise can cause <PLACE_HOLDER> with adding multiple entries to an ordered linked hash map,return new server web exchange matcher ( ) { @ override public mono < match result > matches ( server web exchange exchange ) { return server web exchange matcher . match result . match ( ) ; } } ;,which cause
one observer is the use <PLACE_HOLDER> group . the other observer removes the use <PLACE_HOLDER> upon the lifecycle 's destruction .,assert that ( m lifecycle . get observer count ( ) ) . is equal to ( __num__ ) ;,observer removes
start playback and wait until playback reaches second <PLACE_HOLDER> .,action schedule action schedule = new action schedule . builder ( __str__ ) . pause ( ) . seek ( __num__ ) . wait for seek processed ( ) . seek ( __num__ ) . seek ( __num__ ) . wait for playback state ( player . state_ready ) . seek ( __num__ ) . play until start of window ( __num__ ) . seek ( __num__ ) . seek ( __num__ ) . play ( ) . build ( ) ;,playback reaches
fire should not purge <PLACE_HOLDER>,assert equals ( __num__ @$ test harness . num keyed state entries ( ) ) ;,fire purge
it makes some sense to return true @$ as no <PLACE_HOLDER> implies all shall pass the <PLACE_HOLDER> @$ but if this returns true @$ then any other <PLACE_HOLDER>s can be used as the source of the data to <PLACE_HOLDER> @$ which does n't make sense if this is meant to only be used by itself .,return false ;,all pass
updates global qname set error <PLACE_HOLDER> in the dom error wrapper so if error occurs we can report an error <PLACE_HOLDER>,f configuration . f error handler wrapper . f current node = node ; f current node = node ;,qname set
unknown method @$ let the delegate return a usual <PLACE_HOLDER> .,if ( method == null ) { return delegate ( ) . serve ( ctx @$ req ) ; },delegate return
the owning rule 's divisor controls the <PLACE_HOLDER> of this substitution : rather than keeping a backpointer to the rule @$ we keep a copy of the divisor,this . divisor = rule . get divisor ( ) ; if ( divisor == __num__ ) { throw new illegal state exception ( __str__ + divisor + __str__ + description . substring ( __num__ @$ pos ) + __str__ + description . substring ( pos ) ) ; },divisor controls
if mode is add <PLACE_HOLDER> check if the repository name does not exist in the repository list <PLACE_HOLDER> close this dialog if mode is edit <PLACE_HOLDER> check if the repository name is the same as before if not check if the new name does not exist in the repository . otherwise return true to this method @$ which will mean that repository already exist,if ( meta . get name ( ) != null ) { if ( mode == mode . add ) { if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ; } else { display repository already exist message ( meta . get name ( ) ) ; } } else { if ( master repository name . equals ( meta . get name ( ) ) ) { repository meta = meta ; hide ( ) ; } else if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ;,repository exist
this is the first thread which looks up the <PLACE_HOLDER> this host or the cache entry for this host has been expired so this thread should do the lookup .,try { for ( name service name service : name services ) { try { addresses = name service . lookup all host addr ( host ) ; success = true ; break ; } catch ( unknown host exception uhe ) { if ( host . equals ignore case ( __str__ ) ) { inet address [ ] local = new inet address [ ] { impl . loopback address ( ) } ; addresses = local ; success = true ; break ; } else { addresses = unknown_array ; success = false ; ex = uhe ; } } } if ( req addr != null && addresses . length > __num__ && ! addresses [ __num__ ] . equals ( req addr ) ),which looks
j label contains html <PLACE_HOLDER>,if ( at != null ) { return at . get before index ( part @$ index ) ; },label contains
assert that only elements in the specified bucket shows up @$ and each element shows up 3 <PLACE_HOLDER> .,int bucket count = __num__ ; set < long > expected ids = long stream . range ( __num__ @$ row count ) . filter ( x -> bucket ids . contains ( to int exact ( x % bucket count ) ) ) . boxed ( ) . collect ( to immutable set ( ) ) ;,elements shows
we can inherit parent key serde if user do not provide specific <PLACE_HOLDER>,return new k table impl < k @$ change < vr > @$ vr > ( k tablek table join node . node name ( ) @$ k tablek table join node . key serde ( ) @$ k tablek table join node . value serde ( ) @$ all source nodes @$ k tablek table join node . queryable store name ( ) @$ k tablek table join node . join merger ( ) @$ k tablek table join node @$ builder ) ;,user provide
it is strange that an operation set presence does not have a presence <PLACE_HOLDER> so it may be safer to not mess with it .,forget presence status ( pps ) ; presence . remove provider presence status listener ( presence status listener ) ;,presence have
confirm that peer with state a will reject replication <PLACE_HOLDER> .,verify replication request rejection ( util1 @$ true ) ; verify replication request rejection ( util2 @$ false ) ; util1 . get admin ( ) . disable replication peer ( peer_id ) ; write ( util1 @$ __num__ @$ __num__ ) ; thread . sleep ( __num__ ) ;,peer reject
top left bottom left bottom <PLACE_HOLDER> top <PLACE_HOLDER>,top card . set card vertices ( new float [ ] { __num__ @$ bitmap . get height ( ) @$ __num__ @$ __num__ @$ bitmap . get height ( ) / __num__ @$ __num__ @$ bitmap . get width ( ) @$ bitmap . get height ( ) / __num__ @$ __num__ @$ bitmap . get width ( ) @$ bitmap . get height ( ) @$ __num__ } ) ;,bottom left
the current system does not meet our <PLACE_HOLDER> to disconnect and call recursively to get a new system .,if ( need new system ) { get log writer ( ) . info ( __str__ ) ; disconnect fromds ( ) ; get system ( props ) ; },system meet
this version gives a <PLACE_HOLDER> that rest li runs,request < string > req = builders . < string > action ( __str__ ) . build ( ) ; get client ( ) . send request ( req ) . get response ( ) ;,version gives
this should only be called for level 1 do ms @$ so we do n't have to worry about namespace issues . in later levels @$ it 's possible for a dom to have two attrs with the same node name but different namespaces @$ and we 'd need to get get attribute node ns ... but later levels also have attr.get owner <PLACE_HOLDER>,attr check = elem . get attribute node ( attr . get node name ( ) ) ; if ( check == attr ) parent = elem ; if ( null == parent ) { for ( node node = elem . get first child ( ) ; null != node ; node = node . get next sibling ( ) ) { if ( node . element_node == node . get node type ( ) ) { parent = locate attr parent ( ( element ) node @$ attr ) ; if ( null != parent ) break ; } } } return parent ;,levels have
to prevent concurrent remove see <PLACE_HOLDER> 41646,synchronized ( this . overflow map ) { overflow oplog oplog = get child ( ( int ) oplog id ) ; if ( oplog != null ) { oplog . remove ( dr @$ entry ) ; } },remove see
build the path stack <PLACE_HOLDER> to compare,string [ ] from path stack = get path stack ( from path ) ; string [ ] to path stack = get path stack ( to path ) ; if ( __num__ < to path stack . length && __num__ < from path stack . length ) { if ( ! from path stack [ __num__ ] . equals ( to path stack [ __num__ ] ) ) { return get path ( arrays . as list ( to path stack ) ) ; } } else { return get path ( arrays . as list ( to path stack ) ) ; },path stack
skip it @$ the for loop will skip its <PLACE_HOLDER>,i = i + __num__ ;,the skip
test pascal 255 select the fourth <PLACE_HOLDER>,select rows ( table @$ addr ( __num__ ) ) ; selected row = table . get selected row ( ) ; perform action ( make string action @$ model ) ;,255 select
throws a class not found <PLACE_HOLDER> if not found .,class < ? > cls = class . for name ( ci class ) ;,throws found
ensure that the rvv has recorded the <PLACE_HOLDER>,distributed region r = ( distributed region ) get cache ( ) . get region ( region name ) ; if ( ! r . get version vector ( ) . contains ( xid @$ __num__ ) ) { get log writer ( ) . info ( __str__ + r . get version vector ( ) . full to string ( ) ) ; ( ( local region ) r ) . dump backing map ( ) ; } assert true ( r . contains key ( __str__ ) ) ;,rvv recorded
make a third request to be sure the proxy does not mix <PLACE_HOLDER>,content response response3 = client . new request ( __str__ @$ server connector . get local port ( ) ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( __num__ @$ response3 . get status ( ) ) ; assert true ( response3 . get headers ( ) . contains key ( proxied_header ) ) ; client2 . stop ( ) ;,proxy mix
create a new item list which contain a new adapter item object the id of the new item is changed @$ and will be treated as a new item according to the rule we set in the callback . this test case is to verify the get change payload method still honor the <PLACE_HOLDER> we set up to judge new item,m items . clear ( ) ; adapter item new item = new adapter item ( __num__ @$ __str__ @$ __str__ ) ; m items . add ( new item ) ;,the honor
media controller.set playlist does not ensure the <PLACE_HOLDER> of the items .,for ( int i = __num__ ; i < list . size ( ) ; i ++ ) { assert equals ( list . get ( i ) @$ m player . m playlist . get ( i ) . get media id ( ) ) ; },media ensure
filter enabled @$ injection enabled @$ <PLACE_HOLDER> not expected,run test ( true @$ true ) ;,injection enabled
frame length exceeded the <PLACE_HOLDER>,if ( rbytes > max frame size ) { throw new too long frame exception ( ) ; } else { return null ; },length exceeded
set asynchronous mode @$ since async caller extends <PLACE_HOLDER>,client . set asynchronous mode ( true ) ;,caller extends
in case of <PLACE_HOLDER> shared sd has the base <PLACE_HOLDER> and partition has relative <PLACE_HOLDER>,for ( partition orig part : orig partitions ) { assert . assert equals ( __str__ @$ orig part . get sd ( ) . get location ( ) @$ sharedsd . get location ( ) + partition withoutsds . get ( i ) . get relative path ( ) ) ; assert . assert null ( __str__ @$ partition withoutsds . get ( i ) . get values ( ) ) ; assert . assert null ( __str__ @$ partition withoutsds . get ( i ) . get parameters ( ) ) ; i ++ ; },location has
verify quorum entry chooses active <PLACE_HOLDER>,membership state quorum entry = get namenode registration ( record . get nameservice id ( ) @$ record . get namenode id ( ) ) ; assert not null ( quorum entry ) ; assert equals ( routers [ __num__ ] @$ quorum entry . get router id ( ) ) ;,entry chooses
behind the scenes the server is storing <PLACE_HOLDER> in a file @$ we read it directly here,file file = new file ( __str__ + id + __str__ ) ; return file utils . read file to string ( file @$ __str__ ) ;,server storing
if locality is enabled @$ then fetch tablet <PLACE_HOLDER>,for ( range range : split ranges ) { if ( fetch tablet locations ) { tablet splits . add ( new tablet split metadata ( get tablet location ( table name @$ range . get start key ( ) ) @$ immutable list . of ( range ) ) ) ; } else { tablet splits . add ( new tablet split metadata ( optional . empty ( ) @$ immutable list . of ( range ) ) ) ; } },then fetch
create a binder that will let the activity ui send <PLACE_HOLDER> to the service,mqtt service binder = new mqtt service binder ( this ) ;,ui send
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized . get ( ) ) ;,scheduler captures
dispatching to empty statement will not call back <PLACE_HOLDER>or @$ must call our <PLACE_HOLDER> empty statement explicitly,if ( else block instanceof empty statement ) { visit empty statement ( ( empty statement ) else block ) ; } else { else block . visit ( this ) ; },statement call
we are creating filter here so should not be returning <PLACE_HOLDER> . not sure why calcite return <PLACE_HOLDER>,rex node b = builder . literal ( true ) ; switch ( logic ) { case true_false_unknown : b = e . rel . get cluster ( ) . get rex builder ( ) . make null literal ( sql type name . boolean ) ; case unknown_as_true : operands . add ( builder . call ( sql std operator table . less_than @$ builder . field ( __str__ @$ __str__ ) @$ builder . field ( __str__ @$ __str__ ) ) @$ b ) ; break ; },filter returning
r and s each occupy half the <PLACE_HOLDER>,byte [ ] res = new byte [ k << __num__ ] ; system . arraycopy ( br @$ __num__ @$ res @$ k - br . length @$ br . length ) ; system . arraycopy ( bs @$ __num__ @$ res @$ res . length - bs . length @$ bs . length ) ; return res ;,each occupy
the new reference should have the cloned region <PLACE_HOLDER> as parent @$ if it is a clone .,string cloned region name = bytes . to string ( regions map . get ( bytes . to bytes ( snapshot region name ) ) ) ; if ( cloned region name == null ) cloned region name = snapshot region name ;,reference have
return this instance of local service so clients can call public <PLACE_HOLDER> .,return local service . this ;,clients call
initialize the record reader add a <PLACE_HOLDER> @$ to extract the name,record reader . initialize ( train ) ;,reader add
recursively call get current <PLACE_HOLDER> which will create a new <PLACE_HOLDER> and return it,return get current bucket ( ) ;,which create
this call could cross <PLACE_HOLDER> @$ we should keep the message tight,return volume state . node_ready ;,call cross
ensure sub visible to any new dest add <PLACE_HOLDER> for destination,subscriptions . put ( info . get consumer id ( ) @$ sub ) ; destinations lock . read lock ( ) . unlock ( ) ;,sub add
now try filling with black again @$ but it will come up as white because this fill rect wo n't validate the <PLACE_HOLDER> properly,g . set color ( color . black ) ; g . fill rect ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; buffered image bi = vi . get snapshot ( ) ; if ( bi . getrgb ( __num__ @$ __num__ ) != color . black . getrgb ( ) ) { throw new runtime exception ( __str__ + integer . to hex string ( bi . getrgb ( __num__ @$ __num__ ) ) + __str__ + integer . to hex string ( color . black . getrgb ( ) ) ) ; } system . out . println ( __str__ ) ;,rect validate
verify that the database connection being set to null throws a kettle <PLACE_HOLDER> with the following message .,try { ora bulk loader . verify database connection ( ) ; fail ( __str__ ) ; } catch ( kettle exception a kettle exception ) { assert that ( a kettle exception . get message ( ) @$ contains string ( __str__ ) ) ; },connection throws
string builder does not throw io <PLACE_HOLDER>,return buf . to string ( ) ;,builder throw
low ack should have no <PLACE_HOLDER> .,s . remote ack ( __num__ ) ; sz = s . size in bytes ( ) ; assert equals ( __num__ @$ sz ) ; listing = get sorted directory listing segments ( ) ; assert equals ( listing . size ( ) @$ __num__ ) ; s . become leader ( ) ; wait for master ( s ) ;,ack have
runtime exceptions encountered here <PLACE_HOLDER> up and are handled in replication source,pool . submit ( create replicator ( entries @$ i @$ replicate context . get timeout ( ) ) ) ; futures ++ ;,exceptions encountered
nm 2 do <PLACE_HOLDER> of heartbeats,rm node rm node2 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm2 . get node id ( ) ) ; scheduler node scheduler node2 = cs . get scheduler node ( nm2 . get node id ( ) ) ; cs . handle ( new node update scheduler event ( rm node2 ) ) ;,nm do
when test contains unfinished <PLACE_HOLDER>,verify ( mock ) ;,test contains
set the uid range for this request to the single uid of the requester @$ or to an empty set of ui ds if the caller has the appropriate <PLACE_HOLDER> and ui ds have not been set . this will overwrite any allowed ui ds in the requested capabilities . though there are no visible methods to set the ui ds @$ an app,restrict request uids for caller ( network capabilities ) ; if ( timeout ms < __num__ ) { throw new illegal argument exception ( __str__ ) ; } ensure valid ( network capabilities ) ; network request network request = new network request ( network capabilities @$ legacy type @$ next network request id ( ) @$ type ) ; network request info nri = new network request info ( messenger @$ network request @$ binder ) ; if ( dbg ) log ( __str__ + nri ) ; m handler . send message ( m handler . obtain message ( event_register_network_request @$ nri ) ) ; if ( timeout ms > __num__ ) { m handler . send message delayed ( m handler . obtain message (,caller has
this is used primarily for gtk l & f @$ which expands the <PLACE_HOLDER> to fit the track when it would otherwise be hidden .,if ( ui manager . get boolean ( __str__ ) ) { set thumb bounds ( itemx @$ itracky @$ itemw @$ itrackh ) ; } else { set thumb bounds ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; },which expands
do n't do anything here . the user did n't change the album <PLACE_HOLDER> .,if ( album artist edited == false ) { } else { try { tag . set field ( field key . album_artist @$ song album artist ) ; } catch ( key not found exception e ) { e . print stack trace ( ) ; } catch ( field data invalid exception e ) { e . print stack trace ( ) ; } catch ( no such element exception e ) { e . print stack trace ( ) ; } values . put ( db access helper . song_album_artist @$ song album artist ) ; },user change
special case @$ a collection with only a read <PLACE_HOLDER> we assume we can just add to the connection,if ( collection . class . is assignable from ( i . get property type ( ) ) ) { handled properties . add ( i . get name ( ) ) ; collection property value = ( collection ) i . read ( param ) ; if ( ! property value . is empty ( ) ) { list < deferred parameter > params = new array list < > ( ) ; for ( object c : property value ) { deferred parameter to add = load object instance ( c @$ existing @$ object . class ) ; params . add ( to add ) ; } setup steps . add ( new serialzation step ( ) { @ override public void handle ( method,collection read
if there is duplicate key @$ rollback previous put <PLACE_HOLDER>,if ( old value != null ) { map . put ( key @$ old value ) ; throw duplicate key exception ( key @$ old value @$ value ) ; },rollback put
if the source does not supply a checkpoint <PLACE_HOLDER> skip updating the state .,@ suppress warnings ( __str__ ) final checkpoint markt finished read checkpoint mark = ( checkpoint markt ) microbatch reader . get checkpoint mark ( ) ; byte [ ] coded checkpoint = coder helpers . to byte array ( finished read checkpoint mark @$ checkpoint coder ) ;,source supply
there is no presence op set . let 's check the connected cusax <PLACE_HOLDER> if available,if ( presence == null ) { operation set cusax utils cusax op set = provider . get operation set ( operation set cusax utils . class ) ; if ( cusax op set != null ) { protocol provider service linked cusax provider = cusax op set . get linked cusax provider ( ) ; if ( linked cusax provider != null ) { presence = linked cusax provider . get operation set ( operation set presence . class ) ; } } },let check
the new bounds should use the new type <PLACE_HOLDER> in place of the old,list < type > new bounds = new bounds buf . to list ( ) ; from = tvars ; to = new tvars . to list ( ) ; for ( ; ! new bounds . is empty ( ) ; new bounds = new bounds . tail ) { new bounds . head = subst ( new bounds . head @$ from @$ to ) ; } new bounds = new bounds buf . to list ( ) ;,bounds use
the current implementation expects the component 's scoped <PLACE_HOLDER> .,layout = layout state . create layout ( next . get scoped context ( ) @$ next @$ false ) ;,implementation expects
check snapshot copy of meta change file <PLACE_HOLDER>,i node file meta change file1s copy = child . as file ( ) ; assert true ( meta change file1s copy . is with snapshot ( ) ) ; assert false ( meta change file1s copy . is under construction ( ) ) ; assert equals ( replication_1 @$ meta change file1s copy . get file replication ( snapshot . current_state_id ) ) ; assert equals ( replication_1 @$ meta change file1s copy . get file replication ( snapshot1 . get id ( ) ) ) ; assert equals ( replication @$ meta change file1s copy . get file replication ( snapshot0 . get id ( ) ) ) ;,copy file
bar should have <PLACE_HOLDER> to foo from latest,data type dt = bar . get component ( __num__ ) . get data type ( ) ; assert true ( dt instanceof pointer ) ;,bar have
<PLACE_HOLDER> these elements using insertion <PLACE_HOLDER>,a [ e1 ] = t ; a [ e2 ] = a [ e1 ] ;,elements using
and return the <PLACE_HOLDER> for examination :,return create constraint transaction ;,and return
error will throw <PLACE_HOLDER> other than security <PLACE_HOLDER>,system . out . println ( __str__ ) ;,error throw
let 's create a default 'it ' <PLACE_HOLDER>,if ( parameters . length == __num__ ) { parameter it = new parameter ( class helper . object_type @$ __str__ @$ constant expression . null ) ; parameters = new parameter [ ] { it } ; variable ref = expression . get variable scope ( ) . get declared variable ( __str__ ) ; if ( ref != null ) it . set closure shared variable ( ref . is closure shared variable ( ) ) ; },"s" create
second frame contains the next <PLACE_HOLDER> of data,msg = zmq . recv ( stream @$ __num__ ) ; assert that ( msg @$ not null value ( ) ) ; bytes read += msg . size ( ) ; read . put ( msg . buf ( ) ) ;,frame contains
base invalidate operation is treated as destroy . when the invalidate comes through @$ the entry will no longer satisfy the <PLACE_HOLDER> and will need to be deleted .,if ( b_cq results_old value ) { cq event = message_type_local_destroy ; c query . mark as destroyed in cq result keys ( event key ) ; },entry satisfy
special case for bare arrays @$ change the <PLACE_HOLDER> of the param to the <PLACE_HOLDER> of the complex type .,if ( op . is array ( ) && _parameter style == wsdl operation . soap parameter style . bare ) { op . set name ( schema element . get attribute ( wsdl utils . name_attr ) @$ _wsdl types ) ; } resolved params . add ( op ) ;,case change
first call should return empty @$ second call after network should return the network <PLACE_HOLDER>,when ( persister . get record state ( bar code ) ) . then return ( record state . missing ) ; when ( persister . write ( bar code @$ network1 ) ) . then return ( single . just ( true ) ) ; store . get ( bar code ) . test ( ) . assert error ( sorry ) ; in order in order = in order ( fetcher @$ persister ) ; in order . verify ( persister @$ times ( __num__ ) ) . read ( bar code ) ; in order . verify ( fetcher @$ times ( __num__ ) ) . fetch ( bar code ) ; in order . verify ( persister @$ times ( __num__ ) ) .,call return
return this instance of my service so clients can call public <PLACE_HOLDER>,return media player service . this ;,clients call
we can know the size of the iterable . use an <PLACE_HOLDER> with a leading size field @$ followed by that many elements .,if ( iterable instanceof collection ) { collection < t > collection = ( collection < t > ) iterable ; observer . update ( __num__ ) ; for ( t elem : collection ) { element coder . register byte size observer ( elem @$ observer ) ; } } else { observer . update ( __num__ ) ; long count = __num__ ; for ( t elem : iterable ) { count += __num__ ; element coder . register byte size observer ( elem @$ observer ) ; } if ( count > __num__ ) { observer . update ( var int . get length ( count ) ) ; } observer . update ( __num__ ) ; },size use
alternate 2 : rename across mountpoints with same target . i.e . rename across alias mountpoints . note we compare the ur is . the ur is include the link <PLACE_HOLDER> . hence we allow renames across mount links as long as the mount links point to the same target .,if ( ! src uri . equals ( dst uri ) ) { throw new io exception ( __str__ ) ; } break ; case same_mountpoint :,ur include
if subscribe function invoke before start function @$ then update topic subscribe <PLACE_HOLDER> after initialization .,if ( subscription type == subscription type . subscribe ) { update topic subscribe info when subscription changed ( ) ; },topic subscribe
does application 's executor handle this <PLACE_HOLDER> ?,e . print stack trace ( ) ;,executor handle
postgre sql does not store <PLACE_HOLDER> @$ only the point in time,return new timestamp ( millis utc ) ;,sql store
we used to record use of unsupported class loaders @$ but we no longer do . discard such <PLACE_HOLDER> ; they will be deleted when we next write the file .,if ( unsupported_class_loader_context . equals ( class loader context ) ) { continue ; },use discard
first call should hit the <PLACE_HOLDER>,try { groups . get groups ( __str__ ) ; fail ( __str__ ) ; } catch ( io exception e ) { },call hit
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text specifies
filter should not remember this unrelated <PLACE_HOLDER>,m . visit ldc insn ( __str__ ) ; m . visit var insn ( opcodes . astore @$ __num__ ) ; m . visit var insn ( opcodes . aload @$ __num__ ) ;,filter remember
we only generate merkle tree once here @$ so it 's important to make sure the root hash matches the signed <PLACE_HOLDER> in the apk .,if ( ! arrays . equals ( expected root hash @$ generated root hash ) ) { throw new security exception ( __str__ + bytes to string ( generated root hash ) + __str__ + bytes to string ( expected root hash ) ) ; } int content size = shm buffer factory . get buffer limit ( ) ; shared memory shm = shm buffer factory . release shared memory ( ) ; if ( shm == null ) { throw new illegal state exception ( __str__ ) ; } if ( ! shm . set protect ( os constants . prot_read ) ) { throw new security exception ( __str__ ) ; } return pair . create ( shm @$ content size ) ;,hash matches
regis<PLACE_HOLDER>ra<PLACE_HOLDER>ion requires <PLACE_HOLDER>ha<PLACE_HOLDER> sub<PLACE_HOLDER>ype ex<PLACE_HOLDER>ends <PLACE_HOLDER>,@ suppress warnings ( __str__ ) type adapter < r > delegate = ( type adapter < r > ) subtype to delegate . get ( src type ) ;,subtype extends
client confirms <PLACE_HOLDER> via echo,confirm connection ( client socket @$ client connect future ) ; try { client socket . get session ( ) . get remote ( ) . send string ( __str__ ) ; end point endp = client socket . get end point ( ) ; endp . shutdown output ( ) ; final string orig close reason = __str__ ; client socket . get session ( ) . close ( status code . normal @$ orig close reason ) ; assert that ( __str__ @$ client socket . error latch . await ( __num__ @$ seconds ) @$ is ( true ) ) ; assert that ( __str__ @$ client socket . error . get ( ) @$ instance of ( eof exception . class ) ) ;,client confirms
<PLACE_HOLDER> payment api will add the <PLACE_HOLDER> configured ones to this list,return payment control plugin names ;,api add
check if the table file has <PLACE_HOLDER> to skip .,if ( this . io cxt ref . get current block start ( ) == __num__ ) { footer buffer = null ; path file path = this . io cxt ref . get input path ( ) ; partition desc part = null ; try { if ( path to partition info == null ) { path to partition info = utilities . get map work ( job conf ) . get path to partition info ( ) ; } part = hive file format utils . get from path recursively ( path to partition info @$ file path @$ io prepare cache . get ( ) . get partition desc map ( ) ) ; } catch ( assertion error ae ) { log . info,file has
bad value prevents <PLACE_HOLDER> to next .,trigger action in cell editor ( key event . vk_tab ) ; assert is editing field ( __num__ @$ dt col num ) ;,value prevents
global map of all known distinct <PLACE_HOLDER> : thus enable reuse of the same word string instance appearing multiple times in different synonyms sets,final map < string @$ string > distinct words = new hash map < > ( ) ; for ( final string f : files ) { file ff = new file ( path @$ f ) ; string line ; try { blocking queue < string > list = files . concurent line reader ( ff ) ; while ( ( line = list . take ( ) ) != files . poison_line ) { line = line . trim ( ) ; if ( line . length ( ) == __num__ || line . char at ( __num__ ) == __str__ ) continue ; if ( line . char at ( line . length ( ) - __num__ ) == __str__ ) line = line .,map known
discover the returned <PLACE_HOLDER> implementation if this finder returns a <PLACE_HOLDER>,if ( jpa finder proxy . return type . collection . equals ( finder descriptor . return type ) && finder descriptor . return class != collection . class ) { finder descriptor . return collection type = finder . return as ( ) ; try { finder descriptor . return collection type constructor = finder descriptor . return collection type . get constructor ( ) ; finder descriptor . return collection type constructor . set accessible ( true ) ; } catch ( no such method exception e ) { throw new runtime exception ( __str__ + finder descriptor . return collection type @$ e ) ; } },finder returns
java feeds off manual token <PLACE_HOLDER>,g . add edge ( __str__ @$ __str__ ) ;,java feeds
actually temp table does not support <PLACE_HOLDER> @$ cascade is not applicable here,if ( old_tbl != null ) { alter temp table ( dbname @$ tbl_name @$ old_tbl @$ new_tbl @$ null ) ; return ; },table support
endpoints do not directly forward content <PLACE_HOLDER> to next stage in the filter chain .,if ( new chunk != null ) { zuul req . buffer body contents ( new chunk ) ; if ( new chunk != chunk ) { chunk . release ( ) ; } if ( is filter awaiting body ( zuul req ) && zuul req . has complete body ( ) && ! ( endpoint instanceof proxy endpoint ) ) { invoke next stage ( filter ( endpoint @$ zuul req ) ) ; } },endpoints content
reset posted <PLACE_HOLDER> .,standalone test strategy . posted result = null ; when ( spawn action context . begin execution ( any ( ) @$ any ( ) ) ) . then ( ( invocation ) -> { file system utils . touch file ( actionb . resolve ( get exec root ( ) ) . get xml output path ( ) ) ; return spawn continuation . failed with exec exception ( new spawn exec exception ( __str__ @$ expected spawn result @$ false ) ) ; } ) ;,reset posted
this event expects a <PLACE_HOLDER> @$ so add that expected <PLACE_HOLDER> to the maps of pending events .,if ( request response pairs . contains key ( event . event id ) ) { for ( event manager . timed event pair p : request response pairs . get ( event . event id ) ) { pending responses . put ( p . m response @$ new pending response ( event . event id @$ event . time @$ p . m timeout millis @$ p . m name ) ) ; } },event expects
should use root entity <PLACE_HOLDER> by default,hql executor hql executor unaliased = new hql executor ( ) { protected query get query ( session s ) { return s . create query ( __str__ ) ; } } ;,use root
<PLACE_HOLDER> set change when new set excludes current <PLACE_HOLDER>,assert . assert equals ( relay puller . get servers ( ) @$ exp server info2 @$ __str__ ) ; do execute and change state ( relay puller @$ create set server message ( false @$ relay puller ) ) ; assert . assert equals ( relay puller . get current server idx ( ) == - __num__ @$ true @$ __str__ ) ; assert . assert equals ( relay puller . get curent server ( ) == null @$ true @$ __str__ ) ; assert . assert equals ( relay puller . get servers ( ) @$ exp server info3 @$ __str__ ) ; assert . assert equals ( relay puller . to tear conn after handling response ( ) @$ false @$ __str__ ) ; assert,set excludes
the table metadata has less or more <PLACE_HOLDER> than the event @$ which means the table structure has changed @$ so we need to trigger a refresh ...,if ( msg has missing columns || msg has additional columns ) { logger . info ( __str__ @$ replication column count @$ table column count ) ; return true ; },metadata has
finishing the tasks should also set the end <PLACE_HOLDER>,tasks = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . list ( ) ; assert equals ( __num__ @$ tasks . size ( ) ) ; for ( task task : tasks ) { task service . complete ( task . get id ( ) ) ; } historic activity instances = history service . create historic activity instance query ( ) . activity id ( __str__ ) . list ( ) ; assert equals ( __num__ @$ historic activity instances . size ( ) ) ; for ( historic activity instance historic activity instance : historic activity instances ) { assert not null ( historic activity instance . get end time ( ),tasks set
make sure that container is in running <PLACE_HOLDER> before sending increase request,nm1 . node heartbeat ( container id . get application attempt id ( ) @$ container id . get container id ( ) @$ container state . running ) ; rm1 . drain events ( ) ; am client . request container update ( container @$ update container request . new instance ( container . get version ( ) @$ container . get id ( ) @$ container update type . increase_resource @$ resource . new instance ( __num__ @$ __num__ ) @$ null ) ) ; it . remove ( ) ; allocate response = am client . allocate ( __num__ ) ; rm1 . drain events ( ) ; assert . assert equals ( __str__ @$ __num__ @$ allocate response . get allocated containers ( ),container running
custom grouping target task <PLACE_HOLDER> will be null if this stream is not custom stream grouping,if ( ! helper . is custom grouping empty ( ) ) { custom grouping target task ids = helper . choose tasks for custom stream grouping ( stream id @$ tuple ) ; },stream grouping
verify that container is actually running <PLACE_HOLDER> the rm..,rm container rm container = ( ( capacity scheduler ) scheduler ) . get application attempt ( container . get id ( ) . get application attempt id ( ) ) . getrm container ( container . get id ( ) ) ; assert . assert equals ( rm container state . running @$ rm container . get state ( ) ) ;,container running
the object that we 're modifying here is a copy of the original ! so let 's change the <PLACE_HOLDER> from relative to absolute by grabbing the file object ...,try { if ( ! utils . is empty ( file name ) ) { file object file object = kettlevfs . get file object ( space . environment substitute ( file name ) @$ space ) ; file name = resource naming interface . name resource ( file object @$ space @$ true ) ; } return null ; } catch ( exception e ) { throw new kettle exception ( e ) ; },let change
completing the task will end the process <PLACE_HOLDER>,task service . complete ( task . get id ( ) ) ; assert process ended ( proc id ) ;,task end
we must draw dividers ourselves if we want <PLACE_HOLDER> in a list,return new divider decoration ( get activity ( ) ) ;,dividers want
out lab now contains <PLACE_HOLDER>,xyz tolab ( out lab [ __num__ ] @$ out lab [ __num__ ] @$ out lab [ __num__ ] @$ out lab ) ;,lab contains
cui does n't support mini progress <PLACE_HOLDER> right now,painter . state . set ( size variant = size == size . mini ? size . small : size variant ) ;,cui support
parent view will correctly keep <PLACE_HOLDER> in bounds for us,rotation = get fold rotation ( ) ;,view keep
0 x 1002346 : p 1 repeatable <PLACE_HOLDER> contains p 2 repeatable <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ;,comment contains
in future could check current <PLACE_HOLDER> ... for now this should be enough :,return type deserializer . deserialize typed from array ( p @$ ctxt ) ;,future check
specialized entry point for zero argument execute <PLACE_HOLDER>,if ( arguments . length == __num__ ) { impl . execute void ( receiver ) ; } else { impl . execute void ( receiver @$ arguments ) ; },point execute
if the caller does not have <PLACE_HOLDER> to load the driver then skip it .,for ( driver info a driver : registered drivers ) { if ( is driver allowed ( a driver . driver @$ caller class loader ) ) { try { if ( a driver . driver . acceptsurl ( url ) ) { println ( __str__ + a driver . driver . get class ( ) . get name ( ) ) ; return ( a driver . driver ) ; } } catch ( sql exception sqe ) { } } else { println ( __str__ + a driver . driver . get class ( ) . get name ( ) ) ; } },caller have
complete task a which should ena<PLACE_HOLDER>le task <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name ( plan item instances @$ __str__ ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert plan item instance state ( plan item instances @$ __str__ @$ enabled @$ waiting_for_repetition ) ;,which enable
app 4 ca n't allocate its am <PLACE_HOLDER> on node 1 because app 3 already reserved its <PLACE_HOLDER> on node 1 .,scheduler . handle ( updatee1 ) ; assert equals ( __str__ @$ __num__ @$ app4 . getam resource ( ) . get memory size ( ) ) ; assert equals ( __str__ @$ __num__ @$ app4 . get live containers ( ) . size ( ) ) ; assert equals ( __str__ @$ __num__ @$ queue1 . get am resource usage ( ) . get memory size ( ) ) ; scheduler . update ( ) ;,app reserved
initialize state trackers for all map <PLACE_HOLDER> .,this . partial map progress = new float [ num maps ] ; this . map counters = new counters [ num maps ] ; for ( int i = __num__ ; i < num maps ; i ++ ) { this . map counters [ i ] = new counters ( ) ; } this . partial reduce progress = new float [ num reduces ] ; this . reduce counters = new counters [ num reduces ] ; for ( int i = __num__ ; i < num reduces ; i ++ ) { this . reduce counters [ i ] = new counters ( ) ; } this . num map tasks = num maps ; this . num reduce tasks = num reduces ;,trackers map
since filenames contain valuable <PLACE_HOLDER> @$ split the string right before the filename and truncate both halves .,final int last comp = filename . last index of ( sep ) ; final int split len = maxlen - ( len - last comp ) ; final int split pos = ( split len / __num__ ) - ( ell len / __num__ ) ; if ( split pos > __num__ ) { result . append ( filename . substring ( __num__ @$ ( split pos - ( ell len % __num__ ) ) + ( split len % __num__ ) ) ) ; result . append ( ellipsis ) ; result . append ( filename . substring ( ( last comp - split pos ) + ( ell len % __num__ ) ) ) ; } else { result . append ( filename . substring,filenames contain
period has special <PLACE_HOLDER>,if ( s instanceof period ) { return new comparable period ( ( period ) s ) ; } else { throw new illegal argument exception ( __str__ + s + __str__ ) ; },period has
we set a small batch size to ensure that we have multiple inflight <PLACE_HOLDER> per transaction . if it is left at the default @$ each transaction will have only one batch per partition @$ hence not testing the case with multiple inflights .,props . put ( producer config . batch_size_config @$ __str__ ) ; props . put ( producer config . max_in_flight_requests_per_connection @$ __str__ ) ; return new kafka producer < > ( props ) ;,transaction have
<PLACE_HOLDER> while other threads control the lock grantor future result terminate <PLACE_HOLDER> if other thread has already made us lock grantor terminate <PLACE_HOLDER> if this thread gets control of lock grantor future result,while ( ! own lock grantor future result ) { assert . assert holds lock ( this . destroy lock @$ false ) ; synchronized ( this . lock grantor id lock ) { if ( is currently or is making lock grantor ( ) ) { return ; } else if ( this . lock grantor future result != null ) { lock grantor future result ref = this . lock grantor future result ; } else { own lock grantor future result = true ; lock grantor future result ref = new future result ( this . dm . get cancel criterion ( ) ) ; if ( is debug enabled_dls ) { logger . trace ( log marker . dls_verbose @$ __str__ ) ; },grantor terminate
let 's give charles a <PLACE_HOLDER> @$ this time using method references,charles . set oca ( __num__ ) ; charles . set title ( __str__ ) ; check dirty tracking ( charles @$ __str__ @$ __str__ ) ;,"s" give
serialize and deserialize unconfigured build <PLACE_HOLDER> as string,simple module build target module = new simple module ( __str__ ) ; build target module . add serializer ( unconfigured build target . class @$ new to string serializer ( ) ) ; build target module . add deserializer ( unconfigured build target . class @$ new from string deserializer < unconfigured build target > ( unconfigured build target . class ) { @ override protected unconfigured build target _deserialize ( string value @$ deserialization context ctxt ) { return unconfigured build target parser . parse ( value @$ intern ) ; } } ) ; mapper . register module ( build target module ) ; mapper . register module ( forward relative path module ( ) ) ; return mapper ;,unconfigured build
we also need to let the framework know what type of <PLACE_HOLDER> happened . accessibility services may use this <PLACE_HOLDER> to provide appropriate feedback to the user .,m touch helper . send event for virtual view ( index @$ accessibility event . type_view_clicked ) ; return true ;,services use
delete the created entry @$ such that content provider will delete the <PLACE_HOLDER> .,content resolver . delete ( files uri @$ where @$ selection args ) ;,provider delete
only the good ones will be run and the latest does n't get <PLACE_HOLDER> because of the second,queue task future [ ] task future = new queue task future [ __num__ ] ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ;,latest get
client 1 is attached to bridge server data store 3 client 1 does not register <PLACE_HOLDER> but registers cq,client1 . invoke ( ( ) -> create client cache ( port3 @$ false @$ false @$ true ) ) ;,1 register
boundary symbol has one <PLACE_HOLDER>,rules with word [ boundary word id ] . add ( new int tagged word ( boundary word id @$ boundary tag id ) ) ;,symbol has
a short string referenced once should not have a <PLACE_HOLDER> .,test needed temps ( __str__ @$ __str__ @$ empty_string_set ) ;,referenced have
transfer send some asset issue to default account @$ to test if this transaction use the creator <PLACE_HOLDER> .,assert . assert true ( public methed . transfer asset ( to address @$ asset account id . to byte array ( ) @$ __num__ @$ transfer asset address @$ transfer asset create key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ; asset creator net = public methed . get account net ( asset012 address @$ blocking stub full ) ; asset transfer net = public methed . get account net ( transfer asset address @$ blocking stub full ) ; long creator after net used = asset creator net . get net used ( ) ; long transfer after free net used = asset transfer net . get free net used ( ) ; logger,transaction use
set the input to the <PLACE_HOLDER> of its output . bitcoin core does this but the step has no obvious purpose as the signature covers the hash of the prevout transaction which obviously includes the output <PLACE_HOLDER> already . perhaps it felt safer to him in some way @$ or is another leftover from how the code was written .,transaction input input = tx . inputs . get ( input index ) ; input . set script bytes ( connected script ) ; if ( ( sig hash type & __num__ ) == sig hash . none . value ) { tx . outputs = new array list < > ( __num__ ) ; for ( int i = __num__ ; i < tx . inputs . size ( ) ; i ++ ) if ( i != input index ) tx . inputs . get ( i ) . set sequence number ( __num__ ) ; } else if ( ( sig hash type & __num__ ) == sig hash . single . value ) { if ( input index >= tx . outputs . size,which includes
evaluate if the value matches the search <PLACE_HOLDER>,if ( string utils . contains ignore case ( value @$ search str ) ) { matches . add ( __str__ + descriptor . get name ( ) + __str__ + value ) ; },value matches
manually specified value takes <PLACE_HOLDER> over settings .,set enabled ( system properties . get boolean ( debug_sys_looper_stats_enabled @$ parser . get boolean ( settings_enabled_key @$ default_enabled ) ) ) ;,value takes
cleanup all jms <PLACE_HOLDER> .,for ( int i = __num__ ; i < num_workers ; i ++ ) { workers [ i ] . close ( ) ; } master session . close ( ) ; connection . close ( ) ;,cleanup jms
pairwise : init ouput col has <PLACE_HOLDER> @$ init ouput col is repeating @$ column 1 has <PLACE_HOLDER> @$ column 1 is repeating @$ column 2 has <PLACE_HOLDER> @$ column 2 is repeating,for ( boolean [ ] test matrix : new boolean [ ] [ ] { { true @$ true @$ false @$ true @$ true @$ true } @$ { false @$ false @$ true @$ false @$ false @$ false } @$ { true @$ false @$ true @$ false @$ true @$ true } @$ { true @$ true @$ true @$ true @$ false @$ false } @$ { false @$ false @$ false @$ true @$ true @$ false } @$ { false @$ true @$ false @$ false @$ false @$ true } } ) { string test case = template string ; test case = test case . replace all ( __str__ @$ __str__ + vector exp class name + create null,column has
here we expected that id 1 and id 2 will be reusable @$ even if they were n't marked as such in the previous session making changes to the tree entry where they live will update the <PLACE_HOLDER> and all of a sudden the reusable bits in that entry will matter when we want to allocate . this is why we now want to,mark deleted ( freelist @$ id3 ) ; final immutable long set reused = long sets . immutable . of ( freelist . next id ( ) @$ freelist . next id ( ) ) ; assert equals ( long sets . immutable . of ( id1 @$ id2 ) @$ reused @$ __str__ ) ;,bits update
the final orientation of this activity will change after moving to full <PLACE_HOLDER> . start freezing <PLACE_HOLDER> here to prevent showing a temporary full <PLACE_HOLDER> window .,if ( top != null && ! top . is configuration compatible ( parent config ) ) { top . start freezing screen locked ( top . app @$ config_screen_layout ) ; m service . move tasks to fullscreen stack ( m stack id @$ true ) ; return ; },orientation start
this dead code hushes <PLACE_HOLDER> .,return ;,code hushes
setup an injection dependency to inject the default access timeout <PLACE_HOLDER> in the singleton bean component create <PLACE_HOLDER>,configuration . get create dependencies ( ) . add ( new dependency configurator < singleton component create service > ( ) { @ override public void configure dependency ( service builder < ? > service builder @$ singleton component create service component create service ) throws deployment unit processing exception { service builder . add dependency ( default access timeout service . singleton_service_name @$ default access timeout service . class @$ component create service . get default access timeout injector ( ) ) ; } } ) ; return new singleton component create service ( configuration @$ this . ejb jar configuration @$ this . init on startup @$ depends on ) ;,service create
enter a synthetic class that is used to provide pro<PLACE_HOLDER> info for classes in ct.sym . this class does not have a class <PLACE_HOLDER> .,profile type = enter synthetic annotation ( __str__ ) ; method symbol m = new method symbol ( public | abstract @$ names . value @$ int type @$ profile type . tsym ) ; profile type . tsym . members ( ) . enter ( m ) ;,class have
somebody else did a failover concurrently try that <PLACE_HOLDER> now,synchronized ( lock ) { if ( ( old spi != null ) && ( old spi != spi ) ) { return spi ; } if ( service iterator == null ) { return null ; } while ( service iterator . has next ( ) ) { service s = service iterator . next ( ) ; if ( jce security . can use provider ( s . get provider ( ) ) == false ) { continue ; } try { object inst = s . new instance ( null ) ; if ( inst instanceof key generator spi == false ) { continue ; } key generator spi spi = ( key generator spi ) inst ; if ( reinit ) { if (,failover try
see if our node matches the given key <PLACE_HOLDER> according to the match attribute on xsl : key .,x path match expr = kd . get match ( ) ; double score = match expr . get match score ( xctxt @$ test node ) ; if ( score == kd . get match ( ) . match_score_none ) continue ; return dtm iterator . filter_accept ;,node matches
device does n't support <PLACE_HOLDER> @$ so remove pan profile on disconnect,if ( profile instanceof pan profile && ( ( pan profile ) profile ) . is local role nap ( m device ) ) { m local nap role connected = true ; },device support
if this mime type has an <PLACE_HOLDER> @$ customize the label,final char sequence label ; final string ext = mime map . get default ( ) . guess extension from mime type ( mime type ) ; if ( ! text utils . is empty ( ext ) && ext label id != - __num__ ) { label = res . get string ( ext label id @$ ext . to upper case ( locale . us ) ) ; } else { label = res . get string ( label id ) ; } return new mime type info ( icon . create with resource ( res @$ icon id ) @$ label @$ label ) ;,type has
' ! ' represents an interior <PLACE_HOLDER> that represents an icann entry in the map . ' ? ' represents a leaf <PLACE_HOLDER> @$ which represents an icann entry in map . ' : ' represents an interior <PLACE_HOLDER> that represents a private entry in the map ' @$ ' represents a leaf <PLACE_HOLDER> @$ which represents a private entry in the map .,if ( c == __str__ || c == __str__ || c == __str__ || c == __str__ ) { string domain = prefix_joiner . join ( stack ) ; if ( domain . length ( ) > __num__ ) { builder . put ( domain @$ public suffix type . from code ( c ) ) ; } },node represents
assert that for each now produces no <PLACE_HOLDER>,spliterator . for each remaining ( boxing adapter . apply ( e -> fail ( __str__ + e ) ) ) ;,each produces
note on catch : dom level 1 does not specify this method and the code will throw a no such method <PLACE_HOLDER>,if ( doctype != null ) { try { return doctype . get system id ( ) ; } catch ( error except ) { } },code throw
but the write entity is complete in ddl operations @$ instead ddl sets the write <PLACE_HOLDER> @$ so we use it to determine its lock mode @$ and first we check if the write <PLACE_HOLDER> was set,write entity . write type write type = we . get write type ( ) ; if ( write type == null ) { return lock mode ; } switch ( write type ) { case ddl_exclusive : return hive lock mode . exclusive ; case ddl_shared : return hive lock mode . shared ; case ddl_no_lock : return null ; default : return lock mode ; },ddl sets
if this mapping has a valid file <PLACE_HOLDER> then we close it,if ( fd . valid ( ) ) { try { nd . close ( fd ) ; } catch ( io exception ignore ) { } },mapping has
does the word contain a capitalized <PLACE_HOLDER> ?,boolean is in cap = false ; for ( int i = __num__ ; i < word . length ( ) ; i ++ ) { if ( character . is upper case ( word . char at ( i ) ) ) { is in cap = true ; break ; } } if ( is in cap ) return case_incap ;,word contain
if previous cert has a subject key identifier <PLACE_HOLDER> @$ use it to match on authority key identifier <PLACE_HOLDER> .,sel . set policy ( get matching policies ( ) ) ;,cert has
stop client read <PLACE_HOLDER>,client timeout handler . stop ( ) ; client conn . stop ( ) ; srv conn . stop ( ) ;,client read
viewport dimensions not <PLACE_HOLDER> . return the full <PLACE_HOLDER> of indices .,if ( viewport width == integer . max_value || viewport height == integer . max_value ) { return selected track indices ; },dimensions return
of the candidates @$ find the <PLACE_HOLDER> that meet the minimum protocol version we want to target . we could select the highest version we 've seen on the assumption that newer versions are always better but we do n't want to zap <PLACE_HOLDER> if they upgrade early . if we ca n't find any <PLACE_HOLDER> that have our preferred protocol version or better,int highest version = __num__ @$ preferred version = __num__ ;,peers find
current thread has <PLACE_HOLDER> and this policy entry has <PLACE_HOLDER> . see if policy entry <PLACE_HOLDER> match <PLACE_HOLDER> in current acc,for ( policy parser . principal entry pppe : entry ps ) { if ( pppe . is wildcard class ( ) ) { continue ; } if ( pppe . is wildcard name ( ) ) { if ( wildcard principal name implies ( pppe . principal class @$ principals ) ) { continue ; } if ( debug != null ) { debug . println ( __str__ ) ; } return ; } set < principal > p set = new hash set < > ( arrays . as list ( principals ) ) ; subject subject = new subject ( true @$ p set @$ collections . empty_set @$ collections . empty_set ) ; try { class loader cl = thread . current thread (,thread has
check to see if the existence of this <PLACE_HOLDER> matches the <PLACE_HOLDER> in meta,for ( region info r : regions ) { hbck region info hbi = hbck . get or create info ( r . get encoded name ( ) ) ; hbi . add server ( r @$ rsinfo ) ; },existence matches
in nm @$ each application has exactly one app log aggregator <PLACE_HOLDER> to handle the log aggregation . so @$ it is fine which multiple app log aggregator <PLACE_HOLDER> to update log aggregation status for its own application . this is why we are using read locker here .,this . read locker . lock ( ) ; try { app log aggregation status forrm recovery tracker = recovery statuses . get ( app id ) ; if ( tracker == null ) { application application = this . nm context . get applications ( ) . get ( app id ) ; if ( application == null ) { log . warn ( __str__ + app id + __str__ + __str__ + __str__ + __str__ ) ; return ; } app log aggregation status forrm recovery new tracker = new app log aggregation status forrm recovery ( log aggregation status @$ diagnosis ) ; new tracker . set last modified time ( update time ) ; new tracker . set finalized ( finalized ) ; recovery,app log
if we have prepared the insert @$ we do n't do it again . we asume that all the step insert statements come <PLACE_HOLDER> after the other .,if ( ps job attributes insert == null ) { string sql = database . get insert statement ( kettle database repository . table_r_job_attribute @$ table . get row meta ( ) ) ; ps job attributes insert = database . preparesql ( sql ) ; } database . set values ( table @$ ps job attributes insert ) ; database . insert row ( ps job attributes insert @$ use batch processing ) ; if ( log . is debug ( ) ) { log . log debug ( __str__ + code + __str__ ) ; } return id ;,statements come
then user wants all <PLACE_HOLDER> .,if ( tables to use == null ) { } else if ( tables to use . length != table map . length ) { throw new io exception ( __str__ ) ; },user wants
note that there is no grouping here @$ the bolt should not receive any tuple from spout i increase the <PLACE_HOLDER> of the bolt to check if it is correct when we have a <PLACE_HOLDER> greater than 1 .,topology builder . set bolt ( __str__ @$ new tick tuple test bolt ( ) @$ tick_tuple_bolt_parallelism ) . add configuration ( config . topology_tick_tuple_freq_secs @$ tick_tuple_cycle ) ; set < string > user define metrics = new hash set < string > ( ) ; user define metrics . add ( __str__ ) ; user define metrics . add ( __str__ ) ; j storm unit test validator validator = new j storm unit test metric validator ( user define metrics ) { @ override public boolean validate metrics ( map < string @$ double > metrics ) { double cycle = __num__ / ( metrics . get ( __str__ ) / tick_tuple_bolt_parallelism ) ; log . info ( __str__ + metrics . get ( __str__ ),tuple increase
here @$ we verify that the op size makes <PLACE_HOLDER> and that the data matches its checksum before attempting to construct an op . this is important because otherwise we may encounter an out of memory exception which could bring down the name node or journal node when reading garbage data .,int op length = in . read int ( ) + op_id_length + checksum_length ; if ( op length > max op size ) { throw new io exception ( __str__ + ( int ) op code byte + __str__ + op length + __str__ + max op size ) ; } else if ( op length < min_op_length ) { throw new io exception ( __str__ + ( int ) op code byte + __str__ + op length + __str__ + min_op_length ) ; } long txid = in . read long ( ) ;,size makes
our own copy of content mime <PLACE_HOLDER> .,final string [ ] mime types ;,copy mime
needed for settings changes which affect header <PLACE_HOLDER>,get table header ( ) . repaint ( ) ;,which affect
round this because having this floating may cause bad <PLACE_HOLDER>,m bar gap = math . round ( resources . get dimension ( r . dimen . dad_gap_between_bars ) ) ; m spin = resources . get boolean ( r . bool . dad_spin_bars ) ; m middle arrow size = resources . get dimension ( r . dimen . dad_middle_bar_arrow_size ) ; m paint . set style ( paint . style . stroke ) ; m paint . set stroke join ( paint . join . miter ) ; m paint . set stroke cap ( paint . cap . butt ) ; m paint . set stroke width ( m bar thickness ) ; m max cut for bar size = ( float ) ( m bar thickness / __num__ * math . cos ( arrow_head_angle,floating cause
check that the write table write operation latency does not exceed the configured <PLACE_HOLDER> .,if ( actual write latency > this . configured write table timeout ) { log . error ( __str__ @$ write table string name ) ; },latency exceed
bind and start to accept incoming <PLACE_HOLDER> .,_bootstrap . bind ( new inet socket address ( _port ) ) ;,bind accept
null should not impact <PLACE_HOLDER>,current min = double min kudaf . aggregate ( null @$ current min ) ; assert that ( new big decimal ( __num__ @$ new math context ( __num__ ) ) @$ equal to ( current min ) ) ;,null impact
writing will always give full date <PLACE_HOLDER>,string time zone = new simple date format ( __str__ ) . format ( date ) ; assert equals ( __str__ + time zone @$ formatted ) ; assert equals ( date @$ result ) ;,writing give
every other n appears n <PLACE_HOLDER> . the sum is therefore n x n .,assert equals ( a1 . int value ( ) * a1 . int value ( ) @$ sum . int value ( ) ) ;,n appears
firing the signal should now trigger the <PLACE_HOLDER> with the boundary event too,runtime service . signal event received ( __str__ ) ; assert equals ( __num__ @$ runtime service . create process instance query ( ) . count ( ) ) ; assert equals ( __num__ @$ task service . create task query ( ) . count ( ) ) ; assert equals ( __num__ @$ task service . create task query ( ) . task name ( __str__ ) . count ( ) ) ;,signal trigger
sync sending @$ will return a send <PLACE_HOLDER>,try { producer . send ( prepare message ( input ) ) ; collector . ack ( input ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; collector . report error ( e ) ; collector . fail ( input ) ; },sync return
during callback @$ the callback url is by definition not needed @$ but the saml 2 <PLACE_HOLDER>s does never allow this <PLACE_HOLDER> to be empty ...,saml data . put ( __str__ @$ callback url != null ? callback url : any_url ) ; settings builder builder = new settings builder ( ) ; return builder . from values ( saml data ) . build ( ) ;,settings allow
see if the step receives <PLACE_HOLDER> .,is receiving input = trans meta . find nr prev steps ( step meta ) > __num__ ;,step receives
null <PLACE_HOLDER> cache never returns the <PLACE_HOLDER> that was removed from the cache because it was never in the cache !,assert null ( session ) ;,cache returns
project name and enabled values should match disabled <PLACE_HOLDER> .,this . project name test . assert values ( enabled notification . project ( ) . name ( ) @$ disabled notification . project ( ) . name ( ) ) ; this . enabled switch test . assert values ( true @$ false ) ;,name match
we capture and set the context once the user provided observable <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,user provided
stop emitting at a certain point @$ because log rolling breaks the <PLACE_HOLDER> .,if ( num emitted >= testable topology . max_spout_emits ) { return ; },rolling breaks
this happens on the driver since the call below uses <PLACE_HOLDER> @$ and we have a version conflict with spark . or did .,return targets by tree andid . map values ( numeric targets -> stream support . stream ( numeric targets . spliterator ( ) @$ false ) . collect ( collectors . summarizing double ( f -> ( ( numeric feature ) f ) . get value ( ) ) ) ) . collect ( ) . stream ( ) . map ( p -> { integer treeid = p . _1 ( ) . get first ( ) ; string nodeid = p . _1 ( ) . get second ( ) ; double summary statistics stats = p . _2 ( ) ; return text utils . joinjson ( arrays . as list ( treeid @$ nodeid @$ stats . get average ( ) @$ stats .,call uses
for now @$ start writing the data section <PLACE_HOLDER> after the header but if this store evolves @$ we could decide to start the data section somewhere else .,_data section start off set = header_length ; write ( byte arrays ) ;,start writing
oracle did add <PLACE_HOLDER> for ansi case statements in 9 i,return new ansi case fragment ( ) ;,oracle add
ensure the cloud provider does n't support enterprise org <PLACE_HOLDER>,mbp = mock mbp ( create github enterprise credential ( ) @$ user @$ github enterprise scm . domain_name ) ; assert false ( __str__ @$ provider . support ( mbp ) ) ;,provider support
this enlists the transaction scoped <PLACE_HOLDER> into the transaction,assert . assert true ( bike race . all motor bikes ( ) . is empty ( ) ) ; motorbike bike = bike race . create new bike ( __num__ @$ __str__ ) ; assert . assert false ( bike race . contains ( bike ) ) ;,transaction scoped
check mapping an element returns the expected <PLACE_HOLDER> .,bounded window input window = new interval window ( instant . now ( ) @$ duration . standard minutes ( __num__ ) ) ; assert equals ( global window . instance @$ window mapping fn . get side input window ( input window ) ) ; assert equals ( input window @$ ( ( kv ) test sdk harness . get input values ( ) . get ( __num__ ) . get value ( ) ) . get value ( ) ) ;,check returns
validate the window merge <PLACE_HOLDER> .,if ( windowing strategy . get window fn ( ) instanceof invalid windows ) { string cause = ( ( invalid windows < ? > ) windowing strategy . get window fn ( ) ) . get cause ( ) ; throw new illegal state exception ( __str__ + __str__ + cause ) ; },window merge
create a handler and handle each violated <PLACE_HOLDER>,s3 guard fsck violation handler handler = new s3 guard fsck violation handler ( rawfs @$ metadata store ) ; compare pairs . for each ( handler :: handle ) ; stopwatch . stop ( ) ; log . info ( __str__ @$ stopwatch . now ( time unit . seconds ) ) ; log . info ( __str__ @$ ddb tree . content map . size ( ) ) ; return compare pairs ;,each violated
direct comparison with undefined will always return empty <PLACE_HOLDER>,object key = index info . evaluate index key ( context ) ; create empty set = ( key != null && key . equals ( query service . undefined ) ) ; if ( result type instanceof struct type ) { index fields size = ( ( struct type impl ) result type ) . get field names ( ) . length ; } else { index fields size = __num__ ; },comparison return
note : the following line is necessary for garbage collection because the menus keep <PLACE_HOLDER> to the graph panel and this leads to circular object <PLACE_HOLDER> .,m_menu bar . remove all ( ) ; m_tool bar . remove all ( ) ; m_menu bar = null ; m_tool bar = null ; m_right panel . dispose ( ) ; m_left panel . dispose ( ) ; m_dialogs . dispose ( ) ; remove all ( ) ;,menus keep
filter all message stanzas containing a data stanza <PLACE_HOLDER> @$ matching session id and recipient,return new and filter ( new stanza type filter ( message . class ) @$ new ibb data packet filter ( ) ) ;,stanzas containing
wait a minute and you should get other 6 <PLACE_HOLDER> executed,wait minute quota ( ) ;,minute get
if the text wrapper does n't have a start <PLACE_HOLDER> and the new character is a starting one,if ( parenthesis . get ( i ) . start index == - __num__ && ! is closing parenthesis ( paren ) ) { if ( parenthesis of interest == - __num__ ) { parenthesis of interest = i ; } else { if ( parenthesis . get ( i ) . end index < parenthesis . get ( parenthesis of interest ) . end index ) { parenthesis of interest = i ; } } },wrapper have
this method always throws activation <PLACE_HOLDER>,user service . activate user ( __str__ ) . to blocking ( ) . single ( ) ;,method throws
official java api does not support <PLACE_HOLDER> aligned strides ...,assert equals ( cvmat2 . cols ( ) * cvmat2 . channels ( ) @$ frame3 . image stride ) ; frame3 . image stride = frame2 . image stride ; u byte indexer frame1 idx = frame1 . create indexer ( ) ; u byte indexer frame2 idx = frame2 . create indexer ( ) ; u byte indexer frame3 idx = frame3 . create indexer ( ) ; for ( int i = __num__ ; i < frame idx . rows ( ) ; i ++ ) { for ( int j = __num__ ; j < frame idx . cols ( ) ; j ++ ) { for ( int k = __num__ ; k < frame idx . channels ( ) ; k ++,api support
no existing match found . create a new <PLACE_HOLDER> .,final list < virtual column > virtual columns = new array list < > ( ) ; if ( input . is direct column access ( ) ) { aggregator factory = new t digest sketch aggregator factory ( agg name @$ input . get direct column ( ) @$ compression ) ; } else { virtual column virtual column = virtual column registry . get or create virtual column for expression ( planner context @$ input @$ sql type name . float ) ; virtual columns . add ( virtual column ) ; aggregator factory = new t digest sketch aggregator factory ( agg name @$ virtual column . get output name ( ) @$ compression ) ; } return aggregation . create ( virtual columns @$,match create
anonymous loggers can always add <PLACE_HOLDER>,if ( this . is named ) { log manager . get log manager ( ) . check access ( ) ; } this . handlers . add ( handler ) ; update dalvik log handler ( ) ;,loggers add
the partition must have at least two <PLACE_HOLDER>,if ( partition2 all potential consumers . get ( partition ) . size ( ) <= __num__ ) log . error ( __str__ @$ partition ) ;,partition have
no more directories to watch @$ something happened the root <PLACE_HOLDER> being watched .,if ( watch key to dir bi map . is empty ( ) ) { throw new io exception ( __str__ + watch root path + __str__ ) ; },directories happened
user may have specified an <PLACE_HOLDER> by ... limit clause,if ( m_parsed delete . order by columns ( ) . size ( ) > __num__ && ! is single partition plan && ! target table . get isreplicated ( ) ) { throw new planning error exception ( __str__ + __str__ + __str__ + __str__ ) ; } boolean needs order by node = is order by node required ( m_parsed delete @$ sub select root ) ; abstract expression address expr = new tuple address expression ( ) ; node schema proj_schema = new node schema ( ) ;,user specified
assign level.trace to level so that log server prints whatever log messages received from log clients . if the log server receives a logging event @$ it assumes that the remote log client has the intention of writing this logging event to logs . it does not make much sense for the log client to send the message over the network and wants the,level level = level . trace ; hierarchy client hierarchy = new hierarchy ( new root logger ( level ) ) ;,server receives
host name should be valid @$ but most probably not existing if its not enough @$ then should probably run 'list ' <PLACE_HOLDER> first to be sure ...,final string not existent fake host name = __str__ ; string credentials not found msg = null ; try { run credential program ( not existent fake host name @$ credential helper name ) ; log . warn ( __str__ @$ credential helper name ) ; } catch ( exception e ) { if ( e instanceof invalid result exception ) { credentials not found msg = extract credential provider error message ( ( invalid result exception ) e ) ; } if ( is blank ( credentials not found msg ) ) { log . warn ( __str__ @$ credential helper name @$ e . get message ( ) ) ; } else { log . debug ( __str__ @$ credentials not found msg ) ; },its run
if only one memory space held a valid <PLACE_HOLDER> @$ use it,if ( containing mem space cnt == __num__ && containing addr != null ) { return containing addr . get address space ( ) . get unique spaceid ( ) ; } if ( symbol target cnt == __num__ && symbol target != null ) { return symbol target . get address space ( ) . get unique spaceid ( ) ; },space held
prints gps <PLACE_HOLDER> .,log . v ( tag @$ file name + __str__ + exif interface . get altitude ( __num__ ) ) ; float [ ] lat long = new float [ __num__ ] ; if ( exif interface . get lat long ( lat long ) ) { log . v ( tag @$ file name + __str__ + lat long [ __num__ ] ) ; log . v ( tag @$ file name + __str__ + lat long [ __num__ ] ) ; } else { log . v ( tag @$ file name + __str__ ) ; },prints gps
eat all currently available messages before the <PLACE_HOLDER> is fully sent . this is done to avoid : req sends <PLACE_HOLDER> to a @$ a replies @$ b replies too . a 's reply was first and matches @$ that is used . an hour later req sends a <PLACE_HOLDER> to b. b 's old reply is used .,while ( true ) { msg drop = super . xrecv ( ) ; if ( drop == null ) { break ; } },req sends
if the value type contains no data <PLACE_HOLDER> @$ a null return is expected,if ( v . state ( ) == null ) return ; for ( int i = __num__ ; i < v . state ( ) . size ( ) ; i ++ ) { interface state member = ( interface state ) v . state ( ) . element at ( i ) ; symtab entry entry = ( symtab entry ) member . entry ; util . fill info ( entry ) ; if ( entry . comment ( ) != null ) entry . comment ( ) . generate ( __str__ @$ stream ) ; string modifier = __str__ ; if ( member . modifier == interface state . public ) modifier = __str__ ; util . write initializer ( modifier @$ entry . name,type contains
to produce the final output select the tuples directed to the 'output ' channel then get the <PLACE_HOLDER> pairs that have the greatest iteration counter on a 1 second sliding window,data stream < tuple2 < tuple2 < integer @$ integer > @$ integer > > numbers = step . select ( __str__ ) . map ( new output map ( ) ) ;,pairs get
if its an external table @$ even though the temp table skip <PLACE_HOLDER> is on @$ we create the table since we need the hdfs path to temp table lineage .,return table != null && skip temp tables && table . is temporary ( ) && ! external_table . equals ( table . get table type ( ) ) ;,table skip
the is being initialize @$ preserve the new <PLACE_HOLDER> .,if ( n . has children ( ) ) { parent . remove child ( n ) ; node value = n . get first child ( ) ; n . remove child ( value ) ; node replacement = ir . assign ( n @$ value ) ; replacement . setjs doc info ( parent . getjs doc info ( ) ) ; replacement . use source info if missing from ( parent ) ; node statement = node util . new expr ( replacement ) ; grandparent . replace child ( parent @$ statement ) ; report code change ( __str__ @$ statement ) ; } else { if ( node util . is statement block ( grandparent ) ) { grandparent . remove child (,the preserve
verify sync called or <PLACE_HOLDER>,if ( expect sync || expect sync from log syncer ) { test_util . wait for ( timeout @$ new waiter . predicate < exception > ( ) { @ override public boolean evaluate ( ) throws exception { try { if ( expect sync ) { verify ( wal @$ times ( __num__ ) ) . sync ( any long ( ) ) ; } else if ( expect sync from log syncer ) { verify ( wal @$ times ( __num__ ) ) . sync ( ) ; } } catch ( throwable ignore ) { } return true ; } } ) ; } else { verify ( wal @$ never ( ) ) . sync ( ) ; },sync called
m launch transition end runnable might call show <PLACE_HOLDER> @$ which would execute it again @$ which would lead to infinite recursion . protect against it .,m launch transition end runnable = null ; r . run ( ) ;,end call
the computation threw an <PLACE_HOLDER> @$ so it did not complete successfully .,return false ;,computation threw
note to translators : the stylesheet referred to an <PLACE_HOLDER> to the xsl syntax and indicated that it was defined by xsltc @$ but xsltc does not recognize the particular <PLACE_HOLDER> named . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { basis library . run_time_internal_err @$ __str__ } @$ { basis library . run_time_copy_err @$ __str__ } @$ { basis library . data_conversion_err @$ __str__ } @$ { basis library . external_func_err @$ __str__ } @$ { basis library . equality_expr_err @$ __str__ } @$ { basis library . invalid_argument_err @$ __str__ } @$ { basis library . format_number_err @$ __str__ } @$ { basis library . iterator_clone_err @$ __str__ } @$ { basis library . axis_support_err @$ __str__ } @$ { basis library . typed_axis_support_err @$ __str__ } @$ { basis library . stray_attribute_err @$ __str__ } @$ { basis library . stray_namespace_err @$ __str__ } @$ { basis library . namespace_prefix_err @$ __str__ } @$ { basis library,xsltc recognize
set content assist <PLACE_HOLDER> for various content types .,if ( completion processor == null ) { this . completion processor = new sql completion processor ( editor ) ; } assistant . add content assist processor ( completion processor @$ i document . default_content_type ) ; assistant . add content assist processor ( completion processor @$ sql parser partitions . content_type_sql_quoted ) ;,content assist
get a mapping between the step name and the injection ... get new injection <PLACE_HOLDER>,data . step injection metas map = new hash map < string @$ step meta interface > ( ) ; for ( step meta step meta : data . trans meta . get used steps ( ) ) { step meta interface meta = step meta . get step meta interface ( ) ; if ( bean injection info . is injection supported ( meta . get class ( ) ) ) { data . step injection metas map . put ( step meta . get name ( ) @$ meta ) ; } },mapping get
it 's the engine context factory the one who has the <PLACE_HOLDER> of creating the specific implementation of the engine context needed .,final i engine context factory engine context factory = configuration . get engine context factory ( ) ; return engine context factory . create engine context ( configuration @$ template data @$ template resolution attributes @$ context ) ;,who has
field choice will parse the entire <PLACE_HOLDER> of a field,return parse unknown field ( start element @$ false @$ null ) ;,choice parse
<PLACE_HOLDER> does n't recover any more the recovered <PLACE_HOLDER> should not call retain assignment @$ as it is not a clean startup .,assert false ( __str__ @$ mock load balancer . retain assign called ) ;,master recover
multi valued field <PLACE_HOLDER>,wl multi valued separator = new label ( w settings @$ swt . right ) ; wl multi valued separator . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl multi valued separator ) ; fdl multi valued separator = new form data ( ) ; fdl multi valued separator . left = new form attachment ( __num__ @$ __num__ ) ; fdl multi valued separator . top = new form attachment ( w operation @$ margin ) ; fdl multi valued separator . right = new form attachment ( middle @$ - margin ) ; wl multi valued separator . set layout data ( fdl multi valued separator ) ; w multi valued separator =,multi valued
make sure the client recognizes the underlying <PLACE_HOLDER> otherwise @$ throw a do not retry io <PLACE_HOLDER> .,if ( version info util . has minimum version ( connection header . get version info ( ) @$ request too big exception . major_version @$ request too big exception . minor_version ) ) { req too big . set response ( null @$ null @$ simple rpc server . request_too_big_exception @$ msg ) ; } else { req too big . set response ( null @$ null @$ new do not retryio exception ( ) @$ msg ) ; },a retry
push twice will fail and temp dir <PLACE_HOLDER>,file out dir = new file ( string utils . format ( __str__ @$ config . get storage directory ( ) @$ segment path ) ) ; out dir . set read only ( ) ; try { pusher . push ( segment dir @$ segments [ i ] @$ false ) ; } catch ( io exception e ) { assert . fail ( __str__ ) ; },push fail
simple insert of simple keys @$ with no reprobing on insert until the table gets full exactly . then do a <PLACE_HOLDER> ' on the totally full table .,non blocking hash map < integer @$ object > map = new non blocking hash map < > ( __num__ ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { map . put ( i @$ new object ( ) ) ; } map . get ( __num__ ) ;,insert do
ret has no rop <PLACE_HOLDER> .,return ;,ret has
localized string file <PLACE_HOLDER>,args . filtered resources provider = new resources filter ( aapt target . with flavors ( internal flavor . of ( __str__ ) ) @$ filesystem @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ graph builder @$ immutable list . of ( resource1 . get res ( ) @$ resource2 . get res ( ) ) @$ immutable set . of ( ) @$ immutable set . of ( ) @$ null @$ resources filter . resource compression mode . disabled @$ filter resources steps . resource filter . empty_filter @$ optional . empty ( ) ) ;,string file
if buffer <PLACE_HOLDER> is after queue <PLACE_HOLDER> we use queue <PLACE_HOLDER> . we need to handle overflow so can not use math.min,if ( p buffer limit - p queue limit > __num__ ) { p buffer limit = p queue limit ; },limit use
calling the build <PLACE_HOLDER>,rule builder . build attributes ( context ) ;,the build
each call to next @$ generates a cursor moved <PLACE_HOLDER>,while ( jdbcrs . next ( ) ) { system . out . println ( __str__ + jdbcrs . get string ( __num__ ) ) ; system . out . println ( __str__ + jdbcrs . get string ( __num__ ) ) ; },call moved
submit form property will remove all the <PLACE_HOLDER> which it takes care of,for ( form property handler form property handler : form property handlers ) { form property handler . submit form property ( execution @$ properties copy ) ; },property remove
zap : added the type <PLACE_HOLDER> .,vector < object > v = get list ( uri @$ key ) ; if ( v == null || v . size ( ) == __num__ ) { return null ; } return v . get ( __num__ ) ;,zap added
this prevents that the current writer @$ which holds <PLACE_HOLDER> for the last document he is editing will not override our last changes to the document,if ( filename . equals ( this . current file ) ) { this . current doc = doc ; } break ;,which holds
report all volumes as unmounted until we 've recorded that user 0 has <PLACE_HOLDER> . there are no guarantees that callers will see a consistent view of the volume before that point,final boolean system user unlocked = is system unlocked ( user handle . user_system ) ; final boolean user key unlocked ; final boolean storage permission ; final long token = binder . clear calling identity ( ) ; try { user key unlocked = is user key unlocked ( user id ) ; storage permission = m storage manager internal . has external storage ( uid @$ package name ) ; } finally { binder . restore calling identity ( token ) ; } boolean found primary = false ; final array list < storage volume > res = new array list < > ( ) ; synchronized ( m lock ) { for ( int i = __num__ ; i < m volumes . size (,user has
setup the configurator to inject the pool config in the message driven component create <PLACE_HOLDER>,final message driven component description mdb component description = ( message driven component description ) mdb component configuration . get component description ( ) ; mdb component configuration . get create dependencies ( ) . add ( new pool injecting configurator ( mdb component description ) ) ;,component create
the following code is for mapjoin initialize all the dummy <PLACE_HOLDER>,log . info ( __str__ ) ; list < operator < ? extends operator desc > > dummy ops = local work . get dummy parent op ( ) ; for ( operator < ? extends operator desc > dummy op : dummy ops ) { dummy op . set exec context ( exec context ) ; dummy op . initialize ( jc @$ null ) ; },mapjoin initialize
local variable types must be resolved from the point of view of the annotated class . so do the <PLACE_HOLDER> early @$ because users of the local variable table only have access to the original .,if ( annotated . get local variable table ( ) != null ) { local [ ] orig locals = annotated . get local variable table ( ) . get locals ( ) ; local [ ] new locals = new local [ orig locals . length ] ; resolved java type accessing class = annotated . get declaring class ( ) ; for ( int i = __num__ ; i < new locals . length ; i ++ ) { local orig local = orig locals [ i ] ; new locals [ i ] = new local ( orig local . get name ( ) @$ orig local . get type ( ) . resolve ( accessing class ) @$ orig local . get startbci (,types do
base the data ref on the request parameter but ensure the <PLACE_HOLDER> is based off the incoming request ... this is necessary for scenario 's where the ni fi instance is behind a proxy running a different <PLACE_HOLDER>,ref uri builder . scheme ( request . get scheme ( ) ) ;,instance running
read a property file and convert <PLACE_HOLDER> into configuration lines,file input stream in stream = null ; try { final file f = new file ( args [ __num__ ] ) ; final properties p = new properties ( ) ; in stream = new file input stream ( f ) ; p . load ( in stream ) ; final string builder sb = new string builder ( ) ; sb . append ( __str__ ) ; for ( int i = __num__ ; i <= __num__ ; i ++ ) { sb . append ( __str__ ) ; final string s = p . get property ( __str__ + i ) ; final string [ ] l = common pattern . comma . split ( s ) ; for ( final string element : l,property file
the source character was not nfd . skip this case ; the first step in obtaining a skeleton is to nfd the <PLACE_HOLDER> @$ so the mapping in this line of confusables.txt will never be applied .,if ( ! normalizer . is normalized ( from ) ) { continue ; },character nfd
it 's an array field type @$ lets <PLACE_HOLDER> the inner type,inner field type . check compatibility ( ( ( array field type ) other ) . inner field type @$ conflicts @$ strict ) ;,type lets
if this rule does n't define local <PLACE_HOLDER> @$ no resource processing was done @$ so it does n't produce data binding output .,if ( ! android resources . defines android resources ( rule context . attributes ( ) ) ) { return immutable list . of ( ) ; },rule define
get this <PLACE_HOLDER> <PLACE_HOLDER> get this <PLACE_HOLDER> <PLACE_HOLDER>,byte [ ] [ ] cols = new byte [ ] [ ] { bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) } ;,buffer get
use this setting to improve performance if you know that changes in content do not change the layout <PLACE_HOLDER> of the recycler view,m recycler view . set has fixed size ( true ) ; recycler view . layout manager layout manager = new linear layout manager ( get activity ( ) ) ; m recycler view . set layout manager ( layout manager ) ; m recycler view . set adapter ( adapter ) ; m recycler view . add item decoration ( new spaces item decoration ( quick return utils . dp2px ( get activity ( ) @$ __num__ ) ) ) ; array list < view > header views = new array list < > ( ) ; header views . add ( get action bar view ( ) ) ; array list < view > footer views = new array list < > ( ) ; m,changes change
the min value of disk max used space <PLACE_HOLDER> .,int disk max used space ratio = __num__ ;,value used
the server initially reduces the connection flow control <PLACE_HOLDER> to 0 .,run in channel ( server connected channel @$ new http2 runnable ( ) { @ override public void run ( ) throws http2 exception { http2 server . encoder ( ) . write settings ( server ctx ( ) @$ new http2 settings ( ) . copy from ( http2 server . decoder ( ) . local settings ( ) ) . initial window size ( __num__ ) @$ server new promise ( ) ) ; http2 server . flush ( server ctx ( ) ) ; } } ) ; assert true ( server settings ack latch1 . await ( default_await_timeout_seconds @$ seconds ) ) ;,server reduces
the removed admin might have disabled <PLACE_HOLDER> @$ so update user restrictions .,if ( removed admin ) { push user restrictions ( user handle ) ; },admin disabled
even after the security realm deleted the <PLACE_HOLDER> @$ they can still connect @$ until session invalidation,assert user connected ( wc @$ alice ) ; request renew seed for user ( alice ) ; assert user not connected ( wc @$ alice ) ; assert user connected ( wc @$ __str__ ) ; try { wc . login ( alice ) ; fail ( __str__ ) ; } catch ( failing http status code exception e ) { assert equals ( __num__ @$ e . get status code ( ) ) ; },realm deleted
the button takes <PLACE_HOLDER> over the summary text and the chevron .,int edit button state = get edit button state ( ) ; if ( edit button state == edit_button_gone ) { m edit button view . set visibility ( gone ) ; m chevron view . set visibility ( m display mode == display_mode_expandable ? visible : gone ) ; boolean show summary = m is summary allowed && ! text utils . is empty ( m summary left text view . get text ( ) ) ; m summary layout . set visibility ( show summary ? visible : gone ) ; } else { boolean is button allowed = m display mode == display_mode_expandable || m display mode == display_mode_normal ; m summary layout . set visibility ( gone ) ; m chevron view . set,button takes
there is a possibility that another thread may have defined the same <PLACE_HOLDER> in the meantime,try { the class = cl . load class ( stub class name ) ; } catch ( class not found exception e1 ) { ejb logger . root_logger . dynamic stub creation failed ( stub class name @$ ex ) ; throw ex ; },thread defined
filter and reverse sort the parsed <PLACE_HOLDER> .,tree set < string > parsed set = new tree set < string > ( string . case_insensitive_order ) ; parsed set . add ( text ) ; parsed set . add ( final text ) ; if ( variants != null ) { for ( int i = variants . length ; -- i >= __num__ ; ) { parsed set . add ( variants [ i ] ) ; } } array list < string > parsed list = new array list < string > ( parsed set ) ; collections . reverse ( parsed list ) ; i parsed forms = parsed list . to array ( new string [ parsed list . size ( ) ] ) ;,filter sort
verify that file system does not support unix <PLACE_HOLDER>,try { cfg . file system . read attributes ( path @$ __str__ ) ; throw e ; } catch ( unsupported operation exception unsupported ) { } catch ( io exception ioe ) { throw new runtime exception ( ioe ) ; },system support
iterate through all source paths to make sure we are generating a complete <PLACE_HOLDER> of source folders for the source paths .,set < string > src folders = new hash set < > ( ) ; loop through source path : for ( source path java src path : java srcs ) { if ( rule finder . get rule ( java src path ) . is present ( ) ) { continue ; } path java src relative path = rule finder . get source path resolver ( ) . get relative path ( java src path ) ; for ( string src folder : src folders ) { if ( java src relative path . starts with ( src folder ) ) { continue loop through source path ; } } immutable sorted set < string > paths from root = default java package finder . get,paths generating
now the tricky one . 'before <PLACE_HOLDER> ' le<PLACE_HOLDER>ds to 'c<PLACE_HOLDER>ll <PLACE_HOLDER>ctivity <PLACE_HOLDER> ' @$ which c<PLACE_HOLDER>lls subprocess 02 which termin<PLACE_HOLDER>tes,process instance = runtime service . start process instance by key ( __str__ ) ; tasks = assert task names ( process instance @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; task task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ; assert historic process,a leads
if current seek <PLACE_HOLDER> is already set @$ update the next seek <PLACE_HOLDER> .,m next seek position = new position ;,current seek
note : use a message object to facilite script message log <PLACE_HOLDER>,msg . info ( ghidra script . class @$ new script message ( decorated message ) ) ; if ( is running headless ( ) ) { return ; } plugin tool tool = state . get tool ( ) ; if ( tool == null ) { return ; } console service console = tool . get service ( console service . class ) ; if ( console == null ) { return ; } try { console . add message ( get script name ( ) @$ message ) ; } catch ( exception e ) { msg . error ( this @$ __str__ + message @$ e ) ; },message log
let the data store know about the real <PLACE_HOLDER> at this point so that other v ms which discover the real <PLACE_HOLDER> via a profile exchange can send messages to the data store and safely use the <PLACE_HOLDER> .,if ( buk reg != null ) { observer . before assign bucket ( this . partitioned region @$ possibly free bucket id ) ; assign bucket region ( buk reg . get id ( ) @$ buk reg ) ; buk . set hosting ( true ) ; buk reg . invoke partition listener after bucket created ( ) ; } else { if ( buk . get partitioned region ( ) . get colocated with ( ) == null ) { buk . get bucket advisor ( ) . set shadow bucket destroyed ( true ) ; clear all temp queue for shadowpr ( buk . get bucket id ( ) ) ; } },which discover
if the pom has no <PLACE_HOLDER> @$ we cached a missing artifact @$ only return the cached data if no update forced,if ( cached != null && ( ! request . is force update ( ) || has file ( cached . get pom artifact ( ) ) ) ) { return cached ; },pom has
this one uses a primitive <PLACE_HOLDER> as much as possible,if ( prop . is unboxable ( ) ) return frf . get required unboxed ( ) ; else return frf . get single ( ) ;,one uses
num of keys should get <PLACE_HOLDER> as all values are destroyed,for ( int i = __num__ ; i < __num__ ; i ++ ) { region . destroy ( integer . to string ( i ) ) ; } assert equals ( __num__ @$ key index1 stats . get num updates ( ) ) ; assert equals ( __num__ @$ key index1 stats . get number of keys ( ) ) ; qs . remove index ( key index2 ) ; region . destroy region ( ) ;,num get
test that it works when the shutdown kills the outstanding <PLACE_HOLDER> ...,test shutdown request outstanding ( __num__ @$ __num__ @$ remote invocation exception . class @$ timeout exception . class ) ;,shutdown kills
the spans that do not cross the network boundary should have the same <PLACE_HOLDER> .,assert that ( client bar span . id ( ) ) . is equal to ( service bar span . id ( ) ) ; assert that ( client qux span . id ( ) ) . is equal to ( service qux span . id ( ) ) ;,spans have
to avoid resource store write <PLACE_HOLDER>,dict src . set last modified ( __num__ ) ;,store write
old implementations might call <PLACE_HOLDER> for initialization @$ so ensure it is initialized here as well . initialization has no side effects if already done .,if ( is enabled ( ) ) { initialize ( ) ; clear events list ( ) ; },implementations call
this is the limit node get the <PLACE_HOLDER> and add it to the trie,if ( quotas . limit node . equals ( child name ) ) { p trie . add path ( parent name . substring ( quota zookeeper . length ( ) ) ) ; },node get
to do not introduce the <PLACE_HOLDER> of the separator @$ let us point to the last symbol of it .,if ( ! escaped && space_byte != after next or space && tab_byte != after next or space && linefeed_byte == current ) { return i + __num__ ; },to introduce
note to translators : the message indicates that the encoding requested for the output document was on that requires <PLACE_HOLDER> that is not available from the java virtual machine being used to execute the program .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,document requires
after resume second client join @$ transaction should succesfully await new <PLACE_HOLDER> and commit .,tx fut . get ( ) ;,transaction await
we do n't know how the results have changed after being filtered . must set <PLACE_HOLDER> according to contents of results now .,if ( scanner context . get keep progress ( ) ) { scanner context . set progress ( initial batch progress @$ initial size progress @$ initial heap size progress ) ; } else { scanner context . clear progress ( ) ; } scanner context . increment batch progress ( results . size ( ) ) ; for ( cell cell : results ) { scanner context . increment size progress ( private cell util . estimated serialized size of ( cell ) @$ cell . heap size ( ) ) ; },now set
return null so that server will return 550 <PLACE_HOLDER> not found .,return null ;,server return
increment the generation first @$ so observers always see the new <PLACE_HOLDER>,final int key = make key ( settings_type_secure @$ user id ) ; m generation registry . increment generation ( key ) ; final uri uri = get notification uri for ( key @$ secure . location_mode ) ; m handler . obtain message ( my handler . msg_notify_uri_changed @$ user id @$ __num__ @$ uri ) . send to target ( ) ;,observers see
check if the dataset version contains the correct <PLACE_HOLDER>,assert . assert true ( dataset version . get partition ( ) . get complete name ( ) . equals ignore case ( partition_name ) ) ;,version contains
if these targets were treated as distinct targets @$ the rule will have duplicate <PLACE_HOLDER> .,assume that ( platform . detect ( ) @$ is ( not ( windows ) ) ) ; pair < project workspace @$ project workspace > cells = prepare ( __str__ @$ __str__ ) ; project workspace primary = cells . get first ( ) ; project workspace secondary = cells . get second ( ) ; register cell ( primary @$ __str__ @$ primary ) ; register cell ( secondary @$ __str__ @$ primary ) ; path output = primary . build and return output ( __str__ ) ; assert equals ( __str__ @$ __num__ @$ primary . run command ( output . to string ( ) ) . get exit code ( ) ) ;,rule have
the private <PLACE_HOLDER> section contains both the public <PLACE_HOLDER> and the private <PLACE_HOLDER>,string key type = key buffer . read string ( ) ;,section contains
make sure users with huge dict do n't blow up the <PLACE_HOLDER>,if ( dict . size ( ) <= __num__ ) { user dict cache . put ( user id @$ dict ) ; } else { logger . info ( __str__ + dict . size ( ) + __str__ + user id + __str__ ) ; },users blow
no values provided . read all <PLACE_HOLDER> .,if ( no from ) { m sector range . set text ( get string ( r . string . text_sector_range_all ) ) ; if ( save as default . is checked ( ) ) { save mapping range ( __str__ @$ __str__ ) ; } return ; },values read
not a small battery device @$ so plugged in status should not affect forced app <PLACE_HOLDER>,m is small battery device = false ; final app state tracker testable instance = new instance ( ) ; call start ( instance ) ; assert false ( instance . is force all apps standby enabled ( ) ) ; m power save mode = true ; m power save observer . accept ( get power save state ( ) ) ; assert true ( instance . is force all apps standby enabled ( ) ) ;,device affect
we need a real server and client in this test @$ because netty 's embedded channel is not failing the channel <PLACE_HOLDER> of failed writes .,netty server and client server and client = init server and client ( protocol @$ create config ( ) ) ; channel ch = connect ( server and client ) ; network client handler handler = get client handler ( ch ) ;,channel failing
if it is a node try to parse with the node parser to find out whether we should may use the generated create and get uncached <PLACE_HOLDER> .,if ( node code generator . is specialized node ( parameter . get type ( ) ) ) { node parser parser = node parser . create default parser ( ) ; parser . node only = true ; type element element = element utils . cast type element ( parameter . get type ( ) ) ; if ( ! node only ) { node data parsed node = parser . parse ( element ) ; if ( parsed node != null ) { list < code executable element > executables = node factory factory . create factory methods ( parsed node @$ element filter . constructors in ( element . get enclosed elements ( ) ) ) ; type element type = element utils . cast,generated create
if the scope has no <PLACE_HOLDER> try to remove it,if ( ! ( ( basic scope ) broadcast scope ) . has event listeners ( ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ ) ; } scope . remove child scope ( broadcast scope ) ; } log . debug ( __str__ @$ name ) ;,scope has
insert rows on different streams but since they all go thru one export track <PLACE_HOLDER> thru the check stream,for ( int i = __num__ ; i < loop_count ; i ++ ) { for ( int j = __num__ ; j < m_stream count ; j ++ ) { string stream name = string . format ( stream_template @$ j ) ; for ( int k = row count ; k < row count + row_batch ; k ++ ) { data [ __num__ ] = k ; m_verifier . add row ( m_client @$ check stream @$ k @$ data ) ; m_client . call procedure ( __str__ @$ __str__ + stream name + __str__ + k + __str__ ) ; } row count += row_batch ; } },export track
for now we just propagate statistics for source symbols . handling semi join output symbols requires <PLACE_HOLDER> for correlation for boolean columns .,return optional . of ( source stats ) ;,symbols requires
first @$ get the database . this is the database which contains the <PLACE_HOLDER> @$ with contains the processed definitions from the ddl file .,database db = get database ( ) ;,which contains
this method always throws activation <PLACE_HOLDER>,user service . activate user ( __str__ ) ;,method throws
rather than risk a mysterious class cast <PLACE_HOLDER> during unit tests @$ throw an explanatory <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ + __str__ + __str__ ) ;,class cast
and add a new <PLACE_HOLDER> to that dir :,path uf = ro path . resolve ( system utils . file name ( __str__ ) ) ; files . write ( uf @$ arrays . as list ( __str__ @$ __str__ ) ) ; files . set posix file permissions ( uf @$ posix file permissions . from string ( __str__ ) ) ; files . set posix file permissions ( ro path @$ posix file permissions . from string ( __str__ ) ) ;,and add
super translation should contain the generic private method <PLACE_HOLDER> .,assert translation ( super translation @$ __str__ ) ;,translation contain
json serializer will always close the <PLACE_HOLDER>,try { json writer . write value ( os @$ yielder ) ; os . flush ( ) ; os . close ( ) ; } catch ( exception ex ) { e = ex ; log . no stack trace ( ) . error ( ex @$ __str__ ) ; throw new runtime exception ( ex ) ; } finally { thread . current thread ( ) . set name ( curr thread name ) ; query lifecycle . emit logs and metrics ( e @$ req . get remote addr ( ) @$ os . get count ( ) ) ; if ( e == null ) { successful query count . increment and get ( ) ; } else { failed query count . increment,serializer close
we perform the analogous test on the get <PLACE_HOLDER> .,subclass . set field ( type @$ subclass @$ boolean field @$ illegal argument exception class @$ value ) ;,test get
update queues @$ all queue can access this <PLACE_HOLDER>,for ( queue q : queue collections . values ( ) ) { resources . subtract from ( q . resource @$ oldnm . resource ) ; },queue access
been returned from the discovery agent . up to that point the reconnect delay is used which has a default <PLACE_HOLDER> of 10,long initial reconnect delay = __num__ ; long startt = system . current time millis ( ) ; string group id = __str__ + startt ; try { string url str = __str__ + group id + __str__ + initial reconnect delay ; activemq connection factory factory = new activemq connection factory ( url str ) ; log . info ( __str__ ) ; connection connection = factory . create connection ( ) ; connection . set clientid ( __str__ ) ; fail ( __str__ ) ; } catch ( jms exception expected ) { assert true ( __str__ + expected . get cause ( ) @$ expected . get cause ( ) instanceof java . io . io exception ) ; long duration = system . current,which has
the list has exactly the same <PLACE_HOLDER> of elements as the list of element matchers :,for ( int i = __num__ ; i < pattern token matchers . size ( ) ; i ++ ) { p tokens matched . add ( boolean . false ) ; } int i = __num__ ; int min occur correction = get min occurrence correction ( ) ; while ( i < limit + min occur correction && ! ( rule . is sent start ( ) && i > __num__ ) ) { int skip shift total = __num__ ; boolean all elements match = false ; unified tokens = null ; int matching tokens = __num__ ; int first match token = - __num__ ; int last match token = - __num__ ; int first marker match token = - __num__ ; int last,list has
step 1 : ensure that user has write permissions to the process group . if not @$ then immediately fail . step 2 : retrieve <PLACE_HOLDER> from <PLACE_HOLDER> registry,if ( version control info != null && request process group entity . get versioned flow snapshot ( ) == null ) { final versioned flow snapshot flow snapshot = get flow from registry ( version control info ) ; service facade . discover compatible bundles ( flow snapshot . get flow contents ( ) ) ; service facade . resolve inherited controller services ( flow snapshot @$ group id @$ ni fi user utils . get ni fi user ( ) ) ; request process group entity . set versioned flow snapshot ( flow snapshot ) ; },not retrieve
2 partitions commit <PLACE_HOLDER>,for ( int partition id = __num__ ; partition id < __num__ ; partition id ++ ) { string segment name = new llc segment name ( raw_table_name @$ partition id @$ __num__ @$ current_time_ms ) . get segment name ( ) ; committing segment descriptor committing segment descriptor = new committing segment descriptor ( segment name @$ partition_offset + num_docs @$ __num__ ) ; committing segment descriptor . set segment metadata ( mock segment metadata ( ) ) ; segment manager . commit segment metadata ( realtime_table_name @$ committing segment descriptor ) ; } test set up new partitions ( segment manager @$ false ) ;,partitions commit
these calls should not make any <PLACE_HOLDER> .,call media browser service method ( custom_action_send_progress_update @$ new bundle ( ) @$ get application context ( ) ) ; call media browser service method ( custom_action_send_result @$ new bundle ( ) @$ get application context ( ) ) ; thread . sleep ( wait_time_for_no_response_ms ) ;,calls make
aws silences all the <PLACE_HOLDER> but illegal argument exception @$ throw runtime exception .,rate limited log ( level . error @$ e @$ __str__ @$ m_config . get resourceid ( ) ) ; m_worker . shutdown ( ) ;,aws silences
the face timeout message is not very actionable @$ let 's ask the <PLACE_HOLDER> to manually retry .,if ( msg id == face manager . face_error_timeout ) { show swipe up to unlock ( ) ; } else if ( m status bar keyguard view manager . is bouncer showing ( ) ) { m status bar keyguard view manager . show bouncer message ( err string @$ m initial text color state ) ; } else if ( update monitor . is screen on ( ) ) { show transient indication ( err string ) ; hide transient indication delayed ( hide_delay_ms ) ; } else { m message to show on screen on = err string ; },let ask
setup the build params we 'll pass to description when generating the build <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; cxx source rule factory cxx source rule factorypdc = cxx source rule factory helper . of ( filesystem . get root path ( ) @$ target @$ cxx platform @$ pic type . pdc ) ; cxx library builder cxx library builder = new cxx library builder ( target ) . set exported headers ( immutable sorted map . of ( gen header name @$ default build target source path . of ( gen header target ) ) ) . set srcs ( immutable sorted set . of ( source with flags . of ( fake source path . of ( source name ) ) @$ source with flags . of ( default build,the build
let 's create a dynamic <PLACE_HOLDER>,if ( method != null && ! ambiguous listener && new value instanceof closure ) { object proxy = proxy . new proxy instance ( the class . get class loader ( ) @$ new class [ ] { method . get parameter types ( ) [ __num__ ] . get the class ( ) } @$ new converted closure ( ( closure ) new value @$ name ) ) ; arguments = new object [ ] { proxy } ; new value = proxy ; } else { method = null ; },"s" create
first verify that our function does not already contain the <PLACE_HOLDER> we 're going to add .,collection < ? extends function tag > tags = get all tags ( ) ; assert true ( ! is tag name in list ( tag name1 @$ tags ) ) ; assert true ( ! is tag name in list ( tag name2 @$ tags ) ) ; assert true ( ! is tag name in list ( tag name3 @$ tags ) ) ;,function contain
let the ejb jar metadata tell us <PLACE_HOLDER> the version is,return ejb jar meta data . is version greater than or equal ( ejb jar version . ejb_3_2 ) ;,version is
this worker has this <PLACE_HOLDER> @$ so it is no longer lost .,m lost blocks . remove ( block id ) ;,worker has
check that new dynamic config includes the updated client port . check that server changed server <PLACE_HOLDER> erased client port from static config . check that other servers still have client port in static config .,for ( int i = __num__ ; i < server_count ; i ++ ) { reconfig test . test server has config ( zk [ i ] @$ new servers @$ null ) ; properties static cfg = read properties from file ( mt [ i ] . conf file ) ; if ( i == changed server id ) { assert false ( static cfg . contains key ( __str__ ) ) ; } else { assert true ( static cfg . contains key ( __str__ ) ) ; } } for ( int i = __num__ ; i < server_count ; i ++ ) { mt [ i ] . shutdown ( ) ; zk [ i ] . close ( ) ; zk admin [,server changed
this <PLACE_HOLDER> wraps the results <PLACE_HOLDER> and guarantees the scrolling behaviour,j panel wrapper = new j panel ( new border layout ( ) ) ; wrapper . set background ( color scheme . dark_gray_color ) ; wrapper . add ( search items panel @$ border layout . north ) ;,panel wraps
optimization : default serializer just writes <PLACE_HOLDER> @$ so we can avoid a call :,if ( is default serializer ( ser ) ) { if ( unwrap single == _unwrap single ) { return this ; } return _with resolved ( property @$ unwrap single ) ; },serializer writes
add padding only if the list has items in it @$ that <PLACE_HOLDER> we do n't show the popup if it is not needed,if ( list content > __num__ ) other heights += padding ; return list content + other heights ;,list has
we do not dump acid table related <PLACE_HOLDER> when taking a bootstrap dump of acid tables as part of an incremental dump . so we should n't be dumping any changes to acid table as part of the commit . at the same time we need to dump the commit transaction event so that replication can end a transaction opened when replaying open transaction,if ( within context . hive conf . get bool var ( hive conf . conf vars . repl_bootstrap_acid_tables ) ) { log . debug ( __str__ + __str__ ) ; replicating acid events = false ; } else if ( ! repl utils . include acid table in dump ( within context . hive conf ) ) { log . debug ( __str__ + __str__ ) ; replicating acid events = false ; },bootstrap dump
reset our known page widths ; populate will recompute <PLACE_HOLDER> .,if ( need populate ) { final int child count = get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { final view child = get child at ( i ) ; final layout params lp = ( layout params ) child . get layout params ( ) ; if ( ! lp . is decor ) { lp . width factor = __num__ ; } } set current item internal ( new curr item @$ false @$ true ) ; request layout ( ) ; },populate recompute
call create <PLACE_HOLDER> log and move the resulting <PLACE_HOLDER> to the name dir .,create edits log . main ( new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ test_dir . get absolute path ( ) } ) ; path edits wildcard = new path ( test_dir . get absolute path ( ) @$ __str__ ) ; file context local fc = file context . get localfs file context ( ) ; for ( file status edits : local fc . util ( ) . glob status ( edits wildcard ) ) { path src = edits . get path ( ) ; path dst = new path ( new file ( name dir @$ __str__ ) . get absolute path ( ) @$ src . get name ( ) ) ; local fc . rename,edits log
short codes can not have <PLACE_HOLDER>,if ( separator position < separator_position ) { return false ; },codes have
if a metric is counter @$ the <PLACE_HOLDER> sent via rpc should be the incremental <PLACE_HOLDER> ; i.e . the amount the <PLACE_HOLDER> has changed since the last rpc . the master should equivalently increment its <PLACE_HOLDER> based on the received metric rather than replacing it .,if ( ! metric set . add ( metric ) ) { metric old metric = metric set . get first by field ( full_name_index @$ metric . get full metric name ( ) ) ; if ( metric . get metric type ( ) == metric type . counter ) { if ( metric . get value ( ) != __num__ ) { old metric . add value ( metric . get value ( ) ) ; } } else { old metric . set value ( metric . get value ( ) ) ; } },master increment
if system audio control feature is enabled @$ turn on system audio <PLACE_HOLDER> when new avr is detected . otherwise @$ turn off system audio <PLACE_HOLDER> .,boolean target system audio mode = tv ( ) . is system audio control feature enabled ( ) ; if ( current system audio mode != target system audio mode ) { add and start action ( new system audio action from tv ( tv ( ) @$ m avr address @$ target system audio mode @$ null ) ) ; } else { tv ( ) . set system audio mode ( target system audio mode ) ; },mode turn
delivery timed out @$ and the timeout handling already took <PLACE_HOLDER> of updating our tracking here @$ so we need n't do anything further .,if ( debug_listener_callback ) { slog . i ( tag @$ __str__ + who ) ; },handling took
for other browsers absent title does n't show default <PLACE_HOLDER> for input element,fu . set title ( null ) ;,title show
decide whether to perform a get or put <PLACE_HOLDER>,boolean response ; if ( rand . next double ( ) < config . getputratio ) { mp rand = rand . next double ( ) ; if ( mp rand < config . multisingleratio ) { if ( total connections . get ( ) > __num__ && config . poolsize > __num__ ) { slow = true ; debug = true ; } else debug = false ; response = client . call procedure ( new get callback ( mp rand ) @$ __str__ @$ processor . generate random key for retrieval ( ) ) ; } else { response = client . call procedure ( new get callback ( mp rand ) @$ __str__ @$ processor . generate random key for retrieval ( ) ) ;,a get
j layer 's decoder throws array <PLACE_HOLDER> out of bounds exception sometimes ! ?,bitstream . close frame ( ) ; int length = output buffer . reset ( ) ; system . arraycopy ( output buffer . get buffer ( ) @$ __num__ @$ buffer @$ total length @$ length ) ; total length += length ;,decoder throws
first @$ make an snmp inform <PLACE_HOLDER> : we clone var bind list and insert sys up time and snmp trap oid variables .,snmp var bind list full vbl ; if ( var bind list != null ) full vbl = var bind list . clone ( ) ; else full vbl = new snmp var bind list ( __num__ ) ; snmp timeticks sys up time value = new snmp timeticks ( get sys up time ( ) ) ; full vbl . insert element at ( new snmp var bind ( snmp trap oid oid @$ trap oid ) @$ __num__ ) ; full vbl . insert element at ( new snmp var bind ( sys up time oid @$ sys up time value ) @$ __num__ ) ;,snmp inform
delete interpret odex for android o @$ directory change . fortunately @$ we do n't need to support android o interpret <PLACE_HOLDER> any more,log . i ( tag @$ __str__ ) ; share patch file util . delete dir ( patch version directory + __str__ + share constants . interpret_dex_optimize_path ) ;,o interpret
test missing proxy class class not found <PLACE_HOLDER>,oin = new proxy blind input stream ( new byte array input stream ( bout . to byte array ( ) ) ) ; try { oin . read object ( ) ; throw new error ( ) ; } catch ( class not found exception ex ) { },class found
one millisecond <PLACE_HOLDER>,timeout millis . set ( persists3 . _bucket cache @$ __num__ ) ;,one millisecond
maps must take 2 type <PLACE_HOLDER> @$ not just one,try { tf . construct parametric type ( map . class @$ strc ) ; } catch ( illegal argument exception e ) { verify exception ( e @$ __str__ ) ; },maps take
at first we find out <PLACE_HOLDER> instructions will be part of the new first block and <PLACE_HOLDER> instructions will be part of the new second block .,final list < i navi instruction > upper instructions = new array list < i navi instruction > ( ) ; final list < i navi instruction > lower instructions = new array list < i navi instruction > ( ) ; list < i navi instruction > current block = upper instructions ; for ( final i navi instruction current instruction : original node . get instructions ( ) ) { current block . add ( current instruction ) ; if ( current instruction == inline instruction ) { current block = lower instructions ; } },instructions part
this version does not support fractional <PLACE_HOLDER>,int nano of second = __num__ ;,version support
do not allow to reset password when current user has a managed <PLACE_HOLDER>,if ( ! is managed profile ( user handle ) ) { for ( user info user info : m user manager . get profiles ( user handle ) ) { if ( user info . is managed profile ( ) ) { if ( ! pren ) { throw new illegal state exception ( __str__ ) ; } else { slog . e ( log_tag @$ __str__ ) ; return false ; } } } },user has
note : the lookup is enforcing security across users by making sure the caller can only access <PLACE_HOLDER> it hosts or provides .,widget widget = lookup widget locked ( app widget id @$ binder . get calling uid ( ) @$ calling package ) ; if ( widget != null ) { update app widget instance locked ( widget @$ views @$ partially ) ; },caller access
this indicates that the next 8 bytes represent the chunk <PLACE_HOLDER> @$ and chunk data comes after that .,if ( chunk size == __num__ ) { chunk size = signature input stream . read long ( ) ; if ( chunk size < __num__ ) { return false ; } chunk data offset += __num__ ; },bytes represent
dump the last row which has less than 16 <PLACE_HOLDER> .,if ( remainder != __num__ ) { final int row start index = ( full rows << __num__ ) + start index ; append hex dump row prefix ( dump @$ full rows @$ row start index ) ; final int row end index = row start index + remainder ; for ( int j = row start index ; j < row end index ; j ++ ) { dump . append ( byte2hex [ get unsigned byte ( buffer @$ j ) ] ) ; } dump . append ( hex_padding [ remainder ] ) ; dump . append ( __str__ ) ; for ( int j = row start index ; j < row end index ; j ++ ) { dump . append (,which has
the first read should allocate one shared memory <PLACE_HOLDER> and slot .,dfs test util . read file buffer ( fs @$ test_path1 ) ;,read allocate
any app can add new static shared <PLACE_HOLDER>,if ( scan result . static shared library info != null ) { return collections . singleton list ( scan result . static shared library info ) ; } final boolean has dynamic libraries = ( pkg . application info . flags & application info . flag_system ) != __num__ && scan result . dynamic shared library infos != null ; if ( ! has dynamic libraries ) { return null ; } final boolean is updated system app = pkg . is updated system app ( ) ;,app add
need to dig out actual destination map object and use map property <PLACE_HOLDER> to set the value on that target object ... .,if ( get dest field map get method ( ) != null || mapping utils . is supported map ( determine actual property type ( get dest field name ( ) @$ is dest field indexed ( ) @$ get dest field index ( ) @$ dest obj @$ true ) ) ) { prepare target object result result = prepare target object ( dest obj ) ; target object = result . target object ; prop descriptor = result . prop descriptor ; } else { prop descriptor = super . get dest property descriptor ( dest obj . get class ( ) ) ; },need object
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
perform <PLACE_HOLDER> substitution @$ this is useful when the library is using a <PLACE_HOLDER> in a key element @$ we however do not need to record these substitutions so feed it with a fake merging report .,merging report . builder builder = new merging report . builder ( merging report builder . get logger ( ) ) ; builder . get action recorder ( ) . record default node action ( library document . get root node ( ) ) ; perform place holder substitution ( manifest info @$ library document @$ builder ) ; if ( builder . has errors ( ) ) { builder . build ( ) . log ( m logger ) ; },library using
the shadows on the folds are split into two parts : solid shadows and gradients . every other fold has a solid shadow which overlays the <PLACE_HOLDER> fold . similarly @$ the folds in between these alternating folds also have an overlaying shadow . however @$ it is a gradient that takes up part of the fold as opposed to a solid shadow overlaying,m matrix [ x ] . set poly to poly ( m src @$ __num__ @$ m dst @$ __num__ @$ num_of_poly_points / __num__ ) ;,which overlays
this interface @$ a default implementation is supplied which uses the old <PLACE_HOLDER> . all new implementations must override this interface and should not use the other add response time <PLACE_HOLDER> .,int queue time ms = ( int ) details . get ( processing details . timing . queue @$ time unit . milliseconds ) ; int processing time ms = ( int ) details . get ( processing details . timing . processing @$ time unit . milliseconds ) ; add response time ( call name @$ schedulable . get priority level ( ) @$ queue time ms @$ processing time ms ) ;,which uses
does unaltered <PLACE_HOLDER> still match ref <PLACE_HOLDER> ?,bos . reset ( ) ; bs ref . serialize ( b ref @$ bos ) ; ser deser ( bs @$ b1 @$ bos . to byte array ( ) @$ null @$ null ) ;,block match
complete task a which should start another <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name and state ( plan item instances @$ __str__ @$ active ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active @$ waiting_for_repetition ) ; assert plan item instance state ( plan item instances @$ __str__ @$ available ) ; assert plan item instance state ( plan item instances @$ __str__ @$ available ) ;,which start
so that file not found exception has a <PLACE_HOLDER> to be thrown up from here without being caught .,get input stream ( ) ; return get content handler ( ) . get content ( this ) ;,exception has
shutdown the client <PLACE_HOLDER> which will trigger the channel pool manager <PLACE_HOLDER> sharing connecion shutdown,future callback < none > shutdown callback = new future callback < > ( ) ; client factory . shutdown ( shutdown callback ) ; shutdown callback . get ( __num__ @$ time unit . seconds ) ;,which trigger
no previous session or invalid @$ accept this <PLACE_HOLDER>,if ( session == null || ! is valid ( session ) ) { requested session id = id ; session = s ; } else { if ( s != null && is valid ( s ) ) throw new bad message exception ( __str__ + requested session id + __str__ + id ) ; },session accept
but if we are deserializing an exception too many received versions use a <PLACE_HOLDER> anyway .,long delta = next version - previous version ; if ( use tree sets for testing || ( delta > rvv_max_bitset_span && initial exception count * __num__ < delta ) ) { return new rvv exceptiont ( previous version @$ next version ) ; } return new rvv exceptionb ( previous version @$ next version ) ;,versions use
set up duplicate counter . expect exactly the responses corresponding to needs repair . these may @$ or may not @$ include the local <PLACE_HOLDER> .,list < long > expectedhs ids = new array list < long > ( needs repair ) ; duplicate counter counter = new duplicate counter ( host messenger . valhalla @$ message . get txn id ( ) @$ expectedhs ids @$ message @$ m_mailbox . geths id ( ) ) ; final duplicate counter key dc key = new duplicate counter key ( message . get txn id ( ) @$ message . get sp handle ( ) ) ; update or add duplicate counter ( dc key @$ counter ) ; m_unique id generator . update most recently generated unique id ( message . get unique id ( ) ) ;,these include
internal state updated @$ now notify the view <PLACE_HOLDER> .,m view . dispatch moved to display ( m display @$ config ) ;,updated notify
note to translators : jaxp is an acronym for the java api for xml processing . this message indicates that the xml parser provided to xsltc to process the xml input document had a configuration <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,parser had
if no resources in the holder @$ or if the holder has different resources loaded @$ then load the <PLACE_HOLDER> and set the new resources in the holder,if ( resources == null || ! config files . equals ( resources . get config resources ( ) ) ) { log . debug ( __str__ ) ; resources = new validation resources ( config files @$ get configuration from files ( config files ) ) ; validation resource holder . set ( resources ) ; } final configuration hive config = resources . get configuration ( ) ; problems . add all ( kerberos properties . validate principal and keytab ( this . get class ( ) . get simple name ( ) @$ hive config @$ principal @$ key tab @$ log ) ) ; return problems ;,resources load
only first node needed @$ all other have same <PLACE_HOLDER>,layout node n = s . nodes . get ( __num__ ) ;,other have
check for loops in the chain . if there are repeated <PLACE_HOLDER> @$ the set of <PLACE_HOLDER> in the chain will contain fewer <PLACE_HOLDER> than the chain,set < certificate > set = new hash set < > ( arrays . as list ( cert chain ) ) ; return set . size ( ) == cert chain . length ;,certs contain
stores all found style <PLACE_HOLDER> from the container,if ( container . get style map ( ) != null ) { super . assign style map ( container . get style map ( ) @$ get styles renderer ( ) ) ; },stores found
this check needs node <PLACE_HOLDER> to work correctly . if we 're not in ide mode @$ we do n't have that information @$ so just skip the check .,if ( length == __num__ ) { return ; },check needs
refresh store files post <PLACE_HOLDER> @$ this should not open already compacted files,hr1 . refresh store files ( true ) ; int num regions before split = admin . get regions ( test_table ) . size ( ) ;,files post
app engine does n't support thread <PLACE_HOLDER> @$ so do n't even try,if ( is app engine ( ) ) { return executor ; },engine support
the query delegate does n't support <PLACE_HOLDER> .,query delegate . set order by ( sorters ) ;,delegate support
move out one step @$ if the current instance has an outer <PLACE_HOLDER>,member definition outer member = thisc . find outer member ( ) ; if ( outer member == null ) { thise = null ; continue ; },instance has
change the reference to new topology . so everyone who captured old <PLACE_HOLDER> will see a consistent snapshot .,nodes = updated top ;,who captured
if the query is not set @$ then an incoming flow file is needed . otherwise fail the <PLACE_HOLDER>,if ( ! context . get property ( hiveql_select_query ) . is set ( ) && ! context . has incoming connection ( ) ) { final string error string = __str__ + __str__ ; get logger ( ) . error ( error string ) ; throw new process exception ( error string ) ; },then fail
if we have still have disabling services then we may have a stuck service so throw an <PLACE_HOLDER>,if ( service states . get disabling ( ) > __num__ ) { if ( should print ( properties ) ) { print services still disabling ( flow client @$ pg id ) ; } throw new command exception ( __str__ ) ; },service throw
we do not yet know <PLACE_HOLDER> these numbers are @$ so giving them meaningless names for now .,md signed encoded number a @$ b @$ c @$ name num ; switch ( code ) { case __str__ : { code = dmang . get and increment ( ) ; switch ( code ) { case __str__ : name num = new md signed encoded number ( dmang ) ; name num . parse ( ) ; name = name num . to string ( ) ; break ; case __str__ : { dmang . push modifier context ( ) ; md object object = new md objectcpp ( dmang ) ; object . parse ( ) ; dmang . pop context ( ) ; string builder builder = new string builder ( ) ; object . insert ( builder ) ; dmang . insert string,numbers are
check that the name matches the found <PLACE_HOLDER>,for ( object name found : found names ) { if ( name . apply ( found ) ) { result . add ( found ) ; } },name matches
test contents using contains key set <PLACE_HOLDER>,int size2 = __num__ ; int previous key = - __num__ ; for ( int key : map . key set ( ) ) { assert true ( bs . get ( key ) ) ; size2 ++ ; assert true ( previous key < __num__ || ( ascending ? key - previous key > __num__ : key - previous key < __num__ ) ) ; previous key = key ; } assert equals ( size2 @$ size ) ;,contents contains
if the last marker instruction encountered was an outlineable chunk <PLACE_HOLDER> @$ it means that the current instruction marks the <PLACE_HOLDER> of a chunk that contained child chunks . those children might need to be examined below in case they are better candidates for outlining than the current chunk .,if ( ! open chunk at curr level ) { nested sub chunks = curr level chunks ; curr level chunks = ( array list ) sub chunk stack . pop ( ) ; },instruction marks
the limit and offset drop the first and last <PLACE_HOLDER> @$ leaving 2 <PLACE_HOLDER> of 1 and 1 group of 2 .,validate table of longs ( client @$ __str__ + tb + __str__ @$ new long [ ] [ ] { { __num__ @$ __num__ } @$ { __num__ @$ __num__ } } ) ;,limit drop
attempting to create transform a second time with the same sp is should throw an <PLACE_HOLDER> ...,try { m ip sec service . create transform ( ip sec config @$ new binder ( ) @$ __str__ ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { },time throw
not enough space to hold the whole <PLACE_HOLDER> @$ we will try again later .,if ( offset == limits . iov_max || ! iov array . add ( buf @$ index @$ len ) ) { return false ; },space hold
no exception means <PLACE_HOLDER> passed,return true ;,exception means
if limit specified and is smaller than reasonable large batch <PLACE_HOLDER> then we force batch <PLACE_HOLDER> to be the same as limit @$ but negative @$ this force cursor to close right after result is sent,if ( limit <= large_batch_size ) { cursor . batch size ( - limit ) ; },size force
set view 0 default <PLACE_HOLDER> .,bgfx_set_view_rect ( __num__ @$ __num__ @$ __num__ @$ width @$ height ) ;,set view
ensure every key has a value and substitute <PLACE_HOLDER> for values,if ( ivs . size ( ) > __num__ ) { int template key count = __num__ ; for ( interpolation variable variable : ivs . key set ( ) ) { string value = row data provider . get template key value ( variable . get var name ( ) ) ; if ( ! __str__ . equals ( value ) ) { template key count ++ ; } } if ( action instanceof free form line ) { add action = template key count == ivs . size ( ) ; } else if ( template key count > __num__ ) { add action = true ; } },key has
2 possibilities @$ sometimes livy does n't return the real cancel <PLACE_HOLDER>,assert true ( message . contains ( __str__ ) || message . contains ( __str__ ) ) ;,possibilities return
finally @$ the consumer operation provides formatted log <PLACE_HOLDER>,log . info ( string . format ( __str__ @$ kw . get key ( ) . get key ( ) @$ kw . get value ( ) ) ) ;,operation provides
get unsafe access impl.unsafe get int <PLACE_HOLDER>,registration r = new registration ( invocation plugins @$ my unsafe access . class ) ; truffle graph builder plugins . register unsafe load store plugins ( r @$ false @$ null @$ java kind . int ) ; super . register invocation plugins ( invocation plugins ) ;,impl.unsafe get
if the user specified a custom <PLACE_HOLDER> for the tab indicators @$ then do not draw the bottom strips .,if ( ! m draw bottom strips ) { return ; },user specified
j layer 's decoder throws array <PLACE_HOLDER> out of bounds exception sometimes ! ?,bitstream . close frame ( ) ; output . write ( output buffer . get buffer ( ) @$ __num__ @$ output buffer . reset ( ) ) ;,decoder throws
check the rowcount value @$ if it is equal to 0 it means the call did not delete the <PLACE_HOLDER> from federation state store,if ( cstmt . get int ( __num__ ) == __num__ ) { string err msg = __str__ + request . get application id ( ) + __str__ ; federation state store utils . log and throw store exception ( log @$ err msg ) ; },call delete
if it 's a virtual service then receive the <PLACE_HOLDER> from parent service,if ( object . get virtual ( ) == __num__ ) { rec data = get data from service ( object . get parent ( ) ) ; } else { rec data = get data from service ( service ) ; },service receive
copy is necessary since the instance info builder just uses the original <PLACE_HOLDER> @$ and we do n't want to corrupt the global eureka copy of the object which may be used by other clients in our system,instance info copy = new instance info ( ii ) ; if ( is secure ) { ii = new instance info . builder ( copy ) . set secure port ( override port ) . build ( ) ; } else { ii = new instance info . builder ( copy ) . set port ( override port ) . build ( ) ; },builder uses
should not throw <PLACE_HOLDER>,handler . on throwable ( e ) ;,not throw
another test will make sure disk space health check <PLACE_HOLDER> the right stuff @$ so skipping the textual return values,assert that ( response . get data ( ) . get ( ) @$ has entry ( __str__ @$ total_disk - used ) ) ; assert that ( response . get data ( ) . get ( ) @$ has entry ( __str__ @$ text ) ) ; assert that ( response . get data ( ) . get ( ) @$ has entry ( __str__ @$ total_disk ) ) ;,test make
update scores should not update any <PLACE_HOLDER> since they are both unregistered,m network score service . update scores ( new scored network [ ] { scored_network } ) ;,scores update
do n't use delayed clipboard rendering for the transferable 's data . if we did that @$ we would c<PLACE_HOLDER> transferable.get transfer data on the toolkit thread @$ which is a security hole . get <PLACE_HOLDER> of the target formats into which the transferable can be translated . then @$ for each format @$ translate the data and post it to the clipboard .,data transferer data transferer = data transferer . get instance ( ) ; long [ ] format array = data transferer . get formats for transferable as array ( contents @$ flavor map ) ; declare types ( format array @$ this ) ; map < long @$ data flavor > format map = data transferer . get formats for transferable ( contents @$ flavor map ) ; for ( map . entry < long @$ data flavor > entry : format map . entry set ( ) ) { long format = entry . get key ( ) ; data flavor flavor = entry . get value ( ) ; try { byte [ ] bytes = data transferer . get instance ( ) . translate transferable,formats get
reachable only if the file naming is incorrect by current standard . thus we log an <PLACE_HOLDER> instead of recording failure to uma .,if ( tries >= max_tries_allowed || tries < __num__ ) { log . e ( tag @$ __str__ + minidump file name + __str__ ) ; return ; },reachable log
set the scrollbars <PLACE_HOLDER> . if the thumb has reached the end of the scrollbar @$ then just set the <PLACE_HOLDER> to its maximum . otherwise compute the <PLACE_HOLDER> as accurately as possible .,if ( thumb pos == thumb max ) { if ( scrollbar . get orientation ( ) == j scroll bar . vertical || scrollbar . get component orientation ( ) . is left to right ( ) ) { scrollbar . set value ( model . get maximum ( ) - model . get extent ( ) ) ; } else { scrollbar . set value ( model . get minimum ( ) ) ; } } else { float value max = model . get maximum ( ) - model . get extent ( ) ; float value range = value max - model . get minimum ( ) ; float thumb value = thumb pos - thumb min ; float thumb range = thumb max,set compute
menu currently always enabled @$ but dont do <PLACE_HOLDER> unless a message is being held,if ( breakpoint management interface . is hold message ( null ) ) { breakpoint management interface . cont ( ) ; },dont do
and the outer struct has <PLACE_HOLDER> that are defined to be within the footprint of the embedded struct 's trailing padding .,debug info entry intdie = add int ( cu ) ;,struct has
player reads all <PLACE_HOLDER> in arbitrary directory structure and creates a map task for each file,string bids = string utils . join ( backup ids @$ __str__ ) ; if ( log . is debug enabled ( ) ) { log . debug ( __str__ + bids ) ; } list < pair < table name @$ path > > processed table list = new array list < > ( ) ; boolean finished tables = false ; connection conn = connection factory . create connection ( get conf ( ) ) ; backup system table table = new backup system table ( conn ) ; file system fs = file system . get ( get conf ( ) ) ; try { table . start backup exclusive operation ( ) ; table . start merge operation ( backup ids ) ; string,player reads
set output collector for any reduce sink <PLACE_HOLDER> in the pipeline .,list < operator < ? > > children = new array list < > ( ) ; children . add ( reducer ) ; children . add all ( dummy ops ) ; create output map ( ) ; operator utils . set children collector ( children @$ out map ) ; check abort condition ( ) ; reducer . set reporter ( reporter ) ; mapred context . get ( ) . set reporter ( reporter ) ;,collector reduce
let 's park this <PLACE_HOLDER> and wait for a primary !,synchronized ( this ) { stop watch timer = new stop watch ( true ) ; long warn time = get distribution manager ( ) . get config ( ) . get ack wait threshold ( ) * __num__ ; boolean logged warning = false ; try { for ( ; ; ) { get advisee ( ) . get cancel criterion ( ) . check cancel in progress ( null ) ; final internal cache cache = get bucket ( ) . get cache ( ) ; if ( cache != null && cache . is cache at shutdown all ( ) ) { throw cache . get cache closed exception ( __str__ ) ; } if ( get bucket redundancy ( ) == - __num__ ),"s" park
we expect that both nn 1 and nn 2 will have some <PLACE_HOLDER> of deletions queued up for the d ns .,banner ( __str__ ) ; block manager test util . compute invalidation work ( nn1 . get namesystem ( ) . get block manager ( ) ) ; banner ( __str__ ) ; block manager test util . compute invalidation work ( nn2 . get namesystem ( ) . get block manager ( ) ) ;,nn have
user 1 sends an empty room configuration <PLACE_HOLDER> which indicates that we want an instant room,form form = new form ( form . type_submit ) ; form field field = new form field ( __str__ ) ; field . set type ( __str__ ) ; form . add field ( field ) ; form . set answer ( __str__ @$ arrays . as list ( __str__ ) ) ; muc . send configuration form ( form ) ;,user sends
set up the 2 d array used to hold the names . the first column contains the olson <PLACE_HOLDER> .,string [ ] [ ] result = new string [ available time zone ids . length ] [ __num__ ] ; for ( int i = __num__ ; i < available time zone ids . length ; ++ i ) { result [ i ] [ __num__ ] = available time zone ids [ i ] ; } long native start = system . current time millis ( ) ; fill zone strings ( locale . to string ( ) @$ result ) ; long native end = system . current time millis ( ) ; intern strings ( result ) ;,column contains
update vertex positions for ' z samples ' parallel <PLACE_HOLDER> .,float z rate = ( radius * __num__ ) / ( float ) ( z samples ) ; float z height = - radius + ( z rate / __num__ ) ; float rb = __num__ / z samples ; float b = rb / __num__ ; for ( int k = __num__ ; k < z samples ; k ++ ) { angle = __num__ ; float scale = __num__ * fast math . sqrt ( b - b * b ) ; for ( int i = __num__ ; i < samples ; i ++ ) { float x = radius * fast math . cos ( angle ) ; float y = radius * fast math . sin ( angle ) ; pb . put,"" z
this situation is possible only if a callee throws an <PLACE_HOLDER> which type extends throwable directly,if ( cause instanceof command action execution exception ) { command action execution exception command action execution exception = ( command action execution exception ) cause ; cause = command action execution exception . get cause ( ) ; },callee throws
one file may have several <PLACE_HOLDER>,final list < b object > l = patho . get list ( ) ;,file have
if the app did everything <PLACE_HOLDER> @$ return without logging .,if ( app op mode == app ops manager . mode_allowed ) { return location permission result . allowed ; } else { log . i ( tag @$ query . calling package + __str__ + location type for log + __str__ + __str__ ) ; return app ops mode to permission result ( app op mode ) ; },app did
single project build entry <PLACE_HOLDER>,project building result result = lookup ( org . apache . maven . project . project builder . class ) . build ( pom file @$ configuration ) ; assert equals ( __num__ @$ result . get project ( ) . get artifacts ( ) . size ( ) ) ;,project build
j rockit throws this <PLACE_HOLDER> instead of returning null as the javadocs say it should . see bug 36348,this . set collection usage unsupported ( mp ) ;,rockit throws
completing the first task should not end the <PLACE_HOLDER>,task service . complete ( taska . get id ( ) ) ; assert equals ( __num__ @$ task query . list ( ) . size ( ) ) ;,task end
confirm that the result represents a v<PLACE_HOLDER>eo . otherwise @$ the item will not contain a v<PLACE_HOLDER>eo <PLACE_HOLDER> .,if ( r id . get kind ( ) . equals ( __str__ ) ) { thumbnail thumbnail = single video . get snippet ( ) . get thumbnails ( ) . get default ( ) ; system . out . println ( __str__ + r id . get video id ( ) ) ; system . out . println ( __str__ + single video . get snippet ( ) . get title ( ) ) ; system . out . println ( __str__ + thumbnail . get url ( ) ) ; system . out . println ( __str__ ) ; },item contain
in case the normalized date requires the other calendar <PLACE_HOLDER> @$ we need to recalculate it using the other one .,base calendar ncal = get calendar system ( fast time ) ; if ( ncal != cal ) { date = ( base calendar . date ) ncal . new calendar date ( tz ) ; date . set normalized date ( y @$ m @$ d ) . set time of day ( hh @$ mm @$ ss @$ ms ) ; fast time = ncal . get time ( date ) ; } return date ;,date requires
same test <PLACE_HOLDER> as before @$ with the first 3 byte buffers having data now .,generator . generate events ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ events ) ;,same test
the subscriber still sees the 'error ' <PLACE_HOLDER> since actual.on error is called before the after terminate callback,ts . assert error message ( __str__ ) ;,subscriber sees
timeout . we do n't need to call ` terminate ` here @$ because wait for automatically terminates the <PLACE_HOLDER> in case of a timeout .,return wait result . timeout ; default :,wait terminates
if the autocomplete whitelist does n't contain the <PLACE_HOLDER> @$ skip storing its value,continue ;,whitelist contain
creating a new persistent cache does not cause a <PLACE_HOLDER>,cache . save ( ) ; cache = create cache ( ) ; button . pressed = false ; build artifacts ( persistent builder ( cache ) @$ hello ) ; assert that ( button . pressed ) . is false ( ) ;,cache cause
if we have an image src and the target is reinitializing <PLACE_HOLDER> the get initial image so that it will wait until the target region is fully initialized before responding to the get image request . otherwise @$ the source may respond with no data because it is still initializing @$ e.g . loading a snapshot .,cache observer holder . get instance ( ) . after markinggii started ( ) ;,target reinitializing
genrule uses legacy <PLACE_HOLDER>,genrule output path = build target paths . get gen path ( filesystem @$ build target factory . new instance ( __str__ ) @$ __str__ ) . resolve ( __str__ ) . to string ( ) ;,genrule uses
we wo n't race another upgrade attempt because only one thread will get the <PLACE_HOLDER> from the map,integer timeout = local sessions with timeouts . remove ( session id ) ; if ( timeout != null ) { log . info ( __str__ @$ long . to hex string ( session id ) ) ; track session ( session id @$ timeout ) ; upgrading sessions . put ( session id @$ timeout ) ; local session tracker . remove session ( session id ) ; return timeout ; },thread get
all in the same call stack @$ the upgrade codec should receive the <PLACE_HOLDER> @$ written the upgrade response @$ and upgraded the pipeline .,assert true ( write upgrade message ) ; assert false ( write flushed ) ; assert null ( ctx . pipeline ( ) . get ( http server codec . class ) ) ; assert not null ( ctx . pipeline ( ) . get ( __str__ ) ) ; in read call = false ;,codec receive
if an exception here occurs then there is the danger that urls which had been in the crawler are overwritten a second <PLACE_HOLDER> to prevent that @$ we reject urls in these events,concurrent log . log exception ( e ) ; return __str__ + e . get message ( ) ;,urls overwritten
copy dex apply <PLACE_HOLDER> directly on dex,steps . add ( copy step . for file ( filesystem @$ input primary dex path @$ output primary dex path ) ) ; steps . add ( new default shell step ( filesystem . get root path ( ) @$ immutable list . of ( reorder tool . to string ( ) @$ reorder data file . to string ( ) @$ output primary dex path . to string ( ) ) ) ) ;,dex apply
ruby client should set the <PLACE_HOLDER> unescaped in the api file,assert true ( file utils . read file to string ( file @$ standard charsets . utf_8 ) . contains ( __str__ ) ) ;,client set
get all cookies that domain matches the <PLACE_HOLDER>,for ( map . entry < uri @$ list < http cookie > > entry : map . entry set ( ) ) { if ( uri . equals ( entry . get key ( ) ) ) { continue ; } list < http cookie > entry cookies = entry . get value ( ) ; for ( iterator < http cookie > i = entry cookies . iterator ( ) ; i . has next ( ) ; ) { http cookie cookie = i . next ( ) ; if ( ! http cookie . domain matches ( cookie . get domain ( ) @$ uri . get host ( ) ) ) { continue ; } if ( cookie . has expired ( ),domain matches
on finish : when function returns something : store register 0 ; return value <PLACE_HOLDER> store register 2 <PLACE_HOLDER> store register 1 <PLACE_HOLDER> push register 0 return when function does not return anything : store register 2 <PLACE_HOLDER> store register 1 <PLACE_HOLDER> when original function has some returns @$ but no return at the end of function : push undefined locjump : store register,int return reg = - __num__ ; pos = start index + count - __num__ ; if ( code . get ( pos ) instanceof action return ) { pos -- ; if ( pos < start index ) { return false ; } if ( ! ( code . get ( pos ) instanceof action push ) ) { return false ; } action push pu = ( action push ) code . get ( pos ) ; if ( pu . values . size ( ) != __num__ ) { return false ; } if ( ! ( pu . values . get ( __num__ ) instanceof register number ) ) { return false ; } register number rn = ( register number ) pu .,store register
build a socks 5 stream host info containing the <PLACE_HOLDER> and the port of the proxy,bytestream stream host info = socks5 packet utils . create bytestream response ( proxyjid @$ initiatorjid ) ; stream host info . add stream host ( proxyjid @$ proxy address @$ __num__ ) ;,info containing
update last applied tx <PLACE_HOLDER> even in case of error @$ since some ops may have been successfully applied before the error .,last applied tx id = loader . get last applied tx id ( ) ;,update applied
the second observer should only get the newest <PLACE_HOLDER> and any later <PLACE_HOLDER>s .,live data . observe ( m lifecycle owner @$ new observer < string > ( ) { @ override public void on changed ( @ nullable string s ) { output2 . add ( s ) ; } } ) ; live data . remove observer ( m observer ) ; processor . on next ( __str__ ) ; assert that ( m live data output @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ; assert that ( output2 @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ;,observer get
group set position includes all the <PLACE_HOLDER>,final immutable bit set group set = immutable bit set . of ( group set positions ) ; list < aggregate call > aggregate calls = lists . new array list ( ) ; rel data type agg fn ret type = type converter . convert ( type info factory . long type info @$ cluster . get type factory ( ) ) ; aggregate call aggregate call = hive calcite util . create single arg agg call ( __str__ @$ cluster @$ type info factory . long type info @$ input . get row type ( ) . get field list ( ) . size ( ) @$ agg fn ret type ) ; aggregate calls . add ( aggregate call ) ; return new hive aggregate,position includes
verify the signature .. shows the response was generated by someone who knew the associated private <PLACE_HOLDER>,signature sig = signature . get instance ( sig alg . get object id ( ) . get id ( ) @$ __str__ ) ; sig . init verify ( pubkey ) ; sig . update ( content . get bytes ( ) ) ; return sig . verify ( sig bits ) ;,who knew
a volt db extension to customize the sql function set <PLACE_HOLDER>,parameter arg = __num__ ;,extension customize
indent:11 exp:12 warn indent:8 <PLACE_HOLDER>,return __str__ ;,exp:12 warn
if this <PLACE_HOLDER> is finished @$ notify the client of this @$ so the client will destroy this <PLACE_HOLDER>,if ( pages . is empty ( ) && no more pages ) { return empty results ( task instance id @$ current sequence id . get ( ) @$ true ) ; },client destroy
skip block if block error has no instruction <PLACE_HOLDER>,continue ;,error has
special case because xp has no <PLACE_HOLDER> for menus,if ( part == part . menu ) { if ( flat menus ) { return new xp fill border ( ui manager . get color ( __str__ ) @$ __num__ ) ; } else { return null ; } },xp has
execute process does n't wait for finishing to drain error stream if it 's configure not to redirect stream . this causes test failure when draining the error stream did n't finish fast enough before the thread of this test case method checks the warn msg <PLACE_HOLDER> . so @$ this loop wait for a while until the log msg <PLACE_HOLDER> becomes expected number,final int expected warning messages = __num__ ; final int max retry = __num__ ; for ( int i = __num__ ; i < max retry && ( runner . get logger ( ) . get warn messages ( ) . size ( ) < expected warning messages ) ; i ++ ) { try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { } } final list < log message > warn messages = runner . get logger ( ) . get warn messages ( ) ; assert equals ( __str__ + __str__ @$ expected warning messages @$ warn messages . size ( ) ) ; final list < mock flow file > succeeded = runner . get flow files,thread checks
3 rd parameter has no <PLACE_HOLDER> for ownerless,test ( false @$ true @$ false ) ;,parameter has
base condition 2 : check if we are already at the target . if so @$ return an empty <PLACE_HOLDER> .,if ( ( src node . equals ( dest node ) ) && dest dataset descriptor . contains ( src dataset descriptor ) ) { return new array list < > ( ) ; } linked list < flow edge context > edge queue = new linked list < > ( ) ; edge queue . add all ( get next edges ( src node @$ src dataset descriptor @$ dest dataset descriptor ) ) ; for ( flow edge context flow edge context : edge queue ) { this . path map . put ( flow edge context @$ flow edge context ) ; },condition return
to support delta compilations @$ we always restart the <PLACE_HOLDER> . the individual passes are responsible for not reprocessing old code .,goto phase ( phases . initialization ) ; through phase = math . min ( through phase @$ phases . all ) ; while ( through phase >= phase && phase <= phases . all ) { if ( phase == phases . semantic_analysis ) { do phase operation ( resolve ) ; if ( dequeued ( ) ) continue ; } process phase operations ( phase ) ; process new phase operations ( phase ) ; if ( progress callback != null ) progress callback . call ( this @$ phase ) ; complete phase ( ) ; apply to source units ( mark ) ; if ( dequeued ( ) ) continue ; goto phase ( phase + __num__ ) ; if ( phase == phases,compilations restart
updates the port when the user changes the security <PLACE_HOLDER> . this allows us to show a reasonable default which the user can change .,m security type view . set on item selected listener ( new adapter view . on item selected listener ( ) { @ override public void on item selected ( adapter view < ? > parent @$ view view @$ int position @$ long id ) { if ( m current security type view position != position ) { update port from security type ( ) ; validate fields ( ) ; } } @ override public void on nothing selected ( adapter view < ? > parent ) { } } ) ;,user changes
background uses total duration ms locked @$ while total uses total <PLACE_HOLDER> locked,dump timer ( proto @$ uid proto . sync . total @$ timer @$ raw realtime us @$ which ) ; dump timer ( proto @$ uid proto . sync . background @$ bg timer @$ raw realtime us @$ which ) ; proto . end ( sy token ) ;,background uses
send event notifications saying that all our <PLACE_HOLDER> are offline . the protocol does not implement top level <PLACE_HOLDER> nor subgroups for top level groups so a simple nested loop would be enough .,if ( new status . equals ( offline status ) ) { iterator < contact group > groups iter = get server stored contact list root ( ) . subgroups ( ) ; while ( groups iter . has next ( ) ) { contact group group = groups iter . next ( ) ; iterator < contact > contacts iter = group . contacts ( ) ; while ( contacts iter . has next ( ) ) { contact jabber impl contact = ( contact jabber impl ) contacts iter . next ( ) ; update contact status ( contact @$ offline status ) ; } } iterator < contact > contacts iter = get server stored contact list root ( ) . contacts ( ) ;,protocol implement
bug : diagnostic contains : missing disposable handling : apply auto dispose or cache the disposable <PLACE_HOLDER> manually and enable lenient mode .,single . just ( __num__ ) . subscribe with ( new test observer < > ( ) ) ;,auto dispose
the kings hand should not pass any <PLACE_HOLDER> before he received one,verify zero interactions ( observer ) ;,hand pass
if the java library does n't generate any <PLACE_HOLDER> @$ it does n't contribute a gwt module,if ( java library . get source path to output ( ) == null ) { return rule deps ; } build rule gwt module = graph builder . compute if absent ( java library . get build target ( ) . assert unflavored ( ) . with flavors ( java library . gwt_module_flavor ) @$ gwt module target -> { immutable sorted set < source path > files for gwt module = immutable sorted set . < source path > natural order ( ) . add all ( java library . get sources ( ) ) . add all ( java library . get resources ( ) ) . build ( ) ; immutable sorted set < build rule > deps = immutable sorted set . copy,library generate
<PLACE_HOLDER> of the skip region is verified and <PLACE_HOLDER> is not skipping the verified <PLACE_HOLDER>,if ( verified position > get file pointer ( ) ) { input . seek ( verified position ) ; skip bytes ( pos - verified position ) ; } else { skip bytes ( pos - get file pointer ( ) ) ; },portion skipping
get the super<PLACE_HOLDER> type def . ot the <PLACE_HOLDER> the <PLACE_HOLDER> scope belongs to,try { java type definition super class = get super class type definition ( ( ( class scope ) scope ) . get class declaration ( ) . get node ( ) @$ null ) ; java type definition found type def = get field type ( super class @$ image @$ accessing class ) ; if ( found type def != null ) { return found type def ; } } catch ( class cast exception ignored ) { },def ot
metadata files have no <PLACE_HOLDER>,assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( entropy_marker ) ) ) ; assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( resolved_marker ) ) ) ;,files have
start new transaction does n't recover cache <PLACE_HOLDER> on failed channel .,try ( client transaction tx1 = client . transactions ( ) . tx start ( ) ) { fail ( ) ; } catch ( client exception expected ) { },transaction recover
if that is first collection we split single composite key on several keys @$ each of those composite keys contain single <PLACE_HOLDER> from collection,if ( ! contains collection ) for ( int i = __num__ ; i < collection size ; i ++ ) { final o composite key composite key = new o composite key ( first key . get keys ( ) ) ; composite keys . add ( composite key ) ; } else throw new o index exception ( __str__ ) ;,each contain
simple equals can cause troubles here because of how equals works <PLACE_HOLDER> . between lists and sets .,return collection utils . is equal collection ( state objects @$ that . state objects ) ;,equals works
the finally clause will send an <PLACE_HOLDER> .,remove decoder ( imgd ) ; if ( thread . current thread ( ) . is interrupted ( ) || ! thread . current thread ( ) . is alive ( ) ) { error all consumers ( imgd . queue @$ true ) ; } else { error all consumers ( imgd . queue @$ false ) ; },clause send
assumption : the client has already retrieved <PLACE_HOLDER> to the given address,if ( addr . equals ( code unit address ) ) { continue ; },client retrieved
media player 2 uses <PLACE_HOLDER> instead of application mime types .,if ( mime types . application_cea608 . equals ( mime type ) ) { media format . set string ( media format . key_mime @$ mimetype_text_cea_608 ) ; } else if ( mime types . application_cea708 . equals ( mime type ) ) { media format . set string ( media format . key_mime @$ mimetype_text_cea_708 ) ; },media uses
the expiration thread is just finishing.. so dont do <PLACE_HOLDER>,log . debug ( __str__ @$ e ) ; thread . current thread ( ) . interrupt ( ) ; throw new runtime exception ( __str__ @$ e ) ;,dont do
first byte @$ assume no further <PLACE_HOLDER> .,return primary | ( ( base primary & __num__ ) + ( ( long ) offset << __num__ ) ) ;,byte assume
now let 's visit the <PLACE_HOLDER> of the class,for ( property node pn : get properties ( ) ) { visitor . visit property ( pn ) ; } for ( field node fn : get fields ( ) ) { visitor . visit field ( fn ) ; } for ( constructor node cn : get declared constructors ( ) ) { visitor . visit constructor ( cn ) ; } visit methods ( visitor ) ;,"s" visit
a simple new multipart add multipart <PLACE_HOLDER> @$ multipart body header and data to multipart list,local mixed = false ; current file upload = file upload ; during mixed mode = false ;,multipart add
when data file contains more <PLACE_HOLDER> than header file,int extra columns = __num__ ; run import ( __str__ @$ bad . get absolute path ( ) @$ __str__ @$ integer . to string ( node ids . size ( ) * extra columns ) @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ string . value of ( config . array delimiter ( ) ) @$ __str__ + node header ( config ) . get absolute path ( ) + __str__ + node data ( false @$ config @$ node ids @$ true @$ charset . default charset ( ) @$ extra columns ) . get absolute path ( ) @$ __str__ @$ relationship header ( config ) . get absolute path ( ) + __str__ + relationship data ( false @$ config @$ node ids,file contains
fail fast in case this structure has no associated <PLACE_HOLDER> .,if ( this . template boundaries processors . length == __num__ ) { this . next . handle template end ( itemplate end ) ; return ; },structure has
if the local is just being created @$ mark the store as the <PLACE_HOLDER> of its live range . note that it might have been created by initialize variables already @$ which would have set the <PLACE_HOLDER> of the live range already .,if ( create local ) { _local . set start ( store inst ) ; } string signature = _type . to signature ( ) ;,which set
only recreate the context if the sampler made a <PLACE_HOLDER>,if ( sampled == null && ( sampled = sampler . try sample ( request ) ) != null ) { extracted = extracted . sampled ( sampled . boolean value ( ) ) ; } return extracted . context ( ) != null ? tracer . join span ( extracted . context ( ) ) : tracer . next span ( extracted ) ;,sampler made
generate the <PLACE_HOLDER>s use <PLACE_HOLDER> 1 for data node,string rack1 = get rack ( __num__ @$ level ) ;,racks use
<PLACE_HOLDER> has n't changed ; construct old binding using <PLACE_HOLDER> from new binding,binding old bd = new binding ( new bd . get name ( ) @$ null @$ new bd . is relative ( ) ) ; naming event e = new naming event ( event src @$ naming event . object_changed @$ new bd @$ old bd @$ new long ( changeid ) ) ; support . queue event ( e @$ naming listeners ) ;,construct using
completing the task will end the process <PLACE_HOLDER>,task service . complete ( task . get id ( ) ) ; assert process ended ( proc id ) ;,task end
need shut down background <PLACE_HOLDER> gracefully @$ driver.close will inform background <PLACE_HOLDER> a cancel request is sent .,if ( should run async ( ) && state != operation state . canceled && state != operation state . timedout ) { future < ? > background handle = get background handle ( ) ; if ( background handle != null ) { boolean success = background handle . cancel ( true ) ; string query id = query state . get query id ( ) ; if ( success ) { log . info ( __str__ + query id ) ; } else if ( state == operation state . canceled ) { log . info ( __str__ + query id ) ; } } } if ( driver != null ) { driver . close ( ) ; driver . destroy ( ) ; },driver.close inform
the decimal column vector set method will quickly copy the deserialized decimal writable <PLACE_HOLDER> .,( ( decimal column vector ) col vector ) . set ( batch index @$ deserialize read . current hive decimal writable ) ;,method copy
since a is in the using list @$ lr.a and rr.a are the same . this is not ambiguous . the two aliases reference the same <PLACE_HOLDER> .,query = __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . seqscan ) ; query = __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . projection @$ plan node type . orderby @$ plan node type . nestloop @$ plan node type . seqscan @$ plan node type . seqscan ) ; query = __str__ + __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . projection @$ plan node type . orderby @$ plan node type . seqscan ) ; query = __str__ ; compile to top down tree,aliases reference
non existing offset will be seeking <PLACE_HOLDER> offset,new input split = new kafka input split ( tp . topic ( ) @$ tp . partition ( ) @$ existing input split . get end offset ( ) @$ existing input split . get end offset ( ) @$ existing input split . get path ( ) ) ;,non seeking
check the v 8 has no co location <PLACE_HOLDER>,execution vertex [ ] v8s = eg . get job vertex ( v8 . getid ( ) ) . get task vertices ( ) ; for ( int i = __num__ ; i < v8 . get parallelism ( ) ; i ++ ) { assert null ( v8s [ i ] . get location constraint ( ) ) ; } e . print stack trace ( ) ; fail ( e . get message ( ) ) ;,8 has
wait for monitoring interval time and verify server is still in running <PLACE_HOLDER>,resume and wait ( monitoring_interval + __num__ ) ; assert equals ( operation mode . running @$ voltdb . instance ( ) . get mode ( ) ) ;,server running
change offset to 1 @$ preview should show <PLACE_HOLDER> at offset 1,set offset field value ( __num__ ) ; assert equals ( __str__ @$ preview text field . get text ( ) ) ;,preview show
if requested id contains invalid <PLACE_HOLDER> @$ then sso can not exist and would otherwise cause sso lookup to fail,try { base64 . get url decoder ( ) . decode ( id ) ; } catch ( illegal argument exception e ) { return null ; } batcher < batch > batcher = this . manager . get batcher ( ) ;,id contains
guice will only guarantee this assertion if multibinder ensures the bindings <PLACE_HOLDER> .,injector injector = guice . create injector ( ab @$ modules . override ( ab ) . with ( ab ) ) ; assert equals ( immutable set . of ( __str__ @$ __str__ ) @$ injector . get instance ( key . get ( set of string ) ) ) ;,multibinder ensures
now make all calls to the fake group mapper throw <PLACE_HOLDER>,fake group mapping . set throw exception ( true ) ;,calls throw
prevents warning when script imports <PLACE_HOLDER> that will get compiled,options . add ( __str__ ) ;,script imports
because super.equals compares the event <PLACE_HOLDER> @$ getting here means that we 've somehow managed to create 2 finished events for the same started event .,throw new unsupported operation exception ( __str__ ) ;,super.equals compares
a volt db extension to support the assume unique <PLACE_HOLDER>,index = index . set assume unique ( c . assume unique ) ;,extension support
event bus goes here . this method is only c<PLACE_HOLDER>lled if one of the binding providers provide <PLACE_HOLDER> binding for the given 'item n<PLACE_HOLDER>me ' .,if ( command instanceof on off type ) { final on off type switch command = ( on off type ) command ; final open sprinkler binding provider binding provider = find first matching binding provider ( item name @$ command ) ; final int station = binding provider . get station number ( item name ) ; if ( station < __num__ || station >= number of stations ) { logger . warn ( __str__ + station + __str__ + __num__ + __str__ + number of stations + __str__ ) ; return ; } switch ( switch command ) { case on : open sprinkler . open station ( station ) ; break ; case off : open sprinkler . close station ( station ) ; break,one provide
note to translators : the following message should not normally be displayed to users . it describes a situation in which the processor has detected an internal consistency <PLACE_HOLDER> in itself @$ and it provides this message for the developer to help diagnose the <PLACE_HOLDER> . the substitution text provides further information in order to diagnose the <PLACE_HOLDER> . the name 'redundent expr eliminator,return new object [ ] [ ] { { __str__ @$ __str__ } @$ { er_no_curlybrace @$ __str__ } @$ { er_illegal_attribute @$ __str__ } @$ { er_null_sourcenode_applyimports @$ __str__ } @$ { er_cannot_add @$ __str__ } @$ { er_null_sourcenode_handleapplytemplates @$ __str__ } @$ { er_no_name_attrib @$ __str__ } @$ { er_template_not_found @$ __str__ } @$ { er_cant_resolve_name_avt @$ __str__ } @$ { er_requires_attrib @$ __str__ } @$ { er_must_have_test_attrib @$ __str__ } @$ { er_bad_val_on_level_attrib @$ __str__ } @$ { er_processinginstruction_name_cant_be_xml @$ __str__ } @$ { er_processinginstruction_notvalid_ncname @$ __str__ } @$ { er_need_match_attrib @$ __str__ } @$ { er_need_name_or_match_attrib @$ __str__ } @$ { er_cant_resolve_nsprefix @$ __str__ } @$ { er_illegal_value @$ __str__ } @$ { er_no_ownerdoc @$ __str__ } @$ { er_elemtemplateelem_err @$ __str__ } @$,processor detected
synchronize on scanner read <PLACE_HOLDER>s so that nobody calculates get smallest read <PLACE_HOLDER> @$ before scanner read <PLACE_HOLDER>s is updated .,isolation level isolation level = scan . get isolation level ( ) ; long mvcc read point = package private field accessor . get mvcc read point ( scan ) ; synchronized ( scanner read points ) { if ( mvcc read point > __num__ ) { this . read pt = mvcc read point ; } else if ( nonce == h constants . no_nonce || rs services == null || rs services . get nonce manager ( ) == null ) { this . read pt = get read point ( isolation level ) ; } else { this . read pt = rs services . get nonce manager ( ) . get mvcc from operation context ( nonce group @$ nonce ) ; } scanner,synchronize read
we expect only 1 request @$ but we ask for 2 <PLACE_HOLDER> here so that if a misbehaving client sends more than 1 <PLACE_HOLDER> @$ server call will catch it . note that disabling auto inbound flow control has no effect on unary calls .,call . request ( __num__ ) ; return new unary server call listener ( response observer @$ call ) ;,client sends
note to translators : the stylesheet contained an <PLACE_HOLDER> that was not recognized as part of the xsl syntax . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,stylesheet contained
note : x 10 as apex has not automatic <PLACE_HOLDER>,set property ( codeclimate_remediation_multiplier @$ __num__ ) ; set property ( codeclimate_block_highlighting @$ false ) ; add rule chain visit ( ast field . class ) ;,10 has
the maximum <PLACE_HOLDER> exceeds the default max header <PLACE_HOLDER> of 10 1024,return arrays . as list ( new object [ ] [ ] { { test type . frame_max_greater_than_header_max @$ __num__ * __num__ } @$ { test type . frame_max_less_than_header_max @$ __num__ * __num__ } @$ { test type . frame_max_less_than_action_max @$ __num__ } } ) ;,size exceeds
create threads and make them run workload <PLACE_HOLDER> .,try { workload = new workload [ num threads ] ; for ( int i = __num__ ; i < num threads ; i ++ ) { workload [ i ] = new workload ( append test util . next long ( ) @$ fs @$ i @$ number of files @$ replication @$ __num__ ) ; workload [ i ] . start ( ) ; } mod thread = new modify ( conf @$ cluster ) ; mod thread . start ( ) ; for ( int i = __num__ ; i < num threads ; i ++ ) { try { system . out . println ( __str__ + i + __str__ ) ; workload [ i ] . join ( ) ; if ( i,them run
just read it @$ the checked input stream will update the <PLACE_HOLDER> on it 's own,try ( checked input stream stream = new checked input stream ( new buffered input stream ( files . new input stream ( source file . to path ( ) ) ) @$ new adler32 ( ) ) ) { io utils . skip fully ( stream @$ source file . length ( ) ) ; return stream . get checksum ( ) . get value ( ) ; } catch ( final io exception ignored ) { },stream update
if still null then the implementation does n't offer a presence operation <PLACE_HOLDER> which is unacceptable for gibberish .,if ( op set pers presence2 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ; contact group root group1 = op set pers presence1 . get server stored contact list root ( ) ;,implementation offer
3 function executions took <PLACE_HOLDER>,function service stats function service stats = ids . get function stats manager ( ) . get function service stats ( ) ; wait no functions running ( function service stats ) ; no of execution calls_ aggregate += __num__ ; no of executions completed_ aggregate += __num__ ; assert equals ( no of execution calls_ aggregate @$ function service stats . get function execution calls ( ) ) ; assert equals ( no of executions completed_ aggregate @$ function service stats . get function executions completed ( ) ) ; function stats function stats = function stats manager . get function stats ( test function . test_function2 @$ ids ) ;,executions took
if media file <PLACE_HOLDER> failed @$ let 's stop here and prompt the user,if ( m is media error ) { return upload post task result . error ; } if ( m post . get category id list ( ) . size ( ) > __num__ ) { m has category = true ; },media file
: a blank line matches no <PLACE_HOLDER> @$ so it can serve as a separator for readability .,if ( line . length ( ) == __num__ ) continue ;,line matches
if this method did not throw remote <PLACE_HOLDER> as required @$ generate the error but continue @$ so that multiple such errors can be reported .,if ( ! has remote exception ) { env . error ( __num__ @$ __str__ @$ interface def . get name ( ) @$ member . to string ( ) ) ; errors = true ; continue next member ; },method throw
checks whether the dashboard configuration contains <PLACE_HOLDER> expected fields,get dashboard configuration ( base url ) ; hs . stop ( ) ;,configuration contains
empty <PLACE_HOLDER> empty binary <PLACE_HOLDER>,assert that ( bytes ) . contains sequence ( type_list @$ __num__ @$ __num__ @$ type_struct @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) . contains sequence ( type_list @$ __num__ @$ __num__ @$ type_struct @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ;,annotations empty
bean may have acquired new weak <PLACE_HOLDER>,target = result . get node ( ) ; assert . assert equals ( count ++ @$ result . get value ( ) . int value ( ) ) ;,bean acquired
implicit init should honor <PLACE_HOLDER> already being set,user group information . create user for testing ( __str__ @$ new string [ __num__ ] ) ; assert equals ( rules [ __num__ ] @$ kerberos name . get rules ( ) ) ;,init honor
player reads all <PLACE_HOLDER> in arbitrary directory structure and creates a map task for each file . we use ' ; ' as separator because wal file names contains ' @$ ',string dirs = string utils . join ( dir paths @$ __str__ ) ; string jobname = __str__ + backup id ; path bulk output path = get bulk output dir ( ) ; conf . set ( wal player . bulk_output_conf_key @$ bulk output path . to string ( ) ) ; conf . set ( wal player . input_files_separator_key @$ __str__ ) ; conf . set boolean ( wal player . multi_tables_support @$ true ) ; conf . set ( job_name_conf_key @$ jobname ) ; string [ ] player args = { dirs @$ string utils . join ( table list @$ __str__ ) } ; try { player . set conf ( conf ) ; int result = player . run ( player args ),player reads
param type will stay <PLACE_HOLDER> if there is no parameter,if ( param type == null ) { param type = setter . get parameter type ( __num__ ) ; },type stay
lcn type need connection <PLACE_HOLDER>,dtx local context . make proxy ( ) ;,type need
copy all rocks db sst <PLACE_HOLDER> to local disk,string sst file list = rocks db path + __str__ + sst_file_list ; file file = new file ( sst file list ) ; list < string > sst files = file utils . read lines ( file ) ; log . debug ( __str__ @$ sst files ) ; for ( string sst file : sst files ) { dfs . copy to local ( remote db backup dir + __str__ + sst file @$ rocks db path ) ; } file utils . delete quietly ( file ) ;,rocks db
for compatibility @$ first do the lookup and then verify the provider . this makes the difference between a nsae and a security exception if the provider does not support the <PLACE_HOLDER> .,if ( provider checked == false ) { exception ve = jce security . get verification result ( provider ) ; if ( ve != null ) { string msg = __str__ + provider . get name ( ) ; throw new security exception ( msg @$ ve ) ; } provider checked = true ; },provider support
start the java spark <PLACE_HOLDER>,try ( print writer report writer = checker utils . init report file ( ) ) { spark conf conf = new spark conf ( ) . set app name ( spark integration checker . class . get name ( ) ) ; java spark context sc = new java spark context ( conf ) ; checker . print config info ( conf @$ report writer ) ; status result status = checker . run ( sc @$ report writer @$ alluxio conf ) ; checker . print result info ( result status @$ report writer ) ; report writer . flush ( ) ; system . exit ( result status . equals ( status . success ) ? __num__ : __num__ ) ; },java spark
by design @$ the buffer must always have enough <PLACE_HOLDER> for one packet,if ( build config . debug && len > buffer . remaining ( ) ) { log . e ( tag @$ len + __str__ + buffer . remaining ( ) ) ; log . e ( tag @$ buffer . to string ( ) ) ; throw new assertion error ( __str__ ) ; } buffer . put ( b @$ off @$ len ) ; buffer . flip ( ) ; sink ( ) ; buffer . compact ( ) ;,buffer have
the target did n't specify the <PLACE_HOLDER> for the reply .,handle drag reply ( action @$ x root @$ y root ) ;,target specify
the cache should not contain any <PLACE_HOLDER> now .,assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ;,cache contain
ee subsystem does n't have <PLACE_HOLDER> attributes @$ so make sure that the xml does n't have <PLACE_HOLDER>,require no attributes ( reader ) ; final model node ee sub system = util . create add operation ( path address . path address ( ee extension . path_subsystem ) ) ;,subsystem have
recurring plans will always have an active <PLACE_HOLDER>,if ( plan . get cycle rule ( ) . is recurring ( ) ) { return plan ; } else { final range < zoned date time > cycle = plan . cycle iterator ( ) . next ( ) ; if ( cycle . contains ( zoned date time . now ( m clock ) ) ) { return plan ; } },plans have
and that local files always takes <PLACE_HOLDER> over remote files .,action lookup key action lookup key = new action lookup key ( ) { @ override public sky function name function name ( ) { return sky function name . for_testing ; } } ; sky key action key = action lookup data . create ( action lookup key @$ __num__ ) ; special artifact tree artifact = create tree artifact ( __str__ ) ; tree artifact . get path ( ) . create directory and parents ( ) ; map < path fragment @$ remote file artifact value > tree artifact metadata = new hash map < > ( ) ; tree artifact metadata . put ( path fragment . create ( __str__ ) @$ create remote file artifact value ( __str__ ) ) ; tree artifact,files takes
need to deal with the case where the needed permission has more than one <PLACE_HOLDER> and the collection has individual permissions that sum up to the needed .,for ( int i = __num__ ; i < len ; i ++ ) { service permission x = ( service permission ) perms . get ( i ) ; if ( ( ( needed & x . get mask ( ) ) != __num__ ) && x . implies ignore mask ( np ) ) { effective |= x . get mask ( ) ; if ( ( effective & desired ) == desired ) return true ; needed = ( desired ^ effective ) ; } },permission has
normal case : one size has both width and height <PLACE_HOLDER>,dimension size1 = new dimension ( __num__ @$ __num__ ) ; dimension size2 = new dimension ( __num__ @$ __num__ ) ; assert . assert equals ( __num__ @$ icon entry . get distance ( size1 @$ size2 ) @$ __num__ ) ;,size has
receiver decrypts the <PLACE_HOLDER> using the sender 's public key,byte [ ] decrypt bytes = receiver . decrypt data ( encrypted challenge ) ;,receiver decrypts
we only report the <PLACE_HOLDER> for the original cause of fail and cleanup . otherwise this followup <PLACE_HOLDER> could race the original <PLACE_HOLDER> in failing the task .,try { owner . get environment ( ) . decline checkpoint ( checkpoint meta data . get checkpoint id ( ) @$ checkpoint exception ) ; } catch ( exception unhandled ) { asynchronous exception async exception = new asynchronous exception ( unhandled ) ; owner . handle async exception ( __str__ @$ async exception ) ; } current state = checkpointing operation . async checkpoint state . discarded ; current state = async checkpoint state . get ( ) ;,exception race
ooohml provides the <PLACE_HOLDER> for disconnecting and closing cache on out of off heap memory exception,out of off heap memory listener ooohml = new disconnecting out of off heap memory listener ( ( internal distributed system ) system ) ; return basic create off heap storage ( sf @$ off heap memory size @$ ooohml ) ;,ooohml provides
let 's do sanity <PLACE_HOLDER> ; easier to spot buggy handlers,if ( instantiator == null ) { ctxt . report bad type definition ( bean desc @$ __str__ @$ insts . get class ( ) . get name ( ) ) ; },"s" do
make sure kill the son before kill the <PLACE_HOLDER> .,for ( int i = pid list . size ( ) - __num__ ; i >= __num__ ; i -- ) { string ppid = pid list . get ( i ) ; if ( ! is pid running ( ppid ) ) { continue ; } if ( force ) { exe whole cmd ( __str__ + ppid ) ; } else { exe whole cmd ( __str__ + ppid ) ; } },son kill
ensure the request failed ... need read <PLACE_HOLDER> to the components in the snippet,assert equals ( __num__ @$ response . get status ( ) ) ; response = helper . get read write user ( ) . test post ( create template url @$ create template request ) ;,need read
this statement has a return <PLACE_HOLDER> on the stack .,if ( save != null ) { class definition def = ctx . field . get class definition ( ) ; if ( ! have non local finally ) { local member lf = ctx . get local field ( id finally return value ) ; num = new integer ( lf . number ) ; asm . add ( where @$ opc_istore + save . get type code offset ( ) @$ num ) ; } else { switch ( ctx . field . get type ( ) . get return type ( ) . get type code ( ) ) { case tc_void : break ; case tc_double : case tc_long : asm . add ( where @$ opc_pop2 ) ; break ; default : asm,statement has
kerberos if delegation token is passed from the client side @$ do not set the <PLACE_HOLDER>,if ( matcher . group ( __num__ ) . equals ignore case ( __str__ ) && ! ( conn params . get session vars ( ) . contains key ( jdbc connection params . auth_type ) && conn params . get session vars ( ) . get ( jdbc connection params . auth_type ) . equals ignore case ( jdbc connection params . auth_token ) ) && ! ( conn params . get session vars ( ) . contains key ( jdbc connection params . auth_principal ) ) ) { conn params . get session vars ( ) . put ( jdbc connection params . auth_principal @$ matcher . group ( __num__ ) ) ; },kerberos set
android just does the right <PLACE_HOLDER> .,charset cs = charset . for name ( __str__ ) ; charset encoder e = cs . new encoder ( ) ; e . on malformed input ( cea ) ; e . on unmappable character ( cea ) ; byte buffer bb = byte buffer . allocate ( __num__ ) ; coder result cr = e . encode ( char buffer . wrap ( new char [ ] { __str__ } ) @$ bb @$ false ) ; assert equals ( coder result . underflow @$ cr ) ; assert equals ( __num__ @$ bb . position ( ) ) ; cr = e . encode ( char buffer . wrap ( new char [ ] { __str__ } ) @$ bb @$ false ) ; assert,android does
note end of write . this does not change the <PLACE_HOLDER> of the remote fs .,write operation helper . write successful ( bytes ) ;,end change
removing this line will prevent the <PLACE_HOLDER>,max log summand = math . max ( max log summand @$ log summand ) ;,line prevent
create e service which should process the submitted <PLACE_HOLDER> .,final runnable srv runnable = new service executor ( msg queue ) ;,which process
validate our name matches an existing <PLACE_HOLDER>,file topic = get help topic ( ) ; file image file = get image file ( topic @$ name ) ; finished ( topic @$ image file . get name ( ) ) ;,name matches
if we cant parse the listen address we have no information on how to proceed with the migration the config will handle the <PLACE_HOLDER> later,default values . put ( advertised address @$ new advertised . to string ( ) ) ;,config handle
new root should have only 1 <PLACE_HOLDER>,root . go to ( read cursor ) ; assert equals ( __num__ @$ key count ( ) ) ;,root have
since the api request specified a unique channel section <PLACE_HOLDER> @$ the api response should return exactly one channel section . if the response does not contain a channel section @$ then the specified channel section <PLACE_HOLDER> was not found .,list < channel section > channel section list = channel section list response . get items ( ) ; if ( channel section list . is empty ( ) ) { system . out . println ( __str__ + channel section id ) ; return ; } channel section channel section = channel section list . get ( __num__ ) ;,request specified
if the user had a <PLACE_HOLDER> highlighted and we 're updating the list @$ we want to keep the <PLACE_HOLDER> highlighted if it 's in the updated list,string prev selected element = __str__ ; if ( main . elements to add pane . get selected value ( ) != null ) prev selected element = main . elements to add pane . get selected value ( ) ; if ( main . elements to remove table . get selected row ( ) != - __num__ ) prev selected element = ( string ) main . elements to remove table . get model ( ) . get value at ( main . elements to remove table . get selected row ( ) @$ __num__ ) ; if ( main . elements to remove table . get row count ( ) > __num__ ) main . elements to remove table . remove all elements ( ) ;,user had
required features expect activity launch <PLACE_HOLDER> in,verify ( m mock account manager response ) . on error ( eq ( account manager . error_code_management_disabled_for_account_type ) @$ any string ( ) ) ; verify ( m mock context ) . start activity as user ( any ( intent . class ) @$ eq ( user handle . system ) ) ; verify ( m mock device policy manager internal ) . create show admin support intent ( any int ( ) @$ any boolean ( ) ) ;,features expect
if an user used <PLACE_HOLDER> for projects he perhaps just used the key value for project without a name but the code expects a name for the project . therefore we fill the name according to the project key which is the same .,for ( entry < string @$ project > entry : cfg . get projects ( ) . entry set ( ) ) { if ( entry . get value ( ) . get name ( ) == null ) { entry . get value ( ) . set name ( entry . get key ( ) ) ; } },user used
the entry was not in the cache @$ make a new one . get the data <PLACE_HOLDER> .,snmp table handler handler = get handler ( m ) ;,one get
recursively search given subpackages . if any packages are found @$ add <PLACE_HOLDER> to the list .,if ( ! doc classes ) { map < string @$ list < java file object > > package files = search sub packages ( sub packages @$ names @$ excluded packages ) ; for ( list < string > packs = names . to list ( ) ; packs . non empty ( ) ; packs = packs . tail ) { string package name = packs . head ; parse package classes ( package name @$ package files . get ( package name ) @$ pack trees @$ excluded packages ) ; } if ( messager . nerrors ( ) != __num__ ) return null ; docenv . notice ( __str__ ) ; javadoc enter . main ( class trees . to list ( ) .,search add
currently get imported keys always returns an empty <PLACE_HOLDER> for hive,result set res = dbmd . get imported keys ( null @$ null @$ null ) ; result set meta data md = res . get meta data ( ) ; assert equals ( md . get column count ( ) @$ __num__ ) ; assert false ( res . next ( ) ) ;,keys returns
rs and datanode may have different <PLACE_HOLDER> in local machine test,if ( ! top hosts . contains ( server . get server name ( ) . get hostname ( ) ) ) { continue ; } for ( int j = __num__ ; j < server num ; j ++ ) { server name server name = cluster . get region server ( j ) . get server name ( ) ; assert true ( servers . contains ( server name ) ) ; },rs have
we put 2 photos in album 2 ; delete <PLACE_HOLDER>,assert . assert equals ( _entry res . purge ( long . value of ( __num__ ) @$ null ) @$ __num__ ) ;,photos delete
try to move focus to the next visible stack with a running activity if this stack is not covering the entire <PLACE_HOLDER> or is on a secondary display with no home stack .,if ( next focused stack != null ) { return m root activity container . resume focused stacks top activities ( next focused stack @$ prev @$ null ) ; },stack covering
file has stopped <PLACE_HOLDER> ...,if ( ! properties file exists ) { f last modified = - __num__ ; f xalan properties = null ; },file stopped
below we create a ldap server which will accept a client <PLACE_HOLDER> @$ authenticate it successfully ; but it will never reply to the following query <PLACE_HOLDER> . client of this ldap server is expected to get a read timeout .,final thread ldap server = new thread ( new runnable ( ) { @ override public void run ( ) { try { try ( socket client sock = server sock . accept ( ) ) { io utils . skip fully ( client sock . get input stream ( ) @$ __num__ ) ; client sock . get output stream ( ) . write ( authenticate_success_msg ) ; fin latch . await ( ) ; } } catch ( exception e ) { e . print stack trace ( ) ; } } } ) ; ldap server . start ( ) ; final ldap groups mapping mapping = new ldap groups mapping ( ) ; string ldap url = __str__ + server sock . get local,which accept
volt db supports <PLACE_HOLDER> with only one parameter,preconditions . check state ( aggr expr indexes . size ( ) < __num__ ) ; abstract expression aggr expr = null ; if ( ! aggr expr indexes . is empty ( ) ) { rel data type field field = fields . get ( aggr expr indexes . get ( __num__ ) ) ; aggr expr = rex converter . convert data type field ( field ) ; } else if ( expression type . aggregate_count == aggr type ) { aggr type = expression type . aggregate_count_star ; } assert ( aggr field idx < aggr row type . get field count ( ) ) ; apn . add aggregate ( aggr type @$ aggr call . is distinct ( ) @$ aggr field idx,db supports
the service may not exist on first app <PLACE_HOLDER> . choose sane defaults,if ( m aimsicd service == null ) { toggle attack detection menu item . set checked ( false ) ; toggle cell tracking menu item . set checked ( false ) ; } else { toggle attack detection menu item . set checked ( m aimsicd service . is monitoring cell ( ) ) ; toggle cell tracking menu item . set checked ( m aimsicd service . is tracking cell ( ) ) ; } return true ;,service exist
management server sends back an eds <PLACE_HOLDER> containing cluster load assignments for some cluster not requested .,list < any > cluster load assignments = immutable list . of ( any . pack ( build cluster load assignment ( __str__ @$ immutable list . of ( build locality lb endpoints ( __str__ @$ __str__ @$ __str__ @$ immutable list . of ( build lb endpoint ( __str__ @$ __num__ @$ health status . healthy @$ __num__ ) ) @$ __num__ @$ __num__ ) ) @$ immutable list . < policy . drop overload > of ( ) ) ) ) ; discovery response response = build discovery response ( __str__ @$ cluster load assignments @$ xds client impl . ads_type_url_eds @$ __str__ ) ; response observer . on next ( response ) ;,server sends
running a combiner over the result . the combiner 's accumulator would be the state we use below . however @$ combiners can not emit intermediate <PLACE_HOLDER> @$ thus we need to wait for the pending reduce fn api .,person existing person = person state . read ( ) ; if ( existing person != null ) { for ( auction new auction : c . element ( ) . get value ( ) . get all ( nexmark query util . auction_tag ) ) { new auction counter . inc ( ) ; new old output counter . inc ( ) ; c . output ( kv . of ( new auction @$ existing person ) ) ; } return ; },combiners emit
child left tuple is now null @$ so the next check will attempt <PLACE_HOLDER> for new bucket,child left tuple = next child ;,check attempt
request a <PLACE_HOLDER> for am 2 @$ will reserve a <PLACE_HOLDER> on nm 1,am2 . allocate ( __str__ @$ __num__ @$ __num__ @$ new array list < > ( ) ) ; cs . handle ( new node update scheduler event ( rm . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ) ) ; response = r . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . accept ( media type . application_json ) . get ( client response . class ) ; assert equals ( media type . application_json_type + __str__ + jetty utils . utf_8 @$ response . get type ( ) . to string ( ) ) ; json = response . get entity,container reserve
triggering the task should start the child case <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( plan item instance . get id ( ) ) ; assert equals ( __num__ @$ cmmn runtime service . create case instance query ( ) . count ( ) ) ; assert equals ( __str__ @$ cmmn rule . get cmmn history service ( ) . create historic variable instance query ( ) . case instance id ( case instance . get id ( ) ) . variable name ( __str__ ) . single result ( ) . get value ( ) ) ; remove all deployments ( ) ;,task start
else only the latest could have changed param info & we have those <PLACE_HOLDER> .,if ( latest changed param sig ) { if ( my changed param sig || my changed param info || my changed return ) { save function detail conflict ( functions @$ func_signature ) ; } } else if ( my changed param sig ) { if ( latest changed param info || latest changed return ) { save function detail conflict ( functions @$ func_signature ) ; } else { get merge my ( ) . replace function parameters ( entry @$ monitor ) ; } },only have
ok @$ pop jar open & make md <PLACE_HOLDER>,input stream is = null ; try { is = new file input stream ( jarpath ) ; message digest md5 = message digest . get instance ( __str__ ) ; byte [ ] buf = new byte [ __num__ ] ; int pos ; while ( ( pos = is . read ( buf ) ) > __num__ ) md5 . update ( buf @$ __num__ @$ pos ) ; return md5 . digest ( ) ; } catch ( io exception | no such algorithm exception e ) { log . err ( e ) ; } finally { try { if ( is != null ) is . close ( ) ; } catch ( io exception ignore ) { } },& make
jvms : if neither value 1 ' nor value 2 ' is na n @$ the <PLACE_HOLDER> of the result equals the <PLACE_HOLDER> of the dividend .,if ( result == __num__ && x < __num__ ) { return - result ; } return result ;,sign equals
you may not call the build <PLACE_HOLDER> in a subbuilder .,throw new doclet abort exception ( __str__ ) ;,the build
if still null then the implementation does n't offer a presence operation <PLACE_HOLDER> which is unacceptable for gibberish .,if ( op set pers presence1 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ;,implementation offer
searches started from a folder list activity will provide an <PLACE_HOLDER> @$ but no folder,if ( app data . get string ( extra_search_folder ) != null ) { search . add allowed folder ( app data . get string ( extra_search_folder ) ) ; },searches provide
otherwise @$ throw file already exists <PLACE_HOLDER> @$ which means the file owner is dead,removable = true ;,file exists
the touch was not on the end button so the touch highlight should cover <PLACE_HOLDER> except the end button .,return m overlay panel . get content view width px ( ) - m end button width - get divider line width ( ) ;,highlight cover
all <PLACE_HOLDER> input columns are repeating . generate <PLACE_HOLDER> once . lookup once . since the <PLACE_HOLDER> is repeated @$ we must use entry 0 regardless of selected in use .,if ( all key input columns repeating ) { join util . join result join result ; if ( ! join col vector . no nulls && join col vector . is null [ __num__ ] ) { join result = join util . join result . nomatch ; } else { byte [ ] key bytes = vector [ __num__ ] ; int key start = start [ __num__ ] ; int key length = length [ __num__ ] ; join result = hash multi set . contains ( key bytes @$ key start @$ key length @$ hash multi set results [ __num__ ] ) ; } if ( log . is debug enabled ( ) ) { log . debug ( class_name + __str__ +,input generate
the child case instance has the plan item instance <PLACE_HOLDER> as callback <PLACE_HOLDER> stored . when the child case instance is finished @$ the plan item of the parent case needs to be triggered .,if ( case instance state . terminated . equals ( callback data . get new state ( ) ) || case instance state . completed . equals ( callback data . get new state ( ) ) ) { command context command context = command context util . get command context ( ) ; plan item instance entity plan item instance entity = command context util . get plan item instance entity manager ( command context ) . find by id ( callback data . get callback id ( ) ) ; if ( plan item instance entity != null ) { command context util . get agenda ( command context ) . plan trigger plan item instance operation ( plan item instance entity ) ; } },instance has
when we support these node types @$ the parser should initialize a base uri <PLACE_HOLDER> on these nodes .,return null ; case entity_reference_node :,parser initialize
second consider the first <PLACE_HOLDER> packet . get the value of the piggyback associated with the erased location,if ( is direct ) { int idx to write = __num__ ; do decode by piggy back ( inputs [ __num__ ] @$ tmp outputs [ __num__ ] [ idx to write ] @$ piggy back @$ erased location to fix ) ; } else { byte buffer buffer ; byte [ ] [ ] [ ] new inputs = new byte [ get sub packet size ( ) ] [ inputs [ __num__ ] . length ] [ ] ; int [ ] [ ] input offsets = new int [ get sub packet size ( ) ] [ inputs [ __num__ ] . length ] ; byte [ ] [ ] [ ] new outputs = new byte [ get sub packet size ( ),second consider
noinspection object <PLACE_HOLDER> in loop,bucket = new osb tree bucketv1 < > ( cache entry ) ; if ( item index == osb tree bucketv1 . max_page_size_bytes + __num__ ) { item index = bucket . size ( ) - __num__ ; },noinspection object
make sure we do n't accidently overwrite this transformation so we 'll remove the file<PLACE_HOLDER> and object id modify the <PLACE_HOLDER> so the users sees it 's a result,if ( step meta . get step meta interface ( ) instanceof meta inject meta ) { trans meta . set filename ( null ) ; trans meta . set object id ( null ) ; string append name = __str__ + base messages . get string ( pkg @$ __str__ ) + __str__ ; if ( ! trans meta . get name ( ) . ends with ( append name ) ) { trans meta . set name ( trans meta . get name ( ) + append name ) ; } },id modify
as last resort ask the <PLACE_HOLDER> @$ cache it and return it,internal vertex retrieve vertex = retriever . get ( vertex id ) ; cache . put ( vertex id @$ retrieve vertex ) ; return retrieve vertex ;,resort ask
register before completion callback the before causes this thread to wait until the reaper thread aborts our <PLACE_HOLDER>,final session implementor session = entity manager . unwrap ( session implementor . class ) ; session . get action queue ( ) . register process ( new before callback completion handler ( ) ) ; testing jta platform impl . transaction manager ( ) . commit ( ) ;,thread aborts
if this gets called @$ the transformation has not taken <PLACE_HOLDER>,throw new assertion error ( __str__ ) ;,transformation taken
if we 're not running on the thread with the main looper @$ it 's possible for the state of accessibility to change between checking is enabled and calling this method . so just log the <PLACE_HOLDER> rather than throwing the exception .,log . e ( log_tag @$ __str__ ) ; return ;,state log
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
make sure platform can handle <PLACE_HOLDER>,if ( replaced value . contains ( __str__ ) ) { try { file tmp = file . create temp file ( __str__ @$ __str__ ) ; tmp . delete ( ) ; } catch ( io exception e ) { throw new invalid builds dir ( new builds dir value + __str__ ) ; } },platform handle
window has been removed or hidden ; no draw will now happen @$ so stop <PLACE_HOLDER> .,if ( win . m removed || ! win . m has surface || ! win . is visible by policy ( ) ) { if ( debug_screen_on ) slog . w ( tag_wm @$ __str__ + win ) ; m waiting for drawn . remove ( win ) ; } else if ( win . has drawn lw ( ) ) { if ( debug_screen_on ) slog . d ( tag_wm @$ __str__ + win ) ; m waiting for drawn . remove ( win ) ; },draw happen
convert multimap to a map @$ because every key should have only one <PLACE_HOLDER> .,immutable map . builder < string @$ path > builder = immutable map . builder ( ) ; for ( map . entry < string @$ path > entry : multimap . entries ( ) ) { builder . put ( entry ) ; } return builder . build ( ) ;,key have
disable the native input method so that the other input method could get the input <PLACE_HOLDER> .,disable input method ( ) ; if ( need resetxic ) { resetxic ( ) ; need resetxic client . clear ( ) ; need resetxic = false ; },method get
a locally generated change should always have a later <PLACE_HOLDER> than one received from a wan gateway @$ so fake a <PLACE_HOLDER> if necessary,if ( time <= stamp . get version time stamp ( ) && dsid != tag . get distributed system id ( ) ) { time = stamp . get version time stamp ( ) + __num__ ; } tag . set version time stamp ( time ) ; tag . set distributed system id ( dsid ) ;,change have
audit sql <PLACE_HOLDER> for example wo n't extend entity sql <PLACE_HOLDER>,if ( ! ( type instanceof java . lang . reflect . parameterized type ) ) { return null ; },dao extend
when reader contains multiple <PLACE_HOLDER> @$ set it to indicate we are in batch mode .,boolean is batch = false ;,reader contains
undocumented feature alert ! microsoft compilers will sometimes put field sig <PLACE_HOLDER> in this table @$ too @$ despite that not fitting the iso standard they wrote themselves .,if ( cli sig field . is field sig ( sig blob ) ) { sig = new cli sig field ( sig blob ) ; } else { sig = new cli sig stand alone method ( sig blob ) ; },compilers put
must ensure that all names do not <PLACE_HOLDER> and that variable types are resolved to this program so that they have the proper sizes,list < variable > cloned params = new array list < > ( ) ; for ( int i = __num__ ; i < new params . size ( ) ; i ++ ) { variable p = new params . get ( i ) ; if ( ! use custom storage && ( p instanceof auto parameter impl ) ) { continue ; } if ( p . is unique variable ( ) ) { throw new illegal argument exception ( __str__ ) ; } check for parameter name conflict ( p @$ new params @$ non param names ) ; cloned params . add ( get resolved variable ( p @$ false @$ ! use custom storage ) ) ; } new params = cloned params,names do
api version 8 has package info @$ 10 has loaded <PLACE_HOLDER> . 9 @$ i do n't know .,class < ? > loaded apk class ; try { loaded apk class = class . for name ( __str__ ) ; } catch ( class not found exception e ) { loaded apk class = class . for name ( __str__ ) ; } res dir = find field ( loaded apk class @$ __str__ ) ; packages filed = find field ( activity thread @$ __str__ ) ; if ( build . version . sdk_int < __num__ ) { resource packages filed = find field ( activity thread @$ __str__ ) ; },10 loaded
json payload did not match expected <PLACE_HOLDER>,try ( response response = client . new call ( json request ) . execute ( ) ) { assert equals ( http response status . bad_request . code ( ) @$ response . code ( ) ) ; },payload match
activities containing this fragment must implement its <PLACE_HOLDER> .,if ( ! ( activity instanceof callbacks ) ) { throw new illegal state exception ( __str__ ) ; } m callbacks = ( callbacks ) activity ;,activities implement
mp 4 containers include the proj <PLACE_HOLDER> but webm containers do not . both containers use mshp .,array list < mesh > meshes = null ; try { meshes = is proj ( input ) ? parse proj ( input ) : parse mshp ( input ) ; } catch ( array index out of bounds exception ignored ) { },containers include
verify that the subprocess had no system.out or system.err <PLACE_HOLDER> .,string out string = out . to string ( ) ; string err string = err . to string ( ) ; system . err . println ( __str__ ) ; system . err . print ( out ) ; system . err . println ( __str__ ) ; system . err . print ( err ) ; system . err . println ( __str__ ) ; if ( out string . length ( ) > __num__ || err string . length ( ) > __num__ ) { throw new error ( __str__ ) ; } system . err . println ( __str__ ) ;,subprocess had
has to be a concurrent hash map because tests might access this <PLACE_HOLDER> concurrently via contains job,job leader services = new concurrent hash map < > ( __num__ ) ; state = job leader service . state . created ; owner address = null ; rpc service = null ; high availability services = null ; job leader listener = null ;,tests access
the file had bad characters in the name that the filesystem does n't like . fall <PLACE_HOLDER> with null value @$ caller will get original file back as result and they can deal with io exception errors when they try to use it .,string case sensitive name = case sensitive file . get name ( ) ; return ( canonical name != null ) && canonical name . equals ignore case ( case sensitive name ) && ! canonical name . equals ( case sensitive name ) ? null : case sensitive file ;,file fall
the method should still be present @$ just not public @$ let 's try declared <PLACE_HOLDER>,actual method = actual test instance . get class ( ) . get declared method ( get method ( ) . get name ( ) @$ convert totccl ( get method ( ) . get parameter types ( ) ) ) ; actual method . set accessible ( true ) ;,try declared
test parallelism in detached mode @$ should use parallelism <PLACE_HOLDER>,string [ ] parameters = { __str__ @$ __str__ @$ get test jar path ( ) } ; verify cli frontend ( get cli ( configuration ) @$ parameters @$ __num__ @$ true ) ;,parallelism use
need to close before setting the flag since the close function itself may trigger rebalance <PLACE_HOLDER> that needs the consumer to be open still,if ( ! closed ) { close ( timeout . to millis ( ) @$ false ) ; },itself trigger
for backward compatibility with old <PLACE_HOLDER> service impl which do n't extend region <PLACE_HOLDER> .,if ( coprocessor service . class . is assignable from ( impl class ) ) { coprocessor service cs ; cs = impl class . as subclass ( coprocessor service . class ) . get declared constructor ( ) . new instance ( ) ; return new coprocessor service backward compatiblity . region coprocessor service ( cs ) ; } else { log . error ( __str__ @$ impl class . get name ( ) @$ coprocessor host . region_coprocessor_conf_key ) ; return null ; },which extend
pdx @$ alias @$ nested <PLACE_HOLDER>,return new object [ ] { new object [ ] { __str__ @$ true } @$ new object [ ] { __str__ @$ false } @$ new object [ ] { __str__ @$ true } @$ new object [ ] { __str__ @$ false } } ;,pdx nested
key which indicates the <PLACE_HOLDER> of a record,string record key = null ;,which indicates
client sent an ack lds <PLACE_HOLDER> .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_lds @$ __str__ ) ) ) ;,client sent
sometime @$ the dataset contains duplicate <PLACE_HOLDER> . if the distances are same @$ we sort by the sample index .,return d == __num__ ? index - o . index : d ;,dataset contains
transient status bar is not allowed if status bar is on lockscreen or status bar is expecting the navigation <PLACE_HOLDER> from the user .,if ( ( m force status bar from keyguard || status bar forces showing navigation ) && m status bar controller . is transient showing ( ) ) { m status bar controller . update visibility lw ( false @$ m last system ui flags @$ m last system ui flags ) ; },bar expecting
to avoid changing the output path of binaries built without a flavor @$ we 'll default to no flavor @$ which implicitly builds the default <PLACE_HOLDER> .,return immutable sorted set . of ( ) ;,which builds
apply the changes the apply will drop the undefined at 0 <PLACE_HOLDER> @$ and param size will become 0 x 5 .,invoke ( apply action ) ; assert equals ( __str__ @$ model . get status ( ) ) ; stack = function . get stack frame ( ) ; assert equals ( __num__ @$ stack . get frame size ( ) ) ; assert equals ( __num__ @$ stack . get parameter size ( ) ) ; assert equals ( __num__ @$ stack model . get frame size ( ) ) ; assert equals ( __num__ @$ stack model . get parameter size ( ) ) ; close editor ( ) ;,apply drop
should not work : <PLACE_HOLDER> on custom type,group ds . max by ( __num__ ) ;,not work
trigger source info refresh for lazy source and check that the timeline now contains all <PLACE_HOLDER> for all windows .,test runner . run on playback thread ( ( ) -> lazy sources [ __num__ ] . set new source info ( create fake timeline ( __num__ ) @$ null ) ) ; timeline = test runner . assert timeline change blocking ( ) ; timeline asserts . assert period counts ( timeline @$ __num__ @$ __num__ ) ; timeline asserts . assert window tags ( timeline @$ __num__ @$ __num__ ) ; timeline asserts . assert window is dynamic ( timeline @$ false @$ false ) ; test runner . assert prepare and release all periods ( ) ; test runner . assert completed manifest loads ( __num__ @$ __num__ ) ; assert completed all media period loads ( timeline ) ;,timeline contains
dst address contains invalid <PLACE_HOLDER>,request = __str__ + __str__ ; proxy protocol request response check ( request @$ __str__ ) ;,address contains
upon error do n't process the <PLACE_HOLDER> .,return false ;,error process
<PLACE_HOLDER>re must be 2 keys 42 @$ 43 registered for <PLACE_HOLDER> watermark callback all <PLACE_HOLDER> seen elements must be in <PLACE_HOLDER> priority queues but no nfa yet .,assert equals ( __num__ @$ harness . num event time timers ( ) ) ; assert equals ( __num__ @$ operator . getpq size ( __num__ ) ) ; assert equals ( __num__ @$ operator . getpq size ( __num__ ) ) ; assert true ( ! operator . has non empty shared buffer ( __num__ ) ) ; assert true ( ! operator . has non empty shared buffer ( __num__ ) ) ; harness . process watermark ( new watermark ( __num__ ) ) ; verify watermark ( harness . get output ( ) . poll ( ) @$ long . min_value ) ; verify watermark ( harness . get output ( ) . poll ( ) @$ __num__ ) ;,keys callback
the area of the html content is absolute inside of the entire <PLACE_HOLDER> . however @$ the user is viewing the <PLACE_HOLDER> inside of a scroll pane . so @$ we want the offset of the element within the viewer @$ not the absolute position .,area . y -= view position . y ;,user viewing
basic internal frame ui creates an action with the same name @$ we override it as motif internal frame title pane has a title pane <PLACE_HOLDER> that shadows the title pane <PLACE_HOLDER> in basic internal frame ui @$ making supers action throw an npe for us .,if ( map != null ) { map . put ( __str__ @$ new abstract action ( ) { public void action performed ( action event e ) { title pane . show system menu ( ) ; } public boolean is enabled ( ) { return is key binding active ( ) ; } } ) ; },pane has
set the choice to 0 x 01 if the sender should switch the 0 and 1 <PLACE_HOLDER>,if ( choice bit ^ choices . get bit ( offset @$ false ) == true ) { switch bit [ __num__ ] = __num__ ; } network . send ( resources . get other id ( ) @$ switch bit ) ;,sender switch
if the bounds are currently frozen @$ it means that the layout size that the app sees and the bounds we clip this <PLACE_HOLDER> to might be different . in order to avoid holes @$ we simulate that we are still resizing so the app fills the hole with the resizing background .,return ( get display content ( ) . m divider controller locked . is resizing ( ) || m app token != null && ! m app token . m frozen bounds . is empty ( ) ) && ! task . in freeform windowing mode ( ) && ! is gone for layout lw ( ) ;,size clip
the number of unenqueued bytes that the decoder thread keeps <PLACE_HOLDER> of .,int unqueued bytes = stream . get unqueued buffer bytes ( ) ;,thread keeps
note to translators : this message is used to indicate the severity of another message . the substitution text contains two error <PLACE_HOLDER> . the spacing before the second substitution text indents it the same amount as the first in english .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text contains
1 means asc @$ could really use <PLACE_HOLDER> here in the thrift if,sort order = collections . singleton list ( __num__ ) ;,means use
this finds the first non empty cached group cache <PLACE_HOLDER> .,if ( m cache span group indices ) { int prev key = find first key less than ( m span group index cache @$ adapter position ) ; if ( prev key != - __num__ ) { group = m span group index cache . get ( prev key ) ; start = prev key + __num__ ; span = get cached span index ( prev key @$ span count ) + get span size ( prev key ) ; if ( span == span count ) { span = __num__ ; group ++ ; } } },non empty
is no inheritance on constructors @$ so reloaded superclasses can affect method <PLACE_HOLDER> in the same way .,class < ? > clazz = c . get declaring class ( ) ; reloadable type rtype = get reloadable type if has been reloaded ( clazz ) ; if ( rtype == null ) { c = as accessible constructor ( c @$ true ) ; return c . new instance ( params ) ; } else { boolean ctor changed = rtype . get live version ( ) . has constructor changed ( utils . to constructor descriptor ( c . get parameter types ( ) ) ) ; if ( ! ctor changed ) { c = as accessible constructor ( c @$ true ) ; return c . new instance ( params ) ; } as accessible constructor ( c @$ false ) ;,superclasses affect
null default support <PLACE_HOLDER> .,assert null ( dpm . get long support message ( admin1 ) ) ; assert null ( dpm . get short support message ( admin1 ) ) ; m context . binder . calling uid = dpm mock context . system_uid ; assert null ( dpm . get short support message for user ( admin1 @$ dpm mock context . caller_user_handle ) ) ; assert null ( dpm . get long support message for user ( admin1 @$ dpm mock context . caller_user_handle ) ) ; m mock context . binder . calling uid = dpm mock context . caller_uid ;,default support
rpc lit can not inline repeated <PLACE_HOLDER> in wrapper,if ( parent . get binding ( ) . is rpc lit ( ) || wrapper == null ) return null ;,lit inline
the application can customize its <PLACE_HOLDER> dispatching mechanism .,reactor = new nio reactor ( dispatcher ) ;,application customize
an interface will get the reloadable static state manager <PLACE_HOLDER>,assert equals ( __str__ @$ to string field ( itype . bytes loaded @$ __str__ ) ) ;,interface get
json does not deliver a <PLACE_HOLDER> of all columns for replica identity default,object [ ] values = new object [ columns without toasted . size ( ) < schema columns . size ( ) ? schema columns . size ( ) : columns without toasted . size ( ) ] ; final set < string > undelivered toastable columns = new hash set < > ( schema . get toastable columns for table id ( table . id ( ) ) ) ; for ( replication message . column column : columns ) { final string column name = strings . unquote identifier part ( column . get name ( ) ) ; undelivered toastable columns . remove ( column name ) ; int position = get position ( column name @$ table @$ values ) ; if ( position,json deliver
druid itself does n't explictly handle options <PLACE_HOLDER> @$ no resource handler will authorize such <PLACE_HOLDER> . so this filter catches all options <PLACE_HOLDER> and authorizes them .,if ( http method . options . equals ( http req . get method ( ) ) ) { if ( http req . get attribute ( auth config . druid_authentication_result ) == null ) { if ( allow unauthenticated http options ) { http req . set attribute ( auth config . druid_authentication_result @$ new authentication result ( auth config . allow_all_name @$ auth config . allow_all_name @$ null @$ null ) ) ; } else { ( ( http servlet response ) response ) . send error ( http servlet response . sc_unauthorized ) ; } } http req . set attribute ( auth config . druid_authorization_checked @$ true ) ; },filter catches
small retry number can speed up the failed <PLACE_HOLDER> .,util . get configuration ( ) . set int ( h constants . hbase_client_retries_number @$ __num__ ) ; util . start mini cluster ( ) ;,number speed
deletes realm so key 2 <PLACE_HOLDER> . this should work as a realm should n't be cached if initialization failed .,assert true ( realm . delete realm ( configa ) ) ; realm = realm . get instance ( configb ) ; realm . close ( ) ;,deletes realm
ensure that each consumer should have received at least one <PLACE_HOLDER> we can not guarantee that <PLACE_HOLDER>s will be equally divided @$ since prefetch is one,assert each consumer received at leastx messages ( __num__ ) ;,consumer received
step i : find out <PLACE_HOLDER> instructions belong to the new upper block and <PLACE_HOLDER> instructions belong to the new lower block .,final list < i navi instruction > upper instructions = new array list < i navi instruction > ( ) ; final list < i navi instruction > lower instructions = new array list < i navi instruction > ( ) ; list < i navi instruction > current instructions = upper instructions ; for ( final i navi instruction old instruction : old instructions ) { current instructions . add ( old instruction ) ; if ( old instruction == instruction ) { current instructions = lower instructions ; } },instructions belong
ar link commands can also generate huge command <PLACE_HOLDER> .,if ( link target type . linker or archiver ( ) == linker or archiver . archiver ) { list < string > param file args = new array list < > ( ) ; list < string > commandline args = new array list < > ( ) ; extract arguments for static link param file ( args @$ commandline args @$ param file args ) ; return pair . of ( commandline args @$ param file args ) ; } else { list < string > param file args = new array list < > ( ) ; list < string > commandline args = new array list < > ( ) ; extract arguments for dynamic link param file ( args @$ commandline args @$,commands generate
reconnect internet after testing network health triggered <PLACE_HOLDER>,get device ( ) . execute shell command ( __str__ ) ; get device ( ) . execute shell command ( __str__ ) ;,health triggered
this test is specifically checking loose property check <PLACE_HOLDER> .,disable strict missing property checks ( ) ; test types ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,test checking
java does not recognize <PLACE_HOLDER> or \v @$ apparently .,switch ( b ) { case __num__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __num__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; default : if,java recognize
if liveness hits <PLACE_HOLDER> @$ queue is considered disconnected,liveness = heartbeat_liveness ; heartbeat at = system . current time millis ( ) + heartbeat ;,liveness hits
config log 4 <PLACE_HOLDER> with customized files check whether hive conf initialize log 4 <PLACE_HOLDER> correctly,config log ( hive log4j property @$ hive exec log4j property ) ;,config log
test contents using contains element set <PLACE_HOLDER>,int size2 = __num__ ; int previous element = - __num__ ; for ( int element : set ) { assert true ( bs . get ( element ) ) ; size2 ++ ; assert true ( previous element < __num__ || ( ascending ? element - previous element > __num__ : element - previous element < __num__ ) ) ; previous element = element ; } assert equals ( size2 @$ size ) ;,contents contains
find the currently contacted jid to send typing <PLACE_HOLDER> to him or if we do not have a jid and we have already sent message to the bare jid we will also send typing <PLACE_HOLDER> there,jid tojid = op set basicim . get recentjid for address ( contact . get address as jid ( ) . as bare jid ( ) ) ;,jid send
target float value hard rouding <PLACE_HOLDER>,string [ ] [ ] rounding test cases = { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__,value rouding
reset the schedule and reload the latest card off the top of the stack if required . the card could have been rescheduled @$ the deck could have changed @$ or a change of note type could have <PLACE_HOLDER> to the card being deleted,if ( data != null && data . has extra ( __str__ ) ) { get col ( ) . get sched ( ) . reset ( ) ; deck task . launch deck task ( deck task . task_type_answer_card @$ m answer card handler @$ new deck task . task data ( null @$ __num__ ) ) ; } if ( request code == edit_current_card ) { if ( result code == result_ok ) { timber . i ( __str__ ) ; deck task . launch deck task ( deck task . task_type_update_fact @$ m update card handler @$ new deck task . task data ( m current card @$ true ) ) ; } else if ( result code == result_canceled && ! ( data !=,change have
increment job execution time . this counter gets <PLACE_HOLDER> once this job will be accounted for in metrics .,long exec time = worker . get execute time ( ) ; finished jobs time . add ( exec time ) ; total execution time metric . add ( exec time ) ; max finished jobs time . set if greater ( exec time ) ; if ( job always activate ) { if ( metrics update freq > - __num__ ) update job metrics ( ) ; if ( ! remove from active ( worker ) ) cancelled jobs . remove ( worker . get job id ( ) @$ worker ) ; held jobs . remove ( worker . get job id ( ) ) ; } else { if ( ! rw lock . try read lock ( ) ) { if ( log .,counter gets
specified <PLACE_HOLDER> does not match the existing <PLACE_HOLDER> for the pipeline . returns channel back to the pool and throws exception to the caller .,if ( ! scheme . equals ignore case ( _scheme ) ) { ( ( request with callback ) msg ) . handle ( ) . release ( ) ; throw new illegal state exception ( string . format ( __str__ @$ _scheme @$ scheme @$ ctx . channel ( ) . remote address ( ) ) ) ; },scheme match
the probe covers <PLACE_HOLDER>,probe = join probe factory . create join probe ( page ) ; for ( int join position = __num__ ; probe . advance next position ( ) ; join position ++ ) { lookup join page builder . append row ( probe @$ lookup source @$ join position ) ; } output = lookup join page builder . build ( probe ) ; assert equals ( output . get channel count ( ) @$ __num__ ) ; assert false ( output . get block ( __num__ ) instanceof dictionary block ) ; assert equals ( output . get position count ( ) @$ entries ) ; for ( int i = __num__ ; i < entries ; i ++ ) { assert equals ( output . get,probe covers
in general a filter can not be pushed below a windowing calculation . applying the filter before the aggregation function changes the <PLACE_HOLDER> of the windowing invocation . when the filter is on the partition by expression of the over clause it can be pushed down . for now we do n't support this .,if ( rex over . contains over ( project . get projects ( ) @$ null ) ) { return ; },function changes
only input 2 side has <PLACE_HOLDER>,if ( input col vector1 . no nulls && ! input col vector2 . no nulls ) { if ( ( input col vector1 . is repeating ) && ( input col vector2 . is repeating ) ) { outv . is repeating = true ; output vector [ __num__ ] = vector1 [ __num__ ] | vector2 [ __num__ ] ; output is null [ __num__ ] = ( vector1 [ __num__ ] == __num__ ) && input col vector2 . is null [ __num__ ] ; } else if ( input col vector1 . is repeating && ! input col vector2 . is repeating ) { if ( batch . selected in use ) { for ( int j = __num__ ; j != n ;,side has
test fs 2 contains fs <PLACE_HOLDER> .,fs1 . clear ( ) ; fs2 . clear ( ) ; fs result . clear ( ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs result = new field selection ( fs1 ) ; fs1 . intersect ( fs2 ) ; assert equals ( fs result @$ fs1 ) ;,fs contains
type procname client handle extension <PLACE_HOLDER>,int size = __num__ + __num__ + get proc name bytes ( ) . length + __num__ + __num__ + batch extension size + all partition extension size + partition destination size ;,client handle
return the fragment noinspection simplifiable conditional <PLACE_HOLDER>,return new sentence fragment ( fragment tree @$ fragment . has assumed truth ( ) ? fragment . get assumed truth ( ) : true @$ false ) . change score ( fragment . has score ( ) ? fragment . get score ( ) : __num__ ) ;,noinspection simplifiable
database will result in a log.wtf call that will crash this process on eng builds . to allow the <PLACE_HOLDER> to run through to completion skip this <PLACE_HOLDER> on eng builds .,if ( build . is_eng ) { return ; } account account = new account ( __str__ @$ __str__ ) ; long id = m accounts db . insert ce account ( account @$ __str__ ) ; assert equals ( __str__ @$ - __num__ @$ id ) ;,builds skip
tm ca n't know this application <PLACE_HOLDER> @$ so it needs cancel .,uba . cancel ( ) ;,tm know
if parent provides a <PLACE_HOLDER> @$ do n't measure unlimited .,m layout state . m infinite = resolve is infinite ( ) ; m layout state . m extra = get extra layout space ( state ) ; m layout state . m layout direction = layout direction ; int scrolling offset ; if ( layout direction == layout state . layout_end ) { m layout state . m extra += m orientation helper . get end padding ( ) ; final view child = get child closest to end ( ) ; m layout state . m item direction = m should reverse layout ? layout state . item_direction_head : layout state . item_direction_tail ; m layout state . m current position = get position ( child ) + m layout state . m item direction ;,parent provides
add <PLACE_HOLDER> has thrown an exception ! set future.run ca n't throw any exceptions so this must have been caused by add <PLACE_HOLDER> itself . the most likely explanation is a misconfigured mock . try to switch to failure .,failure failure ; try { failure = new failure ( t ) ; } catch ( throwable oom most likely ) { failure = failure . fallback_instance ; },itself add
2 nd tx should put <PLACE_HOLDER> back into existence,check result ( new index query ( def store @$ predicate condition . of ( text @$ text . contains @$ __str__ ) ) @$ def doc ) ;,tx put
if file has reached max <PLACE_HOLDER> defined size . close current file and open a new file .,if ( ! meta . is file name in field ( ) && ( get lines output ( ) > __num__ ) && ( data . split every > __num__ ) && ( ( get lines output ( ) + meta . get footer shift ( ) ) % data . split every ) == __num__ ) { if ( meta . is footer enabled ( ) ) { write header ( ) ; } close file ( filename ) ; data . splitnr ++ ; data . fos = null ; data . out = null ; data . writer = null ; filename = get output file name ( null ) ; is write header = is write header ( filename ) ; init file stream,file reached
if the tests are run on windows @$ the expected paths need to be adjusted . on platforms that use the unix convention @$ the following does not actually change the test <PLACE_HOLDER> .,for ( int i = __num__ ; i < data . length ; ++ i ) { if ( data [ i ] != null ) { file file = new file ( data [ i ] ) ; if ( data [ i ] . starts with ( __str__ ) ) { file = file . get absolute file ( ) ; } data [ i ] = file . get path ( ) ; } } for ( int i = __num__ ; i < data . length ; i += __num__ ) { build image configuration config = new build image configuration . builder ( ) . context dir ( data [ i ] ) . docker file ( data [ i + __num__ ],following change
the process will throw an error <PLACE_HOLDER> @$ which is caught and escalated by a user org.flowable.task.service.task,assert equals ( __str__ @$ __num__ @$ task service . create task query ( ) . task definition key ( __str__ ) . count ( ) ) ;,process throw
initial subscribe does n't reach the <PLACE_HOLDER>,assert that ( subscription count . get ( ) ) . is zero ( ) ; assert that ( termination . get ( ) ) . is null ( ) ; disposable sub2 = ref counted . subscribe ( ) ;,subscribe reach
dispatching to empty statement will not call back <PLACE_HOLDER>or @$ must call our <PLACE_HOLDER> empty statement explicitly,if ( else block instanceof empty statement ) { visit empty statement ( ( empty statement ) else block ) ; } else { else block . visit ( this ) ; },statement call
stop the timers because they 're implicitly globally referenced and thus do n't let them retain this <PLACE_HOLDER> .,typing timer . stop ( ) ; typing timer . remove action listener ( this ) ; stopped typing timer . stop ( ) ; stopped typing timer . remove action listener ( this ) ; if ( typing state != operation set typing notifications . state_stopped ) stop typing timer ( ) ; if ( outdated resource timer != null ) { outdated resource timer . cancel ( ) ; outdated resource timer . purge ( ) ; outdated resource timer = null ; } editor pane . remove key listener ( this ) ; menu listeners . clear ( ) ; if ( right button menu != null ) { right button menu . dispose ( ) ; right button menu = null ; } scroll,them retain
asynchronous release the <PLACE_HOLDER> @$ but still is single thread,if ( _buffer . get cursor ( ) > _consumer . get ( ) ) consume batch when available ( handler ) ;,asynchronous release
req.get remote host returns ip <PLACE_HOLDER> @$ try to resolve hostname to be consistent with raw protocol .,try { final inet address client address = inet address . get by name ( client host name ) ; client host name = client address . get host name ( ) ; } catch ( unknown host exception e ) { logger . info ( __str__ @$ client host name @$ e . get message ( ) ) ; },returns ip
set sign data and set keep mapping data must before read xml <PLACE_HOLDER> or it will read,read xml config ( config ) ; this . m7zip path = sevenzip path ; this . m zipalign path = zip align path ;,data read
hive does not support result set meta <PLACE_HOLDER> on prepared statement @$ and hive describe does not support queries @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,describe support
options expect activity <PLACE_HOLDER>,wait for latch ( latch ) ; verify ( m mock account manager response ) . on result ( m bundle captor . capture ( ) ) ; bundle result = m bundle captor . get value ( ) ;,options expect
on error @$ report failure to container and signal abort notify <PLACE_HOLDER> of failed localization,container id c id = context . get container id ( ) ; dispatcher . get event handler ( ) . handle ( new container resource failed event ( c id @$ null @$ exception . get message ( ) ) ) ;,abort notify
create threads and make them run <PLACE_HOLDER> concurrently .,thread thread id [ ] = new thread [ num_threads ] ; for ( int i = __num__ ; i < num_threads ; i ++ ) { transactions trans = new transactions ( namesystem @$ num_transactions @$ i * num_transactions ) ; thread id [ i ] = new thread ( trans @$ __str__ + i ) ; thread id [ i ] . start ( ) ; },them run
the wait is necessary to have the poll function complete and propagate the <PLACE_HOLDER> from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one three = database one text node . get comments ( ) ; final list < i comment > two three = database two text node . get comments ( ) ; assert equals ( one two size - __num__ @$ one three . size ( ) ) ; assert equals ( two two size - __num__ @$ two three . size ( ) ) ; assert equals ( one three @$ two three ) ;,function complete
note that concurrent linked queue actually supports doing this the iterator does not throw a concurrent modification <PLACE_HOLDER>,new references . remove all ( collected new references ) ; processed references . remove all ( collected processed references ) ;,iterator throw
impl in filter base might do unnecessary <PLACE_HOLDER> for off heap backed cells .,return false ;,impl do
for posterity : the moment this user unlocked the easter <PLACE_HOLDER>,try { settings . system . put long ( cr @$ __str__ @$ system . current time millis ( ) ) ; } catch ( runtime exception e ) { log . e ( __str__ @$ __str__ @$ e ) ; },user unlocked
zk peek lock interrupt <PLACE_HOLDER> expected,assert true ( e instanceof zk peek lock interrupt exception ) ;,peek lock
and associate <PLACE_HOLDER> with players via the special one v one methods . clear the team reference to players @$ which should orphan the <PLACE_HOLDER> . orphaning the team should delete the team .,session s = open session ( ) ; transaction tx = s . begin transaction ( ) ; soccer team team = new soccer team ( ) ; team . set name ( __str__ ) ; player player1 = new player ( ) ; player1 . set name ( __str__ ) ; team . set one vone player ( player1 ) ; player1 . set one vone team ( team ) ; s . persist ( team ) ; soccer team team2 = new soccer team ( ) ; team2 . set name ( __str__ ) ; player player2 = new player ( ) ; player2 . set name ( __str__ ) ; team2 . set one vone player ( player2 ) ; player2 . set one vone,which orphan
foo 's runtime bind release service should now have a <PLACE_HOLDER> to the new bind,assert true ( foo du binding references . contains ( service name ) ) ;,service have
interface jars artifacts have proper owner <PLACE_HOLDER>,return new java compilation artifacts . builder ( ) . add runtime jars ( jars ) . add full compile time jars ( jars ) . add interface jars ( interface jars ) . build ( ) ;,artifacts have
make the last field occupy the remaining <PLACE_HOLDER>,last field . set width ( last width ) ;,field occupy
check that they are referentially equal . since getting a group for a users that does n't exist creates a new string <PLACE_HOLDER> the only way that they should be referentially equal is if the cache worked and made sure we did n't go to hadoop 's script twice .,assert true ( u one . get group names ( ) == u two . get group names ( ) ) ; assert equals ( __num__ @$ ugi one . get group names ( ) . length ) ;,check creates
this asserts that all <PLACE_HOLDER> have wait wail the testing thread notifies all . we have to do this to guarantee that the thread pool has 10 live <PLACE_HOLDER> before we check the 'created ' meter .,try { all tasks are counted . await ( ) ; } catch ( interrupted exception e ) { interrupted . increment and get ( ) ; thread . current thread ( ) . interrupt ( ) ; },pool has
not null constraint should reference a single <PLACE_HOLDER>,m constraint muk = new m constraint ( constraint name @$ constraint type @$ __num__ @$ null @$ null @$ enable validate rely @$ parent table @$ null @$ parentcd @$ null @$ null @$ parent integer index @$ constraint value ) ;,constraint reference
static shared libs can not declare permission <PLACE_HOLDER>,if ( ! pkg . permission groups . is empty ( ) ) { throw new package manager exception ( __str__ ) ; },libs declare
read the block from io engine based on the bucket entry 's offset and length @$ notice : the block will use the ref <PLACE_HOLDER> of bucket entry @$ which means if two h file block mapping to the same bucket entry @$ then all of the three will share the same ref <PLACE_HOLDER> .,if ( bucket entry . equals ( backing map . get ( key ) ) ) { cacheable cached block = io engine . read ( bucket entry ) ; if ( io engine . uses shared memory ( ) ) { cached block . retain ( ) ; } if ( update cache metrics ) { cache stats . hit ( caching @$ key . is primary ( ) @$ key . get block type ( ) ) ; cache stats . io hit ( system . nano time ( ) - start ) ; } bucket entry . access ( access count . increment and get ( ) ) ; if ( this . io error start time > __num__ ) { io error start time,block use
load balancer will normally shutdown all <PLACE_HOLDER>,subchannel1 . shutdown ( ) ; subchannel2 . shutdown ( ) ;,balancer shutdown
verify that file system does not support unix <PLACE_HOLDER>,try { cfg . file system . read attributes ( path @$ __str__ ) ; throw e ; } catch ( unsupported operation exception unsupported ) { } catch ( io exception ioe ) { throw new runtime exception ( ioe ) ; },system support
if it is a dynamic <PLACE_HOLDER> mapping @$ we can safely assume leaf <PLACE_HOLDER> name does not have ' . ' in it validate if parent <PLACE_HOLDER> is specified @$ then parent <PLACE_HOLDER> exists and an instance of auto create enabled parent <PLACE_HOLDER>,queue mapping new mapping = validate and get auto created queue mapping ( queue manager @$ mapping @$ queue path ) ; if ( new mapping != null ) { new mappings . add ( new mapping ) ; } else { new mappings . add ( mapping ) ; },instance create
a header receives <PLACE_HOLDER> to item else a header receives <PLACE_HOLDER> from item,old position = from position < to position ? to position : from position ; new position = from position < to position ? from position : to position ;,header receives
since each partition may have <PLACE_HOLDER> collected for different set of columns @$ we request them separately .,if ( get col stats ) { for ( partition part : ret ) { string part name = warehouse . make part name ( table . get partition keys ( ) @$ part . get values ( ) ) ; list < column statistics > part col stats list = getms ( ) . get partition column statistics ( parsed cat name @$ parsed db name @$ tbl name @$ collections . singleton list ( part name ) @$ stats setup const . get columns having stats ( part . get parameters ( ) ) @$ engine ) ; if ( part col stats list != null && ! part col stats list . is empty ( ) ) { column statistics part col stats = part,partition have
start new instance of secondary and verify that a new roll edit log <PLACE_HOLDER> in spite of the fact that we had a partially failed checkpoint previously .,secondary = start secondary name node ( conf ) ; secondary . do checkpoint ( ) ;,roll edit
ensure the specification contains all <PLACE_HOLDER> .,return specification . services ( ) . stream ( ) . collect ( to immutable map ( service info :: name @$ function . identity ( ) ) ) ;,specification contains
no context class <PLACE_HOLDER> @$ try the current class <PLACE_HOLDER>,in = ss . get resource as stream ( cl @$ service ) ;,loader try
instance is cached by realm @$ so no need to keep strong <PLACE_HOLDER>,return flowable . create ( new flowable on subscribe < realm > ( ) { @ override public void subscribe ( final flowable emitter < realm > emitter ) throws exception { final realm observable realm = realm . get instance ( realm config ) ; final realm change listener < realm > listener = new realm change listener < realm > ( ) { @ override public void on change ( realm realm ) { if ( ! emitter . is cancelled ( ) ) { emitter . on next ( realm ) ; } } } ; observable realm . add change listener ( listener ) ; emitter . set disposable ( disposables . from runnable ( new runnable ( ) { @ override public void,need keep
false signifies that a marker message has not already been processed . generate and send <PLACE_HOLDER> .,if ( proxy != null ) { proxy . start or resume message dispatcher ( false ) ; },signifies generate
the real value will be sent back as long as the field is marked as dirty and diffstate contains <PLACE_HOLDER> the client has,assert equals ( __str__ @$ get diff state string ( rta @$ __str__ ) ) ; assert true ( __str__ @$ is dirty ( rta ) ) ;,client has
dynamic ser de always writes out bytes <PLACE_HOLDER>,bytes writable bw = ( bytes writable ) r ; out stream . write ( bw . get ( ) @$ __num__ @$ bw . get size ( ) ) ;,ser writes
copy from does not set <PLACE_HOLDER> that have been already set @$ so this must be called after @$ which is a bit in the reverse from what one might think .,m_output format . copy from ( m_stylesheet root . get output properties ( ) ) ;,copy set
adapter 1 should no longer has <PLACE_HOLDER> and adapter 2 will have <PLACE_HOLDER>,assert true ( adapter1 . has observer ( ) ) ; assert null ( fragment . m main fragment list row data adapter ) ;,adapter has
the problem with xerces is that some errors will cause the <PLACE_HOLDER> not to advance the reader and it will keep reporting the same error over and over @$ which will cause the <PLACE_HOLDER> to enter an infinite loop unless we throw the exception .,if ( message != null && is fatal ( message ) ) { throw ex ; },errors cause
we have to make sure that the qname really references the same variable <PLACE_HOLDER> .,if ( get elem variable ( ) != ( ( variable ) expr ) . get elem variable ( ) ) return false ; return true ;,qname references
make sure the component are the same if the input <PLACE_HOLDER> has the same real <PLACE_HOLDER> as the one in the task because either one of them could be the alias <PLACE_HOLDER> .,if ( objects . equals ( real activity @$ r . m activity component ) && this . intent != null ) { intent . set component ( this . intent . get component ( ) ) ; } return intent . filter equals ( this . intent ) ;,activity has
this tells the target entry to not set the size header field @$ to ensure that no tar <PLACE_HOLDER> accidentally extracts only a portion of the file data . if the <PLACE_HOLDER> ca n't read the correct size from the pif data @$ we want the <PLACE_HOLDER> to report that so the user can get a better tar <PLACE_HOLDER> !,pax sized = true ;,user get
this is somewhat unsafe @$ but it 's better than outright throwing an <PLACE_HOLDER> here . returning null will just cause an <PLACE_HOLDER> down the pipeline .,return ( jc expression ) in ;,here cause
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
open a new region which uses this <PLACE_HOLDER>,table descriptor htd = table descriptor builder . new builder ( table name . value of ( this . name . get method name ( ) ) ) . set column family ( column family descriptor builder . of ( b ) ) . build ( ) ; region info hri = region info builder . new builder ( htd . get table name ( ) ) . build ( ) ; chunk creator . initialize ( mem storelab impl . chunk_size_default @$ false @$ __num__ @$ __num__ @$ __num__ @$ null ) ; final h region region = test_util . create localh region ( hri @$ htd @$ log ) ; executor service exec = executors . new fixed thread pool ( __num__ ) ;,which uses
validate in configurations without explicit managed memory <PLACE_HOLDER> @$ to avoid checking against overwritten managed memory <PLACE_HOLDER>,validate in configurations without explicit managed mem ( conf @$ task executor resource spec -> assert that ( task executor resource spec . get managed memory size ( ) @$ is ( managed mem size ) ) ) ;,validate managed
if this node implements the manufacturer specific command <PLACE_HOLDER> @$ we use it to get manufacturer info .,if ( manufacturer specific != null ) { logger . debug ( __str__ @$ node . get node id ( ) ) ; add to queue ( manufacturer specific . get manufacturer specific message ( ) ) ; },node implements
read repair get operation produces different <PLACE_HOLDER> for same entries loaded via read through feature .,if ( context ( ) . read through ( ) ) { throw new unsupported operation exception ( __str__ ) ; },operation produces
it 's important to use the page transition from the suggestion or we might end up saving generated ur ls as typed ur ls @$ which would then pollute the subsequent omnibox <PLACE_HOLDER> . there is one special case where the suggestion text was pasted @$ where we want the transition type to be link .,int transition = suggestion match . get type ( ) == omnibox suggestion type . url_what_you_typed && m url bar . is pasted text ( ) ? page transition . link : suggestion match . get transition ( ) ; load url from omnibox match ( suggestion match url @$ transition @$ suggestion match position @$ suggestion match . get type ( ) ) ;,which pollute
to avoid rejected execution exception in basic directory model wait a <PLACE_HOLDER>,try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; },exception wait
these are all of the events listed in the javadoc for xml event . the spec only really describes <PLACE_HOLDER> of them .,while ( true ) { switch ( event ) { case xml stream constants . start_element : handle start element ( ) ; depth ++ ; break ; case xml stream constants . end_element : depth -- ; handle end element ( ) ; if ( depth == __num__ ) break outer ; break ; case xml stream constants . characters : case xml stream constants . cdata : case xml stream constants . space : handle characters ( ) ; break ; } event = stax stream reader . next ( ) ; },spec describes
the subscriber will flow <PLACE_HOLDER> before an entire term buffer . so @$ send until ca n't send no 'more . then start up subscriber to drain .,final int term buffer length = __num__ * __num__ ; final int num messages per term = __num__ ; final int message length = ( term buffer length / num messages per term ) - header_length ; final int max fails = __num__ ; int messages sent = __num__ ; context . publication term buffer length ( term buffer length ) ; launch ( channel ) ; for ( int i = __num__ ; i < num messages per term ; i ++ ) { int offer fails = __num__ ; while ( publication . offer ( buffer @$ __num__ @$ message length ) < __num__ ) { if ( ++ offer fails > max fails ) { break ; } system test . check interrupted status (,subscriber flow
check template literal has type <PLACE_HOLDER>,test types ( __str__ @$ lines ( __str__ @$ __str__ @$ __str__ ) ) ;,literal has
check whether next token matches the specified <PLACE_HOLDER>,if ( matcher . matches ( ) ) { cached next index = find start index ; match successful = true ; has next = true ; } recover previous status ( ) ; return has next ;,token matches
if event contains new <PLACE_HOLDER> @$ then it may mean that the delta bytes should not be applied . this is possible if the event originated locally .,if ( this . delta bytes != null && this . new value == null && this . new value bytes == null ) { process delta bytes ( old value for delta ) ; } if ( owner != null ) { owner . generate and set version tag ( this @$ reentry ) ; } else { get region ( ) . generate and set version tag ( this @$ reentry ) ; } generate new value from bytes if needed ( ) ; object v = this . new value ; if ( v == null ) { v = is local invalid ( ) ? token . local_invalid : token . invalid ; } else { get region ( ) . set region invalid,event contains
upload service.get pending media for post will be populated only when the user exits the <PLACE_HOLDER> but if the user does n't exit the <PLACE_HOLDER> and sends the app to the background @$ a reattachment for the media within this post is needed as soon as the app comes back to foreground @$ so we get the list of progressing media for this post,if ( m editor fragment instanceof aztec editor fragment && m editor media upload listener != null ) { set < media model > uploading media in post = m edit post repository . get pending media for post ( ) ; list < media model > all uploading media in post = new array list < > ( uploading media in post ) ; for ( media model media1 : m edit post repository . get pending or in progress media uploads for post ( ) ) { boolean found = false ; for ( media model media2 : uploading media in post ) { if ( media1 . get id ( ) == media2 . get id ( ) ) { found = true ; break,user exits
use annotation declared <PLACE_HOLDER>,container . add endpoint ( server endpoint config . builder . create ( pong socket . class @$ __str__ ) . build ( ) ) ;,annotation declared
create a bunch of test files with random replication factors . insert <PLACE_HOLDER> into a linked list .,try { for ( int i = __num__ ; i < number of files ; i ++ ) { final int replication = append test util . next int ( num datanodes - __num__ ) + __num__ ; path test file = new path ( __str__ + i + __str__ ) ; fs data output stream stm = append test util . create file ( fs @$ test file @$ replication ) ; stm . close ( ) ; test files . add ( test file ) ; } workload = new workload [ num threads ] ; for ( int i = __num__ ; i < num threads ; i ++ ) { workload [ i ] = new workload ( cluster @$ i @$ append to,bunch insert
expect an error dialog create 3 <PLACE_HOLDER> of the program,do create versions ( ) ; final g tree node node = get node ( program_a ) ; select node ( node ) ; final docking action if history action = get action ( __str__ ) ; run swing ( ( ) -> history action . action performed ( get domain file action context ( node ) ) ) ; version history dialog dialog = wait for dialog component ( version history dialog . class ) ; docking action if delete action = get delete action ( dialog ) ; g table table = find component ( dialog @$ g table . class ) ; run swing ( ( ) -> table . select row ( __num__ ) ) ; perform action ( delete action @$ false ),dialog create
wrapping the cleanup logic in an auto closable automatically suppresses additional <PLACE_HOLDER>,try ( final cleanup yarn application ignored = new cleanup yarn application ( ) ) { test . run ( ) ; },logic suppresses
assume this can be done ; if not throw <PLACE_HOLDER> as per javadoc,@ suppress warnings ( __str__ ) final comparator < object > comparator2 = ( comparator < object > ) comparator ;,not throw
somebody else did a failover concurrently try that <PLACE_HOLDER> now,synchronized ( lock ) { if ( ( old spi != null ) && ( old spi != spi ) ) { return spi ; } if ( service iterator == null ) { return null ; } while ( service iterator . has next ( ) ) { service s = service iterator . next ( ) ; if ( jce security . can use provider ( s . get provider ( ) ) == false ) { continue ; } try { object obj = s . new instance ( null ) ; if ( obj instanceof secret key factory spi == false ) { continue ; } secret key factory spi spi = ( secret key factory spi ) obj ; provider = s .,failover try
we collapse property definitions after collapsing property references because this step can alter the parse <PLACE_HOLDER> above property references @$ invalidating the node ancestry stored with each reference .,for ( name name : global names ) { collapse declaration of name and descendants ( name @$ name . get base name ( ) @$ escaped ) ; },step alter
open enough file descriptors that we will crash something if we leak f ds or address <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { uri uri = uri . parse ( __str__ ) ; input stream in = resolver . open input stream ( uri ) ; assert not null ( __str__ + i @$ in ) ; byte [ ] buf = new byte [ memory file provider . test_blob . length ] ; int count = in . read ( buf ) ; assert equals ( buf . length @$ count ) ; assert true ( arrays . equals ( memory file provider . test_blob @$ buf ) ) ; in . close ( ) ; },f ds
we do n't need to retry this operation in the case of failure as zk will remove ephemeral <PLACE_HOLDER> and we do n't wan na hang this process when closing if we can not reconnect to zk,if ( ! is closed ( ) && id != null ) { try { zoo keeper operation zopdel = ( ) -> { zookeeper . delete ( id @$ - __num__ ) ; return boolean . true ; } ; zopdel . execute ( ) ; } catch ( interrupted exception e ) { log . warn ( __str__ @$ e ) ; thread . current thread ( ) . interrupt ( ) ; } catch ( keeper exception . no node exception e ) { } catch ( keeper exception e ) { log . warn ( __str__ @$ e ) ; throw new runtime exception ( e . get message ( ) @$ e ) ; } finally { lock listener lock listener = get,zk remove
make sure no partition has more than active <PLACE_HOLDER>,volt table stats = client . call procedure ( __str__ @$ __str__ @$ __num__ ) . get results ( ) [ __num__ ] ; map < string @$ integer > master counters = maps . new hash map ( ) ; while ( stats . advance row ( ) ) { string target = stats . get string ( __str__ ) ; string ttable = stats . get string ( __str__ ) ; string is master = stats . get string ( __str__ ) ; long pid = stats . get long ( __str__ ) ; string key = __str__ + target + __str__ + ttable + __str__ + pid + __str__ + is master ; integer count = master counters . get ( key ) ; if,partition has
some services automatically adds <PLACE_HOLDER> from an addressbook to our roster and this <PLACE_HOLDER> are with subscription none . if such already exist @$ remove it . this is typically our own contact,if ( ! server stored contact list jabber impl . is entry displayable ( item ) ) { if ( contact != null ) { remove contact ( contact ) ; sscl callback . fire contact removed ( this @$ contact ) ; } continue ; } if ( contact != null ) { contact . set resolved ( item ) ; sscl callback . fire contact resolved ( this @$ contact ) ; } else { contact jabber impl new contact = new contact jabber impl ( item @$ sscl callback @$ true @$ true ) ; add contact ( new contact ) ; sscl callback . fire contact added ( this @$ new contact ) ; },services adds
minimum scale reached so do n't pan . adjust start <PLACE_HOLDER> so any expand will zoom in .,if ( scale <= min scale ( ) ) { v dist start = v dist end ; scale start = min scale ( ) ; v center start . set ( v center endx @$ v center endy ) ; v translate start . set ( v translate ) ; } else if ( pan enabled ) { float v left start = v center start . x - v translate start . x ; float v top start = v center start . y - v translate start . y ; float v left now = v left start * ( scale / scale start ) ; float v top now = v top start * ( scale / scale start ) ; v translate .,pan adjust
transition nn <PLACE_HOLDER> to active and do some fs ops .,cluster . transition to active ( __num__ ) ; fs = ha test util . configure failover fs ( cluster @$ conf ) ; assert true ( fs . mkdirs ( new path ( __str__ ) ) ) ;,transition nn
check principal has right <PLACE_HOLDER> and right type and roles are available,check authenticated identity ( response ) ;,principal has
execute client put <PLACE_HOLDER> from multithread client,assert that ( region . size ( ) ) . is zero ( ) ;,client put
draw text is always relative to text view 's origin @$ this translation brings this <PLACE_HOLDER> of text back to the top left corner of the viewport,try { recording canvas . translate ( - left @$ - top ) ; layout . draw text ( recording canvas @$ block begin line @$ block end line ) ; m text render nodes [ block index ] . is dirty = false ; } finally { block display list . end recording ( ) ; block display list . set clip to bounds ( false ) ; },translation brings
since lambda form caches are based on soft references @$ gc can cause element <PLACE_HOLDER> .,if ( lambda form0 != lambda form1 ) { if ( nogc happened ( ) ) { system . err . println ( __str__ ) ; system . err . println ( lambda form0 ) ; system . err . println ( __str__ ) ; system . err . println ( lambda form1 ) ; throw new assertion error ( __str__ + __str__ ) ; } else { system . err . println ( __str__ ) ; } },gc cause
doesnt matter <PLACE_HOLDER> node.id and voldemort.home values are for this test,props . set property ( __str__ @$ __str__ ) ; props . set property ( __str__ @$ __str__ ) ; voldemort config = new voldemort config ( props ) ; voldemort config . set rocksdb prefix keys with partition id ( this . prefix partition id ) ; this . rocks db config = new rocks db storage configuration ( voldemort config ) ; this . rocks db store = ( rocks db storage engine ) rocks db config . get store ( test utils . make store definition ( __str__ ) @$ test utils . make single node routing strategy ( ) ) ; random = new random ( ) ;,values test
we will now anser the call and verify that both parties change <PLACE_HOLDER> accordingly .,basic telephonyp2 . answer call peer ( peer atp2 ) ; state collector for pp1 . wait for event ( __num__ ) ; state collector for pp2 . wait for event ( __num__ ) ;,parties change
return to ascii encodation @$ which will actually handle <PLACE_HOLDER> to new mode,if ( new mode != get encoding mode ( ) ) { context . signal encoder change ( high level encoder . ascii_encodation ) ; break ; },which handle
then evaluation is successful and the nodes have the expected <PLACE_HOLDER> .,assert that evaluation result ( result ) . has entry that ( a key ) . is equal to ( new string value ( __str__ ) ) ; assert that evaluation result ( result ) . has entry that ( b key ) . is equal to ( new string value ( __str__ ) ) ;,nodes have
options processed by abstract java jaxrs server <PLACE_HOLDER>,additional properties . put ( codegen constants . impl_folder @$ __str__ ) ; additional properties . put ( bean validation features . use_beanvalidation @$ __str__ ) ; additional properties . put ( abstract javajaxrs server codegen . server_port @$ __str__ ) ;,options jaxrs
the amount with which to adjust the user provided content <PLACE_HOLDER> to account for stroke and shape corners .,int content padding offset = ( int ) ( ( include corner padding ? calculate actual corner padding ( ) : __num__ ) - get parent card view calculated corner padding ( ) ) ; material card view . set ancestor content padding ( user content padding . left + content padding offset @$ user content padding . top + content padding offset @$ user content padding . right + content padding offset @$ user content padding . bottom + content padding offset ) ;,amount provided
returns object <PLACE_HOLDER> from document loader .,return get document loader ( parent identifier ) . query child documents ( projection @$ parent identifier ) ;,returns object
lots of messages may be lost when deserialize queue has n't finished init <PLACE_HOLDER>,while ( ! bstart rec ) { log . info ( __str__ ) ; boolean is finish init = true ; for ( integer task : worker tasks ) { if ( deserialize queues . get ( task ) == null ) { is finish init = false ; j storm utils . sleep ms ( __num__ ) ; break ; } } if ( is finish init ) { bstart rec = is finish init ; } } short type = message . get_type ( ) ; if ( type == task message . normal_message ) { int task = message . task ( ) ; disruptor queue queue = deserialize queues . get ( task ) ; if ( queue == null ) { log .,queue finished
ensure that the dialog takes up the entire <PLACE_HOLDER> . this is needed because the scrollbar needs to be drawn off the dialog .,window manager . layout params layout params = window . get attributes ( ) ; layout params . height = view group . layout params . match_parent ; layout params . width = view group . layout params . match_parent ; window . set attributes ( layout params ) ;,dialog takes
if the value contains a decimal or grouping <PLACE_HOLDER> or some sort @$ it 's not an integer,if ( ( c == __str__ && cmm . get conversion meta ( ) . is integer ( ) ) || ( c == __str__ && cmm . get conversion meta ( ) . is integer ( ) ) ) { evaluation results . remove ( cmm ) ; stop = true ; break ; } if ( c == __str__ ) { nr dots ++ ; } if ( c == __str__ ) { nr commas ++ ; } pos ++ ;,value contains
fire column resize <PLACE_HOLDER> for all columns but the source of the resize action @$ since an event will fire separately for this .,list < header cell > columns = new array list < header cell > ( available cells . values ( ) ) ; columns . remove ( source ) ; send column width updates ( columns ) ; force realign column headers ( ) ;,column resize
best effort to create a rich error <PLACE_HOLDER>,message proxy creation error = iterables . get only element ( e . get errors ( ) ) ; message cycle dependencies message = create cycle dependencies message ( locks cycle @$ proxy creation error ) ;,effort create
page transformers can do complex <PLACE_HOLDER> that benefit from hardware layers .,if ( m page transformer != null ) { enable layers ( new state != scroll_state_idle ) ; },transformers do
wait for first job begin <PLACE_HOLDER> .,job executed latch . await ( ) ; start grid ( __num__ ) ; for ( ignite g : g . all grids ( ) ) info ( __str__ + g . cluster ( ) . local node ( ) . id ( ) + __str__ + g . cluster ( ) . local node ( ) . metrics ( ) + __str__ ) ; future . get ( ) ; assert null ( __str__ @$ fail . get ( ) ) ;,wait begin
updates cached <PLACE_HOLDER> with the blob information .,path layer file = cache storage files . get layer file ( written layer . layer digest @$ written layer . layer diff id ) ; return cached layer . builder ( ) . set layer digest ( written layer . layer digest ) . set layer diff id ( written layer . layer diff id ) . set layer size ( written layer . layer size ) . set layer blob ( blobs . from ( layer file ) ) . build ( ) ;,updates cached
need to close before emiting file to the subscriber @$ because when subscriber receives <PLACE_HOLDER> in the same thread the file may be truncated,sink . close ( ) ; sink = null ; subscriber . on next ( output ) ; subscriber . on completed ( ) ; if ( sink != null ) { try { sink . close ( ) ; } catch ( io exception e ) { subscriber . on error ( e ) ; } },subscriber receives
let others send <PLACE_HOLDER> . unless there are miltiple oob send calls @$ there can be only one waiter @$ the responder thread . in any case @$ only one needs to be notified .,synchronized ( this ) { sending = false ; notify ( ) ; },others send
force transparence of this layer only . if no buffering we would clear <PLACE_HOLDER> below,if ( buffering ) { comp = g2 . get composite ( ) ; g2 . set composite ( alpha composite . clear ) ; } g2 . set color ( new color ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; g2 . fill rect ( __num__ @$ __num__ @$ bounds . width @$ bounds . height ) ; g2 . set color ( col ) ; if ( comp != null ) { g2 . set composite ( comp ) ; },buffering clear
user jack can not recover <PLACE_HOLDER> from journal @$ in which the root is owned by alluxio .,master test utils . create leader file system master from journal ( new test user state ( user @$ server configuration . global ( ) ) ) . close ( ) ;,jack recover
older versions did not have a resource <PLACE_HOLDER> @$ they stuffed it into the description .,if ( origin type == origin type . resource && resource or null == null ) { resource or null = description ; } return new simple config origin ( description @$ line number != null ? line number : - __num__ @$ end line number != null ? end line number : - __num__ @$ origin type @$ url or null @$ resource or null @$ comments or null ) ;,versions have
0 progress does n't do <PLACE_HOLDER> .,assert . assert equals ( collections . n copies ( __num__ @$ false ) @$ multithreaded executor . invoke all ( collections . n copies ( __num__ @$ ( ) -> allocation completion tracker . update progress ( allocation tree . child1 @$ __num__ ) ) ) ) ; assert . assert equals ( arrays . as list ( allocation tree . root @$ allocation tree . child1 @$ allocation tree . child1 child @$ allocation tree . child2 ) @$ allocation completion tracker . get unfinished allocations ( ) ) ;,progress do
does this operator make any <PLACE_HOLDER> ?,assert equals ( __str__ @$ pretty ( __str__ ) ) ;,operator make
for local queries returning pdx objects wrap the resultset with results collection pdx deserializer <PLACE_HOLDER> which deserializes these pdx objects .,if ( needspdx deserialization wrapper ( true ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ false ) ; },collection pdx
caller is requesting a <PLACE_HOLDER> @$ never accessed before @$ ce . verify that it is the next one in sequence @$ which is all that is allowed .,if ( index != limit ix_ ) { assert ( false ) ; return null ; },caller requesting
check:3 compare the key <PLACE_HOLDER>,for ( int i = __num__ ; i < prev count ; i ++ ) { value meta interface pre value = null ; for ( int j = __num__ ; j < rows . length ; j ++ ) { value meta interface v = rows [ j ] . search value meta ( key list . get ( j ) [ i ] ) ; if ( v == null ) { return false ; } if ( j != __num__ && v . get type ( ) != pre value . get type ( ) ) { log error ( __str__ ) ; return false ; } else { pre value = v ; } } },check:3 compare
wait to ensure nn has fully created its test <PLACE_HOLDER>,thread . sleep ( __num__ ) ;,nn created
map the spring to the feedback bar position so that its hidden off <PLACE_HOLDER> and bounces in on tap .,float bar position = ( float ) spring util . map value from range to range ( value @$ __num__ @$ __num__ @$ m feedback bar . get height ( ) @$ __num__ ) ; m feedback bar . set translationy ( bar position ) ;,its hidden
spawn a new thread modify and cust id <PLACE_HOLDER> in another tx so that outer thread fails,class tx thread extends thread { @ override public void run ( ) { cache transaction manager mgr = get gemfire cache ( ) . get tx manager ( ) ; mgr . set distributed ( true ) ; mgr . begin ( ) ; cust id cust id one = new cust id ( __num__ ) ; customer customer one = new customer ( __str__ @$ __str__ ) ; region < cust id @$ customer > cust region = get cache ( ) . get region ( region name ) ; cust region . put ( cust id one @$ customer one ) ; mgr . commit ( ) ; } } tx thread tx thread = new tx thread ( ) ; tx thread . start,thread modify
we do n't know if rv changed <PLACE_HOLDER> so we should invalidate this index .,m overdraw child position = - __num__ ; float dx = __num__ @$ dy = __num__ ; if ( m selected != null ) { get selected dx dy ( m tmp position ) ; dx = m tmp position [ __num__ ] ; dy = m tmp position [ __num__ ] ; } m callback . on draw ( c @$ parent @$ m selected @$ m recover animations @$ m action state @$ dx @$ dy ) ;,rv changed
the jitter allows the <PLACE_HOLDER> to get the data at different times @$ and avoids thundering herd,if ( ! ( boolean ) topology conf . get ( config . topology_disable_loadaware_messaging ) ) { worker state . refresh load timer . schedule recurring with jitter ( __num__ @$ __num__ @$ __num__ @$ worker . this :: do refresh load ) ; } worker state . refresh connections timer . schedule recurring ( __num__ @$ ( integer ) conf . get ( config . task_refresh_poll_secs ) @$ worker state :: refresh connections ) ; worker state . reset log levels timer . schedule recurring ( __num__ @$ ( integer ) conf . get ( config . worker_log_level_reset_poll_secs ) @$ log config manager :: reset log levels ) ; worker state . refresh active timer . schedule recurring ( __num__ @$ ( integer ) conf . get,jitter allows
listener for changes to the destination pattern entry <PLACE_HOLDER> and not a permission entry .,event context . add naming listener ( topic search base @$ __str__ @$ new search controls ( ) @$ this . new cachedldap authorization map namespace change listener ( destination type . topic @$ null ) ) ;,listener entry
only a ui automation can set this <PLACE_HOLDER> and now that it is finished we make sure it is reset to its default .,m user is monkey = false ;,automation set
first check length of journal to make sure it makes sense to even try . if there is only one journal file with acks in it we do n't need to move it since it wo n't be chained to any later logs . if the logs have n't grown since the last <PLACE_HOLDER> then we need to compact otherwise there seems to still,if ( ++ check point cycles with nogc >= get compact acks after nogc ( ) ) { if ( metadata . ack message file map . size ( ) > __num__ && ( journal log on last compaction check == journal . get current data file id ( ) || is compact acks ignores store growth ( ) ) ) { log . trace ( __str__ ) ; try { scheduler . execute ( new ack compaction runner ( ) ) ; } catch ( exception ex ) { log . warn ( __str__ @$ ex ) ; } } else { log . trace ( __str__ ) ; } check point cycles with nogc = __num__ ; } else { log . trace ( __str__ @$,compaction run
if this dialog has n't got an <PLACE_HOLDER> or has been already closed @$ do n't send notification,if ( hwnd != __num__ ) { window . modal enable ( ( dialog ) target ) ; },dialog got
count the constant method return <PLACE_HOLDER> .,if ( method propagation returnvalue ) { program class pool . classes accept ( new all method visitor ( new constant member filter ( method propagation returnvalue counter ) ) ) ; },method return
server should not return a 416 if at least one syntactically valid ranges are is satisfiable should test for combinations of good and syntactically invalid ranges here @$ but i am not certain <PLACE_HOLDER> the right behavior is anymore return data for valid ranges while ignoring unsatisfiable ranges,string alpha = alpha ;,behavior data
'next ' contains the resulting <PLACE_HOLDER> after transformation in the current iteration . it should not contain null values .,final linked list < event object > next = new linked list < event object > ( ) ; for ( map . entry < integer @$ vector < transform layer > > entry : op set message transform . transform layers . entry set ( ) ) { for ( transform layer transform layer : entry . get value ( ) ) { next . clear ( ) ; while ( ! current . is empty ( ) ) { final event object event = current . remove ( ) ; switch ( event type ) { case message delivered : message delivered event transformed delivered = transform layer . message delivered ( ( message delivered event ) event ) ; if ( transformed delivered != null,"next" contains
build map of all hash <PLACE_HOLDER> note : full outer join does n't use hash <PLACE_HOLDER>,map < hash computation @$ symbol > all hash symbols = new hash map < > ( ) ; if ( node . get type ( ) == inner || node . get type ( ) == left ) { all hash symbols . put all ( left . get hash symbols ( ) ) ; } if ( node . get type ( ) == inner || node . get type ( ) == right ) { all hash symbols . put all ( right . get hash symbols ( ) ) ; } return build join node with preferred hashes ( node @$ left @$ right @$ all hash symbols @$ parent preference @$ optional . of ( left hash symbol ) @$ optional . of,join use
debug mode overrides all saved <PLACE_HOLDER> so no setup needed,set acra config ( __str__ @$ anki droid app . get shared prefs ( instrumentation registry . get instrumentation ( ) . get target context ( ) ) ) ; assert array equals ( __str__ @$ app . get acra core config builder ( ) . build ( ) . logcat arguments ( ) . to array ( ) @$ new immutable list < > ( debug logcat arguments ) . to array ( ) ) ; verify debugacra preferences ( ) ;,all saved
also @$ equality should work <PLACE_HOLDER>,assert equals ( result @$ double node . value of ( value ) ) ;,equality work
group the <PLACE_HOLDER> into those that use indexes and those that do n't @$ so we can evaluate all the <PLACE_HOLDER> that do n't use indexes together in one iteration first get filter <PLACE_HOLDER>,filter filter operands = null ;,iteration get
create new target aspect ratio if the original one does not fit the <PLACE_HOLDER>,if ( crop height == edge . min_crop_length_px ) m target aspect ratio = ( edge . right . get coordinate ( ) - edge . left . get coordinate ( ) ) / edge . min_crop_length_px ; final float half crop height = crop height / __num__ ; edge . top . set coordinate ( centery - half crop height ) ; edge . bottom . set coordinate ( centery + half crop height ) ;,one fit
show that hql does <PLACE_HOLDER>,do in hibernate ( this :: session factory @$ session -> { query query = session . create query ( __str__ @$ foo . class ) ; list < foo > list = query . get result list ( ) ; assert equals ( __num__ @$ list . size ( ) ) ; } ) ;,hql does
completing task will trigger completion <PLACE_HOLDER>,task service . complete ( new task . get id ( ) ) ; assert equals ( __num__ @$ task service . create task query ( ) . count ( ) ) ; assert process ended ( proc id ) ;,task trigger
search backwards from the end of the zip file @$ searching for the eocd signature @$ which designates the <PLACE_HOLDER> of the eocd .,int eocd offset = ( int ) zip size - zip entry . endhdr ; while ( map . get int ( eocd offset ) != zip entry . endsig ) { eocd offset -- ; } long cd entries = short . to unsigned long ( map . get short ( eocd offset + zip entry . endtot ) ) ; if ( ( cd entries & __num__ ) == __num__ ) { int zip64eocd offset = eocd offset ; while ( map . get int ( zip64eocd offset ) != zip64_endsig ) { zip64eocd offset -- ; } cd entries = map . get long ( zip64eocd offset + __num__ ) ; },which designates
this timezone did n't have any daylight <PLACE_HOLDER> prior to 1917 and this date is sometime in 1901 .,assert false ( tz . in daylight time ( new date ( - __num__ ) ) ) ; assert equals ( - __num__ @$ tz . get offset ( - __num__ ) ) ;,timezone have
note : the python version uses slices which return an empty <PLACE_HOLDER> when indexed beyond what the <PLACE_HOLDER> contains . since we ca n't slice out an empty sub<PLACE_HOLDER> in java @$ we must check if we 've reached the end and clear the fnames <PLACE_HOLDER> manually .,if ( cnt == fnames . size ( ) ) { fnames . clear ( ) ; } else { fnames = fnames . sub list ( cnt @$ fnames . size ( ) ) ; } m con . publish progress ( string . format ( anki droid app . get app resources ( ) . get string ( r . string . sync_media_downloaded_count ) @$ m download count ) ) ;,which return
the sub directory should also have the old <PLACE_HOLDER>,acl entry [ ] child dir expected acl = new acl entry [ ] { acl entry ( access @$ user @$ __str__ @$ all ) @$ acl entry ( access @$ group @$ read_execute ) @$ acl entry ( default @$ user @$ all ) @$ acl entry ( default @$ user @$ __str__ @$ all ) @$ acl entry ( default @$ group @$ read_execute ) @$ acl entry ( default @$ mask @$ all ) @$ acl entry ( default @$ other @$ read_execute ) } ; acl status child dir acl = hdfs . get acl status ( child dir ) ; assert array equals ( child dir expected acl @$ child dir acl . get entries ( ) . to array ( ),directory have
the user may have changed <PLACE_HOLDER> .,if ( tab id == m tab model selector . get current tab id ( ) ) { request reader panel show ( state change reason . unknown ) ; },user changed
models . you can write model files using the model template files <PLACE_HOLDER> . if you want to create one template for file @$ you can do so here . for multiple files for model @$ just put another entry in the ` model template files ` with a different extension,model template files . clear ( ) ;,entry files
told server do not hang <PLACE_HOLDER> up if new initializing cache data added in,if ( is initializing cache list ) { headers . add ( __str__ ) ; headers . add ( __str__ ) ; } if ( string utils . is blank ( probe update string ) ) { return collections . empty list ( ) ; } try { long read timeout ms = timeout + ( long ) math . round ( timeout > > __num__ ) ; http result result = agent . http post ( constants . config_controller_path + __str__ @$ headers @$ params @$ agent . get encode ( ) @$ read timeout ms ) ; if ( httpurl connection . http_ok == result . code ) { set health server ( true ) ; return parse update data id response ( result . content,server hang
most hebrew labels should select the <PLACE_HOLDER> to last character,if ( locale . get language ( ) . equals ( __str__ ) || locale . get language ( ) . equals ( __str__ ) ) { if ( m day label calendar . get ( calendar . day_of_week ) != calendar . saturday ) { int len = day name . length ( ) ; day label = day name . substring ( len - __num__ @$ len - __num__ ) ; } else { day label = day name . to upper case ( locale ) . substring ( __num__ @$ __num__ ) ; } },labels select
done @$ no one is asking <PLACE_HOLDER> from us,if ( operation < __num__ ) { return ; },one asking
legacy level does n't report min frame <PLACE_HOLDER>,if ( m static info . is hardware level legacy ( ) ) { if ( candidate size . get width ( ) <= video sz . get width ( ) && candidate size . get height ( ) <= video sz . get height ( ) ) { video snapshot sz = candidate size ; } } else { long jpeg frame duration = min frame duration map . get ( candidate size ) ; assert true ( __str__ + candidate size @$ jpeg frame duration != null ) ; if ( candidate size . get width ( ) <= video sz . get width ( ) && candidate size . get height ( ) <= video sz . get height ( ) && jpeg frame duration,level report
this prevents that the current writer @$ which holds instance for the last document he is editing will not override our last <PLACE_HOLDER> to the document,if ( filename . equals ( this . current file ) ) { this . current doc = doc ; } break ;,writer override
this should never happen with <PLACE_HOLDER> as errors . the plus set should always contain at least the <PLACE_HOLDER> in <PLACE_HOLDER> as errors .,if ( xlint plus . is empty ( ) ) { normalized . add ( __str__ ) ; },set contain
old middle child has <PLACE_HOLDER>,go to ( read cursor @$ middle child ) ; assert equals ( new middle child @$ successor ( read cursor @$ stable generation @$ unstable generation ) ) ;,child has
truncate did not complete immediately @$ we must wait for the operation to complete and re<PLACE_HOLDER> the <PLACE_HOLDER> .,if ( ! truncated ) { wait until lease is revoked ( file system @$ path ) ; },truncate complete
if the list accepts the key events and the key event was a click @$ the text view gets the selected <PLACE_HOLDER> from the drop down as its content,if ( consumed && key event . is confirm key ( key code ) ) { dismiss ( ) ; },view gets
something else caused the <PLACE_HOLDER> @$ throw it ...,if ( ! ( surface data . is null ( dst data ) || surface data . is null ( src data ) ) ) { throw e ; },something caused
creating a size and time <PLACE_HOLDER> does not need a separate triggering <PLACE_HOLDER> set on the appender because this <PLACE_HOLDER> registers the trigger <PLACE_HOLDER>,final size and time based rolling policy < e > size and time based rolling policy = new size and time based rolling policy < > ( ) ; size and time based rolling policy . set max file size ( new file size ( max file size . to bytes ( ) ) ) ; rolling policy = size and time based rolling policy ;,policy need
this provider only understands <PLACE_HOLDER>,int pid ; try { pid = integer . parse int ( vmid ) ; } catch ( number format exception x ) { throw new attach not supported exception ( __str__ ) ; },provider understands
check if the user has this <PLACE_HOLDER> defined in the context,for ( role role : this . user name to user . get ( user . name ) . roles ) { if ( role == null ) continue ; for ( permission permitted : role . permissions ) { if ( permitted . implies ( context ) ) { return true ; } } } return false ;,user has
it depends ... for example @$ tasks and stacks are only visible if there children are visible but @$ window state are not visible if there parent are not visible . maybe have the container specify <PLACE_HOLDER> direction to traverse for visibility ?,for ( int i = m children . size ( ) - __num__ ; i >= __num__ ; -- i ) { final window container wc = m children . get ( i ) ; if ( wc . is visible ( ) ) { return true ; } } return false ;,direction traverse
element has no <PLACE_HOLDER>,if ( elem . get local name ( ) == null ) { if ( fdom error handler != null ) { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { elem . get node name ( ) } ) ; modifydom error ( msg @$ dom error . severity_error @$ null @$ elem ) ; boolean continue process = fdom error handler . handle error ( fdom error ) ; if ( ! continue process ) { throw new runtime exception ( dom message formatter . format message ( dom message formatter . serializer_domain @$ __str__ @$ null ) ) ; } } } else { uri = fns binder . geturi (,element has
setup server certificates and private keys . clients will receive a <PLACE_HOLDER> back to request certificates .,if ( ! client ) { set < string > key types = new hash set < string > ( ) ; for ( string enabled cipher suite : enabled cipher suites ) { if ( enabled cipher suite . equals ( native crypto . tls_empty_renegotiation_info_scsv ) ) { continue ; } string key type = cipher suite . get by name ( enabled cipher suite ) . get server key type ( ) ; if ( key type != null ) { key types . add ( key type ) ; } } for ( string key type : key types ) { try { set certificate ( ssl parameters . get key manager ( ) . choose server alias ( key type @$ null @$ this,clients receive
check if any of the filters disallows this <PLACE_HOLDER>,for ( fetch filter f : fetch filters ) { fetch status s = f . check filter ( uriv ) ; if ( s != fetch status . valid ) { log . debug ( __str__ + uriv + __str__ + s ) ; spider . notify listeners founduri ( uri @$ http request header . get @$ s ) ; return ; } },any disallows
go through the set of partition <PLACE_HOLDER> @$ and find their representatives in the values these represent the bucketed <PLACE_HOLDER>,list < bucket col > bucket cols = extract bucket cols ( rop @$ output values ) ;,these represent
as the generate bucket name function uses a <PLACE_HOLDER> @$ this should pretty much never happen,if ( storage . get ( bucket ) != null ) { fail ( __str__ + bucket + __str__ ) ; },function uses
if the cookie does n't have a <PLACE_HOLDER> @$ set one . if it does @$ validate it .,if ( cookie . get path ( ) == null ) { cookie . set path ( path to cookie path ( uri . get path ( ) ) ) ; } else if ( ! http cookie . path matches ( cookie @$ uri ) ) { continue ; },cookie have
renamed failed so lets do some checks rename the <PLACE_HOLDER> back to the original file new file doesnt exist,if ( ! rename result ) { if ( ! new file . exists ( ) ) { logger . warning ( error message . general_write_failed_new_file_doesnt_exist . get msg ( new file . get absolute path ( ) ) ) ; } if ( ! original file backup . rename to ( af . get file ( ) ) ) { logger . warning ( error message . general_write_failed_to_rename_original_backup_to_original . get msg ( original file backup . get absolute path ( ) @$ af . get file ( ) . get name ( ) ) ) ; } logger . warning ( error message . general_write_failed_to_rename_to_original_file . get msg ( af . get file ( ) . get absolute path ( ) @$ new file . get name,checks rename
set <PLACE_HOLDER> may wrap the given <PLACE_HOLDER> with spanned string . check the contents by casting to string .,try { tv . get text ( ) ; fail ( ) ; } catch ( illegal argument exception e ) { },text wrap
if we are using the bean item container or another container which stores the <PLACE_HOLDER> as ids then just return the instances,if ( id instanceof calendar event ) { event = ( calendar event ) id ; } else { basic event basic event = new container calendar event ( index ) ; if ( caption property != null && item . get item property ids ( ) . contains ( caption property ) ) { basic event . set caption ( string . value of ( item . get item property ( caption property ) . get value ( ) ) ) ; } if ( description property != null && item . get item property ids ( ) . contains ( description property ) ) { basic event . set description ( string . value of ( item . get item property ( description property ) .,which stores
if we have processed all connected vertices and there are edges remaining @$ graph has multiple connected <PLACE_HOLDER> .,if ( edges . size ( ) > __num__ ) return null ; return sorted ;,graph has
if there is a channel @$ then setting the socket <PLACE_HOLDER> should not matter . if there is not a channel @$ it will take effect .,s . set so timeout ( __num__ ) ; if ( with channel ) { assert read timeout ( stm @$ __num__ ) ; } else { assert read timeout ( stm @$ __num__ ) ; } io utils . close stream ( stm ) ; io utils . close socket ( s ) ; ss . close ( ) ;,matter setting
memoized should always return the same <PLACE_HOLDER>,assert same ( class nodes @$ supplier . get ( ) ) ;,memoized return
vertical layout and a child defines <PLACE_HOLDER>,return ol instanceof vertical layout && has non relative width component ( ol ) ;,layout defines
note to translators : the option name and the parameter name do not need to be translated . only translate the messages in parentheses . note also that leading whitespace in the messages is used to indent the usage information for each option in the english messages . do not translate the <PLACE_HOLDER> : xsltc @$ sax @$ dom and dtm .,return new object [ ] [ ] { { __str__ @$ __str__ } @$ { er_no_curlybrace @$ __str__ } @$ { er_illegal_attribute @$ __str__ } @$ { er_null_sourcenode_applyimports @$ __str__ } @$ { er_cannot_add @$ __str__ } @$ { er_null_sourcenode_handleapplytemplates @$ __str__ } @$ { er_no_name_attrib @$ __str__ } @$ { er_template_not_found @$ __str__ } @$ { er_cant_resolve_name_avt @$ __str__ } @$ { er_requires_attrib @$ __str__ } @$ { er_must_have_test_attrib @$ __str__ } @$ { er_bad_val_on_level_attrib @$ __str__ } @$ { er_processinginstruction_name_cant_be_xml @$ __str__ } @$ { er_processinginstruction_notvalid_ncname @$ __str__ } @$ { er_need_match_attrib @$ __str__ } @$ { er_need_name_or_match_attrib @$ __str__ } @$ { er_cant_resolve_nsprefix @$ __str__ } @$ { er_illegal_value @$ __str__ } @$ { er_no_ownerdoc @$ __str__ } @$ { er_elemtemplateelem_err @$ __str__ } @$,note translate
verify the async event listener has received the substituted <PLACE_HOLDER>,my async event listener listener = ( my async event listener ) queue . get async event listener ( ) ; final map events map = listener . get events map ( ) ; assert not null ( events map ) ; assert equals ( expected num invocations @$ events map . size ( ) ) ; for ( iterator i = events map . entry set ( ) . iterator ( ) ; i . has next ( ) ; ) { map . entry < integer @$ string > entry = ( map . entry < integer @$ string > ) i . next ( ) ; assert equals ( my gateway event substitution filter . substitution_prefix + entry . get key ( ) @$ entry,listener received
if the op <PLACE_HOLDER> is null then the implementation does n't offer a typing.n operation <PLACE_HOLDER> which is unacceptable for .,if ( op set typing notifs2 == null ) { throw new null pointer exception ( __str__ ) ; } op set basicim2 = ( operation set basic instant messaging ) supported operation sets2 . get ( operation set basic instant messaging . class . get name ( ) ) ; if ( op set basicim2 == null ) { throw new null pointer exception ( __str__ ) ; },implementation offer
remove any array models which lack <PLACE_HOLDER> and enums,if ( cm . is array model && ! model property . is enum && ! model property . has validation ) { model schemas to remove . put ( cm . name @$ model schema ) ; },which lack
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default media snippet local profile = new crawl profile ( crawl_profile_snippet_local_media @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_snippet_local_media_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ true @$ false @$ false @$ false @$ true @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . ifexist @$ __str__ + crawl_profile_snippet_local_media @$ client identification . yacy intranet crawler agent name @$ null,ip match
poor man 's dependency <PLACE_HOLDER> through the jersey application scope .,m block master = ( ( alluxio master process ) context . get attribute ( master web server . alluxio_master_servlet_resource_key ) ) . get master ( block master . class ) ;,man dependency
if name matches any type in permitted @$ and name does not match or narrow some permitted <PLACE_HOLDER> @$ return false,switch ( per name . constrains ( name ) ) { case general name interface . name_diff_type : continue ; case general name interface . name_widens : case general name interface . name_same_type : same type = true ; continue ; case general name interface . name_match : case general name interface . name_narrows : return true ; },name match
because an unresolved address always has a host <PLACE_HOLDER> .,name resolver . resolve ( unresolved address . get host name ( ) ) . add listener ( new future listener < inet address > ( ) { @ override public void operation complete ( future < inet address > future ) throws exception { if ( future . is success ( ) ) { promise . set success ( new inet socket address ( future . get now ( ) @$ unresolved address . get port ( ) ) ) ; } else { promise . set failure ( future . cause ( ) ) ; } } } ) ;,address has
let the dom organize <PLACE_HOLDER>,thread . sleep ( __num__ ) ;,dom organize
another bookmark @$ then remove the old <PLACE_HOLDER> .,if ( shortcut != __num__ ) { cr . delete ( content_uri @$ s shortcut selection @$ new string [ ] { string . value of ( ( int ) shortcut ) } ) ; } content values values = new content values ( ) ; if ( title != null ) values . put ( title @$ title ) ; if ( folder != null ) values . put ( folder @$ folder ) ; values . put ( intent @$ intent . to uri ( __num__ ) ) ; if ( shortcut != __num__ ) values . put ( shortcut @$ ( int ) shortcut ) ; values . put ( ordering @$ ordering ) ; return cr . insert ( content_uri @$ values ) ;,bookmark remove
log changes in upstream network signal <PLACE_HOLDER> @$ if available .,if ( network . equals ( m tethering upstream network ) && new nc . has signal strength ( ) ) { final int new signal = new nc . get signal strength ( ) ; final string prev signal = get signal strength ( prev . network capabilities ) ; m log . logf ( __str__ @$ prev signal @$ new signal ) ; } m network map . put ( network @$ new network state ( null @$ prev . link properties @$ new nc @$ network @$ null @$ null ) ) ;,changes signal
if the option allows multiple <PLACE_HOLDER> then we intentionally return the empty list as the default value of this option since it is not always the case that an option that allows multiple <PLACE_HOLDER> will have a converter that returns a list value .,if ( allows multiple ) { default value = collections . empty list ( ) ; } else { try { default value = converter . convert ( default value as string ) ; } catch ( options parsing exception e ) { throw new construction exception ( string . format ( __str__ @$ get field ( ) . get name ( ) @$ e . get message ( ) ) @$ e ) ; } },option allows
let the session deliver the <PLACE_HOLDER>,new thread ( ) { @ override public void run ( ) { session . run ( ) ; log . debug ( __str__ ) ; synchronized ( pool ) { try { log . debug ( __str__ ) ; session . commit ( ) ; log . debug ( __str__ ) ; } catch ( jms exception e ) { log . error ( __str__ @$ e ) ; } pool . server session in use = false ; } } } . start ( ) ;,session deliver
set transient state to true to simulate the view is running custom view property <PLACE_HOLDER> .,m recycler view . get child at ( __num__ ) . set has transient state ( true ) ;,state running
javac can put <PLACE_HOLDER> in whatever order it desires . hopper does it one way and mantis another .,test2 ( method @$ variables @$ __str__ @$ __str__ @$ __str__ ) ; test ( method @$ byname @$ __str__ @$ __str__ ) ; test ( method @$ arguments @$ __str__ @$ __str__ ) ;,javac put
to simplify @$ offer messages first offer all the necessary fragment <PLACE_HOLDER> to satisfy deps just be lazy and perturb the buddy response here,plan . generated responses . get ( plan . generated responses . size ( ) - __num__ ) . set status ( fragment response message . unexpected_error @$ new ee exception ( __num__ ) ) ; for ( fragment response message msg : plan . generated responses ) { system . out . println ( __str__ + msg ) ; dut . offer received fragment response ( msg ) ; },messages offer
clear interrupt flag if execute called <PLACE_HOLDER> .,interrupted ( ) ;,flag called
for simplicity add <PLACE_HOLDER> into a block @$ before adding it to the ast .,node block = ir . block ( ) ;,simplicity add
always use uncustomized version for editing . it helps caching and customized lambda forms reuse transform cache <PLACE_HOLDER> to keep a link to uncustomized version .,return new lambda form editor ( lambda form . uncustomize ( ) ) ;,forms transform
dest <PLACE_HOLDER> for node 1 @$ i.e . file <PLACE_HOLDER> on node 1,list < string > dst list1 = get dest file list ( info list1 @$ store name @$ __num__ ) ;,list file
endpoint should have three different <PLACE_HOLDER> in the end order of the <PLACE_HOLDER> is not important,endpoint . expected message count ( __num__ ) ; endpoint . assert is satisfied ( ) ;,endpoint have
if we received invalid end offset <PLACE_HOLDER> @$ we clear the known offset to refetch the last committed offset from metadata . if any end offset <PLACE_HOLDER> are invalid @$ we treat the entire set as invalid as a safety measure .,boolean end offsets are invalid = false ; for ( entry < partition id type @$ sequence offset type > entry : publishing task end offsets . entry set ( ) ) { partition id type partition = entry . get key ( ) ; sequence offset type sequence = entry . get value ( ) ; if ( sequence . equals ( get end of partition marker ( ) ) ) { log . info ( __str__ @$ task id @$ partition ) ; end offsets are invalid = true ; partition offsets . put ( partition @$ get not set marker ( ) ) ; } } if ( ! end offsets are invalid ) { for ( entry < partition id type @$ sequence offset,committed offset
s means <PLACE_HOLDER>,if ( c == __str__ ) { result . append ( __str__ ) ; } else { result . append ( c ) ; },s means
assert that the input script only contains 3 <PLACE_HOLDER>,assert true ( input script . get chunks ( ) . size ( ) == __num__ ) ;,script contains
component 4 is try block map <PLACE_HOLDER> or displacement .,return eh data type utilities . get component address ( get data type ( ) @$ try_block_map_ordinal @$ get mem buffer ( ) ) ;,component try
otherwise tests of deprecated code generate nuisance warnings . do n't report <PLACE_HOLDER> if the current target is also deprecated @$ or if the current context is evaluating an aspect @$ as the base target would have already printed the <PLACE_HOLDER> warnings .,return ( ! for aspect && prerequisite deprecation != null && ! is same logical package ( this package @$ prerequisite package ) && this deprecation == null ) ;,aspect report
filter will decide if the requester wants <PLACE_HOLDER> or pull requests,collection all jobs matchin filter = container filter . filter ( pipeline . mbp . get all jobs ( ) ) ;,requester wants
the method should have caught the <PLACE_HOLDER> and reported it to the logger .,final list < log message > log messages = runner . get logger ( ) . get error messages ( ) ; assert false ( log messages . is empty ( ) ) ; assert equals ( __num__ @$ log messages . size ( ) ) ;,method caught
this implementation is based on that of java.util.logging.file handler @$ which <PLACE_HOLDER> in a synchronized context like this . unfortunately the maximum throughput for generating log entries will be the inverse of the flush latency . that could be as little as one hundred log entries per second on some systems . for higher throughput this should be changed to batch publish operations while,flush ( ) ;,throughput inverse
compatibility with rfc 1779 : semicolon can separate rd ns empty attribute <PLACE_HOLDER>,break ; default : att value = escapedav ( ) ;,rd ns
load read the encoding <PLACE_HOLDER> from the output stream,return block encoding . read block ( this @$ input ) ;,load read
check to ensure mesh has <PLACE_HOLDER> and normals before generating,if ( mesh . get buffer ( type . tex coord ) != null && mesh . get buffer ( type . normal ) != null ) { generate ( geom . get mesh ( ) @$ true @$ split mirrored ) ; },mesh has
use local <PLACE_HOLDER> for embedded spark <PLACE_HOLDER> when spark.master is not found,conf . set if missing ( __str__ @$ __str__ ) ; this . inner interpreter = load spark scala interpreter ( conf ) ; this . inner interpreter . open ( ) ; sc = this . inner interpreter . get spark context ( ) ; jsc = java spark context . from spark context ( sc ) ; spark version = spark version . from version string ( sc . version ( ) ) ; if ( enable supported version check && spark version . is unsupported version ( ) ) { throw new exception ( __str__ + spark version + __str__ + __str__ ) ; } sql context = this . inner interpreter . get sql context ( ) ; spark session = this . inner,mode spark
output call <PLACE_HOLDER>,if ( instr . get flow type ( ) . is call ( ) ) { address call addr = get call address ( instr ) ; if ( call addr != null ) { s = symbol table . get primary symbol ( call addr ) ; if ( s != null ) { buf . append ( __str__ ) ; buf . append ( s . get name ( ) ) ; buf . append ( __str__ ) ; } } },output call
ensure a timer throws an illegal state <PLACE_HOLDER> after cancelled,try { t = new timer ( ) ; timer test task test task = new timer test task ( ) ; t . cancel ( ) ; try { t . schedule ( test task @$ __num__ ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } t = new timer ( ) ; test task = new timer test task ( ) ; test task . cancel ( ) ; try { t . schedule ( test task @$ __num__ ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } t . cancel ( ) ; t = new timer ( ) ; test task = new timer test task ( ),timer throws
this should probably be a java.text.parse exception @$ but illegal argument exception is <PLACE_HOLDER> android 's idn expects .,if ( m - n > ( integer . max_value - delta ) / ( h + __num__ ) ) { throw new illegal argument exception ( __str__ ) ; },idn expects
there is no support for a native boolean type that accepts values of true @$ false or unknown . using the tinyint type requires <PLACE_HOLDER> of true and false .,get default properties ( ) . set property ( environment . query_substitutions @$ __str__ ) ;,type requires
the vorbis decoder expects the <PLACE_HOLDER> of samples in the packet to be appended to the audio data as an int 32,buffer . data [ buffer . limit ( ) - __num__ ] = ( byte ) ( packet sample count & __num__ ) ; buffer . data [ buffer . limit ( ) - __num__ ] = ( byte ) ( ( packet sample count > > > __num__ ) & __num__ ) ; buffer . data [ buffer . limit ( ) - __num__ ] = ( byte ) ( ( packet sample count > > > __num__ ) & __num__ ) ; buffer . data [ buffer . limit ( ) - __num__ ] = ( byte ) ( ( packet sample count > > > __num__ ) & __num__ ) ;,decoder expects
t<PLACE_HOLDER>t that submit do<PLACE_HOLDER> n't throw np <PLACE_HOLDER>,executor service . submit ( new test event handler ( mocked server @$ event type . m_server_shutdown @$ lock @$ counter ) ) ;,submit throw
need to catch this @$ as windows does not support the posix <PLACE_HOLDER> . this is not an error @$ however @$ and should just silently fail .,files . set posix file permissions ( file . to path ( ) @$ perms ) ;,windows support
both listeners should fire and not cause the <PLACE_HOLDER> not to fire .,assert . assert equals ( __num__ @$ io exception on online listener . on online count ) ; assert . assert equals ( __num__ @$ runtime exception on online listener . on online count ) ;,listeners fire
the following should have the same <PLACE_HOLDER> as the previous @$ since it has the same restrictions applied in reverse order .,s = open session ( ) ; s . get transaction ( ) . begin ( ) ; root criteria = s . create criteria ( order . class @$ __str__ ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ;,following have
the list of relational <PLACE_HOLDER> should contain 2 or more <PLACE_HOLDER> : the first represents the discriminator the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ plural attribute source . get attribute role ( ) . get full path ( ) ) @$ mapping document . get origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source discriminator type source = new hibernate type source impl ( jaxb many to any mapping . get meta type ( ) ) ; private final relational value source discriminator relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > discriminator value mapping = new hash,list contain
return the first call to a function which takes <PLACE_HOLDER> as an argument,for ( reference ref : collection utils . as iterable ( ref iter ) ) { if ( ! func . get body ( ) . contains ( ref . get from address ( ) ) ) { continue ; } if ( is valid call reference ( ref ) ) { function called func = current program . get function manager ( ) . get function at ( ref . get to address ( ) ) ; parameter [ ] params = called func . get parameters ( ) ; for ( parameter param : params ) { address addr = param . get min address ( ) ; if ( addr != null && addr . equals ( variable addr ) ) { return ref .,which takes
fake a replay finished <PLACE_HOLDER> for site 1,rejoin message msg1 = new rejoin message ( __num__ @$ rejoin message . type . replay_finished ) ; m_coordinator . deliver ( msg1 ) ;,replay finished
binding view parts to view <PLACE_HOLDER>,view holder . price = cell . find view by id ( r . id . title_price ) ; view holder . time = cell . find view by id ( r . id . title_time_label ) ; view holder . date = cell . find view by id ( r . id . title_date_label ) ; view holder . from address = cell . find view by id ( r . id . title_from_address ) ; view holder . to address = cell . find view by id ( r . id . title_to_address ) ; view holder . requests count = cell . find view by id ( r . id . title_requests_count ) ; view holder . pledge price = cell . find view by,parts view
serialized output should still contain the v.2 <PLACE_HOLDER>,byte [ ] v1 bytes = v1 adapter . encode ( v1 ) ;,output contain
use the selected versions field to embed the original build target <PLACE_HOLDER> . we 'll use this to verify the correct node was recovered from the target graph .,target node builder . set selected versions ( immutable map . of ( build target @$ version . of ( __str__ ) ) ) ; for ( target node < ? > dep : deps ) { target node builder . add dep ( dep . get build target ( ) ) ; } return target node builder . build ( ) ;,original build
store the found <PLACE_HOLDER> back into the intent @$ because now that we have it we never want to do this again . for example @$ if the user navigates back to this point in the history @$ we should always restart the exact same activity .,if ( a info != null ) { intent . set component ( new component name ( a info . application info . package name @$ a info . name ) ) ; if ( ! a info . process name . equals ( __str__ ) ) { if ( ( start flags & ( start_flag_debug | start_flag_native_debugging | start_flag_track_allocation ) ) != __num__ || profiler info != null ) { synchronized ( m service . m global lock ) { final message msg = pooled lambda . obtain message ( activity manager internal :: set debug flags for starting activity @$ m service . m am internal @$ a info @$ start flags @$ profiler info @$ m service . m global lock ) ; m service,the found
client sends an ack rds <PLACE_HOLDER> .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_rds @$ __str__ ) ) ) ; verify ( config watcher @$ never ( ) ) . on config changed ( any ( config update . class ) ) ; verify ( config watcher @$ never ( ) ) . on error ( any ( status . class ) ) ;,client sends
trailers always end the <PLACE_HOLDER> even if not explicitly set .,end of stream = true ; log builder . request trailers ( trailers ) ; log builder . increase request length ( ( http data ) o ) ;,trailers end
enum class needed <PLACE_HOLDER> for jackson 's json creator,if ( additional properties . contains key ( serialization_library_jackson ) ) { model . imports . add ( __str__ ) ; model . imports . add ( __str__ ) ; },class needed
0 x 1002 cfc : p 1 and p 2 have mem <PLACE_HOLDER> @$ but p 1 has offset .,program builder1 . create offset mem reference ( __str__ @$ __str__ @$ __num__ @$ ref type . read @$ source type . user_defined @$ __num__ ) ; program builder2 . create memory reference ( __str__ @$ __str__ @$ ref type . read @$ source type . user_defined @$ __num__ ) ; program diff = new program diff ( p1 @$ p2 ) ; address set as = new address set ( addr ( __num__ ) @$ addr ( __num__ ) ) ; program diff . set filter ( new program diff filter ( program diff filter . reference_diffs ) ) ; assert equals ( as @$ program diff . get differences ( program diff . get filter ( ) @$ null ) ) ;,0 have
this is the case when we 've encountered a decimal separator . the fractional part will not change the <PLACE_HOLDER> @$ but we will verify that the fractional part is well formed .,while ( offset < end ) { int digit = lazy utils . digit ( bytes [ offset ++ ] @$ radix ) ; if ( digit == - __num__ ) { throw new number format exception ( lazy utils . convert to string ( bytes @$ start @$ length ) ) ; } } if ( ! negative ) { result = - result ; if ( result < __num__ ) { throw new number format exception ( lazy utils . convert to string ( bytes @$ start @$ length ) ) ; } } return result ;,part change
this is a user who already configured the <PLACE_HOLDER>,if ( get polling thread count ( ) != threads_default ) { return true ; },who configured
it is not guaranteed that the source record will have a <PLACE_HOLDER> associated with it . for @$ example @$ if this method is being called for processing a pending activity launch @$ it is possible that the activity has been removed from the <PLACE_HOLDER> after the launch was enqueued .,final task record source task = m source record . get task record ( ) ; m new task intent = source task != null ? source task . intent : null ;,record have
the execution attempt id should make no <PLACE_HOLDER> in this case,final function < intermediate result partitionid @$ result partitionid > partitionid mapper = intermediate result partitionid -> new result partitionid ( intermediate result partitionid @$ new execution attemptid ( ) ) ; final result partition availability checker result partition availability checker = new execution graph result partition availability checker ( partitionid mapper @$ partition tracker ) ; for ( intermediate result partitionid irpid : expected availability . key set ( ) ) { assert equals ( expected availability . get ( irpid ) @$ result partition availability checker . is available ( irpid ) ) ; },id make
a volt db <PLACE_HOLDER> to avoid using exceptions for flow control .,if ( parse list . length == __num__ ) { return new expression or exception ( function ) ; },volt db
if we cant parse the advertised address we act as if it has no port specified if invalid hostname config will report the <PLACE_HOLDER>,if ( ! advertised already has port ) { try { int port = socket_address . parse ( listen value ) . get port ( ) ; if ( port >= __num__ ) { socket address new advertised = new socket address ( advertised value @$ port ) ; log . warn ( __str__ @$ port @$ listen address @$ advertised address ) ; default values . put ( advertised address @$ new advertised . to string ( ) ) ; } } catch ( runtime exception e ) { } },config report
the server does n't compress the <PLACE_HOLDER> in the first place @$ so the client can read the <PLACE_HOLDER> .,client . send request ( find request @$ request context ) . get response ( ) ;,server compress
rather than fight it @$ let root have an <PLACE_HOLDER>,nodes . put ( __str__ @$ root ) ; nodes . put without digest ( root zookeeper @$ root ) ; root . add child ( proc child zookeeper ) ; nodes . put ( proc zookeeper @$ proc data node ) ; proc data node . add child ( quota child zookeeper ) ; nodes . put ( quota zookeeper @$ quota data node ) ; add config node ( ) ; node data size . set ( approximate data size ( ) ) ; try { data watches = watch manager factory . create watch manager ( ) ; child watches = watch manager factory . create watch manager ( ) ; } catch ( exception e ) { log . error ( __str__ @$ e,root have
noinspection object <PLACE_HOLDER> in loop,data cache . add ( new o raw pair < > ( entry . key @$ entry . value . get value ( ) ) ) ;,noinspection object
no local class for this descriptor @$ skip over the data for this class . like default read object with a null current object . the code will read the <PLACE_HOLDER> but discard them .,object stream field [ ] fields = current class desc . get fields no copy ( ) ; if ( fields . length > __num__ ) { input class fields ( null @$ current class @$ fields @$ sender ) ; },default read
general strategy : figure out the lengths of the two ancestor chains @$ reconcile the lengths @$ and look for the lowest common ancestor . if that ancestor is one of the nodes being compared @$ it comes before the other . otherwise perform a sibling compare . <PLACE_HOLDER> : if no common ancestor is found @$ ordering is undefined and we return the,is node after = is node after sibling ( parent1 @$ node1 @$ node2 ) ;,strategy perform
streams should have no <PLACE_HOLDER> .,path src = get test root path ( f sys @$ __str__ ) ; fs data output stream out = f sys . create ( src ) ; out . write char ( __str__ ) ;,streams have
lookup does n't find <PLACE_HOLDER>,when ( security context . get user principal ( ) ) . then return ( simple principal . of ( __str__ ) ) ; when ( clientdao . get client ( __str__ ) ) . then return ( optional . empty ( ) ) ;,lookup find
at this point we 've entered the threshold zone so more keys wo n't immediately trigger more <PLACE_HOLDER> .,assert equals ( __num__ @$ listener keys . size ( ) ) ;,keys trigger
after opening the link remove the current <PLACE_HOLDER> to avoid clicking on the window to gain focus to open the link again,this . current href = __str__ ;,link remove
these two objects should not equal each <PLACE_HOLDER>,if ( o1 . equals ( o2 ) ) { throw new assertion error ( string . format ( __str__ @$ o1 @$ o2 ) ) ; } if ( o2 . equals ( o1 ) ) { throw new assertion error ( string . format ( __str__ @$ o2 @$ o1 ) ) ; } verify hash code consistency ( o1 @$ o2 ) ;,objects equal
we do n't have the last output perform a shallow <PLACE_HOLDER>,if ( it . has next ( ) ) { stream record < out > shallow copy = record . copy ( record . get value ( ) ) ; out . collect ( shallow copy ) ; } else { out . collect ( record ) ; break ; },output perform
client worker require verified <PLACE_HOLDER> .,msg . verify ( get local node id ( ) ) ;,worker require
determine which hive <PLACE_HOLDER> we will read,list < hive column handle > read columns = columns . stream ( ) . filter ( column -> column . get column type ( ) == regular ) . collect ( to immutable list ( ) ) ; list < integer > read hive column indexes = read columns . stream ( ) . map ( hive column handle :: get hive column index ) . collect ( to immutable list ( ) ) ;,which hive
error on ri ri throw array <PLACE_HOLDER> out of bounds exception,assert equals ( __str__ @$ f . to string ( ) ) ;,error throw
modifying the ac ls of root directory of the snapshot should refer new acl <PLACE_HOLDER> . and old acl <PLACE_HOLDER> should be referenced by snapshot,hdfs . modify acl entries ( path @$ acl spec ) ; path snapshot path = snapshot test helper . create snapshot ( hdfs @$ path @$ snapshot name ) ; acl feature snapshot acl = fs acl base test . get acl feature ( snapshot path @$ cluster ) ; acl feature = fs acl base test . get acl feature ( path @$ cluster ) ; assert equals ( __str__ @$ __num__ @$ acl feature . get ref count ( ) ) ; list < acl entry > new acl = lists . new array list ( acl entry ( access @$ user @$ __str__ @$ all ) ) ; hdfs . modify acl entries ( path @$ new acl ) ; acl feature = fs,ls refer
construct a new model with the intended architecture and print summary note : this architecture is constructed with the primary intent of demonstrating use of the transfer learning api @$ secondary to what might give better <PLACE_HOLDER>,computation graph vgg16 transfer = new transfer learning . graph builder ( vgg16 ) . fine tune configuration ( fine tune conf ) . set feature extractor ( feature extraction layer ) . n out replace ( __str__ @$ __num__ @$ weight init . xavier ) . remove vertex and connections ( __str__ ) . add layer ( __str__ @$ new dense layer . builder ( ) . activation ( activation . tanh ) . n in ( __num__ ) . n out ( __num__ ) . build ( ) @$ __str__ ) . add layer ( __str__ @$ new output layer . builder ( loss functions . loss function . negativeloglikelihood ) . activation ( activation . softmax ) . n in ( __num__ ) . n,secondary give
clear to let gc do its <PLACE_HOLDER>,element data [ -- size ] = null ;,gc do
if the proxied object implements the wrapper <PLACE_HOLDER> @$ then return the result of it 's is wrapper for method .,if ( wrapper . class . is assignable from ( delegate . get class ( ) ) ) { return ( ( wrapper ) unwrapp6 spy proxy ( ) ) . is wrapper for ( iface ) ; },object implements
service type method automatic recovery deliberately disabled as spring has it 's own recovery <PLACE_HOLDER>,class < ? > channeln class = class . for name ( __str__ ) ; method channeln basic publish = channeln class . get declared method ( __str__ @$ string . class @$ string . class @$ boolean . class @$ boolean . class @$ amqp . basic properties . class @$ byte [ ] . class ) ; expected trace channeln basic publish trace = expectations . event ( rabbitmq test constants . rabbitmq_client @$ channeln basic publish @$ null @$ remote address @$ __str__ + rabbitmq test constants . exchange @$ expectations . annotation ( __str__ @$ rabbitmq test constants . exchange ) @$ expectations . annotation ( __str__ @$ rabbitmq test constants . routing_key_pull ) ) ;,recovery has
what we really want to do here is use a versioned <PLACE_HOLDER> @$ however @$ this will suffice for now .,if ( default_version . equals ( get version ( ) ) ) { return get short name ( ) ; } else { return get version ( ) . to string ( ) ; },here use
attribute change enables capability <PLACE_HOLDER>,if ( ! registered && should register ) { context . register capability ( capability . resolve ( address ) ) ; } else if ( registered && ! should register ) { context . deregister capability ( capability . resolve ( address ) . get name ( ) ) ; },change enables
test that inverting a transformed <PLACE_HOLDER> gives the original <PLACE_HOLDER> .,region placement maintainer . randomized matrix rm = new region placement maintainer . randomized matrix ( rows @$ cols ) ; float [ ] [ ] transformed = rm . transform ( matrix ) ; float [ ] [ ] inverted transformed = rm . invert ( transformed ) ; for ( int i = __num__ ; i < rows ; i ++ ) { for ( int j = __num__ ; j < cols ; j ++ ) { if ( matrix [ i ] [ j ] != inverted transformed [ i ] [ j ] ) { throw new runtime exception ( ) ; } } },test gives
this means the submenu was opened from an overflow menu item @$ indicating the menu popup helper will handle <PLACE_HOLDER> the submenu via its menu popup . return false to ensure that the menu popup acts as presenter for the submenu @$ and acts on its responsibility to display the new submenu .,if ( anchor == null ) { return false ; },submenu handle
wsaw wsdl binding case will have some <PLACE_HOLDER> set on wbo,return wbo != null ? wbo . get anonymous ( ) : wsdl bound operation . anonymous . optional ;,case have
if parent provides a <PLACE_HOLDER> @$ do n't measure unlimited .,m layout state . m infinite = resolve is infinite ( ) ; m layout state . m layout direction = layout direction ; m reusable int pair [ __num__ ] = __num__ ; m reusable int pair [ __num__ ] = __num__ ; calculate extra layout space ( state @$ m reusable int pair ) ; int extra for start = math . max ( __num__ @$ m reusable int pair [ __num__ ] ) ; int extra for end = math . max ( __num__ @$ m reusable int pair [ __num__ ] ) ; boolean layout to end = layout direction == layout state . layout_end ; m layout state . m extra fill space = layout to end ? extra for end : extra,parent provides
configure the cluster and commit the configuration as we know the successful response indicates <PLACE_HOLDER> .,configure ( configuration ) . commit ( ) ; future . complete ( null ) ;,response indicates
without the metadata the status and health check ur ls will not be set and the status page and health check url <PLACE_HOLDER>s will not include the context <PLACE_HOLDER> so set them here,if ( string utils . has text ( management context path ) ) { instance . set health check url path ( management context path + instance . get health check url path ( ) ) ; instance . set status page url path ( management context path + instance . get status page url path ( ) ) ; },page include
note to translators : an element in the stylesheet requires a particular <PLACE_HOLDER> named by the substitution text @$ but that <PLACE_HOLDER> was not specified in the stylesheet .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note requires
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default autocrawl shallow profile = new crawl profile ( crawl_profile_autocrawl_shallow @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ integer . parse int ( sb . get config ( switchboard constants . autocrawl_shallow_depth @$ __str__ ) ) @$ true @$ crawl profile . get recrawl date ( integer . parse int ( sb . get config ( switchboard constants . autocrawl_days @$ __str__ ) ) * __num__ ) @$ - __num__ @$ true @$ true @$ true @$ false @$ sb . get config,url match
only continue if the <PLACE_HOLDER> on this key declaration matches the <PLACE_HOLDER> on the iterator for this walker .,if ( ! kd . get name ( ) . equals ( name ) ) continue ; found key = true ;,name matches
shorter value to make <PLACE_HOLDER> faster,viewer . set popup delay ( __num__ ) ;,value make
foo impl was implicitly bound @$ it is an error to call get instance or get <PLACE_HOLDER> @$ it is ok to call get binding for introspection @$ but an error to get the <PLACE_HOLDER> of the binding,ensure fails ( injector @$ allow_binding @$ foo impl . class ) ;,error get
response account <PLACE_HOLDER>,m ams . get account by type and features ( m mock account manager response @$ null @$ account manager service test fixtures . account_features @$ __str__ ) ;,response account
authentication ok : ' r ' | int 32 len | int 32 <PLACE_HOLDER>,assert that ( response bytes @$ is ( new byte [ ] { __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ) ) ;,| int
old pur stored <PLACE_HOLDER> with no nr when not specified,if ( nr == __num__ && node . has property ( code ) ) { return node . get property ( code ) . get long ( ) ; } else { return __num__ ; },pur stored
inject our empty <PLACE_HOLDER> into the replication queue @$ and then roll the original <PLACE_HOLDER> @$ which enqueues a new <PLACE_HOLDER> behind our empty <PLACE_HOLDER> . we must roll the <PLACE_HOLDER> here as now we use the <PLACE_HOLDER> to determine if the file being replicated currently is still opened for write @$ so just inject a new <PLACE_HOLDER> to the replication queue does not,for ( int i = __num__ ; i < num rs ; i ++ ) { h region server hrs = util1 . geth base cluster ( ) . get region server ( i ) ; replication replication service = ( replication ) hrs . get replication source service ( ) ; replication service . get replication manager ( ) . pre log roll ( empty wal paths . get ( i ) ) ; replication service . get replication manager ( ) . post log roll ( empty wal paths . get ( i ) ) ; region info region info = util1 . geth base cluster ( ) . get regions ( htable1 . get name ( ) ) . get ( __num__ ) . get,which enqueues
signatures <PLACE_HOLDER> jca does not expose via signature . we thus have to support this .,set keymaster purpose override ( keymaster defs . km_purpose_sign ) ; return true ;,jca expose
check failure for partitioned table where where clause can not infer <PLACE_HOLDER>,verify stmt fails ( client @$ __str__ @$ __str__ + __str__ + __str__ + __str__ ) ;,clause infer
expr index replace missing <PLACE_HOLDER> with replace missing <PLACE_HOLDER>,byte [ ] expected cache key = new byte [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ } ;,index replace
it is an error if document has dom l 1 <PLACE_HOLDER> .,if ( attr . get local name ( ) == null ) { if ( f namespace validation ) { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { attr . get node name ( ) } ) ; reportdom error ( f error handler @$ f error @$ f locator @$ msg @$ dom error . severity_fatal_error @$ __str__ ) ; } else { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { attr . get node name ( ) } ) ; reportdom error ( f error handler @$ f error @$ f locator @$ msg @$ dom,document dom
all the stripes are expired @$ so the compactor will not create any <PLACE_HOLDER> . we need to create an empty file to preserve metadata,stripe compactor sc = create compactor ( ) ; list < path > paths = scr . execute ( sc @$ no limit throughput controller . instance @$ null ) ; assert equals ( __num__ @$ paths . size ( ) ) ;,compactor create
print requested <PLACE_HOLDER> @$ unless verbose was selected,if ( verbose ) { zkdu . print size map ( ) ; } else { zkdu . print size map depth ( depth ) ; } zkdu . close ( ) ;,print requested
stop emitting at a certain point @$ because log rolling breaks the <PLACE_HOLDER> .,if ( current num >= testable topology . max_spout_emits ) { return ; },rolling breaks
now set the partitioning <PLACE_HOLDER> for the slave step ... for the slave step @$ we only should those partition i ds that are interesting for the current slave ...,step partitioning meta = target step partitioning meta . clone ( ) ; partition schema = step partitioning meta . get partition schema ( ) ; partition schema . set name ( create slave partition schema name ( partition schema . get name ( ) ) ) ; if ( partition schema . is dynamically defined ( ) ) { partition schema . expand partitions dynamically ( cluster schema . find nr slaves ( ) @$ original transformation ) ; },now set
when an unrelated notification gets a new app <PLACE_HOLDER>,m controller . update notifications for app op ( app ops manager . op_camera @$ __num__ @$ __str__ @$ true ) ;,notification gets
default to number class in exception details @$ else use the specified number <PLACE_HOLDER> .,return cast to number ( object @$ number . class ) ;,default use
o verride the <PLACE_HOLDER> to return our mock protocol,try { mockito . do return ( mock protocol ) . when ( spy ) . get proxy ( mockito . < configuration > any ( ) @$ mockito . any int ( ) ) ; mockito . do return ( mock zkfc protocol ) . when ( spy ) . getzkfc proxy ( mockito . < configuration > any ( ) @$ mockito . any int ( ) ) ; } catch ( io exception e ) { throw new assertion error ( e ) ; },o verride
bg <PLACE_HOLDER> state future finished @$ let the ui thread complete <PLACE_HOLDER> . countdown happens here if test is successful . otherwise it will happen when the bg thread computes <PLACE_HOLDER> @$ which means ui thread still gets unblocked but assertion will fail .,unlockui thread layout . count down ( ) ;,thread computes
no need to write anything on the client side @$ it will just confuse the <PLACE_HOLDER> .,client out = byte buffer . wrap ( __str__ . get bytes ( ) ) ;,need confuse
we have to call default read <PLACE_HOLDER> first .,in . default read object ( ) ;,default read
this is the response for the first page . leave the <PLACE_HOLDER> in the cache so subsequent requests for the first page can return immediately .,if ( state tag . request position == null ) { should remove = false ; } else { should remove = true ; },page leave
now remove the bytes we just processed . if @$ for some reason @$ the bytes array does not contain at least as many bytes as the instruction @$ then there 's a problem with the input . just print a <PLACE_HOLDER> to the user and exit .,if ( all bytes . size ( ) < instruction . get length ( ) ) { msg panel . set message text ( __str__ @$ color . red ) ; return ; } all bytes . sub list ( __num__ @$ instruction . get length ( ) ) . clear ( ) ;,now print
not seen yet ; must add an entry @$ return it . for that @$ we need <PLACE_HOLDER>,object id generator < ? > generator = null ; if ( _object id generators == null ) { _object id generators = new array list < object id generator < ? > > ( __num__ ) ; } else { for ( int i = __num__ @$ len = _object id generators . size ( ) ; i < len ; ++ i ) { object id generator < ? > gen = _object id generators . get ( i ) ; if ( gen . can use for ( generator type ) ) { generator = gen ; break ; } } } if ( generator == null ) { generator = generator type . new for serialization ( this ) ; _object id generators .,not need
reverse the parameter order so that the queue keeps the oldest <PLACE_HOLDER>,return b . last accessed copy < a . last accessed copy ;,queue keeps
add component sparse long <PLACE_HOLDER>,matrix context slc mat = new matrix context ( ) ; slc mat . set name ( sparse_long_mat_comp ) ; slc mat . set row num ( __num__ ) ; slc mat . set col num ( fea num ) ; slc mat . set max col num in block ( fea num / __num__ ) ; slc mat . set row type ( row type . t_long_sparse_component ) ; angel client . add matrix ( slc mat ) ;,component sparse
now @$ the acquire<PLACE_HOLDER>s command does not see the <PLACE_HOLDER> :,acquired jobs = execute acquire jobs command ( ) ; assert equals ( __num__ @$ acquired jobs . size ( ) ) ;,command see
the host matches a <PLACE_HOLDER> ; use its key,hash alias = alias ; key alias = alias ;,host matches
so temp file need have a <PLACE_HOLDER> to compare .,file result parent = create and register temp file ( __str__ ) ; result dir = result parent . touri ( ) . to string ( ) ; result path = new file ( result parent @$ __str__ ) . get absolute path ( ) ;,file have
make sure user who do n't have read <PLACE_HOLDER> to file ca n't <PLACE_HOLDER> raw xattr .,try { dfs . getx attr ( path @$ raw1 ) ; fail ( __str__ ) ; } catch ( access control exception ace ) { },who read
try to verify whether the previous set data <PLACE_HOLDER> or not,if ( is retry ) { try { stat stat = new stat ( ) ; byte [ ] rev data = check zk ( ) . get data ( path @$ false @$ stat ) ; if ( bytes . compare to ( rev data @$ new data ) == __num__ ) { return stat ; } } catch ( keeper exception keeper exception ) { throw keeper exception ; } },previous set
<PLACE_HOLDER> any interfering variables with different <PLACE_HOLDER>s and any variables that can be safely coalesced wih the same <PLACE_HOLDER> .,graph coloring < var @$ void > coloring = new greedy graph coloring < > ( interference graph @$ coloring tie breaker ) ; coloring . color ( ) ; colorings . push ( coloring ) ;,color wih
this fragment causes each execution <PLACE_HOLDER> to create the files that will be written to during the snapshot,byte [ ] hashinator bytes = ( hashinator data != null ? hashinator data . m_ser data : null ) ; long hashinator version = ( hashinator data != null ? hashinator data . m_version : __num__ ) ; return create and execute sys proc plan ( sys proc fragment id . pf_create snapshot targets @$ sys proc fragment id . pf_create snapshot targets results @$ file path @$ file nonce @$ per partition txn ids @$ block @$ format . name ( ) @$ data @$ hashinator bytes @$ hashinator version @$ system . current time millis ( ) @$ path type ) ;,fragment causes
deploy a new version of the child process in which the user task has an updated <PLACE_HOLDER>,process engine . get repository service ( ) . create deployment ( ) . name ( __str__ ) . tenant id ( __str__ ) . add bpmn model ( __str__ @$ childv2 bpmn model ) . deploy ( ) ; runtime service . start process instance by key and tenant id ( __str__ @$ __str__ ) ; list < task > list = task service . create task query ( ) . list ( ) ; assert equals ( __str__ @$ __num__ @$ list . size ( ) ) ; task task = list . get ( __num__ ) ; assert equals ( __str__ @$ __str__ @$ task . get name ( ) ) ;,task has
no output stream . just return the <PLACE_HOLDER> of the serialized trie @$ in bytes .,if ( os == null ) { return length ; },output return
initialize a complete map reduce <PLACE_HOLDER>,try { expr node desc expr1 = new expr node column desc ( type info factory . string type info @$ f1 @$ __str__ @$ false ) ; expr node desc expr2 = new expr node column desc ( type info factory . string type info @$ f2 @$ __str__ @$ false ) ; expr node desc filter expr = type check proc factory . default expr processor . get func expr node desc ( __str__ @$ expr1 @$ expr2 ) ; filter desc filter ctx = new filter desc ( filter expr @$ false ) ; operator < filter desc > op = operator factory . get ( new compilation op context ( ) @$ filter desc . class ) ; op . set conf ( filter,map reduce
.split main body of client we use a lazy pirate strategy in the client . if there 's no reply within our timeout @$ we close the socket and try again . in binary star @$ it 's the client vote that decides <PLACE_HOLDER> server is primary ; the client must therefore try to connect to each server in turn :,if ( poller . pollin ( __num__ ) ) { string reply = client . recv str ( ) ; if ( integer . parse int ( reply ) == sequence ) { system . out . printf ( __str__ @$ reply ) ; expect reply = false ; thread . sleep ( __num__ ) ; } else system . out . printf ( __str__ @$ reply ) ; } else { system . out . printf ( __str__ ) ; poller . unregister ( client ) ; ctx . destroy socket ( client ) ; server nbr = ( server nbr + __num__ ) % __num__ ; thread . sleep ( settle_delay ) ; system . out . printf ( __str__ @$ server [ server nbr ],server primary
show only what the user required but make sure we show something and the spinners have higher <PLACE_HOLDER>,if ( ! spinners shown && ! calendar view shown ) { set spinners shown ( true ) ; } else { set spinners shown ( spinners shown ) ; set calendar view shown ( calendar view shown ) ; },something have
by default @$ we register a drop site that supports all dnd <PLACE_HOLDER> . this approach is not appropriate in plugin scenario if the browser supports motif dn d and does n't support x dn d. if we forcibly set xdnd aware on the browser toplevel @$ any drag source that supports both <PLACE_HOLDER> and prefers x dn d will be unable to drop,if ( ! supported protocols . is empty ( ) ) { drop target protocols = supported protocols . iterator ( ) ; } else { drop target protocols = x drag and drop protocols . get drop target protocols ( ) ; },browser supports
if the publisher 's region attributes do not support <PLACE_HOLDER> or if it is an ack region then do n't even bother with a <PLACE_HOLDER> key,if ( ! super . region allows conflation || get processor id ( ) != __num__ ) { return null ; } else { return new conflation key ( this . key @$ super . region path @$ false ) ; },attributes support
if all maps are assigned @$ then ramp up all reduces <PLACE_HOLDER> of the headroom,if ( scheduled maps == __num__ && num pending reduces > __num__ ) { log . info ( __str__ + __str__ + num pending reduces ) ; schedule all reduces ( ) ; return ; } float completed map percent = __num__ ; if ( total maps != __num__ ) { completed map percent = ( float ) completed maps / total maps ; } else { completed map percent = __num__ ; },ramp reduces
lock acquired @$ current thread owns this instance <PLACE_HOLDER>,try { switch ( state ) { case ready : return instance ; case injecting : return instance ; case validated : state = injectable reference state . injecting ; break ; case new : throw new illegal state exception ( __str__ ) ; default : throw new illegal state exception ( __str__ + state ) ; } try { members injector . inject and notify ( instance @$ key @$ provision callback @$ source @$ injector . options . stage == stage . tool ) ; } catch ( internal provision exception ipe ) { throw ipe . add source ( source ) ; } state = injectable reference state . ready ; return instance ; } finally { lock . unlock ( ) ; },thread owns
implementation could have started <PLACE_HOLDER> when <PLACE_HOLDER> was enabled @$ in which case size can be 0,if ( size < __num__ ) { throw new runtime exception ( __str__ + size ) ; } doit ( ) ;,implementation started
search left <PLACE_HOLDER> for first view,while ( left view offset > left border && left view position >= __num__ ) { if ( m is collapsed ) { left view offset -= ( m settings . get view width px ( ) + math . abs ( overlap distance ) ) ; } else { left view offset -= ( m settings . get view width px ( ) - math . abs ( overlap distance ) ) ; } left view position -- ; },search left
for root elements skip serializer <PLACE_HOLDER>,if ( ! is nested ) container . offset ++ ; if ( serializer . get current serializer ( ) . is serializing class name by default ( ) || is nested ) { read string ( container ) ; } int len = __num__ ; while ( len != __num__ ) { len = o var int serializer . read as integer ( container ) ; if ( len > __num__ ) { container . offset += len ; int pointer = read integer ( container ) ; pointer -= step size ; o integer serializer . instance . serialize literal ( pointer @$ container . bytes @$ container . offset - o integer serializer . int_size ) ; container . offset ++ ; } else if,elements skip
if the color has <PLACE_HOLDER> @$ increase the distinct color count,if ( hist [ color ] > __num__ ) { distinct color count ++ ; },color has
populate status data object <PLACE_HOLDER> .,snapshot result [ ] results = new snapshot result [ result row count ] ; for ( int i = __num__ ; i < result row count ; i ++ ) { assert true ( status results [ __num__ ] . advance row ( ) ) ; results [ i ] = new snapshot result ( ) ; results [ i ] . hostid = status results [ __num__ ] . get long ( __str__ ) ; results [ i ] . table = status results [ __num__ ] . get string ( __str__ ) ; results [ i ] . path = status results [ __num__ ] . get string ( __str__ ) ; results [ i ] . filename = status results [ __num__ ] .,data object
rare averaging improves <PLACE_HOLDER> @$ but might reduce model accuracy,parallel wrapper wrapper = new parallel wrapper . builder ( net ) . prefetch buffer ( __num__ ) . workers ( __num__ ) . averaging frequency ( __num__ ) . report score after averaging ( true ) . build ( ) ;,averaging improves
there are no regions @$ or no samples available . just scan the entire <PLACE_HOLDER> .,if ( sample row keys . is empty ( ) ) { log . info ( __str__ @$ this ) ; return collections . singleton list ( this ) ; } log . info ( __str__ @$ desired bundle size bytes @$ sample row keys . size ( ) @$ sample row keys . get ( __num__ ) ) ; immutable list . builder < bigtable source > splits = immutable list . builder ( ) ; for ( byte key range range : ranges ) { splits . add all ( split range based on samples ( desired bundle size bytes @$ sample row keys @$ range ) ) ; } return splits . build ( ) ;,samples scan
all of the above bundles can have loaders which do not contain a swift <PLACE_HOLDER> @$ so it must get bundled to ensure they run .,return true ; case framework : case dsym : return false ;,which contain
now @$ for each set of paths that have a common input format @$ build a <PLACE_HOLDER> of <PLACE_HOLDER>pers to the paths they 're used for,for ( path path : paths ) { class < ? extends mapper > mapper class = mapper map . get ( path ) ; if ( ! mapper paths . contains key ( mapper class ) ) { mapper paths . put ( mapper class @$ new linked list < path > ( ) ) ; } mapper paths . get ( mapper class ) . add ( path ) ; },now build
assuming side fields are preserving its <PLACE_HOLDER>,for ( string field : origin fields ) { if ( side fields . contains ( field ) ) { ret . add ( side values . get ( side idx ++ ) ) ; } else { ret . add ( state . group . get ( join fields . field index ( field ) ) ) ; } },fields preserving
it 's possible that the supplied buffer only has one <PLACE_HOLDER> in it ... and in that case we will get an underflow when attempting to read the short below .,if ( buffer . remaining ( ) < __num__ ) { carry = buffer . get ( ) ; break ; } else { size = buffer . get short ( ) ; },buffer has
internal or empty means internal <PLACE_HOLDER> @$ neither or them means sdcard <PLACE_HOLDER>,if ( ! __str__ . equals ignore case ( lite pal attr . get storage ( ) ) && ! text utils . is empty ( lite pal attr . get storage ( ) ) ) { string db path = environment . get external storage directory ( ) . get path ( ) + __str__ + lite pal attr . get storage ( ) ; db path = db path . replace ( __str__ @$ __str__ ) ; if ( base utility . is class and method exist ( __str__ @$ __str__ ) && context compat . check self permission ( lite pal application . get context ( ) @$ manifest . permission . write_external_storage ) != package manager . permission_granted ) { throw new database generate,means means
then the listeners receive separate <PLACE_HOLDER> of the bubble clock plugin .,argument captor < clock plugin > captor1 = argument captor . for class ( clock plugin . class ) ; argument captor < clock plugin > captor2 = argument captor . for class ( clock plugin . class ) ; verify ( m mock listener1 ) . on clock changed ( captor1 . capture ( ) ) ; verify ( m mock listener2 ) . on clock changed ( captor2 . capture ( ) ) ; assert that ( captor1 . get value ( ) ) . is instance of ( bubble_clock_class ) ; assert that ( captor2 . get value ( ) ) . is instance of ( bubble_clock_class ) ; assert that ( captor1 . get value ( ) ) . is not same as (,listeners receive
rightmost branch determines the <PLACE_HOLDER> .,return immutable list . of ( __str__ @$ __str__ ) ;,branch determines
the job should wait more than one <PLACE_HOLDER>,if ( cur priority > executable . get default priority ( ) + __num__ ) { add to job pool ( executable @$ cur priority ) ; } else { left job priorities . put ( executable . get id ( ) @$ cur priority + __num__ ) ; },job wait
if not enough room @$ grow the key <PLACE_HOLDER> .,if ( indexer . get size ( ) >= capacity ) { grow ( ) ; },room grow
no node label node <PLACE_HOLDER> @$ so return the strict resource request node <PLACE_HOLDER>,return nodes for reqs . size ( ) ;,node label
no input consumed @$ better add an error <PLACE_HOLDER>,if ( _input . index ( ) == i ) { if ( e instanceof input mismatch exception ) { input mismatch exception ime = ( input mismatch exception ) e ; token tok = e . get offending token ( ) ; int expected token type = token . invalid_type ; if ( ! ime . get expected tokens ( ) . is nil ( ) ) { expected token type = ime . get expected tokens ( ) . get min element ( ) ; } token err token = get token factory ( ) . create ( new pair < token source @$ char stream > ( tok . get token source ( ) @$ tok . get token source ( ) . get input,input add
when view has no <PLACE_HOLDER>,get elements = new get elements . builder ( ) . input ( new entity seed ( __str__ ) @$ new entity seed ( __str__ ) ) . view ( new view . builder ( ) . entity ( test groups . entity ) . build ( ) ) . build ( ) ; results = graph . execute ( get elements @$ new user ( ) ) ;,view has
when one line is larger than the other @$ it contains extra vertical padding . this produces more apparent whitespace above or below the text lines . add a <PLACE_HOLDER> offset to compensate .,if ( line1 height != line2 height ) { vertical offset += ( line2 height - line1 height ) / __num__ ; },vertical add
copy process <PLACE_HOLDER> instantiate the process <PLACE_HOLDER> @$ renaming as necessary,final set < process groupdto > groups = new hash set < > ( ) ; if ( snippet contents . get process groups ( ) != null ) { for ( final process groupdto groupdto : snippet contents . get process groups ( ) ) { final process groupdto cp = dto factory . copy ( groupdto @$ false ) ; cp . set id ( generate id ( groupdto . get id ( ) @$ id generation seed @$ is copy ) ) ; cp . set parent group id ( group id ) ; final flow snippetdto contents copy = copy contents for group ( groupdto . get contents ( ) @$ cp . get id ( ) @$ connectable map @$ service id map,groups instantiate
no such method should throw the caught <PLACE_HOLDER> . so if we get here @$ there was such a method .,if ( ! modifier . is static ( is method . get modifiers ( ) ) && is method . get annotation ( transient . class ) == null ) { check get and is variants ( container class @$ property name @$ get method @$ is method ) ; },method throw
note : the hls spec forbids initialization <PLACE_HOLDER> for packed audio .,if ( init load completed || init data spec == null ) { return ; },spec forbids
specified attributes should already have a normalized <PLACE_HOLDER> since those were added by validator,if ( ! attr . get specified ( ) ) { return value ; },attributes have
verifies write <PLACE_HOLDER> to the source and destination,return with write lock ( service facade @$ request connection entity @$ request revision @$ lookup -> { final authorizable authorizable = lookup . get connection ( id ) . get authorizable ( ) ; authorizable . authorize ( authorizer @$ request action . write @$ ni fi user utils . get ni fi user ( ) ) ; authorizable . get parent authorizable ( ) . authorize ( authorizer @$ request action . write @$ ni fi user utils . get ni fi user ( ) ) ; } @$ ( ) -> service facade . verify delete connection ( id ) @$ ( revision @$ connection entity ) -> { final connection entity entity = service facade . delete connection ( revision @$ connection entity .,verifies write
calculate ifd group data area <PLACE_HOLDER>s . ifd group data area is assigned to save the entry value which has a bigger <PLACE_HOLDER> than 4 bytes .,for ( int i = __num__ ; i < exif_tags . length ; ++ i ) { int sum = __num__ ; for ( map . entry entry : ( set < map . entry > ) m attributes [ i ] . entry set ( ) ) { final exif attribute exif attribute = ( exif attribute ) entry . get value ( ) ; final int size = exif attribute . size ( ) ; if ( size > __num__ ) { sum += size ; } } ifd data sizes [ i ] += sum ; },which has
since we start extra <PLACE_HOLDER> @$ there may be extra start and cancel events @$ so we check only the difference between start and cancel and not start and cancel events individually .,assert equals ( name @$ total instances @$ dummy service . started ( name ) - dummy service . cancelled ( name ) ) ; check count ( name @$ g @$ total instances ) ; stop extra nodes ( extra nodes ) ;,events start
we could move this three statements below registration @$ but then we should change the logic of the subscriber : if you register before any value is in @$ you 'll get a null value before in on initialize and subsequently values in on add . therefore moving those need a <PLACE_HOLDER> of the subscriber,zk persistent connection zk persistent connection = zk store test only util . get zk persistent connection ( port ) ; zoo keeper ephemeral store < string > writer store = zk store test only util . get zoo keeper ephemeral store ( port ) ; writer store . put ( test_zk_prop_name @$ __str__ ) ; property event bus . register ( collections . singleton ( test_zk_prop_name ) @$ new property event subscriber < string > ( ) { @ override public void on initialize ( string property name @$ string property value ) { if ( property name != null ) { initialized latch . count down ( ) ; } } @ override public void on add ( string property name @$ string property value ),those need
drop the request silently if memory aware thread pool has set the <PLACE_HOLDER> .,if ( read suspended ) { e . get future ( ) . set success ( ) ; return true ; },pool set
see if user is opening a new policy <PLACE_HOLDER>,if ( filename == null ) { modified = false ; return ; },user opening
make sure that the scanner does n't throw an <PLACE_HOLDER> after the connection cache timeout,for ( int i = __num__ ; i < num trials ; i ++ ) { list < t result > results = handler . get scanner rows ( scan id @$ __num__ ) ; assert array equals ( bytes . to bytes ( __str__ + i ) @$ results . get ( __num__ ) . get row ( ) ) ; thread . sleep ( trial pause ) ; },scanner throw
if we 're tracking local vars @$ then some phis have side <PLACE_HOLDER> .,if ( ! has side effect ( phi ) ) { no side effect regs . set ( phi . get result ( ) . get reg ( ) ) ; },phis have
drwho is not authorized to access test protocol 1 because it uses the default blocked <PLACE_HOLDER> .,try { service authorization manager . authorize ( drwho @$ test protocol1 . class @$ conf @$ inet address . get by name ( address ) ) ; fail ( ) ; } catch ( authorization exception e ) { },default blocked
each segment should have 3 <PLACE_HOLDER>,for ( map < string @$ string > instance state map : new assignment . values ( ) ) { assert equals ( instance state map . size ( ) @$ num_replicas ) ; },segment have
set hll log 2 <PLACE_HOLDER> .,_hll log2m = segment metadata properties configuration . get int ( segment_hll_log2m @$ hll constants . default_log2m ) ;,hll log
let the emulator view handle <PLACE_HOLDER> if mouse tracking is active,if ( view . is mouse tracking active ( ) ) return false ;,view handle
in this case the top activity on the <PLACE_HOLDER> is the same as the one being launched @$ so we take that as a request to bring the <PLACE_HOLDER> to the foreground . if the top activity in the <PLACE_HOLDER> is the root activity @$ deliver this new intent to it if it desires .,if ( m start activity . m activity component . equals ( intent activity . get task record ( ) . real activity ) ) { if ( ( ( m launch flags & flag_activity_single_top ) != __num__ || launch_single_top == m launch mode ) && intent activity . m activity component . equals ( m start activity . m activity component ) ) { if ( intent activity . front of task ) { intent activity . get task record ( ) . set intent ( m start activity ) ; } deliver new intent ( intent activity ) ; } else if ( ! intent activity . get task record ( ) . is same intent filter ( m start activity ) ) { m adding,request bring
app 2 will get <PLACE_HOLDER> to be allocated on node 1 @$ and node 1 will be all used by app 2 .,fi ca scheduler app scheduler app1 = cs . get application attempt ( am1 . get application attempt id ( ) ) ; fi ca scheduler app scheduler app2 = cs . get application attempt ( am2 . get application attempt id ( ) ) ;,app get
page views . do this once we have the right padding <PLACE_HOLDER> from above .,for ( int i = __num__ ; i < count ; i ++ ) { final view child = get child at ( i ) ; if ( child . get visibility ( ) != gone ) { final layout params lp = ( layout params ) child . get layout params ( ) ; item info ii ; if ( ! lp . is decor && ( ii = info for child ( child ) ) != null ) { int loff = ( int ) ( child width * ii . offset ) ; int child left = padding left + loff ; int child top = padding top ; if ( lp . needs measure ) { lp . needs measure = false ; final,views have
and should have one active <PLACE_HOLDER>,list < string > active activity ids = runtime service . get active activity ids ( execution . get id ( ) ) ; assert equals ( __num__ @$ active activity ids . size ( ) ) ;,and have
user specified default <PLACE_HOLDER> there wo n't be any <PLACE_HOLDER> to annotate @$ so disable them automatically as a usability feature,if ( default package . length ( ) == __num__ ) { package level annotations = false ; },user specified
this tests also object type <PLACE_HOLDER> concurrency,final string drl = __str__ + bean . class . get canonical name ( ) + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; test concurrent insertions ( drl @$ __num__ @$ __num__ @$ true @$ true ) ;,tests object
the time this app will have <PLACE_HOLDER> again .,long in quota time elapsed = stats . in quota time elapsed ; if ( ! is under job count quota && stats . bg job count in window < stats . job count limit ) { in quota time elapsed = math . max ( in quota time elapsed @$ stats . job rate limit expiration time elapsed ) ; },app have
unlike pql @$ sql expects the group <PLACE_HOLDER> in select statements .,string group by columns = string utils . join ( _group columns @$ __str__ ) ;,sql expects
the current path must have <PLACE_HOLDER> to be better than the best match,if ( ! best match . is empty ( ) && path info . distance ( current path ) >= path info . distance ( best match ) ) return false ;,path have
if cr<PLACE_HOLDER> just keep <PLACE_HOLDER>,if ( c == __str__ ) { c = read ( reader ) ; if ( c != __str__ ) { last read = c ; use last read = true ; c = __str__ ; } },crlf keep
lhs shares and rhs shares must necessarily have the same length @$ because everyone uses the same master resource <PLACE_HOLDER> .,for ( int i = lhs shares . length - __num__ ; i >= __num__ ; i -- ) { if ( lhs shares [ i ] == float . positive_infinity || rhs shares [ i ] == float . positive_infinity ) { continue ; } diff = lhs shares [ i ] - rhs shares [ i ] ; if ( diff != __num__ ) { break ; } } return diff ;,everyone uses
a single checkout does not trigger a <PLACE_HOLDER>,return ;,checkout trigger
error arises because user has specified case sensitive affinity column <PLACE_HOLDER>,grid test utils . assert throws ( null @$ new callable < object > ( ) { @ override public object call ( ) throws exception { execute ( __str__ ) ; return null ; } } @$ ignitesql exception . class @$ __str__ ) ;,user specified
let the optional watcher take a <PLACE_HOLDER>,if ( m_watcher != null ) { m_watcher . handle line ( data ) ; },watcher take
we 'll skip words which are <PLACE_HOLDER> . retrieve tags indicating <PLACE_HOLDER> in this treebank .,set < string > punctuation tags = get punctuation tags ( ) ; if ( trees . size ( ) != gold trees . size ( ) ) { log . error ( __str__ ) ; return null ; } int correct arcs = __num__ ; int correct arcs no punc = __num__ ; int correct heads = __num__ ; int correct heads no punc = __num__ ; int correct trees = __num__ ; int correct trees no punc = __num__ ; int correct root = __num__ ; int sum arcs = __num__ ; int sum arcs no punc = __num__ ; for ( int i = __num__ ; i < trees . size ( ) ; ++ i ) { list < core label > tokens =,tags indicating
the default entry point starts the user <PLACE_HOLDER> .,start application ( null ) ;,point starts
these credentials must match <PLACE_HOLDER> in the default password file,string [ ] credentials = new string [ ] { __str__ @$ __str__ } ; cli_env . put ( __str__ @$ credentials ) ; jmxc = jmx connector factory . connect ( url @$ cli_env ) ; m bean server connection mbsc = jmxc . getm bean server connection ( ) ;,credentials match
there exists one independent iterator in the current condition which is also a part of the intermediate resultset identify the final list which will depend upon the complete expansion flag identify the <PLACE_HOLDER> to be expanded to @$ which will also depend upon complete expansion flag..,if ( no of indexes to use == __num__ ) { list total exp list = new array list ( single usableich . expansion list ) ; if ( complete expansion needed ) { support . assert ( expn itrs to ignore != null @$ __str__ ) ; expn itrs to ignore . add all ( single usableich . final list ) ; int size = final list . size ( ) ; for ( int i = __num__ ; i < size ; ++ i ) { runtime iterator curr itr = ( runtime iterator ) final list . get ( i ) ; if ( ! expn itrs to ignore . contains ( curr itr ) ) { total exp list . add ( curr itr,list identify
these classes should have no line <PLACE_HOLDER> @$ except for one in the implicit constructor .,reference type rt = find reference type ( __str__ ) ; if ( rt == null ) { throw new exception ( __str__ ) ; } method method = find method ( rt @$ __str__ @$ __str__ ) ; if ( method == null ) { throw new exception ( __str__ ) ; } test ( method @$ variables @$ __str__ @$ __str__ ) ; test ( method @$ byname @$ __str__ @$ __str__ ) ; test ( method @$ arguments @$ __str__ @$ __str__ ) ; method = find method ( rt @$ __str__ @$ __str__ ) ; if ( method == null ) { throw new exception ( __str__ ) ; } test ( method @$ variables @$ __str__ @$ __str__ ) ; test ( method,classes have
since all types does n't have a pk we now have two <PLACE_HOLDER> .,assert equals ( __num__ @$ realm . where ( all types . class ) . find all ( ) . size ( ) ) ;,types have
if the pattern has an <PLACE_HOLDER> as source it wo n't be relevant for calculation of property reactivity masks,if ( ! ( pattern . get source ( ) instanceof accumulate ) ) { context . set last built pattern ( pattern ) ; },pattern has
do n't do anything here . the user did n't change the <PLACE_HOLDER> .,if ( producer edited == false ) { } else { try { tag . set field ( field key . producer @$ producer edit text . get text ( ) . to string ( ) ) ; } catch ( key not found exception e ) { e . print stack trace ( ) ; } catch ( field data invalid exception e ) { e . print stack trace ( ) ; } catch ( no such element exception e ) { e . print stack trace ( ) ; } },user change
check that above modification did not change internal <PLACE_HOLDER> of the policy qualifier info instance,assert true ( arrays . equals ( encoding @$ encoding ret1 ) ) ;,modification change
since this function may not access the underlying inner <PLACE_HOLDER> @$ we need to validate if <PLACE_HOLDER> is open outside as well .,validate store open ( ) ; final key value iterator < windowed < bytes > @$ byte [ ] > underlying iterator = wrapped ( ) . fetch ( from @$ to @$ time from @$ time to ) ; if ( cache == null ) { return underlying iterator ; } final peeking key value iterator < bytes @$ lru cache entry > cache iterator = wrapped ( ) . persistent ( ) ? new cache iterator wrapper ( from @$ to @$ time from @$ time to ) : cache . range ( name @$ cache function . cache key ( key schema . lower range ( from @$ time from ) ) @$ cache function . cache key ( key schema . upper range (,function access
windows secure container executor will set <PLACE_HOLDER> to allow nm to read the file,line ( __str__ ) ; line with len check ( string . format ( __str__ @$ src . to string ( ) @$ dest . to string ( ) ) ) ;,executor set
data source should start <PLACE_HOLDER> only after command log replay on a recover,source . set ready for polling ( m_start polling ) ; runner . run ( ) ;,source start
ensure broker gets a <PLACE_HOLDER> to send on the new connection,time unit . seconds . sleep ( __num__ ) ; log . info ( __str__ + socket ) ; socket . close ( ) ;,broker gets
backslash and double quote need double the <PLACE_HOLDER> for both java and haskell,special char replacements . remove ( __str__ ) ; special char replacements . remove ( __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ;,backslash double
check that the byte array contains the complete <PLACE_HOLDER>,if ( len > b . length ) { throw new illegal argument exception ( __str__ ) ; } try { synchronized ( send lock ) { send packet0 ( id @$ b ) ; } } catch ( io exception ioe ) { if ( ! is open ( ) ) { throw new closed connection exception ( __str__ ) ; } else { throw ioe ; } },array contains
do n't care about the data sent @$ just the connect <PLACE_HOLDER> .,session . get basic remote ( ) . send text ( __str__ ) ; session . close ( ) ; resp . set content type ( __str__ ) ; resp . get writer ( ) . println ( __str__ + wsuri ) ;,the connect
we ca n't do anr checks after we cease to exist ! reset any blocking behavior <PLACE_HOLDER> we might have made .,if ( m closed . compare and set ( false @$ true ) ) { set detect not responding ( __num__ ) ; if ( m stable ) { return m content resolver . release provider ( m content provider ) ; } else { return m content resolver . release unstable provider ( m content provider ) ; } } else { return false ; },any blocking
create three task generator threads . each of them will submit different <PLACE_HOLDER> of jobs .,final runnable task runnable1 = new task generator ( msg queue @$ __num__ ) ; final runnable task runnable2 = new task generator ( msg queue @$ __num__ ) ; final runnable task runnable3 = new task generator ( msg queue @$ __num__ ) ;,threads submit
check again with hashtable just in case another thread created a <PLACE_HOLDER> since we last checked,handler2 = handlers . get ( protocol ) ; if ( handler2 != null ) { return handler2 ; },thread created
accessible j label implements accessible <PLACE_HOLDER> if the j label contains html <PLACE_HOLDER>,if ( note label != null ) { return note label . get accessible context ( ) . get accessible text ( ) ; },label contains
if no entry keep skipping rows until we come to the end @$ or find <PLACE_HOLDER> that is populated,while ( this . entry == null && this . row < this . length ) { this . entry = this . table [ this . row ] ; this . row ++ ; } return this . entry ;,entry keep
only global stats make <PLACE_HOLDER>,_total stats . register buffer metrics ( r @$ s @$ since @$ free space ) ;,stats make
each source lines below should represent 32 <PLACE_HOLDER> @$ until the next comment .,return new byte array input stream ( ( __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ) . get bytes ( ) ) ;,lines represent
is not atomic with the insertion . this means that concurrent reads can overlap and overwrite <PLACE_HOLDER> another @$ resulting in a lossy buffer .,final atomic long counter = read buffer write count [ buffer index ] ; final long write count = counter . get ( ) ; counter . lazy set ( write count + __num__ ) ; final int index = ( int ) ( write count & read_buffer_index_mask ) ; read buffers [ buffer index ] [ index ] . lazy set ( node ) ; return write count ;,reads overlap
does the class have optimization <PLACE_HOLDER> ?,if ( class optimization info . get class optimization info ( library class ) != null ) { class visitor . visit library class ( library class ) ; },class have
this check is somewhat unnecessary as all partitioned regions should have the same <PLACE_HOLDER> due to the fact that partitioned regions do no support <PLACE_HOLDER> .,if ( profile . scope . is distributed ( ) && other scope . is distributed ( ) ) { if ( profile . scope != other scope ) { result = string . format ( __str__ @$ this . region path @$ profile . scope @$ my id @$ other scope ) ; } },regions do
when streaming @$ we only process whole <PLACE_HOLDER> thus some updates are only done on paragraph boundaries,if ( ( reordering options & option_streaming ) != __num__ ) { length = i ; this . control count = control count ; },streaming process
re<PLACE_HOLDER>ing the input from the previous test does not change the resulting <PLACE_HOLDER>,list < spawn exec > l = test stable sort ( immutable list . of ( f @$ e @$ d @$ c @$ b @$ a ) ) ; assert that ( l ) . contains exactly ( a @$ c @$ d @$ b @$ e @$ f ) . in order ( ) ;,input change
j progress bar.vertical paint each <PLACE_HOLDER> vertically,for ( int i = y - offset ; i < height + tile width ; i += tile width ) { context . get painter ( ) . paint progress bar foreground ( context @$ g @$ x @$ i @$ width @$ tile width @$ p bar . get orientation ( ) ) ; },bar.vertical paint
do everything as bytes ; do n't want any <PLACE_HOLDER>,@ suppress warnings ( __str__ ) buffered input stream istr = new buffered input stream ( new file input stream ( args [ __num__ ] ) ) ; byte array output stream ostr = new byte array output stream ( ) ; byte [ ] bytes = new byte [ __num__ ] ; boolean more = true ; while ( more ) { int len = istr . read ( bytes ) ; if ( len < bytes . length ) { more = false ; if ( len > __num__ ) { ostr . write ( bytes @$ __num__ @$ len ) ; } } else { ostr . write ( bytes ) ; } } byte [ ] data = ostr . to byte array (,everything want
add the mapper scanner as a bean definition rather than explicitly setting a post processor on the context so initialization follows the same code <PLACE_HOLDER> as reading from an xml config file,generic bean definition definition = new generic bean definition ( ) ; definition . set bean class ( mapper scanner configurer . class ) ; definition . get property values ( ) . add ( __str__ @$ __str__ ) ; application context . register bean definition ( __str__ @$ definition ) ; setup sql session factory ( __str__ ) ;,initialization follows
this rule has an <PLACE_HOLDER> .,verifier rule rule1 = verifier component mock factory . create rule1 ( ) ; pattern pattern1 = verifier component mock factory . create pattern1 ( ) ; sub pattern pp1 = new sub pattern ( pattern1 @$ __num__ ) ; sub pattern pp2 = new sub pattern ( pattern1 @$ __num__ ) ; incompatibility i1 = new incompatibility ( pp1 @$ pp2 ) ; sub rule rp1 = new sub rule ( rule1 @$ __num__ ) ; rp1 . add ( pp1 ) ; rp1 . add ( pp2 ) ; sub pattern pp3 = new sub pattern ( pattern1 @$ __num__ ) ; sub pattern pp4 = new sub pattern ( pattern1 @$ __num__ ) ; incompatibility i2 = new incompatibility ( pp1 @$ pp2 ) ;,rule has
pkcs 11 test.main will remove this <PLACE_HOLDER> if needed,providers . set at ( p @$ __num__ ) ; random = new secure random ( ) ; factory = certificate factory . get instance ( __str__ ) ; try { factory . generate certificate ( null ) ; } catch ( certificate exception e ) { },test.main remove
tests all expect to create a new ds reset the test object <PLACE_HOLDER> for the next test .,test object . num instance = __num__ ; portfolio pdx . debug = false ;,ds reset
check that deleting the ip address stops the <PLACE_HOLDER> .,link properties bogus lp = new link properties ( lp ) ; try ( socket keepalive ka = m cm . create socket keepalive ( my net @$ test socket @$ myi pv4 @$ dsti pv4 @$ executor @$ callback ) ) { ka . start ( valid ka interval ) ; callback . expect started ( ) ; bogus lp . remove link address ( new link address ( myi pv4 @$ __num__ ) ) ; bogus lp . add link address ( new link address ( not myi pv4 @$ __num__ ) ) ; m wi fi network agent . send link properties ( bogus lp ) ; callback . expect error ( socket keepalive . error_invalid_ip_address ) ; m wi fi network agent . send,check stops
unknown jvm <PLACE_HOLDER> @$ assuming native .,log . ignore ( t ) ;,unknown jvm
just for overriding the findbugs np <PLACE_HOLDER> @$ as the parameter is marked as nullable in the guava predicate .,return iterables . filter ( files @$ new predicate < file status > ( ) { @ override public boolean apply ( file status file ) { if ( file == null ) { return false ; } string wal = file . get path ( ) . get name ( ) ; boolean log in replication queue = wals . contains ( wal ) ; if ( log in replication queue ) { log . debug ( __str__ @$ wal ) ; } return ! log in replication queue && ( file . get modification time ( ) < readzk timestamp ) ; } } ) ;,findbugs np
the act of persisting does n't flush <PLACE_HOLDER> @$ so our id is still 1 .,assert equals ( integer . value of ( __num__ ) @$ author . get id ( ) ) ;,act flush
if this is a .com or jetpack blog @$ tapping the title shows the associated <PLACE_HOLDER> in the reader,if ( can request post ) { if ( ! post exists ) { app log . d ( t . comments @$ __str__ ) ; reader post actions . request blog post ( site . get site id ( ) @$ post id @$ new reader actions . on request listener ( ) { @ override public void on success ( ) { if ( ! is added ( ) ) { return ; } if ( ! has title ) { string post title = reader post table . get post title ( site . get site id ( ) @$ post id ) ; if ( ! text utils . is empty ( post title ) ) { set post title ( txt post title,title shows
! ! ! would n't preflighting be simpler ? this looks like it is effectively be doing that . it seems that for every true error the code will call <PLACE_HOLDER> @$ which will throw the error @$ which this will catch @$ which this will then rethrow the error . just seems cumbersome .,if ( e . get message ( ) . index of ( __str__ ) >= __num__ ) { warnln ( __str__ ) ; } else { errln ( e . get message ( ) ) ; },code call
this is admittedly a bit simple @$ stats object converter seems to allow old stats attributes to be kept if the new values do not overwrite <PLACE_HOLDER> .,for ( column statistics obj col stat : new col list ) { old stats . put ( col stat . get col name ( ) . to lower case ( ) @$ col stat ) ; },values overwrite
user list notifications have a footer if they have 10 or more users in the body the last block will not have a <PLACE_HOLDER> @$ so we can use that to determine if it is the footer,if ( m note . is follow type ( ) || m note . is like type ( ) || m note . is reblog type ( ) ) { return block object . get type ( ) == null ; },block have
the enum bit set ; classes for specialized enum constants do n't do the <PLACE_HOLDER> .,return ( this . get modifiers ( ) & enum ) != __num__ && this . get superclass ( ) == java . lang . enum . class ;,classes do
ls bit of wordsize byte indicates tlze <PLACE_HOLDER>,return encoding cookie base | __num__ ;,bit indicates
update button deselects bundles @$ revert buttons to defautl <PLACE_HOLDER>,default button state ( ) ;,buttons defautl
if the user sets a message <PLACE_HOLDER> @$ use it .,final string message id = one way feature . get message id ( ) ; if ( ! is message id added && message id != null ) { headers . add ( new string header ( av . messageid tag @$ message id ) ) ; },user sets
need also consider the <PLACE_HOLDER> if surface width is smaller than scissor width .,if ( surface width < scissor width ) { surface percentageh *= ( float ) surface width / scissor width ; },need consider
some services automatically add <PLACE_HOLDER> from their addressbook to the roster and those <PLACE_HOLDER> are with subscription none . if such already exist @$ remove them . this is typically our own contact,if ( ! is entry displayable ( item ) ) { if ( contact != null ) { contact group parent = contact . get parent contact group ( ) ; if ( parent instanceof root contact group jabber impl ) ( ( root contact group jabber impl ) parent ) . remove contact ( contact ) ; else ( ( contact group jabber impl ) parent ) . remove contact ( contact ) ; fire contact removed ( parent @$ contact ) ; } continue ; } if ( contact == null ) { contact = new contact jabber impl ( item @$ this @$ true @$ true ) ; root group . add contact ( contact ) ; fire contact added ( root group @$ contact,services add
check client reads <PLACE_HOLDER>,assert equals ( - __num__ @$ is . read ( ) ) ; assert true ( end point . is output shutdown ( ) ) ;,client reads
if parent statistics is null then that branch of the tree is not walked yet . do n't update the <PLACE_HOLDER> until all branches are walked,if ( stats == null && op . get parent operators ( ) != null ) { if ( is all parents contain statistics ( op ) ) { for ( operator < ? extends operator desc > parent : op . get parent operators ( ) ) { statistics parent stats = parent . get statistics ( ) ; if ( stats == null ) { stats = parent stats . clone ( ) ; } else { stats . add basic stats ( parent stats ) ; } stats . update column stats state ( parent stats . get column stats state ( ) ) ; list < col statistics > col stats = stats utils . get col statistics from expr map ( hconf @$,branch update
indicates that the cell has a the <PLACE_HOLDER> which was modified in the src replication cluster,tag tag = pair . get second ( ) ; if ( cell visibility == null && tag != null ) { cell visibility = new cell visibility ( tag . get value as string ( tag ) ) ; modified tag found = true ; },cell has
call set system ui visibility with flags which will cause window <PLACE_HOLDER> to be dispatched,final int flags = view . system_ui_flag_layout_stable | view . system_ui_flag_layout_fullscreen ; on view ( with id ( r . id . test_content ) ) . perform ( set system ui visibility ( flags ) ) ;,which cause
shallow equals is a middle ground between using default equals @$ which might miss nested <PLACE_HOLDER> with the same elements @$ and deep equality checking @$ which would be expensive . all three choices are sound @$ since shallow equals and default equals are more conservative than deep equals . using shallow equals means that we may unnecessarily consider some values unequal that are,return objects . equals ( this . value @$ that . value ) && objects . equals ( this . error info @$ that . error info ) && transitive events . shallow equals ( that . transitive events ) && transitive postables . shallow equals ( that . transitive postables ) ;,which miss
let authenticator know the <PLACE_HOLDER> of the caller,login options . put int ( account manager . key_caller_uid @$ caller uid ) ; login options . put int ( account manager . key_caller_pid @$ binder . get calling pid ( ) ) ; if ( notify on auth failure ) { login options . put boolean ( account manager . key_notify_on_failure @$ true ) ; } long identity token = clear calling identity ( ) ; try { final byte [ ] caller pkg sig digest = calculate package signature digest ( caller pkg ) ; if ( ! custom tokens && permission granted ) { string auth token = read auth token internal ( accounts @$ account @$ auth token type ) ; if ( auth token != null ) { bundle result = new,authenticator know
default to the global scope because externs may have qualified <PLACE_HOLDER> with undeclared roots .,if ( root var == null ) { return this . get parent ( ) . get global scope ( ) . get own slot ( name ) ; } else { return root var . get scope ( ) . get own slot ( name ) ; },externs have
only new user state has <PLACE_HOLDER> defined ; different,final package user state test user state01 = new package user state ( ) ; test user state01 . disabled components = new array set < > ( ) ; assert that ( test user state01 . equals ( old user state ) @$ is ( false ) ) ;,state has
clear calling <PLACE_HOLDER> as initialization enforces the system <PLACE_HOLDER> but we can be coming from shell .,if ( get user manager ( ) . is user unlocked ( user id ) ) { long old id = binder . clear calling identity ( ) ; try { start service for user ( user id ) ; } finally { binder . restore calling identity ( old id ) ; } },identity enforces
null should not affect <PLACE_HOLDER>,current count = double count kudaf . aggregate ( null @$ current count ) ; assert that ( __num__ @$ equal to ( current count ) ) ;,null affect
get the first person mount <PLACE_HOLDER> and rotate it away from the camera,character held item component character held item component = local player . get character entity ( ) . get component ( character held item component . class ) ; first person held item mount point component mount point component = local player . get camera entity ( ) . get component ( first person held item mount point component . class ) ; if ( character held item component == null || mount point component == null ) { return ; } location component location component = mount point component . mount point entity . get component ( location component . class ) ; if ( location component == null ) { return ; } long time elapsed since last used = time . get game time in,person mount
no node for this letter yet . make <PLACE_HOLDER> .,if ( ! found ) { char node = new trie node ( ) ; m trie [ herep ] = node ; m trie [ m trie [ herep ] + trie_c ] = c ; m trie [ m trie [ herep ] + trie_off ] = trie_null ; m trie [ m trie [ herep ] + trie_next ] = trie_null ; m trie [ m trie [ herep ] + trie_child ] = trie_null ; if ( i == slen - __num__ ) { m trie [ m trie [ herep ] + trie_off ] = off ; return ; } herep = m trie [ herep ] + trie_child ; },node make
order does n't affect <PLACE_HOLDER>,output assembler temp = assembly buffer . create output assembler ( byte order . native order ( ) ) ; write ( temp ) ; return temp . pos ( ) ;,order affect
package 3 has a <PLACE_HOLDER> working on person instances . as we added person instance in advance @$ <PLACE_HOLDER> should fire now,working memory . fire all rules ( ) ; assert equals ( __str__ @$ __str__ @$ bob . get status ( ) ) ; assert equals ( __num__ @$ list . size ( ) ) ; assert equals ( bob @$ list . get ( __num__ ) ) ; kpkgs = serialization helper . serialize object ( load knowledge packages ( __str__ ) ) ; kbase . add packages ( kpkgs ) ; working memory . fire all rules ( ) ; kbase = serialization helper . serialize object ( kbase ) ; assert equals ( __str__ @$ __str__ @$ bob . get status ( ) ) ; assert equals ( __num__ @$ list . size ( ) ) ; assert equals ( bob @$ list . get,package has
if the referred document has a target <PLACE_HOLDER> differing from the caller @$ it 's an error,if ( callertns != curr schema info . f target namespace ) { report schema error ( ns_error_codes [ refer type ] [ second idx ] @$ new object [ ] { callertns @$ curr schema info . f target namespace } @$ schema root ) ; return null ; },document has
check whether first tile covers the <PLACE_HOLDER> of the tile vertically,int first top = traps . get top ( trap list . get int ( __num__ ) ) ; int first bottom = traps . get bottom ( trap list . get int ( __num__ ) ) ; if ( first top > tile starty || first bottom < tile starty ) { return false ; },tile covers
no reducers . just write straight to table . call init table reducer <PLACE_HOLDER> because it sets up the table output format .,job . set mapper class ( importer . class ) ; table map reduce util . init table reducer job ( table name . get name as string ( ) @$ null @$ job ) ; job . set num reduce tasks ( __num__ ) ;,reducers call
if needed @$ update the maximum stack size and number of locals @$ and stack map <PLACE_HOLDER> .,if ( current basic block != null ) { if ( compute == compute_all_frames || compute == compute_inserted_frames ) { current basic block . frame . execute ( opcode @$ __num__ @$ null @$ null ) ; } else { int size = relative stack size + stack_size_delta [ opcode ] ; if ( size > max relative stack size ) { max relative stack size = size ; } relative stack size = size ; } if ( ( opcode >= opcodes . ireturn && opcode <= opcodes . return ) || opcode == opcodes . athrow ) { end current basic block with no successor ( ) ; } },maximum stack
'not found ' can happen if import creates more than one <PLACE_HOLDER>,throw new runtime exception ( __str__ + __str__ + from + __str__ + to + __str__ + req @$ ex ) ;,import creates
java 's replace function replace every <PLACE_HOLDER> in origin with replacement @$ while origin value should not be changed is expected in sql .,if ( target . length ( ) == __num__ ) { return origin ; } return origin . replace ( target @$ replacement ) ;,function replace
if the size of bloom filter is smaller than 1 mb @$ use default max false positive <PLACE_HOLDER>,if ( num bits required <= max num bits ) { return default max false pos probability ; },default max
assume approximately half of values will satisfy a <PLACE_HOLDER>,return cost_node_unspecific_predicate ;,half satisfy
sometimes providers do not encode <PLACE_HOLDER> the same way . e.g . bouncy castle may use long form encoding @$ where jdk uses a short encoding with named curves . this checks the encodings and logs them if they differ .,string hex reencoded key = hex . encode ( pub key . get encoded ( ) ) ; if ( ! hex pub key . equals ( hex reencoded key ) ) { system . out . println ( __str__ + hex pub key ) ; system . out . println ( __str__ + hex reencoded key ) ; },providers encode
this call will put all the basic <PLACE_HOLDER> into the element,element element = super . toxml ( ) ;,call put
moving the <PLACE_HOLDER> 6 minutes should trigger the <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { current time = new date ( current time . get time ( ) + ( __num__ * __num__ * __num__ ) ) ; set clock to ( current time ) ; job job = cmmn management service . create timer job query ( ) . case instance id ( case instance . get id ( ) ) . single result ( ) ; assert true ( job . get duedate ( ) . get time ( ) - current time . get time ( ) <= ( __num__ * __num__ * __num__ ) ) ; job = cmmn management service . move timer to executable job ( job . get id ( ),timer trigger
this checks for null data type manager below since bad data type wo n't have <PLACE_HOLDER> .,source archive source archive = data type . get source archive ( ) ; boolean local source = ( source archive == null ) || ( ( data type manager != null ) && system utilities . is equal ( data type manager . get universalid ( ) @$ source archive . get source archiveid ( ) ) ) ; if ( local source ) { source archive = originaldtm . get source archive ( originaldtm . get universalid ( ) ) ; } data type found data type = originaldtm . get data type ( data type . get data type path ( ) ) ; string display name = __str__ ; if ( found data type != null && ( data type manager != null ),checks have
dispatching to empty statement will not call back <PLACE_HOLDER> @$ must call our visit empty statement explicitly,if ( else block instanceof empty statement ) { visit empty statement ( ( empty statement ) else block ) ; } else { else block . visit ( this ) ; },statement call
at boot time @$ we know that the screen is on and the electron beam animation is not playing . we do n't know the screen 's brightness though @$ so prepare to set it to a known state when the state is next applied . although we set the brightness to full on here @$ the display power controller will reset the brightness,m screen state = display . state_on ; m screen brightness = power manager . brightness_on ; schedule screen update ( ) ; m color fade prepared = false ; m color fade level = __num__ ; m color fade ready = true ;,changes have
use use a <PLACE_HOLDER> of 1 @$ and then wack the matrix each time we actually use it .,shader = new linear gradient ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ shader . tile mode . clamp ) ; paint . set shader ( shader ) ; paint . set xfermode ( new porter duff xfermode ( porter duff . mode . dst_out ) ) ; this . host = host ;,use use
check if the current segment contains the given <PLACE_HOLDER> first in order to prevent an unnecessary map lookup .,if ( current segment != null && index > current segment . index ( ) ) { return current segment ; },segment contains
somebody else did a failover concurrently try that <PLACE_HOLDER> now,synchronized ( lock ) { if ( ( old spi != null ) && ( old spi != spi ) ) { return spi ; } if ( service iterator == null ) { return null ; } while ( service iterator . has next ( ) ) { service s = service iterator . next ( ) ; try { object inst = s . new instance ( null ) ; if ( inst instanceof key pair generator spi == false ) { continue ; } if ( inst instanceof key pair generator ) { continue ; } key pair generator spi spi = ( key pair generator spi ) inst ; if ( reinit ) { if ( init type == i_size ) { spi .,failover try
we will approve the request @$ which will update the <PLACE_HOLDER>,variables = new hash map < > ( ) ; variables . put ( __str__ @$ boolean . true ) ; task task = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) @$ variables ) ;,which update
in case any of the <PLACE_HOLDER>s exceeds the screen <PLACE_HOLDER> we set default one get the left upper point of the window,if ( ! ( virtual bounds . contains ( x @$ y ) ) ) { if ( x < virtual bounds . x ) { x = virtual bounds . x + border distance ; } else if ( x > virtual bounds . x ) { x = virtual bounds . x + virtual bounds . width - width - border distance ; if ( x < virtual bounds . x + border distance ) { x = virtual bounds . x + border distance ; } } if ( y < virtual bounds . y ) { y = virtual bounds . y + border distance ; } else if ( y > virtual bounds . y ) { y = virtual bounds . y,any exceeds
try to find columns in left table which match unique <PLACE_HOLDER> in right table,sql = generate table join by columns ( monitor @$ left table @$ left alias @$ right table @$ right alias ) ; if ( sql != null ) return sql ;,which match
trigger should remove <PLACE_HOLDER> .,assert true ( m accounts db . delete de account ( acc id ) ) ;,trigger remove
this overwrites whatever setting the <PLACE_HOLDER> configured in the properties,adjust auto commit config ( properties @$ offset commit mode ) ; return new kafka fetcher < > ( source context @$ assigned partitions with initial offsets @$ watermarks periodic @$ watermarks punctuated @$ runtime context . get processing time service ( ) @$ runtime context . get execution config ( ) . get auto watermark interval ( ) @$ runtime context . get user code class loader ( ) @$ runtime context . get task name with subtasks ( ) @$ deserializer @$ properties @$ poll timeout @$ runtime context . get metric group ( ) @$ consumer metric group @$ use metrics ) ;,whatever setting
if s does n't cause the current <PLACE_HOLDER> to cross the limit @$ buffer it and return . we 'll decide whether or not we have to wrap it later .,if ( next newline == - __num__ && column + s . length ( ) <= column limit ) { buffer . append ( s ) ; column += s . length ( ) ; return ; },s cause
drop should succeed since no query is using the <PLACE_HOLDER>,final optional < command status > drop table command status2 = statement executor . get status ( drop table command id2 ) ; assert . assert true ( drop table command status2 . is present ( ) ) ; assert that ( drop table command status2 . get ( ) . get status ( ) @$ equal to ( command status . status . success ) ) ;,query using
there are too many configuration processors so we do n't know <PLACE_HOLDER> one to run so report the error .,if ( user supplied configuration processor count > __num__ ) { string builder sb = new string builder ( string . format ( __str__ @$ user supplied configuration processor count ) ) ; for ( entry < string @$ configuration processor > entry : configuration processors . entry set ( ) ) { string hint = entry . get key ( ) ; if ( ! hint . equals ( settings xml configuration processor . hint ) ) { configuration processor configuration processor = entry . get value ( ) ; sb . append ( string . format ( __str__ @$ configuration processor . get class ( ) . get name ( ) ) ) ; } } sb . append ( __str__ ) ; throw new exception,one run
if unable to create file @$ no point testing the <PLACE_HOLDER> . this is the path that microsoft windows takes .,assume true ( true @$ __str__ ) ;,point testing
we need to go through a list of appenders and locate the async ones @$ as those could have <PLACE_HOLDER> left to write . since there is no flushing mechanism built into logback @$ we wait for a short period of time before giving up that the appender will be completely flushed .,try { final logger logger = logger context . get logger ( org . slf4j . logger . root_logger_name ) ; final list < appender < i logging event > > appenders = lists . of ( logger . iterator for appenders ( ) ) ; for ( appender < i logging event > appender : appenders ) { if ( appender instanceof async appender base ) { flush appender ( ( async appender base < ? > ) appender ) ; } else if ( appender instanceof async appender base proxy ) { flush appender ( ( ( async appender base proxy < ? > ) appender ) . get appender ( ) ) ; } } } catch ( interrupted exception ignored ) { thread .,those have
make sure master is fully up before progressing . could take a <PLACE_HOLDER> if regions being reassigned .,while ( ! master . get master ( ) . is initialized ( ) ) { threads . sleep ( __num__ ) ; },progressing take
cache was probably closed which destroyed this lock service <PLACE_HOLDER> : destroyed lock services release all held locks,cache . get cancel criterion ( ) . check cancel in progress ( null ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . lock service @$ e ) ; },which destroyed
now do the same <PLACE_HOLDER> @$ but make all the requests for one group first @$ then the second group .,for ( int i = __num__ ; i < __num__ ; i ++ ) { server location l = sn . get server for connection ( __str__ @$ collections . empty_set ) ; assert true ( l1 . equals ( l ) || l2 . equals ( l ) ) ; } expected = new hash map ( ) ; expected . put ( l1 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; expected . put ( l2 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; expected . put ( l3 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; assert equals ( expected @$ sn . get,now do
request decommission for data nodes <PLACE_HOLDER> and 2 .,list < datanode info > decom data nodes = take node outof service ( __num__ @$ lists . new array list ( get cluster ( ) . get data nodes ( ) . get ( __num__ ) . get datanode uuid ( ) @$ get cluster ( ) . get data nodes ( ) . get ( __num__ ) . get datanode uuid ( ) ) @$ long . max_value @$ null @$ null @$ admin states . decommissioned ) ; generic test utils . wait for ( new supplier < boolean > ( ) { @ override public boolean get ( ) { try { string err msg = check file ( file sys @$ file @$ repl @$ decom data nodes . get ( __num__ ),decommission nodes
scan will cause <PLACE_HOLDER> to be added in block cache,scan all regions forrs ( rs1 ) ; assert equals ( block cache1 . get block count ( ) - initial block count1 @$ htu . get numh files forrs ( rs1 @$ table_name @$ family ) ) ; scan all regions forrs ( rs2 ) ; assert equals ( block cache2 . get block count ( ) - initial block count2 @$ htu . get numh files forrs ( rs2 @$ table_name @$ family ) ) ; cache eviction stats stats = admin . clear block cache ( table_name ) . get ( ) ; assert equals ( stats . get evicted blocks ( ) @$ htu . get numh files forrs ( rs1 @$ table_name @$ family ) + htu . get numh files forrs (,scan cause
expression has n't nested <PLACE_HOLDER>,if ( tail == null ) { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( __str__ @$ offset @$ result ) ; } else { result . add ( new flat field descriptor ( offset @$ field type ) ) ; } } else { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( tail @$ offset @$ result ) ; } else { throw new invalid field reference exception ( __str__ + tail + __str__ + field type + __str__ ) ; } },expression nested
completing this task end the process <PLACE_HOLDER>,task service . complete ( task after sub process . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ;,task end
we have three zk servers @$ each server has two quorum <PLACE_HOLDER> and two election <PLACE_HOLDER> registered,quorum server config builder initial quorum config = new quorum server config builder ( host name @$ __num__ @$ __num__ ) ;,server has
make sure both <PLACE_HOLDER>s are targeting the same <PLACE_HOLDER> .,if ( ! item to move . get target package name ( ) . equals ( m items . get ( parent idx ) . get target package name ( ) ) ) { return false ; } m items . remove ( move idx ) ; final int new parent idx = select ( new parent package name @$ user id ) + __num__ ; m items . add ( new parent idx @$ item to move ) ; return move idx != new parent idx ;,packages targeting
only the stream which was just added will change <PLACE_HOLDER> . so we only need an array of size 1 .,list < parent changed event > events = new array list < parent changed event > ( __num__ ) ; connection state . take child ( state @$ false @$ events ) ; notify parent changed ( events ) ; state only removal queue . remove typed ( state ) ; state . stream = stream ;,stream change
even the looser regime forbids the toplevel <PLACE_HOLDER> to be list or dict .,if ( x instanceof starlark list || x instanceof dict ) { throw new eval exception ( null @$ string . format ( __str__ @$ eval utils . get data type name ( x ) ) ) ; },regime forbids
presently @$ the interference updater only works when adding before the last insn @$ and the last insn must have no <PLACE_HOLDER>,if ( insn index != insns . size ( ) - __num__ ) { throw new illegal argument exception ( __str__ + insn . to human ( ) ) ; },insn have
guess <PLACE_HOLDER> arm of the union is larger . trust @$ but verify .,final int sizeof_optval = size of . get ( socket . linger . class ) ; vm error . guarantee ( size of . get ( c int pointer . class ) <= sizeof_optval @$ __str__ ) ; vm error . guarantee ( size of . get ( socket . linger . class ) <= sizeof_optval @$ __str__ ) ; word pointer optval_ pointer = stack value . get ( sizeof_optval ) ;,arm larger
if non empty basic <PLACE_HOLDER>,if ( end >= start ) { for ( int i = start ; i < end ; ++ i ) { code . data [ i ] = opcodes . nop ; } code . data [ end ] = ( byte ) opcodes . athrow ; start frame ( start @$ __num__ @$ __num__ ) ; frame [ frame index ++ ] = frame . object | cw . add type ( __str__ ) ; end frame ( ) ; },non empty
the encryption <PLACE_HOLDER> factory only returns a <PLACE_HOLDER> if encryption is enabled .,data encryption key encryption key = ! trusted channel resolver . is trusted ( ) ? encryption key factory . new data encryption key ( ) : null ; io stream pair ios = send ( socket . get inet address ( ) @$ underlying out @$ underlying in @$ encryption key @$ access token @$ datanode id @$ null ) ; return ios != null ? ios : new io stream pair ( underlying in @$ underlying out ) ;,factory returns
this would mean we 're explicitly queued or we have no running nodes but do have a cause of blockage @$ which works out the <PLACE_HOLDER>,if ( ! is running && ( is queued || cause of blockage != null ) ) { return blue run state . queued ; },which works
this operation does not change the current <PLACE_HOLDER> of the file,if ( n > __num__ ) { decrypt ( position @$ buffer @$ offset @$ n ) ; },operation change
this needs to do a binary search @$ but a binary search is somewhat tough because the sequence test involves two <PLACE_HOLDER> .,int size = vec . size ( ) @$ i ; for ( i = size - __num__ ; i >= __num__ ; i -- ) { int child = vec . element at ( i ) ; if ( child == node ) { i = - __num__ ; break ; } dtm dtm = m_dtm mgr . getdtm ( node ) ; if ( ! dtm . is node after ( node @$ child ) ) { break ; } },test involves
nested handlers or even servlet filters may implement <PLACE_HOLDER> to decode encoded request data . since the data is no longer encoded @$ we remove the encoding header .,exchange . get request headers ( ) . remove ( headers . content_encoding ) ;,handlers implement
if any event in the set has a <PLACE_HOLDER> associated with it @$ they all will @$ so just grab the first one .,if ( set . size ( ) > __num__ ) { event event = set . iterator ( ) . next ( ) ; thread = event thread ( event ) ; } else { thread = null ; },event has
search in descending order @$ so that the first found <PLACE_HOLDER> is the result .,for ( ninja scope included scope : sub map . descending map ( ) . values ( ) ) { t included value = included scope . find by name and offset recursively ( integer . max_value @$ name @$ map supplier ) ; if ( included value != null ) { return included value ; } } if ( current scope value != null ) { return current scope value . get second ( ) ; } if ( parent scope != null ) { preconditions . check not null ( include point ) ; return parent scope . find by name and offset recursively ( include point - __num__ @$ name @$ map supplier ) ; },first found
load read the encoding <PLACE_HOLDER> from the output stream,return block encoding . read block ( this @$ input ) ;,load read
test 1 : does not include <PLACE_HOLDER>,text view = m grid view . find view holder for adapter position ( __num__ ) . item view . find view by id ( r . id . t1 ) ; rect . set ( __num__ @$ __num__ @$ text view . get width ( ) @$ text view . get height ( ) ) ; m grid view . offset descendant rect to my coords ( text view @$ rect ) ; assert equals ( window align center @$ rect . top @$ delta ) ;,not include
do n't import nested classes because some of them have the same <PLACE_HOLDER> .,annotation mirror am = get current annotation ( ) ; annotation value av = helpers . get annotation type mirror value ( am @$ __str__ ) ; annotation value cv = helpers . get annotation type mirror value ( am @$ __str__ ) ; annotation value min sdk val = helpers . get annotation type mirror value ( am @$ __str__ ) ; int min sdk = min sdk val == null ? - __num__ : helpers . get annotation int value ( min sdk val ) ; annotation value max sdk val = helpers . get annotation type mirror value ( am @$ __str__ ) ; int max sdk = max sdk val == null ? - __num__ : helpers . get annotation int value ( max,some have
next @$ look for errors from the unsuccessfully evaluated transitive traversal <PLACE_HOLDER> .,iterable < sky key > unsuccessful keys = iterables . filter ( transitive traversal keys @$ predicates . not ( predicates . in ( successful keys ) ) ) ; set < map . entry < sky key @$ exception > > error entries = graph . get missing and exceptions ( unsuccessful keys ) . entry set ( ) ; set < sky key > missing keys = new hash set < > ( ) ; for ( map . entry < sky key @$ exception > entry : error entries ) { if ( entry . get value ( ) == null ) { missing keys . add ( ( sky key ) entry . get key ( ) . argument ( ) ) ; },look evaluated
get the next instruction . the loop will perform one extra <PLACE_HOLDER> after it reaches the end of the instruction list @$ with current handle set to null .,do { current handle = instructions . has next ( ) ? ( instruction handle ) instructions . next ( ) : null ; instruction inst = ( current handle != null ) ? current handle . get instruction ( ) : null ; if ( first instruction ) { open chunk at curr level = true ; curr level chunks . add ( current handle ) ; first instruction = false ; } if ( inst instanceof outlineable chunk start ) { if ( open chunk at curr level ) { sub chunk stack . push ( curr level chunks ) ; curr level chunks = new array list ( ) ; } open chunk at curr level = true ; curr level chunks . add (,loop perform
note : lobounds should have at least one <PLACE_HOLDER>,type owntype = lobounds . tail . tail == null ? lobounds . head : infer . types . lub ( lobounds ) ; if ( owntype . is primitive ( ) || owntype . has tag ( error ) ) { throw infer . inference exception . set message ( __str__ @$ uv . qtype @$ lobounds ) ; } else { return owntype ; },lobounds have
check that the api handles the <PLACE_HOLDER>,final string multipart array api = files . get ( __str__ ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ;,api handles
testing if an extremely large pattern will fail the <PLACE_HOLDER>,pattern = __str__ ; for ( int count = __num__ ; count < __num__ ; count ++ ) { pattern += temp ; } try { result = new string search ( pattern @$ new string character iterator ( text ) @$ m_en_us_ @$ null ) ; logln ( __str__ + result . get pattern ( ) ) ; } catch ( exception e ) { errln ( __str__ ) ; return ; },pattern fail
this method does not cache the <PLACE_HOLDER>,return new hash set < > ( get matching beans ( new resolvable ( required type @$ qualifiers ) ) ) ;,method cache
driver join <PLACE_HOLDER> auxiliary join <PLACE_HOLDER>,double inner join row count = left_rows_count * right_rows_count / left_join_column_ndv * left_join_column_non_nulls * right_join_column_non_nulls * unknown_filter_coefficient ;,auxiliary join
the very first notification is likely to come in slower than all the others . because that test is n't targeting the <PLACE_HOLDER> notifications are delivered with @$ we prefer to secure it .,if ( i == __num__ && notif == null ) { system . out . println ( __str__ + time for notification in seconds + __str__ + __str__ ) ; notif = notif list . poll ( time for notification in seconds @$ time unit . seconds ) ; } if ( notif == null ) { error count ++ ; system . out . println ( __str__ + __str__ + time for notification in seconds + __str__ ) ; } else { error count += check notification ( notif @$ ( string ) send notif param [ __num__ ] @$ basic . notification_message @$ obj name ) ; },test targeting
if a target has more than one source <PLACE_HOLDER> @$ the latter one will take effect .,bind mounts . put ( mount target @$ mount source ) ;,target has
this probably wo n't actually be uploaded @$ as android will probably kill all <PLACE_HOLDER> & data before it gets sent to the network .,if ( m is native initialized ) { record histogram . record enumerated histogram ( __str__ @$ option_clear_app_data @$ option_max ) ; },android kill
nobody calls <PLACE_HOLDER>,assert equals ( __num__ @$ calling . size ( ) ) ;,nobody calls
..and most applications are not currently executing an <PLACE_HOLDER> ... it needs to activate some monitoring flags that are usually off..,return null ;,applications executing
build the global include <PLACE_HOLDER> for this stylesheet . this needs to be done ahead of the recompose imports because we need the info from the composed includes .,m_global import list [ j ] . recompose includes ( m_global import list [ j ] ) ;,global include
if we do n't have an mx record @$ try the <PLACE_HOLDER> itself,if ( ( attr == null ) || ( attr . size ( ) == __num__ ) ) { attrs = ictx . get attributes ( host name @$ new string [ ] { __str__ } ) ; attr = attrs . get ( __str__ ) ; if ( attr == null ) { throw new naming exception ( base messages . get string ( pkg @$ __str__ @$ host name ) ) ; } },itself try
toggling check box requires <PLACE_HOLDER>,assert counts ( __num__ @$ __num__ ) ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { e . print stack trace ( ) ; },box requires
negative test that assert glob does throw an <PLACE_HOLDER> when asserting against the wrong values .,illegal argument exception e = assert throws ( illegal argument exception . class @$ ( ) -> assert glob matches ( immutable list . of ( __str__ @$ __str__ ) @$ immutable list . of ( __str__ @$ __str__ ) @$ immutable list . < string > of ( ) @$ true ) ) ;,test throw
fetch the viterbi generated <PLACE_HOLDER>,int [ ] computed = hmm algorithms . viterbi algorithm ( get model ( ) @$ get sequence ( ) @$ false ) ;,viterbi generated
the descendant iterator can only do <PLACE_HOLDER> node test . if there 's more than <PLACE_HOLDER> @$ use another iterator .,while ( op codes . endop != ( step type = compiler . get op ( step op code pos ) ) ) { if ( node test type != op codes . nodetype_node && node test type != op codes . nodetype_root ) return false ; step count ++ ; if ( step count > __num__ ) return false ; boolean might be proximate = might be proximate ( compiler @$ step op code pos @$ step type ) ; if ( might be proximate ) return false ; switch ( step type ) { case op codes . from_following : case op codes . from_following_siblings : case op codes . from_preceding : case op codes . from_preceding_siblings : case op codes . from_parent : case op,iterator do
create two files to ensure each storage has a <PLACE_HOLDER>,dfs test util . create file ( cluster . get file system ( ) @$ new path ( __str__ ) @$ __num__ @$ __num__ @$ __num__ @$ ( short ) __num__ @$ __num__ ) ; dfs test util . create file ( cluster . get file system ( ) @$ new path ( __str__ ) @$ __num__ @$ __num__ @$ __num__ @$ ( short ) __num__ @$ __num__ ) ;,storage has
nosonar expose internal <PLACE_HOLDER>,return decorations ;,nosonar expose
<PLACE_HOLDER> for a non java jndi resource @$ inject factory which does a true jndi <PLACE_HOLDER>,managed reference factory = new managed reference factory ( ) { @ override public managed reference get reference ( ) { try { return new immediate managed reference ( new initial context ( ) . lookup ( lookup name ) ) ; } catch ( naming exception e ) { ee logger . root_logger . tracef ( e @$ __str__ @$ lookup name ) ; return null ; } } } ;,which does
<PLACE_HOLDER> 's equality does n't consider seq so if only seq number changes in resolved override <PLACE_HOLDER> . therefore <PLACE_HOLDER> container does n't change merged override <PLACE_HOLDER> @$ but it 's used to push <PLACE_HOLDER> changes so explicitly update that .,if ( get merged override configuration ( ) . seq != get resolved override configuration ( ) . seq ) { on merged override configuration changed ( ) ; },container change
now asynchronously request a <PLACE_HOLDER> that initial fails .,when ( work executor . request checkpoint ( ) ) . then return ( null ) ; progress updater . request checkpoint ( ) ;,now request
if added only one computed flow reference @$ then recovered a good <PLACE_HOLDER> .,return found count . get ( ) == __num__ ;,then recovered
for any <PLACE_HOLDER>s in the controller callback @$ test browser callback should have overridden the <PLACE_HOLDER> and call matching api in the callback proxy .,for ( int i = __num__ ; i < methods . length ; i ++ ) { assert not equals ( __str__ + methods [ i ] + __str__ @$ browser callback . class @$ methods [ i ] . get declaring class ( ) ) ; assert not equals ( __str__ + methods [ i ] + __str__ @$ controller callback . class @$ methods [ i ] . get declaring class ( ) ) ; },callback overridden
if the parent has a default <PLACE_HOLDER> @$ copy that default <PLACE_HOLDER> an ded with the umask as the new file 's access <PLACE_HOLDER> . if it is a metadata load operation @$ do not consider the umask .,default access control list d acl = current inode directory . get defaultacl ( ) ; short mode = context . is metadata load ( ) ? mode . create full access ( ) . to short ( ) : new file . get mode ( ) ; if ( ! d acl . is empty ( ) ) { access control list acl = d acl . generate child fileacl ( mode ) ; new file . set internal acl ( acl ) ; } if ( file context . is cacheable ( ) ) { new file . set cacheable ( true ) ; } if ( file context . get write type ( ) == write type . async_through ) { new file . set,parent has
zip entry does n't override <PLACE_HOLDER>,assert equals ( entry . get name ( ) @$ entries . next element ( ) . get name ( ) ) ;,entry override
dispatch message received <PLACE_HOLDER>,if ( context . get process engine configuration ( ) . get event dispatcher ( ) . is enabled ( ) ) { delegate execution execution = process instance . get executions ( ) . get ( __num__ ) ; activiti event dispatcher event dispatcher = context . get process engine configuration ( ) . get event dispatcher ( ) ; event dispatcher . dispatch event ( activiti event builder . create message received event ( execution @$ message name @$ null @$ variables ) ) ; },message received
conditions to reset the loop <PLACE_HOLDER>,if ( break loop || end of arguments ( ) || loop count >= get end index ( ) ) { reset break loop ( ) ; reset loop count ( ) ; return null ; },conditions reset
do not reset the partial flag for local tracker @$ as here the local tracker only know the <PLACE_HOLDER> which are modified in this file .,procedure map . merge ( local procedure map ) ;,tracker know
node has both left and right <PLACE_HOLDER> @$ find the minimum of the two .,t left child value = _values . get ( left child index ) ; t right child value = _values . get ( right child index ) ; if ( compare ( left child value @$ right child value ) <= __num__ ) { min index = left child index ; } else { min index = right child index ; },node has
temp array holds those <PLACE_HOLDER> we know we want to keep,if ( len != __num__ ) { int newlen = __num__ ; object [ ] temp = new object [ len ] ; for ( int i = __num__ ; i < len ; ++ i ) { object element = elements [ i ] ; if ( c . contains ( element ) ) temp [ newlen ++ ] = element ; } if ( newlen != len ) { set array ( arrays . copy of ( temp @$ newlen ) ) ; return true ; } },array holds
the leader executes the following <PLACE_HOLDER> @$ which essentially shuts down the peer if it is not the last round .,if ( v . get id ( ) == i ) { log . info ( __str__ @$ i ) ; if ( lc < this . total rounds ) { log . info ( __str__ @$ i ) ; fast leader election election = ( fast leader election ) peer . get election alg ( ) ; election . shutdown ( ) ; assert equals ( - __num__ @$ election . get vote ( ) . get id ( ) ) ; log . info ( __str__ @$ i ) ; break ; } },leader executes
the second update should n't cause a item <PLACE_HOLDER>,model = new model with click listener_ ( ) ; model . click listener ( model click listener ) ; controller . set model ( model ) ; lifecycle helper . build models and bind ( controller ) ; model = new model with click listener_ ( ) ; model . click listener ( view click listener ) ; controller . set model ( model ) ; lifecycle helper . build models and bind ( controller ) ; verify ( observer mock @$ times ( __num__ ) ) . on item range changed ( eq ( __num__ ) @$ eq ( __num__ ) @$ any ( ) ) ; verify no more interactions ( observer mock ) ;,update cause
assert that the table created still has no hcat <PLACE_HOLDER>,table table2 = client . get table ( __str__ @$ __str__ ) ; assert . assert true ( table2 . get sd ( ) . get input format ( ) . equals ( h cat constants . hive_rcfile_if_class ) ) ; driver . run ( __str__ ) ;,table has
by jmf . if the directory does not exist or it does not contain a jmf.properties file . or if the jmf.properties file has 0 <PLACE_HOLDER> then this is the first time we 're running and should continue to with jmf init,string home dir = system . get property ( __str__ ) ; file jmf dir = new file ( home dir @$ __str__ ) ; string classpath = system . get property ( __str__ ) ; classpath += system . get property ( __str__ ) + jmf dir . get absolute path ( ) ; system . set property ( __str__ @$ classpath ) ; if ( ! jmf dir . exists ( ) ) jmf dir . mkdir ( ) ; file jmf properties = new file ( jmf dir @$ __str__ ) ; if ( ! jmf properties . exists ( ) ) { try { jmf properties . create new file ( ) ; } catch ( io exception ex ) { system . out,file has
let the service <PLACE_HOLDER> callbacks handle the <PLACE_HOLDER> .,if ( m has telephony ) return ; boolean airplane mode on = settings . global . get int ( m context . get content resolver ( ) @$ settings . global . airplane_mode_on @$ __num__ ) == __num__ ; m airplane state = airplane mode on ? toggle action . state . on : toggle action . state . off ; m airplane mode on . update state ( m airplane state ) ;,callbacks handle
create three java rules @$ : dep <PLACE_HOLDER> @$ : dep 2 @$ and : lib . : lib depends on : dep <PLACE_HOLDER> and : dep 2 .,build target java dep1 build target = build target factory . new instance ( __str__ ) ; target node < ? > java dep1 node = java library builder . create builder ( java dep1 build target ) . add src ( paths . get ( __str__ ) ) . build ( ) ; build target java dep2 build target = build target factory . new instance ( __str__ ) ; target node < ? > java dep2 node = java library builder . create builder ( java dep2 build target ) . add src ( paths . get ( __str__ ) ) . build ( ) ; build target java lib build target = build target factory . new instance ( __str__ ) ; target node <,rules dep
as part of the process of launching @$ activity thread also performs a <PLACE_HOLDER> .,if ( and resume && ready to resume ( ) ) { stack . minimal resume activity locked ( r ) ; } else { if ( debug_states ) slog . v ( tag_states @$ __str__ + r + __str__ ) ; r . set state ( paused @$ __str__ ) ; },thread performs
else : the command itself @$ such as a shutdown task @$ might have cancelled all the scheduled <PLACE_HOLDER> already .,future = fake clock . get scheduled executor service ( ) . schedule ( wrap @$ delay @$ time unit ) ;,itself cancelled
if the child has an <PLACE_HOLDER> and a behavior @$ let it save some state ...,if ( child id != no_id && b != null ) { parcelable state = b . on save instance state ( this @$ child ) ; if ( state != null ) { behavior states . append ( child id @$ state ) ; } },child has
if another thread has recomputed the <PLACE_HOLDER> @$ no need to recompute again .,synchronized ( this ) { if ( m should update . get ( ) ) { m version = compute ( ) ; m should update . set ( false ) ; } },thread recomputed
insert into its base table get the generated id update <PLACE_HOLDER>,try { return run task ( new query task < long > ( ) { @ override public long handle ( connection connection ) throws exception { pojo info pojo info = pojo info map . get ( pojo . get class ( ) ) ; abstract json entity generic json entity = new generic json entity ( ) ; generic json entity . set create time ( new timestamp ( system . current time millis ( ) ) ) ; generic json entity . set update time ( new timestamp ( system . current time millis ( ) ) ) ; generic json entity . set version ( __num__ ) ; generic json entity . set bean class ( pojo . get class ( ) . get,id update
frame can hold the entire display . use exact <PLACE_HOLDER> .,if ( frame width >= display width && frame height >= display height ) { return new recording info ( display width @$ display height @$ camera frame rate @$ display density ) ; },frame hold
note : the legacy jni code used to do a query right after a load success to synchronize the service cache . instead store the <PLACE_HOLDER> that was requested to load to update the cache later without doing a query .,if ( result == context hub transaction . result_success ) { m nano app state manager . add nano app instance ( context hub id @$ nano app binary . get nano app id ( ) @$ nano app binary . get nano app version ( ) ) ; },note store
because there is no need to expand the visual range @$ no row or spacer contents get <PLACE_HOLDER> . all rows @$ spacers @$ and scroll position simply need to be shifted down accordingly and the spacer indexes need updating .,spacer container . update spacer indexes for row and after ( index @$ old top row logical index + visual row order . size ( ) @$ number of rows ) ;,contents get
the first and second streams in a reprocessable combination have the same <PLACE_HOLDER> and format . the first is the input and the second is the output used for generating the subsequent input buffers .,if ( is reprocessable ) { array list < size > input size = new array list < size > ( ) ; int format ; if ( comb template . m reprocess type == reprocess type . private ) { input size . add ( max private input size ) ; format = image format . private ; } else { input size . add ( maxyuv input size ) ; format = image format . yuv_420_888 ; } streams info . add ( new mandatory stream information ( input size @$ format @$ true ) ) ; streams info . add ( new mandatory stream information ( input size @$ format ) ) ; },streams have
subsequent discoveries should not find <PLACE_HOLDER>,list < kafka topic partition > second discovery = partition discoverer . discover partitions ( ) ; list < kafka topic partition > third discovery = partition discoverer . discover partitions ( ) ; assert equals ( __num__ @$ second discovery . size ( ) ) ; assert equals ( __num__ @$ third discovery . size ( ) ) ;,discoveries find
make sure null set of anchors throws null pointer <PLACE_HOLDER>,try { pkix builder parameters p = new pkix builder parameters ( ( set ) null @$ null ) ; throw new exception ( __str__ ) ; } catch ( null pointer exception npe ) { },set throws
load default font <PLACE_HOLDER> .,default font size = config service . get string ( __str__ ) ;,default font
prime the connection to allow client and server <PLACE_HOLDER> to be exchanged .,content response response = client . new request ( __str__ @$ connector . get local port ( ) ) . path ( __str__ ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( http status . ok_200 @$ response . get status ( ) ) ; org . eclipse . jetty . client . api . request request = client . new request ( __str__ @$ connector . get local port ( ) ) . method ( http method . head ) . path ( __str__ ) ; request . send ( result -> { if ( result . is failed ( ) ) latch . count down ( ) ; } ) ; assert true ( stream latch,connection allow
check whether each account matches the requested <PLACE_HOLDER>,m accounts with features = new array list < account > ( m accounts of type . length ) ; m current account = __num__ ; check account ( ) ;,account matches
if all of the options come from the same package @$ show the application 's label and icon instead of the generic resolver 's . some calls like intent.resolve activity info query the resolve info from here and then throw away the resolve info itself @$ meaning that the caller loses the resolve package name . therefore the activity info.label res above provides a,final string intent package = intent . get package ( ) ; if ( ! text utils . is empty ( intent package ) && all have package ( query @$ intent package ) ) { final application info appi = query . get ( __num__ ) . activity info . application info ; ri . resolve package name = intent package ; if ( user needs badging ( user id ) ) { ri . no resource id = true ; } else { ri . icon = appi . icon ; } ri . icon resource id = appi . icon ; ri . label res = appi . label res ; } ri . activity info . application info = new application info ( ri,res provides
now do a <PLACE_HOLDER> that ensures stuff works when we go over block boundary @$ especially that we return good length on file .,final byte [ ] value = new byte [ __num__ * __num__ ] ;,now do
this color <PLACE_HOLDER> list does not declare an activated <PLACE_HOLDER> so this should not yield a change,boolean changed = result . on state changed ( new int [ ] { android . r . attr . state_activated } ) ; assert false ( changed ) ; assert equals ( result . get color ( ) @$ csl . get default color ( ) ) ;,list declare
if the method has <PLACE_HOLDER> @$ skip it,try { if ( method . is empty ( ) || method . get parameter types ( ) . length != __num__ ) { continue ; } } catch ( not found exception e ) { continue ; },method has
uri parsing has <PLACE_HOLDER> .,try { uri oracle rac = uri . create ( __str__ + __str__ + __str__ + __str__ ) ; logger . debug ( oracle rac . to string ( ) ) ; logger . debug ( oracle rac . get scheme ( ) ) ; assert . fail ( ) ; } catch ( exception e ) { },parsing has
new collection has one less <PLACE_HOLDER>,int new row location = get celly ( __num__ ) ; assert close to ( __str__ @$ new row location @$ row location ) ;,collection has
inclusions are handled like the first entry on the quote include <PLACE_HOLDER>,artifact include file = locate relative ( inclusion . get inclusion ( ) @$ path to legal output artifact @$ source ) ; int context path pos = __num__ ; kind context kind = null ; check for interrupt ( __str__ @$ source ) ;,entry include
subclass might have overriden set <PLACE_HOLDER>,this . seed = new atomic long ( ) ; set seed ( seed ) ;,subclass have
start a container that listens on a poke port @$ and once poked runs a web <PLACE_HOLDER>,final job job = job . new builder ( ) . set name ( test job name ) . set version ( test job version ) . set image ( nginx ) . set command ( as list ( __str__ @$ __str__ @$ __str__ ) ) . add port ( __str__ @$ port mapping . of ( __num__ ) ) . add port ( __str__ @$ port mapping . of ( __num__ ) ) . add registration ( service endpoint . of ( __str__ @$ __str__ ) @$ service ports . of ( __str__ ) ) . set health check ( health check ) . build ( ) ; assert container registers after poke ( client @$ job ) ;,poked runs
could n't find a named 'event bus ' event bus object . try to find the first typed <PLACE_HOLDER> we can :,for ( object v : objects . values ( ) ) { if ( v instanceof event bus ) { return ( event bus ) v ; } } return null ;,bus find
<PLACE_HOLDER> for cert store is collection type @$ using collection with crl create <PLACE_HOLDER>,collection cert store parameters params = new collection cert store parameters ( crl collection ) ;,collection create
deserialize record may return a <PLACE_HOLDER> if there is no more data . however @$ when we are deserializing an edit @$ we do so only when we know that we should have data . this is why the java docs for this method on the interface indicate that this method should never return <PLACE_HOLDER> . as a result @$ if there is no,throw new eof exception ( ) ;,method return
' y ' does n't have a <PLACE_HOLDER> to shadow due to upward referencing .,test ( __str__ + __str__ @$ __str__ + __str__ ) ; test ( __str__ + __str__ @$ __str__ + __str__ ) ; test ( __str__ + __str__ @$ __str__ + __str__ ) ;,"" have
nn should have received a new <PLACE_HOLDER>,long nn image after = nn . getfs image ( ) . get storage ( ) . get most recent checkpoint tx id ( ) ; assert true ( __str__ + nn image before + __str__ + nn image after @$ nn image after > nn image before ) ;,nn received
if we could n't find the region because the cache is closed @$ throw a cache closed <PLACE_HOLDER>,if ( rgn == null ) { if ( cache . is closed ( ) ) { throw new cache closed exception ( ) ; } throw new region not found exception ( string . format ( __str__ @$ this . region path ) ) ; },cache closed
<PLACE_HOLDER> could imagine an empty <PLACE_HOLDER> otherwise,if ( length < __num__ ) { throw new illegal argument exception ( ) ; },one imagine
root states should only have <PLACE_HOLDER>,for ( string root : start symbols ) { if ( new state split counts . get count ( root ) > __num__ ) { new state split counts . set count ( root @$ __num__ ) ; } } if ( new state split counts . get count ( lexicon . boundary_tag ) > __num__ ) { new state split counts . set count ( lexicon . boundary_tag @$ __num__ ) ; } state split counts = new state split counts ;,states have
force line <PLACE_HOLDER> force line <PLACE_HOLDER> force line <PLACE_HOLDER> force line <PLACE_HOLDER>,platform mapping function . platform mapping exception exception = assert throws ( platform mapping function . platform mapping exception . class @$ ( ) -> parse ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,line break
if needed @$ update the maximum stack <PLACE_HOLDER> and number of locals @$ and stack map frames .,if ( current basic block != null && ( compute == compute_all_frames || compute == compute_inserted_frames ) ) { current basic block . frame . execute ( opcodes . iinc @$ var @$ null @$ null ) ; } if ( compute != compute_nothing ) { int current max locals = var + __num__ ; if ( current max locals > max locals ) { max locals = current max locals ; } },maximum stack
if reference identifies a <PLACE_HOLDER> @$ use exclusively,if ( f != null ) { factory = get object factory from reference ( ref @$ f ) ; if ( factory != null ) { return factory . get object instance ( ref @$ name @$ name ctx @$ environment ) ; } return ref info ; } else { answer = processurl addrs ( ref @$ name @$ name ctx @$ environment ) ; if ( answer != null ) { return answer ; } },reference identifies
because kie scanner also check the local <PLACE_HOLDER> @$ ensure the artifact are not deployed on the local maven <PLACE_HOLDER>,repository . remove local artifact ( release id1 ) ; repository . remove local artifact ( release id2 ) ; release id release range = ks . new release id ( __str__ @$ artifact id @$ __str__ ) ; internal kie module k jar1 = create kie jar ( ks @$ release id1 @$ __str__ @$ __str__ ) ; kie container kie container = ks . new kie container ( release range ) ; list < kie scanner event > events = new array list < > ( ) ; internal kie scanner scanner = ( kie repository scanner impl ) ks . new kie scanner ( kie container ) ; kie scanner event listener listener = new kie scanner event listener ( ) { public void on,scanner check
ttl tags specify <PLACE_HOLDER> in milliseconds,region . put ( new put ( row ) . add ( new key value ( row @$ fam1 @$ q3 @$ now + __num__ - __num__ @$ h constants . empty_byte_array @$ new array backed tag [ ] { new array backed tag ( tag type . ttl_tag_type @$ bytes . to bytes ( __num__ ) ) } ) ) ) ;,tags specify
cut the connection @$ so the server will create close <PLACE_HOLDER> as part of expiring the <PLACE_HOLDER> .,client . dont reconnect ( ) ; client . disconnect ( ) ; watcher . reset ( ) ;,server create
recurring alarms may have passed several alarm <PLACE_HOLDER> while the alarm was kept pending . send the appropriate trigger count .,if ( alarm . repeat interval > __num__ ) { alarm . count += ( nowelapsed - alarm . expected when elapsed ) / alarm . repeat interval ; final long delta = alarm . count * alarm . repeat interval ; final long next elapsed = alarm . expected when elapsed + delta ; set impl locked ( alarm . type @$ alarm . when + delta @$ next elapsed @$ alarm . window length @$ max trigger time ( nowelapsed @$ next elapsed @$ alarm . repeat interval ) @$ alarm . repeat interval @$ alarm . operation @$ null @$ null @$ alarm . flags @$ true @$ alarm . work source @$ alarm . alarm clock @$ alarm . uid @$ alarm . package,alarms passed
consume all events ; signal that our text area will handle <PLACE_HOLDER>,e . consume ( ) ;,area handle
count down every time a thread finishes <PLACE_HOLDER> @$ so that we can wait for <PLACE_HOLDER> to complete,final count down latch work finished latch = new count down latch ( num samples ) ;,thread finishes
jvms : if the <PLACE_HOLDER> is finite and the divisor is an infinity @$ the result equals the <PLACE_HOLDER> . jvms : if the <PLACE_HOLDER> is a zero and the divisor is finite @$ the result equals the <PLACE_HOLDER> .,if ( x == __num__ || float . is infinite ( y ) ) { return x ; } float result = safe rem ( x @$ y ) ;,result equals
assume that attributes and namespace nodes immediately follow the <PLACE_HOLDER> .,int identity = make node identity ( node handle ) ; while ( dtm . null != ( identity = get next node identity ( identity ) ) ) { int type = _type ( identity ) ; if ( type == dtm . namespace_node ) { return make node handle ( identity ) ; } else if ( type != dtm . attribute_node ) { break ; } },attributes follow
assertion that accreditations collection table got cleaned up if they did n't @$ the delete should have caused a constraint <PLACE_HOLDER> @$ but just to be sure ...,session s = open session ( ) ; s . begin transaction ( ) ; s . do work ( new work ( ) { @ override public void execute ( connection connection ) throws sql exception { final statement statement = connection . create statement ( ) ; final result set result set = statement . execute query ( __str__ ) ; assert true ( result set . next ( ) ) ; final int count = result set . get int ( __num__ ) ; assert equals ( __num__ @$ count ) ; } } ) ; s . get transaction ( ) . commit ( ) ; s . close ( ) ;,delete caused
now p 's cache still only has key <PLACE_HOLDER> @$ key 3 @$ key 5,byte [ ] r_rvv_bytes = getrvv byte array ( r @$ region_name ) ; close cache ( r ) ;,cache has
second deployment should have two available <PLACE_HOLDER> . the first job should have two available xml descriptors the second job should only have one descriptor .,address = operations . create address ( __str__ @$ deployment_name_2 @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ; validate job xml names ( address @$ __str__ @$ __str__ ) ; address = operations . create address ( __str__ @$ deployment_name_2 @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ; validate job xml names ( address @$ __str__ ) ;,job have
the above transaction now contain the changes we want to expose to the index updater as updates . this will happen when we commit the transaction . the transaction now also holds the schema read <PLACE_HOLDER> @$ so we ca n't begin creating our constraint just yet . we first have to un<PLACE_HOLDER> the schema @$ and then b<PLACE_HOLDER> just before we send off,lock lock blocking data change transaction = get lock service ( ) . acquire node lock ( block data change transaction on lock on id @$ lock type . write_lock ) ;,thread take
a <PLACE_HOLDER> resource ca n't use <PLACE_HOLDER> system api or we get an infinite loop since <PLACE_HOLDER> system uses configuration api . use java.io.<PLACE_HOLDER> instead .,if ( resource instanceof path ) { file file = new file ( ( ( path ) resource ) . to uri ( ) . get path ( ) ) . get absolute file ( ) ; if ( file . exists ( ) ) { if ( ! quiet ) { log . debug ( __str__ + file ) ; } reader = ( xml stream reader2 ) parse ( new buffered input stream ( files . new input stream ( file . to path ( ) ) ) @$ ( ( path ) resource ) . to string ( ) @$ is restricted ) ; } } else if ( resource instanceof input stream ) { reader = ( xml stream reader2 ) parse ( (,resource use
some ipp printers like lexc 710 do not have <PLACE_HOLDER> of supported media but cups can get the media from ppd @$ so we still report as supported category .,if ( is cups printer ) { if ( ! cat list . contains ( media . class ) ) { cat list . add ( media . class ) ; } cat list . add ( media printable area . class ) ; cat list . add ( destination . class ) ; if ( ! print service lookup provider . is linux ( ) ) { cat list . add ( sheet collate . class ) ; } },printers have
render the world using the debug <PLACE_HOLDER>,renderer . render ( world @$ camera . combined ) ; float render time = ( time utils . nano time ( ) - start time ) / __num__ ; batch . begin ( ) ; font . draw ( batch @$ __str__ + gdx . graphics . get frames per second ( ) + __str__ + update time + __str__ + render time @$ __num__ @$ __num__ ) ; batch . end ( ) ;,world using
ca n't simply return null if server is not using shared <PLACE_HOLDER>uration @$ since we need to find out if the locator is running in secure mode or not @$ if yes @$ then we need to throw an exception if server is not using cluster <PLACE_HOLDER> .,map < internal distributed member @$ collection < string > > locators with cluster config = get distribution manager ( ) . get all hosted locators with shared configuration ( ) ;,server using
many tools @$ just keep the largest <PLACE_HOLDER>,analysis shared thread pool size = math . max ( analysis shared thread pool size @$ current tool size ) ;,tools keep
get the currency pattern for this locale . we have to fish it out of the resource bundle directly @$ since decimal format.to pattern will return the localized <PLACE_HOLDER> @$ not \00 a 4,string [ ] num patterns = ( string [ ] ) rb . get object ( __str__ ) ; string pattern = num patterns [ __num__ ] ; if ( pattern . index of ( __str__ ) == - __num__ ) { errln ( __str__ + locales [ i ] + __str__ + pattern ) ; },pattern return
this is important since some of the x content builders write <PLACE_HOLDER> on close . in order to write the footer we need to prevent closing the actual index input .,try ( output stream index output output stream = new index output output stream ( index output ) { @ override public void close ( ) { } } ) { bytes . write to ( index output output stream ) ; },some write
one shortcut when contracting node <PLACE_HOLDER>,add shortcut ( nodea @$ nodeb @$ e3toa . get edge ( ) @$ e3tob . get edge ( ) @$ e3toa . get edge ( ) @$ e3tob . get edge ( ) @$ __num__ ) ; set level equal to node id for all nodes ( ) ;,contracting node
first comment <PLACE_HOLDER> consists of exactly one character . so only <PLACE_HOLDER> break or <PLACE_HOLDER> feed can be left . delete <PLACE_HOLDER> .,if ( m_editable object . get length ( ) == __num__ ) { if ( y > __num__ ) { m_carety = y - __num__ ; final zy line content prev line content = get line content ( m_carety ) ; m_caretx = prev line content . get text ( ) . length ( ) ; } changed text = get multiline comment ( y @$ changed text ) ; } else { m_caretx = get caret end posx ( ) ; m_carety = get caret mouse releasedy ( ) ; return ; },line delete
if both threads finished already @$ assert that both threads throw <PLACE_HOLDER>,assert that ( ai1 . get exception ( ) instanceof unsupported operation exception ) . is true ( ) ; assert that ( ai2 . get exception ( ) instanceof unsupported operation exception ) . is true ( ) ;,threads throw
node might 've not have received the license cluster <PLACE_HOLDER>,if ( current license == null ) { return true ; },might received
arbitrary size matches bitmap <PLACE_HOLDER> .,byte [ ] storage = new byte [ __num__ * __num__ ] ; image decoder decoder = null ; try { decoder = n create ( is @$ storage @$ source ) ; } finally { if ( decoder == null ) { if ( close input stream ) { io utils . close quietly ( is ) ; } } else { decoder . m input stream = is ; decoder . m owns input stream = close input stream ; decoder . m temp storage = storage ; } } return decoder ;,size matches
fetcher has copied the input <PLACE_HOLDER> when object reuse is enabled,fetcher . flat map ( in @$ get fetcher collector ( ) ) ; if ( is left outer join && ! collector . is collected ( ) ) { out row . replace ( in @$ null row ) ; out row . set header ( in . get header ( ) ) ; out . collect ( out row ) ; },fetcher copied
how may patterns specify this <PLACE_HOLDER>,numfixed = get num fixed ( sbit @$ __num__ @$ context ) ;,patterns specify
verify false constant replaces the <PLACE_HOLDER> with the else expression .,assert translation ( translation @$ __str__ ) ;,verify replaces
count unprivileged <PLACE_HOLDER> for the same uid across networks .,int unprivileged count same uid = __num__ ; for ( final hash map < integer @$ keepalive info > ka for network : m keepalives . values ( ) ) { for ( final keepalive info ki : ka for network . values ( ) ) { if ( ki . m uid == m uid ) { unprivileged count same uid ++ ; } } } if ( unprivileged count same uid > m allowed unprivileged slots for uid ) { return error_insufficient_resources ; } return success ;,count unprivileged
without a copy @$ but gives this instance its own <PLACE_HOLDER> and limit .,this . content = content . as read only buffer ( ) ;,but gives
the client has departed . remove this last <PLACE_HOLDER> and unregister it .,boolean needs unregister = false ; synchronized ( chm lock ) { if ( chm registered ) { needs unregister = true ; chm registered = false ; } } if ( unregister client ) { clean client auths ( ) ; },departed remove
completing e should finish the lower <PLACE_HOLDER> and make ' h ' active,task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; org . flowable . task . api . task task = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . task name ( __str__ ) . single result ( ) ; assert not null ( task ) ;,e finish
we know root has 2 <PLACE_HOLDER> because we create 2 memory blocks in the set up .,f1 = ( program fragment ) groups [ __num__ ] ; f2 = ( program fragment ) groups [ __num__ ] ;,root has
this thread has escaped the blocked state <PLACE_HOLDER> the lock and a short wait before continue,try { lock . wait ( __num__ ) ; } catch ( interrupted exception e ) { interrupted . increment and get ( ) ; },thread escaped
note to translators : this message is used to indicate the severity of another message . the substitution text contains two error <PLACE_HOLDER> . the spacing before the second substitution text indents it the same amount as the first in english .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text contains
have we overflowed a dtm identity 's addressing <PLACE_HOLDER> ?,if ( m_dtm ident . size ( ) == ( node index > > > dtm manager . ident_dtm_node_bits ) ) { add newdtmid ( node index ) ; } m_firstch . add element ( can have first child ? notprocessed : dtm . null ) ; m_nextsib . add element ( notprocessed ) ; m_parent . add element ( parent index ) ; m_exptype . add element ( expanded typeid ) ; m_data orq name . add element ( data or prefix ) ; if ( m_prevsib != null ) { m_prevsib . add element ( previous sibling ) ; } if ( dtm . null != previous sibling ) { m_nextsib . set element at ( node index @$ previous sibling ) ; } if (,identity addressing
<PLACE_HOLDER> check should match default <PLACE_HOLDER> of that url,test implies ( thisurl @$ thaturl @$ true ) ; system . out . println ( __str__ ) ;,check match
node id : 3973 cb 8<PLACE_HOLDER> d 7 bef 9 c 9<PLACE_HOLDER> e 5 d 589<PLACE_HOLDER>01 d 788370 f 9 e 24<PLACE_HOLDER>70 dcba 0480 c 0 b 3 b 1 b 0<PLACE_HOLDER>47 d 13 d 0 f 0 fffed 115 dd 2 d 4 b 5 ca 1929287839 dcd 4 e 77 bdc 724302 b 44 ae 48<PLACE_HOLDER>22 a 87<PLACE_HOLDER><PLACE_HOLDER> ee <PLACE_HOLDER>,sys prop configa . props . override params ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,8766 ee
yolo pick the first <PLACE_HOLDER> !,return this ;,yolo pick
if the genrule rewrites the source map @$ we have to create the parent dir @$ and record the build <PLACE_HOLDER>,if ( rewrite sourcemap ) { source path source path to source map = get source path to source map ( ) ; buildable context . record artifact ( source path resolver adapter . get relative path ( source path to source map ) ) ; builder . add ( mkdir step . of ( build cell relative path . from cell relative path ( context . get build cell root path ( ) @$ get project filesystem ( ) @$ source path resolver adapter . get relative path ( source path to source map ) . get parent ( ) ) ) ) ; },dir build
arizona does not observe <PLACE_HOLDER> @$ so even phoenix and denver have the same raw offest @$ they have different rules .,a = time zone . get time zone ( __str__ ) ; b = time zone . get time zone ( __str__ ) ; assert equals ( a . get raw offset ( ) @$ b . get raw offset ( ) ) ; assert false ( a . has same rules ( b ) ) ;,arizona observe
only process if curi contains <PLACE_HOLDER> of fetch attempt,return uri . contains data key ( a_fetch_began_time ) && uri . get recorder ( ) != null && uri . get recorder ( ) . get response content length ( ) >= get lower bound ( ) && uri . get recorder ( ) . get response content length ( ) <= get upper bound ( ) ;,curi contains
if the page has set <PLACE_HOLDER> @$ sonic will set the <PLACE_HOLDER> to kernel .,long start time = system . current time millis ( ) ; map < string @$ list < string > > header fields map = server . get response header fields ( ) ; if ( sonic utils . should log ( log . debug ) ) { sonic utils . log ( tag @$ log . debug @$ __str__ + s id + __str__ + ( system . current time millis ( ) - start time ) + __str__ ) ; } start time = system . current time millis ( ) ; set cookies from headers ( header fields map @$ should set cookie asynchronous ( ) ) ; if ( sonic utils . should log ( log . debug ) ) { sonic utils .,sonic set
clipping handles opaque <PLACE_HOLDER> .,return mask_none ;,clipping handles
for read lock @$ if the caller has locked the same <PLACE_HOLDER> previously @$ it will not try to acquire the same read lock . it simply returns the previous <PLACE_HOLDER> lock .,if ( read lock ) { row lock impl prev row lock impl = ( row lock impl ) prev row lock ; if ( ( prev row lock impl != null ) && ( prev row lock impl . get lock ( ) == row lock context . read write lock . read lock ( ) ) ) { success = true ; return prev row lock ; } result = row lock context . new read lock ( ) ; } else { result = row lock context . new write lock ( ) ; },caller locked
consume native enter <PLACE_HOLDER> if we generate one,if ( id == mouse event . mouse_entered ) { e . consume ( ) ; },native enter
add in reverse order so we can tell that sorting actually changed <PLACE_HOLDER>,points . add ( second ) ; points . add ( first ) ; collections . sort ( points ) ; assert with message ( string . format ( __str__ + __str__ @$ points . index of ( second ) @$ points . index of ( first ) ) ) . that ( points . index of ( second ) ) . is greater than ( points . index of ( first ) ) ;,sorting changed
add snowplow <PLACE_HOLDER> to context @$ contributors can use this <PLACE_HOLDER> to emit other event,emitter = telemetry emitter . builder ( ) . build ( ) ; root context . put ( emitter . class @$ emitter ) ;,contributors use
check that the api handles the single <PLACE_HOLDER>,final string multipart single api = files . get ( __str__ ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ;,api handles
if so use sentences to get so <PLACE_HOLDER> from sentences,if ( sentences != null && options . coreference context size > __num__ ) { list < core label > tokens = sentences . get ( mention . sent num - __num__ ) . get ( core annotations . tokens annotation . class ) ; int context start = math . max ( mention . start index - __num__ - __num__ @$ __num__ ) ; int context end = math . min ( mention . end index - __num__ + __num__ @$ tokens . size ( ) ) ; string left context = string utils . join words ( tokens @$ __str__ @$ context start @$ mention . start index - __num__ ) ; string right context = string utils . join words ( tokens @$ __str__ @$,sentences get
holder of null state since concurrent hash map does not support null <PLACE_HOLDER>,atomic boolean seen null = new atomic boolean ( false ) ; concurrent hash map < t @$ boolean > map = new concurrent hash map < > ( ) ; terminal op < t @$ void > for each op = for each ops . make ref ( t -> { if ( t == null ) seen null . set ( true ) ; else map . put if absent ( t @$ boolean . true ) ; } @$ false ) ; for each op . evaluate parallel ( helper @$ spliterator ) ;,holder support
we only cache get and head <PLACE_HOLDER>,if ( ! exchange . get request method ( ) . equals ( get ) && ! exchange . get request method ( ) . equals ( head ) ) { return false ; } if ( entry == null ) { this . response cachable = mark cacheable ; return false ; },cache get
if the left index has not reached the right <PLACE_HOLDER> of array must now sort the right partition .,if ( lo < hi0 ) { quick sort2 ( v @$ lo @$ hi0 ) ; },index reached
the file filter will tell us which files <PLACE_HOLDER> and which do n't .,try { final file filter filter = config . build file filter ( ) ; while ( ! list . is empty ( ) && state == running ) { final file current = list . poll ( ) ; task . scanned ++ ; if ( current . is file ( ) ) { task . matching ++ ; act on ( current ) ; } if ( current . is directory ( ) ) { final file [ ] content = current . list files ( filter ) ; if ( content == null ) continue ; list . add all ( __num__ @$ arrays . as list ( content ) ) ; } } if ( list . is empty ( ) ) { task,which files
if the option accepts <PLACE_HOLDER> and a non blank argname,if ( option . accept value ( ) && ( option . get arg name ( ) == null || option . get arg name ( ) . length ( ) != __num__ ) ) { buff . append ( is null or empty ( option . get short name ( ) ) ? get long option separator ( ) : __str__ ) ; buff . append ( __str__ ) . append ( option . get arg name ( ) != null ? option . get arg name ( ) : get arg name ( ) ) . append ( __str__ ) ; },option accepts
intent filter verification is only for filters that specify a host . so do n't return <PLACE_HOLDER> that handle all web uris .,if ( ri target user . handle all web datauri ) { continue ; } string package name = ri target user . activity info . package name ; package setting ps = m settings . m packages . get ( package name ) ; if ( ps == null ) { continue ; } long verification state = get domain verification statusl pr ( ps @$ parent user id ) ; int status = ( int ) ( verification state > > __num__ ) ; if ( result == null ) { result = new cross profile domain info ( ) ; result . resolve info = create forwarding resolve info unchecked ( new intent filter ( ) @$ source user id @$ parent user id ),verification return
wraps sticky or default . then has next and next just call the <PLACE_HOLDER> .,if ( is addr disposition retry ) { return true ; } boolean result ; if ( primary to contact info != null ) { result = primary to contact info . has next ( primary contact info @$ previous contact info @$ list of contact infos ) ; } else { result = effective targetior iterator . has next ( ) ; } return result ;,sticky call
we now should only have zero since the disk store had no <PLACE_HOLDER> remaining in it .,file count = __num__ ; for ( file disk dir : disk dirs ) { file [ ] files = disk dir . list files ( ) ; file count += files . length ; } assert that ( file count ) . is equal to ( __num__ ) ;,store had
custom deserializer always produces <PLACE_HOLDER> like this :,assert equals ( __str__ @$ bean . a ) ; assert equals ( __str__ @$ bean . b ) ;,deserializer produces
server does not support range <PLACE_HOLDER> .,test url response info = create url response info ( __num__ ) ;,server support
since task resource has been closed @$ broker should return a new <PLACE_HOLDER> of the object,test factory . shared resource task resource2 = task broker . get shared resource at scope ( new test factory < gobblin scope types > ( ) @$ new test resource key ( __str__ ) @$ gobblin scope types . task ) ; assert . assert not equals ( task resource @$ task resource2 ) ; top broker . close ( ) ; assert . assert true ( job resource . is closed ( ) ) ; assert . assert true ( task resource . is closed ( ) ) ;,broker return
create a walker which walks the <PLACE_HOLDER> in a dfs manner while maintaining the operator stack . the dispatcher generates the plan from the operator <PLACE_HOLDER>,set multimap < integer @$ node processor > ast node to processor = hash multimap . create ( ) ; ast node to processor . put ( hive parser . tok_null @$ tf . get null expr processor ( ) ) ; ast node to processor . put ( hive parser . number @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . integral literal @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . number literal @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . identifier @$ tf . get str expr processor,which walks
class extends or implements <PLACE_HOLDER>,case class_extends : print ( __str__ ) ; print ( pos . type_index ) ; break ;,class extends
use previously defined job entry <PLACE_HOLDER> !,if ( je . get nr ( ) > __num__ ) { je . set entry ( prev . get entry ( ) ) ; prev = find job entry ( je . get name ( ) @$ je . get nr ( ) @$ true ) ; if ( prev != null ) { int idx = index of job entry ( prev ) ; remove job entry ( idx ) ; } },use defined
since behavior files might contain language <PLACE_HOLDER> that are n't present in the class file @$ we might need to update the feature set .,if ( cls . features != null ) { node script node = node util . get enclosing script ( parent ) ; feature set old features = ( feature set ) script node . get prop ( node . feature_set ) ; feature set new features = old features . union ( cls . features ) ; if ( ! new features . equals ( old features ) ) { script node . put prop ( node . feature_set @$ new features ) ; compiler . report change to change scope ( script node ) ; } } if ( node util . is name declaration ( expr root ) ) { node assign expr = var to assign ( expr root ) ; parent . replace,files contain
we do n't use do unchecked here as there is a chance the below method can run <PLACE_HOLDER> supplied code,if ( wild fly security manager . is checking ( ) ) { access controller . do privileged ( ( privileged action < object > ) ( ) -> { session destroyed impl ( se ) ; return null ; } ) ; } else { session destroyed impl ( se ) ; },method run
these tests do n't handle <PLACE_HOLDER> from local parallel,session . session builder session builder = test session builder ( ) . set catalog ( catalog_id ) . set schema ( __str__ ) . set system property ( __str__ @$ __str__ ) ;,tests handle
noinspection object <PLACE_HOLDER> in loop,if ( insertion index >= key bucket . size ( ) ) { entry = key bucket . get entry ( insertion index - __num__ @$ key serializer @$ value serializer ) ; } else { entry = key bucket . get entry ( insertion index @$ key serializer @$ value serializer ) ; },noinspection object
log <PLACE_HOLDER> can not be loaded by the classloader which loaded the custom <PLACE_HOLDER> implementation . the custom implementation is not viable until this is corrected . ensure that the jcl jar and the custom class are available from the same classloader . running with diagnostics on should give information about the classloaders used to load the custom <PLACE_HOLDER> .,log diagnostic ( __str__ + __str__ ) ;,which loaded
the tree accepts a jmx <PLACE_HOLDER>,data flavor [ ] flavors = support . get data flavors ( ) ; for ( data flavor flavor : flavors ) { if ( flavor . is flavor java file list type ( ) ) { return true ; } },tree accepts
to take appropriate action when the activity looses <PLACE_HOLDER>,super . on pause ( ) ; mgl surface view . on pause ( ) ;,activity looses
one for event id set message <PLACE_HOLDER>,message . set message type ( message type . register_instantiators ) ; for ( int i = __num__ ; i < instantiators length ; i = i + __num__ ) { message . add bytes part ( this . serialized instantiators [ i ] ) ; message . add bytes part ( this . serialized instantiators [ i + __num__ ] ) ; message . add bytes part ( this . serialized instantiators [ i + __num__ ] ) ; } message . set transaction id ( __num__ ) ; message . add obj part ( this . get event id ( ) ) ; return message ;,one set
this time @$ we have seven display <PLACE_HOLDER> rad dude 's surfer gal 'replaces ' later 's surfer gal,map dids = get display names ( service ) ; iterator iter = dids . entry set ( ) . iterator ( ) ; int count = __num__ ; while ( iter . has next ( ) ) { ++ count ; entry e = ( entry ) iter . next ( ) ; logln ( __str__ + e . get key ( ) + __str__ + e . get value ( ) ) ; },time have
if the result of the phi has an existing <PLACE_HOLDER> @$ get it . otherwise @$ add it to the list of regs that need <PLACE_HOLDER> .,if ( ssa regs mapped . get ( result reg ) ) { map set . add ( mapper . old to new ( result reg ) ) ; } else { ssa regs . add ( result ) ; } for ( int i = __num__ ; i < sources size ; i ++ ) { register spec source = sources . get ( i ) ; ssa insn def = ssa meth . get definition for register ( source . get reg ( ) ) ; register spec source def = def . get result ( ) ; int source reg = source def . get reg ( ) ; if ( ssa regs mapped . get ( source reg ) ) { map set .,result has
store any attached <PLACE_HOLDER> in drive in a new folder .,if ( as object . first image ( ) != null ) { drive drive interface = get or create drive service ( auth data ) ; string folder id = idempotent executor . execute or throw exception ( __str__ @$ __str__ @$ ( ) -> create album folder ( drive interface ) ) ; for ( link value image : as object . image ( ) ) { try { string new img src = idempotent executor . execute and swallowio exceptions ( image . to string ( ) @$ __str__ @$ ( ) -> upload image ( ( as object ) image @$ drive interface @$ folder id ) ) ; content += __str__ + new img src + __str__ ; } catch ( runtime exception,any attached
some statistics for bitmap : 1 million distinct keys takes about 2 <PLACE_HOLDER> storage 5 million takes 10 <PLACE_HOLDER> 10 million takes 12 <PLACE_HOLDER>,return __num__ * __num__ * __num__ ;,statistics takes
do n't let the region attributes creation <PLACE_HOLDER> to the user,attributes factory factory = new attributes factory ( creation ) ; region attributes attrs = factory . create region attributes ( ) ; cache . set region attributes ( id @$ attrs ) ;,region attributes
websphere servlet which implements websocket <PLACE_HOLDER> @$ dynamically added,if ( servlet class name . equals ( __str__ ) ) { return false ; },which implements
cup cursor <PLACE_HOLDER> .,enter string ( __str__ + ( i + __num__ ) + __str__ + ( j + __num__ ) + __str__ ) ;,cup cursor
verify that the append returned <PLACE_HOLDER> due to timeotu,assert . assert equals ( satus @$ status . backoff ) ;,append returned
less common case : there are no valid omnibox suggestions . this can happen if the user tapped the url <PLACE_HOLDER> to dismiss the suggestions @$ then pressed enter . this can also happen if the user presses enter before any suggestions have been received from the autocomplete controller .,suggestion match = m autocomplete . classify ( url text ) ; suggestion match position = __num__ ;,user tapped
since null could be a valid <PLACE_HOLDER> @$ explicitly check whether map contains the <PLACE_HOLDER>,try { if ( ! serialized to original . contains key ( raw entry . get key ( ) ) ) { log . error ( __str__ + __str__ @$ raw entry . get key ( ) ) ; continue ; } map < string @$ t > orig key = serialized to original . get ( raw entry . get key ( ) ) ; schema and value deserialized schema and value = value converter . to connect data ( namespace @$ raw entry . get value ( ) != null ? raw entry . get value ( ) . array ( ) : null ) ; object deserialized value = deserialized schema and value . value ( ) ; offset utils . validate format ( deserialized,map contains
ok @$ this action contains a <PLACE_HOLDER> to the file containing the definition . such actions do n't have to have component alias defined .,if ( action . get component ( ) == null && ( action . get reference ( ) != null || action . get reference content ( ) != null ) ) { continue ; },action contains
the same variable was moved from one register to another . so add an <PLACE_HOLDER> for its old location .,if ( moved local != null ) { add or update end ( address @$ disposition . end_moved @$ moved local ) ; },variable add
extend is in left @$ top @$ right @$ bottom ; which is very uncommon @$ photon uses the <PLACE_HOLDER>,if ( extent . length == __num__ ) { this . extent = new b box ( extent [ __num__ ] @$ extent [ __num__ ] @$ extent [ __num__ ] @$ extent [ __num__ ] ) ; } else { throw new runtime exception ( __str__ + extent . length ) ; },photon uses
this is the second <PLACE_HOLDER> on row 3 @$ drag the first <PLACE_HOLDER> past the middle of this <PLACE_HOLDER> which causes this <PLACE_HOLDER> to move to the left edge,int startx = bf . get startx ( ) ; int width = bf . get width ( ) ; int middle first fieldx = startx / __num__ ; int past middle second fieldx = startx + width / __num__ + __num__ ; int row = __num__ ; drag field ( row @$ middle first fieldx @$ row @$ past middle second fieldx ) ; wait for swing ( ) ; bf = cb . get current field ( ) ; assert equals ( __num__ @$ bf . get startx ( ) ) ;,which causes
meta key key listener does n't track <PLACE_HOLDER> key state . need to check the key event instead .,boolean is ctrl active = ( ( event . get meta state ( ) & key event . meta_ctrl_on ) != __num__ ) ; boolean is shift active = ( get meta state ( content @$ meta_shift_on @$ event ) == __num__ ) ; boolean is alt active = ( get meta state ( content @$ meta_alt_on @$ event ) == __num__ ) ; if ( is ctrl active ) { if ( is alt active || is shift active ) { return false ; } return delete until word boundary ( view @$ content @$ is forward delete ) ; },listener track
chinese date format has pattern <PLACE_HOLDER>etter ' <PLACE_HOLDER> ' for <PLACE_HOLDER>eap month marker in addition to regu<PLACE_HOLDER>ar date format,cal . clear ( ) ; cal . set ( __num__ @$ calendar . june @$ __num__ ) ;,format has
if this server has not yet applied entries up to the <PLACE_HOLDER> 's session id @$ forward the query to the leader . this ensures that a follower does not tell the <PLACE_HOLDER> its session does n't exist if the follower has n't had a chance to see the session 's registration entry .,if ( raft . get last applied ( ) < request . session ( ) ) { return completable future . completed future ( log response ( query response . builder ( ) . with status ( raft response . status . error ) . with error ( raft error . type . unknown_session @$ __str__ ) . build ( ) ) ) ; },follower tell
this method automatically registers the <PLACE_HOLDER> with the given registry .,@ suppress warnings ( __str__ ) closingfs data input stream pis = closingfs data input stream . wrap safe ( test stream @$ ( safety net closeable registry ) registry @$ debug ) ;,method registers
the native counterpart will swallow <PLACE_HOLDER> exception . even if it did n't @$ it would still not use a logger instance to describe the exception . so describe it on the java side and rethrow it .,if ( t instanceof thread death ) throw ( thread death ) t ; else if ( t instanceof out of memory error ) throw ( out of memory error ) t ; else { logger . error ( __str__ + __str__ + signin name @$ t ) ; throw new runtime exception ( t ) ; },counterpart swallow
redis async commands extends redis <PLACE_HOLDER> async commands,if ( redis cluster async commands . class . is assignable from ( clazz ) ) { key += __str__ ; } else if ( redis cluster reactive commands . class . is assignable from ( clazz ) ) { key += __str__ ; } else { throw new illegal argument exception ( clazz . get name ( ) ) ; },commands extends
let 's not store 'raw ' <PLACE_HOLDER> but nodes,if ( value == null ) { value = null node ( ) ; },"s" store
common inner join result <PLACE_HOLDER> .,switch ( save join result ) { case match : equal key series hash map result indices [ equal key series count ] = hash map result count ; equal key series all match indices [ equal key series count ] = all match count ; equal key series is single value [ equal key series count ] = hash map results [ hash map result count ] . is single row ( ) ; equal key series duplicate counts [ equal key series count ] = __num__ ; all matchs [ all match count ++ ] = batch index ; break ; case spill : spills [ spill count ] = batch index ; spill hash map result indices [ spill count ] = hash map result,inner join
flush the <PLACE_HOLDER> @$ this is necessary for f str buffered <PLACE_HOLDER> .,_printer . flush ( ) ;,f str
convert the image source 's rectangle into the rotated and sheared space . once there @$ we calculate a rectangle that encloses the resulting shape . it is this rectangle which defines the <PLACE_HOLDER> of the buffered image we need to create to hold the transformed image .,rectangle2d . float src rect = new rectangle2d . float ( srcx @$ srcy @$ src width @$ src height ) ; shape rot shape = rot transform . create transformed shape ( src rect ) ; rectangle2d rot bounds = rot shape . get bounds2d ( ) ;,which defines
check row uses <PLACE_HOLDER> to check we are writing in order .,if ( ! check row ( cell ) ) { if ( start offset < __num__ ) { start offset = out . size ( ) ; } rows offsetbaos . write int ( out . size ( ) - start offset ) ; } last cell = cell ; return encoder . write ( cell ) ;,row uses
nightly testing showed some intermittent <PLACE_HOLDER> . check here to get diagnostic information if some strange behavior occurs .,check thread count ( expected count @$ current @$ __num__ ) ;,testing showed
publish topic name must not contain any <PLACE_HOLDER>,for ( char c : topic_wildcards ) { if ( topic name . index of ( c ) >= __num__ ) { return false ; } } return true ;,name contain
get more parameters depending on the kind of search class we 're working with . brute search does n't need anything else . locality sensitive hash search and projection searches need search <PLACE_HOLDER> . projection searches also need the number of projections .,boolean get search size = false ; boolean get num projections = false ; if ( ! searcher class . equals ( brute search . class . get name ( ) ) ) { get search size = true ; get num projections = true ; },search need
step light idle state locked does n't handle the active <PLACE_HOLDER> @$ so the state should stay as active .,verify light state conditions ( light_state_active ) ;,state handle
we do n't need to add the source file <PLACE_HOLDER> as it is a mandatory input .,if ( artifact != null ) { if ( ! artifact . equals ( source file ) ) { inputs . add ( artifact ) ; } continue ; },source file
if global contains <PLACE_HOLDER> @$ individual modules can only contain additional <PLACE_HOLDER> .,if ( ! root config . exclude . is empty ( ) && ! entry . get value ( ) . include . is empty ( ) ) { throw new illegal state exception ( string . format ( __str__ @$ entry . get key ( ) ) ) ; },modules contain
find installed font <PLACE_HOLDER> always returns an available font <PLACE_HOLDER>,return font tag . find installed font name ( get font name ( ) ) ;,name returns
test non zero <PLACE_HOLDER> .,iterator < element > it = vector . non zeroes ( ) . iterator ( ) ; int i = __num__ ; while ( it . has next ( ) ) { it . next ( ) ; ++ i ; },non zero
negative exit values are modulo <PLACE_HOLDER> :,for ( int exit : new int [ ] { - __num__ @$ - __num__ @$ - __num__ } ) { int expected = __num__ + exit ; string [ ] args = { __str__ @$ __str__ @$ __str__ + exit } ; bad exit status exception e = assert throws ( __str__ + expected @$ bad exit status exception . class @$ ( ) -> new command ( args ) . execute ( ) ) ; assert that ( e ) . has message that ( ) . is equal to ( __str__ + expected ) ; check command elements ( e @$ __str__ @$ __str__ @$ __str__ + exit ) ; termination status status = e . get result ( ) . get termination status (,values modulo
1 st root <PLACE_HOLDER>,if ( parent == null ) { long next = ( ( long ) phase << phase_shift ) | adj ; if ( state . compare and set ( this @$ s @$ next ) ) break ; } else { main lock . lock ( ) ; try { if ( state == s ) { parent . do register ( __num__ ) ; do { phase = ( int ) ( root . state > > > phase_shift ) ; } while ( ! state . compare and set ( this @$ state @$ ( ( long ) phase << phase_shift ) | adj ) ) ; break ; } } finally { main lock . unlock ( ) ; } },st root
next try the current class loader which loaded <PLACE_HOLDER> 6 s<PLACE_HOLDER>y,if ( result == null ) { result = p6 util . class . get class loader ( ) . get resource ( filename ) ; },which loaded
add a new routes that will handle endpoints form wire tap route <PLACE_HOLDER> .,camel context . add routes ( new route builder ( ) { @ override public void configure ( ) throws exception { from ( __str__ ) . log ( __str__ ) ; from ( __str__ ) . log ( __str__ ) ; } } ) ;,wire tap
view pager does not give its child views any callbacks when it moves content onto the screen @$ so we need to attach a listener to give us the <PLACE_HOLDER> that we require .,view parent view parent = litho view . get parent ( ) ; while ( view parent != null ) { if ( view parent instanceof view pager ) { final view pager view pager = ( view pager ) view parent ; final incremental mount helper . view pager listener view pager listener = new view pager listener ( m component tree @$ view pager ) ; try { view pager . add on page change listener ( view pager listener ) ; } catch ( concurrent modification exception e ) { view compat . post on animation ( view pager @$ new runnable ( ) { @ override public void run ( ) { view pager . add on page change listener ( view pager listener,pager give
shrink data buffer id <PLACE_HOLDER>,int [ ] new data buffer id table = new int [ new index count ] ; system . arraycopy ( data buffer id table @$ __num__ @$ new data buffer id table @$ __num__ @$ new data buffer id table . length ) ; data buffer id table = new data buffer id table ;,data buffer
check to see if the cipher even supports the <PLACE_HOLDER> before trying to instantiate it .,try { if ( ! match attribute ( service @$ attribute_modes @$ tokenized transformation [ __num__ ] ) || ! match attribute ( service @$ attribute_paddings @$ tokenized transformation [ __num__ ] ) ) { return null ; } cipher spi and provider sap = new cipher spi and provider ( ( cipher spi ) service . new instance ( null ) @$ service . get provider ( ) ) ; if ( sap . cipher spi == null || sap . provider == null ) { return null ; } cipher spi spi = sap . cipher spi ; if ( ( ( type == need to set . mode ) || ( type == need to set . both ) ) && ( tokenized transformation [,cipher supports
the rest api returns <PLACE_HOLDER> in decreasing order @$ but we want them in increasing order,collections . sort ( src versions ) ; for ( final integer src version : src versions ) { final versioned flow snapshot src flow snapshot = src client . get flow snapshot client ( ) . get ( src flow id @$ src version ) ; src flow snapshot . set flow ( null ) ; src flow snapshot . set bucket ( null ) ; final versioned flow snapshot metadata dest metadata = new versioned flow snapshot metadata ( ) ; dest metadata . set bucket identifier ( dest flow . get bucket identifier ( ) ) ; dest metadata . set flow identifier ( dest flow id ) ; dest metadata . set version ( src version ) ; dest metadata . set comments (,api returns
the insert caused <PLACE_HOLDER> @$ so rebalancing is done !,break ;,insert caused
this is a delayed chat room <PLACE_HOLDER> @$ a history <PLACE_HOLDER> for the room coming from server . lets check have we already shown this <PLACE_HOLDER> and if this is the case skip it otherwise save it as last seen delayed <PLACE_HOLDER>,if ( last seen delayed message == null ) { string timestamp = configuration utils . get chat room property ( provider @$ get identifier ( ) @$ last_seen_delayed_message_prop ) ; try { last seen delayed message = new date ( long . parse long ( timestamp ) ) ; } catch ( throwable t ) { } },check shown
a 'direct ' read actually has three phases . the first drains any remaining bytes from the slow read buffer . after this the read is guaranteed to be on a checksum chunk boundary . if there are still bytes to read @$ the fast direct path is used for as many remaining bytes as possible @$ up to a multiple of the checksum,if ( verify checksum ) { if ( slow read buff . has remaining ( ) ) { int from slow read buff = math . min ( buf . remaining ( ) @$ slow read buff . remaining ( ) ) ; write slice ( slow read buff @$ buf @$ from slow read buff ) ; n read += from slow read buff ; } if ( buf . remaining ( ) >= bytes per checksum && offset from chunk boundary == __num__ ) { int len = buf . remaining ( ) - ( buf . remaining ( ) % bytes per checksum ) ; len = math . min ( len @$ slow read buff . capacity ( ) ) ; int oldlimit =,which involves
weighted multi work unit weighted queue @$ the job will add new work <PLACE_HOLDER> to the queue along with a weight for each work unit . the queue will take care of balancing the work <PLACE_HOLDER> amongst a set number of multi work <PLACE_HOLDER>,multi work unit weighted queue multi work unit weighted queue = new multi work unit weighted queue ( this . max work units per job ) ;,queue add
check to see if the resulting graph contains the expected <PLACE_HOLDER>,structured graph replacement = get replacements ( ) . get substitution ( real method @$ - __num__ @$ false @$ null @$ graph . get options ( ) ) ; if ( replacement == null ) { assert in graph ( graph @$ expected node ) ; } option values options ; boolean need check node = true ; if ( java version util . java_spec <= __num__ ) { need check node = false ; } else { list < string > vm args = graal services . get input arguments ( ) ; assume . assume true ( vm args != null ) ; for ( string vm arg : vm args ) { if ( vm arg . equals ( disable_compactstrings_flag ) ) { need,graph contains
not a scheme start <PLACE_HOLDER> .,return - __num__ ;,scheme start
i.e . after the end of the length of the segment @$ when we find that text in question already is set properly @$ we can stop <PLACE_HOLDER>,int check point = offset + length ;,i.e stop
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
very special case to ensure that an indexed property descriptor does n't contain less <PLACE_HOLDER> than the enclosed property descriptor . if it does @$ then recreate as a property descriptor . see 4168833,if ( pd instanceof indexed property descriptor ) { ipd = ( indexed property descriptor ) pd ; if ( ipd . get indexed read method ( ) == null && ipd . get indexed write method ( ) == null ) { pd = new property descriptor ( ipd ) ; } },descriptor contain
we deliberately do not create the call node during parsing @$ because the call target is only created after the code entry is parsed . the code entry might not be yet parsed when we encounter this call . furthermore @$ if the call target is imported from another module @$ then that other module might not have been parsed yet . therefore @$,call nodes . add ( new wasm call stub node ( function ) ) ; break ;,wasm function
playback will claim active <PLACE_HOLDER> . otherwise audio system will .,if ( device type == hdmi device info . device_playback ) { hdmi cec local device playback playback = playback ( ) ; playback . set is active source ( true ) ; playback . wake up if active source ( ) ; playback . may send active source ( source ) ; set active source ( playback . m address @$ physical address ) ; } if ( device type == hdmi device info . device_audio_system ) { hdmi cec local device audio system audio system = audio system ( ) ; if ( playback ( ) != null ) { audio system . set is active source ( false ) ; } else { audio system . set is active source ( true ) ; audio,playback claim
this should n't ever be possible as annotation processor should generate empty <PLACE_HOLDER>,throw new runtime exception ( e ) ;,processor generate
this checks for the build file <PLACE_HOLDER> in the correct precedence order .,for ( root package path entry : pkg locator . get path entries ( ) ) { for ( build file name build file name : build files by priority ) { package lookup value result = get package lookup value ( env @$ package path entry @$ package key @$ build file name ) ; if ( result == null ) { return null ; } if ( result != package lookup value . no_build_file_value ) { return result ; } } },checks build
when entity expansion <PLACE_HOLDER> is set by the application @$ we need to check for the entity expansion <PLACE_HOLDER> set by the parser @$ if number of entity expansions exceeds the entity expansion <PLACE_HOLDER> @$ parser will throw fatal error . note that this represents the nesting level of open entities .,f entity expansion count ++ ; if ( f limit analyzer != null ) { f limit analyzer . add value ( entity expansion index @$ name @$ __num__ ) ; } if ( f security manager != null && f security manager . is over limit ( entity expansion index @$ f limit analyzer ) ) { f security manager . debug print ( f limit analyzer ) ; f error reporter . report error ( xml message formatter . xml_domain @$ __str__ @$ new object [ ] { f security manager . get limit value by index ( entity expansion index ) } @$ xml error reporter . severity_fatal_error ) ; f entity expansion count = __num__ ; },number exceeds
first check to see if this plain old field map has <PLACE_HOLDER> to the actual type .,if ( result == null ) { if ( field map . get dest hint container ( ) != null ) { class < ? > dest hint type = field map . get dest hint type ( src field value . get class ( ) ) ; if ( dest hint type != null ) { dest field type = dest hint type ; } } string map id = field map . get map id ( ) ; class < ? > target class ; if ( field map . get dest hint container ( ) != null && field map . get dest hint container ( ) . get hint ( ) != null ) { target class = field map . get dest hint,map has
indent:8 exp:12 warn indent:8 <PLACE_HOLDER>,try { } catch ( throwable t ) { system . identity hash code ( __str__ ) ; } finally { },exp:12 warn
the final thread that should n't execute releases the <PLACE_HOLDER> once it has run so it is deterministic that the other two fill the thread pool until this one rejects,if ( ! should execute ) { success latch . count down ( ) ; },thread releases
first c<PLACE_HOLDER> contains <PLACE_HOLDER> @$ better use activate ? if you have providers,logger . debug ( __str__ @$ arrays . to string ( provider . get item names ( ) . to array ( ) ) ) ; for ( string item name : provider . get item names ( ) ) { bind gpio pin ( ( mcp3424 binding provider ) provider @$ item name ) ; },call contains
the type flow of a field load is the type flow of the field itself . it accumulates all <PLACE_HOLDER> ever stored to the field .,if ( n instanceof load field node ) { load field node node = ( load field node ) n ; if ( is object ( node ) ) { register flow ( node @$ results . lookup field ( node . field ( ) ) ) ; } } else if ( n instanceof store field node ) { store field node node = ( store field node ) n ; if ( is object ( node . value ( ) ) ) { type flow field flow = results . lookup field ( node . field ( ) ) ; lookup flow ( node . value ( ) ) . add use ( field flow ) ; } } else if ( n instanceof return node,flow accumulates
run streaming k means <PLACE_HOLDER> in parallel by spawning 1 thread per input path to process .,executor service pool = executors . new cached thread pool ( ) ; list < future < iterable < centroid > > > intermediate centroid futures = new array list < > ( ) ; for ( file status status : hadoop util . list status ( file system . get ( conf ) @$ input @$ path filters . logscrc filter ( ) ) ) { intermediate centroid futures . add ( pool . submit ( new streamingk means thread ( status . get path ( ) @$ conf ) ) ) ; } log . info ( __str__ ) ;,k means
dispose the input <PLACE_HOLDER> after removing the window so the window manager does n't interpret the input <PLACE_HOLDER> being closed as an abnormal termination .,if ( m input channel != null ) { m input channel . dispose ( ) ; m input channel = null ; } m display manager . unregister display listener ( m display listener ) ; unschedule traversals ( ) ;,manager interpret
m vpns lock <PLACE_HOLDER> to be hold here to ensure that the active vpn can not be changed between these two calls .,synchronized ( m vpns ) { old blocked = is uid networking with vpn blocked ( nri . m uid @$ uid rules @$ old metered @$ old restrict background ) ; new blocked = is uid networking with vpn blocked ( nri . m uid @$ uid rules @$ new metered @$ new restrict background ) ; } if ( old blocked != new blocked ) { call callback for request ( nri @$ nai @$ connectivity manager . callback_blk_changed @$ encode bool ( new blocked ) ) ; },vpns lock
the outbound buffer contains only one <PLACE_HOLDER> or it contains a file region .,object msg = in . current ( ) ; if ( msg instanceof byte buf ) { return write bytes ( in @$ ( byte buf ) msg ) ; } else if ( msg instanceof default file region ) { return write default file region ( in @$ ( default file region ) msg ) ; } else if ( msg instanceof file region ) { return write file region ( in @$ ( file region ) msg ) ; } else { throw new error ( ) ; },buffer contains
we capture and set the context once the user provided observable <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,user provided
data 's inserted . in theory . now pull some out to stare at it and verify it made the round <PLACE_HOLDER> .,assert true ( location row id != - __num__ ) ;,now pull
sometimes most recent focus <PLACE_HOLDER> may be null @$ but focus <PLACE_HOLDER> is not e.g . we reset most recent focus <PLACE_HOLDER> if user removes focus <PLACE_HOLDER>,if ( focus owner == null ) { focus owner = keyboard focus manager . get current keyboard focus manager ( ) . get focus owner ( ) ; if ( focus owner != null && focus owner . get containing window ( ) != window ) { focus owner = null ; } },user removes
if the caller does not have <PLACE_HOLDER> to load the driver then skip it .,for ( driver info a driver : registered drivers ) { if ( is driver allowed ( a driver . driver @$ caller class loader ) ) { result . add element ( a driver . driver ) ; } else { println ( __str__ + a driver . get class ( ) . get name ( ) ) ; } },caller have
some exceptions rudely do n't allow cause <PLACE_HOLDER>,return new throwable ;,rudely allow
on finish : when function returns something : store register 0 ; return value <PLACE_HOLDER> store register 2 <PLACE_HOLDER> store register 1 <PLACE_HOLDER> push register 0 return when function does not return anything : store register 2 <PLACE_HOLDER> store register 1 <PLACE_HOLDER> when original function has some returns @$ but no return at the end of function : push undefined locjump : store register,int return reg = - __num__ ; pos = start index + count - __num__ ; if ( code . get ( pos ) instanceof action return ) { pos -- ; if ( pos < start index ) { return false ; } if ( ! ( code . get ( pos ) instanceof action push ) ) { return false ; } action push pu = ( action push ) code . get ( pos ) ; if ( pu . values . size ( ) != __num__ ) { return false ; } if ( ! ( pu . values . get ( __num__ ) instanceof register number ) ) { return false ; } register number rn = ( register number ) pu .,push register
the other thread will now try to close the connection map but it will block because this thread has locked <PLACE_HOLDER> of the connections,await ( ) . until ( ( ) -> connection map . closing ) ; try { connection manager . borrow connection ( __num__ ) ; fail ( __str__ ) ; } catch ( cache closed exception e ) { },thread locked
let the os choose the start <PLACE_HOLDER> of the region in memory,mmap ptr = os . mmap ( __num__ @$ mmap region size @$ os constants . prot_read @$ os constants . map_shared | os constants . map_populate @$ m fd @$ mmap file position ) ;,os choose
test method java.util.zip.checked output <PLACE_HOLDER>,try { file output stream out file = new file output stream ( file . create temp file ( __str__ @$ __str__ ) ) ; checked output stream chk out = new checked output stream ( out file @$ new crc32 ( ) ) ; assert equals ( __str__ @$ __num__ @$ chk out . get checksum ( ) . get value ( ) ) ; out file . close ( ) ; } catch ( io exception e ) { fail ( __str__ ) ; } catch ( security exception e ) { fail ( __str__ ) ; },method java.util.zip.checked
let the remote peer know that we support rtcp xr in general and vo ip metrics report <PLACE_HOLDER> in particular .,string rtcpxr = md . get attribute ( rtcp extended report . sdp_attribute ) ; if ( rtcpxr == null ) { md . set attribute ( rtcp extended report . sdp_attribute @$ rtcp extended report . voip metrics report block . sdp_parameter ) ; } int ptime setting = sip activator . get configuration service ( ) . get int ( __str__ + __str__ @$ __num__ ) ;,xr report
the aws region provider chain that is used by default throws an <PLACE_HOLDER> instead of returning null when the region is not defined . for that reason @$ we have to support both throwing an <PLACE_HOLDER> and returning null as the region not being defined .,return null ;,chain throws
we do n't use preconditions.check argument because it requires boxing field id @$ which affects inner loop <PLACE_HOLDER>,if ( ! types [ field id ] . get java type ( ) . equals ( type ) ) { throw new illegal argument exception ( format ( __str__ @$ type @$ types [ field id ] @$ field id ) ) ; },which affects
verify that both the shutdown command and the application finished <PLACE_HOLDER> .,output . should have exit value ( __num__ ) ; process thread . join and throw ( ) ; process thread . get output ( ) . should have exit value ( __num__ ) ;,command finished
when traversing dependencies for <PLACE_HOLDER> @$ we get <PLACE_HOLDER> attached to libraries that are statically linked @$ and <PLACE_HOLDER> attached to the initial bundle . this heuristic is based on the idea that the bundle holding the compiled code of a library also holds the <PLACE_HOLDER> .,case copying_include_shared_resources :,bundle holds
if app does not have any <PLACE_HOLDER> enabled launcher activity or any permissions @$ the app can legitimately have no icon so we do not show the synthetic activity .,return requests permissions ( pkg ) && has default enable launcher activity ( app info . package name ) ;,app have
note : the lookup is enforcing security across users by making sure the caller can only access <PLACE_HOLDER> it hosts or provides .,widget widget = lookup widget locked ( app widget id @$ binder . get calling uid ( ) @$ calling package ) ; if ( widget == null ) { throw new illegal argument exception ( __str__ ) ; },caller access
no luck with this constructor @$ let 's try another <PLACE_HOLDER>,throw new illegal state exception ( __str__ ) ;,"s" try
fake client as client.default follows <PLACE_HOLDER> .,test interface api = feign . builder ( ) . client ( ( request @$ options ) -> response ) . target ( test interface . class @$ __str__ + server . get port ( ) ) ; assert equals ( api . response ( ) . headers ( ) . get ( __str__ ) @$ collections . singleton list ( __str__ ) ) ;,client follows
first round @$ only calculate those have a pa <PLACE_HOLDER>,for ( int i = __num__ ; i < e types . length ; i ++ ) { pa data . salt and params snp = pa data . get salt and params ( e types [ i ] @$ pa list ) ; if ( snp != null ) { if ( e types [ i ] != encrypted data . etype_arcfour_hmac && snp . salt != null ) { salt = snp . salt ; } result [ i ] = encryption key . acquire secret key ( cname @$ password @$ e types [ i ] @$ snp ) ; } },round have
make sure the response has at least 1 <PLACE_HOLDER> with a valid dependency id,if ( response . get table count ( ) == __num__ ) { response . add dependency ( new dependency pair . buffer dependency pair ( m_fragment msg . get output dep id ( __num__ ) @$ m_raw dummy result @$ __num__ @$ m_raw dummy result . length ) ) ; },response has
page views . do this once we have the right padding <PLACE_HOLDER> from above .,for ( int i = __num__ ; i < count ; i ++ ) { final view child = get child at ( i ) ; if ( child . get visibility ( ) == gone ) { continue ; } final layout params lp = ( layout params ) child . get layout params ( ) ; if ( lp . is decor ) { continue ; } final item info ii = info for child ( child ) ; if ( ii == null ) { continue ; } if ( lp . needs measure ) { lp . needs measure = false ; final int width spec = measure spec . make measure spec ( ( int ) ( child width * lp . width,views have
at least one stack trace should contain transaction deadlock <PLACE_HOLDER> .,if ( has cause ( e @$ transaction timeout exception . class ) && has cause ( e @$ transaction deadlock exception . class ) ) { if ( deadlock . compare and set ( false @$ true ) ) u . error ( log @$ __str__ + transaction deadlock exception . class . get simple name ( ) @$ e ) ; },trace contain
key size | expected <PLACE_HOLDER>,should put and get key size ( __num__ @$ __num__ ) ; should put and get key size ( key_one_byte_max @$ __num__ ) ; should put and get key size ( key_two_byte_min @$ __num__ ) ; should put and get key size ( key_two_byte_max @$ __num__ ) ; should put and get key size ( key_two_byte_no_offload_max @$ __num__ ) ;,| expected
predicate may contain any <PLACE_HOLDER> .,for ( string col name : serde . column names ) { analyzer . allow column name ( col name ) ; } list < index search condition > search conditions = new linked list < index search condition > ( ) ; expr node desc residual = analyzer . analyze predicate ( predicate @$ search conditions ) ; decomposed predicate decomposed = new decomposed predicate ( ) ; decomposed . pushed predicate = analyzer . translate search conditions ( search conditions ) ; decomposed . residual predicate = ( expr node generic func desc ) residual ; return decomposed ;,predicate contain
auto expand only support when each cell occupy one <PLACE_HOLDER> .,if ( ! laying out in primary direction && remaining span == __num__ && ( count == consumed span count ) && range style . m is auto expand ) { if ( layout in vertical ) { range style . m size per span = ( m total size - ( count - __num__ ) * range style . mh gap ) / count ; } else { range style . m size per span = ( m total size - ( count - __num__ ) * range style . mv gap ) / count ; } },cell occupy
restart and gii @$ it should be full gii because number of unfinished operations exceeds <PLACE_HOLDER>,change unfinished operation limit ( r @$ __num__ ) ; create distributed region ( r ) ;,number exceeds
empty constructor should produce a new <PLACE_HOLDER> of adapter delegates manager,list delegation adapter < list < object > > adapter = new list delegation adapter < list < object > > ( ) { @ override public int get item count ( ) { assert . assert not null ( this . delegates manager ) ; return __num__ ; } } ;,constructor produce
case 9 : true if an attribute named integer att name of type integer has the <PLACE_HOLDER> integer <PLACE_HOLDER> we cover javax.management.binary rel query exp with a rel op equal to eq and javax.management.numeric <PLACE_HOLDER> exp,queries . add ( query . eq ( query . attr ( integer att name ) @$ query . value ( integer value ) ) ) ;,name has
we expect this to throw an io exception since we 're faking socket connect <PLACE_HOLDER> every time,learner . connect to leader ( new multiple addresses ( addr ) @$ __str__ ) ;,time connect
sets payload to empty as frame contains no <PLACE_HOLDER>,frame builder . payload ( new byte [ ] { } ) ; curr bytes . reset ( ) ; curr state = beats state . complete ; window size = frame builder . data size ; break ;,frame contains
hider and hidden needs to be field @$ method or type and fields hide fields @$ types hide types @$ methods hide methods ie a field does n't hide a methods <PLACE_HOLDER>,element kind hider kind = hider . get kind ( ) ; element kind hidden kind = hidden . get kind ( ) ; if ( hider kind . is field ( ) && ! hidden kind . is field ( ) ) { return false ; } else if ( hider kind . is class ( ) && ! ( hidden kind . is class ( ) || hidden kind . is interface ( ) ) ) { return false ; } else if ( hider kind . is interface ( ) && ! ( hidden kind . is class ( ) || hidden kind . is interface ( ) ) ) { return false ; } else if ( hider kind == element kind . method,field hide
load apdex satisfied <PLACE_HOLDER>,final long apdex satisfied threshold = get required property ( props @$ report_generator_key_apdex_satisfied_threshold @$ report_generator_key_apdex_satisfied_threshold_default @$ long . class ) ; configuration . set apdex satisfied threshold ( apdex satisfied threshold ) ;,apdex satisfied
5 minutes in milliseconds create a <PLACE_HOLDER> from the arguments,config config = new config ( ) ; config . parse ( async benchmark . class . get name ( ) @$ args ) ; system . out . print ( horizontal_rule ) ; log . info ( __str__ ) ; log . info ( horizontal_rule ) ; log . info ( config . get config dump string ( ) ) ; if ( config . latencyreport ) { log . info ( __str__ ) ; },minutes create
... but the three requests that follow requests include an authorization <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { request = server . take request ( ) ; assert equals ( __str__ @$ request . get request line ( ) ) ; assert contains ( request . get headers ( ) @$ __str__ + simple authenticator . base_64_credentials ) ; },requests include
using factory get an <PLACE_HOLDER> of document builder,try { document builder db = dbf . new document builder ( ) ; document doc = db . parse ( is ) ; element doc ele = doc . get document element ( ) ; node list nl = doc ele . get elements by tag name ( __str__ ) ; if ( nl != null && nl . get length ( ) > __num__ ) { element el = ( element ) nl . item ( __num__ ) ; string reason = get text value from attribute ( el @$ __str__ ) ; throw new logout notification exception ( __str__ + reason ) ; } nl = doc ele . get elements by tag name ( __str__ ) ; if ( nl != null && nl,factory get
user trigger <PLACE_HOLDER> to tr<PLACE_HOLDER>nsfer token to b,string param = __str__ + base58 . encode58 check ( transfer token contract address ) + __str__ + asset account id . to string utf8 ( ) + __str__ ; final string trigger txid = public methed . trigger contract ( transfer token contract address @$ __str__ @$ param @$ false @$ __num__ @$ __num__ @$ __str__ @$ __num__ @$ user001 address @$ user001 key @$ blocking stub full ) ; public methed . wait produce next block ( blocking stub full ) ; account infoafter = public methed . query account ( user001 address @$ blocking stub full ) ; account resource message resource infoafter = public methed . get account resource ( user001 address @$ blocking stub full ) ; long after balance = infoafter .,user trigger
test that all subtasks are taken into the account for the summary . the correctness of the actual results is checked in the test of the min max avg <PLACE_HOLDER> .,task state stats . task state stats summary summary = task stats . get summary stats ( ) ; assert equals ( subtasks . length @$ summary . get state size stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get ack timestamp stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get sync checkpoint duration stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get async checkpoint duration stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get alignment buffered stats ( ) . get count (,test max
when the route has empty producible types @$ the result has the highest <PLACE_HOLDER> .,assert that ( get result . has highest score ( ) ) . is true ( ) ; assert that ( get result . negotiated response media type ( ) ) . is null ( ) ;,route has
if user edit <PLACE_HOLDER> or port @$ we have to find the row with the previous value,if ( is edit mode ) { stun server = get stun server ( previous server @$ previous port ) ; } else { stun server = get stun server ( address @$ port ) ; },user edit
determine container and cache names using legacy <PLACE_HOLDER>,string replication config cache name = ( config != null ) ? config . get cache name ( ) : null ; service name replication config service name = service name factory . parse service name ( ( replication config cache name != null ) ? replication config cache name : __str__ ) ; service name base replication config service name = service name . jboss . append ( __str__ ) ; if ( ! base replication config service name . is parent of ( replication config service name ) ) { replication config service name = base replication config service name . append ( replication config service name ) ; } string container name = ( ( replication config service name . length ( ) > __num__,names using
even though we 've added a <PLACE_HOLDER> @$ the original list should have an empty row added to make it the same size,assert equals ( field list . size ( ) @$ other field list . size ( ) ) ; for ( int i = __num__ ; i < field list . size ( ) ; i ++ ) { data type line data type line = ( data type line ) field list . get ( i ) ; data type line other data type line = ( data type line ) other field list . get ( i ) ; if ( i == ( insert indexb + __num__ ) ) { assert true ( data type line instanceof empty data type line ) ; assert not null ( other data type line . get name color ( ) ) ; assert not null ( other data,size added
range start and range end are equivalent <PLACE_HOLDER>s so to be consistent we translate them to the same integer value . that way we can check whether a run covers the entire range by just checking if the start equals the end <PLACE_HOLDER> .,if ( current run end == range end ) { current run end = range start ; },start equals
get the lease renewer now so we can verify it later without calling get lease renewer @$ which will automatically add the <PLACE_HOLDER> into it .,final lease renewer lease renewer = dfs . get client ( ) . get lease renewer ( ) ; stream . write ( __str__ . get bytes ( ) ) ; try { stream . hflush ( ) ; fail ( __str__ ) ; } catch ( ds quota exceeded exception expected ) { },which add
set the default max thread <PLACE_HOLDER> to 100 to limit the <PLACE_HOLDER> of concurrent requests so that rest server does n't oom easily . jetty set the default max thread <PLACE_HOLDER> to 250 @$ if we do n't set it . our default min thread <PLACE_HOLDER> 2 is the same as that used by jetty .,int max threads = servlet . get configuration ( ) . get int ( rest_thread_pool_threads_max @$ __num__ ) ; int min threads = servlet . get configuration ( ) . get int ( rest_thread_pool_threads_min @$ __num__ ) ;,jetty set
map the seek <PLACE_HOLDER> to a <PLACE_HOLDER> in the corresponding timeline .,pair < object @$ long > period position ; try { period position = seek timeline . get period position ( window @$ period @$ seek position . window index @$ seek position . window position us ) ; } catch ( index out of bounds exception e ) { return null ; },the seek
the order in which the partition by keys are listed should not radically change the plan <PLACE_HOLDER> .,windowed query = __str__ ; validate windowed function plan ( windowed query @$ __num__ @$ __num__ @$ __num__ @$ expression type . aggregate_windowed_rank ) ;,order change
ensure that per rfc 6265 @$ cookie.value follows syntax <PLACE_HOLDER>,syntax . require validrfc6265 cookie value ( _value ) ;,cookie.value follows
contract created by create 2 @$ does n't have <PLACE_HOLDER>,assert . assert equals ( __num__ @$ smart contract . get abi ( ) . get entrys count ( ) ) ;,contract have
see which urls the pws did n't give us a <PLACE_HOLDER> for .,set < string > missed = new hash set < > ( broadcast urls ) ; missed . remove all ( found urls ) ; for ( string url : missed ) { pws result callback . on pws result absent ( url ) ; } record response ( ) ; pws result callback . on pws result error ( broadcast urls @$ response code @$ e ) ;,pws give
yay ! let 's extract the port <PLACE_HOLDER>,string s = m . group ( __num__ ) ; port = integer . parse int ( s ) ; inet address add = server . get inet address ( ) ; if ( add != null ) { dest = new inet socket address ( add @$ port ) ; } else { dest = inet socket address . create unresolved ( server addr . get host name ( ) @$ port ) ; },"s" extract
if cipher is null @$ get random <PLACE_HOLDER> failed @$ which means encryption is meaningless . therefore @$ do not save anything . this will cause users to lose incognito state in certain cases . that is annoying @$ but is better than failing to provide the guarantee of incognito mode .,return ;,failed get
calls on monitors from child should reach both <PLACE_HOLDER>,my monitor child monitor = child . new monitor ( my monitor . class ) ; child monitor . a void ( ) ; mockito . verify ( parent listener @$ mockito . times ( __num__ ) ) . a void ( ) ; mockito . verify ( child listener ) . a void ( ) ;,calls reach
clear to let gc do its <PLACE_HOLDER>,int new size = size - ( to index - from index ) ; for ( int i = new size ; i < size ; i ++ ) { element data [ i ] = null ; } size = new size ;,gc do
let 's create <PLACE_HOLDER>,data . sftpclient . create folder ( spool directory ) ; if ( is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ spool directory ) ) ; },"s" create
the wire protocol requires an <PLACE_HOLDER> of keys .,char sequence [ ] sequence = { key to release } ; executor . execute ( driver command . send_keys_to_active_element @$ immutable map . of ( __str__ @$ sequence ) ) ;,protocol requires
be aware that probe index builder will not clear its <PLACE_HOLDER>,probe index builder . clear ( ) ; build page builder . reset ( ) ; estimated probe block bytes = __num__ ; is sequential probe indices = true ;,builder clear
jvms : if the <PLACE_HOLDER> is finite and the divisor is an infinity @$ the result equals the <PLACE_HOLDER> . jvms : if the <PLACE_HOLDER> is a zero and the divisor is finite @$ the result equals the <PLACE_HOLDER> .,if ( x == __num__ || double . is infinite ( y ) ) { return x ; } double result = safe rem ( x @$ y ) ;,result equals
bug : diagnostic contains : missing disposable handling : apply auto dispose or cache the disposable <PLACE_HOLDER> manually and enable lenient mode .,observable . just ( __num__ ) . subscribe with ( new test observer < > ( ) ) ;,auto dispose
process entries in vx works symbol <PLACE_HOLDER>,address vx sym tbl = vx num sym entries addr . subtract ( vx num sym entries * sym_entry_size ) ; for ( int i = __num__ ; i < vx num sym entries ; i ++ ) { if ( monitor . is cancelled ( ) ) { return ; } println ( __str__ + i ) ; address sym entry = vx sym tbl . add ( i * sym_entry_size ) ; address sym name addr = to addr ( mem . get int ( sym entry . add ( sym_name_off ) ) ) ; address sym loc addr = to addr ( mem . get int ( sym entry . add ( sym_loc_off ) ) ) ; byte sym type = mem . get byte,entries works
count variational numbers . every next one either has the same <PLACE_HOLDER> or previous one plus probability of loss .,for ( int i = __num__ ; i < losses . length ; i ++ ) if ( i == __num__ ) loss probs [ i ] = get loss probability ( losses @$ __num__ ) ; else if ( losses [ i ] != losses [ i - __num__ ] ) loss probs [ i ] = get loss probability ( losses @$ i ) + loss probs [ i - __num__ ] ; else loss probs [ i ] = loss probs [ i - __num__ ] ;,one has
close req 1 and make sure req 2 does not affect num active <PLACE_HOLDER> .,actual res1 . close ( ) ; await ( ) . until asserted ( ( ) -> assert that ( client . num active requests ( ) ) . is zero ( ) ) ;,req affect
okay @$ ` former method ' and ` method ' both have the same <PLACE_HOLDER> . see if they are compatible .,all methods . replace ( method ) ;,method have
dummy 1 should get 4 <PLACE_HOLDER> : 1 3 5 7 dummy 2 should get 3 <PLACE_HOLDER> : 2 4 6,list < row meta and data > result rows1 = rc1 . get rows written ( ) ; check rows ( result rows1 @$ compare list1 ) ; list < row meta and data > result rows2 = rc2 . get rows written ( ) ; check rows ( result rows2 @$ compare list2 ) ;,7 get
ensure the original collection entry has <PLACE_HOLDER> @$ persister @$ and key set to null .,assert null ( ce manies . get role ( ) ) ; assert null ( ce manies . get loaded persister ( ) ) ; assert null ( ce manies . get key ( ) ) ;,entry has
resource claim 1 will have a single claimant count while resource claim 2 will have no claimant <PLACE_HOLDER> because resource claim 2 is referenced only by flowfiles that are swapped out .,assert equals ( __num__ @$ recovery claim manager . get claimant count ( resource claim1 ) ) ; assert equals ( __num__ @$ recovery claim manager . get claimant count ( resource claim2 ) ) ; final swap summary summary = queue . recover swapped flow files ( ) ; assert not null ( summary ) ; assert equals ( __num__ @$ summary . get max flow file id ( ) . int value ( ) ) ; assert equals ( new queue size ( __num__ @$ __num__ ) @$ summary . get queue size ( ) ) ; final list < resource claim > swapped out claims = summary . get resource claims ( ) ; assert not null ( swapped out claims ) ; assert equals,2 have
use regular expressions to filter the query result retrieve the attributes needed retrieve the mbeans object <PLACE_HOLDER> specified by the query,return new m beans attribute query filter ( jmx connection @$ attributes @$ new m beans object name query filter ( jmx connection ) ) ;,result retrieve
the following trick leverages the <PLACE_HOLDER> of a record writer via the job thus supporting arbitrary output formats .,job job = job . get instance ( context . get configuration ( ) ) ; job . set output format class ( get named output format class ( context @$ name output ) ) ; job . set output key class ( get named output key class ( context @$ name output ) ) ; job . set output value class ( get named output value class ( context @$ name output ) ) ; task context = new task attempt context impl ( job . get configuration ( ) @$ context . get task attemptid ( ) @$ new wrapped status reporter ( context ) ) ; task contexts . put ( name output @$ task context ) ; return task context ;,trick leverages
any request to open gets a new bogus <PLACE_HOLDER>,mockito . when ( mock gcs util . open ( mockito . any ( gcs path . class ) ) ) . then ( invocation -> file channel . open ( files . create temp file ( __str__ @$ __str__ ) @$ standard open option . create @$ standard open option . delete_on_close ) ) ;,request gets
took header does not make <PLACE_HOLDER> as we stream,return response . ok ( out ) . build ( ) ;,header make
a long does not match empty <PLACE_HOLDER>,assert filter matches skip vectorize ( edf ( __str__ ) @$ immutable list . of ( ) ) ;,long match
can now produce all <PLACE_HOLDER>,if ( t delta == __num__ ) { for ( int i = __num__ ; i < batch size ; i ++ ) { long e sequence ; long e delta ; long s offset ; do { s offset = calc sequence offset ( curr index @$ mask ) ; e sequence = lv sequence ( s buffer @$ s offset ) ; e delta = e sequence - curr index ; } while ( e delta != __num__ ) ; add element ( s buffer @$ e buffer @$ mask @$ curr index ++ @$ s offset @$ producer . produce ( ) ) ; } return batch size ; },now produce
task 2 has committed <PLACE_HOLDER>,task = task service . create task query ( ) . single result ( ) ; task service . complete ( task . get id ( ) ) ; list < current task transaction dependent task listener . current task > current tasks = current task transaction dependent task listener . get current tasks ( ) ; assert equals ( __num__ @$ current tasks . size ( ) ) ; assert equals ( __str__ @$ current tasks . get ( __num__ ) . get task id ( ) ) ; assert equals ( __str__ @$ current tasks . get ( __num__ ) . get task name ( ) ) ; assert equals ( __num__ @$ current tasks . get ( __num__ ) . get execution variables ( ),task committed
if this was the last listener that was registered with us then no long need to have a delegator registered with the call peer <PLACE_HOLDER> handlers . we therefore remove it so that audio level calculations would be ceased .,if ( ( local user audio level listeners == null ) || local user audio level listeners . is empty ( ) ) { iterator < t > call peer iter = get call peers ( ) ; while ( call peer iter . has next ( ) ) { call peer iter . next ( ) . get media handler ( ) . set local user audio level listener ( null ) ; } },handlers peer
completing should end the process <PLACE_HOLDER>,task = task service . create task query ( ) . single result ( ) ; task service . complete ( task . get id ( ) ) ; assert equals ( __num__ @$ runtime service . create process instance query ( ) . count ( ) ) ;,completing end
if write id list is not null @$ that means <PLACE_HOLDER> are requested within a txn context . so set <PLACE_HOLDER> compliant to false @$ if are txn <PLACE_HOLDER> supported is false or the write id which has updated the <PLACE_HOLDER> in not compatible with write id list . this is done within table lock as the number of partitions may be more than,list < column statistics > column statistics = shared cache . get partition col stats list from cache ( cat name @$ db name @$ tbl name @$ part names @$ col names @$ write id list @$ are txn stats supported ) ; if ( column statistics == null ) { return raw store . get partition column statistics ( cat name @$ db name @$ tbl name @$ part names @$ col names @$ engine @$ write id list ) ; } return column statistics ;,which updated
this option has a separator <PLACE_HOLDER>,if ( has value separator ( ) ) { char sep = get value separator ( ) ; int index = value . index of ( sep ) ; while ( index != - __num__ ) { if ( values . size ( ) == number of args - __num__ ) { break ; } add ( value . substring ( __num__ @$ index ) ) ; value = value . substring ( index + __num__ ) ; index = value . index of ( sep ) ; } },option has
print score every 10 parameter <PLACE_HOLDER>,model . set listeners ( new score iteration listener ( __num__ ) ) ;,print score
ticket set to anonymous for anonymous user . simplify <PLACE_HOLDER> .,subject current user = org . apache . shiro . security utils . get subject ( ) ; if ( current user . is authenticated ( ) ) { current user . logout ( ) ; } log . debug ( __str__ + current user ) ; if ( ! current user . is authenticated ( ) ) { username password token token = new username password token ( user name @$ password ) ; response = proceed to login ( current user @$ token ) ; } if ( response == null ) { response = new json response ( response . status . forbidden @$ __str__ @$ __str__ ) ; } log . warn ( response . to string ( ) ) ; return response .,ticket simplify
mp 4 containers include the proj box but webm containers do not . both containers use <PLACE_HOLDER> .,array list < mesh > meshes = null ; try { meshes = is proj ( input ) ? parse proj ( input ) : parse mshp ( input ) ; } catch ( array index out of bounds exception ignored ) { },containers use
each instance should have 18 <PLACE_HOLDER> assigned,num segments assigned per instance = segment assignment utils . get num segments assigned per instance ( new assignment @$ new instances ) ; expected num segments assigned per instance = new int [ new num instances ] ; new num segments per instance = num segments * num_replicas / new num instances ; arrays . fill ( expected num segments assigned per instance @$ new num segments per instance ) ; assert equals ( num segments assigned per instance @$ expected num segments assigned per instance ) ;,instance have
repository objects take priority so let 's overwrite <PLACE_HOLDER> ...,read databases ( trans meta @$ true ) ; read partition schemas ( trans meta @$ true ) ; read slaves ( trans meta @$ true ) ; read clusters ( trans meta @$ true ) ; return trans meta . get shared objects ( ) ;,"s" overwrite
key a was added first and key b has id <PLACE_HOLDER>,assert equals ( __num__ @$ key set utils . get key set ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get pub key ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get key set ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get pub key ref count ( m ksms @$ __num__ ) ) ; assert equals ( keyb @$ key set utils . get pub key ( m ksms @$ __num__ ) ) ; mapping = ks mapping . get ( __num__ ) ; assert equals ( __num__ @$ mapping . size ( ),key id
a newly created one from the source should have a different <PLACE_HOLDER> .,recoverable js ast ast2 = new recoverable js ast ( real ast @$ true ) ; check compile ( real ast @$ make defensive copy ( ast2 ) @$ __str__ ) ;,one have
verify that the class does not also define a <PLACE_HOLDER> with the same stem name with 'is ',try { final method get method = container class . get declared method ( __str__ + stem name ) ; if ( ! modifier . is static ( get method . get modifiers ( ) ) && get method . get annotation ( transient . class ) == null ) { check get and is variants ( container class @$ property name @$ get method @$ is method ) ; } } catch ( no such method exception ignore ) { },class define
children only represent configuration <PLACE_HOLDER> of the parent @$ and are not independent entities,return false ;,children represent
make the icon change between media icon and switch field icon depending on whether editing note <PLACE_HOLDER>,if ( edit model mode && allow field remapping ( ) ) { media button . set background resource ( icons [ __num__ ] ) ; set remap button listener ( media button @$ i ) ; } else if ( edit model mode && ! allow field remapping ( ) ) { media button . set background resource ( __num__ ) ; } else { media button . set background resource ( icons [ __num__ ] ) ; setmm button listener ( media button @$ i ) ; },editing note
if library is using a code <PLACE_HOLDER> rather than an api level @$ make sure this document target sdk version is using the same code <PLACE_HOLDER> .,string library target sdk version = lower priority document . get target sdk version ( string . value of ( this target sdk ) ) ; if ( ! character . is digit ( library target sdk version . char at ( __num__ ) ) ) { if ( ! library target sdk version . equals ( get target sdk version ( default_sdk_version ) ) ) { merging report . add message ( get source file ( ) @$ merging report . record . severity . error @$ string . format ( __str__ + __str__ @$ get target sdk version ( default_sdk_version ) @$ library target sdk version @$ lower priority document . get source file ( ) . print ( false ) ) ) ; return ;,library using
and now pretend that the remote contact has granted us <PLACE_HOLDER>,return new authorization response ( authorization response . accept @$ __str__ ) ;,contact granted
note : this is crazy . epoch <PLACE_HOLDER> up @$ but time format down we maintain this behavior for backward compatibility .,long start units = ( long ) math . ceil ( start . get millis ( ) ) / __num__ ; long end units = ( long ) math . ceil ( end exclusive . get millis ( ) ) / __num__ ; if ( objects . equals ( start units @$ end units ) ) { return string . format ( __str__ @$ get to unix time clause ( time format @$ time field @$ source name ) @$ start units ) ; } return string . format ( __str__ @$ get to unix time clause ( time format @$ time field @$ source name ) @$ start units @$ end units ) ;,note epoch
test that the module map content contains the individual <PLACE_HOLDER> inside the header tree artifact .,assert that ( module map content ) . contains ( __str__ ) ; assert that ( umbrella header content ) . contains ( headers . get exec path string ( ) + __str__ ) ; assert that ( umbrella header content ) . contains ( headers . get exec path string ( ) + __str__ ) ;,content contains
mark unterminated line in case fill buf throws eof <PLACE_HOLDER> or io <PLACE_HOLDER> .,end = - __num__ ; fill buf ( ) ;,line throws
emit a copy files <PLACE_HOLDER> for each destination .,for ( copy file phase destination spec destination spec : rule by destination spec . key set ( ) ) { iterable < target node < ? > > target nodes = rule by destination spec . get ( destination spec ) ; phases . add ( get single copy files build phase ( destination spec @$ target nodes ) ) ; } return phases . build ( ) ;,copy files
now <PLACE_HOLDER> that using different translates with lbm is ok this <PLACE_HOLDER> does n't prove a lot since showing a leak really requires a basher <PLACE_HOLDER> that can run for a long time .,for ( int x = __num__ ; x < __num__ ; x ++ ) { for ( int y = __num__ ; y < __num__ ; y ++ ) { attributed character iterator aci = van gogh . get iterator ( ) ; affine transform tx = affine transform . get translate instance ( x @$ y ) ; font render context frc = new font render context ( tx @$ false @$ false ) ; line break measurer lbm = new line break measurer ( aci @$ frc ) ; lbm . set position ( aci . get begin index ( ) ) ; while ( lbm . get position ( ) < aci . get end index ( ) ) { lbm . next layout (,test requires
<PLACE_HOLDER> service will encapsulate this <PLACE_HOLDER>,assert that ( result . get all ( ) @$ not ( has entry ( is ( __str__ ) @$ anything ( ) ) ) ) ;,service encapsulate
now if we found a region load get the <PLACE_HOLDER> of cost that was requested .,if ( region load list != null ) { cost = ( long ) ( cost + get region load cost ( region load list ) ) ; },load get
if no <PLACE_HOLDER> in the holder @$ or if the holder has different <PLACE_HOLDER> loaded @$ then load the configuration and set the new <PLACE_HOLDER> in the holder,if ( resources == null || ! config resources . equals ( resources . get config resources ( ) ) ) { get logger ( ) . debug ( __str__ ) ; final configuration config = new extended configuration ( get logger ( ) ) ; config . set class loader ( thread . current thread ( ) . get context class loader ( ) ) ; resources = new validation resources ( config resources @$ get configuration from resources ( config @$ config resources ) ) ; validation resource holder . set ( resources ) ; } final configuration conf = resources . get configuration ( ) ; results . add all ( kerberos properties . validate principal and keytab ( this . get class ( ),holder has
needed because this class overrides pull next <PLACE_HOLDER> @$ move down .,move down ( ) ;,overrides pull
file . we 're going to check the consistency of the resource file while building this mapping @$ and throw errors if the file does not meet our <PLACE_HOLDER> .,map < string @$ collection < string > > lines = new hash map < > ( ) ; final check charset mapping mapping = new check charset mapping ( ) ; for ( string key : props . string property names ( ) ) { collection < string > values = get values ( props . get property ( key ) ) ; lines . put ( key @$ values ) ; mapping . add mapping ( key @$ values ) ; },file meet
big query requires <PLACE_HOLDER> so we have to divide here,row . put ( __str__ @$ now in millis / __num__ ) ; client . insert row ( row @$ schema @$ table name ) ;,query requires
after terminate suppressed <PLACE_HOLDER> which itself suppressed original err,assert equals ( __num__ @$ after terminate . get suppressed ( ) . length ) ; assert equals ( error @$ after terminate . get suppressed ( ) [ __num__ ] ) ; assert equals ( __num__ @$ error . get suppressed ( ) . length ) ; assert equals ( err @$ error . get suppressed ( ) [ __num__ ] ) ;,itself suppressed
copy the <PLACE_HOLDER> of the transformation to the step ... do n't share . each copy of the step has its own <PLACE_HOLDER> .,step . initialize variables from ( this ) ; step . set using thread priority managment ( trans meta . is using thread priority managment ( ) ) ;,copy has
create a table with a primary key named 'name ' @$ which holds a <PLACE_HOLDER>,try { create table ( ) ; describe table ( ) ; put item ( new item ( __str__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) ; put item ( new item ( __str__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) ; get item ( __str__ ) ; get item ( __str__ ) ; map < string @$ condition > scan filter = new hash map < string @$ condition > ( ) ; condition condition = new condition ( ) . with comparison operator ( comparison operator . gt . to string ( ) ) . with attribute value list ( new attribute value ( ) . withn ( __str__ ) ) ; scan filter . put ( __str__ @$ condition,which holds
<PLACE_HOLDER> 1 should always beat <PLACE_HOLDER> 2 @$ all other things being equal .,assert equals ( expected phone1 scored suggestion @$ m time zone detector strategy . find best phone suggestion for tests ( ) ) ;,phone beat
when process is creating a lot of connections this can take some time so increase the <PLACE_HOLDER>,if ( holder == null ) { holder = new connection holder ( this @$ the serverid @$ options ) ; connection holder prev holder = connections . put if absent ( the serverid @$ holder ) ; if ( prev holder != null ) { holder = prev holder ; } else { holder . connect ( ) ; } },time increase
this call triggers j <PLACE_HOLDER> to load our spy,new spy go to helper ( ) ; tool . get options ( __str__ ) . set enum ( __str__ @$ navigation options . external navigation enum . navigate to external program ) ;,call triggers
snapshot has no <PLACE_HOLDER> after flush,int number of mem scanners after flush = input cells after snapshot . is empty ( ) ? __num__ : __num__ ; boolean more ; int cell count = __num__ ; do { list < cell > cells = new array list < > ( ) ; more = s . next ( cells ) ; cell count += cells . size ( ) ; assert equals ( more ? number of mem scanners after flush : __num__ @$ count mem store scanner ( s ) ) ; } while ( more ) ; assert equals ( __str__ + input cells before snapshot . size ( ) + __str__ + input cells after snapshot . size ( ) @$ input cells before snapshot . size ( ) +,snapshot has
merge similar styles . doing so will improve <PLACE_HOLDER> .,last style = styles . get ( styles . size ( ) - __num__ ) ; if ( last style . similar to ( style ) && ( last style . start + last style . length == style . start ) ) { last style . length += style . length ; } else { styles . add ( style ) ; },styles improve
if it is not a global @$ it might be accessing a local of the outer <PLACE_HOLDER> . if that 's the case the functions between the variable 's declaring <PLACE_HOLDER> and the variable reference <PLACE_HOLDER> can not be moved .,if ( var . get scope ( ) != t . get scope ( ) ) { for ( name context context : symbol stack ) { if ( context . scope == var . get scope ( ) ) { break ; } context . name . read closure variables = true ; } },functions declaring
if at least one has some <PLACE_HOLDER> @$ mergee has some too page buffer client statuses may be long @$ so we do n't want to combine the lists,return new exchange client status ( ( buffered bytes + other . buffered bytes ) / __num__ @$ math . max ( max buffered bytes @$ other . max buffered bytes ) @$ merge avgs ( average bytes per request @$ successful requests count @$ other . average bytes per request @$ other . successful requests count ) @$ successful requests count + other . successful requests count @$ buffered pages + other . buffered pages @$ no more locations && other . no more locations @$ immutable list . of ( ) ) ;,mergee has
we should be getting the same object from both streams . this ensures that multiple subscribers do n't induce extra <PLACE_HOLDER>,assert true ( all equal . get ( ) ) ;,subscribers induce
the server side will write <PLACE_HOLDER> @$ and in order to proceed with the initial tls handshake we need to start reading before waiting for the callback .,byte [ ] buffer = new byte [ __num__ ] ; int len = client . get input stream ( ) . read ( buffer ) ; assert equals ( __str__ @$ new string ( buffer @$ __num__ @$ len @$ standard charsets . utf_8 ) ) ; assert null ( _write callback . get ( __num__ @$ time unit . seconds ) ) ;,side write
if instruction has fall <PLACE_HOLDER>,address fall thru = null ; if ( instr . has fallthrough ( ) ) { if ( check non returning ( program @$ flow type @$ instr ) ) { target = get next target ( body @$ untried target list ) ; repeat instruction byte tracker . reset ( ) ; continue ; } new target = instr . get fall through ( ) ; fall thru = new target ; } else { address next addr = max addr . next ( ) ; if ( target list . contains ( next addr ) ) { new target = next addr ; } else if ( flow type . is jump ( ) ) { address flows [ ] = instr . get flows (,instruction has
native library merge code <PLACE_HOLDER>,android binary graph enhancer graph enhancer = new android binary graph enhancer ( toolchain provider @$ context . get cell path resolver ( ) @$ build target @$ context . get project filesystem ( ) @$ android platform target @$ params @$ graph builder @$ args . get aapt mode ( ) @$ immutable list . of ( ) @$ resource compression mode . disabled @$ filter resources steps . resource filter . empty_filter @$ enum set . none of ( r type . class ) @$ optional . empty ( ) @$ optional . empty ( ) @$ immutable set . of ( ) @$ null @$ args . get manifest ( ) @$ args . get manifest skeleton ( ) @$ optional . empty ( ),library merge
0 x 100248 f : p 1 and p 2 have same function <PLACE_HOLDER> .,program builder1 . create function comment ( __str__ @$ __str__ ) ; program builder2 . create function comment ( __str__ @$ __str__ ) ; check no comment difference ( ) ;,1 have
without the name @$ the suppression node increments the topology <PLACE_HOLDER>,assert that ( anonymous node topology @$ is ( anonymous_intermediate_topology ) ) ;,node increments
do n't need to clone these @$ since the trigger context does n't allow <PLACE_HOLDER>,for ( map . entry < w @$ value state < bit set > > entry : state . access in each merging window ( finished_bits_tag ) . entry set ( ) ) { builder . put ( entry . get key ( ) @$ read finished bits ( entry . get value ( ) ) ) ; clear finished bits ( entry . get value ( ) ) ; },context allow
add the tail string which contains no <PLACE_HOLDER> and return the result .,sbuf . append ( message pattern . substring ( i @$ message pattern . length ( ) ) ) ; return sbuf . to string ( ) ;,which contains
' : ' is an indicator <PLACE_HOLDER> extensible rules,if ( filter [ i ] == __str__ && ftype == ldap_filter_ext ) { if ( filter [ i - __num__ ] == __str__ ) { throw new invalid search filter exception ( __str__ ) ; } extensible start = i ; break ; },"" indicator
replace spaces because importer ca n't read attributes <PLACE_HOLDER> in quotes,writer . append ( __str__ ) ; edge iterable edge iterable = graph . get edges ( ) ; for ( edge edge : edge iterable ) { print edge data ( edge @$ edge . get source ( ) @$ edge . get target ( ) @$ graph ) ; if ( ! edge . is directed ( ) && ! edge . is self loop ( ) ) { print edge data ( edge @$ edge . get target ( ) @$ edge . get source ( ) @$ graph ) ; } progress . progress ( progress ticket ) ; if ( cancel ) { edge iterable . do break ( ) ; return ; } },importer read
pmd can not resolve array length <PLACE_HOLDER> @$ but only the,if ( expressions . size ( ) == number constants . integer_size_or_length_2 ) { ast primary expression left = expressions . get ( __num__ ) ; ast primary expression right = expressions . get ( __num__ ) ; boolean both array length = is array length ( left ) && is array length ( right ) ; boolean both wrapper type = node utils . is wrapper type ( left ) && node utils . is wrapper type ( right ) ; if ( ! both array length && both wrapper type ) { add violation with message ( data @$ node @$ __str__ ) ; } },pmd resolve
the destination will contain a matching <PLACE_HOLDER> which does not contain a null value which we do n't want to overwrite .,child src child1 = new child ( __num__ @$ __str__ @$ null ) ; parent with child list src = new parent with child list ( src child1 @$ new child ( __num__ @$ __str__ @$ __str__ ) ) ; parent with child list dest = new parent with child list ( new child ( __num__ @$ __str__ @$ __str__ ) @$ new child ( __num__ @$ __str__ @$ __str__ ) ) ;,destination contain
wrap these cached default properties in a new property object just so that the caller of this method ca n't modify the default <PLACE_HOLDER>,return new properties ( default properties ) ;,caller modify
and the inner <PLACE_HOLDER> does not contain the outer <PLACE_HOLDER> .,assert that ( inner pattern . containstbd fortbd ( outer pattern ) ) . is equal to ( containstbd fortbd result . other ) ;,pattern contain
if true @$ returns corrupt <PLACE_HOLDER> instead of correct protobufs .,boolean poisoned = false ; if ( worker options . poison after > __num__ && work unit counter > worker options . poison after ) { poisoned = true ; } if ( poisoned && worker options . hard poison ) { system . err . println ( __str__ ) ; system . exit ( __num__ ) ; } else { int exit code = __num__ ; try { options parser parser = parser helper ( request . get arguments list ( ) ) ; example work multiplexer options options = parser . get options ( example work multiplexer options . class ) ; if ( options . write counter ) { counter output = work unit counter ++ ; } results . add ( executor service .,returns corrupt
if no application can handle the <PLACE_HOLDER> @$ assume that the web view can handle it .,return overriding url loading ;,application handle
let 's use simple baseline value @$ arbitrary date in gmt @$ using the standard <PLACE_HOLDER>,string input str = __str__ ; date input date = mapper . read value ( __str__ + input str + __str__ @$ java . util . date . class ) ; calendar c = calendar . get instance ( time zone . get time zone ( __str__ ) ) ; c . set time ( input date ) ; assert equals ( __num__ @$ c . get ( calendar . year ) ) ; assert equals ( calendar . december @$ c . get ( calendar . month ) ) ; assert equals ( __num__ @$ c . get ( calendar . day_of_month ) ) ;,value using
v \u 00 f 2 \u 00 cd z \u 00 b 8 \u 0002 \u <PLACE_HOLDER> \u 0004 j \u <PLACE_HOLDER>,byte [ ] template = { ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__,j \\u
check if current wait <PLACE_HOLDER> exceeds the max wait <PLACE_HOLDER>,long current wait time ; site tasker next task = m_tasks . peek ( ) ; if ( next task == null ) { current wait time = __num__ ; } else { current wait time = current time - next task . get queue offer time ( ) ; },max wait
both should have the same <PLACE_HOLDER> of servers .,assert equals ( first balancer . get no of servers ( ) @$ second balancer . get no of servers ( ) ) ;,both have
this constructor delays dns <PLACE_HOLDER> to detect changes,channel builder = netty channel builder . for address ( inet server address . get host name ( ) @$ inet server address . get port ( ) ) ; channel builder = netty channel builder . for address ( address ) ;,delays dns
perfect match @$ just shove the replacement <PLACE_HOLDER> in .,if ( text1 . equals ( text2 ) ) { text = text . substring ( __num__ @$ start_loc ) + diff_text2 ( a patch . diffs ) + text . substring ( start_loc + text1 . length ( ) ) ; } else { linked list < diff > diffs = diff_main ( text1 @$ text2 @$ false ) ; if ( text1 . length ( ) > this . match_ max bits && diff_levenshtein ( diffs ) / ( float ) text1 . length ( ) > this . patch_ delete threshold ) { results [ x ] = false ; } else { diff_cleanup semantic lossless ( diffs ) ; int index1 = __num__ ; for ( diff a diff : a patch . diffs,match shove
if the post does n't have location <PLACE_HOLDER> @$ show the picker directly,if ( ! get edit post repository ( ) . has location ( ) ) { show location picker ( ) ; return ; },post have
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( testjdbc connection fail . class ) ;,suite using
check that the starting <PLACE_HOLDER> occurs before the started <PLACE_HOLDER>,assert true ( listener . starting time <= listener . started time @$ __str__ ) ;,the started
as tomcat can not specify the provider <PLACE_HOLDER> in the configuration . it 'll go into this path,if ( secret provider == null ) { string config prefix = filter config . get init parameter ( config_prefix ) ; config prefix = ( config prefix != null ) ? config prefix + __str__ : __str__ ; try { secret provider = authentication filter . construct secret provider ( filter config . get servlet context ( ) @$ super . get configuration ( config prefix @$ filter config ) @$ false ) ; this . is initialized by tomcat = true ; } catch ( exception ex ) { throw new servlet exception ( ex ) ; } },tomcat specify
together @$ these two take <PLACE_HOLDER> that multi exceptions thrown from route resource come out as json or gpx @$ depending on the media type,environment . jersey ( ) . register ( new multi exception mapper ( ) ) ; environment . jersey ( ) . register ( new multi exceptiongpx message body writer ( ) ) ; environment . jersey ( ) . register ( new illegal argument exception mapper ( ) ) ; environment . jersey ( ) . register ( new gh point converter provider ( ) ) ; final graph hopper managed graph hopper managed = new graph hopper managed ( configuration . get graph hopper configuration ( ) @$ environment . get object mapper ( ) ) ; environment . lifecycle ( ) . manage ( graph hopper managed ) ; environment . jersey ( ) . register ( new abstract binder ( ) { @ override,two take
what is current sequenceid ? we read the current sequenceid from the current file . after we read it @$ another thread could come in and compete with us writing out next version of file . the below retries should help in this case some but its hard to do <PLACE_HOLDER> in face of concurrent schema edits .,int current sequence id = current descriptor file == null ? __num__ : get table info sequence id ( current descriptor file . get path ( ) ) ; int new sequence id = current sequence id ;,some do
last element gets <PLACE_HOLDER> : what 's the name ?,data . item element = meta . get input position ( ) [ meta . get input position ( ) . length - __num__ ] ; data . item count = xml handler . count nodes ( data . section @$ data . item element ) ; data . item position = meta . get nr rows to skip ( ) ;,element gets
second renderer handles <PLACE_HOLDER> .,map < string @$ integer > second renderer mapped capabilities = new hash map < > ( ) ; second renderer mapped capabilities . put ( english . id @$ format_unsupported_subtype ) ; second renderer mapped capabilities . put ( german . id @$ format_handled ) ; renderer capabilities second renderer capabilities = new fake mapped renderer capabilities ( c . track_type_audio @$ second renderer mapped capabilities ) ; renderer capabilities [ ] renderer capabilities = new renderer capabilities [ ] { first renderer capabilities @$ second renderer capabilities } ;,renderer handles
this should not throw an ace because groovy.policy grants the code base <PLACE_HOLDER> to javax.print,assert execute ( script @$ __str__ @$ null ) ;,groovy.policy grants
number match includes at least the two <PLACE_HOLDER> being checked,if ( num end idx > pos ) { if ( num end idx > next pos ) { next pos = num end idx ; pos = num end idx ; do { pos = move index32 ( f text @$ pos @$ - __num__ ) ; this char = utf16 . char at ( f text @$ pos ) ; } while ( fcm . contains ( this char ) ) ; } continue ; },match includes
start a new session and attempt to access jenkins @$ which should cause auto login <PLACE_HOLDER>,wc = j . create web client ( ) ; wc . get cookie manager ( ) . add cookie ( c ) ;,which cause
first post test subscribe . to be sure that contacts are in the list so we can further continue and test presences each <PLACE_HOLDER>,suite . add test ( new test operation set presence ( __str__ ) ) ;,subscribe presences
the same . this tests that the resource processor namespaces the r <PLACE_HOLDER> correctly to avoid collisions between the two identical layout values .,java file object model = java file objects . for resource ( __str__ ) ; java file object model with differentr class = java file objects . for resource ( __str__ ) ; java file object generated model = java file objects . for resource ( __str__ ) ; java file object generated model with differentr class = java file objects . for resource ( __str__ ) ; assert_ ( ) . about ( java sources ( ) ) . that ( arrays . as list ( model @$ model with differentr class @$ r @$ r_from_different_package_with_same_value ) ) . processed with ( new epoxy processor ( ) ) . compiles without error ( ) . and ( ) . generates sources ( generated model @$ generated model,processor namespaces
if this item has a uri <PLACE_HOLDER> @$ try using that .,uri uri = get uri ( ) ; if ( uri != null ) { final content resolver resolver = context . get content resolver ( ) ; asset file descriptor descr = null ; file input stream stream = null ; input stream reader reader = null ; try { try { descr = resolver . open typed asset file descriptor ( uri @$ __str__ @$ null ) ; } catch ( security exception e ) { log . w ( __str__ @$ __str__ @$ e ) ; } catch ( file not found exception | runtime exception e ) { } if ( descr != null ) { try { stream = descr . create input stream ( ) ; reader = new input stream reader,item has
let the js logic decode the raw <PLACE_HOLDER> that the server sent,return value ;,logic decode
we need to mark this basic block as changed so that this monitorexit will be visited again . we need to do this to ensure that we have accounted for the possibility that this bytecode will throw an <PLACE_HOLDER> .,basic block bb = get basic block containing ( bci ) ; bb . set changed ( true ) ; bb . _monitor_top = bad_monitors ; if ( trace monitor mismatch ) { report monitor mismatch ( __str__ ) ; },bytecode throw
valve use the <PLACE_HOLDER> to log messages,when ( delta session manager . get the context ( ) ) . then return ( mock ( context . class ) ) ; when ( delta session manager . get the context ( ) . get logger ( ) ) . then return ( mock ( log . class ) ) ;,valve use
setting <PLACE_HOLDER> interval to 1 hour to prevent bp service actor sends <PLACE_HOLDER> periodically to nn during running test case @$ and bp service actor only sends <PLACE_HOLDER> once after startup,conf . set time duration ( dfs_heartbeat_interval_key @$ __num__ @$ time unit . hours ) ; minidfs cluster cluster = new minidfs cluster . builder ( conf ) . build ( ) ; cluster . wait active ( ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; metrics record builder rb = get metrics ( dn . get metrics ( ) . name ( ) ) ; assert counter ( __str__ @$ __num__ @$ rb ) ;,actor sends
can happen if this operator does not carry forward the previous bucketing <PLACE_HOLDER> for e.g . another join operator which does not carry one of the sides ' key <PLACE_HOLDER>,for ( list < string > list bucket cols : grand parent col names ) { if ( list bucket cols . is empty ( ) ) { continue ; } int col count = __num__ ; for ( string col name : parent col names . get ( __num__ ) ) { if ( list bucket cols . size ( ) <= col count ) { return false ; } expr node desc expr node desc = col expr map . get ( col name ) ; if ( expr node desc instanceof expr node column desc ) { if ( ( ( expr node column desc ) expr node desc ) . get column ( ) . equals ( list bucket cols . get ( col,operator carry
make <PLACE_HOLDER>ure the ping take<PLACE_HOLDER> more than 1 <PLACE_HOLDER>,thread . sleep ( __num__ ) ; assert false ( volt . handler . got ping ) ;,ping takes
get the targets from the main app class . only one will be used @$ depending on whether the user is deleting old <PLACE_HOLDER> by date or by row count .,try { timestamp type date target = app . get target date ( ) ; long row target = app . get target rows per partition ( ) ; client response with partition key [ ] responses ; if ( app . config . historyseconds > __num__ ) { responses = app . client . call all partition procedure ( __str__ @$ date target @$ app . config . deletechunksize ) ; } else { responses = app . client . call all partition procedure ( __str__ @$ row target @$ app . config . deletechunksize ) ; } app . update partition count ( responses . length ) ; for ( client response with partition key resp : responses ) { if ( resp . response .,user deleting
the amount with which to adjust the <PLACE_HOLDER> provided content padding to account for stroke and shape corners .,int content padding offset = ( int ) ( ( include corner padding ? calculate actual corner padding ( ) : __num__ ) - get parent card view calculated corner padding ( ) ) ; material card view . set ancestor content padding ( user content padding . left + content padding offset @$ user content padding . top + content padding offset @$ user content padding . right + content padding offset @$ user content padding . bottom + content padding offset ) ;,which adjust
connection does n't throw interrupted <PLACE_HOLDER> @$ but may return some bytes geq 0 or throw an <PLACE_HOLDER>,while ( ! thread . current thread ( ) . is interrupted ( ) || closed ) { if ( closed ) { throw new io exception ( __str__ ) ; } },connection throw
let the caller make <PLACE_HOLDER> of it @$ then .,type = type . t package ; return vset ;,caller make
check for test object <PLACE_HOLDER> . it should be 0,vm1 . invoke ( new cache serializable runnable ( __str__ ) { @ override public void run2 ( ) throws cache exception { assert equals ( __num__ @$ test object . num instance ) ; } } ) ; this . close client ( vm2 ) ; this . close client ( vm3 ) ; this . close client ( vm1 ) ; this . close client ( vm0 ) ;,test object
if we are not the session 's manager @$ only expire it iff : this is our first expiry check and the session expired a long <PLACE_HOLDER> ago or the session expired at least one graceperiod ago,if ( _last expiry check time <= __num__ ) { if ( ( sd . get expiry ( ) > __num__ ) && sd . get expiry ( ) < ( now - ( __num__ * ( __num__ * _grace period sec ) ) ) ) expired . add ( candidate ) ; } else { if ( ( sd . get expiry ( ) > __num__ ) && sd . get expiry ( ) < ( now - ( __num__ * _grace period sec ) ) ) expired . add ( candidate ) ; },session expired
the pubsub test client factory will assert fail on close if the actual published <PLACE_HOLDER> does not match the expected publish <PLACE_HOLDER> .,p . run ( ) ;,message match
make the user agent tester change its <PLACE_HOLDER> and make sure we do n't get notifications as we 're now unsubscribed .,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = get sample status1 ( ) ;,tester change
close the socket @$ which releases <PLACE_HOLDER> if it has created any .,if ( socket != null ) { try { socket . close ( ) ; } catch ( exception e ) { } },which releases
native library merge <PLACE_HOLDER>,android native libs packageable graph enhancer enhancer = new android native libs packageable graph enhancer ( new toolchain provider builder ( ) . with toolchain ( ndk cxx platforms provider . default_name @$ ndk cxx platforms provider . of ( native platforms ) ) . build ( ) @$ test cell path resolver . get ( project filesystem ) @$ graph builder @$ target @$ project filesystem @$ immutable set . of ( target cpu type . armv7 ) @$ cxx platform utils . default_config @$ optional . empty ( ) @$ optional . empty ( ) @$ optional . empty ( ) @$ relinker mode . disabled @$ immutable list . of ( ) @$ apk module graph @$ new noop android native target configuration matcher (,library merge
generate entities that have multiple errors the string a field has a <PLACE_HOLDER> over the length limitation and miss string b fields,if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__ ; i ++ ) { validation demo . union field with inline record union = new validation demo . union field with inline record ( ) ; union . set my enum ( my enum . foofoo ) ; validation demos . add ( new validation demo ( ) . set stringa ( __str__ ) . set inta ( current criteria . get inta ( ) ) . set union field with inline record ( union ) ) ; } } else if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__,string has
init assembler . each plan assembler requires a new <PLACE_HOLDER> of the plan selector to keep track of the best plan,plan assembler assembler = new plan assembler ( m_db @$ m_partitioning @$ ( plan selector ) m_plan selector . clone ( ) @$ m_is large query ) ;,assembler requires
register the jre fonts so that the native platform can access <PLACE_HOLDER> . this is used only on windows so that when printing the printer driver can access the fonts .,access controller . do privileged ( new privileged action ( ) { public object run ( ) { registerjre fonts with platform ( jre font dir name ) ; return null ; } } ) ;,platform access
if we have throttle threads @$ make sure the user also specified <PLACE_HOLDER>,preconditions . check argument ( large threads > __num__ && small threads > __num__ ) ; final string n = thread . current thread ( ) . get name ( ) ; steal job queue < runnable > steal job queue = new steal job queue < runnable > ( comparator ) ; this . long compactions = new thread pool executor ( large threads @$ large threads @$ __num__ @$ time unit . seconds @$ steal job queue @$ new thread factory builder ( ) . set name format ( n + __str__ ) . set daemon ( true ) . build ( ) ) ; this . long compactions . set rejected execution handler ( new rejection ( ) ) ; this . long compactions .,user specified
validate the having <PLACE_HOLDER> if any,validate having clause ( root node ) ; broker request broker request = new broker request ( ) ; root node . update broker request ( broker request ) ; if ( enable_pinot_query ) { try { pinot query pinot query = new pinot query ( ) ; root node . update pinot query ( pinot query ) ; if ( validate_converter ) { pinot query2 broker request converter converter = new pinot query2 broker request converter ( ) ; broker request temp broker request = converter . convert ( pinot query ) ; boolean result = broker request comparison utils . validate ( broker request @$ temp broker request ) ; if ( ! result ) { logger . error ( __str__ @$ expression ) ; if,the having
orc does n't currently handle <PLACE_HOLDER>,case timestamp : timestamp ts = data type utils . to timestamp ( field value @$ ( ) -> data type utils . get date format ( field data type . get format ( ) ) @$ field name ) ;,orc handle
this is called only if the peer has junior <PLACE_HOLDER>,this . seeddb . add potential ( peer ) ;,peer has
a volt db extension to customize the sql function set <PLACE_HOLDER>,volt disabled = disabled_in_functionsql_constructor ;,extension customize
ie sometimes has <PLACE_HOLDER> waiting long enough .,wait until ( expected conditions . presence of element located ( by . css selector ( __str__ ) ) @$ __num__ ) ;,ie has
fetch the field type will throw <PLACE_HOLDER> if the index is illegal,type information < x > field type = this . get type at ( field index ) ; string tail = matcher . group ( __num__ ) ; if ( tail == null ) { return field type ; } else { if ( field type instanceof composite type ) { return ( ( composite type < ? > ) field type ) . get type at ( tail ) ; } else { throw new invalid field reference exception ( __str__ + tail + __str__ + field type + __str__ ) ; } },type throw
let 's add an identical <PLACE_HOLDER> to the child @$ but it 'll appear after the current <PLACE_HOLDER> @$ so has no impact,child . insert ace ( __num__ @$ base permission . delete @$ new principal sid ( auth ) @$ true ) ;,"s" add
this can contain user code . wrap it in case it throws an <PLACE_HOLDER> .,try { invoker . invoke start bundle ( new do fn start bundle context ( ) ) ; } catch ( throwable t ) { throw wrap user code exception ( t ) ; },code throws
initialize m has app ops play <PLACE_HOLDER>,update app ops play audio ( ) ;,m has
create the mapping from index source look up <PLACE_HOLDER> to probe key input,immutable set multimap . builder < symbol @$ integer > builder = immutable set multimap . builder ( ) ; for ( map . entry < symbol @$ symbol > entry : index key trace . entry set ( ) ) { symbol index join symbol = entry . get key ( ) ; symbol index lookup symbol = entry . get value ( ) ; builder . put all ( index lookup symbol @$ index to probe key input . get ( index join symbol ) ) ; } return builder . build ( ) ;,mapping look
null indicates a static method which may still need generics <PLACE_HOLDER>,generics type [ ] generics types = helper method . get generics types ( ) ; if ( generics types != null ) { map < string @$ class node > method spec = generics utils . add method generics ( helper method @$ collections . empty map ( ) ) ; generics type [ ] new gt = generics utils . apply generics context to place holders ( method spec @$ helper method . get generics types ( ) ) ; forwarder . set generics types ( new gt ) ; },which need
failure to handle the event cancels <PLACE_HOLDER> .,if ( ! handled event || clear pressed item ) { clear pressed item ( ) ; },failure cancels
the remaining ids reference <PLACE_HOLDER> that do n't exist in db . they must be deleted from index .,remaining . values ( ) . for each ( item -> bulk indexer . add deletion ( type_active_rule @$ item . get doc id ( ) @$ item . get doc routing ( ) ) ) ; return bulk indexer . stop ( ) ;,the ids
set the new mark so that next time we get new <PLACE_HOLDER> since this point .,if ( scan time since mark ms > __num__ ) { u . m bluetooth scan timer . set mark ( elapsed realtime ms ) ; long scan time rx since mark ms = scan time since mark ms ; long scan time tx since mark ms = scan time since mark ms ; if ( normalize scan rx time ) { scan time rx since mark ms = ( rx time ms * scan time rx since mark ms ) / total scan time ms ; } if ( normalize scan tx time ) { scan time tx since mark ms = ( tx time ms * scan time tx since mark ms ) / total scan time ms ; } final controller activity counter impl counter,time get
on t vs @$ if the app does n't implement <PLACE_HOLDER> @$ we want to launch assist .,if ( ! result && ( get context ( ) . get resources ( ) . get configuration ( ) . ui mode & configuration . ui_mode_type_mask ) == configuration . ui_mode_type_television ) { bundle args = new bundle ( ) ; args . put int ( intent . extra_assist_input_device_id @$ event . get device id ( ) ) ; return ( ( search manager ) get context ( ) . get system service ( context . search_service ) ) . launch legacy assist ( null @$ get context ( ) . get user id ( ) @$ args ) ; },app implement
result of this conversion will exceed the original <PLACE_HOLDER> and cause a newline to be inserted,string data [ ] = { __str__ @$ __str__ @$ __str__ @$ __str__ } ;,result exceed
third heartbeat which reports second <PLACE_HOLDER> as pending . third <PLACE_HOLDER> is scheduled .,response = spy service . heartbeat ( stat ) ; assert equals ( localizer action . live @$ response . get localizer action ( ) ) ; final string loc path2 = response . get resource specs ( ) . get ( __num__ ) . get destination directory ( ) . get file ( ) ;,which reports
we have only two handlers . the first thread will get a write <PLACE_HOLDER> for row 1 and occupy the first handler . the second thread need a read <PLACE_HOLDER> for row 1 @$ it should quit after 1000 ms and give back the handler because it can not get the <PLACE_HOLDER> in time . so we can get the value using the second,try ( table table = test_util . get connection ( ) . get table builder ( table name @$ null ) . set rpc timeout ( __num__ ) . build ( ) ) { table . get ( new get ( row2 ) ) ; } finally { increment thread . interrupt ( ) ; get thread . interrupt ( ) ; },thread get
unicode now contains the four hex digits which represents our unicode <PLACE_HOLDER>,if ( unicode . length ( ) == __num__ ) { int value = integer . parse int ( unicode . to string ( ) @$ __num__ ) ; out . write ( ( char ) value ) ; unicode . set length ( __num__ ) ; in unicode = false ; had slash = false ; },which represents
original capacity assumes 20 <PLACE_HOLDER> per headers,string builder sb = new string builder ( simple name . length ( ) + __num__ + size * __num__ ) . append ( simple name ) . append ( __str__ ) ; while ( headers it . has next ( ) ) { entry < ? @$ ? > header = headers it . next ( ) ; sb . append ( header . get key ( ) ) . append ( __str__ ) . append ( header . get value ( ) ) . append ( __str__ ) ; } sb . set length ( sb . length ( ) - __num__ ) ; return sb . append ( __str__ ) . to string ( ) ;,capacity assumes
if the existing monitor has already been cancelled @$ then do not apply the <PLACE_HOLDER>,if ( delegate . is cancelled ( ) ) { new delegate . cancel ( ) ; return ; } for ( cancelled listener l : listeners ) { new delegate . add cancelled listener ( l ) ; delegate . remove cancelled listener ( l ) ; } new delegate . set maximum ( delegate . get maximum ( ) ) ; new delegate . set progress ( delegate . get progress ( ) ) ; new delegate . set message ( delegate . get message ( ) ) ; new delegate . set indeterminate ( delegate . is indeterminate ( ) ) ; new delegate . set cancel enabled ( delegate . is cancel enabled ( ) ) ; this . delegate = new delegate ;,then apply
make sure the d ns do n't send a <PLACE_HOLDER> for a while @$ so the blocks wo n't actually get completed during lease recovery .,for ( data node dn : cluster . get data nodes ( ) ) { data node test utils . set heartbeats disabled for tests ( dn @$ true ) ; },ns send
1 : no stats since hbm.xml ca n't enable natural id <PLACE_HOLDER>,assert equals ( __num__ @$ session factory ( ) . get statistics ( ) . get natural id cache put count ( ) ) ;,stats enable
create directories one by one with explicit <PLACE_HOLDER>s to ensure no umask is applied @$ using mkdirs will apply the <PLACE_HOLDER> only to the last directory,stack < path > dirs to make = new stack < > ( ) ; dirs to make . push ( hdfs path ) ; path parent = hdfs path . get parent ( ) ; while ( ! hdfs . exists ( parent ) ) { dirs to make . push ( parent ) ; parent = parent . get parent ( ) ; } while ( ! dirs to make . empty ( ) ) { path dir to make = dirs to make . pop ( ) ; if ( ! file system . mkdirs ( hdfs @$ dir to make @$ new fs permission ( options . get mode ( ) . to short ( ) ) ) ) { return false ; },mkdirs apply
the stdouterr file collects all the <PLACE_HOLDER> and system.err writes to disk .,string stdouterrfile = util . extract string option ( __str__ @$ settings ) ;,file collects
notify the listeners . do that from the end of the list so that if a listener removes <PLACE_HOLDER> as the result of being called @$ it wo n't mess up with our iteration,if ( m listeners != null ) { int listener count = m listeners . size ( ) ; for ( int i = listener count - __num__ ; i >= __num__ ; i -- ) { m listeners . get ( i ) . on drawer closed ( drawer view ) ; } },listener removes
stomp out any bad characters since this is from a circular buffer a corruption is seen sometimes that results in the vm crashing this should prevent <PLACE_HOLDER> and the line will probably fail to parse,for ( int j = start index ; j < end index ; j ++ ) { if ( ( wl buffer [ j ] & __num__ ) != __num__ ) wl buffer [ j ] = ( byte ) __str__ ; } boolean parsed = process . parse proc line ( wl buffer @$ start index @$ end index @$ wakeup_sources ? wakeup_sources_format : proc_wakelocks_format @$ name string array @$ wl data @$ null ) ; name = name string array [ __num__ ] . trim ( ) ; count = ( int ) wl data [ __num__ ] ; if ( wakeup_sources ) { total time = wl data [ __num__ ] * __num__ ; } else { total time = ( wl data [ __num__,results prevent
a fourth rebalance should not change <PLACE_HOLDER>,apply assignments ( returned assignments ) ; member configs = member configs ( leader @$ offset @$ assignments ) ; assignor . perform task assignment ( leader @$ offset @$ member configs @$ coordinator @$ protocol version ) ; ++ rebalance num ; returned assignments = assignments capture . get value ( ) ; assert delay ( __num__ @$ returned assignments ) ; expected member configs = member configs ( leader @$ offset @$ returned assignments ) ; assert no reassignments ( member configs @$ expected member configs ) ; assert assignment ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __str__ ) ; verify ( coordinator @$ times ( rebalance num ) ) . config snapshot ( ) ; verify ( coordinator @$ times (,rebalance change
consume all events ; signal that our text area will handle <PLACE_HOLDER>,e . consume ( ) ;,area handle
button text can have a different <PLACE_HOLDER>,if ( m ok color == null ) m ok color = m accent color ; m ok button . set text color ( m ok color ) ; if ( m cancel color == null ) m cancel color = m accent color ; m cancel button . set text color ( m cancel color ) ; if ( get dialog ( ) == null ) { view . find view by id ( r . id . mdtp_done_background ) . set visibility ( view . gone ) ; } int circle background = context compat . get color ( context @$ r . color . mdtp_circle_background ) ; int background color = context compat . get color ( context @$ r . color . mdtp_background_color ),text have
now try <PLACE_HOLDER> and it should work .,admin . split region async ( hri . get region name ( ) ) . get ( __num__ @$ time unit . minutes ) ;,now try
find the next index @$ in case the assumed next index is not uni<PLACE_HOLDER>ue . for instance @$ if there is no p @$ then re<PLACE_HOLDER>uest for p 's position actually returns <PLACE_HOLDER> 's . so we need to look ahead to make sure that there is really a <PLACE_HOLDER> at <PLACE_HOLDER> 's position . if not @$ move further down ...,int next next section = next section + __num__ ; while ( next next section < section count && m section indexer . get position for section ( next next section ) == next index ) { next next section ++ ; next section ++ ; },request returns
should use root entity <PLACE_HOLDER> by default,criteria executor criteria executor = new criteria executor ( ) { protected criteria get criteria ( session s ) { return s . create criteria ( enrolment . class @$ __str__ ) . create alias ( __str__ @$ __str__ @$ criteria . left_join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set projection ( projections . projection list ( ) . add ( projections . property ( __str__ ) ) . add ( projections . property ( __str__ ) ) ) . add order ( order . asc ( __str__ ) ) ; } } ;,use root
completing the user event listener should terminate the <PLACE_HOLDER>,user event listener instance user event listener instance = cmmn runtime service . create user event listener instance query ( ) . case instance id ( case instance . get id ( ) ) . single result ( ) ; assert that ( user event listener instance . get id ( ) ) . is equal to ( user event listener plan item instance . get id ( ) ) ; cmmn runtime service . complete user event listener instance ( user event listener instance . get id ( ) ) ; assert case instance ended ( case instance ) ;,listener terminate
verify user from a group which has column family level <PLACE_HOLDER> can read all the data belonging to that family and group which has no <PLACE_HOLDER> ca n't read any data .,grant on table ( test_util @$ testgroup_1_name @$ table name @$ test_family @$ null @$ permission . action . read ) ; verify allowed ( testgroup1_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup1_user1 @$ scan family action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan family action for group with family level access ) ;,which has
we deduced thoughts from the inferences in the intents . the keynote also carries these <PLACE_HOLDER>,this . get actions clone ( ) . for each ( action -> keynote . add action ( action ) ) ;,keynote carries
expected @$ since android does n't have a validating <PLACE_HOLDER> .,document builder db = dbf . new document builder ( ) ;,android have
out stream has <PLACE_HOLDER> .,throw new illegal state exception ( ioex ) ;,stream has
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default remote profile = new crawl profile ( crawl_profile_remote @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ null @$ - __num__ @$ true @$ true @$ true @$ false @$ true @$ true @$ false @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . iffresh @$ __str__ + crawl_profile_remote @$ client identification . yacy internet crawler agent name @$ null @$ null @$ __num__ ) ;,content match
we can not determine <PLACE_HOLDER> method is the most specific because one parameter of the first candidate was more specific and another parameter of the second candidate was more specific .,if ( best match != null && ! potential match . equals ( best match ) ) { return null ; } else { best match = potential match ; },parameter specific
default to number class in exception details @$ else use the specified number <PLACE_HOLDER> .,return cast to number ( object @$ number . class ) ;,default use
sql result type <PLACE_HOLDER> @$,finish initialization ( query return type list ) ;,sql result
use configuration that is same as <PLACE_HOLDER> the transition will change .,use configuration ( immutable map . of ( __str__ @$ __str__ ) ) ; configured target test = get configured target ( __str__ ) ; @ suppress warnings ( __str__ ) configured target dep = iterables . get only element ( ( list < configured target > ) get my info from target ( test ) . get value ( __str__ ) ) ; assert that ( get core options ( test ) . transition directory name fragment ) . is null ( ) ; assert that ( get core options ( dep ) . transition directory name fragment ) . is not null ( ) ;,transition change
we will be calling a function of index & passing the other index as a parameter . we will be getting list object . each element of the list will contain a two dimesional object <PLACE_HOLDER> . the first row will contain the result objects of the first index & secondrow will contain that of second index . the object contained in each of,query observer observer = query observer holder . get instance ( ) ; context . cache put ( compiled value . index_info @$ indx info ) ;,element contain
seasonal does n't have boss <PLACE_HOLDER>,if ( index < skills . size ( ) ) { hiscore result . set abyssal sire ( skills . get ( index ++ ) ) ; hiscore result . set alchemical hydra ( skills . get ( index ++ ) ) ; hiscore result . set barrows chests ( skills . get ( index ++ ) ) ; hiscore result . set bryophyta ( skills . get ( index ++ ) ) ; hiscore result . set chambers of xeric ( skills . get ( index ++ ) ) ; hiscore result . set chambers of xeric challenge mode ( skills . get ( index ++ ) ) ; hiscore result . set chaos elemental ( skills . get ( index ++ ) ) ; hiscore result,seasonal have
playback play sound <PLACE_HOLDER> .,if ( feature notify and playback devices ) { cnstrnts . gridy = __num__ ; playback play sound button = new j button ( new image icon ( neomedia activator . get resources ( ) . get image in bytes ( __str__ ) ) ) ; playback play sound button . set minimum size ( new dimension ( __num__ @$ __num__ ) ) ; playback play sound button . set preferred size ( new dimension ( __num__ @$ __num__ ) ) ; if ( ( ( device configuration combo box model . capture device ) playback combo . get selected item ( ) ) . info == null ) { playback play sound button . set enabled ( false ) ; } playback play sound button . set,playback play
no security exception @$ do the <PLACE_HOLDER> .,int source user id = content provider . get user id from uri ( uri @$ m user ) ; uri = content provider . get uri without user id ( uri ) ; uri grants manager . get service ( ) . grant uri permission from owner ( m permission owner @$ src uid @$ dest pkg @$ uri @$ flag_grant_read_uri_permission @$ source user id @$ m user ) ; binder . restore calling identity ( ident ) ;,exception do
have that the null does not interfere with the current equal key series @$ if there is one . we do not set save join result . let a current match equal key series keep going @$ or let a current spill equal key series keep going @$ or let a current nomatch keep not <PLACE_HOLDER> .,if ( is null ) { at least one non match = true ; } else { if ( ! have save key || string expr . equal ( vector [ save key batch index ] @$ start [ save key batch index ] @$ length [ save key batch index ] @$ vector [ batch index ] @$ start [ batch index ] @$ length [ batch index ] ) == false ) { if ( have save key ) { switch ( save join result ) { case match : hash map result count ++ ; equal key series count ++ ; break ; case spill : hash map result count ++ ; break ; case nomatch : break ; } } have save key =,series keep
moving a day the notification q calls the commit <PLACE_HOLDER> . now payment is expected,bus handler . push expected events ( next event . invoice @$ next event . payment @$ next event . invoice_payment ) ; clock . add days ( __num__ ) ; assert listener status ( ) ; parent invoice = invoice user api . get invoice ( parent invoice . get id ( ) @$ call context ) ; assert equals ( parent invoice . get status ( ) @$ invoice status . committed ) ; assert equals ( parent invoice . get balance ( ) . compare to ( big decimal . zero ) @$ __num__ ) ;,q calls
bug : diagnostic contains : missing disposable handling : apply auto dispose or cache the disposable <PLACE_HOLDER> manually and enable lenient mode .,single . just ( __num__ ) . subscribe ( ) ;,auto dispose
simulate a 2 nn beginning a <PLACE_HOLDER> @$ but not finishing . this will cause name 1 to be restored .,cluster . get name node rpc ( ) . roll edit log ( ) ; print storages ( fs image ) ;,nn beginning
pass empty name as child reference @$ should return parent <PLACE_HOLDER>,logger log2 = log . get logger ( __str__ ) ; assert that ( __str__ @$ log2 . get name ( ) @$ is ( __str__ ) ) ; assert same ( log2 @$ log @$ __str__ ) ;,name return
xml parser only does string <PLACE_HOLDER>,return get field value ( ) ;,parser does
tests whether the width of the picture changes trough all qualities and every step has a few <PLACE_HOLDER> .,try { int n = __num__ ; int m = __num__ ; int q = __num__ ; int [ ] qualities = { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ; while ( ! end requested ) { frame source = fg . grab frame ( ) ; n ++ ; m ++ ; if ( source . image width != qualities [ q ] ) { q ++ ; assert equals ( source . image width @$ qualities [ q ] ) ; assert true ( m > __num__ ) ; assert true ( m <= __num__ ) ; m = __num__ ; } fr . record ( source ) ; } assert equals ( q @$ qualities,tests has
update controllers pose <PLACE_HOLDER>,environment . getv rinput ( ) . update controller states ( ) ;,controllers pose
the region state node may have no <PLACE_HOLDER> in a test scenario ; allow for this .,update user region location ( region state node . get region info ( ) @$ region state node . get state ( ) @$ region state node . get region location ( ) @$ open seq num @$ region state node . get procedure ( ) != null ? region state node . get procedure ( ) . get proc id ( ) : procedure . no_proc_id ) ;,node have
then : above command should not throw any <PLACE_HOLDER>,authorization validator . check authorization ( service context @$ meta store @$ statement ) ;,command throw
parsing should go all the way to the end of the string . we want the longest match @$ and we do n't care if the plural <PLACE_HOLDER> of the unit matches the plural <PLACE_HOLDER> of the number .,assert equals ( __str__ @$ parse string . length ( ) @$ ppos . get index ( ) ) ;,form matches
closing the entity manager factory should close the <PLACE_HOLDER>,em . get entity manager factory ( ) . close ( ) ;,factory close
no fragment may be present on a toolchain rule in retroactive trimming <PLACE_HOLDER> . this is because trimming expects that platform and toolchain resolution uses only the platform configuration . in theory @$ this means toolchains could use platforms @$ but the current expectation is that toolchains should not use anything at all @$ so better to go with the stricter expectation for now,if ( configuration . trim configurations retroactively ( ) && ! target . get configuration key ( ) . get fragments ( ) . is empty ( ) ) { string extra fragment description = target . get configuration key ( ) . get fragments ( ) . stream ( ) . map ( cl -> cl . get simple name ( ) ) . collect ( joining ( __str__ ) ) ; throw new registered toolchains function exception ( new invalid toolchain label exception ( toolchain label @$ __str__ + __str__ + __str__ + extra fragment description + __str__ ) @$ transience . persistent ) ; },configuration trimming
the data will include corrected <PLACE_HOLDER> :,assert equals ( __num__ @$ histogram . get count at value ( __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get total count ( ) ) ;,data include
the copying thread wo n't generally detect <PLACE_HOLDER>,swin . interrupt ( ) ;,thread detect
no tables have an empty <PLACE_HOLDER>,try ( connection connection = create connection ( ) ) { try ( result set rs = connection . get meta data ( ) . get tables ( __str__ @$ null @$ null @$ null ) ) { assert table metadata ( rs ) ; assert equals ( read rows ( rs ) . size ( ) @$ __num__ ) ; } } try ( connection connection = create connection ( ) ) { try ( result set rs = connection . get meta data ( ) . get tables ( test_catalog @$ __str__ @$ null @$ null ) ) { assert table metadata ( rs ) ; set < list < object > > rows = immutable set . copy of ( read rows ( rs ),tables have
native methods may have null <PLACE_HOLDER>,if ( e . get file name ( ) != null ) { assert that ( json ) . contains ( __str__ + e . get file name ( ) + __str__ ) ; },methods have
note that view.on initialize accessibility node <PLACE_HOLDER> was introduced in ics and we would like to tweak a bit the text that is reported to accessibility services via the accessibility node <PLACE_HOLDER> .,info . set text ( get context ( ) . get string ( r . string . accessibility_delegate_custom_text_added ) ) ;,view.on initialize
can only apply if both sides have <PLACE_HOLDER> .,return ;,sides have
action has no configuration @$ we just keep this <PLACE_HOLDER> for the future,if ( config != null ) { },configuration keep
make sure that an app with coarse permissions ca n't get frequent location <PLACE_HOLDER> by calling location manager.get last known location repeatedly .,if ( allowed resolution level < resolution_level_fine ) { location = m last location coarse interval . get ( name ) ; } else { location = m last location . get ( name ) ; },app get
callback might have started an edge <PLACE_HOLDER> .,if ( m drag state == state_dragging ) { break ; },callback started
just use the first <PLACE_HOLDER> in the batch here to satisfy the <PLACE_HOLDER> cursor . the truth is that we 'll be using the read method which accepts an external <PLACE_HOLDER> anyway so it does n't matter .,try ( page cursor cursor = store . open page cursor for reading ( id ) ) { boolean has next = true ; while ( has next ) { if ( assembler . append ( store @$ cursor @$ batch @$ id @$ i ) ) { i ++ ; } if ( has next = id range . has next ( ) ) { id = id range . next ( ) ; } } } sender . send ( assembler . cut off at ( batch @$ i ) ) ;,which accepts
always consume the drag so that java does not change the <PLACE_HOLDER>,e . consume ( ) ;,java change
otherwise register a dummy provider which would provoke the <PLACE_HOLDER> of bug 8139436,boolean provider prepended = false ; string testprovider = system . get property ( __str__ ) ; if ( testprovider != null && ! testprovider . is empty ( ) ) { try { system . out . println ( __str__ + testprovider ) ; class < ? > providerclass = class . for name ( testprovider ) ; object provider = providerclass . new instance ( ) ; security . insert provider at ( ( provider ) provider @$ __num__ ) ; } catch ( exception e ) { system . out . println ( __str__ + testprovider + __str__ ) ; e . print stack trace ( system . out ) ; } provider prepended = true ; system . out . println ( __str__ +,which provoke
we catch throwable here because netty uses clever <PLACE_HOLDER> to have method signatures that look like they do not throw checked exceptions @$ but they actually do . the compiler wo n't let us catch them explicitly because in theory they should n't be possible @$ so we have to catch throwable and do our own checks to grab them,throw new port bind exception ( initializer . address ( ) @$ e ) ;,netty uses
system apps have <PLACE_HOLDER> over where their default storage context is pointed @$ so we 're always explicit when building paths .,try { final context ce context = create credential protected storage context ( ) ; root dir = ce context . get data dir ( ) . get canonical path ( ) ; files dir = ce context . get files dir ( ) . get canonical path ( ) ; nb files dir = ce context . get no backup files dir ( ) . get canonical path ( ) ; db dir = ce context . get database path ( __str__ ) . get parent file ( ) . get canonical path ( ) ; sp dir = ce context . get shared preferences path ( __str__ ) . get parent file ( ) . get canonical path ( ) ; cache dir = ce context,apps have
check if the crypto <PLACE_HOLDER>s granted to the app contain a crypto <PLACE_HOLDER> for the requested algorithm that does not require any exemption mechanism to be enforced . return that <PLACE_HOLDER> @$ if present .,permission collection app pc = app perms . get permission collection ( alg ) ; if ( app pc == null ) { return default perm ; } enumeration < permission > enum_ = app pc . elements ( ) ; while ( enum_ . has more elements ( ) ) { crypto permission cp = ( crypto permission ) enum_ . next element ( ) ; if ( cp . get exemption mechanism ( ) == null ) { return cp ; } },check return
generate the strongest key using the shared secret assuming the key sizes in aes constants class are in ascending <PLACE_HOLDER>,while ( skey == null && idx >= __num__ ) { if ( keysize >= aes constants . aes_keysizes [ idx ] ) { keysize = aes constants . aes_keysizes [ idx ] ; skey = new secret key spec ( secret @$ __num__ @$ keysize @$ __str__ ) ; } idx -- ; },class ascending
all surrogate pairs with this lead surrogate have only irrelevant <PLACE_HOLDER>,if ( ( norm32 & mask ) == __num__ ) { return __num__ ; } else { return get norm32 from surrogate pair ( norm32 @$ args . c ) ; },pairs have
if optimize hive.optimize.bucketmapjoin.sortedmerge is set @$ add both bucket map join <PLACE_HOLDER> and sorted merge bucket map join <PLACE_HOLDER>,if ( ( hive conf . get bool var ( hive conf @$ hive conf . conf vars . hiveoptsortmergebucketmapjoin ) ) && ! is tez exec engine && ! is spark exec engine ) { if ( ! bucket map join optimizer ) { transformations . add ( new bucket map join optimizer ( ) ) ; } transformations . add ( new sorted merge bucket map join optimizer ( ) ) ; },map join
make sure that the pattern matches a <PLACE_HOLDER> by one .,if ( ! b . is java constant ( ) ) { return null ; } java constant b cst = b . as java constant ( ) ; long b value ; if ( b cst . get java kind ( ) == java kind . int ) { b value = b cst . as int ( ) ; } else if ( b cst . get java kind ( ) == java kind . long ) { b value = b cst . as long ( ) ; } else { return null ; } if ( b value == - __num__ ) { return builder -> get arithmeticlir generator ( ) . emit get mask up to lowest set bit ( operand ( a,pattern matches
otherwise can only duplicate if enough <PLACE_HOLDER> .,return true ;,otherwise duplicate
create calendar which omits <PLACE_HOLDER>,for ( action . handler action handler : action handlers ) { gregorian calendar cal = new gregorian calendar ( get time zone ( ) @$ get locale ( ) ) ; cal . clear ( ) ; cal . set ( current calendar . get ( java . util . calendar . year ) @$ current calendar . get ( java . util . calendar . month ) @$ current calendar . get ( java . util . calendar . date ) ) ; date start = cal . get time ( ) ; cal . add ( java . util . calendar . date @$ __num__ ) ; cal . add ( java . util . calendar . second @$ - __num__ ) ; date end,which omits
assume root always has a <PLACE_HOLDER>,if ( is root ( path ) ) { return true ; } string parent key = get parent path ( path ) ; return parent key != null && is directory ( parent key ) ;,root has
proprietary class format @$ can not set or check logical <PLACE_HOLDER> for now @$ just return,if ( cla < __num__ ) { return ; },format set
check each new char to make sure it matches <PLACE_HOLDER> the group referenced matched last time around,int x = i ; for ( int index = __num__ ; index < group size ; index ++ ) { int c1 = character . code point at ( seq @$ x ) ; int c2 = character . code point at ( seq @$ j ) ; if ( c1 != c2 ) { if ( do unicode case ) { int cc1 = character . to upper case ( c1 ) ; int cc2 = character . to upper case ( c2 ) ; if ( cc1 != cc2 && character . to lower case ( cc1 ) != character . to lower case ( cc2 ) ) return false ; } else { if ( ascii . to lower ( c1 ) != ascii,group referenced
since some records do n't have all possible <PLACE_HOLDER> @$ we initialize average coordinates equal to current centroid coordinates,map < string @$ double > average = centroid . get coordinates ( ) ;,records have
wild card ipv 6 single broadcast ipv 6 <PLACE_HOLDER> : fe 80 : xx : xx ... loopback ipv 6 <PLACE_HOLDER> site local ipv 6 <PLACE_HOLDER> : fec 0 : xx : xx ...,if ( inet addr . is any local address ( ) || inet addr . is link local address ( ) || inet addr . is loopback address ( ) || inet addr . is site local address ( ) ) { return true ; },card ipv
explicit <PLACE_HOLDER> provider implies <PLACE_HOLDER>,config . get ( key_authenticator ) . as string ( ) . if present ( value -> { if ( ! config . get ( key_authenticate ) . exists ( ) ) { builder . authenticate ( true ) ; } } ) ;,provider implies
count keys as equal if they have the same key <PLACE_HOLDER> .,return o instanceof g infopc && arrays . equals ( _gs @$ ( ( g infopc ) o ) . _gs ) ;,keys have
app 1 should have 7 <PLACE_HOLDER> now @$ and no available resource for cluster,fi ca scheduler app scheduler app1 = cs . get application attempt ( am1 . get application attempt id ( ) ) ; assert . assert equals ( __num__ @$ scheduler app1 . get live containers ( ) . size ( ) ) ;,app have
renewing <PLACE_HOLDER> and adding it to timer calls are separated purposefully if user provides incorrect <PLACE_HOLDER> then it should not be added for renewal .,if ( ! token list . is empty ( ) ) { for ( delegation token to renew dtr : token list ) { delegation token to renew current dtr = all tokens . put if absent ( dtr . token @$ dtr ) ; if ( current dtr != null ) { current dtr . referring app ids . add ( application id ) ; app tokens . get ( application id ) . add ( current dtr ) ; } else { app tokens . get ( application id ) . add ( dtr ) ; set timer for token renewal ( dtr ) ; } } },user provides
at this point we are done processing the input . close the record <PLACE_HOLDER>,if ( ! is closed ) { close record processor ( ) ; is closed = true ; },input close
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,object [ ] [ ] contents = new object [ ] [ ] { { msg key . bad_msgkey @$ __str__ } @$ { msg key . bad_msgformat @$ __str__ } @$ { msg key . er_serializer_not_contenthandler @$ __str__ } @$ { msg key . er_resource_could_not_find @$ __str__ } @$ { msg key . er_resource_could_not_load @$ __str__ } @$ { msg key . er_buffer_size_lessthan_zero @$ __str__ } @$ { msg key . er_invalid_utf16_surrogate @$ __str__ } @$ { msg key . er_oierror @$ __str__ } @$ { msg key . er_illegal_attribute_position @$ __str__ } @$ { msg key . er_namespace_prefix @$ __str__ } @$ { msg key . er_stray_attribute @$ __str__ } @$ { msg key . er_stray_namespace @$ __str__ } @$ { msg key . er_could_not_load_resource @$,text specifies
interactive bugreports show progress <PLACE_HOLDER> .,assert that ( callback . has received progress ( ) ) . is true ( ) ; assert that ( callback . get error code ( ) ) . is equal to ( bugreport callback . bugreport_error_user_consent_timed_out ) ; assert fds are closed ( m bugreport fd ) ;,bugreports show
if namespace parents are implicitly created @$ they wo n't have ac ls . so @$ let 's explicitly create <PLACE_HOLDER> .,curator framework null ns fw = zk client . using namespace ( null ) ; ensure path ensure ns = null ns fw . new namespace aware ensure path ( __str__ + zk client . get namespace ( ) ) ; try { ensure ns . ensure ( null ns fw . get zookeeper client ( ) ) ; } catch ( exception e ) { throw new io exception ( __str__ @$ e ) ; },let create
no rows to recycle but update the spacer <PLACE_HOLDER>,spacer container . update spacer indexes for row and after ( index @$ index + number of rows - added row count @$ number of rows - added row count ) ; double new rows height = number of rows * default row height ; if ( added row count > __num__ ) { move viewport and content ( null @$ __num__ @$ __num__ @$ new rows height ) ; row visibility changed = true ; } else { move viewport and content ( index @$ new rows height @$ new rows height @$ new rows height ) ; },rows recycle
the offset is just to get the right stack <PLACE_HOLDER> highlighted in the output,if ( interpreter frame method != null ) { int offset = __num__ ; anno panel . add annotation ( new annotation ( cur frame . address of interpreter frame local ( offset ) @$ cur frame . address of interpreter frame local ( ( int ) interpreter frame method . get max locals ( ) + offset ) @$ __str__ + cur frame . getsp ( ) ) ) ; },right stack
normalization can not remove leading uplevel <PLACE_HOLDER> @$ so this will be true,assert that ( create ( __str__ ) . contains uplevel references ( ) ) . is true ( ) ;,normalization remove
the directory entries typically have different <PLACE_HOLDER>s to the files @$ e.g . execute <PLACE_HOLDER> or ca n't cd into it,files . walk file tree ( target @$ new simple file visitor < path > ( ) { public file visit result pre visit directory ( path dir @$ basic file attributes attrs ) throws io exception { files . set posix file permissions ( dir @$ dir permissions ) ; return file visit result . continue ; } public file visit result visit file ( path file @$ basic file attributes attrs ) throws io exception { files . set posix file permissions ( file @$ permissions ) ; return file visit result . continue ; } } ) ;,directory execute
decrement llc simultaneous segment <PLACE_HOLDER> .,_server metrics . add value to global gauge ( server gauge . llc_simultaneous_segment_builds @$ - __num__ ) ;,decrement llc
prints 0 <PLACE_HOLDER> <PLACE_HOLDER> <PLACE_HOLDER> <PLACE_HOLDER> ... .,no inline ( __str__ ) ;,prints 0
make sure gl does not use our <PLACE_HOLDER> before we start computing,if ( syncc ltogl || should init buffers ) { gl finish ( ) ; } if ( should init buffers ) { initgl objects ( ) ; set kernel constants ( ) ; } if ( rebuild ) { build program ( ) ; set kernel constants ( ) ; } computecl ( double precision ) ; rendergl ( ) ;,gl use
bord<PLACE_HOLDER>r b<PLACE_HOLDER>tw<PLACE_HOLDER><PLACE_HOLDER>n th<PLACE_HOLDER> til<PLACE_HOLDER>s n 42 <PLACE_HOLDER> 011 and n 43 <PLACE_HOLDER> 011,assert equals ( __str__ @$ instance . get file name ( __num__ @$ __num__ ) ) ; assert equals ( __num__ @$ instance . get ele ( __num__ @$ __num__ ) @$ precision ) ; assert equals ( __str__ @$ instance . get file name ( __num__ @$ __num__ ) ) ; assert equals ( __num__ @$ instance . get ele ( __num__ @$ __num__ ) @$ precision ) ;,border n
the log level would actually allow a <PLACE_HOLDER> to be logged .,if ( log warnings ) { try { walk warnings ( statement . get warnings ( ) @$ handler ) ; } catch ( sql exception sql exception ) { log . debug ( __str__ @$ sql exception ) ; } },level allow
assume something generous if we have no <PLACE_HOLDER>,m interarrival time = __num__ ;,something have
load the texture again . but this time @$ force the gwt <PLACE_HOLDER> of pixmap to move to a canvas representation of the image,pixmap pixmap = new pixmap ( gdx . files . internal ( __str__ ) ) ; pixmap . get pixel ( __num__ @$ __num__ ) ; file texture data data1 = new file texture data ( null @$ pixmap @$ null @$ false ) ; bad texture = new texture ( data1 ) ;,texture force
add nodes to cluster @$ so cluster have 20 <PLACE_HOLDER> and 20 vcores,resource new resource = resource . new instance ( __num__ * gb @$ __num__ ) ; rm node node = mock nodes . new node info ( __num__ @$ new resource @$ __num__ @$ __str__ ) ; cs . handle ( new node added scheduler event ( node ) ) ; resource new resource2 = resource . new instance ( __num__ * gb @$ __num__ ) ; rm node node2 = mock nodes . new node info ( __num__ @$ new resource2 @$ __num__ @$ __str__ ) ; cs . handle ( new node added scheduler event ( node2 ) ) ; fi ca scheduler app fi ca app1 = cs . get scheduler applications ( ) . get ( app . get application id ( ) ),cluster have
chromium 's info <PLACE_HOLDER> container may add an info <PLACE_HOLDER> immediately during this initialization call @$ so make sure everything in the info <PLACE_HOLDER> container is completely ready beforehand .,m native info bar container = native init ( ) ;,container add
items to test <PLACE_HOLDER>,item = new privacy item ( privacy item . type . group . name ( ) @$ false @$ i ) ; item . set value ( group name ) ; item . set filter message ( true ) ; original privacy items [ i ] = item ; i = i + __num__ ;,items test
closes disk file which will flush all <PLACE_HOLDER>,close down ( ) ;,which flush
user 4 has another global <PLACE_HOLDER> via a group,group dto administrator group3 = db . users ( ) . insert group ( organization2 ) ; db . users ( ) . insert permission on group ( administrator group3 @$ quality_profile_admin ) ; user dto user4 = db . users ( ) . insert user ( with email ( __str__ ) ) ; db . users ( ) . insert member ( administrator group3 @$ user4 ) ; component dto project = db . components ( ) . insert private project ( ) ;,user has
hive does not support result set meta data on prepared statement @$ and hive describe does not support <PLACE_HOLDER> @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,hive support
check if method has <PLACE_HOLDER> that need profiling,if ( graph . get nodes ( ) . filter ( invoke node . class ) . filter ( ( n ) -> ( ( invoke node ) n ) . get invoke kind ( ) . is indirect ( ) ) . count ( ) > options . simple method indirect calls . get default value ( ) ) { return false ; } return true ;,method has
let 's use the link <PLACE_HOLDER> as default title when no mapping is defined .,if ( link element . has text ( ) && ! this . html mapping . contains key ( __str__ ) ) { doc . set field ( collection schema . title . get solr field name ( ) @$ link element . text ( ) ) ; },let use
check has audio <PLACE_HOLDER> just in case !,if ( ! has audio control ( ) ) { if ( my debug . log ) log . e ( tag @$ __str__ ) ; return ; } this . close popup ( ) ; shared preferences shared preferences = get default shared preferences ( this ) ; string audio_control = shared preferences . get string ( preference keys . get audio control preference key ( ) @$ __str__ ) ; if ( audio_control . equals ( __str__ ) && speech recognizer != null ) { if ( speech recognizer is started ) { speech recognizer . stop listening ( ) ; speech recognizer stopped ( ) ; } else { preview . show toast ( audio_control_toast @$ r . string . speech_recognizer_started ) ; intent intent,check has
now make the <PLACE_HOLDER>er cancel the <PLACE_HOLDER> .,basic telephonyp1 . hangup call peer ( peer atp1 ) ;,caller cancel
append will fail when the file size crosses the checksum chunk <PLACE_HOLDER> @$ if append was called with a stale file stat .,do small appends ( file @$ fs @$ __num__ ) ;,size crosses
return the snapshot from the update <PLACE_HOLDER> as another thread may have already flushed the <PLACE_HOLDER>,return my update request . get finished index snapshot ( ) ;,thread flushed
noop as lzma output stream throws an <PLACE_HOLDER> in flush,return new flush shield filter output stream ( new lzma output stream ( out @$ get options ( opts ) @$ false ) ) ;,stream throws
the class cast exception might occur <PLACE_HOLDER> of the completion stage which might be confusing,return apply ( publisher @$ ( class < r > ) type ) . then apply ( type :: cast ) ;,exception occur
first @$ with debug @$ which should not log index writer <PLACE_HOLDER> :,try { parsed document doc = test parsed document ( __str__ @$ null @$ test document with text field ( ) @$ b_1 @$ null ) ; engine . index ( index for doc ( doc ) ) ; engine . flush ( ) ; assert false ( mock appender . saw index writer message ) ; loggers . set level ( root logger @$ level . trace ) ; engine . index ( index for doc ( doc ) ) ; engine . flush ( ) ; assert true ( mock appender . saw index writer message ) ; } finally { loggers . remove appender ( root logger @$ mock appender ) ; mock appender . stop ( ) ; loggers . set level ( root,which log
parser does n't support this message <PLACE_HOLDER> in this version .,return false ;,parser support
if the process wrapper is enabled @$ we use its timeout feature @$ which first interrupts the subprocess and only kills it after a grace period so that the subprocess can output a stack <PLACE_HOLDER> @$ test log or similar @$ which is incredibly helpful for debugging .,if ( use process wrapper ) { process wrapper util . command line builder command line builder = process wrapper util . command line builder ( process wrapper . get path string ( ) @$ spawn . get arguments ( ) ) . set timeout ( context . get timeout ( ) ) . set kill delay ( duration . of seconds ( local execution options . local sigkill grace seconds ) ) ; if ( local execution options . collect local execution statistics ) { statistics path = tmp dir . get relative ( __str__ ) ; command line builder . set statistics path ( statistics path ) ; } args = command line builder . build ( ) ; } else { subprocess builder . set,subprocess output
if keys do not exist : plus a new <PLACE_HOLDER>,set ( val @$ keys ) ;,keys exist
first time just get <PLACE_HOLDER> .,tables info = load hdfs region infos ( ) ;,time get
scanning the entire table should give us three <PLACE_HOLDER>,meta table accessor . scan meta for table regions ( connection @$ visitor @$ table name ) ; verify ( visitor @$ times ( __num__ ) ) . visit ( ( result ) any object ( ) ) ;,table give
if the millisecond date value contains time <PLACE_HOLDER> @$ mask it out .,super . set time ( date ) ;,value contains
do n't cache these @$ bumps do n't get updated <PLACE_HOLDER>,return new vertical slider thumb icon ( ) ;,bumps get
now chain some statements using flatmap <PLACE_HOLDER>,jdbc . rx get connection ( ) . flat map ( conn -> { single < result set > resa = conn . rx update ( __str__ ) . flat map ( result -> conn . rx update ( __str__ ) ) . flat map ( result -> conn . rx update ( __str__ ) ) . flat map ( result -> conn . rx query ( __str__ ) ) ; return resa . do after terminate ( conn :: close ) ; } ) . subscribe ( result set -> { system . out . println ( __str__ + result set . get rows ( ) ) ; } @$ err -> { system . out . println ( __str__ ) ; err . print stack trace,statements using
file has stopped <PLACE_HOLDER> ...,if ( ! properties file exists ) { f last modified = - __num__ ; f xalan properties = null ; },file stopped
delayed transport has already terminated . terminating the transport terminates the sub<PLACE_HOLDER> @$ which in turn terimates the oob <PLACE_HOLDER> @$ which terminates the <PLACE_HOLDER> .,assert false ( oob1 . is terminated ( ) ) ; verify ( balancer rpc executor pool ) . return object ( balancer rpc executor . get scheduled executor service ( ) ) ; transport info . listener . transport terminated ( ) ; assert true ( oob1 . is terminated ( ) ) ; assert true ( channel . is terminated ( ) ) ; verify ( balancer rpc executor pool @$ times ( __num__ ) ) . return object ( balancer rpc executor . get scheduled executor service ( ) ) ;,which terimates
array precedence equals <PLACE_HOLDER>,if ( array . get precedence ( ) > get precedence ( ) ) { res . enclose ( __str__ @$ __str__ ) ; },precedence equals
the file failed to write @$ try 5 times @$ calling gc and sleep between each iteration sometimes windows takes a <PLACE_HOLDER> to release a lock on a file,while ( ! are byte arrays equal ( data @$ read bytes ( f ) ) && count < __num__ ) { system . gc ( ) ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { throw new runtime exception ( __str__ ) ; } write bytes ( f @$ data ) ; count ++ ; },windows takes
if the user only wants to print the <PLACE_HOLDER> @$ print only the <PLACE_HOLDER>,if ( pod . get selection ( ) ) { print selection ( monitor @$ start date @$ job @$ book @$ lm @$ scale amount @$ scaled height ) ; if ( monitor . is cancelled ( ) ) { return ; } } else if ( pod . get visible ( ) ) { print visible content ( monitor @$ start date @$ job @$ book @$ lm @$ scale amount @$ scaled height ) ; if ( monitor . is cancelled ( ) ) { return ; } } else { int page count = get printable page count ( monitor @$ lm @$ scaled height @$ big integer . zero @$ null ) ; print view ( monitor @$ start date @$ job @$,selection print
for nested result maps @$ partial works the <PLACE_HOLDER> as none,sql session factory . get configuration ( ) . set auto mapping behavior ( auto mapping behavior . partial ) ; try ( sql session sql session = sql session factory . open session ( ) ) { mapper mapper = sql session . get mapper ( mapper . class ) ; user user = mapper . get user with pets_ external ( __num__ ) ; assertions . assert equals ( integer . value of ( __num__ ) @$ user . get id ( ) ) ; assertions . assert equals ( __str__ @$ user . get name ( ) ) ; assertions . assert null ( user . get pets ( ) . get ( __num__ ) . get pet name ( ) @$ __str__ ) ;,partial works
the request has no body @$ or it has a <PLACE_HOLDER> encoding we do not support . in either case @$ we read any data available,log . debug ( __str__ ) ; while ( in . available ( ) > __num__ && ( ( length = in . read ( buffer ) ) != - __num__ ) ) { log . debug ( __str__ @$ length ) ; out . write ( buffer @$ __num__ @$ length ) ; },request has
each entry has the <PLACE_HOLDER> of the row number,b = new array list < > ( arrays . as list ( big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) ) ) ; f = new array list < > ( arrays . as list ( big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) ) ) ; input tableau ( builder ) ; builder . seq ( seq -> { secret tableau . debug info ( seq @$ ps ) ; return null ; } ) ;,entry has
when we get to here the new cache has a use <PLACE_HOLDER> of 1 and when setting a bunch of values on the same node sequence @$ such as when sorting @$ we will keep setting values in that same copy which has a use <PLACE_HOLDER> of 1 .,vec . set element at ( node @$ index ) ; m_last = vec . size ( ) ;,cache has
because the cluster has only 3 <PLACE_HOLDER> @$ and 2 of which are decomm'ed @$ the last block file will remain under replicated .,init exclude hosts ( nodes ) ; refresh nodes ( __num__ ) ;,cluster has
the property ca n't have any <PLACE_HOLDER>,return ;,property have
worker might have <PLACE_HOLDER> that the master does n't yet know about @$ e.g . ufs specific <PLACE_HOLDER> @$ or <PLACE_HOLDER> from a different version of alluxio .,return new property key . builder ( name ) . set is built in ( false ) . build unregistered ( ) ;,worker have
so that everyone sees the <PLACE_HOLDER> .,query contact status ( parent provider . get aim connection ( ) . get screenname ( ) . get formatted ( ) ) ;,everyone sees
create database and tables to simulate impala behavior that impala updates metadata to hms and hms hook sends the metadata to atlas @$ which has to happen before atlas can handle lineage <PLACE_HOLDER>,string db name = __str__ ; create database ( db name ) ; string source table name = __str__ ; create table ( db name @$ source table name @$ __str__ @$ false ) ; string target table name = __str__ ; create table ( db name @$ target table name @$ __str__ @$ false ) ;,atlas handle
we speculate on the first compilation . the global value numbering will merge the two floating integer exact operation <PLACE_HOLDER> .,installed code code = get code ( method ) ; code . execute varargs ( x @$ y ) ; if ( ! code . is valid ( ) ) { code = get code ( method ) ; code . execute varargs ( x @$ y ) ; assert true ( code . is valid ( ) ) ; },numbering merge
every selectable node in the tree is either of type function tree container node or of type function tree function node . only nodes of the first type contain assembler <PLACE_HOLDER> that is displayed .,if ( selected node instanceof function tree block node ) { final function tree block node block node = ( function tree block node ) selected node ; final basic block basic block = block node . get basic block ( ) ; final string builder text = new string builder ( __str__ ) ; for ( final instruction instruction : basic block . get instructions ( ) ) { text . append ( instruction . to string ( ) + __str__ ) ; } m_field . set text ( text . to string ( ) ) ; m_field . set caret position ( __num__ ) ; } else { m_field . set text ( __str__ ) ; },nodes contain
we are careful here to avoid putting the value id into file metadata if we already have a digest . arbitrary filesystems may do weird <PLACE_HOLDER> with the value id ; a digest is more robust .,return new regular file state value ( stat . get size ( ) @$ digest @$ null ) ;,digest do
simply return do n't require <PLACE_HOLDER>,return ;,return require
focused date must have zero <PLACE_HOLDER> @$ mins @$ secs @$ millisecs,focused date = new focused date ( now . get year ( ) @$ now . get month ( ) @$ now . get date ( ) ) ; displayed month = new focused date ( now . get year ( ) @$ now . get month ( ) @$ __num__ ) ;,date have
note to translators : the stylesheet referred to an <PLACE_HOLDER> to the xsl syntax and indicated that it was defined by xsltc @$ but xstlc does not recognized the particular <PLACE_HOLDER> named . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,xstlc recognized
verify the successful flow file has the expected <PLACE_HOLDER> & attributes,final mock flow file mock flow file = test runner . get flow files for relationship ( put kudu . rel_success ) . get ( __num__ ) ; mock flow file . assert attribute equals ( core attributes . filename . key ( ) @$ filename ) ; mock flow file . assert attribute equals ( put kudu . record_count_attr @$ __str__ ) ; mock flow file . assert content equals ( __str__ ) ;,file has
e.<PLACE_HOLDER> <PLACE_HOLDER>reat reat<PLACE_HOLDER> so match <PLACE_HOLDER> @$ <PLACE_HOLDER> and reat @$ reat,if ( is scrambled ( input1 @$ input2 @$ start1 @$ start1 + len @$ end2 - len @$ end2 @$ mem map ) && is scrambled ( input1 @$ input2 @$ start1 + len + __num__ @$ end1 @$ start2 @$ end2 - len - __num__ @$ mem map ) ) { mem map . put ( index @$ true ) ; return true ; },reatg match
since the work used the immediate <PLACE_HOLDER> @$ it is unaffected by the main looper being paused .,assert equals ( __num__ @$ x . get ( ) ) ; observable . just ( __num__ ) . compose ( observe forui ( ) ) . subscribe ( x :: set ) ;,work used
make sure vm 3 can deserialize the <PLACE_HOLDER>,deserialization future = vm3 . invoke async ( ( ) -> { final region r = cache . get region ( region . separator + get test method name ( ) + __str__ ) ; pdx value result = ( pdx value ) r . get ( key_0 ) ; assert equals ( result @$ new pdx value ( __num__ ) ) ; } ) ; try { deserialization future . await ( __num__ @$ time unit . seconds ) ; fail ( __str__ ) ; } catch ( timeout exception e ) { },vm deserialize
existing tables and partitions may have <PLACE_HOLDER> in a different order than the writer is providing @$ so build an index to rearrange <PLACE_HOLDER> in the proper order,list < string > file column names = get column names ( schema ) ; list < type > file column types = get column types ( schema ) . stream ( ) . map ( hive type -> hive type . get type ( type manager ) ) . collect ( to list ( ) ) ; int [ ] file input column indexes = file column names . stream ( ) . map to int ( input column names :: index of ) . to array ( ) ; try { file system file system = hdfs environment . get file system ( session . get user ( ) @$ path @$ configuration ) ; output stream output stream = file system . create ( path,tables have
change needs <PLACE_HOLDER>,final remote animation adapter adapter = new remote animation adapter ( runner @$ __num__ @$ __num__ @$ true ) ;,change needs
properties @$ return an <PLACE_HOLDER> of strings representing its groups .,string [ ] test groups = new string [ ] { __str__ @$ __str__ @$ __str__ } ; do test get groups ( arrays . as list ( test groups ) ) ;,properties return
if the context has the effective <PLACE_HOLDER> @$ we do n't need to schedule an extra task .,if ( effective deadline != null && ! effective deadline . equals ( context . get deadline ( ) ) && deadline cancellation executor != null ) { deadline cancellation future = start deadline timer ( effective deadline ) ; },context has
the flow in the registry may not contain the same <PLACE_HOLDER> of components that we have in our flow . as a result @$ we need to update the flow snapshot to contain compatible bundles .,service facade . discover compatible bundles ( flow snapshot . get flow contents ( ) ) ;,flow contain
this locator will hook <PLACE_HOLDER> up with the first membership to be created,l = internal locator . start locator ( port @$ new file ( __str__ ) @$ null @$ null @$ local host @$ false @$ new properties ( ) @$ null @$ temporary folder . get root ( ) . to path ( ) ) ;,locator hook
but not again after we denied ourselves write <PLACE_HOLDER> with an acl update,verify denied ( incrementq2 @$ user_other @$ group_user ) ;,ourselves write
update should not add new <PLACE_HOLDER> !,assert that ( cursor . get count ( ) ) . is equal to ( users for update . size ( ) ) ;,update add
note to translators : the stylesheet tried to create an attribute with a <PLACE_HOLDER> that was not a valid xml <PLACE_HOLDER> . the substitution text contains the <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text contains
check if contact has additional phone <PLACE_HOLDER> @$ if yes show the call button,meta contact phone util contact phone util = null ; details response listener details listener = null ;,contact has
derive flink internal <PLACE_HOLDER> from explicitly configure task heap <PLACE_HOLDER> size and managed <PLACE_HOLDER> size,final memory size task heap memory size = get task heap memory size ( config ) ; final memory size managed memory size = get managed memory size ( config ) ; final memory size framework heap memory size = get framework heap memory size ( config ) ; final memory size framework off heap memory size = get framework off heap memory size ( config ) ; final memory size task off heap memory size = get task off heap memory size ( config ) ; final memory size shuffle memory size ; final memory size total flink exclude shuffle memory size = framework heap memory size . add ( framework off heap memory size ) . add ( task heap memory size ) . add (,derive flink
thread which will mess around <PLACE_HOLDER> .,final query index idx = index ( idx_name_1 @$ field ( field_name_1 ) ) ; ignite internal future idx fut = multithreaded async ( new callable < void > ( ) { @ override public void call ( ) throws exception { boolean exists = false ; while ( ! stopped . get ( ) ) { ignite node = grid ( thread local random . current ( ) . next int ( __num__ @$ __num__ ) ) ; ignite internal future fut ; if ( exists ) { fut = query processor ( node ) . dynamic index drop ( cache_name @$ cache_name @$ idx_name_1 @$ true ) ; exists = false ; } else { fut = query processor ( node ) . dynamic index create,which mess
first transition triggers the <PLACE_HOLDER>,state . goto state ( connecting ) ; assert equals ( __num__ @$ executor . run due tasks ( ) ) ; assert equals ( __num__ @$ sink . size ( ) ) ; assert equals ( connecting @$ sink . poll ( ) ) ; assert equals ( __num__ @$ executor . num pending tasks ( ) ) ;,transition triggers
we return file so that caller that wants to write knows <PLACE_HOLDER> the next available has location is,if ( desired return type == uri . class ) { return ( t ) file ; },available has
the style block can not contain empty <PLACE_HOLDER> @$ so we assume the input ends when a empty line is found .,string line ; do { line = input . read line ( ) ; } while ( ! text utils . is empty ( line ) ) ;,block contain
sequenced event is a package private class @$ which can not be instantiated by importing . so use reflection to create an <PLACE_HOLDER> .,try { class < ? extends awt event > seq class = ( class < ? extends awt event > ) class . for name ( __str__ ) ; constructor < ? extends awt event > seq const = seq class . get constructor ( awt event . class ) ; seq const . set accessible ( true ) ; return seq const . new instance ( wrap me ) ; } catch ( throwable err ) { throw new runtime exception ( __str__ @$ err ) ; },reflection create
table follows <PLACE_HOLDER>,if ( opcode == opcodes . switch ) { return false ; },table follows
now @$ we have to make sure the finalizer gets run . we will keep allocating more and more memory with the idea that eventually @$ the memory occupied by the big object will get <PLACE_HOLDER> and the finalizer will be run .,list hold alot = new array list ( ) ; for ( int chunk = __num__ ; chunk > __num__ ; chunk = chunk / __num__ ) { if ( finalizer run ) { return ; } try { while ( true ) { hold alot . add ( new byte [ chunk ] ) ; system . err . println ( __str__ + chunk ) ; } } catch ( throwable thrown ) { system . gc ( ) ; } system . run finalization ( ) ; },memory get
if sink and source both are blocking mode @$ source only needs read once to get <PLACE_HOLDER> sink write .,boolean is blocking = sink blocking mode [ i ] && source blocking mode [ i ] ; byte buffer read buf = byte buffer . allocate ( buffer_size ) ; int total count = __num__ ; do { int count = source . read ( read buf ) ; if ( count < __num__ ) { break ; } total count += count ; } while ( total count != buffer_size && ! is blocking ) ; assert equals ( __str__ @$ new string ( read buf . array ( ) @$ iso8859_1 ) ) ; assert equals ( __num__ @$ positioned buffer . position ( ) ) ;,sink write
app : released api 10 <PLACE_HOLDER> : released api 20,verify compute target sdk version ( older_version @$ released @$ true @$ older_version ) ;,app released
if the key is the left operand @$ then reflect the <PLACE_HOLDER> before the index lookup,int op = reflect on operator ( index info . _key ( ) ) ;,then reflect
if its a managed parent <PLACE_HOLDER> @$ it might not have any children,if ( ( q . children == null || q . children . is empty ( ) ) && ! ( q . parent queue instanceof managed parent queue ) ) { return immutable set . of ( q . queue name ) ; } set < string > leaf queue names = new hash set < > ( ) ; for ( temp queue per partition child : q . children ) { leaf queue names . add all ( get leaf queue names ( child ) ) ; } return leaf queue names ;,a managed
it is fully initialized . the previous implementation of this rule did not protect <PLACE_HOLDER> of ha region queue and caused the bug .,return this . owning queue . is queue initialized ( ) ? this . owning queue : null ;,implementation protect
and now fire the message received <PLACE_HOLDER> .,fire message received ( message @$ from ) ;,message received
check memory includes the new <PLACE_HOLDER> .,process tree . update process tree ( ) ; assert . assert equals ( __str__ @$ __num__ @$ process tree . get virtual memory size ( ) ) ; if ( ! smap enabled ) { long cumu rss mem = procfs based process tree . page_size > __num__ ? __num__ * procfs based process tree . page_size : resource calculator process tree . unavailable ; assert . assert equals ( __str__ @$ cumu rss mem @$ process tree . get rss memory size ( ) ) ; } else { assert . assert equals ( __str__ @$ __num__ * kb_to_bytes * __num__ @$ process tree . get rss memory size ( ) ) ; },memory includes
if the jvm is not doing url <PLACE_HOLDER> @$ then the jar files will not be cached either @$ and so they are safe to close,if ( ! get use caches ( ) ) { if ( _jar file != null ) { try { if ( log . is debug enabled ( ) ) log . debug ( __str__ + _jar file . get name ( ) ) ; _jar file . close ( ) ; } catch ( io exception ioe ) { log . ignore ( ioe ) ; } } } _jar file = null ; super . close ( ) ;,jvm doing
expected only metadata update proposed <PLACE_HOLDER> for update schema .,assert custom messages ( __num__ ) ; assert communication messages ( ) ;,metadata update
delete again should not throw any <PLACE_HOLDER>,try { dlm . delete ( ) ; } catch ( io exception ioe ) { fail ( __str__ ) ; },delete throw
this index already exists @$ verify meta data <PLACE_HOLDER> with expectations,mutable boolean paged file open = new mutable boolean ( true ) ; boolean success = false ; try { meta meta = read meta ( null @$ paged file ) ; paged file = map with correct page size ( page cache @$ index file @$ paged file @$ meta . get page size ( ) @$ paged file open @$ open options ) ; success = true ; return paged file ; } catch ( illegal state exception e ) { throw new metadata mismatch exception ( __str__ @$ e ) ; } finally { if ( ! success && paged file open . boolean value ( ) ) { paged file . close ( ) ; } },index exists
strict imports to make sure test bundle only has <PLACE_HOLDER> to these packages,instructions . set property ( __str__ @$ __str__ + __str__ + __str__ ) ;,imports has
starting from 0 so that the steps have nice <PLACE_HOLDER>,return __num__ ;,steps have
the hadoop factory creates various <PLACE_HOLDER>,return __str__ ;,factory creates
in order to find out whether the divide generates the exact <PLACE_HOLDER> @$ we avoid calling the above divide method . 'quotient ' holds the return big decimal object whose scale will be set to 'scl ' .,big decimal quotient ; int scl = check scale non zero ( preferred scale + yscale - xscale + mcp ) ; if ( check scale non zero ( ( long ) mcp + yscale - xscale ) > __num__ ) { int raise = check scale non zero ( ( long ) mcp + yscale - xscale ) ; big integer rb = big multiply power ten ( xs @$ raise ) ; quotient = divide and round ( rb @$ ys @$ scl @$ rounding mode @$ check scale non zero ( preferred scale ) ) ; } else { int new scale = check scale non zero ( ( long ) xscale - mcp ) ; int raise = check scale non zero ( (,divide generates
now create and destroy all of the entries in the new oplog . this should cause us to remove the crf but leave the drf @$ which has creates in reverse order . now we have garbage <PLACE_HOLDER> which have higher i ds than any crate,region . put ( __str__ @$ __str__ ) ; region . put ( __str__ @$ __str__ ) ; region . put ( __str__ @$ __str__ ) ; region . destroy ( __str__ ) ; region . destroy ( __str__ ) ; region . destroy ( __str__ ) ; region . destroy ( __str__ ) ; store . force roll ( ) ;,which have
contract created by create 2 @$ does n't have <PLACE_HOLDER>,assert . assert equals ( __num__ @$ smart contract . get abi ( ) . get entrys count ( ) ) ;,contract have
get <PLACE_HOLDER>s should return the <PLACE_HOLDER>,object [ ] spans = spannable . get spans ( __num__ @$ spannable . length ( ) @$ m class ) ; assert not null ( spans ) ; assert that ( spans @$ array with size ( __num__ ) ) ; assert same ( m watcher @$ spans [ __num__ ] ) ;,spans return
assigned <PLACE_HOLDER> for task will either contain all <PLACE_HOLDER> for the task or be empty @$ so just add all,assigned partitions . add all ( assigned partitions for task ) ;,partitions contain
if the entity has deleted <PLACE_HOLDER> @$ then update that as well,if ( entry . get deleted state ( ) != null ) { entry . get deleted state ( ) [ lazy property numbers [ j ] ] = lazy property types [ j ] . deep copy ( prop value @$ factory ) ; } return field name . equals ( lazy property names [ j ] ) ;,entity deleted
modify the grammar to make checksum comparison detect a <PLACE_HOLDER>,try ( change change = change . of ( base grammar @$ __str__ ) ) { byte [ ] test lexer sum = checksum ( gen test lexer ) ; byte [ ] test parser sum = checksum ( gen test parser ) ; byte [ ] hello sum = checksum ( gen hello ) ; maven . execute mojo ( session @$ project @$ exec ) ; assert false ( arrays . equals ( test lexer sum @$ checksum ( gen test lexer ) ) ) ; assert false ( arrays . equals ( test parser sum @$ checksum ( gen test parser ) ) ) ; assert true ( arrays . equals ( hello sum @$ checksum ( gen hello ) ) ) ; },comparison detect
method name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( operation id ) ) { logger . warn ( operation id + __str__ + camelize ( sanitize name ( __str__ + operation id ) @$ true ) ) ; operation id = __str__ + operation id ; } return camelize ( sanitize name ( operation id ) @$ true ) ;,name use
process instance create <PLACE_HOLDER>,flowable engine entity event event = ( flowable engine entity event ) listener . get events received ( ) . get ( __num__ ) ; assert equals ( flowable engine event type . entity_created @$ event . get type ( ) ) ; assert equals ( process instance . get id ( ) @$ ( ( process instance ) event . get entity ( ) ) . get id ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get process instance id ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get execution id ( ) ) ; assert equals ( process instance . get process definition id ( ) @$ event,instance create
allow all scored <PLACE_HOLDER> @$ unless very common,int tagged word iw = new int tagged word ( word @$ null tag ) ; if ( seen counter . get count ( iw ) > smooth in unknowns threshold ) { return rules with word [ word ] . iterator ( ) ; } else { word taggings = new array list < > ( __num__ ) ; for ( int tagged word itw2 : tags ) { int tagged word itw = new int tagged word ( word @$ itw2 . tag ) ; if ( score ( itw @$ loc @$ word index . get ( word ) @$ null ) > float . negative_infinity ) { word taggings . add ( itw ) ; } } },all scored
destination creation will trigger the <PLACE_HOLDER> since the virtual topic exists,final destination statistics destination statistics = local broker . get destination ( new activemq queue ( test queue name ) ) . get destination statistics ( ) ; final destination statistics remote dest statistics = remote broker . get destination ( new activemq queue ( __str__ ) ) . get destination statistics ( ) ; thread . sleep ( __num__ ) ; assert advisory broker counts ( __num__ @$ __num__ @$ __num__ ) ;,creation trigger
if both old and new table does not satisfies the <PLACE_HOLDER> @$ then do n't dump the event .,log . info ( __str__ ) ; return false ;,table satisfies
all the containers @$ the container monitor and all the container . the container scheduler may use its own <PLACE_HOLDER> .,return new container scheduler ( cntxt @$ dispatcher @$ metrics ) ;,containers use
if the query contains more than one <PLACE_HOLDER>,if ( query properties . get join count ( ) > __num__ ) { profilescbo . add ( extendedcbo profile . join_reordering ) ; },query contains
app is using upgrade key <PLACE_HOLDER> ; make sure all are valid,long [ ] upgrade key sets = old ps . key set data . get upgrade key sets ( ) ; for ( int i = __num__ ; i < upgrade key sets . length ; i ++ ) { if ( ! is id valid key set id ( upgrade key sets [ i ] ) ) { slog . wtf ( tag @$ __str__ + ( old ps . name != null ? old ps . name : __str__ ) + __str__ + upgrade key sets [ i ] + __str__ ) ; return false ; } } return true ;,app using
no direct instances present @$ however another two role <PLACE_HOLDER> subs another single role <PLACE_HOLDER> and has instances,entity type another single role entity = tx . get entity type ( __str__ ) ; assert false ( test tx . rule cache ( ) . absent types ( collections . singleton ( another single role entity ) ) ) ;,present subs
converts interpolated <PLACE_HOLDER> back to polar .,double lat = atan2 ( z @$ sqrt ( x * x + y * y ) ) ; double lng = atan2 ( y @$ x ) ; return new lat lng ( to degrees ( lat ) @$ to degrees ( lng ) ) ;,converts interpolated
new entries should not affect any <PLACE_HOLDER>,assert . assert equals ( record . get record metadata ( __str__ ) @$ __str__ ) ; assert . assert null ( derived1 . get record metadata ( __str__ ) ) ; assert . assert null ( derived2 . get record metadata ( __str__ ) ) ; assert . assert null ( derived3 . get record metadata ( __str__ ) ) ; assert . assert equals ( derived1 . get record metadata ( __str__ ) @$ __str__ ) ; assert . assert null ( record . get record metadata ( __str__ ) ) ; assert . assert null ( derived2 . get record metadata ( __str__ ) ) ; assert . assert null ( derived3 . get record metadata ( __str__ ) ) ; assert . assert equals,entries affect
associate generated component virtual file with mocked generated component <PLACE_HOLDER>,psi class mocked generated component cls = mock ( psi class . class ) ; psi file mocked generated component file = mock ( psi file . class ) ; when ( mocked generated component cls . get containing file ( ) ) . then return ( mocked generated component file ) ; virtual file generated component virtual file = create present in scope virtual file ( ) ; when ( mocked generated component file . get virtual file ( ) ) . then return ( generated component virtual file ) ; virtual file present in scope virtual file = create present in scope virtual file ( ) ;,associate generated
since the api request specified a unique channel <PLACE_HOLDER> @$ the api response should return exactly one channel . if the response does not contain a channel @$ then the specified channel <PLACE_HOLDER> was not found .,list < channel > channel list = channel list response . get items ( ) ; if ( channel list . is empty ( ) ) { system . out . println ( __str__ + channel id ) ; return ; } channel channel = channel list . get ( __num__ ) ;,request specified
delta could not be extracted . we would need to send full <PLACE_HOLDER> . or a create operation has a <PLACE_HOLDER> which has delta . but we send full <PLACE_HOLDER> for create . so serialize it .,if ( this . delta bytes == null || is create ( ) ) { this . _value = serialized value = cache server helper . serialize ( latest value ) ; },which has
call the method from the parent class which initializes the <PLACE_HOLDER> .,super . init debugger ( ) ;,which initializes
only 1 process can have this process <PLACE_HOLDER>,break ;,process have
two delimiter markers @$ which make an empty <PLACE_HOLDER> .,string str = delimiter + __str__ + delimiter + delimiter + __str__ + boundary + __str__ + delimiter ;,which make
check if an image mime <PLACE_HOLDER> .,try { string nat = get native for format ( format ) ; data flavor df = new data flavor ( nat ) ; string primary type = df . get primary type ( ) ; if ( __str__ . equals ( primary type ) ) { mime type = df . get primary type ( ) + __str__ + df . get sub type ( ) ; } } catch ( exception e ) { },image mime
attach the original predicate to the table scan <PLACE_HOLDER> for index optimizations that require the pushed predicate before pcr & later optimizations are applied,if ( hive conf . get bool var ( hive conf @$ hive conf . conf vars . hiveoptindexfilter ) ) { table scan desc . set filter expr ( original predicate ) ; },predicate scan
chris added this next test in 2017 ; a bit weird @$ but kbp setup does n't have <PLACE_HOLDER> in sentence boundary patterns @$ just in to discard,if ( abstract tokenizer . newline_token . equals ( word ) ) { last token was newline = true ; } if ( debug ) { log . info ( __str__ + word + __str__ + debug text ) ; },setup have
when slow reach the <PLACE_HOLDER> of the loop with k step @$ fast already goes for 2 k steps @$ so it is k steps ahead of the <PLACE_HOLDER> of the loop .,return null ;,slow reach
collator.get keyword values return the same <PLACE_HOLDER> for both commonly used true and false .,string [ ] all = collator . get keyword values for locale ( __str__ @$ loc @$ false ) ; boolean match all = false ; if ( pref . length == all . length ) { match all = true ; for ( int j = __num__ ; j < pref . length ; j ++ ) { boolean found match = false ; for ( int k = __num__ ; k < all . length ; k ++ ) { if ( pref [ j ] . equals ( all [ k ] ) ) { found match = true ; break ; } } if ( ! found match ) { match all = false ; break ; } } } if ( ! match,values return
check both users can use their <PLACE_HOLDER>,xml page w1 = ( xml page ) wc1 . go to ( __str__ @$ __str__ ) ; assert that ( w1 @$ hasx path ( __str__ @$ is ( __str__ ) ) ) ; xml page w2 = ( xml page ) wc2 . go to ( __str__ @$ __str__ ) ; assert that ( w2 @$ hasx path ( __str__ @$ is ( __str__ ) ) ) ; u1 . set full name ( __str__ ) ; u1 . save ( ) ;,users use
stub notify shared cache <PLACE_HOLDER> to return true,do return ( true ) . when ( spied ) . notify shared cache manager ( isa ( string . class ) @$ isa ( string . class ) ) ; assert true ( spied . call ( ) ) ;,stub notify
ensure that snapshot directory will always use the local file <PLACE_HOLDER> instead of the default file <PLACE_HOLDER>,configuration configuration = new configuration ( ) ; configuration . set string ( core options . default_filesystem_scheme @$ __str__ ) ; file system . initialize ( configuration ) ; final file folder root = temporary folder . get root ( ) ; try { file folderb = new file ( folder root @$ string . value of ( uuid . randomuuid ( ) ) ) ; snapshot directory snapshot directoryb = snapshot directory . temporary ( folderb ) ; assert . assert equals ( snapshot directoryb . get file system ( ) @$ file system . get local file system ( ) ) ; } finally { file system . initialize ( new configuration ( ) ) ; },directory use
handle @$ mark as processed @$ unless the <PLACE_HOLDER> handler threw an <PLACE_HOLDER>,exception handler . handle event exception ( ex @$ next sequence @$ event ) ; processed sequence = true ;,handler threw
warning the <PLACE_HOLDER> in the finally clause will trump any <PLACE_HOLDER> before,if ( failed ) return failure ;,return trump
event with no message has severity level <PLACE_HOLDER>,assert false ( filter . accept ( ev ) @$ __str__ ) ; final severity level error level = severity level . error ; final localized message error message = new localized message ( __num__ @$ __num__ @$ __str__ @$ __str__ @$ null @$ error level @$ null @$ get class ( ) @$ null ) ; final audit event ev2 = new audit event ( this @$ __str__ @$ error message ) ; assert true ( filter . accept ( ev2 ) @$ __str__ + error level ) ; final severity level info level = severity level . info ; final localized message info message = new localized message ( __num__ @$ __num__ @$ __str__ @$ __str__ @$ null @$ info level @$ null @$ get class,event has
if the new candidate has more <PLACE_HOLDER> than previously tracked mc vs we insert it and bubble others down,int j ; for ( j = num tracked - __num__ ; j > __num__ ; j -- ) { if ( duplicates <= most common value candidates [ j - __num__ ] . count ) { break ; } most common value candidates [ j ] . count = most common value candidates [ j - __num__ ] . count ; most common value candidates [ j ] . first = most common value candidates [ j - __num__ ] . first ; } most common value candidates [ j ] . count = duplicates ; most common value candidates [ j ] . first = i - duplicates ;,candidate has
we allow one trailing <PLACE_HOLDER>,if ( flavor != config syntax . json && t == tokens . close_square ) { put back ( t ) ; } else { throw parse error ( __str__ + t + __str__ + t + __str__ ) ; },one trailing
bob can read <PLACE_HOLDER>,wc = j . create web client ( ) . login ( __str__ ) ; wc . go to ( __str__ ) ;,bob read
check that the jar does not have an <PLACE_HOLDER> for the removed class,try ( jar file abi jar = new jar file ( abi jar path . to file ( ) ) ) { assert that ( abi jar . stream ( ) . map ( jar entry :: get name ) . collect ( collectors . to set ( ) ) @$ matchers . contains in any order ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; manifest manifest = abi jar . get manifest ( ) ; assert null ( manifest . get attributes ( __str__ ) ) ; },jar have
sensless tagging : josm does create a <PLACE_HOLDER> here . we follow the highway tag :,way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . clear tags ( ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . can skip ( ) ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . clear tags ( ) ; way . set tag ( __str__,josm create
inherit connect <PLACE_HOLDER> from server pool configuration .,for ( ldap server server : servers ) { if ( server . get connect timeout ( ) == __num__ && timeout != __num__ ) { server . set connect timeout ( timeout ) ; } } return this ;,inherit connect
one in 4 tests generate a sync and volt bulk loader internal state <PLACE_HOLDER>,if ( ! abort1 && rnd . next int ( ) % __num__ == __num__ ) { bulk loader1 . drain ( ) ; assert ( bulk loader1 . get outstanding row count ( ) == __num__ ) ; assert ( bulk loader1 . get completed row count ( ) == row cnt1 ) ; },bulk loader
url contains port <PLACE_HOLDER>,if ( - __num__ != colon index ) { if ( baseurl . starts with ( __str__ ) && baseurl . ends with ( __str__ ) ) { baseurl = baseurl . substring ( __num__ @$ colon index ) ; } else if ( baseurl . starts with ( __str__ ) && baseurl . ends with ( __str__ ) ) { baseurl = baseurl . substring ( __num__ @$ colon index ) ; } },url contains
then : '' collected must remove <PLACE_HOLDER> '',assert that ( tap . block ( ) ) . contains exactly ( __num__ @$ __num__ @$ __num__ ) ;,collected remove
checking if the current continuous frame contains a correct <PLACE_HOLDER> with the other frames combined,if ( curop == opcode . continuous && current continuous frame != null ) { add to buffer list ( frame . get payload data ( ) ) ; },frame contains
required features expect activity <PLACE_HOLDER>,wait for latch ( latch ) ; verify ( m mock account manager response ) . on error ( account manager . error_code_invalid_response @$ account manager service test fixtures . error_message ) ; verify ( m mock account manager response @$ never ( ) ) . on result ( any ( bundle . class ) ) ;,features expect
identify the x position at which to truncate the final <PLACE_HOLDER> : note : the left position of the <PLACE_HOLDER> is needed for the case of rtl text .,final float ellipsis target = layout width - bounds . width ( ) + new layout . get line left ( ellipsized line number ) ;,which truncate
create the parts add any <PLACE_HOLDER>,for ( j meter property j meter property : get arguments ( ) ) { http argument arg = ( http argument ) j meter property . get object value ( ) ; string parameter name = arg . get name ( ) ; if ( arg . is skippable ( parameter name ) ) { continue ; } string body string body = new string body ( arg . get value ( ) @$ content type . create ( arg . get content type ( ) @$ charset ) ) ; form body part form part = form body part builder . create ( parameter name @$ string body ) . build ( ) ; multipart entity builder . add part ( form part ) ; },parts add
adjust <PLACE_HOLDER> index to pickup first range which contains <PLACE_HOLDER>,try { record rec = range table . get record at or before ( start ) ; if ( rec != null && rec . get long value ( range_to_col ) >= start ) { start = rec . get key ( ) ; } map rec iter = map table . index iterator ( map_range_key_col @$ new long field ( start ) @$ new long field ( end ) @$ true ) ; } catch ( io exception e ) { err handler . db error ( e ) ; },which contains
battery stats impl.update cpu <PLACE_HOLDER> locked,m battery stats impl . set power profile ( null ) ; m battery stats impl . update time bases locked ( unplugged @$ screen state @$ up time @$ real time ) ; m battery stats impl . set power profile ( m power profile ) ;,stats impl.update
we only look for a partial match which does n't match on subid because we can crossfade views that match <PLACE_HOLDER> except for subid .,for ( pair < view @$ string > p : transition view pairs ) { if ( tn . partial equals ( transition name . parse ( view compat . get transition name ( p . first ) ) ) ) { it . remove ( ) ; break ; } },which match
succeed if user has <PLACE_HOLDER> for all regions and all keys for the given operation,if ( allowed permissions . contains ( new resource permission ( resource permission . resource . data @$ permission . get operation ( ) ) ) ) { return true ; },user has
note : for dynamic labels @$ we do n't render the <PLACE_HOLDER> based upon its offcut index @$ but rather we just show the entire <PLACE_HOLDER> . alternatively @$ if there is a label there @$ we show that . the label will show the <PLACE_HOLDER> at its offcut location .,address operand addr = addr ( __str__ ) ; assert operand text ( operand addr @$ __str__ ) ;,label show
check that the alluxio <PLACE_HOLDER> we 're creating does n't shadow a <PLACE_HOLDER> in the parent ufs,mount table . resolution resolution = m mount table . resolve ( alluxio path ) ; try ( closeable resource < under file system > ufs resource = resolution . acquire ufs resource ( ) ) { string ufs resolved path = resolution . get uri ( ) . get path ( ) ; if ( ufs resource . get ( ) . exists ( ufs resolved path ) ) { throw new io exception ( exception message . mount_path_shadows_parent_ufs . get message ( alluxio path @$ ufs resolved path ) ) ; } },path shadow
create a walker which walks the <PLACE_HOLDER> in a dfs manner while maintaining the operator stack .,map < rule @$ node processor > op rules = new linked hash map < rule @$ node processor > ( ) ; op rules . put ( new rule reg exp ( __str__ @$ reduce sink operator . get operator name ( ) + __str__ ) @$ new set reducer parallelism ( ) ) ; op rules . put ( new rule reg exp ( __str__ @$ join operator . get operator name ( ) + __str__ ) @$ new convert join map join ( ) ) ; if ( proc ctx . conf . get bool var ( conf vars . hivemapaggrhashminreductionstatsadjust ) ) { op rules . put ( new rule reg exp ( __str__ @$ group by operator . get operator name ( ),which walks
if the score of this <PLACE_HOLDER> is higher or equal to that of this factory and some other factory is responsible for it @$ then this factory should not track the <PLACE_HOLDER> because it has no hope of satisfying it .,return ! n . requested && ( n . score < m score || n . factory serial number == m serial number ) && n . request . network capabilities . satisfied by network capabilities ( m capability filter ) && accept request ( n . request @$ n . score ) ;,factory track
using just the package name is n't great @$ since it disallows having multiple engines in the same package @$ but that 's <PLACE_HOLDER> the existing api does .,engine . name = service . package name ; char sequence label = service . load label ( pm ) ; engine . label = text utils . is empty ( label ) ? engine . name : label . to string ( ) ; engine . icon = service . get icon resource ( ) ; engine . priority = resolve . priority ; engine . system = is system engine ( service ) ; return engine ;,api does
verify that <PLACE_HOLDER> of the names in the idp matches <PLACE_HOLDER> of the names in the c rl issuer of the cert 's dp,boolean match = false ; for ( iterator < general name > t = point crl issuers . iterator ( ) ; ! match && t . has next ( ) ; ) { general name interface crl issuer name = t . next ( ) . get name ( ) ; for ( iterator < general name > i = idp names . iterator ( ) ; ! match && i . has next ( ) ; ) { general name interface idp name = i . next ( ) . get name ( ) ; match = crl issuer name . equals ( idp name ) ; } } if ( ! match ) { return false ; },one matches
now the tricky one . 'before a ' leads to 'call activity a ' @$ which calls <PLACE_HOLDER> 02 which terminates,process instance = runtime service . start process instance by key ( __str__ ) ; tasks = assert task names ( process instance @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; task task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ; assert historic process,which calls
forbid deprecated <PLACE_HOLDER> .,if ( python opts . incompatible use python toolchains ) { if ( opts . python2 path != null ) { reporter . handle ( event . error ( __str__ + __str__ ) ) ; } if ( opts . python3 path != null ) { reporter . handle ( event . error ( __str__ + __str__ ) ) ; } if ( opts . python top != null ) { reporter . handle ( event . error ( __str__ + __str__ + __str__ + __str__ + __str__ ) ) ; } },forbid deprecated
since the expression must have a single corresponding node <PLACE_HOLDER> is our default assumption,if ( expr nodes . size ( ) != __num__ ) { return nullness . nullable ; },expression have
nodes to start holds the <PLACE_HOLDER> of nodes to be started immediately . we do n't want to start the animations in the loop directly because we first need to set up dependencies on all of the nodes . for example @$ we do n't want to start an animation when some other animation also wants to start when the first animation begins .,final array list < node > nodes to start = new array list < node > ( ) ; for ( int i = __num__ ; i < num sorted nodes ; ++ i ) { node node = m sorted nodes . get ( i ) ; if ( m set listener == null ) { m set listener = new animator set listener ( this ) ; } if ( node . dependencies == null || node . dependencies . size ( ) == __num__ ) { nodes to start . add ( node ) ; } else { int num dependencies = node . dependencies . size ( ) ; for ( int j = __num__ ; j < num dependencies ; ++ j ),nodes holds
signal for factory to start ; sets start <PLACE_HOLDER>,start flag . count down ( ) ;,sets start
make <PLACE_HOLDER> keys change multimedia <PLACE_HOLDER> even if music is not playing now,set volume control stream ( audio manager . stream_music ) ; try { play services available = google api availability . get instance ( ) . is google play services available ( this ) == connection result . success ; } catch ( exception ignored ) { } if ( play services available ) init cast ( ) ;,keys change
second call should throw <PLACE_HOLDER>,assert null ( shard expression . value ( ) ) ;,call throw
ok @$ lets compare this <PLACE_HOLDER> lexicographically,subversion comparision result = v comps1 [ i ] . compare to ( v comps2 [ i ] ) ;,lets compare
type parameter does n't have declared upper <PLACE_HOLDER>,if ( ! node . has type bound ( ) ) { node . set type definition ( java type definition . for class ( upper_bound @$ object . class ) ) ; } else { super . visit ( node @$ data ) ; rollup type unary ( node ) ; },parameter declared
gcm mode needs additional <PLACE_HOLDER>,if ( cipher mode == gcm_mode ) { if ( tag len == - __num__ ) { tag len = galois counter mode . default_tag_len ; } if ( decrypting ) { min bytes = tag len ; } else { require reinit = arrays . equals ( iv bytes @$ last enc iv ) && message digest . is equal ( key bytes @$ last enc key ) ; if ( require reinit ) { throw new invalid algorithm parameter exception ( __str__ ) ; } last enc iv = iv bytes ; last enc key = key bytes ; } ( ( galois counter mode ) cipher ) . init ( decrypting @$ algorithm @$ key bytes @$ iv bytes @$ tag len ) ; },mode needs
some np cs drop <PLACE_HOLDER> onto multiple tiles,final list < item stack > all items = new array list < > ( ) ; for ( int i = __num__ ; i < size ; ++ i ) { for ( int j = __num__ ; j < size ; ++ j ) { final int packed = ( x + i ) << __num__ | ( y + j ) ; final collection < item stack > items = item spawns . get ( packed ) ; all items . add all ( items ) ; } } if ( all items . is empty ( ) ) { return ; } kill points . add ( location ) ; event bus . post ( new npc loot received ( npc @$ all items,np cs
it 's possible the default tag wo n't exist if the user just changed the app 's <PLACE_HOLDER> @$ in which case default to the first tag in the table,if ( ! reader tag table . tag exists ( tag ) ) { tag = reader tag table . get first tag ( ) ; } set current tag ( tag ) ; if ( build config . information_architecture_available && m is top level ) { if ( tag . is followed sites ( ) ) { m view model . set default subfilter ( ) ; } },user changed
window do <PLACE_HOLDER> operator does not use a do <PLACE_HOLDER>,this . requires stable input = do fn != null && do fn signatures . get signature ( do fn . get class ( ) ) . process element ( ) . requires stable input ( ) ;,a do
finishing the tasks should also set the end <PLACE_HOLDER>,tasks = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . list ( ) ; assert equals ( __num__ @$ tasks . size ( ) ) ; for ( org . flowable . task . api . task task : tasks ) { task service . complete ( task . get id ( ) ) ; } wait for history job executor to process all jobs ( __num__ @$ __num__ ) ; historic activity instances = history service . create historic activity instance query ( ) . activity id ( __str__ ) . list ( ) ; assert equals ( __num__ @$ historic activity instances . size ( ) ) ; for ( historic activity,tasks set
the pending intent to launch our activity if the user selects this <PLACE_HOLDER>,pending intent content intent = pending intent . get activity ( this @$ __num__ @$ new intent ( this @$ main activity . class ) @$ __num__ ) ;,user selects
use two different <PLACE_HOLDER>s because the same <PLACE_HOLDER> would share the underlying spawn action <PLACE_HOLDER> .,spawn action template action template = builder ( input tree artifact @$ output tree artifact ) . set executable ( executable ) . set command line template ( create simple command line template ( input tree artifact @$ output tree artifact ) ) . set output path mapper ( identity_mapper ) . set mnemonics ( __str__ @$ __str__ ) . build ( actions test util . null_action_owner ) ; spawn action template action template2 = builder ( input tree artifact @$ output tree artifact ) . set executable ( executable ) . set command line template ( create simple command line template ( input tree artifact @$ output tree artifact ) ) . set output path mapper ( identity_mapper ) . set mnemonics ( __str__ @$ __str__ ),builder share
notification payload <PLACE_HOLDER>,path scheme path = scheme generator . write scheme ( ) ; document builder factory db factory = document builder factory . new instance ( ) ; document builder d builder = db factory . new document builder ( ) ; document scheme = d builder . parse ( project filesystem . new file input stream ( scheme path ) ) ; x path factory xpath factory = x path factory . new instance ( ) ; x path remote runnable launch actionx path = xpath factory . newx path ( ) ; x path expression remote runnable launch action expr = remote runnable launch actionx path . compile ( __str__ ) ; node list remote runnables = ( node list ) remote runnable launch action expr .,notification payload
now test that compare to does what we expect . the exact ordering here does n't matter <PLACE_HOLDER> .,collections . shuffle ( paths ) ; collections . sort ( paths ) ; list < path fragment > expected order = to paths ( immutable list . of ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; assert that ( paths ) . is equal to ( expected order ) ;,test matter
user has specified initial context <PLACE_HOLDER> ; try to get it,if ( my props != null && my props . get ( context . initial_context_factory ) != null ) { get default init ctx ( ) ; },user specified
shift left <PLACE_HOLDER> to restore set flags and mask off anything other than the set flags,return ( ( ~ comb op flags ) > > __num__ ) & flag_mask_is & comb op flags ;,shift left
indicate the the client supports <PLACE_HOLDER>,join presence . add extension ( new muc initial presence ( password @$ max chars @$ max stanzas @$ seconds @$ since ) ) ;,client supports
the file we should write new <PLACE_HOLDER> to .,path output file ; if ( old file != null && old file . exists ( ) && ! is renaming ) { output file = old file ; } else { output file = new file ; },file write
0 indicates an unrepresentable <PLACE_HOLDER>,return __num__ ;,0 indicates
if memory does n't contain the target <PLACE_HOLDER>,if ( ! reference . is memory reference ( ) ) { continue ; } if ( ! memory . contains ( target ) ) { continue ; } if ( ignore new pointers . contains ( target ) ) { continue ; },memory contain
if syntax error occurs then message is printed by error listener and parser throws this runtime <PLACE_HOLDER> to stop parsing . just stop processing current javadoc comment .,if ( parse error message == null ) { parse error message = error listener . get error message ( ) ; },parser throws
flow files those are already committed will not get <PLACE_HOLDER> .,session . rollback ( ) ;,files get
we need to filter the index result if the property is not <PLACE_HOLDER> on some label since the nodes in the index might have both an <PLACE_HOLDER> and a dis<PLACE_HOLDER> label for the property,if ( ! access mode . allows traverse all nodes with label ( label ) || access mode . disallows read property for some label ( prop ) || ! access mode . allows read node property ( ( ) -> labels . from ( label ) @$ prop ) ) { return new node label security filter ( property ids @$ cursor @$ cursors . allocate node cursor ( ) @$ this @$ access mode ) ; },nodes have
make the test file <PLACE_HOLDER> in every test,tmp file = file . create temp file ( __str__ @$ __str__ ) ; tmp file . delete on exit ( ) ; this . write file same ( ) ;,test file
fake phone imei @$ mac <PLACE_HOLDER> @$ bluetooth <PLACE_HOLDER>,virtual core . set phone info delegate ( new my phone info delegate ( ) ) ;,imei bluetooth
keystore contains a hostname which does n't match <PLACE_HOLDER>,client ssl context factory . set key store path ( __str__ ) ; client ssl context factory . set key store password ( __str__ ) ; queued thread pool client threads = new queued thread pool ( ) ; client threads . set name ( __str__ ) ; client = new http client ( client ssl context factory ) ; client . set executor ( client threads ) ; client . start ( ) ;,which match
inspect the source ce 32 s. just copy <PLACE_HOLDER> if none are modified . otherwise copy to modified c es @$ with modifications .,boolean is modified = false ; for ( int i = __num__ ; i < length ; ++ i ) { ce32 = srcce32s [ src index + i ] ; long ce ; if ( collation . is specialce32 ( ce32 ) || ( ce = modifier . modifyce32 ( ce32 ) ) == collation . no_ce ) { if ( is modified ) { modifiedc es [ i ] = collation . ce fromce32 ( ce32 ) ; } } else { if ( ! is modified ) { for ( int j = __num__ ; j < i ; ++ j ) { modifiedc es [ j ] = collation . ce fromce32 ( srcce32s [ src index + j ] ) ; } is,s. copy
zap : added the <PLACE_HOLDER> .,int current idx = __num__ ; if ( is exist status code ) { ps insert . set int ( current idx @$ status code ) ; ++ current idx ; },zap added
no handler in contextual cache @$ make a new <PLACE_HOLDER> .,final snmp table handler handler = cache . get table handler ( ) ; if ( m != null && handler != null ) m . put ( __str__ @$ handler ) ; return handler ;,handler make
below calls should n't propagate any <PLACE_HOLDER>,sut . prepare ( ) ;,calls propagate
oss capabilities request a chrome <PLACE_HOLDER>,map < string @$ object > payload = immutable map . of ( __str__ @$ immutable map . of ( __str__ @$ __str__ ) @$ __str__ @$ immutable map . of ( __str__ @$ immutable map . of ( __str__ @$ __str__ ) @$ __str__ @$ immutable list . of ( immutable map . of ( __str__ @$ __str__ ) @$ immutable map . of ( __str__ @$ __str__ ) ) ) ) ;,capabilities request
two 'union with aliases ' fields under different records but with the same field <PLACE_HOLDER> . the generated record representation for these two unions should include the parent record 's <PLACE_HOLDER> to avoid any <PLACE_HOLDER> conflicts .,return new object [ ] [ ] { { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ translate default @$ __str__ @$ empty foo schema @$ empty foo value @$ null },"union" include
the address should have <PLACE_HOLDER>,deque < property > result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; model node handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; string file name = null ;,address have
if <PLACE_HOLDER> is a domain and the other is n't @$ then they ca n't match . the <PLACE_HOLDER> that 's a domain does n't include the <PLACE_HOLDER> that 's not a domain .,if ( ( this domain != other domain ) && ( constraint type == name_match ) ) { if ( this domain ) { constraint type = name_widens ; } else { constraint type = name_narrows ; } },one include
inline the wsdl as extensibility element write <PLACE_HOLDER> : metadata wrapper,if ( wsdl address != null ) { writer . write start element ( member submission addressing constants . mex_metadata . get prefix ( ) @$ member submission addressing constants . mex_metadata . get local part ( ) @$ member submission addressing constants . mex_metadata . get namespaceuri ( ) ) ; writer . write start element ( member submission addressing constants . mex_metadata_section . get prefix ( ) @$ member submission addressing constants . mex_metadata_section . get local part ( ) @$ member submission addressing constants . mex_metadata_section . get namespaceuri ( ) ) ; writer . write attribute ( member submission addressing constants . mex_metadata_dialect_attribute @$ member submission addressing constants . mex_metadata_dialect_value ) ; write wsdl ( writer @$ service @$ wsdl address ) ; writer,element write
the problem with xerces is that some errors will cause the <PLACE_HOLDER> not to advance the reader and it will keep reporting the same error over and over @$ which will cause the <PLACE_HOLDER> to enter an infinite loop unless we throw the exception .,if ( message != null && is fatal ( message ) ) { throw ex ; },which cause
initialize client cache and region . get the <PLACE_HOLDER> of the primary connected server .,membervm server_to_fail = determine primary server ( client ) ;,cache get
make sure the blob server can not create any <PLACE_HOLDER> in its storage dir,temp file dir = server . create temporary filename ( ) . get parent file ( ) . get parent file ( ) ; assert true ( temp file dir . set executable ( true @$ false ) ) ; assert true ( temp file dir . set readable ( true @$ false ) ) ; assert true ( temp file dir . set writable ( false @$ false ) ) ; byte [ ] data = new byte [ __num__ ] ; rnd . next bytes ( data ) ;,server create
each enhanced dep needs a unique build <PLACE_HOLDER> @$ so we parameterize the build <PLACE_HOLDER> by the java package .,string java package = entry . get key ( ) ; flavor flavor = internal flavor . of ( __str__ + java package . replace ( __str__ @$ __str__ ) ) ; build target build target with flavors = original build target . with appended flavors ( flavor ) ; build rule params build config params = new build rule params ( suppliers . of instance ( immutable sorted set . of ( ) ) @$ suppliers . of instance ( immutable sorted set . of ( ) ) @$ immutable sorted set . of ( ) ) ;,the build
these ms counts all presume <PLACE_HOLDER>,double [ ] [ ] exp = new double [ ] [ ] { d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d (,ms counts
clear 'links extracted ' <PLACE_HOLDER> .,this . link extractor finished = false ; extra info = null ; out links = null ; this . revisit profile = null ;,clear "links"
the component has timeout <PLACE_HOLDER> @$ it needs a 'real ' timer service,if ( component description . is timer service required ( ) ) { final string deployment name ; if ( module description . get distinct name ( ) == null || module description . get distinct name ( ) . length ( ) == __num__ ) { deployment name = module description . get application name ( ) + __str__ + module description . get module name ( ) ; } else { deployment name = module description . get application name ( ) + __str__ + module description . get module name ( ) + __str__ + module description . get distinct name ( ) ; } root_logger . debugf ( __str__ @$ component description . get component name ( ) ) ; component description . get,component has
now we must read and convert this file to the target format with the target size <PLACE_HOLDER> x <PLACE_HOLDER>,try { file new png file = new file ( png file . get absolute path ( ) + __str__ ) ; png file . rename to ( new png file ) ; final image img = image parser . parse ( png file . get absolute path ( ) @$ file utils . read ( new png file ) ) ; if ( img == null ) { return false ; } final image scaled = img . get scaled instance ( width @$ height @$ image . scale_area_averaging ) ; final media tracker media tracker = new media tracker ( new container ( ) ) ; media tracker . add image ( scaled @$ __num__ ) ; try { media tracker . wait forid ( __num__,1024 size
modal bottom sheets have auto peek <PLACE_HOLDER> by default .,assert that ( behavior . get peek height ( ) @$ is ( bottom sheet behavior . peek_height_auto ) ) ;,sheets have
we do n't have <PLACE_HOLDER> to read boolean creds s ince the creds have no boolean creds we should get an empty set,try { set priv cred set1 = s . get private credentials ( boolean . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { e . print stack trace ( ) ; throw new runtime exception ( __str__ ) ; } system . out . println ( __str__ ) ;,creds have
second @$ check if this work has multiple reduce sinks . if so @$ do <PLACE_HOLDER> .,split base work ( spark work @$ work @$ child works ) ;,second do
for each aggregation <PLACE_HOLDER> in the reference node @$ create a corresponding aggregation <PLACE_HOLDER> that points to the null row . map the symbols from the aggregations in reference aggregation to the symbols in these new aggregations .,immutable map . builder < symbol @$ symbol > aggregations symbol mapping builder = immutable map . builder ( ) ; immutable map . builder < symbol @$ aggregation node . aggregation > aggregations over null builder = immutable map . builder ( ) ; for ( map . entry < symbol @$ aggregation node . aggregation > entry : reference aggregation . get aggregations ( ) . entry set ( ) ) { symbol aggregation symbol = entry . get key ( ) ; aggregation node . aggregation aggregation = entry . get value ( ) ; if ( ! is using symbols ( aggregation @$ sources symbol mapping . key set ( ) ) ) { return optional . empty ( ) ; } aggregation over,function node
compare the results fetched last <PLACE_HOLDER>,while ( res . next ( ) ) { assert equals ( results . get ( row num ++ ) @$ res . get string ( col name ) ) ; assert equals ( row num @$ res . get row ( ) ) ; assert false ( res . is before first ( ) ) ; if ( one row only ) { break ; } },results fetched
xlsx needs full <PLACE_HOLDER>,if ( data . wb instanceof xssf workbook ) { formula evaluator evaluator = data . wb . get creation helper ( ) . create formula evaluator ( ) ; for ( int sheet num = __num__ ; sheet num < data . wb . get number of sheets ( ) ; sheet num ++ ) { sheet sheet = data . wb . get sheet at ( sheet num ) ; for ( row r : sheet ) { for ( cell c : r ) { if ( c . get cell type ( ) == cell . cell_type_formula ) { evaluator . evaluate formula cell ( c ) ; } } } } } else if ( data . wb instanceof hssf workbook ) {,xlsx needs
this streamer had internal <PLACE_HOLDER> before getting updated block,if ( ! streamer . is healthy ( ) && coordinator . get new blocks ( ) . peek ( streamer . get index ( ) ) != null ) { failed . add ( streamer ) ; },streamer had
field field file name <PLACE_HOLDER>,wl field filename = new label ( w file name @$ swt . right ) ; wl field filename . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl field filename ) ; fdl field filename = new form data ( ) ; fdl field filename . left = new form attachment ( __num__ @$ __num__ ) ; fdl field filename . top = new form attachment ( w file name in field @$ margin ) ; fdl field filename . right = new form attachment ( middle @$ - margin ) ; wl field filename . set layout data ( fdl field filename ) ; w field filename = new c combo ( w file,field file
test increase container api and make sure requests can reach <PLACE_HOLDER>,test increase container resource ( container ) ; test restart container ( container . get id ( ) ) ; test get container status ( container @$ i @$ container state . running @$ __str__ @$ exit code ) ; wait for container transition count ( container @$ org . apache . hadoop . yarn . server . nodemanager . containermanager . container . container state . running @$ __num__ ) ; if ( i % __num__ == __num__ ) { test re initialize container ( container . get id ( ) @$ clc @$ false ) ; test get container status ( container @$ i @$ container state . running @$ __str__ @$ exit code ) ; wait for container transition count ( container @$ org .,requests reach
we do our own adjustment as calendar can not handle a <PLACE_HOLDER> .,time -= value ; value to use = __num__ ;,calendar handle
check that one of them has the correct match <PLACE_HOLDER>,boolean has match = false ; for ( match action match : patterns . get post patterns ( ) . get ( __num__ ) . get match actions ( ) ) { if ( ! ( match instanceof function start analyzer . context action ) ) { continue ; } has match = true ; assert equals ( __str__ @$ ( ( function start analyzer . context action ) match ) . get name ( ) ) ; assert equals ( new big integer ( __str__ ) @$ ( ( function start analyzer . context action ) match ) . get value ( ) ) ; } assert true ( has match ) ;,one has
build period offset <PLACE_HOLDER>,offset criteria = new offset criteria . offset criteria builder ( ) . with offset as period ( __str__ ) ; assert . assert false ( offset criteria . is smallest ( ) ) ; assert . assert false ( offset criteria . is largest ( ) ) ; assert . assert true ( offset criteria . is period ( ) ) ; assert . assert false ( offset criteria . is custom ( ) ) ; assert . assert equals ( offset criteria . get offset string ( ) @$ __str__ ) ;,period offset
db 2 ca n't handle <PLACE_HOLDER> in sql statements ...,if ( ! supports new lines insql ( ) ) { for ( int i = sbsql . length ( ) - __num__ ; i >= __num__ ; i -- ) { if ( sbsql . char at ( i ) == __str__ || sbsql . char at ( i ) == __str__ ) { sbsql . set char at ( i @$ __str__ ) ; } } },db handle
each zoom level multiplies viewable <PLACE_HOLDER> by 2,for ( int i = default_min_zoom ; i < default_max_zoom ; i ++ ) { max intensity array [ i ] = get max value ( m data @$ m bounds @$ radius @$ ( int ) ( screen_size * math . pow ( __num__ @$ i - __num__ ) ) ) ; if ( i == default_min_zoom ) { for ( int j = __num__ ; j < i ; j ++ ) max intensity array [ j ] = max intensity array [ i ] ; } },level multiplies
if the entity does n't have an <PLACE_HOLDER> @$ it will be created instead of just being updated,rest comment mock mvc . perform ( put ( __str__ ) . content type ( test util . application_json_utf8 ) . content ( test util . convert object to json bytes ( comment ) ) ) . and expect ( status ( ) . is created ( ) ) ;,entity have
ensure that all headed tests use swing <PLACE_HOLDER> when displaying errors . setting this to false would force errors to only be written to the console .,set errorgui enabled ( true ) ;,tests use
weak values allows transform executor services that are no longer in use to be reclaimed . executing transform executor services have a strong <PLACE_HOLDER> to their transform executor service which stops the transform executor services from being prematurely garbage collected,serial executor services = cache builder . new builder ( ) . weak values ( ) . removal listener ( shutdown executor service listener ( ) ) . build ( serial transform executor service cache loader ( ) ) ; this . visible updates = new queue message receiver ( ) ; parallel executor service = transform executor services . parallel ( executor service ) ; executor factory = new direct transform executor . factory ( context @$ registry @$ transform enforcements ) ;,services have
load dexposed <PLACE_HOLDER> for hook .,return load dexposed lib ( context ) ;,load dexposed
the classes bellow have a static final secure random field . note that if the classes are not found as reachable by the analysis registering them form class initialization rerun does n't have any <PLACE_HOLDER> .,image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( javax . net . ssl . ssl context . class @$ __str__ ) ;,rerun have
check whether the callable threw an <PLACE_HOLDER> .,if ( future . is done ( ) ) { try { future . get ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; throw new runtime exception ( e ) ; } it . remove ( ) ; },callable threw
add legacy subsystems the regular model will have the new attributes because they are in the xml @$ but the reverse controller model will not because transformation strips <PLACE_HOLDER>,builder . create legacy kernel services builder ( null @$ controller version @$ model version ) . add maven resourceurl ( __str__ + controller version . get maven gav version ( ) ) . configure reverse controller check ( additional initialization . management @$ model node -> { for ( model node node : model node . get ( global_modules ) . as list ( ) ) { if ( __str__ . equals ( node . get ( name ) . as string ( ) ) ) { if ( ! node . has ( annotations ) ) { node . get ( annotations ) . set ( false ) ; } if ( ! node . has ( meta_inf ) ) { node . get ( meta_inf,model strips
bad value allows <PLACE_HOLDER> .,trigger action in cell editor ( key event . vk_escape ) ; assert not editing field ( ) ; assert equals ( __num__ @$ model . get num selected rows ( ) ) ; assert equals ( __num__ @$ model . get min index selected ( ) ) ; assert equals ( get data type ( __num__ ) @$ dt ) ;,value allows
if we attempt to interact with the server too quickly @$ we will get a zoo keeper connection loss exception @$ <PLACE_HOLDER> the provider wraps in an io exception . we will wait 1 second in this case and try again . the test will timeout if this does not succeeed within 20 seconds .,thread . sleep ( __num__ ) ; e . print stack trace ( ) ; assert . fail ( __str__ + e . get class ( ) + __str__ @$ e ) ;,provider wraps
this could happen when the time zone is not associated with a country @$ and its id is not hierarchical @$ for example @$ cst 6 cdt . we use the canonical id <PLACE_HOLDER> as the location for this case .,if ( location == null ) { location = tzid ; },canonical id
create a simple rule which just writes <PLACE_HOLDER> new to the output file .,build target target = build target factory . new instance ( __str__ ) ; build rule rule = new write file ( target @$ filesystem @$ __str__ @$ output @$ false ) ;,which writes
calls set image <PLACE_HOLDER> internally,super . set image bitmap ( bitmap ) ;,calls set
dispatching to empty statement will not call back <PLACE_HOLDER> @$ must call our visit empty statement explicitly,if ( finally statement instanceof empty statement ) { visit empty statement ( ( empty statement ) finally statement ) ; } else { finally statement . visit ( this ) ; },statement call
first dfs client has no files open so does n't renew <PLACE_HOLDER> .,final dfs client mock client1 = create mock client ( ) ; mockito . do return ( false ) . when ( mock client1 ) . renew lease ( ) ; assert same ( renewer @$ lease renewer . get instance ( fake_authority @$ fake_ugi_a @$ mock client1 ) ) ; long file id = __num__ ; renewer . put ( mock client1 ) ;,files renew
make sure any exceptions caused by handlers do n't crash <PLACE_HOLDER>,return noop aware finished span handler . create ( defensive copy @$ noop ) ;,exceptions crash
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,object [ ] [ ] contents = new object [ ] [ ] { { msg key . bad_msgkey @$ __str__ } @$ { msg key . bad_msgformat @$ __str__ } @$ { msg key . er_serializer_not_contenthandler @$ __str__ } @$ { msg key . er_resource_could_not_find @$ __str__ } @$ { msg key . er_resource_could_not_load @$ __str__ } @$ { msg key . er_buffer_size_lessthan_zero @$ __str__ } @$ { msg key . er_invalid_utf16_surrogate @$ __str__ } @$ { msg key . er_oierror @$ __str__ } @$ { msg key . er_illegal_attribute_position @$ __str__ } @$ { msg key . er_namespace_prefix @$ __str__ } @$ { msg key . er_stray_attribute @$ __str__ } @$ { msg key . er_stray_namespace @$ __str__ } @$ { msg key . er_could_not_load_resource @$,text specifies
let the application handle the <PLACE_HOLDER> .,return __num__ ;,application handle
apply the predicates and fetch the primary key <PLACE_HOLDER>s look up the <PLACE_HOLDER> and convert them to bean,try { return run task ( new query task < list < e > > ( ) { @ override public list < e > handle ( connection connection ) throws exception { list < e > ret = new array list < > ( ) ; if ( ! ids to find . is empty ( ) ) { list < generic json entity > entities ; try ( prepared statement select statement = sql query builder . create find by id statement ( connection @$ generic json entity . class @$ ids to find ) ) { try ( result set result set = select statement . execute query ( ) ) { entities = generic result set mapper . map all ( result set @$,ids look
larger than default for default empty <PLACE_HOLDER> . it 's already supposed to be at default size .,if ( min capacity > min expand ) { ensure explicit capacity ( min capacity ) ; },default empty
close server 1 and pause so server has <PLACE_HOLDER> to close,close server ( server1 ) ; wait . pause ( __num__ * __num__ ) ; wait for cqs disconnected ( client @$ __str__ @$ __num__ ) ;,server has
we do n't know how many primaries will move @$ because the move buckets algorithm could move the primary or it could move a redundant <PLACE_HOLDER> . but after we 're done @$ we should only have one primary per member .,set < partition member info > detail set = model . get partitioned member details ( __str__ ) ; for ( partition member info member : detail set ) { assert equals ( __num__ @$ member . get primary count ( ) ) ; assert equals ( __num__ @$ member . get bucket count ( ) ) ; },primaries move
promo snapshot collapses as the panel reaches the maximized <PLACE_HOLDER> .,update appearance ( __num__ - percentage ) ;,panel reaches
if fifo <PLACE_HOLDER> reaches max <PLACE_HOLDER> we evict the eldest entry .,if ( queue . size ( ) > max size ) { evict one ( ) ; } return result ;,size reaches
this doc should unblock <PLACE_HOLDER>,c . save ( new basicdb object ( __str__ @$ __num__ ) @$ write concern . acknowledged ) ; assert equals ( __num__ @$ ( long ) future . get ( __num__ @$ seconds ) ) ; cur . close ( ) ;,doc unblock
if this edit is creating the <PLACE_HOLDER> for the first time @$ every index must have a value .,if ( success && ! entry . readable ) { for ( int i = __num__ ; i < value count ; i ++ ) { if ( ! editor . written [ i ] ) { editor . abort ( ) ; throw new illegal state exception ( __str__ + i ) ; } if ( ! entry . get dirty file ( i ) . exists ( ) ) { editor . abort ( ) ; return ; } } } for ( int i = __num__ ; i < value count ; i ++ ) { file dirty = entry . get dirty file ( i ) ; if ( success ) { if ( dirty . exists ( ) ) { file clean =,edit creating
log a warning if the resource uses an unnecessary service error definition <PLACE_HOLDER>,if ( service error def annotation != null && ! resource model . is any service error list defined ( ) ) { log . warn ( string . format ( __str__ + __str__ + __str__ @$ resource class . get name ( ) @$ service error def . class . get simple name ( ) @$ service errors . class . get simple name ( ) @$ param error . class . get simple name ( ) ) ) ; },resource uses
client sends a cds <PLACE_HOLDER> for the only cluster being watched to management server .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_cds @$ __str__ ) ) ) ;,client sends
nb . this is always encoded with the implicit tag the checks only make <PLACE_HOLDER> if we assume implicit tagging @$ with explicit tagging the form is always constructed .,if ( opt . is context specific ( tag_id ) && ! opt . is constructed ( ) ) { if ( id != null ) throw new io exception ( __str__ + __str__ ) ; opt . reset tag ( der value . tag_ octet string ) ; id = new key identifier ( opt ) ; } else if ( opt . is context specific ( tag_names ) && opt . is constructed ( ) ) { if ( names != null ) throw new io exception ( __str__ + __str__ ) ; opt . reset tag ( der value . tag_ sequence ) ; names = new general names ( opt ) ; } else if ( opt . is context specific ( tag_serial_num ),checks make
ensure a timer throws an illegal state <PLACE_HOLDER> if task already cancelled,t = new timer ( ) ; test task = new timer test task ( ) ; test task . cancel ( ) ; try { t . schedule ( test task @$ __num__ @$ __num__ ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } t . cancel ( ) ;,timer throws
we capture and set the context once the user provided observable <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,user provided
make sure the user can set the file <PLACE_HOLDER> to null @$ to allow the clearing of a value,options options = load search options ( ) ; help location help = null ; file default value = null ; string option name = __str__ ; options . register option ( option name @$ option type . file_type @$ default value @$ help @$ __str__ ) ; file option value = options . get file ( option name @$ null ) ; assert null ( option value ) ; file file = new file ( __str__ ) ; options . put object ( option name @$ file ) ; option value = options . get file ( option name @$ null ) ; assert equals ( file @$ option value ) ;,user set
calculate the current radius at which to place the selection <PLACE_HOLDER> .,final int sel radius = m selector radius ; final float sel length = m circle radius - math utils . lerp ( hours inset @$ minutes inset @$ m hours to minutes ) ; final double sel angle rad = math . to radians ( math utils . lerp deg ( hours angle deg @$ minutes angle deg @$ m hours to minutes ) ) ; final float sel centerx = mx center + sel length * ( float ) math . sin ( sel angle rad ) ; final float sel centery = my center - sel length * ( float ) math . cos ( sel angle rad ) ;,which place
data actually contains a 'data ' <PLACE_HOLDER> so process data using this,mp4 box header header = new mp4 box header ( data ) ; mp4 data box databox = new mp4 data box ( header @$ data ) ; data size = header . get data length ( ) ; numbers = databox . get numbers ( ) ;,data contains
this newly opened jar file has its own <PLACE_HOLDER> @$ merge it into the parent 's <PLACE_HOLDER> @$ taking into account the relative path .,jar index new index = new loader . get index ( ) ; if ( new index != null ) { int pos = jar name . last index of ( __str__ ) ; new index . merge ( this . index @$ ( pos == - __num__ ? null : jar name . substring ( __num__ @$ pos + __num__ ) ) ) ; },file has
stupid layoutlib can not handle simple class <PLACE_HOLDER> .,if ( is in edit mode ( ) ) { class loader = this . get class ( ) . get class loader ( ) ; } else { class loader = context . get class loader ( ) ; },layoutlib handle
drop partitioned <PLACE_HOLDER> .,region . destroy region ( ) ;,drop partitioned
minimum two level stored artifact <PLACE_HOLDER>,try ( in memory artifact cache in memory artifact cache = new in memory artifact cache ( ) ; two level artifact cache decorator two level cache = new two level artifact cache decorator ( in memory artifact cache @$ test project filesystems . create project filesystem ( tmp . get root ( ) ) @$ buck event bus for tests . new instance ( ) @$ true @$ __num__ @$ optional . empty ( ) ) ; two level artifact cache decorator two level cache no store = new two level artifact cache decorator ( in memory artifact cache @$ test project filesystems . create project filesystem ( tmp . get root ( ) ) @$ buck event bus for tests . new instance ( ) @$,level stored
verify user from a group which has column family level access can read all the <PLACE_HOLDER> belonging to that family and group which has no access ca n't read any <PLACE_HOLDER> .,grant on table ( test_util @$ testgroup_1_name @$ table name @$ test_family @$ null @$ permission . action . read ) ; verify allowed ( testgroup1_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup1_user1 @$ scan family action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan family action for group with family level access ) ;,access read
the application should not use the <PLACE_HOLDER> to connect,assert user connected ( wc @$ username ) ;,application use
disabling the single plan item will terminate the <PLACE_HOLDER>,cmmn runtime service . disable plan item instance ( plan item instance . get id ( ) ) ; assert case instance ended ( case instance ) ;,item terminate
if user provides a config <PLACE_HOLDER> @$ use it as base configs .,if ( string utils . is none empty ( config file path ) ) { log . info ( __str__ + config file path ) ; final file config file = new file ( config file path ) ; final uri config uri = config file . touri ( ) ; final config factory config factory = options . get config factory ( ) . get declared constructor ( ) . new instance ( ) ; log . info ( __str__ + config factory . get class ( ) . get name ( ) ) ; if ( config factory instanceof properties config factory ) { check argument ( config file . exists ( ) @$ __str__ @$ config file path ) ; } config . put all,user provides
if target has <PLACE_HOLDER> remaining @$ need n't to check the is read only,target . flip ( ) ; assert equals ( __num__ @$ source . read ( target ) ) ;,target has
make sure task 1 is running and blocking the <PLACE_HOLDER>,assert that ( task1 running . await ( __num__ @$ time unit . seconds ) ) . is true ( ) ;,task running
check if content matches minimum length <PLACE_HOLDER>,if ( algorithm . equals ( tlsh ) && flow file . get size ( ) < __num__ ) { return false ; } else { return true ; },content matches
say that a null label matches no positive <PLACE_HOLDER> @$ but any negated patern,if ( lab == null ) { return negated pattern ; } else { if ( basic cat ) { lab = basic cat function . apply ( lab ) ; } matcher m = pattern . matcher ( lab ) ; return m . find ( ) != negated pattern ; },label matches
caller wants <PLACE_HOLDER> of boot class field,type = vm . find boot type ( signature ) ;,caller wants
if preflight timed out @$ m result will contain error <PLACE_HOLDER> as int .,if ( total size < __num__ ) { return ( int ) total size ; } if ( more_debug ) { slog . v ( tag @$ __str__ + total size ) ; } i backup transport transport = m transport client . connect or throw ( __str__ ) ; result = transport . check full backup size ( total size ) ; if ( result == backup transport . transport_quota_exceeded ) { if ( more_debug ) { slog . d ( tag @$ __str__ + pkg . package name + __str__ + total size + __str__ + m quota ) ; } remote call . execute ( callback -> agent . do quota exceeded ( total size @$ m quota @$ callback ) @$ m agent timeout,result contain
this property is n't contributing <PLACE_HOLDER> this happens when you try to map an empty sequence to a property,if ( prop . ref ( ) . is empty ( ) ) return ;,property contributing
null separator means use <PLACE_HOLDER>,if ( separator chars == null ) { while ( i < len ) { if ( character . is whitespace ( str . char at ( i ) ) ) { if ( match || preserve all tokens ) { last match = true ; if ( size plus1 ++ == max ) { i = len ; last match = false ; } list . add ( str . substring ( start @$ i ) ) ; match = false ; } start = ++ i ; continue ; } last match = false ; match = true ; i ++ ; } } else if ( separator chars . length ( ) == __num__ ) { char sep = separator chars . char at (,separator means
now reptable has 2000 <PLACE_HOLDER> and parttable has 252 <PLACE_HOLDER>,save tables with path ( client @$ testnonce + __str__ @$ snapshots path ) ; client . close ( ) ;,reptable has
insert the test hooks on local and remote node . test hook on remote node will throw cache <PLACE_HOLDER> while test hook on local node will throw query <PLACE_HOLDER> .,vm1 . invoke ( ( ) -> { query observer holder . set instance ( new counting bucket destroy query observer ( __num__ ) ) ; } ) ; vm0 . invoke ( ( ) -> { default query query = ( default query ) pr queryd unit helper . get cache ( ) . get query service ( ) . new query ( __str__ + partitioned_region_name ) ; query observer holder . set instance ( new counting bucket destroy query observer ( __num__ ) ) ; query . execute ( ) ; } ) ;,hook throw
the component has entered the focused <PLACE_HOLDER> either if it is larger than half of the viewport and it occupies at least half of the viewport or if it is smaller than half of the viewport and it is fully visible .,return ( total component area >= half viewport area ) ? ( visible component area >= half viewport area ) : component bounds . equals ( component visible bounds ) ;,component entered
calculate how many <PLACE_HOLDER>s we need . as each task handles a separate <PLACE_HOLDER> of data @$ so we want the number of <PLACE_HOLDER>s equal to the number of tasks,int workergroup number = conf . get int ( angel conf . angel_workergroup_number @$ angel conf . default_angel_workergroup_number ) ; int task num in worker = conf . get int ( angel conf . angel_worker_task_number @$ angel conf . default_angel_worker_task_number ) ; int split num = workergroup number * task num in worker ; log . info ( __str__ + split num ) ; if ( ! use newapi ) { log . info ( __str__ ) ; org . apache . hadoop . mapred . input split [ ] split array = generate splits use oldapi ( conf @$ split num ) ; log . info ( __str__ + split array . length ) ; if ( log . is debug enabled ( ) ) { int,task handles
usually happens when permission denied listing <PLACE_HOLDER> in directory,log . e ( __str__ @$ __str__ + remote path @$ e ) ;,permission denied
assume properties contain <PLACE_HOLDER>,if ( property value == null ) { if ( require property ) { throw new illegal argument exception ( __str__ + property key + __str__ + property key + __str__ ) ; } else { return null ; } } property value = get slashy path ( property value ) ; property value = correct double slash ( property value @$ property index end @$ str ) ; result += property value ; property index end ++ ; property index start = property index end ;,properties contain
avoid output warn : <PLACE_HOLDER> rejected,request config global config = request config . custom ( ) . set cookie spec ( cookie specs . ignore_cookies ) . build ( ) ; builder . set default request config ( global config ) ; closeable http client http client = builder . build ( ) ; return http client ;,output warn
merge input <PLACE_HOLDER> : data <PLACE_HOLDER> override header <PLACE_HOLDER>,if ( fields from header != null ) { if ( ! ( fields from header instanceof point builder ) ) { throw new illegal state exception ( __str__ + fields from header ) ; } fields from data . merge with header ( ( point builder ) fields from header ) ; },fields override
if the ancestor has a forced preferred <PLACE_HOLDER> @$ its layout manager may be able to give a good enough estimation .,if ( ancestor . is preferred size set ( ) ) { layout manager ancestor layout = ancestor . get layout ( ) ; if ( ancestor layout != null ) { dimension preferred layout size = ancestor layout . preferred layout size ( ancestor ) ; if ( preferred layout size != null ) { component = ancestor ; width = preferred layout size . width ; height = preferred layout size . height ; } } } else { dimension pref size = ancestor . get preferred size ( ) ; if ( pref size != null ) { component = ancestor ; width = pref size . width ; height = pref size . height ; } },ancestor has
\u 00 a <PLACE_HOLDER> and \uffe <PLACE_HOLDER> are actually the same symbol @$ just different code points . but the ri returns the \uffe <PLACE_HOLDER> and android returns those with \u 00 a <PLACE_HOLDER>,string [ ] yen = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; string [ ] dollar = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ;,ri returns
add all udtf columns following sel will do <PLACE_HOLDER> for columns from udtf @$ not adding sel in here,new cols . add all ( from column names ( output cols . sub list ( num sel columns @$ output cols . size ( ) ) ) ) ; op . get conf ( ) . set output internal col names ( to column names ( new cols ) ) ; prune operator ( ctx @$ op @$ new cols ) ; cpp ctx . get pruned col lists ( ) . put ( op @$ cols after replacement ) ; return null ;,sel do
rack 1 has file 1 @$ file 2 and file <PLACE_HOLDER> and file 4 rack 2 has file 2 and file <PLACE_HOLDER> and file 4 rack <PLACE_HOLDER> has file <PLACE_HOLDER> and file 4 setup a filter so that only file 1 and file 2 can be combined,in format = new dummy input format ( ) ; file input format . add input path ( job @$ in dir ) ; in format . set min split size rack ( __num__ ) ;,2 file
for the best integration possible @$ we will ask ognl which property accessor it would use for this target object @$ and then depending on the result apply our equivalent or just <PLACE_HOLDER> to ognl evaluation if it is a custom property accessor we do not implement .,final class < ? > target class = ognl runtime . get target class ( target ) ; final property accessor ognl property accessor = ognl runtime . get property accessor ( target class ) ;,accessor apply
if either of the requests have empty stream <PLACE_HOLDER> @$ the merge should have empty stream <PLACE_HOLDER> .,download request merged request = request1 . copy with merged request ( request2 ) ; assert that ( merged request . stream keys ) . is empty ( ) ; merged request = request2 . copy with merged request ( request1 ) ; assert that ( merged request . stream keys ) . is empty ( ) ;,either have
this can only happen if one of our entries has an unsupported <PLACE_HOLDER> .,throw wrapper . invalid ctb converter name ( ucne @$ codeset . get name ( ) ) ;,one has
walk the args : the various drivers expect unpacked <PLACE_HOLDER> of the elements,object [ ] used args = new object [ args . length ] ; for ( int i = __num__ ; i < args . length ; i ++ ) { used args [ i ] = unpack wrapped element ( args [ i ] ) ; } return used args ;,drivers expect
view table can not have ttl <PLACE_HOLDER>,if ( m_in strict mat view diff mode ) { return __str__ ; },table have
java needs the obsolete <PLACE_HOLDER> @$ icu needs the modern <PLACE_HOLDER> @$ but we let icu know about both so that we never need to convert back when talking to it .,linked hash set < locale > set = new linked hash set < locale > ( ) ; for ( string locale name : locale names ) { set . add ( locale from icu locale id ( locale name ) ) ; } return set . to array ( new locale [ set . size ( ) ] ) ;,icu needs
we no longer know what bin has the smallest <PLACE_HOLDER>,_ssx = - __num__ ;,bin has
proxy servlet will receive an absolute uri from the client @$ and convert it to a relative uri . the connect handler wo n't modify <PLACE_HOLDER> the client sent @$ which must be a relative uri .,assert that ( request . length ( ) @$ matchers . greater than ( __num__ ) ) ; if ( server ssl context factory == null ) assert false ( request . contains ( __str__ ) ) ; else assert false ( request . contains ( __str__ ) ) ; string response = __str__ + __str__ + __str__ ; get end point ( ) . write ( callback . noop @$ byte buffer . wrap ( response . get bytes ( standard charsets . utf_8 ) ) ) ;,client sent
create dummy get all <PLACE_HOLDER> operation as some of the methods in accumulo store test if the operation is a get all <PLACE_HOLDER> operation and if so set some options . we need those options if operation is returning all the <PLACE_HOLDER> .,if ( operation instanceof getrdd of all elements ) { derived operation = get get all elements ( operation ) ; } else { derived operation = operation ; },operation returning
client 1 did not register <PLACE_HOLDER>,await ( ) . until asserted ( ( ) -> { assert that ( client1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ ) ; assert that ( client2 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ ) ; assert that ( server1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ ) ; assert that ( server2 . invoke ( (,client register
6 attribute header <PLACE_HOLDER> @$ 6 <PLACE_HOLDER> for name @$ flags and version @$ and 5 2 <PLACE_HOLDER> for counts .,int size = __num__ + requires . length + exports . length + opens . length + uses index . length + provides . length ; if ( package count > __num__ ) { symbol table . add constant utf8 ( constants . module_packages ) ; size += __num__ + package index . length ; },6 attribute
document can contain null <PLACE_HOLDER> by key,if ( doc . contains key ( field ) ) { modify field with dot notation ( doc @$ field ) ; return ; } list < field name and value > new fields = null ; iterator < map . entry < string @$ object > > it = doc . entry set ( ) . iterator ( ) ; while ( it . has next ( ) ) { map . entry < string @$ object > entry = it . next ( ) ; string [ ] origin key nodes = dot . split ( entry . get key ( ) ) ; string [ ] key nodes = exclude numeric items ( origin key nodes ) ; if ( key nodes . length >,document contain
models . you can write model files using the model template files <PLACE_HOLDER> . if you want to create one template for file @$ you can do so here . for multiple files for model @$ just put another entry in the ` model template files ` with a different extension,model template files . put ( __str__ @$ __str__ ) ;,entry files
search for issues of project 1 having <PLACE_HOLDER> than 14 days and project 2 having <PLACE_HOLDER> then 25 days,assert that search returns only ( issue query . builder ( ) . created after by project uuids ( immutable map . of ( project1 . uuid ( ) @$ new issue query . period start ( add days ( now @$ - __num__ ) @$ true ) @$ project2 . uuid ( ) @$ new issue query . period start ( add days ( now @$ - __num__ ) @$ true ) ) ) @$ project1 issue1 . key ( ) @$ project2 issue1 . key ( ) ) ;,days having
verify that an inv<PLACE_HOLDER>e method works <PLACE_HOLDER> after a pop frames in a thread suspended by an event .,main thread . pop frames ( frame for ( __str__ ) ) ; system . out . println ( __str__ ) ; system . out . println ( __str__ ) ; method invokeee method = ( method ) target class . methods by name ( __str__ ) . get ( __num__ ) ; try { target class . invoke method ( main thread @$ invokeee method @$ new array list ( ) @$ __num__ ) ; } catch ( exception ex ) { failure ( __str__ + ex ) ; ex . print stack trace ( ) ; } system . out . println ( __str__ ) ;,method works
entry does not have attributes <PLACE_HOLDER> : ca certs can have no attributes open ssl generates pkcs 12 with no attr for ca certs .,attr set = null ;,entry attributes
relative paths have no <PLACE_HOLDER> .,return new gcs path ( fs @$ __str__ @$ component ) ;,paths have
expecting success transition because jackson is taking <PLACE_HOLDER> of escaping the bad json characters,test runner . get flow files for relationship ( attributes tojson . rel_success ) . get ( __num__ ) . assert attribute exists ( attributes tojson . json_attribute_name ) ; test runner . assert transfer count ( attributes tojson . rel_success @$ __num__ ) ; test runner . assert transfer count ( attributes tojson . rel_failure @$ __num__ ) ;,jackson taking
january 1 of the year does n't exist . use gregorian cutover <PLACE_HOLDER> as the first day of the year .,if ( year == gregorian cutover year && cal == gcal && fixed date < gregorian cutover date && gregorian cutover year != gregorian cutover year julian ) { fixed date = gregorian cutover date ; },january use
test using default value for the <PLACE_HOLDER> . individually specified env properties do not have defaults @$ so this should just get things from the default prop name <PLACE_HOLDER> .,string bogus prop = prop name + __str__ ; apps . set env from input property ( env @$ bogus prop @$ default prop value @$ conf @$ file . path separator ) ;,things prop
this configuration does not accept <PLACE_HOLDER>,return null ;,configuration accept
test does n't support <PLACE_HOLDER> since the guts view is not attached .,do nothing ( ) . when ( guts ) . open controls ( eq ( true ) @$ any int ( ) @$ any int ( ) @$ any boolean ( ) @$ any ( runnable . class ) ) ;,test support
to start @$ offer a box which contains <PLACE_HOLDER> of the colors,pq . offer ( new vbox ( __num__ @$ m colors . length - __num__ ) ) ;,which contains
more complicated . vh 2 has a higher <PLACE_HOLDER> @$ but has some exceptions that vh 1 does not have .,region version holder vh1 = new region version holder ( member ) ; region version holder vh2 = new region version holder ( member ) ; bit set bs1 = new bit set ( ) ; bs1 . set ( __num__ @$ __num__ ) ; bs1 . set ( __num__ @$ __num__ ) ; record versions ( vh1 @$ bs1 ) ; bit set bs2 = new bit set ( ) ; bs2 . set ( __num__ @$ __num__ ) ; bs2 . set ( __num__ @$ __num__ ) ; record versions ( vh2 @$ bs2 ) ;,vh has
confirm both iterables contain the same <PLACE_HOLDER> of elements,assert . assert equals ( all points list . size ( ) @$ iterable with size . size ( ) ) ;,iterables contain
the caller must hold <PLACE_HOLDER> to synchronize the update .,final ref count += n ; if ( final ref count > peak final ref count ) { peak final ref count = final ref count ; },caller hold
check if the two collections have at least one <PLACE_HOLDER> in common .,boolean jdk = ! collections . disjoint ( collection1 @$ collection2 ) ;,collections have
this is to see if the server has any <PLACE_HOLDER>,try { orb port info [ ] serverorb and port list = entry . lookup ( iiop_clear_text . value ) ; } catch ( exception exc ) { return null ; },server has
ok @$ so perhaps @$ we can use the <PLACE_HOLDER> from a previous execution ?,string [ ] saved = props . is initialized ( ) ? props . get instance ( ) . get last arguments ( ) : null ;,ok use
no rows match purge <PLACE_HOLDER>,verify proc fails ( client @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __num__ ) ; client response cr = client . call procedure ( __str__ @$ __str__ + __str__ + __str__ ) ; assert equals ( client response . success @$ cr . get status ( ) ) ;,rows match
2 range conditions are used on different columns @$ but not all sql databases properly optimize it . some databases can only use an <PLACE_HOLDER> on one of the columns . an additional condition provides explicit knowledge that 'start ' can not be greater than 'end ' .,list < data segment > matching segments = connector . in read only transaction ( new transaction callback < list < data segment > > ( ) { @ override public list < data segment > in transaction ( final handle handle @$ final transaction status status ) { return handle . create query ( string utils . format ( __str__ + __str__ @$ db tables . get segments table ( ) @$ connector . get quote string ( ) ) ) . set fetch size ( connector . get streaming fetch size ( ) ) . bind ( __str__ @$ data source ) . bind ( __str__ @$ interval . get start ( ) . to string ( ) ) . bind ( __str__ @$ interval .,databases use
0 : no stats since hbm.xml ca n't enable natural id <PLACE_HOLDER>,assert equals ( __num__ @$ session factory ( ) . get statistics ( ) . get natural id cache hit count ( ) ) ;,stats enable
we treat calls from a profile as if made by its parent as profiles share the accessibility state of the parent . the call below performs the current profile parent <PLACE_HOLDER> .,synchronized ( m lock ) { final int resolved user id = m security policy . resolve calling user id enforcing permissions locked ( user id ) ; user state user state = get user state locked ( resolved user id ) ; client client = new client ( callback @$ binder . get calling uid ( ) @$ user state ) ; if ( m security policy . is caller interacting across users ( user id ) ) { m global clients . register ( callback @$ client ) ; if ( debug ) { slog . i ( log_tag @$ __str__ + binder . get calling pid ( ) ) ; } return int pair . of ( user state . get client state ( ),call performs
the user asked for stats to be collected . some stats like number of rows require a <PLACE_HOLDER> of the data however @$ some other stats @$ like number of files @$ do not require a complete <PLACE_HOLDER> update the stats which do not require a complete <PLACE_HOLDER> .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },user require
lets change some <PLACE_HOLDER> to best fit the view .,m week view . set column gap ( ( int ) typed value . apply dimension ( typed value . complex_unit_dip @$ __num__ @$ get resources ( ) . get display metrics ( ) ) ) ; m week view . set text size ( ( int ) typed value . apply dimension ( typed value . complex_unit_sp @$ __num__ @$ get resources ( ) . get display metrics ( ) ) ) ; m week view . set event text size ( ( int ) typed value . apply dimension ( typed value . complex_unit_sp @$ __num__ @$ get resources ( ) . get display metrics ( ) ) ) ;,lets change
announce what cloud we think we are in . publish our <PLACE_HOLDER> as well .,udp heartbeat . build_and_multicast ( cloud @$ hb ) ;,announce publish
tests get maximum <PLACE_HOLDER>,stores to versions = get admin client ( ) . readonly ops . getro max version ( __num__ @$ lists . new array list ( __str__ @$ __str__ ) ) ; assert equals ( stores to versions . size ( ) @$ __num__ ) ; assert equals ( stores to versions . get ( __str__ ) . long value ( ) @$ __num__ ) ; assert equals ( stores to versions . get ( __str__ ) . long value ( ) @$ __num__ ) ;,tests get
apply settings to selected document builder may throw <PLACE_HOLDER> if incompatible,if ( settings != null ) { for ( int i = __num__ ; i < settings . length ; i ++ ) { } },settings throw
create a new item list which contain a new adapter item object the id of the new item is changed @$ and will be treated as a new item according to the rule we set in the callback . this test case is to verify the get change payload <PLACE_HOLDER> still honor the standard we set up to judge new item,m items . clear ( ) ; adapter item new item = new adapter item ( __num__ @$ __str__ @$ __str__ ) ; m items . add ( new item ) ;,the honor
make sure a status update does report <PLACE_HOLDER>,map task status mock status = new map task status ( attemptid @$ __num__ @$ __num__ @$ task status . state . running @$ __str__ @$ __str__ @$ __str__ @$ task status . phase . map @$ new counters ( ) ) ; feedback = listener . status update ( attemptid @$ mock status ) ; assert true ( feedback . get task found ( ) ) ; verify ( hb handler ) . progressing ( eq ( attempt id ) ) ;,update does
if obj extends unicast remote <PLACE_HOLDER> @$ set its ref .,if ( obj instanceof unicast remote object ) { ( ( unicast remote object ) obj ) . ref = sref ; } return sref . export object ( obj @$ null @$ false ) ;,obj extends
this simulates the completion of txnid : id txn update <PLACE_HOLDER>,long write id = txn mgr2 . get table write id ( __str__ @$ __str__ ) ; add dynamic partitions adp = new add dynamic partitions ( txn mgr2 . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr2 . commit txn ( ) ;,txn update
if this component already has focus @$ then activate the input method by dispatching a synthesized focus gained <PLACE_HOLDER> .,if ( is focus owner ( ) ) { input context input context = get input context ( ) ; if ( input context != null ) { focus event focus gained event = new focus event ( this @$ focus event . focus_gained ) ; input context . dispatch event ( focus gained event ) ; } } event mask |= awt event . input_methods_enabled_mask ; if ( ( event mask & awt event . input_methods_enabled_mask ) != __num__ ) { input context input context = get input context ( ) ; if ( input context != null ) { input context . end composition ( ) ; input context . remove notify ( this ) ; } } event mask &= ~ awt event . input_methods_enabled_mask,focus gained
verify tracker in server contains no <PLACE_HOLDER>,servervm . invoke ( new cache serializable runnable ( __str__ ) { @ override public void run2 ( ) throws cache exception { local region region = ( local region ) get root region ( ) . get subregion ( region name ) ; map event state = region . get event state ( ) ; assert equals ( __num__ @$ event state . size ( ) ) ; } } ) ;,tracker contains
skip notes that match csum but not key add <PLACE_HOLDER> of note to every position in duplicates array corresponding to the current key,if ( key to indexes map . contains key ( note . get key ( ) ) ) { list < integer > output pos = key to indexes map . get ( note . get key ( ) ) ; for ( int i = __num__ ; i < output pos . size ( ) ; i ++ ) { add note to duplicates array ( i > __num__ ? new note info ( note ) : note @$ duplicates @$ output pos . get ( i ) ) ; } },notes add
if this project has correlated <PLACE_HOLDER> @$ create value generator and produce the correlated variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel ) ; },project correlated
case . it consists of two arrays of lines . the first array of lines is the test input @$ and the second one is the expected output . if the second array has a single <PLACE_HOLDER> starting with ! ! then it is expected that import orderer will throw a formatter exception with that message . if a line ends with \ then,string [ ] [ ] [ ] inputs outputs = { { { } @$ { } } @$ { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__,array has
check that class does not import the same parameterized <PLACE_HOLDER> with two different argument lists .,chk . check class bounds ( tree . pos ( ) @$ c . type ) ; tree . type = c . type ; for ( list < jc type parameter > l = tree . typarams ; l . non empty ( ) ; l = l . tail ) { assert . check non null ( env . info . scope . lookup ( l . head . name ) . scope ) ; },class import
add <PLACE_HOLDER> has thrown an exception ! set future.run ca n't throw any exceptions so this must have been caused by add <PLACE_HOLDER> itself . the most likely explanation is a misconfigured mock . try to switch to failure .,failure failure ; try { failure = new failure ( t ) ; } catch ( throwable oom most likely ) { failure = failure . fallback_instance ; },itself add
note to translators : the stylesheet referred to an <PLACE_HOLDER> to the xsl syntax and indicated that it was defined by xsltc @$ but xstlc does not recognized the particular <PLACE_HOLDER> named . the substitution text gives the <PLACE_HOLDER> name .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,xstlc recognized
copy local variables to event scope execution by value . this way @$ the event scope execution references a <PLACE_HOLDER> ' of the local variables,new sub process variable snapshotter ( ) . set variables snapshots ( sub process execution @$ event scope execution ) ;,execution references
normal template literals still throw <PLACE_HOLDER>,parse error ( __str__ @$ __str__ ) ;,literals throw
let 's build the response <PLACE_HOLDER>,datagram packet resp packet = make response packet ( packet ) ;,"s" build
for local queries returning pdx <PLACE_HOLDER> wrap the resultset with results collection pdx deserializer wrapper which deserializes these pdx <PLACE_HOLDER> .,if ( needspdx deserialization wrapper ( true ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ false ) ; },which deserializes
figure out what type of glob path was given @$ will convert globbed <PLACE_HOLDER> to match the type to preserve relativity,path type glob type ; uri glob uri = glob path . to uri ( ) ; if ( glob uri . get scheme ( ) != null ) { glob type = path type . has_scheme ; } else if ( ! glob uri . get path ( ) . is empty ( ) && new path ( glob uri . get path ( ) ) . is absolute ( ) ) { glob type = path type . schemeless_absolute ; } else { glob type = path type . relative ; },figure convert
special handle <PLACE_HOLDER>,if ( key . equals ( __str__ ) ) { key = resource information . gpu_uri ; },special handle
when the encoded graph has <PLACE_HOLDER> inlining @$ we can already have a proper caller state . if not @$ we set the caller state here .,if ( outer state . outer frame state ( ) == null && method scope . caller != null ) { ensure outer state decoded ( method scope . caller ) ; outer state . set outer frame state ( method scope . caller . outer state ) ; } method scope . outer state = outer state ;,graph has
assert the package tracker triggered an <PLACE_HOLDER> .,check update check triggered ( new package versions ) ; check token token2 = m fake intent helper . capture and reset last token ( ) ;,tracker triggered
ensure the spec executor has the correct <PLACE_HOLDER>,spec executor = job spec with executor . get spec executor ( ) ; assert . assert equals ( spec executor . get uri ( ) . to string ( ) @$ __str__ ) ; assert . assert equals ( spec executor . get class ( ) . get canonical name ( ) @$ __str__ ) ;,executor has
each application asks 10 5 gb <PLACE_HOLDER>,am1 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; am2 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; am3 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; capacity scheduler cs = ( capacity scheduler ) rm1 . get resource scheduler ( ) ; rm node rm node1 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ; fi ca scheduler app scheduler app1 = cs . get application attempt ( am1 . get application attempt id ( ) ) ; fi ca scheduler app scheduler app2 = cs . get application attempt ( am2 . get,application asks
test reads that cross checksum <PLACE_HOLDER>,stm . seek ( __num__ ) ; assert equals ( stm . get pos ( ) @$ __num__ ) ; stm . read fully ( actual @$ __num__ @$ half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ half_chunk_size ) ; stm . read fully ( actual @$ half_chunk_size @$ block_size - half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ block_size ) ; stm . read fully ( actual @$ block_size @$ bytes_per_sum + half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ block_size + bytes_per_sum + half_chunk_size ) ; stm . read fully ( actual @$ __num__ * block_size - half_chunk_size @$ file_size - ( __num__ * block_size - half_chunk_size ) ) ; assert,test reads
null value will always have null <PLACE_HOLDER>,if ( value == null ) { this . value quotes = null ; } else { if ( value quotes == null ) { this . value quotes = attribute value quotes . double ; } else if ( value quotes == attribute value quotes . none && value . length ( ) == __num__ ) { this . value quotes = attribute value quotes . double ; } else { this . value quotes = value quotes ; } },value have
this pattern does n't match any <PLACE_HOLDER> @$ but should be compiled anyway,pattern = __str__ ; pattern . compile ( pattern ) ;,pattern match
block until submissions create further <PLACE_HOLDER>,if ( m_clients pending notification . is empty ( ) ) { run submissions ( true ) ; m_limiter = rate limiter . create ( notification_rate @$ warmup_ms @$ time unit . milliseconds ) ; } else { run submissions ( false ) ; },submissions create
if map has a null comparator @$ the keys should have a natural <PLACE_HOLDER> @$ even though k does n't explicitly implement comparable .,if ( comparator == null ) { comparator = ( comparator < ? super k > ) natural_order ; },keys have
when a runtime exception gets thrown out @$ this provider will get called again if the object is asked for again . this will have the same failed result @$ 'cause when it 's called no parameters will have actually changed . guice will then report the same <PLACE_HOLDER> multiple times @$ which is pretty annoying . cache a null supplier and return that,ret val = suppliers . of instance ( null ) ; throw e ;,times report
buffer wo n't contain the <PLACE_HOLDER> if the caller does n't have permission .,return - __num__ ;,buffer contain
always a good idea to set up default <PLACE_HOLDER> ...,blur shader . use ( ) ; blur shader . set uniformf ( __str__ @$ __num__ @$ __num__ ) ;,idea set
getting new <PLACE_HOLDER> will again acknowledge the previously acknowledged <PLACE_HOLDER> but this is ok,buffer . get ( first @$ __num__ @$ size of pages ( __num__ ) ) . cancel ( true ) ; assert queue state ( buffer @$ first @$ __num__ @$ __num__ ) ;,pages acknowledge
failure during parameter serialization ; call may have reached <PLACE_HOLDER> @$ so call retry not possible .,throw e ;,call reached
let the main thread know and exit it may be that this exception comes because the main thread closed the <PLACE_HOLDER> @$ in which case the below reporting is irrelevant @$ but does not hurt either,handover . report error ( t ) ;,thread closed
now remove first region in table t 2 to see if catalogjanitor scan <PLACE_HOLDER> .,list < region info > t2 ris = meta table accessor . get table regions ( test_util . get connection ( ) @$ t2 ) ; meta table accessor . delete region info ( test_util . get connection ( ) @$ t2 ris . get ( __num__ ) ) ; gc = janitor . scan ( ) ; report = janitor . get last report ( ) ; assert false ( report . is empty ( ) ) ; assert equals ( __num__ @$ report . get holes ( ) . size ( ) ) ; assert true ( report . get holes ( ) . get ( __num__ ) . get first ( ) . get table ( ) . equals ( t1 ) ) ; assert,catalogjanitor scan
let 's get child <PLACE_HOLDER>,if ( cce . node count ( ) > __num__ ) { add loopx path ( cnode @$ monitor ) ; },"s" get
each test will populate the <PLACE_HOLDER> as needed,test visual graph g = new test visual graph ( ) ; return g ;,test populate
we call parse with 1 to get only one . this also means if we change the implementation to use 2 @$ it would use every other <PLACE_HOLDER> and so on . not that it is really useful @$ but a person could use it that way if they have a huge gigabyte log file and they only want to use a quarter of,int this count = parser . parse and configure ( __num__ @$ this ) ; if ( this count < __num__ ) { return error result ( new error ( __str__ ) @$ new http sample result ( ) ) ; },person use
log if the installed pkg has a higher version <PLACE_HOLDER> .,if ( existing pkg info != null ) { if ( existing pkg info . get long version code ( ) == pkg . get long version code ( ) ) { if ( skip if same version ) { log . w ( tag @$ __str__ + pkg . get long version code ( ) + __str__ + package name + __str__ ) ; return ; } else { log . w ( tag @$ __str__ + pkg . get long version code ( ) + __str__ + package name ) ; } } else if ( existing pkg info . get long version code ( ) > pkg . get long version code ( ) ) { if ( skip if lower version ) { log,pkg has
we explicitly test <PLACE_HOLDER> against one here @$ saving a division at the cost of a test and branch . is it worth it ?,float s = ( norm == __num__ ) ? __num__ : ( norm > __num__ ) ? __num__ / norm : __num__ ;,here test
check if this queue need more resource @$ simply skip allocation if this queue does n't need more <PLACE_HOLDER> .,if ( ! has pending resource request ( candidates . get partition ( ) @$ cluster resource @$ scheduling mode ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + get queue path ( ) + __str__ + scheduling mode . name ( ) + __str__ + candidates . get partition ( ) ) ; } activities logger . queue . record queue activity ( activities manager @$ node @$ get parent ( ) . get queue name ( ) @$ get queue name ( ) @$ activity state . skipped @$ activity diagnostic constant . queue_do_not_need_more_resource ) ; return cs assignment . null_assignment ; } map < string @$ cached user limit > user limits = new,queue need
get ast allocate result <PLACE_HOLDER> ; note that keeping ts and bounds separate helps with threads,type [ ] ts = new type [ fts . length ] ;,ast allocate
this map will extract pegasus options.generation <PLACE_HOLDER> to project property,project . get extensions ( ) . get extra properties ( ) . set ( __str__ @$ arrays . stream ( pegasus options . generation mode . values ( ) ) . collect ( collectors . to map ( pegasus options . generation mode :: name @$ function . identity ( ) ) ) ) ; synchronized ( static_project_evaluated_lock ) { if ( ! project . get root project ( ) . has property ( run_once ) || ! boolean . parse boolean ( string . value of ( project . get root project ( ) . property ( run_once ) ) ) ) { project . get gradle ( ) . projects evaluated ( gradle -> gradle . get root project ( ) . subprojects ( subproject,map extract
ok we have a real file get the local <PLACE_HOLDER>,this . gpgexe = kettlevfs . get filename ( file ) ; try { if ( file != null ) { file . close ( ) ; } } catch ( exception e ) { },file get
init the robot to drive to view which triggers <PLACE_HOLDER> on the presenter,home presenter presenter = new dependency injection ( ) . new home presenter ( ) ;,which triggers
pairwise : init ouput col has <PLACE_HOLDER> @$ init ouput col is repeating @$ column 1 has <PLACE_HOLDER> @$ column 1 is repeating @$ column 2 has <PLACE_HOLDER> @$ column 2 is repeating,for ( boolean [ ] test matrix : new boolean [ ] [ ] { { true @$ true @$ false @$ true @$ true @$ true } @$ { false @$ false @$ true @$ false @$ false @$ false } @$ { true @$ false @$ true @$ false @$ true @$ true } @$ { true @$ true @$ true @$ true @$ false @$ false } @$ { false @$ false @$ false @$ true @$ true @$ false } @$ { false @$ true @$ false @$ false @$ false @$ true } } ) { string test case = template string ; test case = test case . replace all ( __str__ @$ __str__ + vector exp class name + create null,column has
the aq listner should get expected <PLACE_HOLDER> .,await ( ) . until asserted ( ( ) -> assert equals ( __num__ @$ events . size ( ) ) ) ;,listner get
user specified the <PLACE_HOLDER> for local mode hadoop run,console . print info ( __str__ + hadoop mem + __str__ ) ; variables . put ( hadoop_mem_key @$ string . value of ( hadoop mem ) ) ;,user specified
test that offering another element preserves the priority queue <PLACE_HOLDER> .,assert offer ( queue @$ e ) ; assert equals ( __num__ @$ queue . size ( ) ) ; assert same ( a @$ queue . peek ( ) ) ; assert same ( a @$ queue . poll ( ) ) ; assert equals ( __num__ @$ queue . size ( ) ) ;,priority queue
this request has a bad <PLACE_HOLDER> @$ so we are dismissing it early .,if ( err != keeper exception . code . ok . int value ( ) ) { dec in process ( ) ; reply header rh = new reply header ( request . cxid @$ __num__ @$ err ) ; try { request . cnxn . send response ( rh @$ null @$ null ) ; } catch ( io exception e ) { log . error ( __str__ @$ e ) ; } },request has
we must create a new <PLACE_HOLDER> @$ otherwise the subclass serializer cache can create concurrency problems,return new pojo serializer < > ( clazz @$ duplicate field serializers @$ fields @$ execution config ) ;,cache create
update <PLACE_HOLDER> pointer in node after the new <PLACE_HOLDER> has points the older <PLACE_HOLDER> so that old <PLACE_HOLDER> can be accessed concurrently,skip list utils . put value pointer ( node segment @$ offset in node segment @$ value pointer ) ; return old value pointer ;,value points
user 2 joins the new <PLACE_HOLDER>,try { multi user chat muc2 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc2 . join ( __str__ ) ; multi user chat muc3 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc3 . join ( __str__ ) ; muc . grant ownership ( get barejid ( __num__ ) ) ; muc . grant moderator ( __str__ ) ; collection < affiliate > affiliates = muc . get owners ( ) ; assert equals ( __str__ @$ __num__ @$ affiliates . size ( ) ) ; for ( affiliate affiliate1 : affiliates ) { if ( get barejid ( __num__ ) . equals ( affiliate1 . get jid ( ) ) ) {,user joins
finish stream calls transport report <PLACE_HOLDER> .,client stream . transport state ( ) . transport report status ( status . unavailable @$ false @$ new metadata ( ) ) ; argument captor < status > status captor = argument captor . for class ( status . class ) ; verify ( client listener ) . closed ( status captor . capture ( ) @$ isa ( rpc progress . class ) @$ isa ( metadata . class ) ) ; status status = status captor . get value ( ) ; assert equals ( status . unavailable . get code ( ) @$ status . get code ( ) ) ;,stream calls
the file only has one <PLACE_HOLDER>,located block lblock = locatedblocks . get ( __num__ ) ; datanode info [ ] datanodeinfos = lblock . get locations ( ) ; extended block block = lblock . get block ( ) ;,file has
dismiss the grouped notification if a user dismisses all <PLACE_HOLDER> from a wear device,if ( ! mgcm message handler . has notifications ( ) ) { notification manager compat notification manager = notification manager compat . from ( m context ) ; notification manager . cancel ( group_notification_id ) ; },user dismisses
verify owner has active tx <PLACE_HOLDER>,d lock batch [ ] batches = grantor . get lock batches ( owner ) ; if ( batches . length == __num__ ) { logger . debug ( __str__ @$ owner ) ; return ; } send recovery msgs ( dlock . get distribution manager ( ) @$ batches @$ owner @$ grantor ) ;,owner has
visibility @$ like all package groups @$ does n't have a <PLACE_HOLDER>,nested set < package group contents > visibility = convert visibility ( prerequisite map @$ analysis environment . get event handler ( ) @$ target @$ null ) ; if ( target instanceof output file ) { output file output file = ( output file ) target ; target context target context = new target context ( analysis environment @$ target @$ config @$ prerequisite map . get ( dependency resolver . output_file_rule_dependency ) @$ visibility ) ; if ( analysis environment . get skyframe env ( ) . values missing ( ) ) { return null ; } rule configured target rule = ( rule configured target ) target context . find direct prerequisite ( output file . get generating rule ( ) . get label (,visibility have
skip gc 'd weak <PLACE_HOLDER>,for ( hash entry < k @$ v > p = e ; p != last run ; p = p . next ) { k key = p . key ref . get ( ) ; if ( key == null ) { reduce ++ ; continue ; } int k = p . hash & size mask ; hash entry < k @$ v > n = new table [ k ] ; new table [ k ] = new hash entry < k @$ v > ( key @$ p . hash @$ n @$ p . value @$ ref queue ) ; },gc "d"
puts @$ gets @$ hits @$ <PLACE_HOLDER> @$ <PLACE_HOLDER> .,@ suppress warnings ( __str__ ) atomic integer [ ] counts = { in . caches . put count @$ is ppd ? in . caches . get by expr count : in . caches . get count @$ is ppd ? in . caches . get hit by expr count : in . caches . get hit count @$ is ppd ? in . caches . get count : in . caches . get by expr count @$ is ppd ? in . caches . get hit count : in . caches . get hit by expr count } ; verify splits ( original hs @$ splits ) ; verify call counts ( counts @$ __num__ @$ __num__ @$ __num__ ) ; assert equals ( __num__ @$,puts hits
we need to overwrite the default scan <PLACE_HOLDER> @$ which does not expand views . the expanding scan <PLACE_HOLDER> uses the flink planner to translate a view into a rel tree @$ before applying any subsequent rules .,context chain = contexts . chain ( context @$ contexts . of ( rel factories . expanding scan factory ( create flink planner ( current catalog @$ current database ) @$ rel factories . default_table_scan_factory ) ) ) ;,default scan
the url has been seen before . notify <PLACE_HOLDER> with the new distance estimate .,if ( m nearby urls . contains ( url info . get url ( ) ) ) { if ( url info . get distance ( ) >= __num__ && m pws result map . contains key ( url info . get url ( ) ) ) { safe notify native listeners on distance changed ( url info . get url ( ) @$ url info . get distance ( ) ) ; } return ; },url notify
it 's fine to just check departure time @$ as the above pass ensures that all stop times have either both <PLACE_HOLDER> and departure times @$ or neither,if ( stop times [ __num__ ] . departure_time == entity . int_missing || stop times [ stop times . length - __num__ ] . departure_time == entity . int_missing ) { throw new first and last stops do not have times ( ) ; },times have
the linker matches <PLACE_HOLDER> to bindings by their type .,@ suppress warnings ( __str__ ) binding < t > binding = ( binding < t > ) get injectable type binding ( class loader @$ injectable type key @$ key ) ;,linker matches
now let 's allow <PLACE_HOLDER> for foot,foot encoder . set block fords ( boolean . false ) ; node = new reader node ( __num__ @$ - __num__ @$ - __num__ ) ; node . set tag ( __str__ @$ __str__ ) ; assert true ( foot encoder . handle node tags ( node ) == __num__ ) ; node = new reader node ( __num__ @$ - __num__ @$ - __num__ ) ; node . set tag ( __str__ @$ __str__ ) ; assert true ( foot encoder . handle node tags ( node ) == __num__ ) ;,"s" allow
the semantic graph does n't explicitly encode the root <PLACE_HOLDER> @$ so we print that out ourselves,for ( indexed word root : graph . get roots ( ) ) { string rel = grammatical relation . root . get long name ( ) ; rel = rel . replace all ( __str__ @$ __str__ ) ; int source = __num__ ; int target = root . index ( ) ; string source word = __str__ ; string target word = tokens . get ( target - __num__ ) . word ( ) ; final boolean is extra = false ; add dependency info ( dep info @$ rel @$ is extra @$ source @$ source word @$ null @$ target @$ target word @$ null @$ curns ) ; },graph encode
the following code uses an aes <PLACE_HOLDER> to encrypt the message .,final buffered block cipher cipher = new padded buffered block cipher ( new cbc block cipher ( new aes fast engine ( ) ) ) ; cipher . init ( true @$ key ) ; final byte [ ] encrypted bytes = new byte [ cipher . get output size ( plain text as bytes . length ) ] ; final int process len = cipher . process bytes ( plain text as bytes @$ __num__ @$ plain text as bytes . length @$ encrypted bytes @$ __num__ ) ; final int do final len = cipher . do final ( encrypted bytes @$ process len ) ;,code uses
this creates the ephemeral sequential node with host id 0 <PLACE_HOLDER> this node already used for itself . just recording that fact .,final int selected host id = select new host id ( m_config . coordinator ip . to string ( ) ) ; if ( selected host id != __num__ ) { org . voltdb . voltdb . crash local voltdb ( __str__ + selected host id @$ false @$ null ) ; },node used
copied from catalog iterable @$ this is what interprets user <PLACE_HOLDER> for install,version gv = get local registry ( ) . get graal version ( ) ; version . match selector = gv . match ( version . match . type . mostrecent ) ; list < component info > infos ;,what interprets
if baseline provider returns both current <PLACE_HOLDER> and baseline <PLACE_HOLDER> @$ using them as the result,if ( df baseline . contains ( col_current ) ) { df aligned = df baseline ; df aligned . rename series ( col_value @$ col_baseline ) ; } else { data frame df current = this . time series loader . load ( slice view current ) ; df aligned = df current . rename series ( col_value @$ col_current ) . join outer ( df baseline . rename series ( col_value @$ col_baseline ) ) ; },provider returns
sometimes the network connection notifier lags the actual connection <PLACE_HOLDER> @$ especially when the gcm nm wakes us from doze state . if we are really connected @$ report the connection <PLACE_HOLDER> from android .,if ( connection type == connection type . connection_none ) { connectivity manager cm = ( connectivity manager ) context . get system service ( context . connectivity_service ) ; network info active network = cm . get active network info ( ) ; boolean is connected = active network != null && active network . is connected or connecting ( ) ; if ( is connected ) { connection type = convert android network type to connection type ( active network . get type ( ) ) ; } },type report
otherwise @$ checking enbling it according to group divider enabled <PLACE_HOLDER> .,if ( m group divider != null ) { m group divider . set visibility ( ! m has list divider && group divider enabled ? view . visible : view . gone ) ; },checking enabled
empty list since the parser will only use <PLACE_HOLDER> 1 as <PLACE_HOLDER>s .,list < d res < s bool > > in2 = new array list < > ( ) ; bristol circuit parser parser = bristol circuit parser . read circuit description ( __str__ @$ input @$ in2 ) ; return builder . seq ( parser ) ;,list use
this test requires two task <PLACE_HOLDER> @$ but uberization overrides max to 1,conf . set boolean ( mr job config . job_ubertask_enable @$ false ) ; job job = app . submit ( conf ) ; app . wait for state ( job @$ job state . succeeded ) ; map < task id @$ task > tasks = job . get tasks ( ) ; assert . assert equals ( __str__ @$ __num__ @$ tasks . size ( ) ) ; task task = tasks . values ( ) . iterator ( ) . next ( ) ; assert . assert equals ( __str__ @$ task state . succeeded @$ task . get report ( ) . get task state ( ) ) ; map < task attempt id @$ task attempt > attempts = tasks . values (,test requires
complete the task on the same level should not trigger the <PLACE_HOLDER> of the user event listener,cmmn task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; assert that ( cmmn runtime service . create user event listener instance query ( ) . case instance id ( case instance . get id ( ) ) . count ( ) ) . is equal to ( __num__ ) ;,task trigger
remove duplicates ca n't use <PLACE_HOLDER> since the base classes are different,ans collection . add all ( tmp answer ) ;,duplicates use
note to translators : the stylesheet tried to create an attribute with a <PLACE_HOLDER> that was not a valid xml <PLACE_HOLDER> . the substitution text contains the <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text contains
since the api request specified a unique playlist <PLACE_HOLDER> @$ the api response should return exactly one playlist . if the response does not contain a playlist @$ then the specified playlist <PLACE_HOLDER> was not found .,list < playlist > playlist list = playlist list response . get items ( ) ; if ( playlist list . is empty ( ) ) { system . out . println ( __str__ + playlist id ) ; return ; } playlist playlist = playlist list . get ( __num__ ) ;,request specified
split does not affect consumed <PLACE_HOLDER> ; consumed <PLACE_HOLDER> is still 0 .,assert equals ( __num__ @$ consumed parallelism from progress ( iter . get progress ( ) ) @$ __num__ ) ;,split affect
delegate already handles pk <PLACE_HOLDER>,if ( get factory ( ) . get session factory options ( ) . is comments enabled ( ) ) { insert . set comment ( __str__ + get entity name ( ) ) ; } return insert . to statement string ( ) ;,delegate handles
when instance registers second <PLACE_HOLDER>,instance id refresh id = registry . register ( registration . create ( __str__ @$ __str__ ) . build ( ) ) . block ( ) ; assert that ( refresh id ) . is equal to ( id ) ; step verifier . create ( registry . get instance ( id ) ) . assert next ( ( registered ) -> { assert that ( registered . get info ( ) ) . is equal to ( info ) ; assert that ( registered . get status info ( ) ) . is equal to ( status ) ; } ) . verify complete ( ) ;,instance registers
check whether we got group created <PLACE_HOLDER>,assert equals ( __str__ @$ __num__ @$ group change collector . collected events . size ( ) ) ; assert equals ( __str__ @$ test group name2 @$ ( ( server stored group event ) group change collector . collected events . get ( __num__ ) ) . get source group ( ) . get group name ( ) ) ;,group created
start at 1 @$ since a miss in the open object int hash map returns a <PLACE_HOLDER>,int i = __num__ ;,map returns
class file to write @$ if directory then use the <PLACE_HOLDER> of the input,path output = paths . get ( args [ __num__ ] ) ; if ( files . is directory ( output ) ) output = output . resolve ( input . get file name ( ) ) ;,directory use
if the requestable capabilities have n't changed @$ and the score has n't changed @$ then the change we 're processing ca n't affect any requests @$ it can only affect the listens on this network . we might have been called by rematch network and requests when a network changed foreground <PLACE_HOLDER> .,if ( nai . get current score ( ) == old score && new nc . equal requestable capabilities ( prev nc ) ) { process listen requests ( nai ) ; } else { rematch all networks and requests ( ) ; notify network callbacks ( nai @$ connectivity manager . callback_cap_changed ) ; },network changed
default lists namenode <PLACE_HOLDER> only,jmx . init ( ) ;,default lists
client reads <PLACE_HOLDER>,assert equals ( __num__ @$ client . get input stream ( ) . read ( ) ) ;,client reads
transition to standby will set rm 's ha <PLACE_HOLDER> and then reinitialize in a separate thread . despite asserting for standby state @$ it 's possible for reinitialization to be unfinished . wait here for it to finish @$ otherwise closing rm 1 will close zk manager and the unfinished reinitialization will throw an exception .,thread . sleep ( __num__ ) ; rm1 . close ( ) ; rm2 . close ( ) ;,transition set
if the request indicates a <PLACE_HOLDER> that is greater than the current <PLACE_HOLDER> then assign that <PLACE_HOLDER> and leader to the current context .,boolean transition = update term and leader ( request . term ( ) @$ null ) ; completable future < vote response > future = completable future . completed future ( log response ( handle vote ( request ) ) ) ; if ( transition ) { raft . transition ( raft server . role . follower ) ; } return future ;,request indicates
static puts cause an <PLACE_HOLDER> to escape globally,esc set . escape = escape state . global ; break ; case reg ops . invoke_static : case reg ops . invoke_virtual : case reg ops . invoke_super : case reg ops . invoke_direct : case reg ops . invoke_interface : case reg ops . return : case reg ops . throw :,puts cause
do not invoke class descriptor write <PLACE_HOLDER> with old protocol,if ( protocol == protocol_version_1 ) { desc . write non proxy ( this ) ; } else { write class descriptor ( desc ) ; },descriptor write
process persist <PLACE_HOLDER> .,for ( long file id : m persist requests . key set ( ) ) { if ( thread . interrupted ( ) ) { throw new interrupted exception ( __str__ ) ; } boolean remove = true ; alluxio . time . exponential timer timer = m persist requests . get ( file id ) ; if ( timer == null ) { continue ; } alluxio . time . exponential timer . result timer result = timer . tick ( ) ; if ( timer result == alluxio . time . exponential timer . result . not_ready ) { continue ; } alluxiouri uri = null ; try { try ( locked inode path inode path = m inode tree . lock full inode path (,process persist
worker is still running this <PLACE_HOLDER>,task location task location = task runner work item . get location ( ) ; final url url = task runner utils . make task locationurl ( task location @$ __str__ @$ task id ) ; return optional . of ( new byte source ( ) { @ override public input stream open stream ( ) throws io exception { try { return http client . go ( new request ( http method . get @$ url ) @$ new input stream response handler ( ) ) . get ( ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; } catch ( execution exception e ) { throwables . propagate if possible ( e . get cause ( ),worker running
check that both animators ' listeners have received the animation <PLACE_HOLDER> .,assert true ( l1 . start called ) ; assert true ( l1 . end called ) ; assert false ( a1 . is started ( ) ) ; assert true ( l1 . end time >= l1 . start time ) ; assert true ( l2 . start called ) ; assert true ( l2 . end called ) ; assert false ( a2 . is started ( ) ) ; assert true ( l2 . end time >= l1 . start time ) ;,listeners received
invalid port number yields an <PLACE_HOLDER> .,check error state ( new ignite callable < void > ( ) { @ override public void call ( ) throws exception { driver manager . get connection ( __str__ ) ; return null ; } } @$ __str__ @$ __str__ ) ;,number yields
if a query has <PLACE_HOLDER> in clause and each in clause has <PLACE_HOLDER> values @$ then it will easily generate thousands of fuzzy keys . when there are lots of fuzzy keys @$ the scan performance is bottle necked on it . so simply choose to abandon all fuzzy keys in this case .,if ( exceed cap ( dims @$ cap ) ) { return lists . new array list ( ) ; } else { return combination ( dims ) ; },query has
name of the attribute containing the <PLACE_HOLDER> roles . if not specified @$ this defaults to roles .,module options . put ( __str__ @$ __str__ ) ;,name containing
compile internal will automatically reset the perf <PLACE_HOLDER>,if ( ! already compiled ) { compile internal ( command @$ true ) ; } else { driver context . get plan ( ) . set query start time ( driver context . get query display ( ) . get query start time ( ) ) ; },internal reset
at this point the clone is complete . next step is enabling the <PLACE_HOLDER> .,string msg = __str__ + snapshot . get name ( ) + __str__ + table name + __str__ ; log . info ( msg ) ; monitor status . set status ( msg + __str__ ) ;,step enabling
housekeeper should sweep these abandoned <PLACE_HOLDER>,house keeper . abandoned subscriptions . add ( con client id ) ;,housekeeper sweep
historic rules may contain null <PLACE_HOLDER> when original zoneinfo data includes non transition data .,if ( historic rules != null ) { for ( int i = __num__ ; i < historic rules . length ; i ++ ) { if ( historic rules [ i ] != null ) { size ++ ; } } },rules contain
xshift and yshift represent the <PLACE_HOLDER> & direction to shift the tab in their respective axis .,for ( int j = start + __num__ ; j <= end ; j ++ ) { int xshift = __num__ ; int yshift = __num__ ; switch ( tab pane . get tab placement ( ) ) { case j tabbed pane . top : case j tabbed pane . bottom : xshift = ltr ? tab overlap : - tab overlap ; break ; case j tabbed pane . left : case j tabbed pane . right : yshift = tab overlap ; break ; default : } rects [ j ] . x += xshift ; rects [ j ] . y += yshift ; rects [ j ] . width += math . abs ( xshift ) ; rects [ j ] . height,xshift represent
4 historic form properties should be created . <PLACE_HOLDER> when process started @$ <PLACE_HOLDER> when task completed,list < historic detail > props = history service . create historic detail query ( ) . form properties ( ) . process instance id ( process instance . get id ( ) ) . order by form property id ( ) . asc ( ) . list ( ) ; historic form property historic property1 = ( historic form property ) props . get ( __num__ ) ; assert equals ( __str__ @$ historic property1 . get property id ( ) ) ; assert equals ( __str__ @$ historic property1 . get property value ( ) ) ; assert equals ( started date @$ historic property1 . get time ( ) ) ; assert equals ( process instance . get id ( ) @$ historic property1 .,process started
if we 're becoming visible @$ immediately change client visibility as well . there seem to be some edge cases where we change our visibility but client visibility never gets <PLACE_HOLDER> . if we 're becoming invisible @$ update the client visibility if we are not running an animation . otherwise @$ we 'll update client visibility in on animation finished .,if ( visible || ! is really animating ( ) ) { set client hidden ( ! visible ) ; } if ( ! get display content ( ) . m closing apps . contains ( this ) && ! get display content ( ) . m opening apps . contains ( this ) ) { get display content ( ) . get docked divider controller ( ) . notify app visibility changed ( ) ; m wm service . m task snapshot controller . notify app visibility changed ( this @$ visible ) ; },visibility gets
inline editor is n't supported but <PLACE_HOLDER> viewer is enable <PLACE_HOLDER>,if ( array utils . contains ( supported edit types @$ i value controller . edit type . panel ) ) { controller . activate panel ( value viewer panel . panel_id @$ true @$ true ) ; return null ; },viewer enable
the number of got <PLACE_HOLDER>,int got = skipped + cis . read ( result @$ __num__ @$ __num__ ) ;,number got
can this left constituent leave <PLACE_HOLDER> for a right constituent ?,boolean i possiblel = ( narrowr < end ) ;,constituent leave
check if any of the references cross the module <PLACE_HOLDER> .,if ( check modules && ref . module != null ) { if ( ref . module != fn module && ! module graph . depends on ( ref . module @$ fn module ) ) { is removable = false ; check modules = false ; } },any cross
get a <PLACE_HOLDER> @$ if collection has only one <PLACE_HOLDER>,string guava = iterables . get only element ( iterable ) ;,collection has
cleanup session log <PLACE_HOLDER> .,cleanup session log dir ( ) ; hive history hive hist = session state . get hive history ( ) ; if ( null != hive hist ) { hive hist . close stream ( ) ; } try { session state . reset thread name ( ) ; session state . close ( ) ; } finally { session state = null ; } if ( session state != null ) { try { session state . reset thread name ( ) ; session state . close ( ) ; } catch ( throwable t ) { log . warn ( __str__ @$ t ) ; } session state = null ; } if ( session hive != null ) { try { session hive . close,session log
let evaluation prints stats how often the right output had the highest <PLACE_HOLDER>,evaluation eval = new evaluation ( ) ; eval . eval ( ds . get labels ( ) @$ output ) ; system . out . println ( eval . stats ( ) ) ;,output had
if the scan matches all <PLACE_HOLDER> @$ we can throw away the scan nodes and use a truncate delete node .,if ( delete is truncate ( m_parsed delete @$ sub select root ) ) { delete node . set truncate ( true ) ; } else { if ( m_parsed delete . order by columns ( ) . size ( ) > __num__ && ! is single partition plan && ! target table . get isreplicated ( ) ) { throw new planning error exception ( __str__ + __str__ + __str__ + __str__ ) ; } boolean needs order by node = is order by node required ( m_parsed delete @$ sub select root ) ; abstract expression address expr = new tuple address expression ( ) ; node schema proj_schema = new node schema ( ) ; proj_schema . add column ( abstract parsed stmt . temp_table_name,scan matches
check whether the server returned the status <PLACE_HOLDER> that we 've set .,assert equals ( __str__ @$ __num__ @$ status event collector . collected stat msg events . size ( ) ) ; assert equals ( __str__ @$ old status message @$ ( ( property change event ) status event collector . collected stat msg events . get ( __num__ ) ) . get old value ( ) ) ; assert equals ( __str__ @$ new status message @$ ( ( property change event ) status event collector . collected stat msg events . get ( __num__ ) ) . get new value ( ) ) ;,server returned
fallback uses the timeout <PLACE_HOLDER>,assert true ( results . observe on thread . get ( ) . get name ( ) . starts with ( __str__ ) ) ;,fallback uses
note : each event loop has its own connection <PLACE_HOLDER> .,factory with pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( true ) . build ( ) ; factory without pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( false ) . build ( ) ;,loop has
this is technically a recursive constraint for cast @$ but type registry.can cast has explicit <PLACE_HOLDER> for row to row cast,super ( cast @$ immutable list . of ( new type variable constraint ( __str__ @$ false @$ false @$ __str__ @$ immutable set . of ( new type signature ( __str__ ) ) @$ immutable set . of ( ) ) @$ with variadic bound ( __str__ @$ __str__ ) ) @$ immutable list . of ( ) @$ new type signature ( __str__ ) @$ immutable list . of ( new type signature ( __str__ ) ) @$ false ) ;,cast has
if this instruction has a delay <PLACE_HOLDER> @$ adjust max addr accordingly,if ( instr . get prototype ( ) . has delay slots ( ) ) { max addr = instr . get min address ( ) . add ( instr . get default fall through offset ( ) - __num__ ) ; } v context . set current instruction ( instr ) ; v context . flow to address ( flow from addr @$ max addr ) ; if ( evaluator != null ) { if ( evaluator . evaluate context before ( v context @$ instr ) ) { body . add ( conflicts ) ; return body ; } },instruction has
makes it more easy to see which release <PLACE_HOLDER> process in thread dump,thread . current thread ( ) . set name ( __str__ + uri . get host ( ) ) ;,which release
package cache holds all <PLACE_HOLDER> @$ try to check if we need to update .,v package exist one = package cache manager . get ( pkg . package name ) ; package setting exist setting = exist one != null ? ( package setting ) exist one . m extras : null ; if ( exist one != null ) { if ( ( flags & install strategy . ignore_new_version ) != __num__ ) { res . is update = true ; return res ; } if ( ! can update ( exist one @$ pkg @$ flags ) ) { return install result . make failure ( __str__ ) ; } res . is update = true ; } file app dir = v environment . get data app package directory ( pkg . package name ) ; file lib dir,cache holds
we must sort i pv 4 and i pv <PLACE_HOLDER> here,set < string > ipv6 = new hash set < > ( ) ; set < string > ipv4 = new hash set < > ( ) ; for ( string ip : ips ) if ( is properip ( ip ) ) { if ( ip . index of ( __str__ ) >= __num__ ) ipv6 . add ( ip ) ; else ipv4 . add ( ip ) ; } if ( ipv4 . size ( ) == __num__ ) { if ( ipv6 . size ( ) == __num__ ) { this . dna . put ( seed . ip @$ ipv6 . iterator ( ) . next ( ) ) ; this . dna . put ( seed . ip6 @$ __str__ ) ;,pv pv
phone numbers must be in the direction of the locale 's <PLACE_HOLDER> . most locales have ltr <PLACE_HOLDER> @$ but some locales @$ such as those written in the adlam or n'ko scripts @$ have rtl <PLACE_HOLDER> .,if ( ( text view . get input type ( ) & editor info . type_mask_class ) == editor info . type_class_phone ) { final decimal format symbols symbols = decimal format symbols . get instance ( text view . get text locale ( ) ) ; final string zero = symbols . get digit strings ( ) [ __num__ ] ; final int first codepoint = zero . code point at ( __num__ ) ; final byte digit direction = character . get directionality ( first codepoint ) ; if ( digit direction == character . directionality_right_to_left || digit direction == character . directionality_right_to_left_arabic ) { return text direction heuristics . rtl ; } else { return text direction heuristics . ltr ; } },locales have
callback might have started an edge <PLACE_HOLDER> .,if ( m drag state == state_dragging ) { break ; },callback started
for a normal <PLACE_HOLDER> @$ we can not ensure that the underlying compression stream is closed @$ so we decompress the full record set here . use cases which call for a lower memory footprint can use ` streaming <PLACE_HOLDER> ` at the cost of additional complexity,try ( closeable iterator < record > iterator = compressed iterator ( buffer supplier . no_caching @$ false ) ) { list < record > records = new array list < > ( count ( ) ) ; while ( iterator . has next ( ) ) records . add ( iterator . next ( ) ) ; return records . iterator ( ) ; },footprint use
<PLACE_HOLDER> set change when new set excludes current <PLACE_HOLDER>,assert . assert equals ( relay puller . get servers ( ) @$ exp server info2 @$ __str__ ) ; do execute and change state ( relay puller @$ create set server message ( false @$ relay puller ) ) ; assert . assert equals ( relay puller . get current server idx ( ) == - __num__ @$ true @$ __str__ ) ; assert . assert equals ( relay puller . get curent server ( ) == null @$ true @$ __str__ ) ; assert . assert equals ( relay puller . get servers ( ) @$ exp server info3 @$ __str__ ) ; assert . assert equals ( relay puller . to tear conn after handling response ( ) @$ false @$ __str__ ) ; assert,set excludes
result will contain the <PLACE_HOLDER> which have to be output,final sorted set < attr > result = this . result ; result . clear ( ) ; if ( element . has attributes ( ) ) { named node map attrs = element . get attributes ( ) ; int attrs length = attrs . get length ( ) ; for ( int i = __num__ ; i < attrs length ; i ++ ) { attr attribute = ( attr ) attrs . item ( i ) ; string n uri = attribute . get namespaceuri ( ) ; string n name = attribute . get local name ( ) ; string n value = attribute . get value ( ) ; if ( ! xmlns_uri . equals ( n uri ) ) { result .,result contain
the <PLACE_HOLDER> of each item in the array is a node object from an array node ... must get the <PLACE_HOLDER> of it .,for ( int ix = __num__ ; ix < list . size ( ) ; ix ++ ) { add object ( value array @$ get value ( get typed value from json node ( list . get ( ix ) ) @$ include type ) ) ; },object get
get jpa managed <PLACE_HOLDER> through the repository,message message = message repository . find one ( __num__ ) ; assert not null ( message ) ; assert equals ( __str__ @$ message . get text ( ) ) ;,jpa managed
if an entry already exists for a dataset @$ add it to the current <PLACE_HOLDER> @$ else create a new <PLACE_HOLDER>,for ( string dataset : iterables . filter ( datasets @$ new dataset predicate ( dataset name element . get as string ( ) ) ) ) { if ( dataset specific config map . contains key ( dataset ) ) { dataset specific config map . get ( dataset ) . add all ( state utils . json object to state ( object @$ dataset ) ) ; } else { dataset specific config map . put ( dataset @$ state utils . json object to state ( object @$ dataset ) ) ; } },else create
make sure processor has <PLACE_HOLDER> to do .,if ( ! is work to do ( ) ) { logger . debug ( __str__ @$ connectable ) ; return invocation result . yield ( __str__ ) ; } if ( num relationships > __num__ ) { final int required number of available relationships = connectable . is trigger when any destination available ( ) ? __num__ : num relationships ; if ( ! repository context . is relationship availability satisfied ( required number of available relationships ) ) { logger . debug ( __str__ @$ connectable ) ; return invocation result . yield ( __str__ ) ; } } logger . debug ( __str__ @$ connectable ) ; final long batch nanos = connectable . get run duration ( time unit . nanoseconds ) ; final,processor has
use a small long here which will only occupy one property <PLACE_HOLDER>,my node . set property ( __str__ @$ small value ) ; tx . commit ( ) ;,which occupy
there are two special cases below @$ because 2 cliques have 2 <PLACE_HOLDER>,c . set domain ( domain ) ; if ( clique == cliquec ) { featuresc ( c info @$ loc @$ c ) ; } else if ( clique == clique cpc ) { features cpc ( c info @$ loc @$ c ) ; features cnc ( c info @$ loc - __num__ @$ c ) ; } else if ( clique == clique cp2c ) { features cp2c ( c info @$ loc @$ c ) ; } else if ( clique == clique cp3c ) { features cp3c ( c info @$ loc @$ c ) ; } else if ( clique == clique cp4c ) { features cp4c ( c info @$ loc @$ c ) ; } else if ( clique ==,cliques have
if the current <PLACE_HOLDER> of the window do n't match the desired <PLACE_HOLDER> @$ then adjust the min width and min height arrays according to the weights .,diffw = parent . width - r . width ; if ( diffw != __num__ ) { weight = __num__ ; for ( i = __num__ ; i < info . width ; i ++ ) weight += info . weightx [ i ] ; if ( weight > __num__ ) { for ( i = __num__ ; i < info . width ; i ++ ) { int dx = ( int ) ( ( ( ( double ) diffw ) * info . weightx [ i ] ) / weight ) ; info . min width [ i ] += dx ; r . width += dx ; if ( info . min width [ i ] < __num__ ) { r . width -= info,dimensions match
key value should allow negative <PLACE_HOLDER> for backwards compat . otherwise @$ if the user already has negative <PLACE_HOLDER> in cluster data @$ h base wo n't be able to handle that,try { new key value ( bytes . to bytes ( __num__ ) @$ bytes . to bytes ( __num__ ) @$ bytes . to bytes ( __num__ ) @$ - __num__ @$ bytes . to bytes ( __num__ ) ) ; } catch ( illegal argument exception ex ) { fail ( __str__ ) ; },user has
the test passed @$ so just <PLACE_HOLDER> from main and harness will interepret this <PLACE_HOLDER> as a pass,return ;,return interepret
check non singleton <PLACE_HOLDER> for types do not eagerly initialize factor <PLACE_HOLDER> when getting bean factory post processor <PLACE_HOLDER>,collection < bean factory post processor > factory post processors = bean factory . get beans of type ( bean factory post processor . class @$ true @$ false ) . values ( ) ; if ( factory post processors . is empty ( ) ) { factory post processors = collections . singleton ( new property placeholder configurer ( ) ) ; } for ( bean factory post processor factory post processor : factory post processors ) { factory post processor . post process bean factory ( bean factory ) ; } abstract engine configuration engine configuration = ( abstract engine configuration ) bean factory . get bean ( bean name ) ; engine configuration . set beans ( new spring bean factory proxy map ( bean,types initialize
the ri starts datagram <PLACE_HOLDER> in broadcast mode .,assert true ( ds . get broadcast ( ) ) ;,ri starts
only log the message if it 's from a recipient we 're waiting for . it could have been multicast to all members @$ which would give us one <PLACE_HOLDER> per member,if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . processor id @$ is present @$ sender @$ this . key @$ serialized @$ version tag ) ; },which give
if we are unable to write the current buffer to socket channel we should break @$ as we might have reached max socket send buffer <PLACE_HOLDER> .,break ;,socket send
clear defined <PLACE_HOLDER> command @$ create defined <PLACE_HOLDER> command @$ create index command @$ define index command @$ destroy index command @$ list index command,create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster read query ) ;,indexes command
no children left . request <PLACE_HOLDER> on the rv itself since one of its children was holding <PLACE_HOLDER> previously .,if ( m child helper . get child count ( ) == __num__ ) { request focus ( ) ; return ; },focus holding
we check that cookie manager returns the <PLACE_HOLDER> for the main domain,url url main domain = new url ( __str__ ) ; cookies = cookie handler . get cookies for url ( man . get cookies ( ) @$ url main domain @$ cookie manager . allow_variable_cookies ) ; assert . assert equals ( __num__ @$ cookies . size ( ) ) ;,manager returns
resort file <PLACE_HOLDER> @$ make i node id not sorted .,for ( int j = __num__ ; j < files . length / __num__ ; j ++ ) { path temp path = files [ j ] ; files [ j ] = files [ files . length - __num__ - j ] ; files [ files . length - __num__ - j ] = temp path ; byte [ ] temp bytes = bytes [ __num__ + j ] ; bytes [ __num__ + j ] = bytes [ files . length - __num__ - j + __num__ ] ; bytes [ files . length - __num__ - j + __num__ ] = temp bytes ; },resort file
resetting max length should not remove maxlength <PLACE_HOLDER>,tf . add focus listener ( event -> { tf . set max length ( __num__ ) ; } ) ;,length remove
rather than create a validated network which complicates <PLACE_HOLDER> by registering it 's own network request during startup @$ just bump up the score to cancel out the unvalidated penalty .,test agent . adjust score ( __num__ ) ; cv = test factory . get network stoppedcv ( ) ;,which complicates
for an incoming message : shall remove and store the <PLACE_HOLDER> envelope @$ including the delimiter . shall pass the remaining data frames to its calling application . shall wait for a single reply message from its calling application . shall prepend the <PLACE_HOLDER> envelope and delimiter . shall deliver this message back to the originating peer .,for ( string bind address : binds ) { envelope ( ctx @$ bind address @$ zmq . zmq_rep @$ zmq . zmq_dealer ) ; },envelope prepend
take in two stringified lists @$ and return true if the first list contains <PLACE_HOLDER> that are not in the second list,bi function < string @$ string @$ boolean > has exclusive elements = ( string a @$ string b ) -> { if ( a == null || a . is empty ( ) ) { return false ; } else if ( b == null || b . is empty ( ) ) { return true ; } set < string > b set = stream . of ( b . split ( element sep ) ) . collect ( collectors . to set ( ) ) ; return ! stream . of ( a . split ( element sep ) ) . filter ( ( x ) -> ! b set . contains ( x ) ) . collect ( collectors . to set ( ),list contains
also check what happens if a value xml file also contains the same <PLACE_HOLDER> .,merged android data actual = unwritten merged android data . of ( source . resolve ( __str__ ) @$ direct @$ parsed android data builder . empty ( ) ) . write ( merged data writer ) ; assert about ( paths ) . that ( actual . get manifest ( ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve ( __str__ ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve ( __str__ ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve (,check contains
do n't add trust <PLACE_HOLDER> if not already present @$ the builder will inherit the <PLACE_HOLDER> from its parent @$ and that 's where the trust <PLACE_HOLDER> should be added .,if ( ! builder . has certificates entry refs ( ) ) { return ; } builder . add certificates entry refs ( debug config builder . get certificates entry refs ( ) ) ;,builder inherit
case 6 : true if an attribute named primitive float att name of type float has the <PLACE_HOLDER> float <PLACE_HOLDER> we cover javax.management.binary rel query exp with a rel op equal to eq and javax.management.numeric <PLACE_HOLDER> exp,queries . add ( query . eq ( query . attr ( primitive float att name ) @$ query . value ( float value ) ) ) ;,name has
use modified portions of do fast scale down <PLACE_HOLDER> here since we do not want to allocate a temporary fast hive decimal object .,long compare0 ; long compare1 ; long compare2 ; int scale down ; if ( left scale < right scale ) { scale down = right scale - left scale ; if ( scale down < longword_decimal_digits ) { final long divide factor = power of ten table [ scale down ] ; final long multiply factor = power of ten table [ longword_decimal_digits - scale down ] ; compare0 = right fast0 / divide factor + ( ( right fast1 % divide factor ) * multiply factor ) ; compare1 = right fast1 / divide factor + ( ( right fast2 % divide factor ) * multiply factor ) ; compare2 = right fast2 / divide factor ; } else if ( scale down < two_x_longword_decimal_digits ),portions scale
the ws loop consumes <PLACE_HOLDER> from the beginning of each line .,while ( true ) { ws loop : while ( true ) { switch ( c ) { case __str__ : case __str__ : c = in . read ( ) ; break ; case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : default : break ws loop ; } } if ( c == __str__ ) { do { c = in . read ( ) ; } while (,loop consumes
apply the offset to the begin <PLACE_HOLDER> to find the absolute <PLACE_HOLDER> .,stats . m last time used = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_active_attr ) ; try { stats . m last time visible = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_visible_attr ) ; } catch ( io exception e ) { log . i ( tag @$ __str__ ) ; } try { stats . m last time foreground service used = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_service_used_attr ) ; } catch ( io exception e ) { log . i ( tag @$ __str__ ) ; } stats . m total time in foreground = xml utils .,offset begin
this method handles row <PLACE_HOLDER> after the first in a multivalued row @$ so just return false,return false ;,method handles
handle provisioning internally ; it 'll reset m prepare <PLACE_HOLDER> in progress,int result = handle provisioninig ( uuid ) ;,m prepare
client reads <PLACE_HOLDER>,buffer util . clear ( buffer ) ; len = c . client . fill ( buffer ) ; assert equals ( __num__ @$ len ) ; assert equals ( __str__ @$ buffer util . to string ( buffer ) ) ;,client reads
as subsequent compare and set <PLACE_HOLDER> will fail .,cluster id before deserialize . compare and set ( no cluster id @$ cluster meta . get ( ) ) ; return data ;,subsequent compare
fast path @$ if the stream can take a <PLACE_HOLDER> directly just write to it,if ( output stream instanceof buffer writable output stream ) { try { ( ( buffer writable output stream ) output stream ) . write ( buffers ) ; return true ; } catch ( io exception e ) { callback . on exception ( exchange @$ this @$ e ) ; return false ; } },stream take
issue 668 : do n't inline singleton getter methods <PLACE_HOLDER> as this confused class removing logic .,if ( convention . get singleton getter class name ( call node ) != null ) { return false ; },issue inline
clear non backed up <PLACE_HOLDER> from expected interval stats,m interval stats . active configuration = null ; m interval stats . configurations . clear ( ) ; m interval stats . events . clear ( ) ;,non backed
user 1 sends the <PLACE_HOLDER> that contains the xhtml to user 2,try { chat1 . send message ( msg ) ; } catch ( exception e ) { fail ( __str__ ) ; } packet packet = chat2 . next result ( __num__ ) ; message message = ( message ) packet ; assert not null ( __str__ @$ message . get body ( ) ) ; try { xhtml extension = ( xhtml extension ) message . get extension ( __str__ @$ __str__ ) ; assert not null ( __str__ @$ xhtml extension ) ; assert true ( __str__ @$ xhtml extension . get bodies count ( ) > __num__ ) ; for ( iterator < string > it = xhtml extension . get bodies ( ) ; it . has next ( ) ; ) { string,user sends
expected since only test role can call that <PLACE_HOLDER>,assert . fail ( __str__ ) ;,role call
provoke an exception when the nl emits a <PLACE_HOLDER> @$ must bubble up and nl must stop,expected exception . expect message ( __str__ ) ; execute ( __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ) ;,nl emits
check that the room 's owner can ban a simple <PLACE_HOLDER>,muc . ban user ( get barejid ( __num__ ) @$ __str__ ) ; thread . sleep ( __num__ ) ; assert null ( __str__ @$ muc . get occupant ( room + __str__ ) ) ; assert false ( __str__ @$ muc2 . is joined ( ) ) ;,owner ban
launcher on a managed profile is referring ot <PLACE_HOLDER> 0 !,run with caller ( launcher_1 @$ user_p0 @$ ( ) -> { m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_2 @$ list ( __str__ @$ __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_3 @$ list ( __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_p0 ) ; } ) ; run with caller ( launcher_1 @$ user_10 @$ ( ) -> { m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_10 ) ;,launcher referring
maybe a read timeout exception @$ as the server does not <PLACE_HOLDER> .,throw new exception ( __str__ @$ ne ) ;,server does
the shard is relocating @$ the <PLACE_HOLDER> it is relocating to will be in initializing state @$ so we do n't count it,if ( shard routing . relocating ( ) ) { compute relocating shards ++ ; },shard relocating
test releasing mms network request does not disconnect main cellular network <PLACE_HOLDER>,m cm . unregister network callback ( network callback ) ; mms network agent . expect disconnected ( ) ; verify active network ( transport_cellular ) ;,request disconnect
check put <PLACE_HOLDER> .,boolean failed = false ; try { dflt cache . put ( __num__ @$ __num__ ) ; } catch ( cache exception e ) { failed = true ; check and wait ( e ) ; } assert true ( failed ) ; dflt cache . put ( __num__ @$ __num__ ) ; return true ; assert true ( ( boolean ) o ) ; assert equals ( __num__ @$ dflt cache . get ( __num__ ) ) ; return true ;,check put
this point is only reached if the operation failed <PLACE_HOLDER> than the allowed retry count,throw new io exception ( __str__ + m upload id + __str__ + m key @$ last exception ) ;,operation failed
note that the only current caller of this method checks first to see if there is a path before calling this method . it is not clear <PLACE_HOLDER> the return value should be here .,return null ;,value be
code browser expands <PLACE_HOLDER> to the code unit .,expected addresses . add ( addr ( __num__ ) @$ addr ( __num__ ) ) ; expected addresses . add ( addr ( __num__ ) @$ addr ( __num__ ) ) ; program selection expected selection = new program selection ( expected addresses ) ; perform action ( select all flows from action @$ get action context ( ) @$ true ) ; program selection current selection = code browser plugin . get current selection ( ) ; assert equals ( new my selection ( expected selection ) @$ new my selection ( current selection ) ) ;,browser expands
last valid y coordinate which did n't have a missing <PLACE_HOLDER>,int last = - __num__ ;,which have
let the visitor visit the <PLACE_HOLDER> referenced in the constant element value .,enum constant element value . referenced classes accept ( class visitor ) ;,visitor visit
this layout has red @$ blue @$ and green <PLACE_HOLDER> from the top .,enter scene ( r . layout . scene3 ) ;,layout has
when the animation completes @$ adjust the relative layout params of the recycler to make sure the bar does n't overlap the bottom <PLACE_HOLDER> when showing,long ms delay = ani utils . duration . short . to millis ( this ) ; m handler . post delayed ( new runnable ( ) { @ override public void run ( ) { if ( ! is finishing ( ) ) { relative layout . layout params params = ( relative layout . layout params ) m recycler . get layout params ( ) ; if ( show ) { params . add rule ( relative layout . above @$ r . id . container_selection_bar ) ; } else { params . add rule ( relative layout . above @$ __num__ ) ; } } } } @$ ms delay ) ;,bar overlap
retry with the old formatter which uses synchronization <PLACE_HOLDER>,try { parsed = generic formatter . short_day_formatter . parse ( date str @$ __num__ ) . get time ( ) ; } catch ( final parse exception pe ) { parsed = new date ( ) ; },which uses
stop automatically reconnecting to this <PLACE_HOLDER> in the future . automatically connecting to a <PLACE_HOLDER> that provides no or limited connectivity is not useful @$ because the user can not use that <PLACE_HOLDER> except through the notification shown by this method @$ and the notification is only shown if the <PLACE_HOLDER> is explicitly selected by the user .,nai . async channel . send message ( network agent . cmd_prevent_automatic_reconnect ) ;,user use
should use root entity <PLACE_HOLDER> by default,criteria executor criteria executor aliased1 = new criteria executor ( ) { protected criteria get criteria ( session s ) { return s . create criteria ( student . class @$ __str__ ) . create criteria ( __str__ @$ __str__ @$ criteria . left_join ) . add order ( order . asc ( __str__ ) ) ; } } ;,use root
no param constructors do not require js <PLACE_HOLDER>,test same ( __str__ ) ;,constructors require
additionally @$ this condition reduces computation <PLACE_HOLDER>,if ( ! m answer sounds added ) { string answer sound source = remove front side audio ( answer ) ; m sound player . add sounds ( m base url @$ answer sound source @$ sound . sounds_answer ) ; m answer sounds added = true ; },condition reduces
and @$ ime dialogs should always have an higher <PLACE_HOLDER> than the ime .,assert window higher ( m ime dialog window @$ m ime window ) ;,dialogs have
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( test admin mode from command line . class ) ;,suite using
this is what always should happen . a sort sets the items <PLACE_HOLDER> by <PLACE_HOLDER> @$ temporarily breaking the uniqueness requirement .,if ( dupl == __num__ ) { duplicates . remove ( e ) ; } else { duplicates . put ( e @$ dupl - __num__ ) ; },sort sets
explicit <PLACE_HOLDER> provider implies <PLACE_HOLDER>,config . get ( key_authorizer ) . as string ( ) . if present ( value -> { if ( ! config . get ( key_authorize ) . exists ( ) ) { builder . authorize ( true ) ; } } ) ; return builder . build ( ) ;,provider implies
this leads to missing the after command profiles of the other modules in the profile . since the bep currently shuts down at the build complete event @$ we can not just move posting the build tool <PLACE_HOLDER> to after command of this module .,if ( profile path != null ) { try { profiler . instance ( ) . stop ( ) ; event . get result ( ) . get build tool log collection ( ) . add local file ( profile path . get base name ( ) @$ profile path ) ; } catch ( io exception e ) { reporter . handle ( event . error ( __str__ + e . get message ( ) ) ) ; } },the build
higher priority @$ so accept new work <PLACE_HOLDER>,assert . assert true ( add files ( list @$ __str__ @$ __num__ ) ) ; assert . assert equals ( list . get work units ( ) . size ( ) @$ __num__ ) ;,priority accept
block any external content resolving actions since we do n't need them and a report says these actions may cause security <PLACE_HOLDER> .,document builder . set entity resolver ( new entity resolver ( ) { @ override public input source resolve entity ( string public id @$ string system id ) throws sax exception @$ io exception { return new input source ( ) ; } } ) ;,actions cause
if the query h<PLACE_HOLDER>s <PLACE_HOLDER> h<PLACE_HOLDER>ving cl<PLACE_HOLDER>use <PLACE_HOLDER>nd both h 2 <PLACE_HOLDER>nd pinot h<PLACE_HOLDER>ve no groups @$ th<PLACE_HOLDER>t is expected @$ so we do n't need to comp<PLACE_HOLDER>re the number of docs sc<PLACE_HOLDER>nned,if ( pinot query . contains ( __str__ ) ) { return ; } if ( pinot num records selected != __num__ ) { string failure message = __str__ + pinot num records selected + __str__ ; failure ( pinot query @$ sql queries @$ failure message ) ; },query has
... if the cache did not contain a color state list @$ try and create <PLACE_HOLDER>,if ( tint == null ) { tint = ( m hooks == null ) ? null : m hooks . get tint list for drawable res ( context @$ res id ) ; if ( tint != null ) { add tint list to cache ( context @$ res id @$ tint ) ; } },and create
empty control batch should not cause an <PLACE_HOLDER>,default record batch . write empty header ( buffer @$ record batch . magic_value_v2 @$ __num__ @$ ( short ) __num__ @$ - __num__ @$ __num__ @$ __num__ @$ record batch . no_partition_leader_epoch @$ timestamp type . create_time @$ time . milliseconds ( ) @$ true @$ true ) ; current offset += append transactional records ( buffer @$ __num__ @$ current offset @$ new simple record ( time . milliseconds ( ) @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) @$ new simple record ( time . milliseconds ( ) @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) ) ; commit transaction ( buffer @$ __num__ @$ current offset ) ; buffer .,batch cause
when location represents one particular <PLACE_HOLDER>,for ( storage dir dir : tier . get storage dirs ( ) ) { location = dir . to block store location ( ) ; assert equals ( m meta manager . get available bytes ( location ) @$ m metadata view . get available bytes ( location ) ) ; },location represents
then : should get created interpreters which have different <PLACE_HOLDER>,md1 = setting ;,which have
ensure a timer throws an illegal argument <PLACE_HOLDER> if period is negative,t = new timer ( ) ; d = new date ( system . current time millis ( ) + __num__ ) ; test task = new timer test task ( ) ; try { t . schedule ( test task @$ d @$ - __num__ ) ; fail ( __str__ ) ; } catch ( illegal argument exception expected ) { } t . cancel ( ) ;,timer throws
t qk js n 2 q 2 s zv 9 h 2 d q 5 x 2 r sne k ny l qgeg <PLACE_HOLDER>,ec key ec key2 = new ec key ( utils . get random ( ) ) ; byte [ ] low bal address2 = ec key2 . get address ( ) ; ret1 = public methed . sendcoin2 ( low bal address2 @$ __num__ @$ from address @$ test key002 @$ blocking stub full ) ; assert . assert equals ( ret1 . get code ( ) @$ grpcapi . return . response_code . success ) ; witness list witnesslist = blocking stub full . list witnesses ( grpcapi . empty message . new builder ( ) . build ( ) ) ; optional < witness list > result = optional . of nullable ( witnesslist ) ; witness list witness list = result . get ( ),l qgeg
pre grouped symbols reflect <PLACE_HOLDER> of the input . splitting the aggregation and pushing partial aggregation through the exchange may or may not preserve these <PLACE_HOLDER> . hence @$ it is safest to drop pre grouped symbols here .,plan node partial = new aggregation node ( context . get id allocator ( ) . get next id ( ) @$ node . get source ( ) @$ intermediate aggregation @$ node . get grouping sets ( ) @$ immutable list . of ( ) @$ partial @$ node . get hash symbol ( ) @$ node . get group id symbol ( ) ) ;,properties preserve
allow a connection from a new key with the 'always allow ' option <PLACE_HOLDER> to false,run adb test ( test_key_1 @$ true @$ false @$ false ) ;,connection allow
do not set ms lab to null as scanners may still be reading the <PLACE_HOLDER> here and need to decrease the counter when they finish,this . mem storelab . close ( ) ;,scanners reading
init logs read <PLACE_HOLDER>,lenient ( ) . do return ( test_log_infos ) . when ( test workspace logs reader ) . get log infos ( ) ; lenient ( ) . do return ( test_read_first_log_command ) . when ( test workspace logs reader ) . get read logs command ( test_workspace_id @$ paths . get ( format ( __str__ @$ path_to_store_logs @$ test_workspace_id @$ first_log_info . get name ( ) ) ) @$ first_log_info . get location inside workspace ( ) ) ; lenient ( ) . do return ( test_read_second_log_command ) . when ( test workspace logs reader ) . get read logs command ( test_workspace_id @$ paths . get ( format ( __str__ @$ path_to_store_logs @$ test_workspace_id @$ second_log_info . get name ( ) ) ) @$ second_log_info .,init logs
we are setting an empty value to these attributes @$ since now we have a new entity type called hive process execution which captures these <PLACE_HOLDER> . we have to set empty <PLACE_HOLDER> here because these attributes are mandatory attributes for hive process entity type .,ret . set attribute ( attribute_start_time @$ empty_attribute_value ) ; ret . set attribute ( attribute_end_time @$ empty_attribute_value ) ; ret . set attribute ( attribute_user_name @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_text @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_id @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_plan @$ __str__ ) ; ret . set attribute ( attribute_recent_queries @$ collections . singleton list ( query str ) ) ; return ret ;,which captures
check existing schema contains all the <PLACE_HOLDER> in to validate schema,if ( to validate . get types ( ) . size ( ) != expected . get types ( ) . size ( ) ) { return false ; } hash set < schema > types = new hash set < schema > ( expected . get types ( ) ) ; for ( schema to validate type : to validate . get types ( ) ) { schema equal schema = null ; for ( schema type : types ) { if ( compare ( type @$ to validate type ) ) { equal schema = type ; break ; } } if ( equal schema == null ) { return false ; } types . remove ( equal schema ) ; } return true ;,schema contains
check that the memory manager got all <PLACE_HOLDER> back,if ( ! this . memman . verify empty ( ) ) { assert . fail ( __str__ ) ; } this . memman . shutdown ( ) ; this . memman = null ;,manager got
local get can not be used with mvcc as local node can contain some visible <PLACE_HOLDER> which is not latest .,boolean fast loc get = ! cctx . mvcc enabled ( ) && ( ! force primary || aff nodes . get ( __num__ ) . is local ( ) ) && cctx . reserve for fast local get ( part @$ top ver ) ; if ( fast loc get ) { try { if ( local get ( top ver @$ key @$ part @$ loc vals ) ) return true ; } finally { cctx . release for fast local get ( part @$ top ver ) ; } } return false ;,node contain
converts interpolated <PLACE_HOLDER> back to polar .,double lat = atan2 ( z @$ sqrt ( x * x + y * y ) ) ; double lng = atan2 ( y @$ x ) ; return new geo point ( to degrees ( lat ) @$ to degrees ( lng ) ) ;,converts interpolated
our filter passes <PLACE_HOLDER>,assert row count ( full count + __num__ ) ;,filter passes
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,object [ ] [ ] contents = new object [ ] [ ] { { msg key . bad_msgkey @$ __str__ } @$ { msg key . bad_msgformat @$ __str__ } @$ { msg key . er_serializer_not_contenthandler @$ __str__ } @$ { msg key . er_resource_could_not_find @$ __str__ } @$ { msg key . er_resource_could_not_load @$ __str__ } @$ { msg key . er_buffer_size_lessthan_zero @$ __str__ } @$ { msg key . er_invalid_utf16_surrogate @$ __str__ } @$ { msg key . er_oierror @$ __str__ } @$ { msg key . er_illegal_attribute_position @$ __str__ } @$ { msg key . er_namespace_prefix @$ __str__ } @$ { msg key . er_stray_attribute @$ __str__ } @$ { msg key . er_stray_namespace @$ __str__ } @$ { msg key . er_could_not_load_resource @$,text specifies
re<PLACE_HOLDER> the first occurance with the variable 's x path @$ so that further reduction may take <PLACE_HOLDER> if needed .,paths . set element at ( var . get select ( ) @$ first occurance index ) ;,reduction take
find function name @$ pointed by function . long timestamp can shift if position so use <PLACE_HOLDER> of timestamp to find the function which is <PLACE_HOLDER>ed by ' : ' .,left index = right index + __num__ ; while ( character . is whitespace ( line . char at ( left index ) ) ) { ++ left index ; } right index = line . index of ( __str__ @$ left index ) ; final string function = line . substring ( left index @$ right index ) ; if ( ! function . equals ( function_tracing_mark_write ) ) { continue ; },position use
we read a further byte @$ which indicates the zmtp <PLACE_HOLDER> .,byte protocol = greeting recv . get ( revision pos ) ; if ( protocol == protocol . v1 . revision || protocol == protocol . v2 . revision ) { greeting send . limit ( v2_greeting_size ) ; greeting send . position ( signature_size + __num__ ) ; greeting send . put ( ( byte ) options . type ) ; outsize += __num__ ; } else { greeting send . limit ( v3_greeting_size ) ; greeting send . position ( signature_size + __num__ ) ; greeting send . put ( ( byte ) __num__ ) ; outsize += __num__ ; greeting send . mark ( ) ; greeting send . put ( new byte [ __num__ ] ) ; assert ( mechanism == mechanisms . null,which indicates
while it does seem a bit silly @$ we need a check to make sure the document pane is enabled since this method will get called during the processing stage and we do n't want anything below to be fired during so . this checks to make sure the user has selected appropriate <PLACE_HOLDER> for the combine sentences option to be enabled .,if ( e . is popup trigger ( ) && main . document pane . is enabled ( ) ) { if ( main . editor driver . new caret position [ __num__ ] > main . editor driver . new caret position [ __num__ ] ) { start = main . editor driver . new caret position [ __num__ ] ; stop = main . editor driver . new caret position [ __num__ ] ; } else { start = main . editor driver . new caret position [ __num__ ] ; stop = main . editor driver . new caret position [ __num__ ] ; } if ( start == stop ) { right click menu . enable combine sentences ( false ) ; right click,user selected
now initialize the <PLACE_HOLDER> in the first ; <PLACE_HOLDER>s in both groups should get initialized,hibernate . initialize ( groups [ __num__ ] . get employees ( ) ) ; assert equals ( __num__ @$ session factory ( ) . get statistics ( ) . get prepare statement count ( ) ) ; session factory ( ) . get statistics ( ) . clear ( ) ;,now initialize
this one passes the <PLACE_HOLDER> .,return ;,one passes
vm <PLACE_HOLDER> locks and frees key <PLACE_HOLDER>,vm1 . invoke ( new serializable runnable ( ) { @ override public void run ( ) { logger . info ( __str__ ) ; connect distributed system ( ) ; d lock service dls = ( d lock service ) distributed lock service . create ( dls name @$ get system ( ) ) ; assert that ( dls . lock ( key1 @$ - __num__ @$ - __num__ ) ) . is true ( ) ; assert that ( dls . is lock grantor ( ) ) . is false ( ) ; assert that ( dls . get token ( key1 ) ) . is not null ( ) ; dls . unlock ( key1 ) ; assert that ( dls . get token (,vm locks
flush enough files to get up to the threshold @$ does n't need <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { put put = new put ( table name . to bytes ( ) ) . add column ( family @$ family @$ table name . to bytes ( ) ) ; region . put ( put ) ; fr = region . flush ( true ) ; assert true ( fr . is flush succeeded ( ) ) ; assert false ( fr . is compaction needed ( ) ) ; },files need
this will change the source text <PLACE_HOLDER> instead @$ which will update the extract text <PLACE_HOLDER> .,mime . on extracted delete text ( start @$ end ) ;,which update
set new items without calling notify cell data set changed <PLACE_HOLDER> of cell recycler view adapter,m row header recycler view adapter . set items ( new row header @$ ! m enable animation ) ; m cell recycler view adapter . set items ( new column items @$ ! m enable animation ) ; if ( m enable animation ) { final row header sort callback diff callback = new row header sort callback ( old row header @$ new row header ) ; final diff util . diff result diff result = diff util . calculate diff ( diff callback ) ; diff result . dispatch updates to ( m row header recycler view adapter ) ; diff result . dispatch updates to ( m cell recycler view adapter ) ; },data set
let 's strip off <PLACE_HOLDER> after the last ' . ',int slash idx = answer . last index of ( __str__ ) ; int separator idx = answer . last index of ( file . separator char ) ; int dot idx = answer . last index of ( __str__ ) ; if ( dot idx > __num__ && dot idx > math . max ( slash idx @$ separator idx ) ) { answer = answer . substring ( __num__ @$ dot idx ) ; },"s" strip
the specific device <PLACE_HOLDER> does not match the generic device <PLACE_HOLDER> .,if ( specific device class . generic device class != generic . not_known && specific device class . generic device class != this . generic device class ) { throw new illegal argument exception ( __str__ ) ; } this . specific device class = specific device class ;,class match
check that participant status listener is working <PLACE_HOLDER>,assert equals ( __str__ @$ room + __str__ @$ answer [ __num__ ] ) ;,listener working
completing the task should trigger the event <PLACE_HOLDER> . this interupts the main flow .,task service . complete ( task . get id ( ) ) ; task sub one task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( sub one task ) ; task service . complete ( sub one task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ;,task trigger
check whether the caller can read the core <PLACE_HOLDER> ?,sm . check read ( corefile ) ;,caller read
trigger a bucket creation in vm <PLACE_HOLDER> @$ which should cause server <PLACE_HOLDER> to close it 's cache .,assert that thrown by ( ( ) -> server1 . invoke ( ( ) -> put data ( __num__ @$ __num__ @$ __str__ ) ) ) . is instance of ( rmi exception . class ) . has cause instance of ( distributed system disconnected exception . class ) ; assert that ( server2 . invoke ( ( ) -> get bucket list ( ) ) ) . contains exactly ( __num__ ) ;,which cause
the override priority for implementation options p 1 camera event <PLACE_HOLDER> on repeating options p 2 session config options,capture config . builder capture config builder = capture config . builder . from ( capture config ) ; camera event callbacks event callbacks = new camera2 config ( m session config . get implementation options ( ) ) . get camera event callback ( camera event callbacks . create empty callback ( ) ) ; m camera event on repeating options = merge options ( event callbacks . create combo callback ( ) . on repeating ( ) ) ; if ( m camera event on repeating options != null ) { capture config builder . add implementation options ( m camera event on repeating options ) ; } capture request capture request = camera2 capture request builder . build ( capture config builder . build (,priority p
reorder only down to the lowest odd level and reorder at an odd min level in a separate @$ simpler loop . see <PLACE_HOLDER> above for why min level is always incremented .,++ min level ; runs = bidi . runs ; levels = bidi . levels ; run count = bidi . run count ;,reorder see
user ca n't edit any other <PLACE_HOLDER> .,return false ;,user edit
run the trash emptier for 120 ms @$ it should run 2 times deletion as the interval is 50 ms. verify the checkpoints <PLACE_HOLDER> when shutting down the emptier .,verify auditable trash emptier ( trash @$ __num__ @$ __num__ ) ;,ms. verify
let 's close the first <PLACE_HOLDER> with evict on close turned on,conf . set boolean ( __str__ @$ true ) ; cache conf = new cache config ( conf @$ bc ) ; hsf = new h store file ( this . fs @$ path cow off @$ conf @$ cache conf @$ bloom type . none @$ true ) ; hsf . init reader ( ) ; reader = hsf . get reader ( ) ; reader . close ( cache conf . should evict on close ( ) ) ;,"s" close
verify user from a group which has table level <PLACE_HOLDER> can read all the data and group which has no <PLACE_HOLDER> ca n't read any data .,grant on table ( test_util @$ testgroup_1_name @$ table name @$ null @$ null @$ action . read ) ; verify allowed ( testgroup1_user1 @$ scan table action for group with table level access ) ; verify denied ( testgroup2_user1 @$ scan table action for group with table level access ) ;,which has
making subcluster 0 slow to reply @$ should only get d <PLACE_HOLDER> from nn 1,minidfs cluster dfs cluster = cluster . get cluster ( ) ; name node nn0 = dfs cluster . get name node ( __num__ ) ; simulate slow namenode ( nn0 @$ __num__ ) ; wait update live nodes ( json string2 @$ metrics ) ; final string json string3 = metrics . get live nodes ( ) ; assert equals ( __num__ @$ get num datanodes ( json string3 ) ) ;,1 get
nobody has the correct <PLACE_HOLDER>,test protected denied ( url @$ __str__ @$ __str__ ) ; test protected denied ( url @$ __str__ @$ __str__ ) ; test protected denied ( url @$ __str__ @$ __str__ ) ;,nobody has
verify the server got the <PLACE_HOLDER> with right size,assert equals ( request size @$ result size ) ;,server got
the caller is responsible for reading all the data @$ <PLACE_HOLDER> tee input stream will copy to output stream .,if ( stdout reading callback . is present ( ) ) { stdout reading callback . get ( ) . read stdout ( stdout wrapper stream ) ; } else { stdout wrapper stream . close ( ) ; byte streams . copy ( launched process . get stdout ( ) @$ output stream ) ; },stream copy
if we use <PLACE_HOLDER> @$ setup the dex reporter to notify package manager of any relevant dex loads . the idle maintenance job will use the information reported to optimize the loaded dex files . note that we only need one global reporter per app . make sure we do this before invoking app code for the first time so that we can capture,base dex class loader . set reporter ( dex load reporter . get instance ( ) ) ;,job use
a stubborn caller will get string reader <PLACE_HOLDER> .,if ( m_bytes == null ) { m_string reader = new string reader ( __str__ ) ; throw new io exception ( string . format ( __str__ @$ m_path ) ) ; },caller get
the codec has fewer <PLACE_HOLDER> than required by the avc codec string format .,if ( parts . length < __num__ ) { log . w ( tag @$ __str__ + codec ) ; return null ; },codec has
clean the build <PLACE_HOLDER> @$ so that we need to go to cache .,workspace . run buck command ( __str__ @$ __str__ ) ;,the build
russian internal passports use <PLACE_HOLDER> for the name and have a number one digit longer than fits into the standard mrz format,if ( __str__ . equals ( result . issuing country ) && mrz lines [ __num__ ] . char at ( __num__ ) == __str__ ) { result . type = result . type_internal_passport ; string [ ] names = result . first name . split ( __str__ ) ; result . first name = cyrillic to latin ( russian passport translit ( names [ __num__ ] ) ) ; if ( names . length > __num__ ) result . middle name = cyrillic to latin ( russian passport translit ( names [ __num__ ] ) ) ; result . last name = cyrillic to latin ( russian passport translit ( result . last name ) ) ; if ( result . number != null ) result .,passports use
always <PLACE_HOLDER> before installing . we might be downgrading @$ which requires an <PLACE_HOLDER> @$ or we might just want a clean installation .,uninstall agent ( event bus @$ device ) ; agent info = optional . empty ( ) ;,which requires
run two animators @$ one of which has a start <PLACE_HOLDER> @$ after setting the duration scale to 0,a1 . set start delay ( __num__ ) ; final my listener l1 = new my listener ( ) ; final my listener l2 = new my listener ( ) ; a1 . add listener ( l1 ) ; a2 . add listener ( l2 ) ; m activity rule . run on ui thread ( new runnable ( ) { @ override public void run ( ) { assert false ( l1 . start called ) ; assert false ( l2 . start called ) ; assert false ( l1 . end called ) ; assert false ( l2 . end called ) ; a1 . start ( ) ; a2 . start ( ) ; assert equals ( a2_start_value @$ a2 . get animated value (,one has
iteration throught all the <PLACE_HOLDER> of the list to gather the materials,build mat cache ( l ) ;,iteration throught
initial security properties should only contain initial <PLACE_HOLDER> of values,this . server starter . start server ( props @$ this . ls rule . get member ( __num__ ) . get port ( ) ) ; distributed system ds = this . server starter . get cache ( ) . get distributed system ( ) ;,properties contain
populate the cache and then reset the build <PLACE_HOLDER>,process result cache populating result = workspace . run buck command ( __str__ @$ target . get fully qualified name ( ) ) ; cache populating result . assert success ( ) ;,the build
only master stream can resolve the data <PLACE_HOLDER>,assert ( m_coordinator . is master ( ) ) ;,stream resolve
if in zip 64 format or using strict entry numbers @$ use the parsed information as is to read the central directory file <PLACE_HOLDER> .,if ( zip data . is zip64 ( ) || strict entries ) { read central directory file headers ( zip data . get expected entries ( ) @$ zip data . get central directory offset ( ) ) ; } else { long central directory offset = eocd location - zip data . get central directory size ( ) ; if ( ( int ) central directory offset == ( int ) zip data . get central directory offset ( ) ) { read central directory file headers ( central directory offset ) ; } else { read central directory file headers ( zip data . get expected entries ( ) @$ zip data . get central directory offset ( ) ) ; } },directory file
use raw socket instead of telnet client here because telnet client sends an extra newline char after each write which causes the <PLACE_HOLDER> to become unresponsive .,socket = new socket ( ) ; socket . connect ( new inet socket address ( connection . get host ( ) @$ connection . get telnet port ( ) ) @$ timeout ) ; socket . set keep alive ( true ) ; socket . set so timeout ( timeout ) ; in = new buffered reader ( new input stream reader ( socket . get input stream ( ) ) ) ; out = new output stream writer ( socket . get output stream ( ) @$ __str__ ) ; connected = true ; callback . listener connected ( ) ;,which causes
if multiple threads are doing the <PLACE_HOLDER> at the same time it is not a problem because they will all get to the same result in the end .,if ( format == null ) { format = logging support . get simple format ( ) ; },threads doing
if we have more than one stream @$ lets the <PLACE_HOLDER> be the master,if ( ! master stream set ) { if ( remote descriptions . size ( ) > __num__ ) { if ( media type . audio . equals ( media type ) ) { master stream = true ; master stream set = true ; } } else { master stream = true ; master stream set = true ; } } media stream stream = init stream ( connector @$ dev @$ fmt @$ target @$ direction @$ rtp extensions @$ master stream ) ;,master lets
if job is already succeed and h base segment in ready <PLACE_HOLDER> @$ remove the build <PLACE_HOLDER>,if ( executable state . succeed . equals ( job state ) ) { cube segment cube segment = cube instance . get segment ( segment state . get segment name ( ) @$ null ) ; if ( cube segment != null && segment status enum . ready == cube segment . get status ( ) ) { logger . info ( __str__ @$ job id @$ segment state . get segment name ( ) ) ; coordinator . get stream metadata store ( ) . remove segment build state ( cube name @$ segment state . get segment name ( ) ) ; } return false ; },the build
rose and durant have 5 <PLACE_HOLDER> per game but only rose does not play in okc,iterable < player > filtered players = filter ( players ) . with ( __str__ ) . equals to ( __num__ ) . and ( __str__ ) . not equals to ( __str__ ) . get ( ) ; assert that ( filtered players ) . contains only ( kobe ) ;,durant have
relay state data may be included with a saml protocol message transmitted with this binding . the value must not exceed 80 <PLACE_HOLDER> in length and should be integrity protected by the entity creating the message independent of any other protections that may or may not exist during message transmission .,if ( relay state != null ) { if ( relay state . length ( ) > __num__ ) { throw new illegal argument exception ( __str__ + relay state . length ( ) ) ; } encoder . add param ( relay_state @$ relay state ) ; },value exceed
we create a class <PLACE_HOLDER> @$ configure it with specified classpath values and set the same as context <PLACE_HOLDER> . note that script engine manager uses context <PLACE_HOLDER> to load script engines . so @$ this ensures that user defined script engines will be loaded . for classes referred from scripts @$ rhino engine uses thread context <PLACE_HOLDER> but this is script engine dependent,if ( class path != null ) { class loader parent = main . class . get class loader ( ) ; url [ ] urls = path tour ls ( class path ) ; url class loader loader = new url class loader ( urls @$ parent ) ; thread . current thread ( ) . set context class loader ( loader ) ; },manager uses
inversion does not handle <PLACE_HOLDER> correctly .,test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; invert = true ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ;,inversion handle
1 : no stats since hbm.xml ca n't enable natural id <PLACE_HOLDER>,assert equals ( __num__ @$ session factory ( ) . get statistics ( ) . get natural id cache put count ( ) ) ;,stats enable
else release pause <PLACE_HOLDER>,pause reader ( ) ;,release pause
by convention @$ full message <PLACE_HOLDER> extends first message <PLACE_HOLDER> .,if ( content . readable bytes ( ) > max content length - m . content ( ) . readable bytes ( ) ) { @ suppress warnings ( __str__ ) s s = ( s ) current message ; invoke handle oversized message ( ctx @$ s ) ; return ; },type extends
create a build <PLACE_HOLDER> that gives 30000 pairs with 3 values sharing the same key,mutable object iterator < binary row > build input = new uniform binary row generator ( num keys @$ build vals per key @$ false ) ;,a build
grid and escalator take care of their own cleanup at removal @$ no <PLACE_HOLDER> to clear details from those . because this removal happens instantly any pending scroll to row or such should not <PLACE_HOLDER> another attempt and unless something else causes such <PLACE_HOLDER> the pending operations should be cleared out .,mark details added or updated for delayed alert to grid ( false ) ;,grid take
note to translators : the substitution text is the name of a function . a literal string here means a constant string <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note means
then inner comparison fails as the fields have different <PLACE_HOLDER>,comparison difference value difference = diff ( __str__ @$ witha . inner @$ withb . inner @$ __str__ ) ; verify should be equal by comparing field by field recursively call ( witha @$ withb @$ value difference ) ;,fields have
we have a real error @$ so do <PLACE_HOLDER> the appropriate action tells us <PLACE_HOLDER> to do ...,coding error action action = result . is unmappable ( ) ? unmappable character action : malformed input action ; if ( action == coding error action . report ) { return result ; } else if ( action == coding error action . replace ) { if ( out . remaining ( ) < replacement bytes . length ) { return coder result . overflow ; } out . put ( replacement bytes ) ; } in . position ( in . position ( ) + result . length ( ) ) ;,action tells
base set but with label b and different <PLACE_HOLDER>,new equals tester ( ) . add equality group ( dependency . with configuration and aspects ( a @$ host @$ two aspects ) @$ dependency . with configuration and aspects ( a explicit @$ host @$ two aspects ) @$ dependency . with configuration and aspects ( a @$ host @$ inverse aspects ) @$ dependency . with configuration and aspects ( a explicit @$ host @$ inverse aspects ) @$ dependency . with configured aspects ( a @$ host @$ two aspects @$ two aspects host map ) @$ dependency . with configured aspects ( a explicit @$ host @$ two aspects @$ two aspects host map ) ) . add equality group ( dependency . with configuration and aspects ( b @$ host @$ two,base set
delete notifications do n't need all <PLACE_HOLDER> . hence the special handling for delete operation,if ( instance converter != null ) { if ( operation == entity operation . delete ) { for ( atlas entity header entity header : entity headers ) { ret . add ( new referenceable ( entity header . get guid ( ) @$ entity header . get type name ( ) @$ entity header . get attributes ( ) ) ) ; } } else { for ( atlas entity header entity header : entity headers ) { ret . add ( to referenceable ( entity header . get guid ( ) ) ) ; } } },notifications need
hadoop 's external sort stores the <PLACE_HOLDER> of available memory bytes in an int @$ this prevents integer overflow,check argument ( memorymb < __num__ @$ __str__ ) ; this . memorymb = memorymb ; return this ;,sort stores
if the message has no <PLACE_HOLDER> @$ or the bodies list is empty,if ( old msg . get body ( ) == null && old msg . get bodies ( ) . size ( ) == __num__ ) { return old msg ; } message new msg = new message ( ) ; new msg . set stanza id ( packet . get stanza id ( ) ) ; new msg . set to ( packet . get to ( ) ) ; new msg . set from ( packet . get from ( ) ) ;,message has
control all of the rows which has same column <PLACE_HOLDER> .,if ( should fit columns ( column position @$ my position ) ) { if ( m last dx < __num__ ) { log . e ( log_tag @$ __str__ + column position + __str__ + my position + __str__ + __str__ ) ; m cell layout manager . fit width size ( column position @$ true ) ; } else { m cell layout manager . fit width size ( column position @$ false ) ; log . e ( log_tag @$ __str__ + column position + __str__ + my position + __str__ + __str__ ) ; } m need fit for vertical scroll = false ; },which has
use the new provider api to better leverage the build <PLACE_HOLDER>,_target . get jvm argument providers ( ) . add ( new data template jvm argument provider ( resolver path str @$ get project ( ) . get root dir ( ) ) ) ; _target . get argument providers ( ) . add ( new data template argument provider ( arrays . as list ( _target . get destination dir ( ) . get path ( ) @$ _target . get input dir ( ) . get path ( ) ) ) ) ;,the build
user 1 creates <PLACE_HOLDER>,note note public = notebook . create note ( __str__ @$ new authentication info ( __str__ ) ) ;,user creates
check that the password is disguised and the toggle button reflects the same <PLACE_HOLDER>,assert not equals ( input_text @$ text input . get layout ( ) . get text ( ) . to string ( ) ) ; on view ( with id ( r . id . textinput_password ) ) . perform ( click icon ( true ) ) ;,button reflects
we just wait for expected events to arrive . caller will do <PLACE_HOLDER> and throw exception .,return true ;,caller do
resolution of <PLACE_HOLDER> is funky on windows . python does n't follow <PLACE_HOLDER> @$ but java does so : do an extra resolution in java @$ and see what happens . yay .,path original pwd = pwd ; try { pwd = pwd . to real path ( ) . to absolute path ( ) ; } catch ( io exception e ) { },python follow
number of state infos that <PLACE_HOLDER> .,int match count = __num__ ; int maxo styles = style . states . length ; for ( int this counter = states . length - __num__ ; this counter >= __num__ ; this counter -- ) { int state = states [ this counter ] . get component state ( ) ; boolean found = false ; for ( int o counter = maxo styles - __num__ - match count ; o counter >= __num__ ; o counter -- ) { if ( state == style . states [ o counter ] . get component state ( ) ) { style . states [ o counter ] = states [ this counter ] . add to ( style . states [ o counter ] ) ; state,number infos
we need this delay so wm can not treat two <PLACE_HOLDER> on title as double click,robot . delay ( __num__ ) ; util . click on title ( owner_frame @$ robot ) ; util . wait for idle ( robot ) ; system . out . println ( __str__ ) ;,wm treat
check if session handler sends a goaway <PLACE_HOLDER> when closing,session handler . write inbound ( close message ) ; assert go away ( session handler . read outbound ( ) @$ local stream id ) ; assert null ( session handler . read outbound ( ) ) ; local stream id += __num__ ;,handler sends
this way we can verify the <PLACE_HOLDER> of the runnable as well .,verify ( m network queue ) . put ( m request ) ; assert same ( entry @$ m request . get cache entry ( ) ) ; verify ( m delivery @$ never ( ) ) . post error ( any ( request . class ) @$ any ( volley error . class ) ) ;,way verify
this method will be called by the printer job on a thread other that the application 's thread . we hold on to the graphics until we can rendevous with the application 's thread and hand over the graphics . the application then does all the <PLACE_HOLDER> . when the application is done <PLACE_HOLDER> we rendevous again with the printer job thread and release,int result ;,application does
when the trailer does not contain orca <PLACE_HOLDER> @$ listener callback will not be invoked .,metadata trailer = new metadata ( ) ; tracer . inbound trailers ( trailer ) ; verify no more interactions ( orca listener1 ) ;,trailer contain
now @$ send the second request so that the client reuses the <PLACE_HOLDER> .,final http response res2 = client . get ( __str__ ) ;,client reuses
nodeset comparisons @$ we always call the nodeset <PLACE_HOLDER> . because the arguments are backwards @$ we call the opposite comparison <PLACE_HOLDER> .,if ( obj2 . get type ( ) == x object . class_nodeset ) return obj2 . greater than ( this ) ; return this . num ( ) < obj2 . num ( ) ;,comparisons call
evaluate any karate expression even on lhs will throw <PLACE_HOLDER> if variable does not exist,actual = eval karate expression ( expression @$ context ) ; if ( actual . is json like ( ) ) { path = var_root ; },expression throw
for every time when a user has not selected a <PLACE_HOLDER> but a basic block this breaks . as it does throw a null pointer exception .,first function . load ( ) ; second function . load ( ) ; final creation thread creation thread = new creation thread ( module @$ source block @$ target block @$ first function @$ second function ) ; progress dialog . show ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ @$ creation thread ) ; if ( ( ! ( creation thread . threw exception ( ) ) ) && ( creation thread . get created view ( ) == null ) ) { message box . show information ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ ) ; } else { new thread ( ) { @ override public void,user selected
application log <PLACE_HOLDER>,if ( app id != null ) { app log dir present . set true ( ) ; if ( should clean app log dir ( child path @$ now @$ fs @$ retain millis ) ) { delete dir ( child path ) ; } } else { clean app log dir ( child path @$ retain millis @$ app log dir present ) ; },application log
insert one <PLACE_HOLDER> @$ so calls below produce just one <PLACE_HOLDER> .,client . call procedure ( __str__ @$ __num__ @$ __num__ ) ;,calls produce
management server sends back an rds <PLACE_HOLDER> containing route configurations more than requested .,list < any > route configs = immutable list . of ( any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) @$ any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) @$ any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) ) ;,server sends
this method returns <PLACE_HOLDER> to support chaining .,return this ;,method returns
this particular removal does n't affect our <PLACE_HOLDER>,return ;,removal affect
the default <PLACE_HOLDER> provider uses a <PLACE_HOLDER> of 1 for all calls @$ ignoring the processing details @$ so an empty one is fine,processing details empty processing details = new processing details ( time unit . milliseconds ) ; scheduler . add response time ( __str__ @$ mock call @$ empty processing details ) ; return priority ;,provider uses
components with their own comp context do their own <PLACE_HOLDER>,if ( naming mode == component naming mode . create && comp binding ) { continue ; },components do
update log reg <PLACE_HOLDER> with new portion of data .,logistic regression model mdl2 = trainer . update ( mdl @$ ignite @$ data cache @$ split . get test filter ( ) @$ normalization preprocessor ) ; system . out . println ( __str__ + mdl ) ; double accuracy = evaluator . evaluate ( data cache @$ mdl2 @$ normalization preprocessor @$ metric name . accuracy ) ; system . out . println ( __str__ + accuracy ) ; system . out . println ( __str__ + ( __num__ - accuracy ) ) ; system . out . println ( __str__ ) ;,update log
everyone can submit <PLACE_HOLDER> on queue c .,verify submit reservation success ( queue_b_user @$ queuec ) ; verify submit reservation success ( queue_b_admin @$ queuec ) ; verify submit reservation success ( queue_a_user @$ queuec ) ; verify submit reservation success ( queue_a_admin @$ queuec ) ; verify submit reservation success ( common_user @$ queuec ) ;,everyone submit
update the context chars and the unsafe backward set while copying @$ in case a character had conditional <PLACE_HOLDER> in the source builder and they were removed later .,modified |= src . modified ;,character had
in order to find out whether the divide generates the exact <PLACE_HOLDER> @$ we avoid calling the above divide method . 'quotient ' holds the return big decimal object whose scale will be set to 'scl ' .,big decimal quotient ; int scl = check scale non zero ( preferred scale + yscale - xscale + mcp ) ; if ( check scale non zero ( ( long ) mcp + yscale - xscale ) > __num__ ) { int raise = check scale non zero ( ( long ) mcp + yscale - xscale ) ; big integer rb = big multiply power ten ( xs @$ raise ) ; quotient = divide and round ( rb @$ ys @$ scl @$ rounding mode @$ check scale non zero ( preferred scale ) ) ; } else { int new scale = check scale non zero ( ( long ) xscale - mcp ) ; if ( new scale == yscale ) { quotient,divide generates
test echo <PLACE_HOLDER>,echo request proto echo request = echo request proto . new builder ( ) . set message ( __str__ ) . build ( ) ; echo response proto echo response = stub . echo ( null @$ echo request ) ; assert equals ( __str__ @$ echo response . get message ( ) ) ; stub . error ( null @$ empty request ) ; fail ( __str__ ) ; rpc client . close ( ) ;,test echo
second insert will violate the primary key <PLACE_HOLDER>,volt queuesql ( insertp1 @$ __num__ @$ __num__ ) ; volt executesql ( ) ;,insert violate
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,return new object [ ] [ ] { { basis library . run_time_internal_err @$ __str__ } @$ { basis library . run_time_copy_err @$ __str__ } @$ { basis library . data_conversion_err @$ __str__ } @$ { basis library . external_func_err @$ __str__ } @$ { basis library . equality_expr_err @$ __str__ } @$ { basis library . invalid_argument_err @$ __str__ } @$ { basis library . format_number_err @$ __str__ } @$ { basis library . iterator_clone_err @$ __str__ } @$ { basis library . axis_support_err @$ __str__ } @$ { basis library . typed_axis_support_err @$ __str__ } @$ { basis library . stray_attribute_err @$ __str__ } @$ { basis library . stray_namespace_err @$ __str__ } @$ { basis library . namespace_prefix_err @$ __str__ } @$ { basis library,text specifies
array must not have <PLACE_HOLDER>,return new object [ ] [ ] { { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$ { __str__ @$ new string [ ] { __str__ } } @$,array have
on finish : when function <PLACE_HOLDER>s something : store register 0 ; <PLACE_HOLDER> value pop store register 2 pop store register 1 pop push register 0 <PLACE_HOLDER> when function does not <PLACE_HOLDER> anything : store register 2 pop store register 1 pop when original function has some <PLACE_HOLDER>s @$ but no <PLACE_HOLDER> at the end of function : push undefined locjump : store register,int return reg = - __num__ ; pos = start index + count - __num__ ; if ( code . get ( pos ) instanceof action return ) { pos -- ; if ( pos < start index ) { return false ; } if ( ! ( code . get ( pos ) instanceof action push ) ) { return false ; } action push pu = ( action push ) code . get ( pos ) ; if ( pu . values . size ( ) != __num__ ) { return false ; } if ( ! ( pu . values . get ( __num__ ) instanceof register number ) ) { return false ; } register number rn = ( register number ) pu .,store register
default nine should not appear @$ not checked checkbox ten should not appear @$ disabled eleven should not appear @$ <PLACE_HOLDER>,assert equals ( __str__ @$ data . get ( __num__ ) . to string ( ) ) ;,checkbox appear
the instance specific dynamic property should take <PLACE_HOLDER> over everything,assert equals ( true @$ properties . circuit breaker force closed ( ) . get ( ) ) ;,instance take
if the first activity has a higher <PLACE_HOLDER> @$ or a different default @$ then it is always desireable to pick it .,if ( r0 . priority != r1 . priority || r0 . preferred order != r1 . preferred order || r0 . is default != r1 . is default ) { return query . get ( __num__ ) ; },activity has
get image cell layout which has image <PLACE_HOLDER> instead of text <PLACE_HOLDER> .,layout = inflater . inflate ( r . layout . table_view_image_cell_layout @$ parent @$ false ) ; return new gender cell view holder ( layout ) ; default :,which has
default method below so we they can get the default <PLACE_HOLDER> when needed .,return g tree . this . get tool tip text ( event ) ;,method get
then should not throw <PLACE_HOLDER>,commit ( ) ;,then throw
the wait is necessary to have the poll function complete and propagate the <PLACE_HOLDER> from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one after = database onecode node . get comments ( ) . get local code node comment ( ) ; final list < i comment > two after = database twocode node . get comments ( ) . get local code node comment ( ) ; assert not null ( one after ) ; assert not null ( two after ) ; assert equals ( one before . size ( ) + __num__ @$ one after . size ( ) ) ; assert equals ( two before . size ( ) + __num__ @$ two after . size ( ) ) ; assert equals,function complete
d 2 should contain all <PLACE_HOLDER> of d 1 's @$ and may or may not contain extra <PLACE_HOLDER>,assert true ( d2 . get properties ( ) . entry set ( ) . contains all ( d1 . get properties ( ) . entry set ( ) ) ) ;,2 contain
the title is same @$ compare their <PLACE_HOLDER>,if ( result == __num__ ) { result = lhs [ __num__ ] . compare to ignore case ( rhs [ __num__ ] ) ; },title compare
in hive one can set conflicting <PLACE_HOLDER> for the same property @$ in such case it looks like table properties are used,if ( serde property value != null && table property value != null && ! table property value . equals ( serde property value ) ) { throw new presto exception ( hive_invalid_metadata @$ format ( __str__ @$ key @$ serde property value @$ table property value ) ) ; },one set
producer is not doing any <PLACE_HOLDER> .,create client ( producer @$ the port @$ host0 ) ; final int size = __num__ ; create values ( producer @$ regions [ __num__ ] @$ size ) ; createcq ( client @$ pool name @$ __str__ @$ cqs [ __num__ ] ) ; executecq ( client @$ __str__ @$ true @$ null ) ; create values ( producer @$ regions [ __num__ ] @$ ( __num__ * size ) ) ; for ( int i = __num__ ; i <= size ; i ++ ) { if ( i % __num__ == __num__ ) wait for updated ( client @$ __str__ @$ key + i ) ; } for ( int i = ( size + __num__ ) ; i <= __num__ * size ; i,producer doing
test case of child filling its <PLACE_HOLDER> @$ but its <PLACE_HOLDER> is n't filling its own <PLACE_HOLDER> .,builder . set layer ( __num__ ) . set is visible ( true ) ; final test window container visible unspecified root child child fills parent = visible unspecified root child . add child window ( builder ) ; visible unspecified root child child fills parent . set orientation ( screen_orientation_portrait ) ; assert equals ( screen_orientation_portrait @$ visible unspecified root child child fills parent . get orientation ( ) ) ; assert equals ( screen_orientation_unset @$ visible unspecified root child . get orientation ( ) ) ; assert equals ( screen_orientation_behind @$ root . get orientation ( ) ) ; visible unspecified root child . set fills parent ( true ) ; assert equals ( screen_orientation_portrait @$ visible unspecified root child . get orientation ( ) ),parent filling
if the declared type can be assigned into the actual type @$ or the expected type @$ then the compiler already has sufficient type <PLACE_HOLDER> .,return false ;,compiler has
use pojo type info which will create a new <PLACE_HOLDER> when create <PLACE_HOLDER> is invoked .,final test state descriptor < string > desc = new test state descriptor < > ( name @$ new pojo type info < > ( string . class @$ new array list < > ( ) ) ) ; final int thread number = __num__ ; final array list < checked thread > threads = new array list < > ( thread number ) ; final execution config execution config = new execution config ( ) ; final concurrent hash map < integer @$ type serializer < string > > serializers = new concurrent hash map < > ( ) ; for ( int i = __num__ ; i < thread number ; i ++ ) { threads . add ( new checked thread ( ) { @,which create
key 1 value 1 key <PLACE_HOLDER> ...,node key = n . get first child ( ) ; while ( key != null ) { switch ( key . get token ( ) ) { case getter_def : case setter_def : case string_key : case member_function_def : if ( is strip name ( key . get string ( ) ) ) { node next = key . get next ( ) ; n . remove child ( key ) ; node util . mark functions deleted ( key @$ compiler ) ; key = next ; compiler . report change to enclosing scope ( n ) ; break ; } default : key = key . get next ( ) ; } },key value
if the user specified the <PLACE_HOLDER> @$ then keep that,if ( from parms . _network == network . auto || from parms . _network == null ) { if ( from parms . _network_definition_file != null && ! from parms . _network_definition_file . equals ( __str__ ) ) { if ( ! from parms . _quiet_mode ) log . info ( __str__ ) ; to parms . _network = network . user ; } else { if ( to parms . _problem_type == problem type . image ) to parms . _network = network . inception_bn ; if ( to parms . _problem_type == problem type . text || to parms . _problem_type == problem type . dataset ) { to parms . _network = null ; if ( from parms . _hidden == null ) {,user specified
see if the step receives <PLACE_HOLDER> .,is receiving input = trans meta . find nr prev steps ( step meta ) > __num__ ;,step receives
check if query 1 see optimistic <PLACE_HOLDER>,utils . instance . assert response ( apollo client . query ( query1 ) . response fetcher ( cache_only ) @$ new predicate < response < hero and friends names withi ds query . data > > ( ) { @ override public boolean test ( response < hero and friends names withi ds query . data > response ) throws exception { assert that ( response . data ( ) . hero ( ) . id ( ) ) . is equal to ( __str__ ) ; assert that ( response . data ( ) . hero ( ) . name ( ) ) . is equal to ( __str__ ) ; assert that ( response . data ( ) . hero ( ) . friends (,query see
test that ` source path ` coercion does n't change the <PLACE_HOLDER> from ` path ` cercion .,assert same message ( path coerce exception @$ get coerce exception ( source path . class @$ invalid path ) ) ;,path change
traverse the declared entities and check if the node name and namespace uri of the <PLACE_HOLDER> reference matches an <PLACE_HOLDER> . if so @$ check the if the notation name is not null @$ if so @$ report an error .,document type doc type = node . get owner document ( ) . get doctype ( ) ; if ( doc type != null ) { named node map entities = doc type . get entities ( ) ; for ( int i = __num__ ; i < entities . get length ( ) ; i ++ ) { entity ent = ( entity ) entities . item ( i ) ; string node name = node . get node name ( ) == null ? __str__ : node . get node name ( ) ; string node namespaceuri = node . get namespaceuri ( ) == null ? __str__ : node . get namespaceuri ( ) ; string ent name = ent . get node name (,name matches
we have now read the file header @$ and obtained the position for <PLACE_HOLDER> of the data items . now read <PLACE_HOLDER> in turn @$ first seeking the input stream to the position of the data item .,bytes . reset ( ) ; icu binary . skip bytes ( bytes @$ cfu keys offset ) ; fcfu keys = icu binary . get ints ( bytes @$ cfu keys size @$ __num__ ) ; bytes . reset ( ) ; icu binary . skip bytes ( bytes @$ cfu values offset ) ; fcfu values = icu binary . get shorts ( bytes @$ cfu values size @$ __num__ ) ; bytes . reset ( ) ; icu binary . skip bytes ( bytes @$ cfu string table offset ) ; fcfu strings = icu binary . get string ( bytes @$ cfu string table size @$ __num__ ) ;,position read
create a temporary <PLACE_HOLDER> @$ as jaudiotagger requires a <PLACE_HOLDER> rather than an input stream,temp file = file . create temp file ( filename @$ __str__ + file ext ) ; long bytes copied = file utils . copy ( source @$ temp file @$ max bytes ) ; partially parsed = bytes copied == max bytes && source . read ( ) != - __num__ ; f = audio fileio . read ( temp file ) ;,jaudiotagger requires
<PLACE_HOLDER> <PLACE_HOLDER> modified modified added elements added <PLACE_HOLDER> modified elements modified & added <PLACE_HOLDER> modified added <PLACE_HOLDER> <PLACE_HOLDER> <PLACE_HOLDER> added <PLACE_HOLDER> <PLACE_HOLDER>,string incoming = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ;,& added
click on the input icon means : <PLACE_HOLDER> to a new hop in this case @$ we set the end hop step ...,selected step = null ; start hop step = null ; end hop step = ( step meta ) area owner . get parent ( ) ; candidate hop type = null ; start error hop step = false ;,click means
roll over the master key do allocate again . the am should get the latest amrm <PLACE_HOLDER>,rm . getrm context ( ) . getamrm token secret manager ( ) . roll master key ( ) ; response = am . allocate ( records . new record ( allocate request . class ) ) ; assert . assert not null ( response . getamrm token ( ) ) ; token < amrm token identifier > amrm token = converter utils . convert from yarn ( response . getamrm token ( ) @$ new text ( response . getamrm token ( ) . get service ( ) ) ) ; assert . assert equals ( amrm token . decode identifier ( ) . get key id ( ) @$ rm . getrm context ( ) . getamrm token secret manager ( ) . get master key,am get
remote exception contains useful <PLACE_HOLDER> as against the java.lang.reflect exceptions .,if ( cause instanceof io exception ) { throw ( io exception ) cause ; } else if ( cause instanceof runtime exception ) { throw ( runtime exception ) cause ; } else { throw new io exception ( se ) ; },exception contains
not testing string equality since some browsers return the <PLACE_HOLDER> with quotes around the url argument and some without quotes .,assert true ( background image + __str__ @$ background image . contains ( __str__ ) ) ;,equality return
a media check on anki droid will also update the media <PLACE_HOLDER>,col . get media ( ) . find changes ( true ) ;,check update
all rows have this <PLACE_HOLDER>,model = new object table model ( columns @$ calculator . class @$ new functor [ ] { new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) } @$ new functor [ ] { null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null } @$ new class [ ] { string . class @$ integer . class @$ long .,rows have
check if we can use the fast path @$ resuming a session . we can do so iff we have a valid record for that session @$ and the cipher suite for that session was on the list <PLACE_HOLDER> the client requested @$ and if we 're not forgetting any needed authentication on the part of the client .,if ( previous != null ) { resuming session = previous . is rejoinable ( ) ; if ( resuming session ) { protocol version old version = previous . get protocol version ( ) ; if ( old version != mesg . protocol version ) { resuming session = false ; } } if ( resuming session && use extended master secret ) { if ( requested to useems && ! previous . get use extended master secret ( ) ) { resuming session = false ; } else if ( ! requested to useems && previous . get use extended master secret ( ) ) { fatalse ( alerts . alert_handshake_failure @$ __str__ + __str__ ) ; } else if ( ! requested to useems &&,client requested
asif : take a lock on active cache while removing the connection from the map . it is possible that while the connection is being returned the cleaner thread migh have also removed it from the active map . if that is the case we do n't do anything bcoz the cleaner thread will take <PLACE_HOLDER> of decrementing the total & available connection .,if ( connection object != null ) { synchronized ( connection object ) { if ( this . active cache . contains key ( connection object ) ) { this . active cache . remove ( connection object ) ; returned happened = true ; } } },thread take
there are context registers to worry about @$ so we need a function start <PLACE_HOLDER> and a function start analyzer.context <PLACE_HOLDER> for each context register,map < string @$ big integer > regs to values = c reg filter . get value map ( ) ; match action [ ] actions = new match action [ __num__ + regs to values . size ( ) ] ; actions [ __num__ ] = func start analyzer . new function start action ( ) ; int match index = __num__ ; for ( string register : regs to values . key set ( ) ) { big integer value = regs to values . get ( register ) ; actions [ match index ] = func start analyzer . new context action ( register @$ value ) ; match index ++ ; } return actions ;,function start
update the backup if the user id or any of the backed up android preferences <PLACE_HOLDER> .,shared prefs . register on shared preference change listener ( new shared preferences . on shared preference change listener ( ) { @ override public void on shared preference changed ( shared preferences shared preferences @$ string key ) { if ( key . equals ( chrome signin controller . signed_in_account_key ) ) { on backup prefs changed ( ) ; return ; } for ( string pref : chrome backup agent . backup_android_bool_prefs ) { if ( key . equals ( pref ) ) { on backup prefs changed ( ) ; return ; } } } } ) ;,id backed
the first page load is used only to cause the web view to fetch the proxy settings . do n't update the url <PLACE_HOLDER> @$ and do n't check if the captive portal is still there .,if ( m pages loaded == __num__ ) return ;,settings update
each sample has up to three <PLACE_HOLDER> of overhead for the start code that replaces its length . allow ten source samples per output sample @$ like the platform extractor .,int max input size = track sample table . maximum size + __num__ * __num__ ; format format = track . format . copy with max input size ( max input size ) ; if ( track . type == c . track_type_video && track duration us > __num__ && track sample table . sample count > __num__ ) { float frame rate = track sample table . sample count / ( track duration us / __num__ ) ; format = format . copy with frame rate ( frame rate ) ; } format = metadata util . get format with metadata ( track . type @$ format @$ udta metadata @$ mdta metadata @$ gapless info holder ) ; mp4 track . track output . format (,sample has
try to update something that does n't exists should not change existing <PLACE_HOLDER>,repository . update all ( collections . singleton ( generator . get ( ) ) ) ;,something change
the task manager wo n't send heartbeat <PLACE_HOLDER> to the resource manager,resource manager gateway . heartbeat from task manager ( resourceid @$ heartbeat payload ) ;,manager send
if we 're only parsing integers @$ or if we already saw the decimal @$ then do n't parse this <PLACE_HOLDER> .,if ( is parse integer only ( ) || saw decimal ) { break ; } digits . decimal at = digit count ;,then parse
button.foreground @$ button.shadow @$ button.dark shadow @$ button.disabled forground @$ and button.disabled shadow are only used for windows classic . windows xp will use <PLACE_HOLDER> from the current visual style .,object [ ] defaults = { __str__ @$ null @$ __str__ @$ boolean . value of ( use system font settings ) @$ __str__ @$ field input map @$ __str__ @$ password input map @$ __str__ @$ multiline input map @$ __str__ @$ multiline input map @$ __str__ @$ multiline input map @$ __str__ @$ control font @$ __str__ @$ control background color @$ __str__ @$ control text color @$ __str__ @$ control shadow color @$ __str__ @$ control dark shadow color @$ __str__ @$ control light color @$ __str__ @$ control highlight color @$ __str__ @$ inactive text color @$ __str__ @$ control highlight color @$ __str__ @$ button focus color @$ __str__ @$ new xp value ( integer . value of ( __num__ ) @$ integer,shadow use
somebody took away our unwanted <PLACE_HOLDER> .,if ( delta <= unused guaranteed ) { int result = ( unused guaranteed -= delta ) ; if ( metrics != null ) { metrics . set wm unused guaranteed ( result ) ; } wm_log . info ( __str__ + result + __str__ + delta ) ; return ; } else { delta -= unused guaranteed ; unused guaranteed = __num__ ; to update = new array list < > ( ) ; int total updated = revoke guaranteed ( delta @$ null @$ to update ) ; if ( metrics != null ) { metrics . set wm unused guaranteed ( __num__ ) ; } wm_log . info ( __str__ + total updated + __str__ + delta ) ; if ( delta != total updated,somebody took
for each dimension read the <PLACE_HOLDER> of each dimension followed by the values,try { for ( int i = __num__ ; i < size ; i ++ ) { int length = in . read int ( ) ; byte [ ] b = new byte [ length ] ; in . read fully ( b ) ; dimension values [ i ] = new string ( b @$ __str__ ) ; } } catch ( exception e ) { logger . info ( arrays . to string ( bytes ) @$ e ) ; throw new runtime exception ( e ) ; } return new dimension key ( dimension values ) ;,dimension read
we need something that will measure the amount of time since our consumer has seen a <PLACE_HOLDER> ...,time since time since last record = threads . time since ( clock . system ) ;,consumer seen
platform.run later is necessary or <PLACE_HOLDER>s will be empty since check <PLACE_HOLDER>s adds 2 base <PLACE_HOLDER> later .,platform . run later ( ( ) -> { initialized = true ; selected profile . set ( profiles . stream ( ) . filter ( it -> it . get name ( ) . equals ( config ( ) . get selected profile ( ) ) ) . find first ( ) . or else ( profiles . get ( __num__ ) ) ) ; } ) ; event bus . event_bus . channel ( refreshed versions event . class ) . register weak ( event -> { run infx ( ( ) -> { profile profile = selected profile . get ( ) ; if ( profile != null && profile . get repository ( ) == event . get source ( ) ) { selected,profiles adds
any activity on the power button stops the accessibility <PLACE_HOLDER>,cancel pending accessibility shortcut action ( ) ; result &= ~ action_pass_to_user ; is wake key = false ;,activity stops
check if the grantor matches current <PLACE_HOLDER>,if ( grant info . get grantor ( ) != null && grant info . get grantor ( ) . equals ( user name ) && grant info . get grantor type ( ) == principal type . user ) { priv2priv obj . put ( grant info . get privilege ( ) @$ ms obj priv ) ; },grantor matches
for annotation type @$ if annotation only contain single mandatory <PLACE_HOLDER> which is called 'value ' it will be automatically turned into constructor parameter,if ( is annotation value attribute ( ) && there are no other mandatory attributes ( ) ) { return constructor_parameter_default_order ; },annotation contain
avoid potentially aggressive splitting which would cause <PLACE_HOLDER> to fail,conf . set ( h constants . hbase_region_split_policy_key @$ constant size region split policy . class . get name ( ) ) ;,which cause
update application catalog sometimes changes the <PLACE_HOLDER> . the second procedure call will fail in that case @$ so do n't call it a second time .,if ( pre hash && ! proc name . equals ( __str__ ) ) { params . put ( __str__ @$ get hashed password forhttp var ( password @$ client auth scheme . hash_sha1 ) ) ; call proc overjson raw ( params @$ http port @$ expected code @$ session id ) ; },catalog changes
dalvik vm rejects <PLACE_HOLDER> in an apk that are already defined . framework <PLACE_HOLDER> take precedence over local <PLACE_HOLDER> .,string class name = smali file . get class name ( ) ; if ( is framework class ( class name ) && ! class name . starts with ( __str__ ) ) { log . warn ( __str__ @$ class name ) ; } else { smali files . add ( smali file ) ; },vm rejects
local merge source must have a single <PLACE_HOLDER>,context . set driver instance count ( __num__ ) ; plan node source node = get only element ( node . get sources ( ) ) ; local execution plan context sub context = context . create sub context ( ) ; physical operation source = source node . accept ( this @$ sub context ) ; int operators count = sub context . get driver instance count ( ) . or else ( __num__ ) ; list < type > types = get source operator types ( node @$ context . get types ( ) ) ; local exchange factory exchange factory = new local exchange factory ( node . get partitioning scheme ( ) . get partitioning ( ) . get handle ( ) @$ operators,source have
second elector joins <PLACE_HOLDER> @$ becomes standby .,electors [ __num__ ] . join election ( app datas [ __num__ ] ) ; mockito . verify ( cbs [ __num__ ] @$ mockito . timeout ( __num__ ) ) . become standby ( ) ; check fatals and reset ( ) ;,elector joins
need to go through all elements @$ extract data @$ and check distance agains new centroids create <PLACE_HOLDER> of all centroids,int i ; int j ; int k ; int m ; double [ ] temp = new double [ __num__ ] ;,centroids create
static clinits are generated in gen @$ so we need to use a fake one . attr creates a fake clinit <PLACE_HOLDER> while attributing lambda expressions used as initializers of static fields @$ so let 's use that one .,if ( is static ) { method symbol clinit = attr . remove clinit ( csym ) ; if ( clinit != null ) { clinits . put ( csym @$ clinit ) ; return clinit ; } clinit = ( method symbol ) clinits . get ( csym ) ; if ( clinit == null ) { clinit = make private synthetic method ( static @$ names . clinit @$ new method type ( list . < type > nil ( ) @$ syms . void type @$ list . < type > nil ( ) @$ syms . method class ) @$ csym ) ; clinits . put ( csym @$ clinit ) ; } return clinit ; } else { for ( symbol s :,one creates
when table limit feature is fully supported @$ there needs to be more test cases . generalize this test within a loop @$ maybe . test max <PLACE_HOLDER> 0,vt = client . call procedure ( __str__ @$ __str__ ) . get results ( ) [ __num__ ] ; validate table of scalar longs ( vt @$ new long [ ] { __num__ } ) ; verify proc fails ( client @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ ) ; vt = client . call procedure ( __str__ @$ __str__ ) . get results ( ) [ __num__ ] ; validate table of scalar longs ( vt @$ new long [ ] { __num__ } ) ;,cases test
previously we cloned the artifact @$ but it is more efficient to just update the artifact scope if problems are later discovered that the original object needs its original artifact scope <PLACE_HOLDER> @$ cloning may again be appropriate,nearest artifact . set scope ( farthest artifact . get scope ( ) ) ;,object needs
this is okay . the function has a <PLACE_HOLDER> @$ but it is empty .,break ; case param_list :,function has
two different properties can satisfy the routing field <PLACE_HOLDER>,chained field extractor . no value handler routing response = chained field extractor . no value handler . skip ; list < field extractor > routings = new array list < field extractor > ( __num__ ) ; if ( settings . get mapping routing ( ) != null ) { settings . set property ( constant field extractor . property @$ settings . get mapping routing ( ) ) ; field extractor extractor = object utils . < field extractor > instantiate ( settings . get mapping routing extractor class name ( ) @$ settings ) ; routing response = chained field extractor . no value handler . not_found ; routings . add ( extractor ) ; },properties satisfy
global declaration must have a <PLACE_HOLDER>,if ( name attr == null ) { report schema error ( __str__ @$ new object [ ] { __str__ @$ __str__ } @$ elm node ) ; name attr = no_name ; } attr grp . f name = name attr ; attr grp . f target namespace = schema doc . f target namespace ;,declaration have
if they have a display title use only display <PLACE_HOLDER> @$ otherwise use our best bets,if ( ! text utils . is empty ( display text ) ) { text [ __num__ ] = display text ; text [ __num__ ] = get text ( metadata_key_display_subtitle ) ; text [ __num__ ] = get text ( metadata_key_display_description ) ; } else { int text index = __num__ ; int key index = __num__ ; while ( text index < text . length && key index < preferred_description_order . length ) { char sequence next = get text ( preferred_description_order [ key index ++ ] ) ; if ( ! text utils . is empty ( next ) ) { text [ text index ++ ] = next ; } } },title use
step 2 relays the <PLACE_HOLDER> from step 1 to step 3,thread step2 = new step2 ( context ) ; step2 . start ( ) ;,step relays
the list includes odd <PLACE_HOLDER>,assert that ( selected @$ has item ( entry_0001 ) ) ;,list includes
wait a little bit to let the delete take <PLACE_HOLDER> .,thread . sleep ( __num__ ) ;,delete take
wait until the task created the <PLACE_HOLDER>,long deadline = system . current time millis ( ) + __num__ ; while ( ! temp test file . exists ( ) && system . current time millis ( ) < deadline ) { thread . sleep ( __num__ ) ; } assert true ( __str__ @$ temp test file . exists ( ) ) ;,task created
modifier letter glottal <PLACE_HOLDER>..modifier letter reversed glottal <PLACE_HOLDER>,if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ch == __num__ ) { return true ; } else if ( ch == __num__ ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else { return false ;,letter reversed
the top of the stack always represents the <PLACE_HOLDER> filter . when back is pressed @$ the top is popped off and the new top indicates what filter to use . if there are no filters remaining @$ the activity itself is closed .,if ( m back stack . size ( ) > __num__ ) { m back stack . pop ( ) ; m download manager ui . update for url ( m back stack . peek ( ) ) ; } else { if ( ! m back stack . is empty ( ) ) m back stack . pop ( ) ; super . on back pressed ( ) ; },top represents
partitioned tables do n't have table <PLACE_HOLDER> set on the fetch task . instead they have a list of partition <PLACE_HOLDER> objects @$ each with a table <PLACE_HOLDER> . let 's try to fetch the <PLACE_HOLDER> for the first partition and use it 's deserializer .,if ( td == null && ft . get work ( ) != null && ft . get work ( ) . get part desc ( ) != null ) { if ( ft . get work ( ) . get part desc ( ) . size ( ) > __num__ ) { td = ft . get work ( ) . get part desc ( ) . get ( __num__ ) . get table desc ( ) ; } } if ( td == null ) { log . info ( __str__ ) ; } else { string table name = __str__ ; list < field schema > lst = null ; try { lst = hive meta store utils . get fields from deserializer ( table,tables have
when clearing the selection after a text change @$ state is not reset correctly so hitting down again will cause it to start from the previous selection point . we still have to send the key down event to let the list view items take <PLACE_HOLDER> @$ but then we select the first item explicitly .,if ( m suggestion list . get selected item position ( ) == list view . invalid_position ) { boolean result = m suggestion list . on key down ( key code @$ event ) ; m suggestion list . set selection ( __num__ ) ; return result ; } else { return m suggestion list . on key down ( key code @$ event ) ; },items take
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
if the option has a <PLACE_HOLDER> and a non blank argname,if ( option . has arg ( ) && ( option . get arg name ( ) == null || option . get arg name ( ) . length ( ) != __num__ ) ) { buff . append ( option . get opt ( ) == null ? long opt separator : __str__ ) ; buff . append ( __str__ ) . append ( option . get arg name ( ) != null ? option . get arg name ( ) : get arg name ( ) ) . append ( __str__ ) ; },option has
ensure the insertion logic can handle a <PLACE_HOLDER> appended to the front,do in hibernate ( this :: session factory @$ s -> { query < employee > query = s . create query ( __str__ @$ employee . class ) . set comment ( __str__ ) . add query hint ( __str__ ) . set parameter ( __str__ @$ __str__ ) ; list < employee > results = query . list ( ) ; assert equals ( results . size ( ) @$ __num__ ) ; } ) ; sql statement interceptor . assert executed count ( __num__ ) ; assert that ( sql statement interceptor . get sql queries ( ) . get ( __num__ ) @$ contains string ( __str__ ) ) ; sql statement interceptor . clear ( ) ;,logic handle
flag undeclared <PLACE_HOLDER> for checking after the dtd is fully processed,if ( notations . get ( external id . notation ) == null ) notations . put ( external id . notation @$ boolean . true ) ;,flag undeclared
equal class with one maybe a primitive @$ the later explicit cast arguments will solve this <PLACE_HOLDER>,continue ;,class solve
if we 're in system server and in a binder transaction we need to clear the calling uid . this works around code in system server that did not call clear calling <PLACE_HOLDER> @$ previously this was n't needed because reading settings did not do permission checking but thats no longer the case . long term this should be removed and callers should properly,if ( settings . is in system server ( ) && binder . get calling uid ( ) != process . my uid ( ) ) { final long token = binder . clear calling identity ( ) ; try { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ) ; } finally { binder . restore calling identity ( token ) ; } } else { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ),callers call
check that slave satisfies min <PLACE_HOLDER> .,if ( cpus < cluster props . min cpu per node ( ) || mem < cluster props . min memory per node ( ) ) { log . log ( level . fine @$ __str__ @$ offer . get resources list ( ) ) ; return null ; } double total cpus = __num__ ; double total mem = __num__ ; double total disk = __num__ ;,slave satisfies
wake up the reader ... there 's stuff to do @$ <PLACE_HOLDER> to read,has read ahead = false ; lock support . unpark ( this ) ;,stuff "s"
let 's test the default <PLACE_HOLDER>,lst . add ( __str__ ) ;,"s" test
the ' : ' at pos p may be either a port divider or a part of an i pv 6 <PLACE_HOLDER>,if ( p > target . last index of ( __str__ ) ) { target = target . substring ( __num__ @$ p ) ; },part pv
when final state in last <PLACE_HOLDER> is not default @$ it means last <PLACE_HOLDER> has finished . create a new <PLACE_HOLDER> for this node @$ and add it to the <PLACE_HOLDER> list . return this new <PLACE_HOLDER> . when final state in last <PLACE_HOLDER> is default @$ it means last <PLACE_HOLDER> has not finished . just get last <PLACE_HOLDER> .,if ( node allocation . get final allocation state ( ) != allocation state . default ) { node allocation = new node allocation ( nodeid ) ; node allocations . add ( node allocation ) ; },return create
for now @$ simply assert since this class has a hardcoded <PLACE_HOLDER>,if ( ! get name ( ) . equals ( name ) ) { throw new runtime exception ( __str__ + get name ( ) + __str__ + name + __str__ ) ; },class has
if the test specifies a specific <PLACE_HOLDER> @$ do not re<PLACE_HOLDER> .,return get description ( ) . get annotation ( fix method order . class ) != null ;,test specifies
simple equals can cause <PLACE_HOLDER> here because of how equals works e.g . between lists and sets .,return collection utils . is equal collection ( state objects @$ that . state objects ) ;,equals cause
j 2 se does not support xalan <PLACE_HOLDER>,do exit ( msg ) ;,se support
then the write behind action gets 3 <PLACE_HOLDER> to write,awaitility . await ( ) . until true ( writer called ) ; assert . assert equals ( __num__ @$ number of entries . int value ( ) ) ;,write gets
first read existing <PLACE_HOLDER> from the <PLACE_HOLDER> table . because we 'll be deleting entries for missing <PLACE_HOLDER> as we go @$ we need to query the database in small batches @$ to avoid problems with cursor window positioning .,if ( prescan files ) { long last id = long . min_value ; uri limit uri = m files uri . build upon ( ) . append query parameter ( media store . param_limit @$ __str__ ) . build ( ) ; while ( true ) { selection args [ __num__ ] = __str__ + last id ; if ( c != null ) { c . close ( ) ; c = null ; } c = m media provider . query ( limit uri @$ files_prescan_projection @$ where @$ selection args @$ media store . files . file columns . _id @$ null ) ; if ( c == null ) { break ; } int num = c . get count ( ) ;,first read
this happens at least if the server is restarted without preserving the session . after restart the client reconnects @$ gets a session expired <PLACE_HOLDER> and then closes the connection and ends up here,get logger ( ) . log ( level . finer @$ __str__ @$ e ) ; return ;,session expired
run the <PLACE_HOLDER> build a new <PLACE_HOLDER>,subprocedure subproc = member . create subprocedure ( op @$ data ) ; member . submit subprocedure ( subproc ) ;,operation build
and has a <PLACE_HOLDER>,select conf . set ( csv_input_header @$ csv_header_opt_use ) ; select conf . set boolean ( select_errors_include_sql @$ true ) ; input must ( select conf @$ csv_input_header @$ csv_header_opt_use ) ; input must ( select conf @$ select_input_format @$ select_format_csv ) ; input must ( select conf @$ select_output_format @$ select_format_csv ) ; input must ( select conf @$ select_input_compression @$ compression_opt_gzip ) ;,and has
robo vm note : seems like the gc can not reclaim the <PLACE_HOLDER> after this method finishes . it 's probably still referenced by some register . clear it to make its elements reclaimable .,list . clear ( ) ; return true ;,gc reclaim
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
pairwise : init ouput col has <PLACE_HOLDER> @$ init ouput col is repeating @$ column has <PLACE_HOLDER> @$ column is repeating,for ( boolean [ ] test matrix : new boolean [ ] [ ] { { false @$ true @$ true @$ true } @$ { false @$ false @$ false @$ false } @$ { true @$ false @$ true @$ false } @$ { true @$ true @$ false @$ false } @$ { true @$ false @$ false @$ true } } ) { string test case = template string ; test case = test case . replace all ( __str__ @$ __str__ + vector exp class name + create null repeating name fragment ( __str__ @$ test matrix [ __num__ ] @$ test matrix [ __num__ ] ) + create null repeating name fragment ( __str__ @$ test matrix [ __num__ ] @$ test,col has
the anonymous class ca n't have <PLACE_HOLDER> @$ but we may be binding <PLACE_HOLDER> from super classes,if ( clazz . is anonymous class ( ) ) { if ( clazz . get interfaces ( ) . length != __num__ ) { type parameters = clazz . get interfaces ( ) [ __num__ ] . get type parameters ( ) ; } else { type parameters = clazz . get superclass ( ) . get type parameters ( ) ; } } else { type parameters = clazz . get type parameters ( ) ; },class have
email 2 specifies a <PLACE_HOLDER>,if ( email2 . starts with ( __str__ ) ) { if ( within domain ( _sub @$ email2 ) ) { union . add ( email2 ) ; } else { union . add ( email1 ) ; union . add ( email2 ) ; } } else { if ( _sub . equals ignore case ( email2 ) ) { union . add ( email2 ) ; } else { union . add ( email1 ) ; union . add ( email2 ) ; } },email specifies
the configuration distinguisher is only set by apple <PLACE_HOLDER> transition and apple binary transition @$ both of which also set the <PLACE_HOLDER> and the cpu to apple ones . so we are fine not doing anything .,if ( apple options . configuration distinguisher != configuration distinguisher . unknown ) { return build options ; },which set
accessibility needs to be able to interact with the device . if these settings are already configured @$ we will not overwrite them . if they are already set @$ it means that the user has performed a global <PLACE_HOLDER> to enable accessibility or set these settings in the accessibility portion of the setup wizard @$ and definitely needs these features working after the,switch ( name ) { case settings . secure . accessibility_enabled : case settings . secure . touch_exploration_enabled : case settings . secure . accessibility_display_daltonizer_enabled : case settings . secure . accessibility_display_magnification_enabled : case settings . secure . accessibility_display_magnification_navbar_enabled : return settings . secure . get int ( m context . get content resolver ( ) @$ name @$ __num__ ) != __num__ ; case settings . secure . touch_exploration_granted_accessibility_services : case settings . secure . enabled_accessibility_services : case settings . secure . accessibility_display_daltonizer : return ! text utils . is empty ( settings . secure . get string ( m context . get content resolver ( ) @$ name ) ) ; case settings . secure . accessibility_display_magnification_scale : float default scale = m context . get,user performed
this is a 'patch configuration ' which considers <PLACE_HOLDER> ' as default collection,remote instance instance = new remote instance ( target baseurl + __str__ @$ null @$ null @$ __num__ @$ trust self signed on authenticated server @$ long . max_value @$ false ) ;,which considers
tab beyond last <PLACE_HOLDER> : add a <PLACE_HOLDER> to table !,if ( active table row >= maxrows ) { table item item = new table item ( table @$ swt . none @$ active table row ) ; item . set text ( __num__ @$ __str__ ) ; set row nums ( ) ; } if ( sel ) { edit ( active table row @$ active table column ) ; } table . set focus ( ) ;,tab add
the rollback operation should have rolled back the first nn 's local <PLACE_HOLDER> @$ and the shared dir @$ but not the other nn 's <PLACE_HOLDER> . those have to be done by bootstrapping the standby .,check nn previous dir existence ( cluster @$ __num__ @$ false ) ; check jn previous dir existence ( qj cluster @$ false ) ; if ( fs != null ) { fs . close ( ) ; } if ( qj cluster != null ) { qj cluster . shutdown ( ) ; },operation rolled
test results of within using contains <PLACE_HOLDER>,prefix = __str__ ; sql = __str__ + __str__ + __str__ ; vt1 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; sql = __str__ + __str__ + __str__ ; vt2 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; assert tables are equal ( prefix @$ vt2 @$ vt1 @$ geography_distance_epsilon ) ; sql = __str__ + __str__ + __str__ ; vt1 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; sql = __str__ + __str__ + __str__ ; vt2 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__,results contains
hosted providers index will match ' p ' attribute in widget 's entry in the xml file being restored if there 's no live entry for this provider @$ add an inactive <PLACE_HOLDER> so that widget i ds referring to them can be properly allocated,string cl = parser . get attribute value ( null @$ __str__ ) ;,index match
res stores the first found abstract <PLACE_HOLDER>,method res = null ; for ( method mi : methods ) { if ( ! modifier . is abstract ( mi . get modifiers ( ) ) ) continue ; if ( mi . get annotation ( traits . implemented . class ) != null ) continue ; try { object . class . get method ( mi . get name ( ) @$ mi . get parameter types ( ) ) ; continue ; } catch ( no such method exception e ) { } if ( res != null ) return null ; res = mi ; },stores found
scope might have set to database in some previous iteration of loop @$ so reset it to false if database tracker has no <PLACE_HOLDER> .,scope . database = false ;,tracker has
standalone server does n't need myid <PLACE_HOLDER> .,if ( ! my id file . is file ( ) ) { return ; } buffered reader br = new buffered reader ( new file reader ( my id file ) ) ; string my id string ; try { my id string = br . read line ( ) ; } finally { br . close ( ) ; } try { server id = long . parse long ( my id string ) ; mdc . put ( __str__ @$ my id string ) ; } catch ( number format exception e ) { throw new illegal argument exception ( __str__ + my id string + __str__ ) ; },server need
this estimate will not take into account the <PLACE_HOLDER> saved by inlining the keys .,return vm thin disk region entry off heap object key . class ;,estimate take
let 's traverse the <PLACE_HOLDER> and override all text colors !,stack < view > views to process = new stack < > ( ) ; views to process . add ( root ) ; while ( ! views to process . is empty ( ) ) { view v = views to process . pop ( ) ; if ( v instanceof text view ) { text view text view = ( text view ) v ; text view . set text ( contrast color util . clear color spans ( text view . get text ( ) ) ) ; text view . set text color ( text color ) ; } if ( v instanceof view group ) { view group view group = ( view group ) v ; for ( int i = __num__,"s" traverse
for editable combos the editor component has the <PLACE_HOLDER> not the combo box its self @$ so we should make the combo paint <PLACE_HOLDER>ed when its editor has <PLACE_HOLDER>,int basic state = synth look and feel . get component state ( c ) ; if ( box . is editable ( ) && box . get editor ( ) . get editor component ( ) . is focus owner ( ) ) { basic state |= focused ; } return basic state ;,editor has
else the average cluster latency is between low and high @$ or we ca n't change <PLACE_HOLDER> more @$ then do not change <PLACE_HOLDER> .,if ( new current avg cluster latency > __num__ && total cluster call count >= config . get min cluster call count low water mark ( ) ) { if ( new current avg cluster latency <= config . get low water mark ( ) && current override drop rate != __num__ ) { new drop level = math . max ( __num__ @$ new drop level - config . get global step down ( ) ) ; } } else { new drop level = math . max ( __num__ @$ new drop level - config . get global step down ( ) ) ; },then change
'digest ' already has the <PLACE_HOLDER> from the stream @$ just finish the op,byte [ ] sd = digest . digest ( ) ; digest . reset ( ) ;,"digest" has
start tx state update during pre commit <PLACE_HOLDER>,set updating tx state during pre commit ( true ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ + __str__ @$ secondary transactional operations . size ( ) ) ; },update commit
need to roll the wals and make the replication source for this peer track the new <PLACE_HOLDER> . if we do not do this @$ there will be two problems that can not be addressed at the same time . first @$ if we just throw away the current wal <PLACE_HOLDER> @$ and later when we transit the peer to da @$ and the,log roller roller = rs . get wal roller ( ) ; roller . request roll all ( ) ; try { roller . wait until wal roll finished ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; throw ( io exception ) new interruptedio exception ( __str__ ) . init cause ( e ) ; },source track
recursive evaluation should not trigger a suspended <PLACE_HOLDER>,context . eval ( test source ) ; suspension count . increment and get ( ) ;,evaluation trigger
never registered before if null ... also as of current @$ there is ever only 1 notification type per m bean @$ so we do need need a while <PLACE_HOLDER> here,if ( notifications != null ) { set < map . entry < refresh notification type @$ integer > > entries = notifications . entry set ( ) ; for ( map . entry < refresh notification type @$ integer > e : entries ) { integer timer notification id = e . get value ( ) ; if ( null != timer notification id ) { try { refresh timer . remove notification ( timer notification id ) ; } catch ( instance not found exception xptn ) { log stack trace ( level . debug @$ xptn ) ; } } } try { if ( mbean server != null && mbean server . is registered ( refresh timer object name ) ) { mbean server,need need
add the previous certificate as a trust anchor . if prev cert is not null @$ we want to construct a trust anchor using the cert object because when the certpath for the crl is built later @$ the cert selector will make <PLACE_HOLDER> with the trust anchor 's trusted cert member rather than its pub key .,if ( prev key != null ) { trust anchor temporary ; if ( prev cert != null ) { temporary = new trust anchor ( prev cert @$ null ) ; } else { x500 principal principal = cert impl . get issuerx500 principal ( ) ; temporary = new trust anchor ( principal @$ prev key @$ null ) ; } new trust anchors . add ( temporary ) ; },selector make
empty if nobody has the specified global <PLACE_HOLDER>,query = permission query . builder ( ) . set organization uuid ( organization . get uuid ( ) ) . set permission ( __str__ ) . build ( ) ; expect permissions ( query @$ empty list ( ) ) ;,nobody has
unknown slot which basically means the exists <PLACE_HOLDER>,if ( ! ( predicates . length == __num__ && predicates [ __num__ ] instanceof exists predicate ) ) { throw new illegal state exception ( __str__ + arrays . to string ( predicates ) ) ; } return true ;,which means
phantom reference does not allow <PLACE_HOLDER> to its object @$ so it is mostly useless to have a phantom reference on the image heap . but some jdk code uses it @$ e.g. @$ for marker values @$ so we can not disallow phantom reference for the image heap .,if ( receiver instanceof phantom reference ) { return null ; },reference allow
ipv 4 occupies last two <PLACE_HOLDER>,if ( index > octets . length - __num__ || index > __num__ ) { return false ; },ipv occupies
controller and mode events have status byte 0 x <PLACE_HOLDER> @$ where c is the channel they are sent on .,if ( ( status & __num__ ) == __num__ ) { for ( int i = __num__ ; i < count ; i ++ ) { try { ( ( controller event listener ) event info . get listener ( i ) ) . control change ( event ) ; } catch ( throwable t ) { if ( printer . err ) t . print stack trace ( ) ; } } } return ;,events have
set timestamp before moving to cmroot @$ so we can avoid race condition cm remove the <PLACE_HOLDER> before setting timestamp,long now = system . current time millis ( ) ; fs . set times ( path @$ now @$ - __num__ ) ; boolean success = false ; if ( fs . exists ( cm path ) && file check sum . equals ignore case ( checksum for ( cm path @$ fs ) ) ) { success = false ; } else { switch ( type ) { case move : { log . info ( __str__ @$ path . to string ( ) @$ cm path . to string ( ) ) ; success = fs . rename ( path @$ cm path ) ; break ; } case copy : { log . info ( __str__ @$ path . to string ( ) @$,cm remove
for mwt created core labels set <PLACE_HOLDER> of them to the character offsets of the mwt,int lastmwt char begin = - __num__ ; int lastmwt char end = - __num__ ; for ( core label cl : core labels ) { if ( sentence . mwt data . contains key ( cl . index ( ) - __num__ ) ) { if ( sentence . mwt data . get ( cl . index ( ) - __num__ ) == processedmwt tokens ) { cl . set begin position ( doc . doc text . length ( ) ) ; doc . doc text += sentence . mwt tokens . get ( processedmwt tokens ) ; cl . set end position ( doc . doc text . length ( ) ) ; lastmwt char begin = cl . begin position ( ) ; lastmwt,labels set
if the container is running a health <PLACE_HOLDER> @$ any previous image failure has been resolved,reset image failure ( ) ; update state ( healthchecking ) ;,container running
optimized for code which will add the same value again and again @$ more specifically this is used to add new <PLACE_HOLDER> for watcher @$ and the same watcher may watching thousands or even millions of nodes @$ which will call add the same value of this function @$ check exist using read lock will optimize the performance here .,integer bit = get bit ( value ) ; if ( bit != null ) { return bit ; } rw lock . write lock ( ) . lock ( ) ; try { bit = value2 bit . get ( value ) ; if ( bit != null ) { return bit ; } bit = freed bit set . next set bit ( __num__ ) ; if ( bit > - __num__ ) { freed bit set . clear ( bit ) ; } else { bit = next bit ++ ; } value2 bit . put ( value @$ bit ) ; bit2 value . put ( bit @$ value ) ; return bit ; } finally { rw lock . write lock ( ),which add
inspect the source c es . just copy <PLACE_HOLDER> if none are modified . otherwise copy to modified c es @$ with modifications .,boolean is modified = false ; for ( int i = __num__ ; i < length ; ++ i ) { long srcce = srcc es [ src index + i ] ; long ce = modifier . modifyce ( srcce ) ; if ( ce == collation . no_ce ) { if ( is modified ) { modifiedc es [ i ] = srcce ; } } else { if ( ! is modified ) { for ( int j = __num__ ; j < i ; ++ j ) { modifiedc es [ j ] = srcc es [ src index + j ] ; } is modified = true ; } modifiedc es [ i ] = ce ; } } if ( is modified,es copy
mark a fragment as completing @$ but do n't actually complete it yet . the wait queue should now have <PLACE_HOLDER> to accept one more fragment .,task executor service . fragment completing ( r1 . get request id ( ) @$ scheduler fragment completing listener . state . success ) ; submission state = task executor service . schedule ( r4 ) ; assert equals ( scheduler . submission state . accepted @$ submission state ) ; assert equals ( __num__ @$ task executor service . wait queue . size ( ) ) ; assert equals ( __num__ @$ task executor service . completing fragment map . size ( ) ) ; r1 . complete ( ) ; r1 . await end ( ) ;,queue have
make sure load balancer does not recreate the server <PLACE_HOLDER>,assert true ( server instanceof discovery enabled server ) ;,balancer recreate
ima sometimes unexpectedly decreases the ad <PLACE_HOLDER> in an ad group .,log . w ( tag @$ __str__ + ad count + __str__ + old ad count ) ;,ima decreases
label visitor has some legacy special <PLACE_HOLDER> of output files .,if ( target instanceof output file ) { rule rule = ( ( output file ) target ) . get generating rule ( ) ; observe edge ( target @$ null @$ rule ) ; visit ( null @$ null @$ rule @$ depth + __num__ @$ count + __num__ ) ; } label visitation utils . visit target exceptionally ( target @$ edge filter @$ ( from target @$ attribute @$ to label ) -> enqueue target ( target @$ attribute @$ to label @$ depth @$ count ) ) ;,visitor has
if right expression is a closure expression @$ store parameter type <PLACE_HOLDER>,if ( left expression instanceof variable expression ) { if ( right expression instanceof closure expression ) { parameter [ ] parameters = ( ( closure expression ) right expression ) . get parameters ( ) ; left expression . put node meta data ( static types marker . closure_arguments @$ parameters ) ; } else if ( right expression instanceof variable expression && ( ( variable expression ) right expression ) . get accessed variable ( ) instanceof expression && ( ( expression ) ( ( variable expression ) right expression ) . get accessed variable ( ) ) . get node meta data ( static types marker . closure_arguments ) != null ) { variable target variable = find target variable ( ( variable expression ),store parameter
lets avoid an infinite loop if we are given a bad <PLACE_HOLDER>ion strategy for a bad strategy lets just not <PLACE_HOLDER>,if ( messages to evict == __num__ ) { log . warn ( __str__ @$ new object [ ] { destination @$ message eviction strategy @$ list . size ( ) } ) ; break ; },strategy lets
actions will consume key <PLACE_HOLDER> @$ so do n't process them,if ( enable action key bindings ) { return ; },actions consume
set max object size a little less than 1024 1024 @$ because the key of the segment result cache is long if set to 1024 1024 will cause memcached client exceed max size <PLACE_HOLDER>,memcached cache config . set max object size ( __num__ ) ; memcached cache config . set hosts ( config hosts ) ;,client exceed
some controls include <PLACE_HOLDER> based on the view dimensions @$ so update now .,update controls ( ) ; surface view sv = ( surface view ) find view by id ( r . id . hardware scaler_surface view ) ; m render thread = new render thread ( sv . get holder ( ) ) ; m render thread . set name ( __str__ ) ; m render thread . start ( ) ; m render thread . wait until ready ( ) ; render handler rh = m render thread . get handler ( ) ; if ( rh != null ) { rh . send set flat shading ( m flat shading checked ) ; rh . send surface created ( ) ; },controls include
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default text snippet global profile = new crawl profile ( crawl_profile_snippet_global_text @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_snippet_global_text_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ true @$ false @$ true @$ true @$ true @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . ifexist @$ __str__ + crawl_profile_snippet_global_text @$ client identification . yacy intranet crawler agent name @$ null,url match
only post the callback to stop the service if the service does not have an open <PLACE_HOLDER> .,if ( m opened count . decrement and get ( ) != __num__ ) { return ; },service have
send message to all peers to find out who hosts the <PLACE_HOLDER>,if ( ! tx . is real deal local ( ) ) { find remotetx message reply processor processor = send find remotetx message ( server connection . get cache ( ) @$ tx id ) ; try { processor . wait for replies uninterruptibly ( ) ; } catch ( reply exception e ) { e . handle cause ( ) ; } internal distributed member hosting member = processor . get hosting member ( ) ; if ( hosting member != null ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ hosting member ) ; } if ( tx . get target ( ) == null ) { tx . set target ( hosting member ),who hosts
this is necessary to have the rm respect our vcore allocation <PLACE_HOLDER>,conf . set class ( capacity scheduler configuration . resource_calculator_class @$ dominant resource calculator . class @$ resource calculator . class ) ; conf . set boolean ( yarn configuration . nm_disk_health_check_enable @$ false ) ; miniyarn cluster = new miniyarn cluster ( test dynamometer infra . class . get name ( ) @$ __num__ @$ minicluster_num_nms @$ __num__ @$ __num__ ) ; miniyarn cluster . init ( conf ) ; miniyarn cluster . start ( ) ; yarn conf = miniyarn cluster . get config ( ) ; minidfs cluster = new minidfs cluster . builder ( conf ) . format ( true ) . num data nodes ( minicluster_num_dns ) . build ( ) ; minidfs cluster . wait cluster up ( ) ; file system,rm respect
trivially : all recipients received the <PLACE_HOLDER>,return null ;,recipients received
jls does not allow array <PLACE_HOLDER> to be used as bounds of type variables,return false ;,jls allow
grab plugin executions that are bound to the selected lifecycle phases from project . the effective model of the project already contains the plugin executions induced by the project 's packaging type . remember @$ all phases of interest and only those are in the lifecycle mapping @$ if a phase has no <PLACE_HOLDER> in the map @$ we are not interested in any,for ( plugin plugin : project . get build ( ) . get plugins ( ) ) { for ( plugin execution execution : plugin . get executions ( ) ) { if ( execution . get phase ( ) != null ) { map < integer @$ list < mojo execution > > phase bindings = mappings . get ( execution . get phase ( ) ) ; if ( phase bindings != null ) { for ( string goal : execution . get goals ( ) ) { mojo execution mojo execution = new mojo execution ( plugin @$ goal @$ execution . get id ( ) ) ; mojo execution . set lifecycle phase ( execution . get phase ( ) ) ; add mojo,phase has
setup filesystem and buck <PLACE_HOLDER> .,path canonical root path = project root . to real path ( ) . normalize ( ) ; immutable map < cell name @$ path > root cell mapping = get cell mapping ( canonical root path ) ; immutable list < string > args = buck args methods . expand at files ( unexpanded command line args @$ root cell mapping ) ;,setup filesystem
now @$ try increasing the reservation <PLACE_HOLDER> and it should fail,try { store definition defa = test utils . make store definition ( __str__ @$ __num__ ) ; bdb storage . update ( defa ) ; fail ( __str__ ) ; } catch ( storage initialization exception sie ) { },try increasing
iterate over alluxio <PLACE_HOLDER> and process persisted <PLACE_HOLDER> .,for ( map . entry < string @$ inode > inode entry : inode children . entry set ( ) ) { if ( ! inode entry . get value ( ) . is persisted ( ) ) { continue ; } try ( locked inode path descendant = inode path . lock descendant ( inode path . get uri ( ) . join unsafe ( inode entry . get key ( ) ) @$ lock pattern . write_edge ) ) { if ( sync descendant type != descendant type . all ) { sync descendant type = descendant type . none ; } sync result sync result = sync inode metadata ( rpc context @$ descendant @$ sync descendant type @$ status cache ) ; paths to,iterate persisted
do this last because the prior operations could throw <PLACE_HOLDER> .,maybe set up metrics recorder ( context @$ configs ) ;,operations throw
the facade only contains the fallback <PLACE_HOLDER>,assert . assert true ( router facade . global policy map . contains key ( def queue key ) && router facade . global policy map . size ( ) == __num__ ) ;,facade contains
here client opens initial <PLACE_HOLDER> and fetches topology . only first server in list should be contacted .,try ( grid client client = client ( ) ) { assert equals ( __num__ @$ srvs [ __num__ ] . get connect count ( ) ) ; for ( int i = __num__ ; i < client test rest server . servers_cnt ; i ++ ) assert equals ( __num__ @$ srvs [ i ] . get connect count ( ) ) ; srvs [ __num__ ] . reset counters ( ) ; int contacted srv = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { int failed = contacted srv ; srvs [ failed ] . fail ( ) ; while ( true ) try { client . compute ( ) . refresh topology ( false @$ false,client opens
the two consume the same p <PLACE_HOLDER> and can exist in the same stage .,if ( existing consumers . stream ( ) . all match ( collection consumer -> greedyp collection fusers . is compatible ( collection consumer . consuming transform ( ) @$ new consumer . consuming transform ( ) @$ pipeline ) ) ) { existing consumers . add ( new consumer ) ; found siblings = true ; break ; },two consume
setup the full outer intersect map join 's input obj <PLACE_HOLDER> to include the small table @$ etc .,intersect map join operator . set input obj inspectors ( intercept test desc . input object inspectors ) ;,setup map
this is taken care in wrapper which generates xni <PLACE_HOLDER> @$ there are no next events,throw new java . io . eof exception ( ) ;,which generates
get twitter created a flow <PLACE_HOLDER> @$ then it 's sent via s 2 s,tc . add lineage ( create lineage ( prs @$ __num__ @$ __num__ @$ __num__ ) ) ; test ( tc ) ; wait notifications get delivered ( ) ; final lineage lineage = get lineage ( ) ; final node flow = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patha = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathb = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathc = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patht = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathi = lineage . find node (,twitter created
add the input file string and date <PLACE_HOLDER> to lists,file strings . add ( input line ) ; dt objects . add ( calculateddt ) ;,input file
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text specifies
we 'll be able to remove this once we have proper <PLACE_HOLDER>d key subclasses @$ like frame key . set the new <PLACE_HOLDER> both in schema and in schema v 3 @$ so that they are always in sync .,string new type = __str__ + get keyed class type ( ) + __str__ ; __meta . schema_type = new type ; set schema type_do not call ( new type ) ;,3 set
this happens when the code inside the jmx bean threw an <PLACE_HOLDER> @$ so log it and do n't output the bean .,log . error ( __str__ + qry + __str__ + oname @$ e ) ; continue ;,code threw
ideally @$ we would just install the next phase using the singleton policy @$ however <PLACE_HOLDER> unit phases do not currently support restarts restart the <PLACE_HOLDER> using the attached phase builder @$ but only if a builder was not already attached,if ( unit . put attachment ( attachments . deployment_unit_phase_builder @$ new singleton deployment unit phase builder ( support @$ policy ) ) == null ) { singleton logger . root_logger . singleton deployment detected ( policy ) ; service controller < ? > controller = context . get service registry ( ) . get required service ( unit . get service name ( ) ) ; controller . add listener ( this ) ; controller . set mode ( mode . never ) ; },restarts restart
if the request does n't have a session <PLACE_HOLDER> @$ we 're not going to renew one .,if ( ! request . get cookies ( ) . contains key ( session cookie name ) ) { return ; } cookie request cookie = request . get cookies ( ) . get ( session cookie name ) ; optional < user > optional user = authenticator . authenticate ( request cookie ) ; if ( optional user . is present ( ) ) { session login resource . cookies for user ( optional user . get ( ) ) . for each ( c -> response . get headers ( ) . add ( http headers . set_cookie @$ c ) ) ; },request have
none of the results got the data form the <PLACE_HOLDER> .,blocks . add ( no_data ) ;,data form
create a role filesystem which does not have read <PLACE_HOLDER> under a path it still has write <PLACE_HOLDER> @$ which can be explored in the final step to delete files and directories .,role config = create assumed role config ( ) ; bind role policy statements ( role config @$ statement_s3guard_client @$ statement_allow_sse_kms_rw @$ statement ( true @$ s3_all_buckets @$ s3_all_operations ) @$ new statement ( effects . deny ) . add actions ( s3_all_get ) . add resources ( directory ( no read dir ) ) ) ; readonlyfs = ( s3a file system ) base path . get file system ( role config ) ; verifys3 guard settings ( readonlyfs @$ __str__ ) ;,which read
callbacks may remove <PLACE_HOLDER> . perform a shallow copy to avoid concurrent modification .,entries = new array map < > ( m callbacks ) ;,callbacks remove
note to translators : jaxp is an acronym for the java api for xml processing . this message indicates that the xml parser provided to xsltc to process the xml input document had a configuration <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,parser had
we set <PLACE_HOLDER> to empty here explicitly @$ so that the lookup will succeed as user does n't know the exact <PLACE_HOLDER> .,for ( member response member response : response . member responses ( ) ) { member errors . put ( new member identity ( ) . set member id ( join group request . unknown_member_id ) . set group instance id ( member response . group instance id ( ) ) @$ errors . for code ( member response . error code ( ) ) ) ; },user know
both came from db @$ no <PLACE_HOLDER> to run this .,return ;,both came
most commands use the object <PLACE_HOLDER> for p 2 @$ nut not all,int param2 = config . get device ( ) . get properties ( ) . get number ( ) ; logger . debug ( __str__ + config . get object type ( ) ) ; switch ( config . get object type ( ) ) { case unit : { unit unit = ( unit ) config . get device ( ) ; boolean resend to children = false ; if ( command == on off type . on ) { cmd = omni link cmd . cmd_unit_on . get number ( ) ; } else if ( command == on off type . off ) { cmd = omni link cmd . cmd_unit_off . get number ( ) ; } else if ( command == increase decrease,commands use
run the k means <PLACE_HOLDER>,path answer = new path ( output @$ __str__ ) ; k means driver . run ( conf @$ data @$ initialclusters @$ answer @$ convergence delta @$ max iterations @$ true @$ __num__ @$ false ) ;,k means
empty string signifies broadest possible <PLACE_HOLDER>,owner . set last name ( __str__ ) ;,string signifies
changing the display name has surely brought a change in the order as well so let 's tell the <PLACE_HOLDER>,fire meta contact group event ( find parent meta contact group ( meta contact ) @$ null @$ null @$ meta contact group event . child_contacts_reordered ) ;,"s" tell
start data change <PLACE_HOLDER> from several threads .,final atomic boolean stopped = new atomic boolean ( ) ; ignite internal future update fut = multithreaded async ( new callable < void > ( ) { @ override public void call ( ) throws exception { while ( ! stopped . get ( ) ) { ignite node = grid ( thread local random . current ( ) . next int ( __num__ @$ __num__ ) ) ; int key = thread local random . current ( ) . next int ( __num__ @$ large_cache_size ) ; int val = thread local random . current ( ) . next int ( ) ; binary object key obj = key ( node @$ key ) ; if ( thread local random . current ( ) . next,data change
creating the handler starts the preview @$ which can also throw a runtime <PLACE_HOLDER> .,if ( handler == null ) { handler = new capture activity handler ( this @$ decode formats @$ decode hints @$ character set @$ camera manager ) ; } decode or store saved bitmap ( null @$ null ) ;,which throw
avoid infinite recursion @$ since installing a module always installs <PLACE_HOLDER>,if ( module instanceof checked provider methods module ) { return modules . empty_module ; } return new checked provider methods module ( module ) ;,module installs
return false because function types do not have a null <PLACE_HOLDER>,if ( definition argument properties . get ( i ) . get argument type ( ) == function_type ) { if ( invocation argument convention != invocation argument convention . function ) { return false ; } throw new unsupported operation exception ( __str__ ) ; },types have
first see if the file matches the regular <PLACE_HOLDER> !,if ( pattern source != null ) { matcher matcher = pattern source . matcher ( filename ) ; unzip = matcher . matches ( ) ; } if ( unzip ) { if ( ! unzip file ( children [ i ] @$ real targetdirectory @$ real wildcard @$ real wildcard exclude @$ result @$ parent job @$ movetodir @$ real movetodirectory ) ) { update errors ( ) ; } else { update success ( ) ; } },file matches
manager wants this <PLACE_HOLDER> to be paused,if ( get state ( ) == generic fragment . state_paused ) { return ; } if ( util . sdk_int > __num__ ) { perform initialization ( ) ; },manager wants
js 2 e does not throw this <PLACE_HOLDER>,key pair gen . initialize ( __num__ @$ new secure random ( ) ) ; try { key pair gen . initialize ( __num__ @$ new secure random ( ) ) ; fail ( __str__ ) ; } catch ( invalid parameter exception e ) { } key pair gen . initialize ( __num__ @$ null ) ; assert null ( __str__ @$ key pair gen . generate key pair ( ) ) ; assert null ( __str__ @$ key pair gen . gen key pair ( ) ) ; break ; case __num__ : key pair gen . initialize ( pp @$ new secure random ( ) ) ; key pair gen . initialize ( pp ) ; key pair gen . initialize ( __num__ @$ new,e throw
exclusion only makes <PLACE_HOLDER> for a union type .,if ( outcome . boolean values == boolean literal set . empty && get native type ( boolean_type ) . is subtype of ( type ) ) { if ( type . is union type ( ) ) { type = type . to maybe union type ( ) . get restricted union ( get native type ( boolean_type ) ) ; } },exclusion makes
the page supplier has incremented the page <PLACE_HOLDER> count @$ and add pages below also increments the <PLACE_HOLDER> count @$ so we need to drop the page supplier <PLACE_HOLDER> . the call de<PLACE_HOLDER> page is performed outside of synchronized to avoid making a callback while holding a lock .,page references = pages supplier . get pages ( max size ) ;,supplier incremented
deregister as a listener to reduce computational <PLACE_HOLDER>,remove event listener ( ) ;,listener reduce
2 nd time launch service to handle if service exist <PLACE_HOLDER>,system service . launch user service ( user services ) ; verify for launched user services ( ) ;,service exist
before inlining happens remove unused code sees one use of inner c @$ which prevents its <PLACE_HOLDER> . after inlining it sees ` this instanceof inner c ` as the only use of inner c. make sure remove unused code recognizes that the value of inner c escapes .,test ( options @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,which prevents
first handle special <PLACE_HOLDER> . if one of the special case methods can not handle it @$ it returns null .,vector expression ve = null ; if ( udf instanceof genericudf between ) { ve = get between expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf in ) { ve = get in expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf if ) { ve = get if expression ( ( genericudf if ) udf @$ child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf when ) { ve = get when expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudfop positive ) { ve = get identity expression ( child expr,one handle
corresponding user feature contains user <PLACE_HOLDER>,item vectors [ item index ] [ user_bias_index ] = __num__ ;,feature contains
idcs requires a web <PLACE_HOLDER> for redirects,routing . builder routing = routing . builder ( ) . register ( web security . create ( security @$ config . get ( __str__ ) ) ) . register ( oidc support . create ( config ) ) . register ( __str__ @$ jersey support . builder ( ) . register ( security feature . builder ( security ) . build ( ) ) . register ( jersey resource . class ) . build ( ) ) . get ( __str__ @$ ( req @$ res ) -> { optional < security context > security context = req . context ( ) . get ( security context . class ) ; res . headers ( ) . content type ( media type . text_plain . with charset,idcs requires
test release of resources : this code should throw an <PLACE_HOLDER> as the db is not available anymore in fact every resource which is used afterwards should throw an sql <PLACE_HOLDER> .,try { res . next ( ) ; fail ( __str__ ) ; } catch ( sql exception e ) { },release throw
each polling event will trigger a <PLACE_HOLDER> and comparison of config tree,return optional . of ( instant . now ( ) ) ;,event trigger
move file <PLACE_HOLDER> ahead !,data . filenr ++ ; data . readrow = get row ( ) ;,move file
by default @$ let 's just shift the <PLACE_HOLDER> offset .,do { final int x change = mx offset direction * burn_in_shift_step ; m last burn inx offset += x change ; if ( m last burn inx offset > m max horizontal burn in offset || m last burn inx offset < m min horizontal burn in offset ) { m last burn inx offset -= x change ; mx offset direction *= - __num__ ; final int y change = my offset direction * burn_in_shift_step ; m last burn iny offset += y change ; if ( m last burn iny offset > m max vertical burn in offset || m last burn iny offset < m min vertical burn in offset ) { m last burn iny offset -= y change ; my offset direction,let shift
get a <PLACE_HOLDER> @$ if collection has only one <PLACE_HOLDER>,string guava = iterables . get only element ( iterable ) ;,collection has
default starts one <PLACE_HOLDER> only .,test_util . get configuration ( ) . set boolean ( load balancer . tables_on_master @$ true ) ;,default starts
the token has now changed <PLACE_HOLDER> to having all windows shown ... what to do @$ what to do ?,if ( m freezing screen ) { show all windows locked ( ) ; stop freezing screen ( false @$ true ) ; if ( debug_orientation ) slog . i ( tag @$ __str__ + this + __str__ + m num interesting windows + __str__ + m num drawn windows ) ; set app layout changes ( finish_layout_redo_wallpaper @$ __str__ ) ; } else { set app layout changes ( finish_layout_redo_anim @$ __str__ ) ; if ( ! get display content ( ) . m opening apps . contains ( this ) && can show windows ( ) ) { show all windows locked ( ) ; } },token changed
now test <PLACE_HOLDER> should contain only old junit <PLACE_HOLDER> .,test classes . remove all ( new junit classes ) ; throw new j unit exception ( format ( __str__ + __str__ + __str__ @$ test clazz . get name ( ) @$ sorted classes ( new junit classes ) @$ sorted classes ( test classes ) ) ) ;,classes contain
this flow covers instance not found exception . actual <PLACE_HOLDER> just eating the exception . i.e actual <PLACE_HOLDER> just printing the stacktrace @$ whenever an exception of type instance not found exception occurs .,mbean container . destroy ( ) ;,exception found
0 : id 1 : indexed string add <PLACE_HOLDER>,db . execsql ( __str__ + constraint + __str__ + __str__ ) ;,string add
if this is a call to ya cys special search <PLACE_HOLDER> @$ enhance the query with field assignments,if ( ( response writer instanceof y json response writer || response writer instanceof opensearch response writer ) && __str__ . equals ( mmsp . get ( __str__ @$ __str__ ) ) ) { if ( ! mmsp . get map ( ) . contains key ( __str__ ) ) mmsp . get map ( ) . put ( __str__ @$ new string [ ] { q } ) ; if ( ! mmsp . get map ( ) . contains key ( __str__ ) ) mmsp . get map ( ) . put ( __str__ @$ new string [ ] { collection schema . description_txt . get solr field name ( ) + __str__ + collection schema . h4_txt . get solr field name ( ) +,ya cys
the bucket move will send a destroy region <PLACE_HOLDER> .,return new distribution message observer ( ) { private volatile boolean done ; @ override public void before send message ( cluster distribution manager dm @$ distribution message message ) { if ( message instanceof destroy region message && ! done ) { task . run ( ) ; done = true ; } } } ;,move send
test that the from row function simply returns the original <PLACE_HOLDER> back .,simplepojo extracted = registry . get from row function ( simplepojo . class ) . apply ( row ) ; assert same ( pojo @$ extracted ) ;,the returns
loading child elements modifies the <PLACE_HOLDER> of the attribute set 's underlying parser @$ so it needs to happen after obtaining attributes and extracting <PLACE_HOLDER>s .,if ( dr == null ) { int type ; while ( ( type = parser . next ( ) ) == xml pull parser . text ) { } if ( type != xml pull parser . start_tag ) { throw new xml pull parser exception ( parser . get position description ( ) + item_missing_drawable_error ) ; } if ( parser . get name ( ) . equals ( __str__ ) ) { dr = vector drawable compat . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else if ( sdk_int >= lollipop ) { dr = drawable . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else { dr = drawable,elements modifies
jump to second write back if not equal . reserve <PLACE_HOLDER> for the two jcc and first write back when calculating target address .,final long second write back offset = instructions . size ( ) + first write back . size ( ) + __num__ ; final string second write back goal = string . format ( __str__ @$ instruction . get address ( ) . to long ( ) @$ second write back offset ) ; instructions . add ( reil helpers . create jcc ( base offset + instructions . size ( ) @$ current size @$ comparison result @$ operand size . address @$ second write back goal ) ) ;,jump write
now run the k means <PLACE_HOLDER>,fuzzyk means driver . run ( testdata @$ new path ( output @$ __str__ ) @$ fuzzyk means output @$ __num__ @$ __num__ @$ __num__ @$ true @$ true @$ __num__ @$ true ) ; int num iterations = __num__ ; path clusters in = new path ( fuzzyk means output @$ __str__ ) ; representative points driver . run ( conf @$ clusters in @$ new path ( fuzzyk means output @$ __str__ ) @$ fuzzyk means output @$ measure @$ num iterations @$ true ) ; representative points driver . print representative points ( fuzzyk means output @$ num iterations ) ; cluster evaluator evaluator = new cluster evaluator ( conf @$ clusters in ) ;,k means
first invoke on a method which has a specific <PLACE_HOLDER> and that invocation should pass,final string caller principal name = deny access bean . method with specific role ( ) ; assert . assert equals ( __str__ @$ __str__ @$ caller principal name ) ;,which has
when filtering @$ let the server decide the <PLACE_HOLDER> unless we 've set the filter to empty and explicitly said that we want to see the results starting from <PLACE_HOLDER> 0 .,if ( ! filter . equals ( last filter ) ) { if ( filter . is empty ( ) && page != __num__ ) { page = - __num__ ; } else { page = __num__ ; } },server decide
first assume there are no named capture backreferences @$ because the spec says they should only be recognized if the pattern contains at least <PLACE_HOLDER> named capture group .,class parser { int pos ; int num capturing groups ; set < string > capturing group names = new hash set < > ( ) ; final int limit = pattern . length ( ) ; boolean look for named capture backreferences ; reg exp tree parse top level ( ) { this . pos = __num__ ; this . num capturing groups = __num__ ; this . look for named capture backreferences = false ; reg exp tree out = parse ( ) ; if ( ! capturing group names . is empty ( ) ) { this . pos = __num__ ; this . num capturing groups = __num__ ; this . look for named capture backreferences = true ; out = parse ( ),pattern contains
3 elements and each element has it 's own <PLACE_HOLDER> and hence own delegate,list < object > items = arrays . as list ( new object ( ) @$ new object ( ) @$ new object ( ) ) ; spyable adapter delegate < list < object > > d0 = new spyable adapter delegate < > ( __num__ ) ; spyable adapter delegate < list < object > > d1 = new spyable adapter delegate < > ( __num__ ) ; spyable adapter delegate < list < object > > d2 = new spyable adapter delegate < > ( __num__ ) ; adapter delegates manager < list < object > > manager = new adapter delegates manager < > ( ) ; manager . add delegate ( d0 ) ; manager . add delegate ( d1 ) ; manager .,elements has
test file does n't have an ec <PLACE_HOLDER>,final path file = new path ( dir @$ __str__ ) ; fs . create ( file ) . close ( ) ; assert null ( client . get file info ( file . to string ( ) ) . get erasure coding policy ( ) ) ; contract test utils . assert not erasure coded ( fs @$ file ) ; fs . delete ( file @$ true ) ; final erasure coding policy ec policy1 = get ec policy ( ) ;,file have
if component metrics @$ no more information required from tag so break the <PLACE_HOLDER>,if ( tag . name ( ) . equals ( __str__ ) ) { app id = tag . value ( ) ; },metrics break
if there are quotes @$ build a <PLACE_HOLDER> wide list,if ( quote annotator . gather quotes ( this . annotation document ) != null ) build document quotes list ( ) ;,list build
if any the fields of struct are representing <PLACE_HOLDER> @$ then return true,for ( int i = __num__ ; i < struct fields . size ( ) ; i ++ ) { if ( has any null object ( soi . get struct field data ( o @$ struct fields . get ( i ) ) @$ struct fields . get ( i ) . get field object inspector ( ) ) ) { return true ; } } return false ;,fields representing
the sdk requires an unbounded thread <PLACE_HOLDER> because a step may create x writers each requiring their own thread to perform the writes otherwise a writer may block causing deadlock for the step because the writers buffer is full . also @$ the map task executor launches the steps in reverse order and completes them in forward order thus requiring enough threads so that,return new thread pool executor ( __num__ @$ integer . max_value @$ long . max_value @$ time unit . nanoseconds @$ new synchronous queue < > ( ) @$ thread factory builder . build ( ) ) ;,sdk requires
if this next line runs a <PLACE_HOLDER> @$ it fails,lambda l = dummy method to make check style happy ( static fail if clinit runs :: static method ) ; try { l . run ( __num__ @$ __num__ ) ; fail ( __str__ ) ; } catch ( assertion error ae ) { },line runs
fourth try @$ calling stop build with a parameter not matching build <PLACE_HOLDER> does n't interrupt the build reset semaphore and result code,semaphore . drain permits ( ) ; atomic result . set ( __num__ ) ; queue task future < free style build > future build = p . schedule build2 ( __num__ ) ; future build . wait for start ( ) ; web request request = new web request ( new url ( j . geturl ( ) + __str__ ) @$ http method . post ) ; page page = wc . get page ( request ) ; assert equals ( __num__ @$ page . get web response ( ) . get status code ( ) ) ; assert request was not blocked ( ) ;,the build
interface has no <PLACE_HOLDER> and is therefore not handled as pojo,if ( modifier . is interface ( clazz . get modifiers ( ) ) ) { return new generic type info < out > ( clazz ) ; },interface has
the final part of this check is to verify that the <PLACE_HOLDER> does actually indicate a <PLACE_HOLDER> in user .,try { cached security context = security context association . get security context ( ) ; final security context next context = security context factory . create security context ( desired user @$ new current user credential ( connection user . get name ( ) ) @$ new subject ( ) @$ __str__ ) ; security context association . set security context ( next context ) ; context set = true ; remoting context . clear ( ) ; } catch ( exception e ) { logger . error ( __str__ @$ e ) ; throw new ejb access exception ( __str__ ) ; },change indicate
sorting on due date asc should put the <PLACE_HOLDER> at the end,list < task > tasks = task service . create task query ( ) . order by due date nulls last ( ) . asc ( ) . list ( ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { assert not null ( tasks . get ( i ) . get due date ( ) ) ; } assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get,sorting put
we start out by loading an initial configuration where we started to write a task update @$ and then compaction cleaned up the earlier <PLACE_HOLDER> .,expect configure ( ) ; list < consumer record < string @$ byte [ ] > > existing records = arrays . as list ( new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$ __num__ @$ connector_config_keys . get ( __num__ ) @$ configs_serialized . get ( __num__ ) ) @$ new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$ __num__ @$ task_config_keys . get ( __num__ ) @$ configs_serialized . get ( __num__ ) ) @$ new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$,compaction cleaned
buffers use negative id <PLACE_HOLDER> ; file bytes use positive id <PLACE_HOLDER> .,long source id = - buf . get id ( ) ;,bytes use
ensure that really late tasks do n't completely saturate the <PLACE_HOLDER> of the task queue,if ( late > period ) { period = __num__ ; } else if ( late > __num__ ) { period -= late ; },tasks saturate
subclasses like forall not node might override this <PLACE_HOLDER>,insert child left tuple ( sink @$ trg left tuples @$ left tuple @$ right tuple . get propagation context ( ) @$ true ) ;,subclasses override
active user count has to decrease from queue 2 due to app has no pending <PLACE_HOLDER>,assert equals ( __num__ @$ queue2 . get abstract users manager ( ) . get num active users ( ) ) ;,count has
adapterview does not support click <PLACE_HOLDER>,if ( view instanceof adapter view ) { return ; } view . set on click listener ( new view . on click listener ( ) { @ override public void on click ( view v ) { if ( on click listener == null ) { return ; } on click listener . on click ( dialog plus . this @$ v ) ; } } ) ;,adapterview support
eliminate loops in case of the boot class loader returning <PLACE_HOLDER> as a parent,return ( parent == cl ) ? null : parent ;,case returning
invalidate the handshake so that the caller can dispose this <PLACE_HOLDER> .,invalidated = true ;,caller dispose
if inheriting all <PLACE_HOLDER> then trans executor can initialize <PLACE_HOLDER> from parent trans,assert . assert equals ( parent value @$ internal trans . get variable ( variable name ) ) ;,executor initialize
sigar uses <PLACE_HOLDER> and does not work in local mode,if ( ! nimbus client . is local override ( ) ) { worker metrics . put ( __str__ @$ __str__ ) ; },sigar uses
text view wo n't center compound <PLACE_HOLDER> in both dimensions without a little coercion . pad in to center the icon after we 've measured .,if ( ! text visible && m icon != null ) { final int w = get measured width ( ) ; final int dw = m icon . get bounds ( ) . width ( ) ; super . set padding ( ( w - dw ) / __num__ @$ get padding top ( ) @$ get padding right ( ) @$ get padding bottom ( ) ) ; },view center
both ruby should pass : ruby.jar has no jdk <PLACE_HOLDER>,collection < component info > cat infos = contents . load components ( __str__ @$ m @$ false ) ; assert equals ( __num__ @$ cat infos . size ( ) ) ; set < url > urls = new hash set < > ( arrays . as list ( test data . resolve ( __str__ ) . to uri ( ) . tourl ( ) @$ test data . resolve ( __str__ ) . to uri ( ) . tourl ( ) ) ) ; iterator < component info > itc = cat infos . iterator ( ) ; component info ci = itc . next ( ) ; assert true ( urls . remove ( ci . get remoteurl ( ) ) ) ; ci =,ruby.jar has
zap : added the <PLACE_HOLDER> .,extension hook . get hook view ( ) . add option panel ( get options database panel ( ) ) ; extension hook . get hook view ( ) . add option panel ( get options jvm panel ( ) ) ;,zap added
unknown backend should get default <PLACE_HOLDER> of registry,thread pool bulkhead bulkhead3 = bulkhead registry . bulkhead ( __str__ ) ; assert that ( bulkhead3 ) . is not null ( ) ; assert that ( bulkhead3 . get bulkhead config ( ) . get core thread pool size ( ) ) . is equal to ( __num__ ) ; assert that ( event consumer registry . get all event consumer ( ) ) . has size ( __num__ ) ;,backend get
verify that the read delta call will only return deltas when the previous call had null <PLACE_HOLDER> .,m callback . clear ( ) ; final long [ ] [ ] new times3 = increase time ( new times2 ) ; write to file ( m headline + uid lines ( m uids @$ new times3 ) ) ; m reader . read delta ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; ++ i ) { m callback . verify ( m uids [ i ] @$ get active time ( new times3 [ i ] ) - get active time ( new times2 [ i ] ) ) ; } m callback . verify no more interactions ( ) ; assert true ( m test file . delete ( ) ) ;,call had
if we have a constraints changed intent in the queue do n't add a second <PLACE_HOLDER> . we are treating this intent as special because every time a worker with constraints is complete it kicks off an update for constraint proxies .,if ( command handler . action_constraints_changed . equals ( action ) && has intent with action ( command handler . action_constraints_changed ) ) { return false ; } intent . put extra ( key_start_id @$ start id ) ; synchronized ( m intents ) { boolean has commands = ! m intents . is empty ( ) ; m intents . add ( intent ) ; if ( ! has commands ) { process command ( ) ; } },intent add
in theory node name ca n't be null but better be careful who knows <PLACE_HOLDER> other implementations may be doing ? ...,if ( get node name ( ) == null ) { if ( arg . get node name ( ) != null ) { return false ; } } else if ( ! get node name ( ) . equals ( arg . get node name ( ) ) ) { return false ; } if ( get local name ( ) == null ) { if ( arg . get local name ( ) != null ) { return false ; } } else if ( ! get local name ( ) . equals ( arg . get local name ( ) ) ) { return false ; } if ( get namespaceuri ( ) == null ) { if ( arg . get namespaceuri ( ),implementations doing
the following will change the <PLACE_HOLDER> and feel of the toolbar to match the current design,m recycler view . set toolbar background color ( context compat . get color ( context @$ r . color . primary ) ) ; m recycler view . set toolbar spinner text color ( context compat . get color ( context @$ android . r . color . white ) ) ; m recycler view . set toolbar spinner drawable ( r . drawable . ic_dropdown_primary_30_24dp ) ; if ( build config . information_architecture_available && m is top level ) { m recycler view . set toolbar title ( r . string . reader_screen_title @$ get resources ( ) . get dimension pixel size ( r . dimen . margin_extra_large ) ) ; } else { m recycler view . set toolbar left and right padding (,following change
the keys of probe and build sides are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer join <PLACE_HOLDER> well in this case .,final int probe vals per key = __num__ ;,side join
no intersection between clip area and segment computed <PLACE_HOLDER> : bottom left,clippable . init ( ) ; segment clipper . clip ( - __num__ @$ - __num__ @$ __num__ @$ __num__ ) ; assert . assert equals ( __num__ @$ points . size ( ) ) ;,intersection computed
no need to reset the <PLACE_HOLDER> ; we 're exiting,if ( _is stopped ) { break ; } logger . warn ( __str__ @$ e ) ; break ;,need reset
if invalid label exception is received means the requested label doesnt have <PLACE_HOLDER> so killing job in this case .,string diag msg = __str__ + string utils . stringify exception ( e ) ; log . info ( diag msg ) ; job id job id = this . get job ( ) . getid ( ) ; event handler . handle ( new job diagnostics update event ( job id @$ diag msg ) ) ; event handler . handle ( new job event ( job id @$ job event type . job_kill ) ) ; throw e ;,doesnt have
check if the current node has a <PLACE_HOLDER> of the appropriate type if it does : increment the count and proceed otherwise : create a new node @$ and to map and set as a <PLACE_HOLDER>,g tree node current node child = current node child map . get ( key ) ; if ( current node child == null ) { current node child = new function bit patternsg tree node ( key @$ current seq . get instructions ( ) [ level ] @$ current seq . get sizes ( ) [ level ] ) ; ( ( function bit patternsg tree node ) current node child ) . increment count ( __num__ ) ; current node child map . put ( key @$ current node child ) ; current node . add node ( current node child ) ; } else { ( ( function bit patternsg tree node ) current node child ) . increment count ( __num__ ) ;,node has
verify script listener has done its <PLACE_HOLDER> @$ on create before flowable entity event was fired,assert equals ( __str__ @$ __str__ @$ task from event . get assignee ( ) ) ; assert equals ( __str__ @$ __num__ @$ task from event . get priority ( ) ) ;,listener done
each segment should have 3 <PLACE_HOLDER>,for ( map < string @$ string > instance state map : new assignment . values ( ) ) { assert equals ( instance state map . size ( ) @$ num_replicas ) ; },segment have
for bootstrap class <PLACE_HOLDER> bean deployment archive always use the parent resource <PLACE_HOLDER>,if ( module == null ) { new bda services . add ( resource loader . class @$ service registry . get ( resource loader . class ) ) ; },archive use
no handlers left so close the actual <PLACE_HOLDER> the done handler needs to be executed on the context that calls close @$ not the context of the actual <PLACE_HOLDER>,actual server . actual close ( completion ) ;,handlers left
total block count fragment <PLACE_HOLDER> in bytes .,return sb . f_blocks * sb . f_frsize ;,block count
if carrier required @$ always show signal bar of primary subscription . otherwise @$ show <PLACE_HOLDER> subscription is currently active for internet .,boolean always show primary = carrier config manager . get default config ( ) . get boolean ( carrier config manager . key_always_show_primary_signal_bar_in_opportunistic_network_boolean ) ; if ( always show primary ) { subscriptions . remove ( info1 . is opportunistic ( ) ? info1 : info2 ) ; } else { subscriptions . remove ( info1 . get subscription id ( ) == m active mobile data subscription ? info2 : info1 ) ; },subscription active
do delay send coin <PLACE_HOLDER> .,long create account fee = __num__ ; logger . info ( __str__ ) ;,delay send
user can override <PLACE_HOLDER> of spark clone configuration in hive config to true,conf . set ( spark clone configuration @$ __str__ ) ; check spark conf ( conf @$ spark clone configuration @$ __str__ ) ;,user override
when detach from <PLACE_HOLDER> @$ should have <PLACE_HOLDER> stop buffering .,mockito . verify ( host callback @$ times ( __num__ ) ) . on buffering state changed ( false ) ; mockito . verify ( host callback @$ times ( __num__ ) ) . on video size changed ( any int ( ) @$ any int ( ) ) ; mockito . verify ( host callback @$ times ( __num__ ) ) . on error ( any int ( ) @$ any string ( ) ) ;,detach have
fallback to maven manifest <PLACE_HOLDER>,hash map < string @$ string > maven versions = get maven versions ( ) ; string [ ] maven libs = { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; for ( string lib : maven libs ) { lib versions . put ( lib @$ maven versions . get ( lib ) ) ; } return lib versions ;,fallback manifest
if list <PLACE_HOLDER> exceeds pref <PLACE_HOLDER> @$ close connection,if ( closed || ( pref size > __num__ && conns . size ( ) > pref size ) ) { d ( __str__ @$ conn ) ; td ( __str__ @$ conn ) ; conns . remove ( entry ) ; conn . close connection ( ) ; } else { d ( __str__ @$ conn ) ; td ( __str__ @$ conn ) ; entry = conns . get ( loc ) ; entry . release ( ) ; },size exceeds
abort : option to queue a batch with a query which can cause the <PLACE_HOLDER> to abort .,boolean has abort = arg . contains ( __str__ ) ;,which cause
already scrolling to the correct page @$ but not yet there . only handle instant <PLACE_HOLDER> because then we need to interrupt the current smooth scroll .,if ( item == m current item && smooth scroll ) { return ; },page handle
if cr<PLACE_HOLDER> just keep <PLACE_HOLDER>,if ( c == __str__ ) { c = read ( reader ) ; if ( c != __str__ ) { last read = c ; use last read = true ; c = __str__ ; } },crlf keep
this property access would be an unknown property error unless the polymer pass had successfully parsed the element <PLACE_HOLDER> .,compiler compiler = compile ( options @$ new string [ ] { lines ( __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) } ) ;,pass parsed
a client will merge databases if the <PLACE_HOLDER> of databases exceeds the maximum <PLACE_HOLDER> per client times the amount of clients,int max database files = options . get max database files ( ) * all database files map . key set ( ) . size ( ) ; boolean too many database files = number of database files > max database files ; boolean removed old versions = result . get removed old versions count ( ) > __num__ ; return removed old versions || too many database files || options . is force ( ) ;,number exceeds
taps close a dimmed open <PLACE_HOLDER> but only if it is n't locked open .,if ( dx * dx + dy * dy < slop * slop ) { final view open drawer = find open drawer ( ) ; if ( open drawer != null ) { peeking only = get drawer lock mode ( open drawer ) == lock_mode_locked_open ; } },taps close
a volt db extension to tweak <PLACE_HOLDER> with standard sql error handling,if ( double . is nan ( val ) || double . is infinite ( val ) ) { throw error . error ( error code . x_2201f ) ; },extension tweak
construction a <PLACE_HOLDER> transition table @$ this tells the controller the next <PLACE_HOLDER> given initial and final <PLACE_HOLDER>s .,for ( string state : state priority list ) { string key = state + __str__ ; if ( state . equals ( online_state ) ) { map < string @$ string > metadata = new hash map < string @$ string > ( ) ; metadata . put ( offline_state @$ offline_state ) ; metadata . put ( dropped_state @$ dropped_state ) ; record . set map field ( key @$ metadata ) ; } if ( state . equals ( __str__ ) ) { map < string @$ string > metadata = new hash map < string @$ string > ( ) ; metadata . put ( online_state @$ online_state ) ; metadata . put ( dropped_state @$ dropped_state ) ; record . set map field,construction tells
h base connector does n't support local date time use <PLACE_HOLDER> as conversion class for now .,for ( int j = __num__ ; j < family type . get arity ( ) ; j ++ ) { class clazz = qualifier types [ j ] . get type class ( ) ; if ( local date time . class . equals ( clazz ) ) { clazz = timestamp . class ; } else if ( local date . class . equals ( clazz ) ) { clazz = date . class ; } else if ( local time . class . equals ( clazz ) ) { clazz = time . class ; } hbase schema . add column ( name @$ qualifier names [ j ] @$ clazz ) ; },connector support
nodes without outgoing edges into the subgraph formed by the selected nodes are unselected . those are just the nodes which have no selected <PLACE_HOLDER> .,graph . iterate selected ( new i node callback < node type > ( ) { @ override public iteration mode next ( final node type node ) { if ( are all children deselected ( node ) ) { if ( ! deselect . contains ( node ) ) { deselect . add ( node ) ; } } return iteration mode . continue ; } } ) ;,which have
more methods than advertised @$ either broken request or not actually socks 5 do this <PLACE_HOLDER> last so that any waiting for data is already done,return actual readable bytes == ( __num__ + number of authentication methods ) ;,methods do
we loaded plain <PLACE_HOLDER> and the caller wants styled <PLACE_HOLDER> @$ that is all we have so return it .,if ( styled ) { return text ; } else { return html . escape html ( text ) ; },text wants
a media period may report a discontinuity at the current playback position to ensure the renderers are flushed . only report the discontinuity <PLACE_HOLDER> if the position changed .,if ( period position us != playback info . position us ) { playback info = playback info . copy with new position ( playback info . period id @$ period position us @$ playback info . content position us @$ get total buffered duration us ( ) ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( ) ; period position us = playing period holder . to period time ( renderer position us ) ; maybe trigger pending messages ( playback info . position us @$ period position us ) ; playback info . position us = period position us ;,period report
clear res monitor <PLACE_HOLDER>,res monitor work = null ; m_periodic works . clear ( ) ; m_snapshot completion monitor . shutdown ( ) ; m_periodic work thread . shutdown ( ) ; m_periodic work thread . await termination ( __num__ @$ time unit . days ) ; m_periodic priority work thread . shutdown ( ) ; m_periodic priority work thread . await termination ( __num__ @$ time unit . days ) ; if ( m_elastic service != null ) { m_elastic service . shutdown ( ) ; } if ( m_leader appointer != null ) { m_leader appointer . shutdown ( ) ; } m_global service elector . shutdown ( ) ; if ( m_has started sampler . get ( ) ) { m_sampler . set should stop ( ) ;,res monitor
a volt db extension to support limit partition rows <PLACE_HOLDER>,case tokens . limit : read ( ) ; process alter table add limit constraint ( t @$ cname ) ; return ;,extension support
failed attempt to renew the seed does not have any <PLACE_HOLDER>,assert user connected ( wc @$ alice ) ; user seed property user seed property = alice . get property ( user seed property . class ) ; user seed property . renew seed ( ) ;,attempt have
the order of tasks below is waiting @$ pending @$ running to prevent skipping a task @$ it 's the order in which tasks will change <PLACE_HOLDER> if they do while this is code is executing @$ so a task might be counted twice but never skipped,if ( runner task state . waiting . equals ( state ) ) { collection < ? extends task runner work item > runners known tasks = runner . get known tasks ( ) ; set < string > runner known task ids = runners known tasks . stream ( ) . map ( task runner work item :: get task id ) . collect ( collectors . to set ( ) ) ; final list < any task > waiting tasks = new array list < > ( ) ; for ( task runner work item task : all tasks ) { if ( ! runner known task ids . contains ( task . get task id ( ) ) ) { waiting tasks . add (,tasks change
test should n't have waited the full 30 <PLACE_HOLDER> @$ since the thread throws faster than that,assert true ( __str__ + ( et - st ) + __str__ @$ et - st < __num__ ) ;,test waited
note : do n't use schema <PLACE_HOLDER> internally we will get stack overflaw because xml schema validator will be instantiating xsd handler ...,f schema grammar description = new xsd description ( ) ;,note use
if this rel references cor <PLACE_HOLDER> and now it needs to be rewritten it must have been pulled above the correlate replace the input ref to account for the lhs of the correlate,if ( current rel instanceof logical correlate ) { final int left input field count = ( ( logical correlate ) current rel ) . get left ( ) . get row type ( ) . get field count ( ) ; rel data type new type = input ref . get type ( ) ; if ( project pulled above left correlator ) { new type = type factory . create type with nullability ( new type @$ true ) ; } int pos = input ref . get index ( ) ; rex input ref new input ref = new rex input ref ( left input field count + pos @$ new type ) ; if ( ( is count != null ) && is count,references cor
all values have the same <PLACE_HOLDER> so they all appear as a single output element,return input . apply ( reify . windows ( ) ) . apply ( with keys . < integer @$ value in single window < t > > of ( __num__ ) . with key type ( new type descriptor < integer > ( ) { } ) ) . apply ( window . into ( new identity window fn < kv < integer @$ value in single window < t > > > ( original window fn . window coder ( ) ) ) . triggering ( never . ever ( ) ) . with allowed lateness ( input . get windowing strategy ( ) . get allowed lateness ( ) ) . discarding fired panes ( ) ) . apply ( group by key . create,values have
some cases use this absolute <PLACE_HOLDER>,if ( fs != null ) { if ( root dir test enabled ( ) ) { cleanup dir ( path ( __str__ ) ) ; } cleanup dir ( get test base dir ( ) ) ; },cases use
the compressed stream should reset before compressing stream reset state since in gz reset statue will write <PLACE_HOLDER> in the outputstream .,compressed stream . reset ( ) ; compressing stream . reset state ( ) ; compressing stream . write ( buffer @$ offset @$ length ) ; compressing stream . flush ( ) ; compressed stream . to byte array ( ) ; final long finish time = system . nano time ( ) ;,statue write
deselect will fire the <PLACE_HOLDER>,if ( item ids != null ) { return deselect ( arrays . as list ( item ids ) ) ; } else { throw new illegal argument exception ( __str__ ) ; },deselect fire
user ca n't edit the <PLACE_HOLDER> .,return false ;,user edit
create a table which will throw <PLACE_HOLDER> for requests,table name table name = table name . value of ( name . get method name ( ) ) ; h table descriptor table desc = new h table descriptor ( table name ) ; table desc . add coprocessor ( error throwing get observer . class . get name ( ) ) ; table desc . add family ( new h column descriptor ( family ) ) ; table table = util . create table ( table desc @$ null ) ; table . put ( new put ( rowkey ) . add column ( family @$ col @$ bytes . to bytes ( __str__ ) ) ) ; thrifth base service handler hbase handler = create handler ( ) ; thrift metrics metrics = get metrics,which throw
ok so apparently the index has a node <PLACE_HOLDER> outside node high id,reporter . for index entry ( new index entry ( descriptor @$ context . token name lookup @$ entity id ) ) . node not in use ( record loader . node ( entity id ) ) ;,index has
developer is explicitly using the dark <PLACE_HOLDER> .,if ( args . get boolean ( base dialog builder . arg_use_dark_theme ) ) { use light theme = false ; } else if ( args . get boolean ( base dialog builder . arg_use_light_theme ) ) { use light theme = true ; },developer using
ensure that this real user even has kerberos <PLACE_HOLDER> :,user real user = proxy user provider . get user ( ) ; kerberos principal real principal = real user . get kerberos principal ( ) ; if ( real principal == null ) { throw new es hadoop illegal argument exception ( __str__ + real user . get user name ( ) + __str__ + run as user + __str__ ) ; } if ( log . is debug enabled ( ) ) { log . debug ( __str__ + real user . get user name ( ) + __str__ + run as user + __str__ ) ; } credential user provider = proxy user provider ;,user has
its is imperative that we test in a single shot since multiple batches would mean the proxy bridges being torn down and established multiple times and we can not test against the source cluster topology then . get rebalance kit <PLACE_HOLDER> batch size of infinite @$ so this should be fine .,string bootstrap url = get bootstrap url ( updated current cluster @$ __num__ ) ; int max parallel = __num__ ; final cluster test utils . rebalance kit rebalance kit = cluster test utils . get rebalance kit ( bootstrap url @$ max parallel @$ final cluster ) ; populate data ( current cluster @$ rw store def with replication ) ; final admin client admin client = rebalance kit . controller . get admin client ( ) ;,size get
if the regular output and reprocess output have the same <PLACE_HOLDER> and format @$ they can share one image reader .,if ( input format == reprocess output format && input size . equals ( reprocess output size ) ) { max images *= __num__ ; m share one image reader = true ; },output have
user 2 joins the new <PLACE_HOLDER>,try { multi user chat muc2 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc2 . join ( __str__ ) ; muc2 . add user status listener ( new default user status listener ( ) { public void kicked ( string actor @$ string reason ) { super . kicked ( actor @$ reason ) ; answer [ __num__ ] = actor ; answer [ __num__ ] = reason ; } } ) ; multi user chat muc3 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc3 . join ( __str__ ) ; muc3 . add participant status listener ( new default participant status listener ( ) { public void kicked ( string participant,user joins
rows with ` technology ` have ` <PLACE_HOLDER> ` in the quality numeric string field,regex filtered dimension spec regex spec = new regex filtered dimension spec ( new default dimension spec ( __str__ @$ __str__ @$ value type . long ) @$ __str__ ) ; list filtered dimension spec list filtered spec = new list filtered dimension spec ( new default dimension spec ( __str__ @$ __str__ @$ value type . float ) @$ sets . new hash set ( __str__ ) @$ true ) ; group by query query = make query builder ( ) . set data source ( query runner test helper . data_source ) . set query segment spec ( query runner test helper . first_to_third ) . set dimensions ( regex spec @$ list filtered spec ) . set dim filter ( new in dim filter (,technology have
this code uses a calendar <PLACE_HOLDER> to create a unique name and description for test purposes so that you can easily upload multiple files . you should remove this code from your project and use your own standard names instead .,calendar cal = calendar . get instance ( ) ; snippet . set title ( __str__ + cal . get time ( ) ) ; snippet . set description ( __str__ + __str__ + cal . get time ( ) ) ;,code uses
in command line . stop jmx <PLACE_HOLDER> and then start it again with different property values stop jmx <PLACE_HOLDER> again and then start it without property value make sure these properties overridden corectly,system . out . println ( __str__ ) ; something s = do something ( __str__ @$ __str__ + port1 @$ __str__ @$ __str__ ) ; try { test no connect ( port1 ) ; jcmd ( cmd_stop ) ; jcmd ( cmd_start @$ __str__ @$ __str__ + port1 ) ; test connect ( port1 ) ; jcmd ( cmd_stop ) ; jcmd ( cmd_start @$ __str__ + port1 ) ; test no connect ( port1 ) ; } finally { s . stop ( ) ; },overridden stop
first things first : let 's flush the buffer to get some more <PLACE_HOLDER>,_flush buffer ( ) ;,buffer get
note to translators : the message indicates that the encoding requested for the output document was on that requires <PLACE_HOLDER> that is not available from the java virtual machine being used to execute the program .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,document requires
make sure that the <PLACE_HOLDER> of column aliases in the as clause matches the <PLACE_HOLDER> of columns output by the udtf,int num supplied aliases = col aliases . size ( ) ; if ( num udtf cols != num supplied aliases ) { throw new semantic exception ( error msg . udtf_alias_mismatch . get msg ( __str__ + num udtf cols + __str__ + __str__ + num supplied aliases ) ) ; },number matches
insert dummy dimension so all subtotals queries have <PLACE_HOLDER> rows with the same shape . use a field name that does not appear in the main query <PLACE_HOLDER> @$ to assure the <PLACE_HOLDER> will be null .,string dim name = __str__ + i ; while ( query . get result row position lookup ( ) . get int ( dim name ) >= __num__ ) { dim name = __str__ + dim name ; } new dimensions . add ( default dimension spec . of ( dim name ) ) ;,rows have
only chunks in the old from space have a remembered <PLACE_HOLDER> .,final heap impl heap = heap impl . get heap impl ( ) ; final old generation old gen = heap . get old generation ( ) ; if ( that . get space ( ) == old gen . get from space ( ) ) { if ( ! card table . verify ( get card table start ( that ) @$ get first object table start ( that ) @$ get aligned heap chunk start ( that ) @$ that . get top ( ) ) ) { final log verify log = heap . get heap verifier impl ( ) . get witness log ( ) . string ( __str__ ) ; verify log . string ( __str__ ) . string ( __str__ ) .,chunks have
main : 1 finished main : 1 received 0 on 11 received 1 on 11 received <PLACE_HOLDER> on 11,system . out . println ( __str__ + thread . current thread ( ) . get id ( ) ) ;,main received
if the order by expression list contains a <PLACE_HOLDER> by expression then we wo n't have to sort by it twice . we sort by the <PLACE_HOLDER> by expressions first @$ and we do n't care what order we sort by them . so @$ find the sort direction in the order by list and use that in the <PLACE_HOLDER> by list @$ and,boolean [ ] dontsort = new boolean [ win expr . get orderby size ( ) ] ; list < abstract expression > order by expressions = win expr . get order by expressions ( ) ; list < sort direction type > order by directions = win expr . get order by directions ( ) ; order by plan node onode = new order by plan node ( ) ; for ( int idx = __num__ ; idx < win expr . get partitionby size ( ) ; ++ idx ) { sort direction type pdir = sort direction type . asc ; abstract expression partition by expression = partition by expressions . get ( idx ) ; int sidx = win expr . get sort index,order contains
model name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( model name ) ) { logger . warn ( model name + __str__ + camelize ( __str__ + model name ) ) ; model name = __str__ + model name ; },name use
measure with exactly . that way @$ paged tile layout will only use excess <PLACE_HOLDER> and will be measured last @$ after other views and padding is accounted for .,mqs panel . measure ( width measure spec @$ measure spec . make measure spec ( max qs @$ measure spec . exactly ) ) ; int width = mqs panel . get measured width ( ) ; int height = layout params . top margin + layout params . bottom margin + mqs panel . get measured height ( ) + get padding bottom ( ) ; super . on measure ( measure spec . make measure spec ( width @$ measure spec . exactly ) @$ measure spec . make measure spec ( height @$ measure spec . exactly ) ) ;,layout use
null constraint has no <PLACE_HOLDER>,if ( constraint data != null ) { number flag = ( number ) constraint data . get field value ( __str__ ) ; if ( flag != null ) { this . flag = flag . int value ( ) ; } },constraint has
when activity is idle @$ we consider the relaunch must be successful @$ so let 's clear the <PLACE_HOLDER> .,r . m relaunch reason = relaunch_reason_none ;,"s" clear
in the absence of metadata art @$ the controller dialog takes <PLACE_HOLDER> of creating it .,if ( is casting ) { builder . put string ( media metadata compat . metadata_key_display_icon_uri @$ image location ) ; },dialog takes
load state store with app node split <PLACE_HOLDER> of 2 .,store = zk tester . getrm state store ( create conf for app node split ( __num__ ) ) ; store . setrm dispatcher ( dispatcher ) ; rm state state = store . load state ( ) ; application id app id21 = application id . new instance ( __num__ @$ __num__ ) ; store app ( store @$ dispatcher @$ app id21 @$ submit time @$ start time ) ;,store split
the my sql binlog always returns a year <PLACE_HOLDER> ...,if ( data instanceof java . time . year ) { r . deliver ( adjust temporal ( java . time . year . of ( ( ( java . time . year ) data ) . get value ( ) ) ) . get ( chrono field . year ) ) ; } else if ( data instanceof java . sql . date ) { r . deliver ( ( ( java . sql . date ) data ) . get year ( ) + __num__ ) ; } else if ( data instanceof string ) { mut data = integer . value of ( ( string ) data ) ; },sql returns
passing null matches <PLACE_HOLDER>,if ( argument passed . equals ( type info factory . void type info ) ) { return __num__ ; },null matches
each query will have a unique consumer <PLACE_HOLDER> id . in this case we have two queries and 3 consumers . so we should expect two results from the current consumption rate by query call .,assert equals ( __num__ @$ consumption by query . size ( ) ) ;,query have
no existing note info <PLACE_HOLDER> mapping to same key as the current note so add a new list,if ( sparse array index < __num__ ) { list < note info > duplicates for key = new array list < > ( ) ; duplicates for key . add ( note ) ; duplicates . put ( position @$ duplicates for key ) ; } else { duplicates . value at ( sparse array index ) . add ( note ) ; },no note
ensure that the 2 nn can still perform a <PLACE_HOLDER> .,secondary . do checkpoint ( ) ;,nn perform
elf file does not contain an .llvmbc <PLACE_HOLDER>,if ( llvmbc == null ) { return null ; },file contain
run the buck kill <PLACE_HOLDER>,buck kill command handler handler = new buck kill command handler ( project @$ buck command . kill ) ; buck build manager . get instance ( project ) . run buck command while connected to buck ( handler @$ action_title @$ buck module ) ;,buck kill
there is a problem with handling simultaneous auto <PLACE_HOLDER> after restart and manual <PLACE_HOLDER> . to properly catch the moment when cluster <PLACE_HOLDER> has finished we temporary disable auto <PLACE_HOLDER> .,disable auto activation = true ; start grids ( __num__ ) ; ig = grid ( __num__ ) ; ig . cluster ( ) . active ( true ) ; cache = ig . cache ( cache name ) ; grid dht partition full map part map = ig . cachex ( cache name ) . context ( ) . topology ( ) . partition map ( false ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { ignite ex ig0 = grid ( i ) ; for ( int p = __num__ ; p < __num__ ; p ++ ) assert equals collections ( ig . affinity ( cache name ) . map partition to primary and backups ( p,activation disable
consolidate passed ops at the current slot duration ensuring each <PLACE_HOLDER> is full . to achieve this we put all passed and existing ops in a list and will merge them to ensure each represents a <PLACE_HOLDER> at the current granularity .,final list < historical ops > all ops = new linked list < > ( passed ops ) ; if ( existing ops != null ) { all ops . add all ( existing ops ) ; } if ( debug ) { enforce ops well formed ( all ops ) ; },each represents
if the app does n't override the close <PLACE_HOLDER>,if ( m iconified by default ) { if ( m on close listener == null || ! m on close listener . on close ( ) ) { clear focus ( ) ; update views visibility ( true ) ; } },app override
client app left the restore <PLACE_HOLDER> dangling . we know that it ca n't be in the middle of an actual restore operation because the timeout is suspended while a restore is in progress . clean up now .,if ( backup manager service . get active restore session ( ) != null ) { slog . w ( tag @$ __str__ ) ; backup manager service . get active restore session ( ) . mark timed out ( ) ; post ( backup manager service . get active restore session ( ) . new end restore runnable ( backup manager service @$ backup manager service . get active restore session ( ) ) ) ; },app left
wait for monitoring interval time and verify server is still in running <PLACE_HOLDER>,resume and wait ( monitoring_interval + __num__ ) ; assert equals ( operation mode . running @$ voltdb . instance ( ) . get mode ( ) ) ;,server running
if default material button background has been overwritten @$ we will let app compat button handle the <PLACE_HOLDER>,super . set support background tint list ( tint ) ;,button handle
resources missing from used are using <PLACE_HOLDER> of that resource,if ( i >= used . other resources . length ) { return __num__ ; },resources using
this pass does not do flow <PLACE_HOLDER> .,test same ( __str__ ) ;,pass do
assignment of object arrays requires <PLACE_HOLDER> of widening conversion of component types,return is assignable to ( from type @$ to type ) ;,assignment requires
we do not want to truncate logs based on these exceptions . since users can influence <PLACE_HOLDER> with config changes the users are able to workaround this if truncations is really needed .,throw e ; if ( fail on corrupted log files ) { throw unable to clean recover ( t ) ; } if ( last transaction != null ) { log entry commit commit entry = last transaction . get commit entry ( ) ; monitor . fail to recover transactions after commit ( t @$ commit entry @$ recovery to position ) ; } else { monitor . fail to recover transactions after position ( t @$ recovery start position ) ; },users influence
when calling set lock task <PLACE_HOLDER> for locked <PLACE_HOLDER> on both tasks,m lock task controller . start lock task mode ( tr1 @$ false @$ test_uid ) ; m lock task controller . start lock task mode ( tr2 @$ false @$ test_uid ) ;,calling set
completing both tasks normally ends the <PLACE_HOLDER>,task service . complete ( review tasks . get ( __num__ ) . get id ( ) ) ; variables . put ( __str__ @$ false ) ; task service . complete ( review tasks . get ( __num__ ) . get id ( ) @$ variables ) ; assert process ended ( proc id ) ;,tasks ends
rm does n't have app <PLACE_HOLDER> and job history server is not configured,client service delegate client service delegate = get client service delegate ( null @$ getrm delegate ( ) ) ; job status job status = client service delegate . get job status ( old job id ) ; assert . assert equals ( __str__ @$ job status . get username ( ) ) ; assert . assert equals ( job status . state . prep @$ job status . get state ( ) ) ;,rm have
test that the second parse build file call repopulated the <PLACE_HOLDER> .,assert equals ( __str__ @$ __num__ @$ counter . calls ) ;,call repopulated
if new if old expected old <PLACE_HOLDER> require old <PLACE_HOLDER>,if ( basic put ( event @$ false @$ false @$ null @$ false ) ) { old value = event . get old value ( ) ; },value require
we have to compare in this order @$ because the comparator order has special <PLACE_HOLDER> when the 'left side ' is a special key .,int cmp = comparator . compare ( key @$ arr [ mid ] ) ;,order has
latest supported <PLACE_HOLDER> for focused test runs,string builder buf = new string builder ( super . get name ( ) ) ; if ( include variant markers in test name || always include variant markers in name ) { buf . append ( __str__ ) . append ( get sdk ( ) . get api level ( ) ) . append ( __str__ ) ; if ( default res mode strategy == res mode strategy . both ) { buf . append ( __str__ ) . append ( resources mode . name ( ) ) . append ( __str__ ) ; } } return buf . to string ( ) ;,latest supported
is the outermost query and it will actually get run as a native query . druid 's native query layer will finalize <PLACE_HOLDER> for the outermost query even if we do n't explicitly ask it to .,final druid query query = to druid query ( false ) ; if ( query != null ) { return get query maker ( ) . run query ( query ) ; } else { return sequences . empty ( ) ; },layer finalize
this is an approximation as different host process make significantly different <PLACE_HOLDER>,int tot = __num__ ; for ( host process process : this . get host processes ( ) ) { tot += process . get percentage complete ( ) ; } int latest progress = tot / this . get host processes ( ) . size ( ) ; if ( latest progress != this . progress ) { this . progress = latest progress ; active scan event publisher . publish scan progress event ( this . get id ( ) @$ this . progress ) ; },process make
the agent can read <PLACE_HOLDER> from user content,string content = s . get channel ( ) . call ( new read files2m callable ( target ) ) ;,agent read
catch everything such that the statement does not crash the <PLACE_HOLDER>,throw new sql execution exception ( __str__ @$ t ) ;,statement crash
test that the second parse build file call repopulated the <PLACE_HOLDER> .,assert equals ( __str__ @$ __num__ @$ counter . calls ) ;,call repopulated
... figure out which direct dependencies can possibly have <PLACE_HOLDER> attached to them ...,multimap < attribute @$ label > deps with possible aspects = ( ( rule ) target ) . get transitions ( ( rule rule @$ attribute attribute ) -> { for ( aspect aspect with parameters : attribute . get aspects ( rule ) ) { if ( ! aspect with parameters . get definition ( ) . get attributes ( ) . is empty ( ) ) { return true ; } } return false ; } ) ;,dependencies have
0 x 1002346 : p 1 repeatable <PLACE_HOLDER> contains p 2 repeatable <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ;,comment contains
add a keyed stateful map operator @$ which uses <PLACE_HOLDER> for state serialization,event stream = event stream . key by ( event :: get key ) . map ( create artificial keyed state mapper ( ( map function < event @$ event > ) in -> in @$ ( event event @$ complex payload avro last state ) -> { if ( last state != null && ! last state . get str payload ( ) . equals ( keyed_state_oper_with_avro_ser . get name ( ) ) && last state . get inner pay load ( ) . get sequence number ( ) == ( event . get sequence number ( ) - __num__ ) ) { system . out . println ( __str__ ) ; } complex payload avro payload = new complex payload avro ( ) ; payload .,which uses
reset our known page widths ; populate will recompute <PLACE_HOLDER> .,if ( need populate ) { final int child count = get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { final view child = get child at ( i ) ; final layout params lp = ( layout params ) child . get layout params ( ) ; if ( ! lp . is decor ) { lp . height factor = __num__ ; } } set current item internal ( new curr item @$ false @$ true ) ; request layout ( ) ; },populate recompute
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,return new object [ ] [ ] { { basis library . run_time_internal_err @$ __str__ } @$ { basis library . run_time_copy_err @$ __str__ } @$ { basis library . data_conversion_err @$ __str__ } @$ { basis library . external_func_err @$ __str__ } @$ { basis library . equality_expr_err @$ __str__ } @$ { basis library . invalid_argument_err @$ __str__ } @$ { basis library . format_number_err @$ __str__ } @$ { basis library . iterator_clone_err @$ __str__ } @$ { basis library . axis_support_err @$ __str__ } @$ { basis library . typed_axis_support_err @$ __str__ } @$ { basis library . stray_attribute_err @$ __str__ } @$ { basis library . stray_namespace_err @$ __str__ } @$ { basis library . namespace_prefix_err @$ __str__ } @$ { basis library,text specifies
0 <PLACE_HOLDER> 10018 a 6 : op 0 has reg ref to esi . 0 <PLACE_HOLDER> 100295 a : op 1 has reg ref to c<PLACE_HOLDER> . 0 <PLACE_HOLDER> 1002 d 0 b : op 0 has reg ref to edi ; op 1 has reg ref to ea<PLACE_HOLDER> . 0 <PLACE_HOLDER> 10033 fe : op 0 both have reg ref to edi .,mtf . initialize ( __str__ @$ new program modifier listener ( ) { @ override public void modify latest ( programdb program ) { int tx id = program . start transaction ( __str__ ) ; boolean commit = false ; try { program context context = program . get program context ( ) ; register esi reg = context . get register ( __str__ ) ; register cx reg = context . get register ( __str__ ) ; reference manager ref mgr = program . get reference manager ( ) ; reference [ ] refs ; reference new ref ; refs = ref mgr . get references from ( addr ( program @$ __str__ ) @$ __num__ ) ; assert equals ( __num__ @$ refs . length,fe 0
if the service is not already holding the wake <PLACE_HOLDER> for itself @$ acquire it now to keep the system running until we get this work dispatched . we use a timeout here to protect against whatever problem may cause it to not get the work .,if ( ! m service processing ) { m launch wake lock . acquire ( __num__ * __num__ ) ; },service holding
undefined blocks have no source <PLACE_HOLDER>,assert true ( ranges . is empty ( ) ) ;,blocks have
cleanup cached <PLACE_HOLDER> before tests,compile utils . compiled_cache . invalidate all ( ) ;,cleanup cached
special join <PLACE_HOLDER> that uses the poly join columns,join sequence = get session factory helper ( ) . create join sequence ( implied join @$ property type @$ table alias @$ join type @$ poly join columns ) ;,special join
go from the back of the list to front @$ look for the request closes to the beginning that requests the <PLACE_HOLDER> in which activity will end after all callbacks are executed .,int last requested state = undefined ; int last requesting callback = - __num__ ; for ( int i = callbacks . size ( ) - __num__ ; i >= __num__ ; i -- ) { final client transaction item callback = callbacks . get ( i ) ; final int post execution state = callback . get post execution state ( ) ; if ( post execution state != undefined ) { if ( last requested state == undefined || last requested state == post execution state ) { last requested state = post execution state ; last requesting callback = i ; } else { break ; } } },activity end
constant values do n't need <PLACE_HOLDER>,if ( token . is constant encoding ( ) ) { return ; },values need
server socket will accept <PLACE_HOLDER>,socket base server = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( server @$ not null value ( ) ) ; socket base client = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( client @$ not null value ( ) ) ; zmq . set socket option ( server @$ zmq . zmq_zap_domain @$ __str__ ) ; rc = zmq . bind ( server @$ host ) ; assert that ( rc @$ is ( true ) ) ; rc = zmq . connect ( client @$ host ) ; assert that ( rc @$ is ( true ) ) ; int ret = zmq . send ( client @$ __str__ @$ __num__ ) ; assert,socket accept
start use bound port will throw <PLACE_HOLDER>,server config server config2 = new server config ( ) ; server config2 . set bound host ( host ) ; server config2 . set port ( port ) ; server config2 . set protocol ( rpc constants . protocol_type_bolt ) ; http2 clear text server server2 = new http2 clear text server ( ) ; server2 . init ( server config2 ) ; boolean error = false ; try { server2 . start ( ) ; } catch ( exception e ) { error = true ; } assert . assert true ( error ) ; server . stop ( ) ; assert . assert false ( server . started ) ; thread . sleep ( __num__ ) ;,port throw
<PLACE_HOLDER> x ac <PLACE_HOLDER> f as basic conjoining jamo,string sc ko pat3 = __str__ ;,x ac
set zoom . this helps encourage the user to pull back . some devices like the behold have a zoom <PLACE_HOLDER>,if ( max zoom string != null || mot zoom values string != null ) { parameters . set ( __str__ @$ string . value of ( ten desired zoom / __num__ ) ) ; },behold have
text node needs special case <PLACE_HOLDER>,if ( f start container . get node type ( ) == node . text_node ) { string s = f start container . get node value ( ) ; string sub = s . substring ( f start offset @$ f end offset ) ; if ( how != clone_contents ) { ( ( text impl ) f start container ) . delete data ( f start offset @$ f end offset - f start offset ) ; collapse ( true ) ; } if ( how == delete_contents ) return null ; frag . append child ( f document . create text node ( sub ) ) ; return frag ; },node needs
this filter will keep the <PLACE_HOLDER> from being sent accross the wire . this is good for counting or other scans that are checking for existence and do n't rely on the value .,s . set filter ( new key only filter ( ) ) ;,filter keep
after the job ran check to see if the input from the localized cache match the real <PLACE_HOLDER> . check if there are 3 instances or not .,path result = new path ( test_root_dir + __str__ ) ; if ( count != __num__ ) return new test result ( job @$ false ) ;,input match
codec expects the <PLACE_HOLDER> of samples appended to audio data,append number of samples ( packet @$ samples in packet ) ;,codec expects
value follows <PLACE_HOLDER>,xdr . write boolean ( true ) ;,value follows
short form has all other details @$ but if we have collected ram from smaller native processes let 's dump a <PLACE_HOLDER> of that .,if ( extra native ram > __num__ ) { append basic mem entry ( short native builder @$ process list . native_adj @$ - __num__ @$ extra native ram @$ extra native memtrack @$ __str__ ) ; short native builder . append ( __str__ ) ; extra native ram = __num__ ; } append mem info ( full java builder @$ mi ) ;,"s" dump
this test case does not work if optimized message dispatch is used as the main <PLACE_HOLDER> send block until the consumer receives the message . this test depends on <PLACE_HOLDER> decoupling so that the main <PLACE_HOLDER> can stop the consumer <PLACE_HOLDER> .,connection . set optimized message dispatch ( false ) ; connection . start ( ) ;,thread stop
if we do n't need a thread pool @$ create a dummy <PLACE_HOLDER> that executes the task synchronously noinspection nullable problems,this . executor = create thread pool ? executors . new fixed thread pool ( threads ) : new executor ( ) { @ override public void execute ( runnable command ) { command . run ( ) ; } } ;,problems create
we wo n't compare the candidate display <PLACE_HOLDER> against the current item . this is to prevent an validation warning if the user sets the display <PLACE_HOLDER> to what the existing display <PLACE_HOLDER> is,if ( item . get name ( ) . equals ( current job name ) ) { continue ; } else if ( display name . equals ( item . get display name ( ) ) ) { return false ; },user sets
do not wrap null <PLACE_HOLDER> . we do not want to impede error signaling .,if ( resolver == null ) { return null ; } return new forwarding name resolver ( resolver ) { @ override public string get service authority ( ) { return authority override ; } } ;,wrap null
upsert requires a primary <PLACE_HOLDER> on the table to work,if ( is upsert ) { boolean has pkey = false ; for ( constraint c : cat table . get constraints ( ) ) { if ( c . get type ( ) == constraint type . primary_key . get value ( ) ) { has pkey = true ; break ; } } if ( ! has pkey ) { throw new volt abort exception ( string . format ( __str__ + __str__ @$ table name ) ) ; } },upsert requires
user has <PLACE_HOLDER> disabled,if ( request . get requested session id ( ) == null ) { system messages system messages = get service ( ) . get system messages ( servlet portlet helper . find locale ( null @$ null @$ request ) @$ request ) ; get service ( ) . write uncached string response ( response @$ json constants . json_content_type @$ vaadin service . create critical notificationjson ( system messages . get cookies disabled caption ( ) @$ system messages . get cookies disabled message ( ) @$ null @$ system messages . get cookies disabledurl ( ) ) ) ; return false ; },user has
see if any of the installed providers supply a <PLACE_HOLDER> from the given algorithm name to an oid string,string oid string ; if ( ! init oid table ) { provider [ ] provs = security . get providers ( ) ; for ( int i = __num__ ; i < provs . length ; i ++ ) { for ( enumeration < object > enum_ = provs [ i ] . keys ( ) ; enum_ . has more elements ( ) ; ) { string alias = ( string ) enum_ . next element ( ) ; string upper case alias = alias . to upper case ( locale . english ) ; int index ; if ( upper case alias . starts with ( __str__ ) && ( index = upper case alias . index of ( __str__ @$ __num__ ) ) !=,any supply
if a query have a local offset or have a local <PLACE_HOLDER> without a global <PLACE_HOLDER> and if a query have a limit lower than the global different from other query we can not build globalquery,iterator < query info > it query info = has local containers . values ( ) . iterator ( ) ; query info query info = it query info . next ( ) ; if ( query info . get low limit ( ) > __num__ || orders . is empty ( ) && ! query info . get orders ( ) . is empty ( ) ) { return null ; } integer limit = query info . get high limit ( ) ; while ( it query info . has next ( ) ) { query info = it query info . next ( ) ; if ( query info . get low limit ( ) > __num__ || ( orders . is empty ( ),query have
restli request options takes <PLACE_HOLDER> over response compression config,return new object [ ] [ ] { { true @$ null @$ restli request options . default_options @$ large id count @$ default_accept_encoding @$ null @$ true } @$ { true @$ null @$ restli request options . default_options @$ small id count @$ default_accept_encoding @$ null @$ false } @$ { true @$ new compression config ( tiny ) @$ restli request options . default_options @$ large id count @$ default_accept_encoding @$ tiny . to string ( ) @$ true } @$ { true @$ new compression config ( tiny ) @$ restli request options . default_options @$ small id count @$ default_accept_encoding @$ tiny . to string ( ) @$ true } @$ { true @$ new compression config ( huge ) @$ restli request,options takes
test when third argument has <PLACE_HOLDER> and repeats,batch = get batch4 long vectors ( ) ; r = ( long column vector ) batch . cols [ __num__ ] ; batch . cols [ __num__ ] . no nulls = false ; batch . cols [ __num__ ] . is null [ __num__ ] = true ; batch . cols [ __num__ ] . is repeating = true ; expr . evaluate ( batch ) ; assert equals ( true @$ r . is null [ __num__ ] ) ; assert equals ( true @$ r . is null [ __num__ ] ) ; assert equals ( - __num__ @$ r . vector [ __num__ ] ) ; assert equals ( - __num__ @$ r . vector [ __num__ ] ) ; assert equals (,argument has
this job does not return any <PLACE_HOLDER> .,return integer . parse int ( this . < string > argument ( __num__ ) ) ;,job return
ui.js just sets up the <PLACE_HOLDER>,assert equals ( __str__ @$ messages . get ( __num__ ) . get text ( ) ) ;,ui.js sets
we need to create the first <PLACE_HOLDER> because combine fn.merger <PLACE_HOLDER>s can modify the first <PLACE_HOLDER>,accumt first = combine fn . create accumulator ( ) ; iterable < accumt > accumulators to merge = iterables . concat ( collections . singleton ( first ) @$ accums and instants for merged window . stream ( ) . map ( x -> x . _1 ( ) ) . collect ( collectors . to list ( ) ) ) ; result . add ( windowed value . of ( combine fn . merge accumulators ( accumulators to merge ) @$ timestamp combiner . combine ( accums and instants for merged window . stream ( ) . map ( x -> x . _2 ( ) ) . collect ( collectors . to list ( ) ) ) @$ merged window @$ pane info . no_firing,accumulators modify
initializes status <PLACE_HOLDER> components if the given meta contact contains a status <PLACE_HOLDER> .,init display details ( contact . get display details ( ) ) ;,contact contains
initialize checked graphic <PLACE_HOLDER> ?,if ( m wrapped . on create action mode ( mode @$ menu ) ) { set long clickable ( false ) ; return true ; },initialize checked
mock server to allow dynamically generated <PLACE_HOLDER> to be accepted,httpsurl connection . set defaultssl socket factory ( new key store factory ( new mock server logger ( ) ) . ssl context ( ) . get socket factory ( ) ) ; mock server = client and server . start client and server ( port factory . find free port ( ) ) ;,server generated
for the first grandkid replace the original <PLACE_HOLDER>,for ( filter grandkid : grand kids ) { if ( first ) { first = false ; children . set ( i @$ grandkid ) ; } else { children . add ( ++ i @$ grandkid ) ; } },grandkid replace
now prepare the implementation of the build method . see bean <PLACE_HOLDER> interface,visit build method definition ( annotation metadata @$ collections . empty map ( ) @$ collections . empty map ( ) ) ;,method see
go to ocp and check if there a user project has expected <PLACE_HOLDER>,open shift project catalog page . open ( ) ; open shift login page . login ( new_test_user . get name ( ) @$ new_test_user . get password ( ) ) ; open shift project catalog page . wait project ( user_project_name ) ; open shift project catalog page . click on project ( user_project_name ) ; open shift project catalog page . wait resource ( __str__ ) ;,project expected
hive does not support result set meta <PLACE_HOLDER> on prepared statement @$ and hive describe does not support queries @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,hive support
hides the popup menu when the parent window loses <PLACE_HOLDER> .,add component listener ( new component adapter ( ) { @ override public void component resized ( component event evt ) { final window parent window ; component parent = get parent ( ) ; if ( parent instanceof j popup menu ) parent window = swing utilities . get window ancestor ( ( ( j popup menu ) parent ) . get invoker ( ) ) ; else parent window = swing utilities . get window ancestor ( get invoker ( ) ) ; if ( parent window != null ) { if ( ! parent window . is active ( ) ) set visible ( false ) ; parent window . add window listener ( new window adapter ( ) { @ override public void window,window loses
issues with stables pages @$ hence we move the <PLACE_HOLDER> back one page,long synced bytes = ( ( position at sync / bits . page size ( ) ) - __num__ ) * bits . page size ( ) ; if ( posix advise . sync_file_range_supported ) { final long retval = posix advise . sync_file_range ( fd @$ sync start @$ synced bytes - sync start @$ posix advise . sync_file_range_sync ) ; if ( retval != __num__ ) { logger . error ( __str__ + retval ) ; logger . error ( __str__ + synced bytes + __str__ + ( synced bytes - sync start ) + __str__ + posix advise . sync_file_range_sync ) ; fc . force ( false ) ; } } else { fc . force ( false ) ; } return synced bytes ;,issues move
using & nbsp for the no message case so that the lable retains its <PLACE_HOLDER> .,string status = format status ( is valid ? __str__ : __str__ @$ true ) ; status label . set text ( status ) ;,lable retains
otherwise @$ this meeting needs a new <PLACE_HOLDER>,heap . offer ( intervals [ i ] ) ;,meeting needs
authenticated user 's account and requires <PLACE_HOLDER> to use an ssl connection .,list < string > scopes = lists . new array list ( __str__ ) ; try { credential credential = auth . authorize ( scopes @$ __str__ ) ; youtube = new you tube . builder ( auth . http_transport @$ auth . json_factory @$ credential ) . set application name ( __str__ ) . build ( ) ; string channel id = get channel id ( ) ; system . out . println ( __str__ + channel id + __str__ ) ; string video id = get video id ( ) ; system . out . println ( __str__ + video id + __str__ ) ; string text = get text ( ) ; system . out . println ( __str__ + text + __str__ ) ;,account requires
this entry aggregates multiple <PLACE_HOLDER> .,if ( entry . get journal entries count ( ) > __num__ ) { for ( journal entry e : entry . get journal entries list ( ) ) { apply entry ( e ) ; } } else if ( entry . get sequence number ( ) < __num__ ) { m last primary start sequence number = entry . get sequence number ( ) ; } else if ( entry . to builder ( ) . clear sequence number ( ) . build ( ) . equals ( journal entry . get default instance ( ) ) ) { } else { apply single entry ( entry ) ; },entry aggregates
a volt db <PLACE_HOLDER> to support indexed expressions .,boolean has non column exprs = false ; if ( set == null ) { has non column exprs = true ; set = get base column names ( index exprs ) ; },volt db
nn should not bind the wildcard <PLACE_HOLDER> by default .,try { conf . set ( dfs_namenode_http_address_key @$ localhost_server_address ) ; cluster = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) . build ( ) ; cluster . wait active ( ) ; string address = cluster . get name node ( ) . get http address ( ) . to string ( ) ; assert false ( __str__ @$ address . starts with ( wildcard_address ) ) ; } finally { if ( cluster != null ) { cluster . shutdown ( ) ; cluster = null ; } } log . info ( __str__ + dfs_namenode_http_bind_host_key ) ;,nn bind
if any thread has seen <PLACE_HOLDER> or operation failed then we do n't have to process further .,if ( last exception != null || ! operation status ) { log . warn ( __str__ + __str__ @$ this . operation @$ file . get key ( ) ) ; break ; },thread seen
so everyone has the same <PLACE_HOLDER> .,return app priority ;,everyone has
verify new paths reflect <PLACE_HOLDER> of previous resources,local resource request lr2 = create local resource request ( user @$ __num__ @$ __num__ @$ local resource visibility . application ) ; localizer context lc2 = new localizer context ( user @$ c id1 @$ null ) ; resource event req event2 = new resource request event ( lr2 @$ local resource visibility . application @$ lc2 ) ; tracker . handle ( req event2 ) ; dispatcher . await ( ) ; path hierarchical path2 = tracker . get path for localization ( lr2 @$ local dir @$ null ) ; long localized id2 = long . parse long ( hierarchical path2 . get name ( ) ) ; assert . assert equals ( localized id1 + __num__ @$ localized id2 ) ; if ( dispatcher,paths reflect
add some test data now add 5 main objects . <PLACE_HOLDER> will contain key <PLACE_HOLDER> @$ 2 will contain key <PLACE_HOLDER> & key 2 and so on,for ( int i = __num__ ; i < num objects ; i ++ ) { portfolio p = new portfolio ( i ) ; p . pkid = ( __str__ + ( num objects - i ) ) ; test rgn . put ( __str__ + i @$ p ) ; } qs = cache utils . get query service ( ) ; string queries [ ] = { __str__ @$ __str__ } ; object r [ ] [ ] = new object [ queries . length ] [ __num__ ] ;,1 contain
ui mode includes <PLACE_HOLDER> and night ...,int ui mode type = get ui mode type ( configuration ) ; int res tab type = res tab . ui mode type ( ) ; if ( res tab type != res table_config . ui_mode_type_any ) { ui mode type = res tab type ; } int ui mode night = get ui mode night ( configuration ) ; int res tab night = res tab . ui mode night ( ) ; if ( res tab night != res table_config . ui_mode_night_any ) { ui mode night = res tab night ; } configuration . ui mode = ui mode type | ui mode night ; if ( res tab . density != res table_config . density_default ) { set density ( res tab .,mode includes
we move the error classification into extensions which allows downstream <PLACE_HOLDER> to see them but still be spec compliant,if ( error classification != null ) { if ( extensions != null ) { extensions = new linked hash map < > ( extensions ) ; } else { extensions = new linked hash map < > ( ) ; } if ( ! extensions . contains key ( __str__ ) ) { extensions . put ( __str__ @$ error classification . to specification ( error ) ) ; } },which allows
get data will activate deferred <PLACE_HOLDER> if necessary,byte [ ] the header = get data ( ic sig head ) ; int to big endian ( rendering intent @$ the header @$ ic hdr rendering intent ) ;,data activate
box drawings light horizontal @$ so box drawings light <PLACE_HOLDER> @$ so box drawings light down and right @$ so box drawings light down and left @$ so box drawings light up and left @$ so box drawings light up and right @$ so box drawings light <PLACE_HOLDER> and right @$ so box drawings light down and horizontal @$ so box drawings light <PLACE_HOLDER>,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,drawings light
if a topic has k <PLACE_HOLDER> @$ and in the previous run @$ each partition recorded its avg time to pull a record @$ then use the geometric mean of these k numbers as the estimated avg time to pull a record in this run .,double est avg millis for topic = geometric mean ( prev avg millis for partitions ) ; this . est avg millis . put ( topic @$ est avg millis for topic ) ; log . info ( string . format ( __str__ @$ topic @$ est avg millis for topic ) ) ; all est avg millis . add ( est avg millis for topic ) ;,topic has
check can not delete <PLACE_HOLDER> via rest because it contains tables .,response = client . delete ( namespace path ) ; namespace path = __str__ + ns name ; assert equals ( __num__ @$ response . get code ( ) ) ;,check delete
get a sender from the pool @$ or create a new one if the pool is empty if we ca n't create a new connection then route flow files to failure and yield acquire sender will handle the <PLACE_HOLDER> to failure and yielding,channel sender sender = acquire sender ( context @$ session @$ flow file ) ; if ( sender == null ) { return ; } try { string delimiter = context . get property ( message_delimiter ) . evaluate attribute expressions ( flow file ) . get value ( ) ; if ( delimiter != null ) { delimiter = delimiter . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) ; } if ( delimiter == null ) { process single message ( context @$ session @$ flow file @$ sender ) ; } else { process delimited messages ( context @$ session @$ flow file @$ sender @$ delimiter ) ; } } finally {,sender handle
not enough data to decrypt . compact the <PLACE_HOLDER> so that we keep the data we have but prepare the <PLACE_HOLDER> to be written to again .,logger . debug ( __str__ ) ; stream buffer . compact ( ) ; return __num__ ;,data compact
it is possible to turn off the semantic graphs @$ in which case we do n't want to recreate them using the dependency <PLACE_HOLDER> . this might be relevant if using core nlp for a language which does n't have dependencies @$ for example .,if ( sentence . get ( semantic graph core annotations . enhanced plus plus dependencies annotation . class ) != null ) { pw . println ( ) ; pw . println ( __str__ ) ; pw . print ( sentence . get ( semantic graph core annotations . enhanced plus plus dependencies annotation . class ) . to list ( ) ) ; },them using
resolve initial seek <PLACE_HOLDER> .,if ( pending initial seek position != null ) { pair < object @$ long > period position = resolve seek position ( pending initial seek position @$ true ) ; pending initial seek position = null ; if ( period position == null ) { handle source info refresh ended playback ( ) ; return ; } new content position us = period position . second ; new period id = queue . resolve media period id for ads ( period position . first @$ new content position us ) ; } else if ( old content position us == c . time_unset && ! timeline . is empty ( ) ) { pair < object @$ long > default position = get period position ( timeline,initial seek
no need for lazy details scroll adjuster @$ because the start is always 0 @$ wo n't change a <PLACE_HOLDER> .,register rpc ( grid client rpc . class @$ new grid client rpc ( ) { @ override public void scroll to start ( ) { scheduler . get ( ) . schedule finally ( new scheduled command ( ) { @ override public void execute ( ) { grid . scroll to start ( ) ; } } ) ; } @ override public void scroll to end ( ) { scheduler . get ( ) . schedule finally ( new scheduled command ( ) { @ override public void execute ( ) { grid . scroll to end ( ) ; lazy details scroller . scroll to row ( data source . size ( ) - __num__ @$ scroll destination . end ) ; } },adjuster change
step 5 : if the class object for c is in an erroneous state @$ then initialization is not possible . release <PLACE_HOLDER> and throw a no class def found error .,if ( is in error state ( ) ) { throw new no class def found error ( __str__ + hub . get name ( ) ) ; },step release
msp 3 has <PLACE_HOLDER> which hsql does not support,if ( ! ishsql ( ) ) { vt = client . call procedure ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ ) . get results ( ) ; assert content of table ( new object [ ] [ ] { { __num__ } } @$ vt [ __num__ ] ) ; assert content of table ( new object [ ] [ ] { { __num__ } } @$ vt [ __num__ ] ) ; assert content of table ( new object [ ] [ ] { { __num__ @$ __str__ } @$ { __num__ @$ __str__ } } @$ vt [ __num__ ] ) ; },msp has
first @$ get the database . this is the database which contains the catalog @$ with contains the processed <PLACE_HOLDER> from the ddl file .,database db = get database ( ) ;,which contains
secondary ce @$ or a ce with a short primary @$ copy the case <PLACE_HOLDER> .,if ( minice1 <= collation fast latin . secondary_mask || collation fast latin . min_short <= minice1 ) { case1 = ( case1 > > ( __num__ - __num__ ) ) + collation fast latin . lower_case ; minice1 |= case1 ; },ce copy
null should not impact <PLACE_HOLDER>,current max = integer max kudaf . aggregate ( null @$ current max ) ; assert that ( __num__ @$ equal to ( current max ) ) ;,null impact
assert that the correct dir does not contain the <PLACE_HOLDER> .,assert true ( compare pairs . stream ( ) . none match ( p -> p . get path ( ) . equals ( cwd ) ) ) ;,dir contain
any implementations must register <PLACE_HOLDER>,return super . parse and validate metadata ( target type ) ;,implementations register
redundant store does n't change the <PLACE_HOLDER>,accept ( storage . span consumer ( ) @$ trace ) ; assert that ( row count ( tables . annotations_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_remote_service_name_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_name_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_span_name_index ) ) . is equal to ( __num__ ) ;,store change
output l 2 <PLACE_HOLDER> and ignite <PLACE_HOLDER> stats . you may notice that at this point the object is not yet stored in l 2 <PLACE_HOLDER> @$ because the read was not yet performed .,print stats ( ses factory ) ; system . out . println ( ) ; system . out . println ( __str__ ) ;,output l
there is n't a place for these tests so just stick <PLACE_HOLDER> here,assert equals ( __str__ @$ __num__ @$ double . double to long bits ( strict math . e ) ) ; assert equals ( __str__ @$ __num__ @$ double . double to long bits ( strict math . pi ) ) ; for ( int i = __num__ ; i >= __num__ ; i -- ) { double d = strict math . random ( ) ; assert true ( __str__ + d @$ d >= __num__ && d < __num__ ) ; },tests stick
6 th build @$ <PLACE_HOLDER> @$ accumulation continues up to this point,scm . add change ( ) . with author ( __str__ ) ; p . get builders list ( ) . clear ( ) ; b = j . build and assert success ( p ) ; assert culprits ( b @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,th build
we have specific code that customizes log <PLACE_HOLDER> . use this when the case .,if ( error . starts with ( __str__ ) || error . starts with ( __str__ ) ) { logger . debug ( error @$ e ) ; } else { string message = format ( __str__ @$ default log message . get ( ) @$ e . get class ( ) . get simple name ( ) @$ error ) ; logger . debug ( message @$ e ) ; },customizes log
jvms : if the <PLACE_HOLDER> is finite and the divisor is an infinity @$ the result equals the <PLACE_HOLDER> . jvms : if the <PLACE_HOLDER> is a zero and the divisor is finite @$ the result equals the <PLACE_HOLDER> .,if ( x == __num__ || float . is infinite ( y ) ) { return x ; } float result = safe rem ( x @$ y ) ;,result equals
update once at end of iteration to reduce heap write <PLACE_HOLDER>,last ret = cursor = i ; check for comodification ( ) ; return cursor ; return cursor - __num__ ; if ( last ret < __num__ ) throw new illegal state exception ( ) ; check for comodification ( ) ; try { sub list . this . remove ( last ret ) ; cursor = last ret ; last ret = - __num__ ; expected mod count = array list . this . mod count ; } catch ( index out of bounds exception ex ) { throw new concurrent modification exception ( ) ; } if ( last ret < __num__ ) throw new illegal state exception ( ) ; check for comodification ( ) ; try { array list . this . set (,heap write
subsequent error triggers does n't trigger another <PLACE_HOLDER>,tracker . process response ( new response ( __str__ @$ null ) @$ __str__ @$ __num__ ) ; assert tracker mismatch count ( tracker @$ __num__ ) ;,triggers trigger
do not recycle the old bitmap if such as it may be set as an error state to any of the page views . just let the gc take <PLACE_HOLDER> of it .,m error state = new bitmap drawable ( m context . get resources ( ) @$ error bitmap ) ;,gc take
resolves dependency if setting has a dependency and the calling user has a <PLACE_HOLDER>,if ( s system clone from parent on dependency . contains key ( setting ) && ( parent id = get group parent locked ( user id ) ) != user id ) { string dependency = s system clone from parent on dependency . get ( setting ) ; final long token = binder . clear calling identity ( ) ; try { setting setting obj = get secure setting ( dependency @$ user id ) ; if ( setting obj != null && setting obj . get value ( ) . equals ( __str__ ) ) { return parent id ; } } finally { binder . restore calling identity ( token ) ; } },user has
module invoked in do <PLACE_HOLDER>,invoke priv ( logout_method ) ;,module do
a mapped superclass can have no <PLACE_HOLDER> if the id is set below in the hierarchy,if ( mapping type . get identifier mapper ( ) != null ) { @ suppress warnings ( __str__ ) iterator < property > property iterator = mapping type . get identifier mapper ( ) . get property iterator ( ) ; set < singular persistent attribute < ? super x @$ ? > > attributes = build id class attributes ( jpa mapping type @$ property iterator ) ; jpa mapping type . get in flight access ( ) . apply id class attributes ( attributes ) ; },superclass have
reposition & adjust the <PLACE_HOLDER> for new orientation,if ( m is expanded ) { m expanded view container . set translationy ( get expanded viewy ( ) ) ; m expanded bubble . get expanded view ( ) . update view ( ) ; },& adjust
the peer is going away <PLACE_HOLDER> is broken,for ( stream holder holder : current streams . values ( ) ) { if ( holder . source channel != null ) { holder . source channel . rst stream ( ) ; } if ( holder . sink channel != null ) { holder . sink channel . rst stream ( ) ; } } frame data . close ( ) ; send go away ( error_no_error ) ; break ;,peer going
user can not see these <PLACE_HOLDER>,index for user ( user2 @$ new doc ( metric key @$ __num__ ) @$ new doc ( metric key @$ __num__ ) @$ new doc ( metric key @$ __num__ ) ) ;,user see
a volt db <PLACE_HOLDER> to diagnose array out of bounds .,boolean voltd breclaimed = ( reclaimed node pointer != - __num__ ) ;,volt db
only the killed case requires a <PLACE_HOLDER> to be sent out to the am .,switch ( result . get end reason ( ) ) { case success : log . debug ( __str__ @$ request id ) ; if ( metrics != null ) { metrics . incr executor total success ( ) ; } break ; case container_stop_requested : log . info ( __str__ @$ request id ) ; if ( metrics != null ) { metrics . incr executor total killed ( ) ; } break ; case kill_requested : log . info ( __str__ @$ request id ) ; if ( killtimer watch . is running ( ) ) { killtimer watch . stop ( ) ; long elapsed = killtimer watch . elapsed ( time unit . milliseconds ) ; log . info ( __str__ @$ elapsed ),case requires
reserved thread must have incremented <PLACE_HOLDER> but not yet added itself to queue . we will spin until it is added .,if ( thread == null ) { thread . yield ( ) ; continue ; },thread incremented
otherwise use inject bean method instead which triggers reflective <PLACE_HOLDER>,push inject method for index ( inject method visitor @$ inject instance index @$ current method index @$ __str__ ) ;,which triggers
this node changes the input <PLACE_HOLDER> completely @$ so we do not pass through parent preferences,return plan and enforce children ( node @$ single stream ( ) @$ default parallelism ( session ) ) ;,node changes
register task manager success will trigger monitoring heartbeat <PLACE_HOLDER> between tm and jm,final task manager location task manager location1 = task manager location future . get ( timeout . to milliseconds ( ) @$ time unit . milliseconds ) ; assert that ( task manager location1 @$ equal to ( task manager location ) ) ;,success trigger
self signed <PLACE_HOLDER>,this . javass cert = cf . generate certificate ( new byte array input stream ( self signed cert . get bytes ( ) ) ) ; this . javaxss cert = x509 certificate . get instance ( java cert . get encoded ( ) ) ; myss provider = cf . get provider ( ) ; security . add provider ( myss provider ) ;,self signed
this set contains <PLACE_HOLDER>,for ( string name : common names ) { voltxml diff child diff = compute diff ( before . find child ( name ) @$ after . find child ( name ) ) ; if ( ! child diff . is empty ( ) ) { result . m_changed elements . put ( name @$ child diff ) ; } } return result ;,set contains
loop over all jump <PLACE_HOLDER> .,int [ ] jump offsets = switch instruction . jump offsets ; for ( int index = __num__ ; index < jump offsets . length ; index ++ ) { evaluate instruction block ( clazz @$ method @$ code attribute @$ offset + jump offsets [ index ] ) ; },loop jump
in the case there is a directory @$ the has <PLACE_HOLDER> value should not be converted . otherwise @$ if the has <PLACE_HOLDER> value is a file @$ convert it using the path and size values .,return fav . get type ( ) . is file ( ) ? file artifact value . create for normal file using path ( path @$ fav . get size ( ) ) : new has digest . byte string digest ( fav . get value fingerprint ( ) . to byte array ( ) ) ;,the has
setting address object line <PLACE_HOLDER> by common convention .,address . set address line ( address_line_0_room_desk_floor @$ address line0 ) ; address . set address line ( address_line_1_number_road_suffix_apt @$ address line1 ) ; address . set address line ( address_line_2_city @$ address line2 ) ; address . set address line ( address_line_3_state_postal_code @$ address line3 ) ; address . set address line ( address_line_4_country @$ address line4 ) ;,address object
render state list has special loading <PLACE_HOLDER> @$ so we can leave out the null values,if ( o == null ) { if ( ! name . equals ( __str__ ) ) { element before = current element ; append element ( __str__ ) ; current element = before ; } } else { write ( o @$ o . get class ( ) . get name ( ) @$ null ) ; },list has
check whether current am could get container complete <PLACE_HOLDER>,rm app recovered app0 = rm2 . getrm context ( ) . getrm apps ( ) . get ( app0 . get application id ( ) ) ; rm app attempt loaded attempt1 = recovered app0 . get current app attempt ( ) ; assert equals ( __num__ @$ loaded attempt1 . get just finished containers ( ) . size ( ) ) ;,am get
removals in array list wo n't break <PLACE_HOLDER>,boolean changed = new list . remove ( o ) ; list = collections . unmodifiable list ( new list ) ; return changed ;,removals break
verify async remove all cache <PLACE_HOLDER> 2,server2 . invoke ( ( ) -> { assert that ( get cache ( ) . get region ( region name ) . size ( ) ) . is zero ( ) ; } ) ;,async remove
no shebang and no elf @$ use standard <PLACE_HOLDER> .,interpreter = termux service . prefix_path + __str__ ;,shebang use
see if the values had the <PLACE_HOLDER> to adjust the ` connection provider ` used,final connection provider connection provider = emf . unwrap ( session factory implementor . class ) . get service registry ( ) . get service ( connection provider . class ) ; assert that ( connection provider @$ instance of ( driver manager connection provider impl . class ) ) ; emf . close ( ) ;,values had
due to approximations @$ very close angles can give the same cos <PLACE_HOLDER> we make sure outer cos is bellow inner cos .,if ( ( ( int ) packed angle cos ) == ( ( int ) ( outer angle cos * __num__ ) ) ) { outer angle cos -= __num__ ; } packed angle cos += outer angle cos ; if ( packed angle cos == __num__ ) { throw new illegal argument exception ( __str__ ) ; },angles give
eyes distance 3 usually fits an entire <PLACE_HOLDER>,int face size = ( int ) ( face . eyes distance ( ) * __num__ ) ; if ( size mode . face margin px . equals ( m size mode ) ) { face size += m face margin px * __num__ ; } else if ( size mode . eye distance factor margin . equals ( m size mode ) ) { face size += face . eyes distance ( ) * m eye distance factor margin ; },eyes fits
let user choose email <PLACE_HOLDER>,if ( package name == null ) { for ( intent intent : initial intents ) { grant permission ( context @$ intent @$ intent . get package ( ) @$ attachments ) ; } show chooser ( context @$ initial intents ) ; } else if ( attachment intent . resolve activity ( pm ) != null ) { grant permission ( context @$ attachment intent @$ package name @$ attachments ) ; context . start activity ( attachment intent ) ; } else { acra . log . w ( log_tag @$ __str__ ) ; context . start activity ( resolve intent ) ; },user choose
rethrow ls <PLACE_HOLDER> .,throw lse ; throw ( ls exception ) createls exception ( ls exception . serialize_err @$ e ) . fill in stack trace ( ) ; if ( fdom error handler != null ) { fdom error handler . handle error ( new dom error impl ( dom error . severity_fatal_error @$ e . get message ( ) @$ null @$ e ) ) ; } throw ( ls exception ) createls exception ( ls exception . serialize_err @$ e ) . fill in stack trace ( ) ;,rethrow ls
skip block if block error has no instruction <PLACE_HOLDER>,continue ;,error has
bitwise or combines the sign bits so any negative value fails the <PLACE_HOLDER> .,if ( ( index | limit | bytes . length - limit ) < __num__ ) { throw new array index out of bounds exception ( string . format ( __str__ @$ bytes . length @$ index @$ limit ) ) ; } long offset = index ; final long offset limit = limit ; if ( state != complete ) { if ( offset >= offset limit ) { return state ; } int byte1 = ( byte ) state ; if ( byte1 < ( byte ) __num__ ) { if ( byte1 < ( byte ) __num__ || unsafe util . get byte ( bytes @$ offset ++ ) > ( byte ) __num__ ) { return malformed ; } } else if ( byte1,value fails
the user can specify the hadoop <PLACE_HOLDER>,map < string @$ string > variables = new hash map < string @$ string > ( system . getenv ( ) ) ;,user specify
after terminate suppressed <PLACE_HOLDER>or which itself suppressed original <PLACE_HOLDER>,assert equals ( __num__ @$ after terminate . get suppressed ( ) . length ) ; assert equals ( error @$ after terminate . get suppressed ( ) [ __num__ ] ) ; assert equals ( __num__ @$ error . get suppressed ( ) . length ) ; assert equals ( err @$ error . get suppressed ( ) [ __num__ ] ) ;,itself suppressed
might need to reconsider using collector 's accept <PLACE_HOLDER> for link statistics . we need to convert to <PLACE_HOLDER> window 's <PLACE_HOLDER>stamp . if not @$ it may lead to oom due to mismatch in <PLACE_HOLDER>slots .,long timestamp = time window . refine timestamp ( span . get collector accept time ( ) ) ; if ( parent application . get service type ( ) . is user ( ) ) { if ( logger . is trace enabled ( ) ) { logger . trace ( __str__ @$ parent application @$ span . get agent id ( ) @$ span application @$ span . get agent id ( ) ) ; } final link data map source link data = link data duplex map . get source link data map ( ) ; source link data . add link data ( parent application @$ span . get agent id ( ) @$ span application @$ span . get agent id ( ) @$,collector accept
the map has no guarantees on order @$ but the lists should match up <PLACE_HOLDER> with key : val associations preserved .,list < string > keys = conf ( fake config ) . keys ( ) ; list < string > vals = conf ( fake config ) . vals ( ) ; for ( int i = __num__ ; i < fake config . size ( ) ; i ++ ) { assert that ( fake config . get ( keys . get ( i ) ) @$ is ( vals . get ( i ) ) ) ; } map < string @$ string > map = conf ( keys @$ vals ) . as map ( ) ; for ( int i = __num__ ; i < fake config . size ( ) ; i ++ ) { assert that ( vals . get ( i,lists match
parameter in select list needs a <PLACE_HOLDER> .,ddl error test ( __str__ @$ __str__ + __str__ + __str__ @$ __str__ ) ;,parameter needs
if a thread has acquired the <PLACE_HOLDER> and a close is n't pending then use a deferred close . also decrement <PLACE_HOLDER> use count to signal the last thread that releases the <PLACE_HOLDER> to close it .,if ( ! close pending ) { close pending = true ; fd use count -- ; socket pre close ( ) ; },thread acquired
check has min run <PLACE_HOLDER>,memory . get bytes ( filler addr @$ prgm filler bytes ) ; if ( arrays . equals ( prgm filler bytes @$ target filler bytes ) ) { int filler len = __num__ ; string undef data string rep = undefined data . get default value representation ( ) ; address set set = new address set ( current program @$ filler addr @$ current program . get max address ( ) ) ; address iterator addr iter = set . get addresses ( filler addr . next ( ) @$ true ) ; while ( addr iter . has next ( ) ) { address next addr = addr iter . next ( ) ; if ( listing . get undefined data at ( next addr,check run
fall back to default value suppose column label contains table <PLACE_HOLDER>,if ( column label . contains ( __str__ ) ) { table name = string utils . substring before last ( column label @$ __str__ ) ; },label contains
ensure the declared dependency only has one possible <PLACE_HOLDER>,if ( coordinates != null && ! coordinates . contains ( bundle dependency coordinate ) ) { if ( coordinates . size ( ) == __num__ ) { final bundle coordinate coordinate = coordinates . stream ( ) . find first ( ) . get ( ) ; final optional < bundle > matching dependency id bundle = get bundle ( coordinate ) ; if ( matching dependency id bundle . is present ( ) ) { final string dependency coordinate str = bundle dependency coordinate . get coordinate ( ) ; logger . warn ( string . format ( __str__ @$ bundle detail . get coordinate ( ) . get coordinate ( ) @$ dependency coordinate str @$ coordinate . get coordinate ( ) ) ) ;,dependency has
initial run with no previous state . should get only last 4 <PLACE_HOLDER>,runner . run ( ) ; runner . assert all flow files transferred ( query database table . rel_success @$ __num__ ) ; in = new byte array input stream ( runner . get flow files for relationship ( query database table . rel_success ) . get ( __num__ ) . to byte array ( ) ) ; assert equals ( __num__ @$ get number of records from stream ( in ) ) ; runner . get state manager ( ) . assert state equals ( __str__ + abstract database fetch processor . namespace_delimiter + __str__ @$ __str__ @$ scope . cluster ) ; runner . clear transfer state ( ) ;,initial get
but our test framework does n't support that <PLACE_HOLDER> directly .,session old kathmandu time zone offset session = session . builder ( this . session ) . set time zone key ( time_zone_key ) . set start time ( new date time ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ date_time_zone ) . get millis ( ) ) . build ( ) ; time zone key europe warsaw time zone key = get time zone key ( __str__ ) ; date time zone europe warsaw time zone = get date time zone ( europe warsaw time zone key ) ; session europe warsaw session winter = session . builder ( this . session ) . set time zone key ( europe warsaw time zone key ) . set start time ( new date,framework support
note : m result handler uses main <PLACE_HOLDER> @$ so this must not be blocked .,m result handler = new handler ( context . get main looper ( ) ) ; m closed = false ;,handler uses
this first query retrieves the last version for each file history matching the three requested <PLACE_HOLDER> . however @$ it does not guarantee that this version is indeed the last version in that particular file history @$ so we need another query to verify that .,try ( prepared statement prepared statement = get statement ( __str__ ) ) { prepared statement . set string ( __num__ @$ filecontent checksum ) ; prepared statement . set long ( __num__ @$ size ) ; prepared statement . set timestamp ( __num__ @$ new timestamp ( modified date . get time ( ) ) ) ; try ( result set result set = prepared statement . execute query ( ) ) { collection < partial file history > file histories = new array list < > ( ) ; while ( result set . next ( ) ) { string file history id = result set . get string ( __str__ ) ; partial file history file history = get last version by file history id,three requested
do n't let a single processor crash the processor <PLACE_HOLDER> .,m logger . w ( __str__ @$ e ) ;,processor crash
only set the badge number if it exists in the style . defaulting it to 0 means the badge will incorrectly show <PLACE_HOLDER> when the user may want a numberless badge .,if ( a . has value ( r . styleable . badge_number ) ) { set number ( a . get int ( r . styleable . badge_number @$ __num__ ) ) ; } set background color ( read color from attributes ( context @$ a @$ r . styleable . badge_background color ) ) ;,badge show
server 2 will close <PLACE_HOLDER> after creating 10 keys,add listener and put all in client1 . await ( ) ;,server close
third is the get <PLACE_HOLDER>,ref get twin ref = iterables . get ( namea . get refs ( ) @$ __num__ ) ;,the get
health check satisfies <PLACE_HOLDER> config health conditions,probe result consumer . accept ( new probe result ( probe factory . get workspace id ( ) @$ probe factory . get machine name ( ) @$ probe factory . get server name ( ) @$ probe status . passed ) ) ;,check satisfies
not possible to expand since we have more than one chunk with a single segment . this is the case when user wants to append a segment with coarser granularity . case metadata storage has segments with granularity hour and segments to append have day granularity . druid shard specs does not support multiple <PLACE_HOLDER> for same interval with different granularity .,if ( existing chunks . size ( ) > __num__ ) { throw new illegal state exception ( string . format ( __str__ + __str__ @$ data source @$ segment . get interval ( ) @$ existing chunks . size ( ) ) ) ; },specs support
set app submit <PLACE_HOLDER> to be consumable by the am .,application id application id = application . get app attempt id ( ) . get application id ( ) ; environment . put ( application constants . app_submit_time_env @$ string . value of ( rm context . getrm apps ( ) . get ( application id ) . get submit time ( ) ) ) ; credentials credentials = new credentials ( ) ; data input byte buffer dibb = new data input byte buffer ( ) ; byte buffer tokens = container . get tokens ( ) ; if ( tokens != null ) { dibb . reset ( tokens ) ; credentials . read token storage stream ( dibb ) ; tokens . rewind ( ) ; },app submit
clients expected initial <PLACE_HOLDER> .,int [ ] results cnt = new int [ ] { __num__ @$ __num__ @$ __num__ } ;,clients expected
intent is explicit and there 's no <PLACE_HOLDER> . this happens @$ e.g . @$ when a system component sends a broadcast to its own runtime receiver @$ and there 's no manifest <PLACE_HOLDER> for it @$ because this method is called twice for each broadcast @$ for runtime <PLACE_HOLDER> and manifest <PLACE_HOLDER> and the later check would find no <PLACE_HOLDER> .,if ( receivers == null || receivers . size ( ) == __num__ ) { return ; },check find
this overwrites whatever setting the <PLACE_HOLDER> configured in the properties,adjust auto commit config ( properties @$ offset commit mode ) ;,whatever setting
add formats which match <PLACE_HOLDER> but the encoding second,if ( input media format . matches without ( mf @$ mime type key @$ encoding key ) ) { formats . add ( matching count @$ mf . append ( input media format ) ) ; } else { formats . add ( mf . append ( input media format ) ) ; },which match
app read buffer will extended upto current application buffer size we need to read the existing <PLACE_HOLDER> into dst before we can do unwrap again . if there are no space in dst we can break here .,if ( dst . has remaining ( ) ) read += read from app buffer ( dst ) ; else break ;,app read
file status.equals only compares path field @$ must explicitly compare all <PLACE_HOLDER>,assert . assert false ( src status . get permission ( ) . equals ( dst status . get permission ( ) ) ) ; assert . assert false ( src status . get owner ( ) . equals ( dst status . get owner ( ) ) ) ; assert . assert false ( src status . get group ( ) . equals ( dst status . get group ( ) ) ) ; assert . assert true ( src status . get access time ( ) == dst status . get access time ( ) ) ; assert . assert true ( src status . get modification time ( ) == dst status . get modification time ( ) ) ;,status.equals compare
sdk default policy should honor the <PLACE_HOLDER> config level max error retry,test actual retries ( client_config_level_max_retry ) ;,policy honor
run the do fn <PLACE_HOLDER> now that all side inputs are ready .,for ( bag state < windowed value < inputt > > elements bag : elements bags ) { iterable < windowed value < inputt > > elements = elements bag . read ( ) ; for ( windowed value < inputt > elem : elements ) { simple do fn runner . process element ( elem ) ; } elements bag . clear ( ) ; } side input fetcher . release blocked windows ( ready windows ) ;,the do
asif : remove the <PLACE_HOLDER> from active map so that destroy can be called on it . we will first collect all the <PLACE_HOLDER>s which have timed out & then expire them . in that gap even if the client genuinely closes the <PLACE_HOLDER> @$ it will not be returned to the pool as the active map no longer contains it,if ( ( now - then ) > time out ) { this . active cache . remove ( conn ) ; this . expired conns . add ( conn ) ; ++ num conn timed out ; } else { sleep time = then + time out - now ; to continue = false ; },genuinely closes
when animation or scrolling removes a focused child @$ focus to grid view <PLACE_HOLDER> to avoid losing focus .,if ( retain focus for child ) { m private flag |= pflag_retain_focus_for_child ; request focus ( ) ; },focus view
ugly but this is <PLACE_HOLDER> hive uses internally anyway,try { class . for name ( driver name ) ; } catch ( class not found exception ex ) { throw new runtime exception ( ex ) ; },hive uses
the last rollback should cause the 1 st <PLACE_HOLDER> to get sent to the dlq,m = ( text message ) consumer . receive ( __num__ ) ; assert not null ( m ) ; assert equals ( __str__ @$ m . get text ( ) ) ; session . commit ( ) ;,rollback cause
if the caller did n't specify an explicit time <PLACE_HOLDER> @$ we want to continue tracking under any it has .,if ( r . app time tracker == null && source record != null ) { r . app time tracker = source record . app time tracker ; },caller specify
making an uncoordinated call @$ which initialize the <PLACE_HOLDER> to observer node .,dfs . get client ( ) . getha service state ( ) ; dfs . mkdir ( test path @$ fs permission . get default ( ) ) ; assert sent to ( __num__ ) ; thread reader = new thread ( ( ) -> { try { dfs . get file status ( test path ) ; read status . set ( __num__ ) ; } catch ( io exception e ) { e . print stack trace ( ) ; read status . set ( - __num__ ) ; } } ) ;,which initialize
check if consumer set fail <PLACE_HOLDER> @$ then let the test fail .,if ( null != fail msg ) { log . error ( fail msg ) ; fail ( fail msg ) ; },set fail
first result set has results @$ second has <PLACE_HOLDER>,when ( result set . next ( ) ) . then return ( true ) . then return ( false ) ; when ( result set . get string ( constants . col_trigger_type ) ) . then return ( constants . ttype_simple ) ; operable trigger trigger = jdbc delegate . select trigger ( conn @$ trigger key . trigger key ( __str__ ) ) ; assert null ( trigger ) ; verify ( persistence delegate ) . load extended trigger properties ( any ( connection . class ) @$ any ( trigger key . class ) ) ;,set has
string all matches fruit <PLACE_HOLDER>,do xquery test ( xml_snippet @$ __str__ @$ arrays . as list ( fruit names ) ) ;,all matches
when layout is suppressed @$ rv does not intercept the motion <PLACE_HOLDER> . a child view e.g . a button may still get the click .,if ( m layout suppressed ) { return false ; },rv intercept
the passive provider can not be disabled @$ but the passive provider did n't exist in previous versions of this shadow . for backwards compatibility @$ we let the passive provider be disabled . this also help emulate the situation where an app only has coarse permissions @$ <PLACE_HOLDER> this shadow normally ca n't emulate .,if ( passive_provider . equals ( name ) ) { passive provider enabled = enabled ; return ; },shadow emulate
one from a <PLACE_HOLDER> if one exists . if there is a newer <PLACE_HOLDER> then use that <PLACE_HOLDER> .,try { get envelope ( ) ; } catch ( soap exception e ) { } return document . get document element ( ) ;,one use
we ca n't use the same tactic as for intersection since abstract collection only does a <PLACE_HOLDER> on the first element it encounters .,comparator < t > number comparator = new number aware comparator < t > ( ) ; if ( nlgn sort && ( head instanceof comparable ) ) { set < t > answer ; if ( head instanceof number ) { answer = new tree set < t > ( number comparator ) ; answer . add all ( self ) ; for ( t t : self ) { if ( t instanceof number ) { for ( object t2 : remove me ) { if ( t2 instanceof number ) { if ( number comparator . compare ( t @$ ( t ) t2 ) == __num__ ) answer . remove ( t ) ; } } } else { if ( remove me .,collection does
cool union should have a byte data <PLACE_HOLDER> as the last component,dtc = union . get component ( __num__ ) ; assert true ( dtc . get data type ( ) . is equivalent ( new byte data type ( ) ) ) ; dtc = union . get component ( __num__ ) ; assert equals ( s @$ dtc . get data type ( ) ) ;,union have
track the <PLACE_HOLDER>erver reported online <PLACE_HOLDER> in memory .,synchronized ( rs reports ) { rs reports . put ( server name @$ region names ) ; } if ( region names . is empty ( ) ) { log . trace ( __str__ @$ server name ) ; return ; },regionserver reported
the database to query from db the columns to return from the query from db the columns for the where clause the values for the where clause do n't <PLACE_HOLDER> the rows do n't filter by row <PLACE_HOLDER>s the sort order,cursor cursor = query builder . query ( db @$ projection @$ selection @$ selection args @$ null @$ null @$ sort order ) ;,clause do
the newly effective conf does not have <PLACE_HOLDER> 0 and <PLACE_HOLDER> 2 .,string [ ] effective volumes = dn . get conf ( ) . get ( dfs_datanode_data_dir_key ) . split ( __str__ ) ; assert equals ( __num__ @$ effective volumes . length ) ; for ( string ev : effective volumes ) { assert that ( new file ( storage location . parse ( ev ) . get uri ( ) ) . get canonical path ( ) @$ is ( not ( any of ( is ( new dirs . get ( __num__ ) ) @$ is ( new dirs . get ( __num__ ) ) ) ) ) ) ; },conf have
all splits must have the same query <PLACE_HOLDER>,set < string > actual = split completed events . stream ( ) . map ( split completed event :: get query id ) . collect ( to set ( ) ) ; assert equals ( actual @$ immutable set . of ( query completed event . get metadata ( ) . get query id ( ) ) ) ;,splits have
the instrumentation runner requires the package <PLACE_HOLDER> of the test,args . add ( __str__ @$ get test package ( ) ) ; args . add ( __str__ @$ get target package ( ) ) ; args . add ( __str__ @$ get test runner ( ) ) ; args . add ( __str__ @$ get path to adb executable ( ) ) ; if ( get test filter ( ) . is present ( ) ) { args . add ( __str__ @$ __str__ + get test filter ( ) . get ( ) ) ; } for ( map . entry < string @$ string > arg pair : get environment overrides ( ) . entry set ( ) ) { args . add ( __str__ @$ string . format ( locale . us @$ __str__,runner requires
do a temp tag <PLACE_HOLDER> on app 2,eph atm . clean temp containers ( test utils . get mock application id ( __num__ ) ) ; assert . assert equals ( __num__ @$ atm . get node cardinality by op ( node id . from string ( __str__ ) @$ allocation tags . create single app allocation tags ( test utils . get mock application id ( __num__ ) @$ immutable set . of ( __str__ ) ) @$ long :: sum ) ) ;,temp tag
overcautiousness : each event logger has its own events list @$ so one bad test wo n't spoil <PLACE_HOLDER> .,m logger events = new array list < > ( ) ; m logger = new event logger ( m logger events ) ; local broadcast manager . register receiver ( m logger @$ intent filter ) ;,test spoil
communicate the <PLACE_HOLDER> of the authenticated user to the container . in many cases the handler will just store the <PLACE_HOLDER> and the container will actually handle the login after we return from this method .,try { handler . handle ( callbacks ) ; } catch ( io exception | unsupported callback exception e ) { throw ( auth exception ) new auth exception ( ) . init cause ( e ) ; },handler store
window frames may have changed . tell the input <PLACE_HOLDER> about it .,m input monitor . layout input consumers ( dw @$ dh ) ; m input monitor . set update input windows needed lw ( ) ; if ( update input windows ) { m input monitor . update input windows lw ( false ) ; },window tell
some side effect @$ otherwise the jit will remove the <PLACE_HOLDER>,c ++ ;,effect remove
this from element will be rendered as part of the origins join <PLACE_HOLDER>,from element . set text ( __str__ ) ; from elements . add ( from element ) ;,part join
previously a update readers twice in a row would cause an <PLACE_HOLDER> . in test this would also normally cause an <PLACE_HOLDER> because scan.store is null . so as long as we get through these two calls we are good and the bug was quashed .,try ( store scanner scan = new store scanner ( new scan ( ) @$ scan info @$ get cols ( __str__ @$ __str__ ) @$ scanners ) ) { scan . update readers ( collections . empty list ( ) @$ collections . empty list ( ) ) ; scan . update readers ( collections . empty list ( ) @$ collections . empty list ( ) ) ; scan . peek ( ) ; },readers cause
banker 's rounding : <PLACE_HOLDER> to nearest even when previous digit is odd,assert equals ( str . to string ( ) @$ __str__ ) ;,banker rounding
generate the map join <PLACE_HOLDER>,return map join processor . convertsmb join to map join ( physical context . get conf ( ) @$ newsmb join op @$ map join pos @$ true ) ;,map join
ranges do not intersect @$ long leap not a <PLACE_HOLDER> .,if ( m tmp range [ __num__ ] > m prev range [ __num__ ] || m prev range [ __num__ ] > m tmp range [ __num__ ] ) { m scroll hint = view callback . hint_scroll_none ; } else if ( m tmp range [ __num__ ] < m prev range [ __num__ ] ) { m scroll hint = view callback . hint_scroll_desc ; } else if ( m tmp range [ __num__ ] > m prev range [ __num__ ] ) { m scroll hint = view callback . hint_scroll_asc ; },ranges intersect
uncompressed flash z lib compressed flash lzma compressed flash uncompressed scale form g <PLACE_HOLDER> compressed scale form g <PLACE_HOLDER>,int pos = __num__ ; long biggest size = __num__ ; long smallest size = long . max_value ; address loop : for ( long addr : ret . key set ( ) ) { set progress ( pos * __num__ / ret . size ( ) ) ; pos ++ ; try { memory input stream mis = ( memory input stream ) ret . get ( addr ) ; mis . reset ( ) ; pos marked input stream pmi = new pos marked input stream ( mis ) ; swf swf = no check ? new swf ( pmi ) : new swf ( pmi @$ null @$ null @$ null @$ false @$ true @$ true ) ; boolean valid = swf . file,compressed flash
we 've already found a constructor match before @$ this means that power mock can not determine which method to expect since there are two methods with the same name and the same number of arguments but one is using wrapper <PLACE_HOLDER> .,throw exception when multiple constructor matches found ( new java . lang . reflect . constructor [ ] { potential constructor . get java constructor ( ) @$ constructor . get java constructor ( ) } ) ;,one using
mimic some other call in the channel triggers a throttle <PLACE_HOLDER>,assert true ( throttle . on qualified failure then check is above threshold ( ) ) ;,call triggers
set up initialization . if no initialization : use a random <PLACE_HOLDER>,if ( initialization == null ) { initialization = string . value of ( iter . get random character ( ) ) ; },initialization use
in case if verifier visited the <PLACE_HOLDER> already,if ( node . get declared field ( verifier . __timestamp ) == null ) { field node time tag field = new field node ( verifier . __timestamp @$ acc_public | acc_static | acc_synthetic @$ class helper . long_type @$ node @$ new constant expression ( system . current time millis ( ) ) ) ; time tag field . set synthetic ( true ) ; node . add field ( time tag field ) ; time tag field = new field node ( verifier . __timestamp__ + string . value of ( system . current time millis ( ) ) @$ acc_public | acc_static | acc_synthetic @$ class helper . long_type @$ node @$ new constant expression ( ( long ) __num__ ) ) ; time,verifier visited
java does n't allow simple <PLACE_HOLDER> of resources as directories when the resources are inside a jar file . this searches the contents of the jar to get the list,try { url class url = jar hash . class . get resource ( __str__ ) ; if ( class url != null && class url . get protocol ( ) . equals ( __str__ ) ) { string jar path = class url . get path ( ) . substring ( __num__ @$ class url . get path ( ) . index of ( __str__ ) ) ; jar file jar = new jar file ( url decoder . decode ( jar path @$ __str__ ) ) ; enumeration < jar entry > files = jar . entries ( ) ; while ( files . has more elements ( ) ) { string f name = files . next element ( ) . get name ( ),java allow
shut down the platform when user closed log <PLACE_HOLDER> .,platform . run later ( ( ) -> { platform . set implicit exit ( true ) ; launcher . stop without platform ( ) ; } ) ;,user closed
method name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( sanitized operation id ) ) { logger . warn ( operation id + __str__ + underscore ( __str__ + operation id ) ) ; sanitized operation id = __str__ + sanitized operation id ; } return camelize ( sanitized operation id ) ;,name use
if this is a segment expression @$ it will already have been dealt with in the <PLACE_HOLDER> of the memory access during the call to translate children of node . just return the <PLACE_HOLDER> .,if ( is segment expression ( current node value ) ) { return partial results . get ( __num__ ) ; } else if ( is memory access ( current node value ) ) { return process simple memory access ( environment @$ segment override @$ size @$ load operand @$ partial results . get ( __num__ ) ) ; } else { throw new internal translation exception ( __str__ ) ; },children return
tests might add scm <PLACE_HOLDER> to indicate which scm should be used to find credential we have to do this because api url might be of wire mock server and not github,if ( api url . starts with ( git hubscm source . github_url ) || ( string utils . is not blank ( scm id ) && scm id . equals ( github scm . id ) ) ) { scm = new github scm ( new reachable ( ) { @ override public link get link ( ) { preconditions . check not null ( organization ) ; return organization . get link ( ) . rel ( __str__ ) ; } } ) ; } else { scm = new github enterprise scm ( ( new reachable ( ) { @ override public link get link ( ) { preconditions . check not null ( organization ) ; return organization . get link ( ) .,tests add
the exception thrown by <PLACE_HOLDER> added will not be passed to the exception caught below because netty will remove a <PLACE_HOLDER> if <PLACE_HOLDER> added throws an exception .,exception caught ( ctx @$ e ) ;,netty remove
skip non union r dot <PLACE_HOLDER>,execution context execution context = test execution context . new instance ( ) ; assert equals ( __num__ @$ merge step . execute ( execution context ) . get exit code ( ) ) ;,r dot
check this i pv 4 address has 3 <PLACE_HOLDER> @$ ie . a.b.c.d,int dot_count = __num__ @$ index = __num__ ; while ( ( index = ia4 . index of ( __str__ @$ index ) ) != - __num__ ) { dot_count ++ ; index ++ ; } if ( dot_count != __num__ ) { return null ; } byte [ ] v4addr = text to numeric formatv4 ( ia4 ) ; if ( v4addr == null ) { return null ; } for ( int k = __num__ ; k < inaddr4sz ; k ++ ) { dst [ j ++ ] = v4addr [ k ] ; } saw_xdigit = false ; break ;,address has
the ri silently swallows <PLACE_HOLDER> @$ but android has always logged .,system . loge ( __str__ @$ ex ) ;,ri swallows
no deployment defined repository @$ use the <PLACE_HOLDER>,if ( job repository name != null ) { service builder . add dependency ( support . get capability service name ( capabilities . job_repository_capability . get name ( ) @$ job repository name ) @$ job repository . class @$ service . get job repository injector ( ) ) ; } else { service . get job repository injector ( ) . set value ( new immediate value < > ( job repository ) ) ; },deployment use
client should have noticed the <PLACE_HOLDER>,assert that ( __str__ @$ client socket . error latch . await ( __num__ @$ seconds ) @$ is ( true ) ) ; assert that ( __str__ @$ client socket . error . get ( ) @$ instance of ( message too large exception . class ) ) ;,client noticed
default all lyrics 3 fields to sa<PLACE_HOLDER>e . id 3 <PLACE_HOLDER> 1 fields are indi<PLACE_HOLDER>idual settings . id 3 <PLACE_HOLDER> 2 fields are always looked at to sa<PLACE_HOLDER>e .,iterator < string > iterator = lyrics3v2 fields . get instance of ( ) . get id to value map ( ) . key set ( ) . iterator ( ) ; string field id ; while ( iterator . has next ( ) ) { field id = iterator . next ( ) ; lyrics3 save field map . put ( field id @$ true ) ; } try { add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword,all id
do n't exceed max file compact threshold <PLACE_HOLDER> : file selection starts with largest to smallest .,compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ max size - __num__ ) @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( max size - __num__ @$ max size - __num__ @$ max size -,max file
the new code browser selection should have <PLACE_HOLDER> of our vertex selections,assert true ( code browser selection . get min address ( ) . equals ( first selection . get min address ( ) ) ) ; assert true ( code browser selection . get max address ( ) . equals ( second selection . get max address ( ) ) ) ;,selection have
legacy parser was signaling all created index antlr is parsing only those @$ which will make any model <PLACE_HOLDER>,int number of created indexes which not make change on tables model = __num__ ; int number of alter view statements = __num__ ; int number of dropped views = __num__ ; assert that ( listener . total ( ) ) . is equal to ( __num__ - number of altered tables which does not exists - number of indexes on non existing tables - number of created indexes which not make change on tables model + number of alter view statements + number of dropped views ) ; listener . for each ( this :: print event ) ;,which make
skip gc 'd weak <PLACE_HOLDER>,for ( hash entry < k @$ v > p = e ; p != last run ; p = p . next ) { k key = p . key ( ) ; if ( key == null ) { reduce ++ ; continue ; } int k = p . hash & size mask ; hash entry < k @$ v > n = new table [ k ] ; new table [ k ] = new hash entry ( key @$ p . hash @$ n @$ p . value ( ) ) ; },gc "d"
set the usage analyzer as parent to make sure that the usage log contains the subclass <PLACE_HOLDER> .,logger . set parent ( logger . get logger ( resource usage analyzer . class . get name ( ) ) ) ;,log contains
the splits have an implicit <PLACE_HOLDER> on the base apk . this means that we have to add the base apk file in addition to the shared libraries .,string base apk name = new file ( info . get base code path ( ) ) . get name ( ) ; string base class path = base apk name ;,splits have
offline compaction should not have created a new <PLACE_HOLDER> .,assert equals ( original if length @$ if file . length ( ) ) ; connectd sand cache ( ) ; dsf = cache . create disk store factory ( ) ; disk store = dsf . create ( name ) ; af = new attributes factory ( ) ; af . set disk store name ( name ) ; af . set data policy ( data policy . persistent_replicate ) ; r = cache . create region ( __str__ @$ af . create ( ) ) ; assert equals ( __num__ @$ r . size ( ) ) ; assert equals ( __str__ @$ r . get ( __str__ ) ) ; assert equals ( __str__ @$ r . get ( __str__ ) ) ;,compaction created
note that @$ by construction in this method @$ a jsr block has at least two successors in the control flow graph : the first one leads the next <PLACE_HOLDER> after the jsr @$ while the second one leads to the jsr target .,next insn = new label ( ) ;,one leads
triggers update system ui visibility lw which will reset the <PLACE_HOLDER> as needed,int finish post layout policy lw = m display policy . focus changed lw ( m window @$ m window ) ; assert equals ( window manager policy . finish_layout_redo_layout @$ finish post layout policy lw ) ; assert equals ( __num__ @$ m display policy . m last system ui flags ) ; assert equals ( __num__ @$ m window . m attrs . system ui visibility ) ; assert inset by top bottom ( m window . get content frame lw ( ) @$ status_bar_height @$ nav_bar_height ) ;,which reset
top left top <PLACE_HOLDER> bottom left bottom <PLACE_HOLDER>,matrix . set poly to poly ( new float [ ] { __num__ @$ __num__ @$ width @$ __num__ @$ __num__ @$ height @$ width @$ height } @$ __num__ @$ m display orientation == __num__ ? new float [ ] { __num__ @$ height @$ __num__ @$ __num__ @$ width @$ height @$ width @$ __num__ } : new float [ ] { width @$ __num__ @$ width @$ height @$ __num__ @$ __num__ @$ __num__ @$ height } @$ __num__ @$ __num__ ) ;,top left
if src area is beyond the bounds of the image @$ we must clip it . the transform is based on the specified area @$ not the clipped <PLACE_HOLDER> .,if ( sx1 < __num__ ) { sx1 = __num__ ; } else if ( sx1 > img width ) { sx1 = img width ; },the clipped
if the historic variable already has a later <PLACE_HOLDER> @$ we do n't need to change its details to something that is already superseded by later data .,if ( historic variable . get last updated time ( ) . after ( time ) ) { return ; },variable has
embms temp files can contain arbitrary <PLACE_HOLDER> .,return __str__ ;,files contain
next load keystore data to build <PLACE_HOLDER> 's,key store ks = key store . get instance ( key store . get default type ( ) ) ; ks . load ( new file input stream ( system . get property ( __str__ @$ __str__ ) + file . separator char + __str__ ) @$ store password ) ; no_store_domain = new protection domain ( new code source ( new url ( __str__ ) @$ ( java . security . cert . certificate [ ] ) null ) @$ null @$ null @$ null ) ;,data build
add conference invite <PLACE_HOLDER> .,invite button = ui component registry . get button factory ( ) . create invite conference button ( ) ; invite button . set tool tip text ( res . get string ( __str__ ) ) ; chat room . add chat room button ( invite button ) ; invite button . add action listener ( this ) ;,conference invite
try to encode two c es as one ce <PLACE_HOLDER> .,if ( ces length == __num__ ) { long ce0 = ces [ __num__ ] ; long ce1 = ces [ __num__ ] ; long p0 = ce0 > > > __num__ ; if ( ( ce0 & __num__ ) == collation . common_secondary_ce && ( ce1 & __num__ ) == collation . common_tertiary_ce && p0 != __num__ ) { return ( int ) p0 | ( ( ( int ) ce0 & __num__ ) << __num__ ) | ( ( ( int ) ce1 > > __num__ ) & __num__ ) | collation . special_ce32_low_byte | collation . latin_expansion_tag ; } },one ce
if header does n't exist @$ method get <PLACE_HOLDER> creates a new random <PLACE_HOLDER>,if ( response packet . get message ( ) . get headers ( ) . get ( av . messageid tag @$ false ) == null ) { string newid = message . generate messageid ( ) ; hl . add ( new string header ( av . messageid tag @$ newid ) ) ; },id creates
perform cache directive operations using a closed file <PLACE_HOLDER> .,distributed file system dfs1 = ( distributed file system ) cluster . get new file system instance ( __num__ ) ; dfs1 . close ( ) ; try { dfs1 . list cache directives ( null ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic test utils . assert exception contains ( __str__ @$ ioe ) ; } try { dfs1 . add cache directive ( alpha ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic test utils . assert exception contains ( __str__ @$ ioe ) ; } try { dfs1 . modify cache directive ( alpha ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic,operations using
if we 're copying in a model we need a model schema v <PLACE_HOLDER> of the right class to fill into .,model m = model metrics . model ( ) ; if ( m != null ) { this . model = new model keyv3 ( m . _key ) ; this . model_category = m . _output . get model category ( ) ; this . model_checksum = m . checksum ( ) ; },schema v
the data is locked in the <PLACE_HOLDER> @$ or we 're ignoring the <PLACE_HOLDER> . bypass the <PLACE_HOLDER> and read from upstream .,if ( next span == null ) { next data source = upstream data source ; next data spec = new data spec ( uri @$ http method @$ http body @$ read position @$ read position @$ bytes remaining @$ key @$ flags ) ; } else if ( next span . is cached ) { uri file uri = uri . from file ( next span . file ) ; long file position = read position - next span . position ; long length = next span . length - file position ; if ( bytes remaining != c . length_unset ) { length = math . min ( length @$ bytes remaining ) ; } next data spec = new data spec ( file uri,cache bypass
record the fact that 'label ' designates a <PLACE_HOLDER> @$ if not already done .,if ( base opcode == opcodes . jsr ) { if ( ( label . flags & label . flag_subroutine_start ) == __num__ ) { label . flags |= label . flag_subroutine_start ; has subroutines = true ; } current basic block . flags |= label . flag_subroutine_caller ; add successor to current basic block ( relative stack size + __num__ @$ label ) ; next basic block = new label ( ) ; } else { relative stack size += stack_size_delta [ base opcode ] ; add successor to current basic block ( relative stack size @$ label ) ; },"label" designates
avoid setting the x include processing <PLACE_HOLDER> if the value is false . this will keep the configuration from throwing an exception if it does not support x include .,if ( spf . isx include aware ( ) ) { xml reader . set feature0 ( xinclude_feature @$ true ) ; } xml reader . set property0 ( xml_security_property_manager @$ f security property mgr ) ; xml reader . set property0 ( security_manager @$ f security manager ) ; if ( secure processing ) { if ( features != null ) { boolean temp = features . get ( xml constants . feature_secure_processing ) ; if ( temp != null ) { if ( temp && constants . is_jdk8_or_above ) { f security property mgr . set value ( xml security property manager . property . access_external_dtd @$ xml security property manager . state . fsp @$ constants . external_access_default_fsp ) ; f security property mgr .,x include
case 23 : true if an attribute named primitive int att name of type int has a <PLACE_HOLDER> equal to int <PLACE_HOLDER> multiplicated by one we cover javax.management.binary rel query exp with a rel op equal to times,queries . add ( query . eq ( query . attr ( primitive int att name ) @$ query . times ( query . value ( int value ) @$ query . value ( __num__ ) ) ) ) ;,name has
check that older snapshot still have the old ec policy <PLACE_HOLDER>,assert null ( __str__ @$ fs . get erasure coding policy ( snap1 ) ) ; assert equals ( __str__ @$ ec63 policy @$ fs . get erasure coding policy ( snap2 ) ) ; assert null ( __str__ @$ fs . get erasure coding policy ( snap3 ) ) ;,snapshot have
run <PLACE_HOLDER> without indexes run all <PLACE_HOLDER> .,for ( int i = __num__ ; i < queries . length ; i ++ ) { try { results [ i ] [ __num__ ] = qs . new query ( __str__ + queries [ i ] ) . execute ( ) ; } catch ( exception e ) { throw new runtime exception ( __str__ + queries [ i ] @$ e ) ; } } qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$,queries run
done is always true until the last stream has be put in the queue . then the stream should be spouting good input <PLACE_HOLDER> .,synchronized ( done ) { return ! done . get ( ) || ! queue . is empty ( ) ; } if ( fail . get ( ) != null ) { throw new re ( fail . get ( ) ) ; } try { return dequeue ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; throw new runtime exception ( e ) ; },stream spouting
the configuration provider will return null instead of throwing out exceptions @$ if there are no configuration <PLACE_HOLDER> provided . rm will not load the remote configuration <PLACE_HOLDER> @$ and should start successfully .,try { rm = new mockrm ( configuration ) ; rm . init ( configuration ) ; rm . start ( ) ; } catch ( exception ex ) { fail ( __str__ ) ; },rm load
now c 2 should replace c <PLACE_HOLDER> for source tag <PLACE_HOLDER> .,pcm . add constraint ( app id1 @$ source tag1 @$ c2 @$ true ) ; assert . assert equals ( __num__ @$ pcm . get constraints ( app id1 ) . size ( ) ) ; assert . assert equals ( c2 @$ pcm . get constraint ( app id1 @$ source tag1 ) ) ;,c replace
synchronized search improves <PLACE_HOLDER> we must create and add new value if there are no needed entry,synchronized ( this . queue ) { current = get entry value ( key @$ hash @$ this . table [ index ( hash @$ this . table ) ] ) ; if ( current != null ) { return current ; } v value = create ( key ) ; objects . require non null ( value @$ __str__ ) ; int index = index ( hash @$ this . table ) ; this . table [ index ] = new cache entry < > ( hash @$ key @$ value @$ this . table [ index ] ) ; if ( ++ this . size >= this . threshold ) { if ( this . table . length == maximum_capacity ) { this . threshold =,search improves
setting volume on ui sounds stream type also controls silent <PLACE_HOLDER>,if ( ( ( flags & audio manager . flag_allow_ringer_modes ) != __num__ ) || ( stream == get ui sounds stream type ( ) ) ) { set ringer mode ( get new ringer mode ( stream @$ index @$ flags ) @$ tag + __str__ @$ false ) ; },type controls
grid near tx prepare <PLACE_HOLDER> unmarshalling failed test,read cnt . set ( __num__ ) ; fail optimistic ( ) ;,grid prepare
system can retrieve the <PLACE_HOLDER> .,m context . binder . clear calling identity ( ) ; assert equals ( bug report request time @$ dpm . get last bug report request time ( ) ) ;,system retrieve
the old rule references another <PLACE_HOLDER> instance and we wa n't to keep the invariant that every rule references the <PLACE_HOLDER> it is contained within,try { rule new rule = builder . create rule ( rule . get label ( ) @$ rule . get rule class object ( ) @$ rule . get location ( ) @$ rule . get attribute container ( ) ) ; new rule . populate output files ( null event handler . instance @$ builder ) ; if ( rule . contains errors ( ) ) { new rule . set contains errors ( ) ; } builder . add rule ( new rule ) ; } catch ( label syntax exception e ) { throw new illegal state exception ( e ) ; },rule references
arrays.as list does not accept <PLACE_HOLDER> as parameter,if ( single == null ) { return feel fn result . of error ( new invalid parameters event ( severity . error @$ __str__ @$ __str__ ) ) ; },list accept
upon error do n't process the <PLACE_HOLDER> .,return false ;,error process
make assignments . the host on top of the set has the smallest total <PLACE_HOLDER>,for ( string id : scores . key set ( ) ) { server assignment least loaded = host scores . get ( __num__ ) ; least loaded . score += scores . get ( id ) ; balanced assignments . put ( id @$ least loaded . host ) ; host scores . sort ( comparator . comparing long ( o -> o . score ) ) ; } return balanced assignments ;,host has
since derby database which is used for unit test does not have <PLACE_HOLDER> in date and time type @$ and put sql converts date string into long representation using local <PLACE_HOLDER> @$ we need to use local <PLACE_HOLDER> .,simple date format time format = new simple date format ( time format string ) ; java . util . date parsed local time = time format . parse ( time str ) ; simple date format date format = new simple date format ( date format string ) ; java . util . date parsed local date = date format . parse ( date str ) ;,database have
saving the tool saves the <PLACE_HOLDER>,tool = save tool ( env . get project ( ) @$ tool ) ;,tool saves
we only register the app advanced <PLACE_HOLDER> as all of the infinispan ones are explicitly defined,add reflection for class ( advanced externalizer . class @$ true @$ app only index @$ reflective class @$ collections . empty set ( ) ) ;,app advanced
exclude locals which do n't have valid <PLACE_HOLDER>,if ( ! var . is valid ( ) ) { continue ; },which have
delete local file to make sure that the get requests <PLACE_HOLDER> from ha,file blob file = server . get storage location ( job id @$ key ) ; assert true ( blob file . delete ( ) ) ;,the get
an attempt is asking if it can commit its output . this can be decided only by the task which is managing the multiple <PLACE_HOLDER> . so redirect the request there .,org . apache . hadoop . mapreduce . v2 . api . records . task attempt id attemptid = type converter . to yarn ( task attemptid ) ; task heartbeat handler . progressing ( attemptid ) ;,which managing
batch @$ legacy @$ jdk <PLACE_HOLDER>,options . set streaming ( false ) ; system . set property ( __str__ @$ __str__ ) ; assert that ( get container image for job ( options ) @$ equal to ( __str__ ) ) ;,batch jdk
all other variable queries with null should throw <PLACE_HOLDER>,try { runtime service . create execution query ( ) . variable value greater than ( __str__ @$ null ) ; fail ( __str__ ) ; } catch ( flowable illegal argument exception ae ) { assert text present ( __str__ @$ ae . get message ( ) ) ; } try { runtime service . create execution query ( ) . variable value greater than or equal ( __str__ @$ null ) ; fail ( __str__ ) ; } catch ( flowable illegal argument exception ae ) { assert text present ( __str__ @$ ae . get message ( ) ) ; } try { runtime service . create execution query ( ) . variable value less than ( __str__ @$ null ) ; fail ( __str__,queries throw
system processes can do <PLACE_HOLDER> @$ and when they do we want to have them trim their memory after the user leaves the <PLACE_HOLDER> . to facilitate this @$ here we need to determine whether or not it is currently showing <PLACE_HOLDER> .,app . system no ui = true ; if ( app == top_app ) { app . system no ui = false ; app . set current scheduling group ( process list . sched_group_top_app ) ; app . adj type = __str__ ; } else if ( app . has top ui ( ) ) { app . system no ui = false ; app . adj type = __str__ ; } else if ( wpc . has visible activities ( ) ) { app . system no ui = false ; },processes do
then delegate to the database metadata driver identifier casing selection which can override these <PLACE_HOLDER> .,return super . build identifier helper ( builder @$ db meta data ) ;,which override
pi array node implements array length <PLACE_HOLDER> and value proxy . we want to treat it as an array length <PLACE_HOLDER> @$ therefore we check this case first .,do { if ( current instanceof array length provider ) { return ( ( array length provider ) current ) . find length ( mode @$ constant reflection ) ; } else if ( current instanceof value phi node ) { return phi array length ( ( value phi node ) current @$ mode @$ constant reflection ) ; } else if ( current instanceof value proxy node ) { value proxy node proxy = ( value proxy node ) current ; value node length = array length ( proxy . get original node ( ) @$ mode @$ constant reflection ) ; if ( mode == array length provider . find length mode . canonicalize_read && length != null && ! length . is constant ( ),node implements
make a store file and write <PLACE_HOLDER> to it .,store file writer writer = new store file writer . builder ( conf @$ cache conf @$ fs ) . with file path ( f ) . with file context ( meta ) . build ( ) ; write store file ( writer ) ; writer . close ( ) ; reader context context = new reader context builder ( ) . with file system and path ( fs @$ f ) . build ( ) ; h file info file info = new h file info ( context @$ conf ) ; store file reader reader = new store file reader ( context @$ file info @$ cache conf @$ new atomic integer ( __num__ ) @$ conf ) ; file info . init meta and index,store file
this can only happen for a throwable thats not an exception @$ i.e . an <PLACE_HOLDER>,if ( ! success && ! state changed ) { unlock ( ) ; },throwable thats
make sure both servers can deserialize the <PLACE_HOLDER>,serverb . invoke ( ( ) -> assert equals ( new simple class ( __num__ @$ ( byte ) __num__ ) @$ get cache ( ) . get region ( __str__ ) . get ( __str__ ) ) ) ; servera . invoke ( ( ) -> assert equals ( new simple class ( __num__ @$ ( byte ) __num__ ) @$ get cache ( ) . get region ( __str__ ) . get ( __str__ ) ) ) ;,servers deserialize
implementors should override this <PLACE_HOLDER>,throw new unsupported operation exception ( __str__ + this . get class ( ) . get name ( ) + __str__ + __str__ + this . get class ( ) . get package ( ) . get specification title ( ) + __str__ + __str__ + this . get class ( ) . get package ( ) . get specification version ( ) + __str__ ) ;,implementors override
assertion on auto onboard another dummy data <PLACE_HOLDER>,metadata source config another dummymd source = ds to onboards map . get ( __str__ ) . get ( __num__ ) . get metadata source config ( ) ; assert . assert equals ( another dummymd source . get class name ( ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . size ( ) @$ __num__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __num__ ) ;,assertion onboard
bacause script pid utils.is pid running do n't support <PLACE_HOLDER>,if ( ! os . is family unix ( ) || os . is family mac ( ) ) { return ; } final int shard count = __num__ ; final string job name = __str__ ; job config job config = new job config ( ) ; job config . set job name ( job name ) ; job config . set cron ( __str__ ) ; job config . set job type ( job type . shell_job . to string ( ) ) ; job config . set job class ( longtime java job . class . get canonical name ( ) ) ; job config . set timeout seconds ( __num__ ) ; job config . set sharding total count ( shard count ) ;,utils.is support
drawable container ' constant state has drawables <PLACE_HOLDER> . in order to leave the constant state intact in the cache @$ we need to create a new drawable container after added to cache .,if ( dr instanceof drawable container ) { needs new drawable after cache = true ; },state drawables
if overwritten not specified @$ it should be <PLACE_HOLDER> user specified,verify queue mapping ( new queue mapping ( mapping type . group @$ __str__ @$ __str__ @$ __str__ ) @$ __str__ @$ __str__ @$ __str__ @$ true ) ;,user specified
0 x 100415 a : p 1 and p 2 have same plate <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . plate_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . plate_comment ) ; program merge = new program merge manager ( p1 @$ p2 @$ task monitor adapter . dummy_monitor ) ; program merge . set diff filter ( new program diff filter ( program diff filter . comment_diffs ) ) ; program merge . set merge filter ( new program merge filter ( program merge filter . comments @$ program merge filter . merge ) ) ; address set as = new address set ( ) ; as . add range ( addr ( __num__ ) @$ addr ( __num__ ) ) ; as . add range (,0 have
no key id specified @$ return no <PLACE_HOLDER>,if ( sought keyid == null ) { return mono . just ( collections . empty list ( ) ) ; },id return
0 x 10018 a 6 : op 0 has reg <PLACE_HOLDER> to esi . 0 x 100295 a : op 1 has reg <PLACE_HOLDER> to cx . 0 x 1002 d 0 b : op 0 has reg <PLACE_HOLDER> to edi ; op 1 has reg <PLACE_HOLDER> to eax . 0 x 10033 fe : op 0 both have reg <PLACE_HOLDER> to edi .,mtf . initialize ( __str__ @$ new program modifier listener ( ) { @ override public void modify latest ( programdb program ) { int tx id = program . start transaction ( __str__ ) ; boolean commit = false ; try { program context context = program . get program context ( ) ; register eax reg = context . get register ( __str__ ) ; reference manager ref mgr = program . get reference manager ( ) ; reference [ ] refs ; refs = ref mgr . get references from ( addr ( program @$ __str__ ) @$ __num__ ) ; assert equals ( __num__ @$ refs . length ) ; ref mgr . update ref type ( refs [ __num__ ] @$ ref type,op has
message has no <PLACE_HOLDER> @$ subject has default <PLACE_HOLDER>,control = xml builder . create ( __str__ ) . a ( __str__ @$ __str__ ) . a ( __str__ @$ __str__ ) . a ( __str__ @$ __str__ ) . a ( __str__ @$ __str__ ) . e ( __str__ ) . a ( __str__ @$ default language ) . t ( default language ) . as string ( output properties ) ; message = packet parser utils . parse message ( packet parser utils . get parser for ( control ) ) ; assert false ( message . get subject languages ( ) . is empty ( ) ) ; assert equals ( default language @$ message . get subject ( default language ) ) ; assert null ( message . get subject ( other language,message has
a user in role contractor can only do a <PLACE_HOLDER> if confidential,response = _connector . get response ( __str__ + __str__ + encoded chris + __str__ + __str__ ) ; assert that ( response @$ starts with ( __str__ ) ) ; assert that ( response @$ contains string ( __str__ ) ) ;,user do
if the callback executes <PLACE_HOLDER> @$ we close the dialog,if ( callback != null ) { close = callback . execute ( old master password @$ get new password ( ) ) ; } if ( close ) { dialog = null ; dispose ( ) ; },callback executes
p 2 post comments contain the p 1 comment <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . post_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . post_comment ) ; check comment difference ( __num__ ) ;,comments contain
invalidate cached accessibility <PLACE_HOLDER> .,m touch helper . invalidate root ( ) ;,invalidate cached
we fake only once the expired iterator exception at the specified get records attempt <PLACE_HOLDER>,if ( ( integer . value of ( shard iterator ) == order of call to expire - __num__ ) && ! expired once already ) { expired once already = true ; throw new expired iterator exception ( __str__ ) ; } else if ( expired once already && ! expired iterator refreshed ) { throw new runtime exception ( __str__ ) ; } else { return new get records result ( ) . with records ( shard itr to record batch . get ( shard iterator ) ) . with millis behind latest ( millis behind latest ) . with next shard iterator ( ( integer . value of ( shard iterator ) == total num of get records calls - __num__ ) ? null :,records attempt
we use recursive algorithm with pure complexity and do n't want it to take forever stacked barcode can have up to 11 <PLACE_HOLDER> @$ so 25 seems reasonable enough,if ( this . rows . size ( ) > __num__ ) { this . rows . clear ( ) ; return null ; },barcode have
1 : no stats since hbm.xml ca n't enable natural id <PLACE_HOLDER>,assert equals ( __num__ @$ session factory ( ) . get statistics ( ) . get natural id cache hit count ( ) ) ;,stats enable
this flag will be set to false after first incremental load is done . this flag is used by repl copy task to check if duplicate file check is required or not . this flag is used by compaction to check if compaction can be done for this database or not . if compaction is done before first incremental then duplicate check will fail,parameters . put ( repl utils . repl_first_inc_pending_flag @$ __str__ ) ; return parameters ;,compaction change
old line spacing . all lines should get their <PLACE_HOLDER> and descents from the first font .,static layout layout = static layout . builder . obtain ( text @$ __num__ @$ text . length ( ) @$ paint @$ para width ) . set include pad ( false ) . set use line spacing from fallbacks ( false ) . build ( ) ; assert equals ( __num__ @$ layout . get line count ( ) ) ; assert equals ( - text size @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * text size @$ layout . get line descent ( __num__ ) ) ; assert equals ( - text size @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * text size @$ layout . get line descent (,lines get
m device idle controller.step idle state locked does n't handle the active <PLACE_HOLDER> @$ so the state should stay as active .,verify state conditions ( state_active ) ;,state handle
oh well @$ we tried . this port probably wo n't work next <PLACE_HOLDER> they pick it .,server socket = null ;,port work
completing the task will end the process <PLACE_HOLDER>,task service . complete ( task . get id ( ) ) ; assert process ended ( proc id ) ;,task end
jwt proxy container does n't need proxy <PLACE_HOLDER> since it never does any outbound requests @$ and setting of it may fail accessing internal addresses .,k8s env . get pods data ( ) . entry set ( ) . stream ( ) . filter ( entry -> ! entry . get key ( ) . equals ( jwt_proxy_pod_name ) ) . flat map ( entry -> stream . concat ( entry . get value ( ) . get spec ( ) . get containers ( ) . stream ( ) @$ entry . get value ( ) . get spec ( ) . get init containers ( ) . stream ( ) ) ) . for each ( container -> proxy env vars . for each ( ( k @$ v ) -> container . get env ( ) . add ( new env var ( k @$ v @$ null ) ),container need
zero distance means a <PLACE_HOLDER>,if ( distance == __num__ ) { byte x = input [ in offset + ip - __num__ ] ; while ( ip < ip bound ) { if ( input [ in offset + ref ++ ] != x ) { break ; } else { ip ++ ; } } } else { for ( ; ; ) { if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ],distance means
top left bottom left bottom <PLACE_HOLDER> top <PLACE_HOLDER>,top card . set card vertices ( new float [ ] { __num__ @$ view height @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ view width / __num__ @$ __num__ @$ __num__ @$ view width / __num__ @$ view height @$ __num__ } ) ;,bottom left
new user gets registration <PLACE_HOLDER>,new user . set activation key ( random util . generate activation key ( ) ) ; set < authority > authorities = new hash set < > ( ) ; authority repository . find by id ( authorities constants . user ) . if present ( authorities :: add ) ; new user . set authorities ( authorities ) ; user repository . save ( new user ) ; log . debug ( __str__ @$ new user ) ; return new user ;,user gets
we can leave the nested loop . every <PLACE_HOLDER> in the query can match at most one <PLACE_HOLDER> in the term .,term character match found = true ;,character match
some types do n't have a static <PLACE_HOLDER> @$ that is ok,have looked for static initializer = true ;,types have
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
if remote client have <PLACE_HOLDER>,if ( local video port > __num__ ) { if ( settings manager . get local preferences ( ) . get video device ( ) != null && ! __str__ . equals ( settings manager . get local preferences ( ) . get video device ( ) ) ) { video media session video media session = media manager . create video media session ( call . get remote sdp description ( ) . to string ( ) @$ local video port ) ; if ( video media session != null ) { video media session . start trasmit ( ) ; video media session . start receive ( ) ; call . set video media session ( video media session ) ; } } } evt .,client have
h<PLACE_HOLDER>yes modem with dos configur<PLACE_HOLDER>tor,hayes . accept ( con dos ) ;,instructions addis
prepare the field object <PLACE_HOLDER>,struct object inspector soi = ( struct object inspector ) obj inspector ; list < ? extends struct field > fields = soi . get all struct field refs ( ) ; list < object > list = soi . get struct fields data as list ( obj ) ; list < ? extends struct field > declared fields = ( serde params . get row type info ( ) != null && ( ( struct type info ) serde params . get row type info ( ) ) . get all struct field names ( ) . size ( ) > __num__ ) ? ( ( struct object inspector ) get object inspector ( ) ) . get all struct field refs ( ) : null ;,field object
our randomly generated <PLACE_HOLDER> significantly .,return name . length ( ) == __num__ ;,our generated
in a <PLACE_HOLDER> class @$ this.chardata has one <PLACE_HOLDER> @$ that is to say @$ a pair of surrogates is composed and stored to this.chardata .,if ( this . context == s_inbrackets ) { switch ( ch ) { case __str__ : ret = t_backsolidus ; if ( this . offset >= this . regexlen ) throw ex ( __str__ @$ this . offset - __num__ ) ; this . chardata = this . regex . char at ( this . offset ++ ) ; break ; case __str__ : if ( this . offset < this . regexlen && this . regex . char at ( this . offset ) == __str__ ) { this . offset ++ ; ret = t_xmlschema_cc_subtraction ; } else ret = t_char ; break ; case __str__ : if ( ! this . is set ( regular expression . xmlschema_mode ) && this . offset <,this.chardata has
lets change some <PLACE_HOLDER> to best fit the view .,m week view . set column gap ( ( int ) typed value . apply dimension ( typed value . complex_unit_dip @$ __num__ @$ get resources ( ) . get display metrics ( ) ) ) ; m week view . set text size ( ( int ) typed value . apply dimension ( typed value . complex_unit_sp @$ __num__ @$ get resources ( ) . get display metrics ( ) ) ) ; m week view . set event text size ( ( int ) typed value . apply dimension ( typed value . complex_unit_sp @$ __num__ @$ get resources ( ) . get display metrics ( ) ) ) ;,lets change
j 2 se does not support xalan <PLACE_HOLDER>,sax transformer factory stf = ( sax transformer factory ) tfactory ;,se support
the instance class loader should n't have <PLACE_HOLDER> of the resources yet,instance class loader = extension manager . get instance class loader ( id ) ; assert not null ( instance class loader ) ; assert true ( contains resource ( instance class loader . getur ls ( ) @$ resource1 ) ) ; assert true ( contains resource ( instance class loader . getur ls ( ) @$ resource2 ) ) ; assert true ( contains resource ( instance class loader . getur ls ( ) @$ resource3 ) ) ; assert equals ( __num__ @$ instance class loader . get additional resource urls ( ) . size ( ) ) ;,loader have
a change in priority is only relevant for static rr os : specifically @$ a regular rro should not have its state <PLACE_HOLDER> only because a change in priority,if ( the truth . is static overlay package ( ) && the truth . overlay priority != old settings . priority ) { return true ; } return false ;,rro have
hsql erroneously matches an extra <PLACE_HOLDER> ?,if ( ! ishsql ( ) ) { validate table of longs ( client @$ __str__ @$ empty_table ) ; },hsql matches
emergency calling requires voice <PLACE_HOLDER> .,if ( m is voice capable ) { if ( is in call ( ) ) { visible = true ; } else { final boolean sim locked = keyguard update monitor . get instance ( m context ) . is sim pin voice secure ( ) ; if ( sim locked ) { visible = m enable emergency call while sim locked ; } else { visible = m lock pattern utils . is secure ( keyguard update monitor . get current user ( ) ) ; } } },calling requires
if the focus changes very quickly before the first <PLACE_HOLDER> is returned each focus change triggers a new partition and we end up with many duplicate partitions . this is enhanced as the focus change can be much faster than the taking of the assist structure . hence remove the currently queued <PLACE_HOLDER> and replace it with the one queued after the structure is,cancel current request locked ( ) ; try { final bundle receiver extras = new bundle ( ) ; receiver extras . put int ( extra_request_id @$ request id ) ; final long identity = binder . clear calling identity ( ) ; try { if ( ! activity task manager . get service ( ) . request autofill data ( m assist receiver @$ receiver extras @$ m activity token @$ flags ) ) { slog . w ( tag @$ __str__ + m activity token ) ; } } finally { binder . restore calling identity ( identity ) ; } } catch ( remote exception e ) { },one fill
does this target accept the drop action <PLACE_HOLDER> being dropped on it ?,int da = e . get drop action ( ) ; if ( ( da & drop actions ) == __num__ ) { return false ; },target accept
if the new kproject has a different <PLACE_HOLDER> than the original one it has to be initialized,if ( class loader != k project . get class loader ( ) ) { k project . init ( ) ; },kproject has
print suppressed <PLACE_HOLDER> @$ if any,for ( throwable se : get suppressed ( ) ) se . print enclosed stack trace ( s @$ trace @$ suppressed_caption @$ __str__ @$ deja vu ) ;,print suppressed
resets the content view scroll <PLACE_HOLDER> when swiping the panel down after being maximized .,if ( m content != null && ty > __num__ && get panel state ( ) == panel state . maximized ) { m content . reset content view scroll ( ) ; },content view
separately handle the case where the grid has no table <PLACE_HOLDER> . in this situation @$ there is a single default column .,if ( column count == __num__ ) { this . column = ( column == __num__ ) ? __num__ : - __num__ ; layout ( ) ; return ; } if ( this . column > - __num__ && this . column < column count ) { this . column = - __num__ ; } if ( column < __num__ || column >= grid . get column count ( ) ) return ; this . column = column ; layout ( ) ;,grid has
a volt db extension to customize the sql function set <PLACE_HOLDER>,volt disabled = disabled_in_functionsql_constructor ;,extension customize
we have converted the sax events generated by tika into a dom object so we can now use the usual html resources from nutch get meta <PLACE_HOLDER>,html meta processor . get meta tags ( meta tags @$ root @$ base ) ; if ( log . is trace enabled ( ) ) { log . trace ( __str__ + base + __str__ + meta tags . to string ( ) ) ; },resources get
fail silently . apparently some other event closed the debug <PLACE_HOLDER> .,if ( ! debugger . is connected ( ) ) { return ; },event closed
0 x 10018 a 6 : op 0 has reg <PLACE_HOLDER> to esi . 0 x 100295 a : op 1 has reg <PLACE_HOLDER> to cx . 0 x 1002 d 0 b : op 0 has reg <PLACE_HOLDER> to edi ; op 1 has reg <PLACE_HOLDER> to eax . 0 x 10033 fe : op 0 both have reg <PLACE_HOLDER> to edi .,mtf . initialize ( __str__ @$ new program modifier listener ( ) { @ override public void modify latest ( programdb program ) { int tx id = program . start transaction ( __str__ ) ; boolean commit = false ; try { program context context = program . get program context ( ) ; register esi reg = context . get register ( __str__ ) ; register cx reg = context . get register ( __str__ ) ; reference manager ref mgr = program . get reference manager ( ) ; reference [ ] refs ; reference new ref ; refs = ref mgr . get references from ( addr ( program @$ __str__ ) @$ __num__ ) ; assert equals ( __num__ @$ refs . length,op has
get the partition id and its crc and validate it . validating the partition id for the chunk separately makes it possible to continue processing <PLACE_HOLDER> from other partitions if only one partition has corrupt <PLACE_HOLDER> in the file .,final checksum partition idcrc = m_checksum type == checksum type . crc32c ? new pure java crc32c ( ) : new pure java crc32 ( ) ; chunk lengthb . mark ( ) ; final int next chunk partition id = chunk lengthb . get int ( ) ; final int next chunk partition idcrc = chunk lengthb . get int ( ) ; chunk lengthb . reset ( ) ; byte partition id bytes [ ] = new byte [ __num__ ] ; chunk lengthb . get ( partition id bytes ) ; partition idcrc . update ( partition id bytes @$ __num__ @$ partition id bytes . length ) ; int generated value = ( int ) partition idcrc . get value ( ) ; if,partition has
the monitor we are exiting is not verifiably the one on the top of our monitor stack . this causes a monitor <PLACE_HOLDER> .,if ( ! actual . is lock reference ( ) || ! expected . equal ( actual ) ) { _monitor_top = bad_monitors ; _monitor_safe = false ; basic block bb = get basic block containing ( bci ) ; bb . set changed ( true ) ; bb . _monitor_top = bad_monitors ; if ( trace monitor mismatch ) { report monitor mismatch ( __str__ ) ; } } else { replace allcts matches ( actual @$ cell type state . make line ref ( bci ) ) ; },monitor causes
skip animations @$ just show the correct final <PLACE_HOLDER> .,jump to current state ( ) ;,animations show
create the index scan <PLACE_HOLDER> with all its metadata,scan node . set lookup type ( path . lookup type ) ; scan node . set bindings ( path . bindings ) ; scan node . set end expression ( expression util . combine predicates ( expression type . conjunction_and @$ path . end exprs ) ) ; if ( ! path . index . get predicatejson ( ) . is empty ( ) ) { try { scan node . set partial index predicate ( abstract expression . fromjson string ( path . index . get predicatejson ( ) @$ table scan ) ) ; } catch ( json exception e ) { throw new planning error exception ( e . get message ( ) @$ __num__ ) ; } } scan node . set predicate,index scan
check strip hidden configurations removes the <PLACE_HOLDER>,configuration conf2 = new configuration ( conf ) ; conf2 . set ( hidden config @$ __str__ ) ; conf . strip hidden configurations ( conf2 ) ;,configurations removes
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default surrogate profile = new crawl profile ( crawl_profile_surrogate @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_surrogate_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ false @$ false @$ true @$ false @$ false @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . nocache @$ __str__ + crawl_profile_surrogate @$ client identification . yacy intranet crawler agent name @$ null @$ null,country match
now compare <PLACE_HOLDER> of row .,lpart = ( left far delimiter < __num__ ? llength + loffset : left far delimiter ) - left delimiter ; rpart = ( right far delimiter < __num__ ? rlength + roffset : right far delimiter ) - right delimiter ; result = super . compare rows ( left @$ left delimiter @$ lpart @$ right @$ right delimiter @$ rpart ) ; if ( result != __num__ ) { return result ; } else { if ( left delimiter < __num__ && right delimiter >= __num__ ) { return - __num__ ; } else if ( right delimiter < __num__ && left delimiter >= __num__ ) { return __num__ ; } else if ( left delimiter < __num__ && right delimiter < __num__ ) { return,now compare
replication factor does <PLACE_HOLDER>,assert true ( bc known10 . compare to ( bc known1000 ) < __num__ ) ; assert true ( bc unknown10 . compare to ( bc unknown1000 ) < __num__ ) ;,factor does
these key managers may implement <PLACE_HOLDER> ... see above ...,for ( src = __num__ @$ dst = __num__ ; src < tma . length ; ) { if ( ! ( tma [ src ] instanceof javax . net . ssl . trust manager ) ) { if ( tma [ src ] instanceof x509 trust manager ) { tmaw [ dst ] = ( javax . net . ssl . trust manager ) new x509 trust manager javax wrapper ( ( x509 trust manager ) tma [ src ] ) ; dst ++ ; } } else { tmaw [ dst ] = ( javax . net . ssl . trust manager ) tma [ src ] ; dst ++ ; } src ++ ; },managers implement
call this first @$ to let the parent prepare the <PLACE_HOLDER>,new loading dialog ( this @$ r . string . afc_msg_loading @$ false ) { @ override protected void on pre execute ( ) { super . on pre execute ( ) ; switch ( display prefs . get view type ( file chooser activity . this ) ) { case grid : display prefs . set view type ( file chooser activity . this @$ view type . list ) ; break ; case list : display prefs . set view type ( file chooser activity . this @$ view type . grid ) ; break ; } setup view files ( ) ; if ( build . version . sdk_int >= build . version_codes . honeycomb ) activity compat . invalidate options menu ( file chooser,parent prepare
this point is only reached if the operation failed <PLACE_HOLDER> than the allowed retry count,log . warn ( __str__ + __str__ + __str__ @$ m key @$ m upload id @$ m bucket name @$ property key . underfs_cleanup_enabled . get name ( ) @$ last exception ) ;,operation failed
else this schema index provider does n't have any persistent <PLACE_HOLDER> to delete .,delete obsolete indexes = true ;,provider have
hopefully given collection wo n't grow more than 10 <PLACE_HOLDER> between now and the synchronized block ...,int initial = math . min ( c . size ( ) + __num__ @$ max ) ; int count = __num__ ; array list < t > list = new array list < t > ( initial ) ; synchronized ( c ) { iterator < t > iter = c . iterator ( ) ; while ( iter . has next ( ) && ( count < max ) ) { list . add ( iter . next ( ) ) ; count ++ ; } } return list ;,collection grow
selecting a item with enter will lose the <PLACE_HOLDER> and selected item @$ which means that further keyboard selection wo n't work unless we do this :,if ( event . get type int ( ) == event . onkeydown && event . get key code ( ) == key codes . key_enter ) { final menu item item = get selected item ( ) ; super . on browser event ( event ) ; scheduler . get ( ) . schedule deferred ( new scheduled command ( ) { @ override public void execute ( ) { select item ( item ) ; focus ( ) ; } } ) ; } else { super . on browser event ( event ) ; },item lose
tree display prefs <PLACE_HOLDER>,box tree display prefs = box . create vertical box ( ) ; tree display prefs . set border ( border factory . create titled border ( __str__ ) ) ; j panel tree display options = new j panel ( ) ; tree display options . set layout ( new grid layout ( __num__ @$ __num__ ) ) ; j label font name = new j label ( __str__ ) ; final j combo box font picker = new j combo box ( graphics environment . get local graphics environment ( ) . get available font family names ( ) ) ; font picker . set selected item ( preferences . get font ( ) ) ; j label size label = new j label ( __str__,display prefs
state change listener sets final query <PLACE_HOLDER> and then clears scheduler when the query finishes .,sql query scheduler scheduler = query scheduler . get ( ) ; optional < query info > final query info = state machine . get final query info ( ) ; if ( final query info . is present ( ) ) { return final query info . get ( ) . get query stats ( ) . get total memory reservation ( ) ; } if ( scheduler == null ) { return new data size ( __num__ @$ byte ) ; } return succinct bytes ( scheduler . get total memory reservation ( ) ) ;,listener sets
small chance that an unexpected message was delivered after this check @$ but we would catch that next time we check <PLACE_HOLDER>,assert equals ( __num__ @$ watchers [ i ] . events . size ( ) ) ;,time check
forward any remaining selected <PLACE_HOLDER> .,if ( batch . size > __num__ ) { forward big table batch ( batch ) ; },any selected
see if we only have a mnemonic and no operands . if so @$ then just add the mnemonic to the preview <PLACE_HOLDER> and increment our counter .,if ( metadata != null ) { if ( metadata . get operands ( ) == null || metadata . get operands ( ) . size ( ) == __num__ ) { mask container mask container = metadata . get mask container ( ) ; if ( mask container != null && mask container . get value ( ) != null ) { instr size = mask container . get value ( ) . length ; } } else if ( metadata . get operands ( ) != null ) { operand metadata operand = metadata . get operands ( ) . get ( __num__ ) ; if ( operand == null ) { continue ; } instr size = operand . get mask container ( ) .,counter add
check that the listener on the animation that was being clone receive the animation lifecycle <PLACE_HOLDER> for the clones .,assert true ( only contains ( started animators @$ s1 @$ s2 @$ s3 ) ) ; assert true ( only contains ( canceled animators @$ s3 ) ) ; assert true ( only contains ( ended animators @$ s1 @$ s2 @$ s3 ) ) ;,animation lifecycle
<PLACE_HOLDER> equals 1 : all byte buffers have array . <PLACE_HOLDER> equals 2 : all byte buffers are direct byte buffers . <PLACE_HOLDER> equals 3 : some byte buffers are direct and some have array . <PLACE_HOLDER> greater than 3 : other cases .,int flag = __num__ ;,buffers have
the ipport does not represent any existing h 2 o cloud <PLACE_HOLDER> or client,throw new illegal argument exception ( __str__ + node idx + __str__ ) ;,ipport represent
another call to get <PLACE_HOLDER> should give 3 <PLACE_HOLDER> instead of 2,assert that ( groups . get groups ( __str__ ) . size ( ) ) . is equal to ( __num__ ) ;,call give
when we get here @$ the test is pretty much over . which side threw the <PLACE_HOLDER> ?,exception local ; exception remote ; string which remote ; if ( separate server thread ) { remote = server exception ; local = client exception ; which remote = __str__ ; } else { remote = client exception ; local = server exception ; which remote = __str__ ; },side threw
verify the distance is zero as long as two nodes have the same <PLACE_HOLDER> . they do n't need to refer to the same object .,node base node1 = new node base ( data nodes [ __num__ ] . get host name ( ) @$ data nodes [ __num__ ] . get network location ( ) ) ; node base node2 = new node base ( data nodes [ __num__ ] . get host name ( ) @$ data nodes [ __num__ ] . get network location ( ) ) ; assert equals ( __num__ @$ cluster . get distance ( node1 @$ node2 ) ) ;,nodes have
if mode is add <PLACE_HOLDER> check if the repository name does not exist in the repository list <PLACE_HOLDER> close this dialog if mode is edit <PLACE_HOLDER> check if the repository name is the same as before if not check if the new name does not exist in the repository . otherwise return true to this method @$ which will mean that repository already exist,if ( meta . get name ( ) != null ) { if ( mode == mode . add ) { if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ; } else { display repository already exist message ( meta . get name ( ) ) ; } } else { if ( master repository name . equals ( meta . get name ( ) ) ) { repository meta = meta ; hide ( ) ; } else if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ;,name exist
default constructor requires same <PLACE_HOLDER> flags . exception : enum constructor which is always private,if ( ! is enum && ( ( class acces flags & accessibility_flags ) != ( method access flags & accessibility_flags ) ) ) { return false ; } int count = __num__ ; for ( struct method mt : cl . get methods ( ) ) { if ( code constants . init_name . equals ( mt . get name ( ) ) ) { if ( ++ count > __num__ ) { return false ; } } } return true ;,constructor requires
class we must guarantee that only one thread at a time uses our number <PLACE_HOLDER> .,synchronized ( number format ) { return ( number format ) number format . clone ( ) ; },thread uses
quick exit : if the filesystem does not support <PLACE_HOLDER> @$ we can exit early .,if ( ! is encryption supported ( ) ) { return device policy manager . encryption_status_unsupported ; },filesystem support
every adapter has an unique index <PLACE_HOLDER>,for ( adapter adapter : adapters ) { adapter data observer observer = new adapter data observer ( m total @$ m index gen == null ? m index ++ : m index gen . increment and get ( ) ) ; adapter . register adapter data observer ( observer ) ; has stable ids = has stable ids && adapter . has stable ids ( ) ; layout helper helper = adapter . on create layout helper ( ) ; helper . set item count ( adapter . get item count ( ) ) ; m total += helper . get item count ( ) ; helpers . add ( helper ) ; pair = pair . create ( observer @$ adapter ) ; m index ary,adapter has
thread does not have a preference so return <PLACE_HOLDER>,if ( b == null ) { return ! share sockets ; } else { return b ; },preference return
kudu does n't support general not predicates @$ but some not operators can be converted into kudu predicates . see <PLACE_HOLDER> to predicates below .,translate ( root . get children ( ) . get ( __num__ ) @$ leaves @$ ! is not @$ schema @$ results ) ; return ; case leaf : predicate leaf leaf = leaves . get ( root . get leaf ( ) ) ; if ( schema . has column ( leaf . get column name ( ) ) ) { results . add all ( leaf to predicates ( leaf @$ is not @$ schema ) ) ; } return ; case constant : return ;,some see
change the default <PLACE_HOLDER> for byte data type ; should not affect the typedef default <PLACE_HOLDER>,settings . set long ( __str__ @$ format settings definition . binary ) ; bdefs [ __num__ ] . copy setting ( settings @$ default settings ) ; def settings = td . get default settings ( ) ; assert null ( def settings . get value ( __str__ ) ) ;,settings affect
before or after the boolean flag introduces the <PLACE_HOLDER> .,construction exception e = assert throws ( __str__ + __str__ @$ construction exception . class @$ ( ) -> construct ( example prefixed foo options . class @$ example bar was named foo option . class ) ) ; assert that ( e ) . has cause that ( ) . is instance of ( duplicate option declaration exception . class ) ; assert that ( e ) . has message that ( ) . contains ( __str__ ) ; e = assert throws ( __str__ + __str__ @$ construction exception . class @$ ( ) -> construct ( example bar was named foo option . class @$ example prefixed foo options . class ) ) ; assert that ( e ) . has cause that ( ),flag introduces
if user code does n't call end <PLACE_HOLDER> @$ the framework automatically does with no cause .,log . end response ( ) ; assert that ( log . response duration nanos ( ) ) . is zero ( ) ; assert that ( log . response cause ( ) ) . is same as ( error ) ;,code call
replace spaces because importer ca n't read attributes <PLACE_HOLDER> in quotes,writer . append ( __str__ ) ;,importer read
the aqua caret does not remove <PLACE_HOLDER> as a listener,for ( property change listener l : listeners ) { if ( l . get class ( ) . get simple name ( ) . contains ( __str__ ) ) { text area . remove property change listener ( l ) ; } },caret remove
recycler view has also the same <PLACE_HOLDER> to provide better performance .,m column header recycler view . set has fixed size ( has fixed width ) ;,view has
now @$ using error.two inner profile @$ which uses another <PLACE_HOLDER> :,assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ; assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ; assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ;,which uses
no known conditional special case <PLACE_HOLDER> @$ use a normal <PLACE_HOLDER>,if ( has slot ( exc word @$ exc_full_mappings ) ) { long value = get slot value and offset ( exc word @$ exc_full_mappings @$ exc offset ) ; full = ( int ) value & __num__ ; exc offset = ( int ) ( value > > __num__ ) + __num__ ; exc offset += full & full_lower ; full >>= __num__ ; exc offset += full & __num__ ; full >>= __num__ ; if ( upper not title ) { full &= __num__ ; } else { exc offset += full & __num__ ; full = ( full > > __num__ ) & __num__ ; } if ( full != __num__ ) { try { out . append ( exceptions @$ exc offset @$ exc,case use
system scoped <PLACE_HOLDER> which should,d . set scope ( artifact . scope_system ) ; file file = new file ( get basedir ( ) @$ __str__ ) ; assert true ( file . exists ( ) ) ; d . set system path ( file . get canonical path ( ) ) ; artifact = repository system . create dependency artifact ( d ) ;,system scoped
let linger run its <PLACE_HOLDER> .,callback . assert no callback ( ) ; final int linger timeout ms = m service . m linger delay ms + m service . m linger delay ms / __num__ ; callback . expect callback ( callback entry . lost @$ m cell network agent @$ linger timeout ms ) ;,linger run
verify only the other tenant has the new state <PLACE_HOLDER>,final tenant key value empty tenant key1 = tenant api . get plugin payment state machine config ( plugin_name @$ request options for original tenant ) ; assert . assert equals ( empty tenant key1 . get values ( ) . size ( ) @$ __num__ ) ; final tenant key value tenant key1 other tenant = tenant api . get plugin payment state machine config ( plugin_name @$ request options ) ; assert . assert equals ( tenant key1 other tenant . get key ( ) @$ tenantkv . tenant key . plugin_payment_state_machine_ . to string ( ) + plugin_name ) ; assert . assert equals ( tenant key1 other tenant . get values ( ) . size ( ) @$ __num__ ) ;,tenant has
share ud set includes <PLACE_HOLDER> that share same upgrade domain with another datanode .,list < t > shareud set = get shareud set ( ud map ) ;,domain includes
if properties is null @$ it means a resource was newly added but the props were cleared so as to load it upon future requests . so lets <PLACE_HOLDER> a load by asking a properties list .,if ( properties == null ) { get props ( ) ; },load lets
no room on current <PLACE_HOLDER> . so start a new <PLACE_HOLDER> .,pw . println ( ) ; line length = __num__ ;,room start
ok let 's set the default parameter exclusion list evaluate the <PLACE_HOLDER> to load it from an external file ...,if ( this . excluded params . is empty ( ) ) { add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_post_data @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_post_data @$ __str__ ) ; add scanner param,list evaluate
bar refcount has reached <PLACE_HOLDER> @$ a destroying task is scheduled,assert equals ( __num__ @$ scheduled destroy tasks . size ( ) ) ; scheduled destroy task = scheduled destroy tasks . poll ( ) ; assert equals ( shared resource holder . destroy_delay_seconds @$ scheduled destroy task . get delay ( time unit . seconds ) ) ;,refcount reached
delete table once more ; the resource not found <PLACE_HOLDER> swallowed silently,ddbms . destroy ( ) ; verify table not exist ( table name @$ dynamodb ) ; intercept ( io exception . class @$ __str__ @$ __str__ @$ ( ) -> ddbms . list children ( test path ) ) ; ddbms . destroy ( ) ; intercept ( file not found exception . class @$ __str__ @$ __str__ @$ ( ) -> ddbms . prune ( prune mode . all_by_modtime @$ __num__ ) ) ; destroy ( ddbms ) ;,resource found
ensure that the enum type does n't match a reserved word or the variable name does n't match the generated enum type or the swift compiler will generate an <PLACE_HOLDER>,if ( is reserved word ( property . datatype with enum ) || to var name ( property . name ) . equals ( property . datatype with enum ) ) { enum name = property . datatype with enum + __str__ ; },compiler generate
a very slow export decoder must have noticed the export <PLACE_HOLDER> shutting down,if ( m_closed ) { export log . info ( __str__ ) ; container . internal discard ( ) ; } else { m_pending container . set ( container ) ; },decoder noticed
the input object inspectors describe the <PLACE_HOLDER> and values of the big table and small table .,big table standard object inspector = object inspector factory . get standard struct object inspector ( big table column name list @$ big table object inspector list ) ; small table standard object inspector = object inspector factory . get standard struct object inspector ( small table column name list @$ small table object inspector list ) ; input object inspectors = new object inspector [ ] { big table standard object inspector @$ small table standard object inspector } ;,inspectors describe
pkix validation has failed . throw an <PLACE_HOLDER> .,if ( expecting trust manager . has exception ( ) ) { throw expecting trust manager . get exception ( ) ; },pkix throw
note to translators : the stylesheet referred to an extension to the xsl syntax and indicated that it was defined by xsltc @$ but xsltc does not recognize the particular extension <PLACE_HOLDER>d . the substitution text gives the extension <PLACE_HOLDER> .,return new object [ ] [ ] { { basis library . run_time_internal_err @$ __str__ } @$ { basis library . run_time_copy_err @$ __str__ } @$ { basis library . data_conversion_err @$ __str__ } @$ { basis library . external_func_err @$ __str__ } @$ { basis library . equality_expr_err @$ __str__ } @$ { basis library . invalid_argument_err @$ __str__ } @$ { basis library . format_number_err @$ __str__ } @$ { basis library . iterator_clone_err @$ __str__ } @$ { basis library . axis_support_err @$ __str__ } @$ { basis library . typed_axis_support_err @$ __str__ } @$ { basis library . stray_attribute_err @$ __str__ } @$ { basis library . stray_namespace_err @$ __str__ } @$ { basis library . namespace_prefix_err @$ __str__ } @$ { basis library,text gives
this should be a rare case : normally this <PLACE_HOLDER> is already captured . but it does happen if the upper bound of a type variable contains a <PLACE_HOLDER>,if ( type instanceof wildcard type ) { return ( ( wildcard type ) type ) . get upper bounds ( ) ; } else if ( type instanceof capture type ) { return ( ( capture type ) type ) . get upper bounds ( ) ; } else if ( type instanceof generic array type ) { return get array exact direct super types ( type ) ; } else { throw new runtime exception ( __str__ + type ) ; },bound contains
historic rules may contain null entries when original zoneinfo <PLACE_HOLDER> includes non transition <PLACE_HOLDER> .,if ( historic rules != null ) { for ( int i = __num__ ; i < historic rules . length ; i ++ ) { if ( historic rules [ i ] != null ) { size ++ ; } } },data includes
iterate over the instructions @$ adding information for each place that the active variable set <PLACE_HOLDER> .,for ( int i = __num__ ; i < insn sz ; i ++ ) { if ( can throw during last insn && ( i == freeze secondary state at ) ) { primary state . set immutable ( ) ; primary state = primary state . mutable copy ( ) ; } insn insn = insns . get ( i ) ; register spec result ; result = insn . get local assignment ( ) ; if ( result == null ) { result = insn . get result ( ) ; if ( result != null && primary state . get ( result . get reg ( ) ) != null ) { primary state . remove ( primary state . get ( result .,variable set
test validation expects this <PLACE_HOLDER>,factory . set concurrency checks enabled ( false ) ;,validation expects
set alpha will call invalidate <PLACE_HOLDER> and drive the animation .,int partial alpha = ( int ) ( alpha * normalized ) ; super . set alpha ( partial alpha ) ; super . draw ( canvas ) ; super . set alpha ( alpha ) ;,alpha call
the main menu does a <PLACE_HOLDER> to the local crawler page @$ but in case this table is empty @$ the overview page is shown,if ( post != null && post . contains key ( __str__ ) && tabletype == event origin . local_crawling && resultur ls . get stack size ( event origin . local_crawling ) == __num__ ) { tabletype = ( resultur ls . get stack size ( event origin . surrogates ) == __num__ ) ? event origin . unknown : event origin . surrogates ; },menu does
no previous id @$ always accept this <PLACE_HOLDER>,if ( requested session id == null ) { requested session id = id ; session = s ; } else if ( requested session id . equals ( id ) ) { } else if ( session == null || ! is valid ( session ) ) { requested session id = id ; session = s ; } else { if ( s != null && is valid ( s ) ) throw new bad message exception ( __str__ + requested session id + __str__ + id ) ; },id accept
this gnarly block of code will release all <PLACE_HOLDER> and all thread @$ even if any close fails .,try { server socket . close ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ; } for ( iterator < socket > s = open client sockets . key set ( ) . iterator ( ) ; s . has next ( ) ; ) { try { s . next ( ) . close ( ) ; s . remove ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ; } } try { executor . shutdown ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ;,block release
we copy all returned <PLACE_HOLDER> from the get value call in case an optimised model is reusing one object to return many <PLACE_HOLDER> . the number subclasses in the jdk are immutable and so will not be used in this way but other subclasses of number might want to do this to save space and avoid unnecessary heap allocation .,if ( type . get superclass ( ) == java . lang . number . class ) { number n1 = ( number ) data . get value at ( row1 @$ column ) ; double d1 = n1 . double value ( ) ; number n2 = ( number ) data . get value at ( row2 @$ column ) ; double d2 = n2 . double value ( ) ; if ( d1 < d2 ) { return - __num__ ; } else if ( d1 > d2 ) { return __num__ ; } else { return __num__ ; } } else if ( type == java . util . date . class ) { date d1 = ( date ) data . get value at,all returned
async wait <PLACE_HOLDER> .,thread . sleep ( __num__ ) ;,async wait
row set log should contain <PLACE_HOLDER> as well as performance logs,verify fetched log ( row set log @$ expected logs execution ) ; verify fetched log ( row set log @$ expected logs performance ) ; verify missing contents in fetched log ( row set log @$ expected logs verbose ) ;,log contain
check the optimized plan when join should have forward <PLACE_HOLDER> on both sides,sink plan node sink node = o plan . get data sinks ( ) . iterator ( ) . next ( ) ; dual input plan node join node = ( dual input plan node ) sink node . get predecessor ( ) ; ship strategy type join in1 = join node . get input1 ( ) . get ship strategy ( ) ; ship strategy type join in2 = join node . get input2 ( ) . get ship strategy ( ) ; assert . assert equals ( __str__ @$ ship strategy type . forward @$ join in1 ) ; assert . assert equals ( __str__ @$ ship strategy type . forward @$ join in2 ) ;,join have
default root queue allows <PLACE_HOLDER> to have admin acl,if ( is capacity scheduler ) { capacity scheduler configuration csconf = new capacity scheduler configuration ( ) ; csconf . set acl ( __str__ @$ queueacl . administer_queue @$ __str__ ) ; csconf . set acl ( __str__ @$ queueacl . administer_queue @$ __str__ ) ; rm . get resource scheduler ( ) . reinitialize ( csconf @$ rm . getrm context ( ) ) ; },queue allows
check that the meta group changed its <PLACE_HOLDER> .,assert equals ( __str__ @$ new meta group . get group name ( ) @$ renamed group name ) ;,group changed
instantiate the class specified in the code or object section of the m let <PLACE_HOLDER>,object o ; object instance obj inst ; if ( code != null && ser name != null ) { final string msg = __str__ + __str__ ; mlet_logger . logp ( level . finer @$ m let . class . get name ( ) @$ mth @$ msg ) ; mbeans . add ( new error ( msg ) ) ; continue ; } if ( code == null && ser name == null ) { final string msg = __str__ + __str__ ; mlet_logger . logp ( level . finer @$ m let . class . get name ( ) @$ mth @$ msg ) ; mbeans . add ( new error ( msg ) ) ; continue ; } try { if ( code !=,class let
add <PLACE_HOLDER> has thrown an exception ! set future.run ca n't throw any exceptions so this must have been caused by add <PLACE_HOLDER> itself . the most likely explanation is a misconfigured mock . try to switch to failure .,failure failure ; try { failure = new failure ( t ) ; } catch ( throwable oom most likely ) { failure = failure . fallback_instance ; },itself add
the filter ca n't accept more <PLACE_HOLDER> and is effectively broken,overflowed = true ; throw new overflowed error ( ) ;,filter accept
extension registry may be either extension registry or extension registry lite . since the type we are parsing is a full message @$ only a full extension registry could possibly contain <PLACE_HOLDER> of it . otherwise we will treat the registry as if it were empty .,if ( type . is extension number ( field number ) ) { if ( extension registry instanceof extension registry ) { final extension registry . extension info extension = target . find extension by number ( ( extension registry ) extension registry @$ type @$ field number ) ; if ( extension == null ) { field = null ; } else { field = extension . descriptor ; default instance = extension . default instance ; if ( default instance == null && field . get java type ( ) == descriptors . field descriptor . java type . message ) { throw new illegal state exception ( __str__ + field . get full name ( ) ) ; } } } else { field =,registry contain
the class throttles every 5 <PLACE_HOLDER> @$ so the first time it is called is true . the second time is throttled and returns false .,clock . set time ( clock . current time millis ( ) + duration . standard minutes ( __num__ ) . get millis ( ) ) ; assert false ( hot key logger . is throttled ( ) ) ; assert true ( hot key logger . is throttled ( ) ) ;,class throttles
binder.java closes the <PLACE_HOLDER> for us .,@ suppress warnings ( __str__ ) final indenting print writer pw = new indenting print writer ( writer @$ __str__ ) ; if ( ! dump utils . check dump permission ( m context @$ tag @$ pw ) ) return ; pw . println ( __str__ ) ; pw . increase indent ( ) ; pw . println ( __str__ ) ; pw . increase indent ( ) ; final tethering configuration cfg = m config ; cfg . dump ( pw ) ; pw . decrease indent ( ) ; pw . println ( __str__ ) ; pw . increase indent ( ) ; m entitlement mgr . dump ( pw ) ; pw . decrease indent ( ) ; synchronized ( m public sync ),binder.java closes
the script could create an <PLACE_HOLDER> that persists and retains a reference to the bindings,bindings . put ( __str__ @$ null ) ; bindings . put ( __str__ @$ null ) ;,script create
for local servant case just generate the <PLACE_HOLDER> @$ but for others generate the stubs also,if ( ! ( i . is local ( ) ) ) { generate skeleton ( ) ; arguments the arguments = ( arguments ) compile . compiler . arguments ; if ( ( the arguments . tie server == true ) && ( the arguments . emit == the arguments . all ) ) { the arguments . tie server = false ; generate skeleton ( ) ; the arguments . tie server = true ; } generate stub ( ) ; },others generate
the two crawlers should have different storage <PLACE_HOLDER> for their intermediate data .,config1 . set crawl storage folder ( crawl storage folder + __str__ ) ; config2 . set crawl storage folder ( crawl storage folder + __str__ ) ; config1 . set politeness delay ( __num__ ) ; config2 . set politeness delay ( __num__ ) ; config1 . set max pages to fetch ( __num__ ) ; config2 . set max pages to fetch ( __num__ ) ;,crawlers have
best attempt @$ should n't really kill <PLACE_HOLDER> for this,return null ;,best kill
have the x object return its <PLACE_HOLDER> as a node set dtm .,return list . nodeset ( ) ;,object return
the state restore listener will append one <PLACE_HOLDER> to the log,streams one . set global state restore listener ( new updating source topic on restore start state restore listener ( ) ) ; streams one . start ( ) ; produce key values ( __str__ @$ __str__ @$ __str__ ) ; assert number values read ( read key values @$ expected results with data written during restore map @$ __str__ ) ; streams one . close ( duration . of seconds ( __num__ ) ) ;,listener append
this pointer may be in work <PLACE_HOLDER>s because archiver did not copy the <PLACE_HOLDER> yet @$ check that it is not too far forward .,cur wal segm idx = start . index ( ) ;,archiver copy
this node has been inactive @$ but other node has more recent <PLACE_HOLDER> .,if ( ! is inactive ) { updated latest success transfer = latest reported cluster activity ; },node has
set the socket buffer <PLACE_HOLDER> for the underlying socket .,if ( options . sndbuf != __num__ ) { tcp utils . set tcp send buffer ( fd @$ options . sndbuf ) ; } if ( options . rcvbuf != __num__ ) { tcp utils . set tcp receive buffer ( fd @$ options . rcvbuf ) ; },socket buffer
this sets up a channel for each consumer thread . we do n't track <PLACE_HOLDER> @$ as the connection will close its <PLACE_HOLDER> implicitly,try { channel channel = connection . create channel ( ) ; rabbitmq span consumer consumer = new rabbitmq span consumer ( channel @$ collector @$ metrics ) ; channel . basic consume ( builder . queue @$ true @$ consumer tag @$ consumer ) ; } catch ( io exception e ) { throw new illegal state exception ( __str__ + consumer tag @$ e ) ; },connection close
let 's sort <PLACE_HOLDER> !,collections . sort ( nodes ) ;,"s" sort
and location manager which are in our process . the callbacks in these components may require <PLACE_HOLDER> our remote caller does not have .,final long identity = binder . clear calling identity ( ) ; try { final int callback count = callbacks . size ( ) ; for ( int i = __num__ ; i < callback count ; i ++ ) { final active callback callback = callbacks . value at ( i ) ; try { callback . m callback . op active changed ( code @$ uid @$ package name @$ active ) ; } catch ( remote exception e ) { } } } finally { binder . restore calling identity ( identity ) ; },callbacks require
create the authentication response header . it includes 1 required and 2 option <PLACE_HOLDER> length value triples . the required triple has a <PLACE_HOLDER> of 0 x 00 and is the response digest . the first optional <PLACE_HOLDER> is 0 x 01 and represents the user id . if no user id is provided @$ then no user id will be sent . the,if ( user name != null ) { header . m auth resp = new byte [ __num__ + user name . length ] ; header . m auth resp [ __num__ ] = ( byte ) __num__ ; header . m auth resp [ __num__ ] = ( byte ) user name . length ; system . arraycopy ( user name @$ __num__ @$ header . m auth resp @$ __num__ @$ user name . length ) ; } else { header . m auth resp = new byte [ __num__ ] ; },triple has
sort to get a deterministic canonical order . this probably is n't necessary because the options parser will do its own <PLACE_HOLDER> when canonicalizing @$ but it seems like it ca n't hurt .,incompatible changes . sort ( null ) ; return immutable list . copy of ( incompatible changes ) ;,parser do
add 1 element @$ which should use same <PLACE_HOLDER> .,m layout manager . expect layouts ( __num__ ) ; m test adapter . add and notify ( __num__ ) ; m layout manager . wait for layout ( __num__ ) ; m activity rule . run on ui thread ( new runnable ( ) { @ override public void run ( ) { assert true ( __str__ @$ target child [ __num__ ] == m recycler view . get child at ( __num__ ) ) ; assert equals ( expected important for accessibility @$ view compat . get important for accessibility ( target child [ __num__ ] ) ) ; } } ) ;,which use
local class b must also capture the <PLACE_HOLDER> and pass them to a 's constructor .,assert translated lines ( translation @$ __str__ @$ __str__ @$ __str__ ) ;,b capture
we do n't have permission to read boolean <PLACE_HOLDER> s ince the <PLACE_HOLDER> have no boolean <PLACE_HOLDER> we should get an empty set,try { set priv cred set1 = s . get private credentials ( boolean . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { e . print stack trace ( ) ; throw new runtime exception ( __str__ ) ; } system . out . println ( __str__ ) ;,creds have
address 2 will hit the <PLACE_HOLDER>,assert . assert equals ( access privilege . read_write @$ matcher . get access privilege ( address2 @$ hostname2 ) ) ; thread . sleep ( __num__ ) ;,address hit
user defined <PLACE_HOLDER>,if ( source inclusions . length > __num__ ) { return path pattern . create ( source inclusions ) ; },user defined
wrap the inner parts of the loop in a catch throwable so that any errors in the loop do n't doom the entire <PLACE_HOLDER> .,try { handle = txn handler . get mutexapi ( ) . acquire lock ( txn store . mutex_key . initiator . name ( ) ) ; started at = system . current time millis ( ) ; long compaction interval = ( prev start < __num__ ) ? prev start : ( started at - prev start ) / __num__ ; prev start = started at ; show compact response current compactions = txn handler . show compact ( new show compact request ( ) ) ; set < compaction info > potentials = txn handler . find potential compactions ( aborted threshold @$ compaction interval ) . stream ( ) . filter ( ci -> check compaction elig ( ci ) ) . collect ( collectors,errors doom
map works the <PLACE_HOLDER> as collection,if ( param instanceof map ) { for ( map . entry < ? @$ ? > i : ( ( map < ? @$ ? > ) param ) . entry set ( ) ) { deferred parameter key = load object instance ( i . get key ( ) @$ existing @$ i . get key ( ) . get class ( ) ) ; deferred parameter val = i . get value ( ) != null ? load object instance ( i . get value ( ) @$ existing @$ i . get value ( ) . get class ( ) ) : null ; setup steps . add ( new serialzation step ( ) { @ override public void handle ( method context context,map works
let app <PLACE_HOLDER> manager package manage app <PLACE_HOLDER>,dpm . set application restrictions managing package ( admin1 @$ app restrictions manager package ) ; assert equals ( app restrictions manager package @$ dpm . get application restrictions managing package ( admin1 ) ) ;,package manage
verify data object attributes <PLACE_HOLDER>,attributes = get data object attributes ( data obj ) ; assert equals ( __num__ @$ attributes . size ( ) ) ; for ( string key : attributes . key set ( ) ) { if ( key . equals ( __str__ ) ) { assert true ( __str__ . equals ( attributes . get ( key ) ) ) ; } else if ( key . equals ( __str__ ) ) { assert true ( __str__ . equals ( attributes . get ( key ) ) ) ; } else { fail ( __str__ ) ; } },object attributes
now we know the monitor has observed the initial <PLACE_HOLDER> @$ so if we want to test notify behaviour we can trigger by exceeding the threshold .,if ( when == when . in_notify ) { final thread tested thread = new thread ( sensitive thing ) ; final atomic integer notif count = new atomic integer ( ) ; final notification listener listener = new notification listener ( ) { public void handle notification ( notification n @$ object h ) { tested thread . start ( ) ; try { tested thread . join ( ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; } notif count . increment and get ( ) ; } } ; mbs . add notification listener ( monitor name @$ listener @$ null @$ null ) ; observed proxy . set thing ( __num__ ) ; system .,monitor observed
we 've found the start of a rule or id . <PLACE_HOLDER> is its first <PLACE_HOLDER>hara<PLACE_HOLDER>ter @$ and pos points past <PLACE_HOLDER> .,-- pos ;,pos points
force an extra begin <PLACE_HOLDER> in case any guarded <PLACE_HOLDER>s are inputs to the state split,if ( current next instanceof abstract begin node && current next instanceof state split && ( ( state split ) current next ) . state after ( ) != null ) { begin node begin = graph ( ) . add ( new begin node ( ) ) ; begin . set node source position ( get no deopt successor position ( ) ) ; begin . set next ( current next ) ; current next = begin ; },extra begin
address pointers will only be created if extent of table contains only undefined <PLACE_HOLDER> or pointers,builder . create memory ( __str__ @$ __str__ @$ __num__ ) ;,extent contains
no primary key @$ but contains only public key <PLACE_HOLDER> .,keyset valid keyset = keyset . new builder ( ) . add key ( keyset . key . new builder ( ) . set key data ( test util . create key data ( key data . new builder ( ) . build ( ) @$ __str__ @$ key data . key material type . asymmetric_public ) ) . set key id ( __num__ ) . set status ( key status type . enabled ) . set output prefix type ( output prefix type . tink ) . build ( ) ) . build ( ) ; try { util . validate keyset ( valid keyset ) ; } catch ( general security exception e ) { fail ( __str__ + e ) ; },key contains
set the table write <PLACE_HOLDER> in all of the ac<PLACE_HOLDER> file sinks,if ( ! driver context . get plan ( ) . get acid sinks ( ) . is empty ( ) ) { list < file sink desc > acid sinks = new array list < > ( driver context . get plan ( ) . get acid sinks ( ) ) ; acid sinks . sort ( ( file sink desc fsd1 @$ file sink desc fsd2 ) -> fsd1 . get dir name ( ) . compare to ( fsd2 . get dir name ( ) ) ) ; for ( file sink desc desc : acid sinks ) { table desc table info = desc . get table info ( ) ; final table name tn = hive table name . of nullable ( table,table write
check that user status listener is working <PLACE_HOLDER>,assert equals ( __str__ @$ __str__ @$ answer [ __num__ ] ) ;,listener working
mkdirs command does n't propagate user <PLACE_HOLDER> .,props = properties ( null @$ null @$ __str__ ) ;,command propagate
test that an invalid offset length causes an error <PLACE_HOLDER> and clears the view text,set offset field value ( provider @$ __num__ ) ; preview text = get preview text ( provider ) ; assert equals ( __str__ @$ preview text ) ;,length causes
now let 's build an identical <PLACE_HOLDER> for get,binary object builder bldr = grid ( __num__ ) . binary ( ) . builder ( __str__ ) ; bldr . set field ( __str__ @$ __num__ ) ; bldr . set field ( __str__ @$ __str__ ) ; binary object bin key = bldr . build ( ) ; assert equals ( __str__ @$ c . get ( bin key ) ) ;,"s" build
if the value tag does not specify a <PLACE_HOLDER> which is a valid field and it is not used within the comments of a valid field @$ return null .,return null ;,tag specify
force refresh since file id should get <PLACE_HOLDER>,repository item = null ;,id get
print found <PLACE_HOLDER> only for those that apply correctly or were already applied,if ( found data ) { println ( __str__ + data name + __str__ + current program . get executable path ( ) + __str__ + found list . get ( i ) . to string ( ) ) ; num data found ++ ; },print found
the peer may have closed the <PLACE_HOLDER> because of the unmatched server name indication .,ssl socket . close ( ) ; ssl server socket . close ( ) ;,peer closed
tell the user which methods force this <PLACE_HOLDER> to be abstract .,modifiers |= m_abstract ;,methods force
since it is possible that the callsite uses some <PLACE_HOLDER> as well @$ we may have to add additional elements here,add missing entries ( connections @$ resolved method generics ) ;,callsite uses
management server encounters an <PLACE_HOLDER> and closes the stream .,response observer . on error ( status . unknown . as exception ( ) ) ;,server encounters
this call to load <PLACE_HOLDER> may eventually call find <PLACE_HOLDER> again @$ in case the parent does n't find anything .,class < ? > c ; try { c = super . load class ( name @$ resolve ) ; } catch ( class not found exception e ) { c = load class from system ( name ) ; } catch ( security exception e ) { c = load class from system ( name ) ; } return c ;,parent find
validate selected controller services implement the <PLACE_HOLDER> required by the processor,final collection < validation result > referenced service validation results = validate referenced controller services ( validation context ) ; validation results . add all ( referenced service validation results ) ; logger . debug ( __str__ @$ validation context @$ validation results ) ; return validation results ;,services implement
rebuilding will use the new <PLACE_HOLDER>,msg = builder . build ( ) ; assert that ( msg . repeated_double ) . is equal to ( doubles ) ;,rebuilding use
this is a direct call to a method that the static analysis did not see as invoked . this can happen when the receiver is always null . in most cases @$ the method profile also has a <PLACE_HOLDER> of 0 and the below code to kill the invoke would trigger . but not all methods have profiles @$ for example methods with manually,if ( call target . invoke kind ( ) . is direct ( ) && ! ( ( hosted method ) call target . target method ( ) ) . get wrapped ( ) . is simply implementation invoked ( ) ) { unreachable invoke ( graph @$ invoke @$ call target ) ; continue ; },profile has
puff @$ everything went <PLACE_HOLDER>,return return value ;,everything went
when drag is enabled mouse drags wo n't change the <PLACE_HOLDER> in the list @$ so we only set the is adjusting flag when it 's not enabled,list . set value is adjusting ( true ) ;,drags change
if the user has not used the default group <PLACE_HOLDER> @$ add it,if ( ! group list . contains ( default group name ) ) { group list . add ( default group name ) ; } for ( int i = __num__ ; i < packages . length ; i ++ ) { package doc pkg = packages [ i ] ; string pkg name = pkg . name ( ) ; string group name = pkg name group map . get ( pkg name ) ; if ( group name == null ) { group name = reg exp group name ( pkg name ) ; } if ( group name == null ) { group name = default group name ; } get pkg list ( group package map @$ group name ) . add ( pkg,user used
transition nn <PLACE_HOLDER> to active even though nn 1 still thinks it 's active,banner ( __str__ ) ; name node adapter . abort edit logs ( nn1 ) ; name node adapter . enter safe mode ( nn1 @$ false ) ; cluster . transition to active ( __num__ ) ;,transition nn
remove all modifies the <PLACE_HOLDER> that is being called on . so create a copy,collection < string > nodes diff = new array list < string > ( all node names . size ( ) ) ; nodes diff . add all ( all node names ) ; nodes diff . remove all ( nodes in result ) ; if ( nodes diff . size ( ) > __num__ ) { check result = false ; for ( string node name : nodes diff ) { system . err . println ( key name + __str__ + node name ) ; } } return check result ;,all modifies
check if the instruction is generalizing a simple <PLACE_HOLDER> to a different type .,invocation offset = offset ; clazz . constant pool entry accept ( constant instruction . constant index @$ parameter checker ) ; break ;,instruction generalizing
shuffling spreads the <PLACE_HOLDER> out across physical hosts better,collections . shuffle ( warehouse ids ) ; m_available warehouse ids . add all ( warehouse ids ) ; boolean do make replicated = true ; for ( load thread load thread : m_load threads ) { load thread . start ( do make replicated ) ; do make replicated = false ; } for ( int ii = __num__ ; ii < m_load threads . length ; ii ++ ) { try { m_load threads [ ii ] . join ( ) ; } catch ( interrupted exception e ) { e . print stack trace ( ) ; system . exit ( - __num__ ) ; } } try { m_volt client . drain ( ) ; } catch ( interrupted exception e ) { return,shuffling spreads
listener added later so unneeded <PLACE_HOLDER> not thrown,text menu . add action listener ( this ) ;,listener added
if we need to hold the x<PLACE_HOLDER> @$ then we need to make sure that no one holds any <PLACE_HOLDER> @$ including the shared <PLACE_HOLDER> @$ otherwise @$ we just need to make sure that no one holds the x<PLACE_HOLDER>,return xlock req ? ! s . is locked ( ) : ! s . has exclusive lock ( ) ;,one holds
'is file name in field ' will always return <PLACE_HOLDER> ',concat fields meta = new concat fields meta ( ) ; concat fields meta . set file name in field ( true ) ; assert false ( concat fields meta . is file name in field ( ) ) ; concat fields meta . set file name in field ( false ) ; assert false ( concat fields meta . is file name in field ( ) ) ;,name return
initialize <PLACE_HOLDER>s if eos is turned on @$ which will block if the previous <PLACE_HOLDER> has not completed yet ; do not start the first <PLACE_HOLDER> until the topology has been initialized later,if ( eos enabled ) { initialize transactions ( ) ; },transactions start
process variables should have no <PLACE_HOLDER>,task service . set variable ( current task . get id ( ) @$ __str__ @$ __str__ ) ; current task = task service . create task query ( ) . single result ( ) ; assert equals ( __num__ @$ ( ( counting task entity ) current task ) . get variable count ( ) ) ; map < string @$ object > local vars = new hash map < > ( ) ; local vars . put ( __str__ @$ __str__ ) ; local vars . put ( __str__ @$ __str__ ) ; local vars . put ( __str__ @$ __str__ ) ; task service . set variables local ( current task . get id ( ) @$ local vars ) ; current task = task,variables have
custom proguard <PLACE_HOLDER>,pro guard obfuscate step . create ( test android platform target factory . create ( ) @$ java compilation constants . default_java_command_prefix @$ new fake project filesystem ( ) @$ optional . empty ( ) @$ __str__ @$ optional . empty ( ) @$ immutable set . of ( proguard config . get filesystem ( ) . resolve ( proguard config . get relative path ( ) ) @$ proguard config . get filesystem ( ) . resolve ( aapt proguard dir . resolve ( __str__ ) ) ) @$ pro guard obfuscate step . sdk proguard type . none @$ pro guard obfuscate step . default_optimization_passes @$ optional . empty ( ) @$ immutable map . of ( build target paths . get gen path ( library,custom proguard
does this node support <PLACE_HOLDER> at all ?,if ( ! supported command classes . contains key ( command class . security ) ) { result = false ; } else { final int command class code = ( byte ) serial message . get message payload byte ( __num__ ) & __num__ ; final command class command class of message = command class . get command class ( command class code ) ; if ( command class of message == null ) { logger . warn ( string . format ( __str__ @$ get node id ( ) @$ command class code @$ serial message ) ) ; result = false ; } else if ( command class . security == command class of message ) { final byte message code = byte . value,node support
this command also supports retrieving <PLACE_HOLDER> for an importing job .,string importing jobid = request . get parameter ( __str__ ) ; if ( importing jobid != null ) { long jobid = long . parse long ( importing jobid ) ; importing job job = importing manager . get job ( jobid ) ; if ( job != null ) { project = job . project ; } } if ( project == null ) { project = get project ( request ) ; } response . set header ( __str__ @$ __str__ ) ; map < string @$ language info > prefixes map = new hash map < > ( ) ; for ( string language prefix : meta parser . get language prefixes ( ) ) { language info info = meta parser . get,command supports
a well behaved runner would always check the <PLACE_HOLDER> .,for ( int i = __num__ ; i < __num__ ; i ++ ) { c . append ( event builder . with body ( ( __str__ + string . value of ( i ) ) . get bytes ( ) ) ) ; },runner check
when prepare deps of patterns function completes <PLACE_HOLDER> @$,evaluation context evaluation context = evaluation context . new builder ( ) . set keep going ( keep going ) . set num threads ( loading_phase_threads ) . set event hander ( new reporter ( new event bus ( ) @$ event collector ) ) . build ( ) ; evaluation result < sky value > evaluation result = get skyframe executor ( ) . get driver ( ) . evaluate ( singleton target pattern @$ evaluation context ) ;,function completes
second @$ we must ensure that this method must be statically checked for example @$ in a mixed mode where only some methods are statically checked we must not visit a method which used dynamic dispatch . we do not check for an annotation because some other ast transformations may use this <PLACE_HOLDER> without the annotation being explicitely set,if ( ! type checking context . methods to be visited . is empty ( ) && ! type checking context . methods to be visited . contains ( node ) ) return ;,transformations use
since family has a join table @$ we will first left join all family <PLACE_HOLDER> and then do the with clause on the target entity table join normally this produces 2 results which is wrong and can only be circumvented by converting the join table and target entity table join to a subquery,list list = s . create query ( __str__ ) . list ( ) ; assert equals ( __str__ @$ __num__ @$ list . size ( ) ) ; txn . commit ( ) ; s . close ( ) ; data . cleanup ( ) ;,table join
now that all types have depth <PLACE_HOLDER> @$ the result can be sorted,arrays . sort ( sortable types @$ sortable type . nulls_last_order ) ;,types have
output reduce sleep count num reduce <PLACE_HOLDER> of random values @$ so that each reducer will get reduce sleep count <PLACE_HOLDER> of keys .,int k = key . get ( ) ; for ( int i = __num__ ; i < value . get ( ) ; ++ i ) { context . write ( new int writable ( k + i ) @$ null writable . get ( ) ) ; },output reduce
previous status must be load now as measure matrix mutate the live measure <PLACE_HOLDER> which are passed to it,metric . level previous status = load previous status ( metrics @$ db measures ) ; configuration config = project configuration loader . load project configuration ( db session @$ project ) ; debt rating grid debt rating grid = new debt rating grid ( config ) ; measure matrix matrix = new measure matrix ( components @$ metrics per id . values ( ) @$ db measures ) ; formula context impl context = new formula context impl ( matrix @$ debt rating grid ) ; long beginning of leak = get beginning of leak period ( last analysis @$ branch ) ; components . for each ( c -> { issue counter issue counter = new issue counter ( db client . issue dao ( ),matrix mutate
app ops manager can return <PLACE_HOLDER> when there is no requested data .,if ( packages != null ) { final int num packages = packages . size ( ) ; for ( int package ind = __num__ ; package ind < num packages ; package ind ++ ) { app ops manager . package ops package op = packages . get ( package ind ) ; list < app ops manager . op entry > op entries = package op . get ops ( ) ; if ( op entries != null ) { final int num ops = op entries . size ( ) ; for ( int op ind = __num__ ; op ind < num ops ; op ind ++ ) { app ops manager . op entry op entry = op entries . get ( op,manager return
for now @$ we can use the deprecated get credentials list <PLACE_HOLDER> .,return client . get credentials list ( ) ;,deprecated get
footer has an invisible first <PLACE_HOLDER>,c10 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c10 . get text ( ) ) ; assert equals ( __str__ @$ __str__ @$ c10 . get attribute ( __str__ ) ) ; grid cell element c11 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c11 . get text ( ) ) ; grid cell element c20 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c20 . get text ( ) ) ; grid cell element c21 = get grid element ( ) . get footer cell ( __num__,footer has
the procedure runner nt generator has all of the dangerous and slow stuff in it . like <PLACE_HOLDER> @$ instantiation @$ and reflection .,runner generator map . put ( procedure . get type name ( ) @$ new procedure runnernt generator ( clz ) ) ;,runner like
test a call where one partition has an <PLACE_HOLDER> .,api error value = api error . from throwable ( new cluster authorization exception ( null ) ) ; list < replica election result > election results = new array list < > ( ) ; replica election result election result = new replica election result ( ) ; election result . set topic ( topic1 . topic ( ) ) ;,partition has
a field filter @$ hence skip the mappings filtering <PLACE_HOLDER> as a whole @$ as it requires parsing mappings into a map .,if ( first == mapper plugin . noop_field_filter ) { return second ; } if ( second == mapper plugin . noop_field_filter ) { return first ; } return index -> { predicate < string > first predicate = first . apply ( index ) ; predicate < string > second predicate = second . apply ( index ) ; if ( first predicate == mapper plugin . noop_field_predicate ) { return second predicate ; } if ( second predicate == mapper plugin . noop_field_predicate ) { return first predicate ; } return first predicate . and ( second predicate ) ; } ;,filter skip
returning an empty array essentially means that the element does n't declare any <PLACE_HOLDER> @$ but we know that it is not true since the reason the annotation parsing failed is because some annotation referenced a missing class . however @$ this allows us to defend against crashing the image builder if the above jdk bug is encountered in user code or if the,return new annotation [ __num__ ] ;,element declare
before sending client ready @$ lets make sure the stats already reflect 0 queued <PLACE_HOLDER>,check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ; check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ; check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ;,stats reflect
our host did not provide a custom flutter <PLACE_HOLDER> . create a flutter <PLACE_HOLDER> to back our flutter view .,log . d ( tag @$ __str__ + __str__ ) ; is flutter engine from host = false ;,host provide
if any configs in previous dipped into outer context @$ that means that input up to t actually finished entry rule at least for sll decision . full ll does n't dip into outer so do n't need special case . we will get an <PLACE_HOLDER> no matter what so delay until after decision ; better <PLACE_HOLDER> message . also @$ no reachable target,if ( d == error ) { no viable alt exception e = no viable alt ( input @$ outer context @$ previousd . configs @$ start index ) ; input . seek ( start index ) ; int alt = get syn valid or sem invalid alt that finished decision entry rule ( previousd . configs @$ outer context ) ; if ( alt != atn . invalid_alt_number ) { return alt ; } throw e ; },ll get
do not overwrite interface <PLACE_HOLDER> with instance <PLACE_HOLDER> do not overwrite private <PLACE_HOLDER> note : private <PLACE_HOLDER> from parent classes are not shown here @$ but when doing the multimethod connection step @$ we overwrite <PLACE_HOLDER> of the parent class with <PLACE_HOLDER> of a subclass and in that case we want to keep the private <PLACE_HOLDER>,if ( match . is private ( ) || ( ! is non real method ( match ) && match . get declaring class ( ) . is interface ( ) && ! method . get declaring class ( ) . is interface ( ) && ! method . is static ( ) ) ) { } else { cached class methodc = method . get declaring class ( ) ; cached class matchc = match . get declaring class ( ) ; if ( methodc == matchc ) { if ( is non real method ( method ) ) { list . set ( found @$ method ) ; } } else if ( ! methodc . is assignable from ( matchc . get the class (,methods overwrite
since the bulk create <PLACE_HOLDER> out on a single failure @$ this has to be done as a workaround,batch create types ( types def ) ;,bulk create
note : parse offset <PLACE_HOLDER> will validate the given <PLACE_HOLDER> and throws illegal argument exception when <PLACE_HOLDER> is not valid,object [ ] parsed items = parse offset pattern ( gmt offset patterns [ idx ] @$ t . required ( ) ) ; gmt offset pattern items [ idx ] = parsed items ;,pattern validate
normalization scale signed <PLACE_HOLDER>,assert divide all signs ( new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } @$ new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) ;,scale signed
bean may have acquired new weak <PLACE_HOLDER>,target = result . get node ( ) ; assert . assert equals ( count ++ @$ result . get value ( ) . int value ( ) ) ;,bean acquired
second attempt should have the same <PLACE_HOLDER> as exponential backoff is disabled,assert equals ( reconnect backoff ms test @$ delay ) ;,attempt have
we encounter a vm error . this includes for example out of memory exceptions . in such a case we silently swallow the error . if it happens again the application thread will most likely encounter the same <PLACE_HOLDER> . if not the watchdog thread will no longer monitor the compilation and thus the error can not happen again .,thread . sleep ( spin_timeout_ms ) ;,thread encounter
if we 're a relaunch we may need to adjust button enable <PLACE_HOLDER>,if ( icicle != null ) { m did acknowledge = icicle . get boolean ( key_did_acknowledge @$ false ) ; m allow button . set enabled ( ! m did acknowledge ) ; m deny button . set enabled ( ! m did acknowledge ) ; },button enable
the paths of the headers written out to the depfile are the paths to the symlinks from the root of the repo if the compilation included <PLACE_HOLDER> from the header search paths pointing to the symlink trees @$ or paths to headers relative to the source file if the compilation included <PLACE_HOLDER> using source relative include paths . to handle both cases we check,logger . get ( depfiles . class ) . debug ( __str__ @$ source dep file ) ; try ( simple perf event . scope perf event = simple perf event . scope ( event bus @$ perf event id . of ( __str__ ) @$ immutable map . of ( __str__ @$ input path @$ __str__ @$ output path ) ) ) { list < string > headers = get raw used headers from depfile ( filesystem @$ source dep file @$ input path @$ dependency tracking mode @$ use unix path separator ) ; return normalize and verify headers ( event bus @$ filesystem @$ path resolver @$ header path normalizer @$ header verification @$ input path @$ headers @$ source dep file @$ dependency tracking,compilation included
and will not trigger fallback <PLACE_HOLDER>,verify ( failover service @$ at least once ( ) ) . activate ( ) ;,and trigger
self gateway should adapt its fencing <PLACE_HOLDER>,assert equals ( new fencing token @$ fenced gateway . get fencing token ( ) ) ; assert equals ( new fencing token @$ fenced testing endpoint . get fencing token ( ) ) ; rpc utils . terminate rpc endpoint ( fenced testing endpoint @$ timeout ) ;,gateway adapt
<PLACE_HOLDER> set change when new set includes current <PLACE_HOLDER>,assert . assert equals ( bs puller . get current server idx ( ) != - __num__ @$ true @$ __str__ ) ; assert . assert equals ( bs puller . get curent server ( ) != null @$ true @$ __str__ ) ; server info old server = bs puller . get curent server ( ) ; assert . assert equals ( bs puller . get servers ( ) @$ exp_serverinfo_1 @$ __str__ ) ; map < long @$ list < register response entry > > entries = new hash map < long @$ list < register response entry > > ( ) ; entries . put ( __num__ @$ new array list < register response entry > ( ) ) ; conn state . set sources,set includes
sort operator create a new <PLACE_HOLDER> for the moment,return true ;,operator create
first result set has results @$ second has <PLACE_HOLDER>,when ( result set . next ( ) ) . then return ( true ) . then return ( false ) ; when ( result set . get string ( constants . col_trigger_type ) ) . then return ( constants . ttype_blob ) ; operable trigger trigger = jdbc delegate . select trigger ( conn @$ trigger key . trigger key ( __str__ ) ) ; assert null ( trigger ) ;,set has
if this thread is synchronized on rmi server impl @$ then the thread that does connector server.stop will acquire the client list <PLACE_HOLDER> and then b<PLACE_HOLDER> waiting for the rmi server impl <PLACE_HOLDER> . our call to super.client closed will then dead<PLACE_HOLDER> because it needs to acquire the client list <PLACE_HOLDER> .,system . out . println ( __str__ ) ; system . out . flush ( ) ; super . client closed ( conn ) ;,server.stop acquire
client eventually sees broken <PLACE_HOLDER>,assert throws ( io exception . class @$ ( ) -> { int i = __num__ ; for ( i = __num__ ; i < __num__ ; i ++ ) { client . get output stream ( ) . write ( __num__ ) ; } } ) ;,client sees
second gbk forces the <PLACE_HOLDER> to be materialized,p assert . that ( result ) . contains in any order ( kv . of ( __str__ @$ immutable list . of ( __num__ @$ __num__ @$ __num__ ) ) @$ kv . of ( __str__ @$ immutable list . of ( __num__ ) ) ) ;,gbk forces
ca of the certificate @$ for which this <PLACE_HOLDER> is checked @$ has also signed <PLACE_HOLDER> @$ so skip the path validation @$ because is already done,if ( signing cert . equals ( defaultcrl sign cert ) ) { valid certs . add ( signing cert ) ; valid keys . add ( defaultcrl sign key ) ; continue ; } try { cert path builder builder = cert path builder . get instance ( __str__ @$ bouncy castle provider . provider_name ) ; selector = new x509 cert store selector ( ) ; selector . set certificate ( signing cert ) ; extendedpkix parameters temp = ( extendedpkix parameters ) paramspkix . clone ( ) ; temp . set target cert constraints ( selector ) ; extendedpkix builder parameters params = ( extendedpkix builder parameters ) extendedpkix builder parameters . get instance ( temp ) ; if ( cert path certs . contains,certificate signed
verify the puts reach <PLACE_HOLDER> 2,vm2 . invoke ( ( ) -> wan test base . validate region size ( get test method name ( ) + __str__ @$ __num__ ) ) ;,puts reach
see if the input row contains the filename <PLACE_HOLDER> ...,int idx = get input row meta ( ) . index of value ( meta . get accepting field ( ) ) ; if ( idx < __num__ ) { throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ meta . get accepting field ( ) ) ) ; },row contains
make sure the type matches <PLACE_HOLDER> the return statements are returning .,type type = val needed ? this . type : type . t void ; expression e = new inline method expression ( where @$ type @$ field @$ new compound statement ( where @$ body ) ) ; return val needed ? e . inline value ( env @$ ctx ) : e . inline ( env @$ ctx ) ;,statements returning
dw streams specifies the <PLACE_HOLDER> of streams in the file . for example @$ a file with audio and video has two streams .,d . write int ( ( int ) largest buffer size ) ;,streams specifies
set the screen 's skin using the previously generated ui skin <PLACE_HOLDER> .,widget . set skin ( assets . generate asset ( data @$ ui skin . class ) ) ; selected screen box . set content ( widget ) ;,skin using
confirm that we report at least as much throttle time as our server sent <PLACE_HOLDER> for . we will actually report more due to backoff in restarting streams .,assert true ( this . client . get and reset throttle time ( ) > throttle time ) ; stream . close ( ) ; assert true ( stream . await termination ( __num__ @$ time unit . seconds ) ) ;,time sent
number of threads to use during crawling . increasing this typically makes crawling faster . but crawling speed depends on many other factors as well . you can experiment with this to figure out what number of threads works <PLACE_HOLDER> for you .,int number of crawlers = __num__ ;,number works
all other errors indicate a bigger <PLACE_HOLDER> @$ so just stop the task .,if ( thread . interrupted ( ) ) { return true ; } else if ( ! response . is valid ( ) ) { return response . get error ( ) . get error code ( ) != error code . io_exception ; } else { return false ; },errors indicate
system apps have <PLACE_HOLDER> over where their default storage context is pointed @$ so we 're always explicit when building paths .,final context ce context = context . create credential protected storage context ( ) ; files_dir = ce context . get files dir ( ) ; database_dir = ce context . get database path ( __str__ ) . get parent file ( ) ; root_dir = ce context . get data dir ( ) ; sharedpref_dir = ce context . get shared preferences path ( __str__ ) . get parent file ( ) ; cache_dir = ce context . get cache dir ( ) ; nobackup_dir = ce context . get no backup files dir ( ) ; final context de context = context . create device protected storage context ( ) ; device_files_dir = de context . get files dir ( ) ; device_database_dir = de context,apps have
replace load method counters node with resolve method and load counters node @$ expose klass <PLACE_HOLDER> .,replace load method counters ( graph @$ state mapper @$ context ) ;,counters node
remove existent <PLACE_HOLDER> should remove ownership <PLACE_HOLDER>,cache . remove owner from stream ( stream @$ owner @$ __str__ ) ; assert null ( __str__ + owner + __str__ @$ cache . get owner ( stream ) ) ; ownership map = cache . get stream owner mapping ( ) ; assert equals ( __str__ + ( num proxies * num streams per proxy - __num__ ) + __str__ @$ num proxies * num streams per proxy - __num__ @$ ownership map . size ( ) ) ; ownership distribution = cache . get stream ownership distribution ( ) ; assert equals ( __str__ + num proxies + __str__ @$ num proxies @$ ownership distribution . size ( ) ) ; set < string > owned streams = ownership distribution . get ( owner ),mapping remove
get the tenant root <PLACE_HOLDER> . if the tenant root <PLACE_HOLDER> does not exist then exit .,tenant root folder = repository file dao . get file by absolute path ( server repository paths . get tenant root folder path ( the tenant ) ) ; if ( tenant root folder != null ) { tenant home folder = repository file dao . get file by absolute path ( server repository paths . get tenant home folder path ( the tenant ) ) ; if ( tenant home folder == null ) { string owner id = user name utils . get principle id ( the tenant @$ username ) ; repository file sid owner sid = new repository file sid ( owner id @$ type . user ) ; string tenant authenticated role id = role name utils . get principle id ( the,tenant root
send key action does n't need any incoming cec <PLACE_HOLDER> @$ hence does not consume it .,return false ;,action need
actual overlap @$ just skip the <PLACE_HOLDER> .,if ( last event . event equals ( cur event ) ) { if ( debug ) slog . d ( tag @$ __str__ + last nanos ) ; } else { assign log id ( cur event ) ; m pending logs . add ( cur event ) ; if ( debug ) slog . d ( tag @$ __str__ + last nanos ) ; },overlap skip
and finally @$ find the variation @$ caused by the fact that the sun 's gravitational pull on the moon varies depending on which <PLACE_HOLDER> of the earth the moon is on,double variation = __num__ * pi / __num__ * math . sin ( __num__ * ( moon longitude - sun long ) ) ; moon longitude += variation ;,moon on
decorate outgoing exceptions with basic tree <PLACE_HOLDER> . this is similar to how the constructor appends its <PLACE_HOLDER> @$ but the constructor has read more <PLACE_HOLDER> at that point so this one is a bit more sparse on <PLACE_HOLDER> .,with message ( t @$ t . get message ( ) + __str__ + format ( __str__ @$ index file ) ) ; throw t ;,constructor read
0 means <PLACE_HOLDER> already set and we should not block any threads .,return get state ( ) == __num__ ? __num__ : - __num__ ;,0 means
then draw the arc . finally @$ as the optimizer has n't stroke <PLACE_HOLDER> the path @$ we close and fill it @$ and mark the stroke as closed . note : the lineto to the centre of the object is required @$ because the fill only fills the arc . skipping this includes an extra chord @$ which is n't correct . peter,close block ( ) ; draw arc ( x @$ y @$ w @$ h @$ sa @$ aa ) ; lineto ( x + ( w > > __num__ ) @$ y + ( h > > __num__ ) ) ; close block ( __str__ ) ;,optimizer has
aapt should issue an <PLACE_HOLDER> @$ but do a bit of sanity checking here just in case .,if ( old value != null && ! old value . equals ( value ) ) { integer lower = old value . or null ( ) ; integer higher = value . or null ( ) ; if ( ordering . natural ( ) . compare ( old value . or null ( ) @$ value . or null ( ) ) > __num__ ) { lower = higher ; higher = old value . or null ( ) ; } logger . warning ( string . format ( __str__ @$ type @$ name @$ lower @$ higher ) ) ; },aapt issue
if the receiver is not included in the contract @$ unfreeze frozen balance for this <PLACE_HOLDER> . otherwise @$ unfreeze delegated frozen balance provided this <PLACE_HOLDER> .,if ( ! array utils . is empty ( receiver address ) && dynamic store . supportdr ( ) ) { if ( arrays . equals ( receiver address @$ owner address ) ) { throw new contract validate exception ( __str__ ) ; } if ( ! decode util . address valid ( receiver address ) ) { throw new contract validate exception ( __str__ ) ; } account capsule receiver capsule = account store . get ( receiver address ) ; if ( dynamic store . get allow tvm constantinople ( ) == __num__ && receiver capsule == null ) { string readable receiver address = string util . create readable string ( receiver address ) ; throw new contract validate exception ( __str__ + readable,balance provided
check if two vh have the same <PLACE_HOLDER> @$ if so @$ print that as an error,final int child count = m child helper . get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { view view = m child helper . get child at ( i ) ; view holder other = get child view holder int ( view ) ; if ( other == holder ) { continue ; } final long other key = get changed holder key ( other ) ; if ( other key == key ) { if ( m adapter != null && m adapter . has stable ids ( ) ) { throw new illegal state exception ( __str__ + __str__ + __str__ + other + __str__ + holder ) ; } else {,vh have
the set of <PLACE_HOLDER> with unknown leader contains <PLACE_HOLDER> with leader election pending as well as <PLACE_HOLDER> which may have expired . add the topic again to metadata to ensure it is included and request metadata update @$ since there are messages to send to the topic .,if ( ! result . unknown leader topics . is empty ( ) ) { for ( string topic : result . unknown leader topics ) this . metadata . add ( topic @$ now ) ; log . debug ( __str__ @$ result . unknown leader topics ) ; this . metadata . request update ( ) ; },set contains
language agnostic inlining expects this particular <PLACE_HOLDER> to be present in the graph,assert . assert equals ( __num__ @$ graph . get nodes ( inline decision node . type ) . count ( ) ) ; for ( inline decision node decision node : graph . get nodes ( inline decision node . type ) ) { assert . assert equals ( __num__ @$ decision node . usages ( ) . count ( ) ) ; for ( node usage : decision node . usages ( ) ) { if ( usage instanceof integer equals node ) { final node if node = usage . usages ( ) . first ( ) ; assert . assert true ( if node instanceof if node ) ; final fixed node invoke = ( ( if node ) if node ) . false,inlining expects
we want to clip off the last predicate @$ but only if we are a sub context node list @$ not if we are a context list . see <PLACE_HOLDER> 68 test @$ also test against bug 4638 .,if ( pred count > __num__ && is predicate test ) { clone . m_pred count = m_predicate index ; },test see
get all records inclusive the <PLACE_HOLDER> before the startdate,for ( long f : files ) { if ( start long <= f && f <= end long ) { result as long . add ( f ) ; } },records inclusive
brand new <PLACE_HOLDER> @$ not refresh @$ directly add the <PLACE_HOLDER>,zn record segment metadata zn record = _pinot helix resource manager . get segment metadata zn record ( offline table name @$ segment name ) ; if ( segment metadata zn record == null ) { logger . info ( __str__ @$ segment name @$ raw table name ) ; string crypter = headers . get header string ( file upload download client . custom headers . crypter ) ; process new segment ( segment metadata @$ final segment locationuri @$ current segment location @$ zk downloaduri @$ crypter @$ raw table name @$ segment name @$ move segment to final location ) ; return ; } logger . info ( __str__ @$ segment name @$ raw table name ) ; process existing segment ( segment metadata @$,segment add
let 's check partition meta <PLACE_HOLDER> .,if ( rec instanceof page snapshot ) { page snapshot snp rec = ( page snapshot ) rec ; assert false ( snapshots . contains key ( snp rec . full page id ( ) ) ) ; snapshots . put ( snp rec . full page id ( ) @$ snp rec ) ; } else if ( rec instanceof meta page update partition data record ) { meta page update partition data record meta rec = ( meta page update partition data record ) rec ; assert true ( snapshots . contains key ( meta rec . full page id ( ) ) ) ; },check partition
only used if framework does not support the object <PLACE_HOLDER>,callbacks [ __num__ ] = new password callback ( __str__ @$ false ) ;,framework support
a spawn exec a blocks b if a produces an <PLACE_HOLDER> consumed by b,multimap < spawn exec @$ spawn exec > blocked by = multimap builder . hash keys ( ) . array list values ( ) . build ( ) ; multimap < spawn exec @$ spawn exec > blocking = multimap builder . hash keys ( ) . array list values ( ) . build ( ) ; for ( spawn exec ex : inputs ) { for ( file s : ex . get inputs list ( ) ) { if ( output producer . contains key ( s . get path ( ) ) ) { spawn exec blocker = output producer . get ( s . get path ( ) ) ; blocked by . put ( ex @$ blocker ) ; blocking . put (,a produces
produce a message to broker 1 's test queue and verify that broker 1 's memory <PLACE_HOLDER> has increased @$ but broker 2 still has no memory <PLACE_HOLDER> .,send messages ( __str__ @$ test queue @$ __num__ ) ; assert true ( broker1 test queue . get memory usage ( ) . get usage ( ) > __num__ ) ; assert equals ( __num__ @$ broker2 test queue . get memory usage ( ) . get usage ( ) ) ;,broker has
first item gets <PLACE_HOLDER> of its new padding pushed out of sight . the last item will get this implicitly from layout .,if ( i == __num__ && ! lp . prevent edge offset ) { lp . left margin = - extra pixels / __num__ ; },item gets
placeholders get the last view type <PLACE_HOLDER>,if ( position < num headers and placeholders && ( position % m num columns != __num__ ) ) { return m adapter != null ? m adapter . get view type count ( ) : __num__ ; },placeholders get
animate each child moving and changing <PLACE_HOLDER> to match their final locations .,array list < animator > animators = new array list < animator > ( ) ; value animator child animator = value animator . of float ( __num__ @$ __num__ ) ; animators . add ( child animator ) ; for ( int i = __num__ ; i < m layout . get child count ( ) ; i ++ ) { if ( m initial tops . get ( i ) . compare to ( final child tops . get ( i ) ) == __num__ && m initial tops . get ( i + __num__ ) . compare to ( final child tops . get ( i + __num__ ) ) == __num__ ) { continue ; } final view child = m layout . get,child moving
if we need to hold the x<PLACE_HOLDER> @$ then we need to make sure that no one holds any <PLACE_HOLDER> @$ including the shared <PLACE_HOLDER> @$ otherwise @$ we just need to make sure that no one holds the x<PLACE_HOLDER>,return xlock req ? ! s . is locked ( ) : ! s . has exclusive lock ( ) ;,one holds
reading saved <PLACE_HOLDER> from ignite .,list < list < ? > > data = cache . query ( new sql fields query ( __str__ ) ) . get all ( ) ; system . out . println ( data ) ;,reading saved
<PLACE_HOLDER>scriber connects and creates durable <PLACE_HOLDER>,mqtt client client = create client ( false @$ __str__ @$ listener ) ; final string account_prefix = __str__ ; client . subscribe ( account_prefix + __str__ ) ; client . subscribe ( account_prefix + __str__ ) ; client . subscribe ( account_prefix + __str__ ) ; assert true ( client . get pending delivery tokens ( ) . length == __num__ ) ; string expected result = __str__ ; client . publish ( account_prefix + __str__ @$ expected result . get bytes ( standard charsets . utf_8 ) @$ __num__ @$ false ) ;,subscriber connects
bt configures the interface elsewhere : only start <PLACE_HOLDER> .,final inet4 address srv addr = ( inet4 address ) parse numeric address ( bluetooth_iface_addr ) ; return configure dhcp ( enabled @$ srv addr @$ bluetooth_dhcp_prefix_length ) ;,configures start
note : make sure the array has a <PLACE_HOLDER> of at least 1,int array size = math . max ( default_size @$ list size ) ; list = new light [ array size ] ; dist to owner = new float [ array size ] ; for ( int i = __num__ ; i < list size ; i ++ ) { list [ i ] = lights . get ( i ) ; } arrays . fill ( dist to owner @$ float . negative_infinity ) ;,array has
prepopulate the dep <PLACE_HOLDER> rule key and dep <PLACE_HOLDER> .,build info recorder recorder = create build info recorder ( rule . get build target ( ) ) ; recorder . add build metadata ( build info . metadata key . dep_file_rule_key @$ dep file rule key . to string ( ) ) ; recorder . add metadata ( build info . metadata key . dep_file @$ immutable list . of ( file to dep file entry string ( input ) ) ) ;,dep rule
transition to standby will set rm 's ha status and then reinitialize in a separate thread . despite asserting for standby state @$ it 's possible for reinitialization to be unfinished . wait here for it to finish @$ otherwise closing rm 1 will close zk manager and the unfinished reinitialization will throw an <PLACE_HOLDER> .,thread . sleep ( __num__ ) ; rm1 . close ( ) ; rm2 . close ( ) ;,reinitialization throw
let the first client receive all <PLACE_HOLDER>,msgs client1 += receive all messages ( client1 ) ; client1 . close ( ) ;,client receive
rr <PLACE_HOLDER> matches but app <PLACE_HOLDER> does not . rr <PLACE_HOLDER> should be set to app <PLACE_HOLDER>,rr . set node label expression ( enforced label1 ) ; scheduler utils . enforce partition exclusivity ( rr @$ enforced exclusive label set @$ null ) ; assert . assert null ( rr . get node label expression ( ) ) ; rr . set node label expression ( enforced label2 ) ; scheduler utils . enforce partition exclusivity ( rr @$ enforced exclusive label set @$ app label ) ; assert . assert equals ( app label @$ rr . get node label expression ( ) ) ;,label does
set paint style @$ style.fill will fill the <PLACE_HOLDER> @$ style.stroke will stroke the <PLACE_HOLDER>,paint . set style ( paint . style . fill ) ; canvas . draw rect ( get dimension in pixel ( __num__ ) @$ get height ( ) - ( __num__ + random . next int ( ( int ) ( get height ( ) / __num__ ) - __num__ ) ) @$ get dimension in pixel ( __num__ ) @$ get height ( ) - __num__ @$ paint ) ; canvas . draw rect ( get dimension in pixel ( __num__ ) @$ get height ( ) - ( __num__ + random . next int ( ( int ) ( get height ( ) / __num__ ) - __num__ ) ) @$ get dimension in pixel ( __num__ ) @$ get height ( ) - __num__,style.fill fill
native wrappers for methods do n't have a <PLACE_HOLDER>,if ( c != null ) { c . setn method ( nm ) ; },wrappers have
whether new am could get container complete <PLACE_HOLDER>,allocate response allocate response = am2 . allocate ( new array list < resource request > ( ) @$ new array list < container id > ( ) ) ; list < container status > container statuses = allocate response . get completed containers statuses ( ) ; if ( is container id in container status ( container statuses @$ container id2 ) == false ) { assert . fail ( ) ; } container statuses = attempt2 . get just finished containers ( ) ; if ( is container id in container status ( container statuses @$ container id2 ) ) { assert . fail ( ) ; },am get
we wo n't compare the candidate display name against the current item . this is to prevent an validation warning if the user sets the display name to <PLACE_HOLDER> the existing display name is,if ( item . get name ( ) . equals ( current job name ) ) { continue ; } else if ( display name . equals ( item . get display name ( ) ) ) { return false ; },name is
first request has no continuation <PLACE_HOLDER> .,state tag state tag = new state tag ( state tag . kind . bag @$ encoded tag @$ state family ) ;,request has
change link properties should have updated tcp buffer <PLACE_HOLDER> .,link properties lp = new link properties ( ) ; lp . set tcp buffer sizes ( test tcp buffer sizes ) ; m cell network agent . send link properties ( lp ) ; network callback . expect callback ( callback entry . link_properties_changed @$ m cell network agent ) ; verify tcp buffer size change ( test tcp buffer sizes ) ;,properties have
user 3 joins the new <PLACE_HOLDER>,multi user chat muc3 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc3 . join ( __str__ ) ;,user joins
60 fps would require 16 msec <PLACE_HOLDER> here .,uninterruptibles . sleep uninterruptibly ( __num__ @$ time unit . milliseconds ) ;,fps require
add the parceler instance which uses the <PLACE_HOLDER> to parcel the field data,type name parceler type = parameterized type name . get ( class name . get ( package_parceler @$ __str__ ) @$ class name ) ; builder . add field ( field spec . builder ( parceler type @$ __str__ @$ modifier . static @$ modifier . final ) . initializer ( __str__ @$ parceler type @$ entity generator . type_name ) . build ( ) ) ;,which uses
metrics undock <PLACE_HOLDER>,toggle split screen mode ( - __num__ @$ - __num__ ) ;,metrics undock
repeatedly bounce a receiving site member which will cause partition offline <PLACE_HOLDER>,async invocation < integer > close open invocation = vm3 . invoke async ( ( ) -> close recreate cache ( ny port @$ region name @$ __num__ ) ) ;,which cause
test inner segment queries test base <PLACE_HOLDER>,aggregation operator aggregation operator = get operator for query ( base_query ) ; intermediate results block results block = aggregation operator . next block ( ) ; execution statistics execution statistics = aggregation operator . get execution statistics ( ) ; queries test utils . test inner segment execution statistics ( execution statistics @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ; list < object > aggregation result = results block . get aggregation result ( ) ; assert . assert equals ( ( ( hyper log log ) aggregation result . get ( __num__ ) ) . cardinality ( ) @$ __num__ ) ; assert . assert equals ( ( ( hyper log log ) aggregation result . get ( __num__ ) ) . cardinality (,queries test
after terminate suppressed error <PLACE_HOLDER> itself suppressed original err,assert equals ( __num__ @$ after terminate . get suppressed ( ) . length ) ; assert equals ( error @$ after terminate . get suppressed ( ) [ __num__ ] ) ; assert equals ( __num__ @$ error . get suppressed ( ) . length ) ; assert equals ( err @$ error . get suppressed ( ) [ __num__ ] ) ;,terminate suppressed
services make no <PLACE_HOLDER> between actual & valid,fmt . set locale ( uloc @$ uloc ) ;,services make
validate <PLACE_HOLDER> here instead of in port mapping parser to guaruntee every instance has a valid <PLACE_HOLDER>,if ( ip != null && ! inet addresses . is inet address ( this . ip ) ) { throw new illegal argument exception ( ip + __str__ ) ; } this . internal port = internal port ; this . external port = external port ; this . protocol = optional . from nullable ( protocol ) . or ( tcp ) ;,instance has
constructor is public @$ but class has package <PLACE_HOLDER>,system . out . println ( __str__ ) ; system . err . println ( __str__ ) ; ex . print stack trace ( ) ;,class has
while visit the unmatched build <PLACE_HOLDER> @$ the probe element is null @$ and the unmatched build iterator would iterate all the unmatched build <PLACE_HOLDER> @$ so we return false during the second calling of this method .,this . unmatched build visited = true ; return true ;,unmatched build
it is a good practice to remove location requests when the activity is in a paused or stopped state . doing so helps battery <PLACE_HOLDER> and is especially recommended in applications that request frequent location updates .,m fused location client . remove location updates ( m location callback ) . add on complete listener ( this @$ new on complete listener < void > ( ) { @ override public void on complete ( @ non null task < void > task ) { m requesting location updates = false ; set buttons enabled state ( ) ; } } ) ;,state helps
mark the view drawn to ensure that it gets invalidated properly the next <PLACE_HOLDER> it is visible and gets invalidated,m private flags |= pflag_drawn ;,view invalidated
this simulates the completion of txnid : id txn update <PLACE_HOLDER>,long write id = txn mgr2 . get table write id ( __str__ @$ __str__ ) ; add dynamic partitions adp = new add dynamic partitions ( txn mgr2 . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr2 . commit txn ( ) ;,txn update
check if remainder of href contains any illegal <PLACE_HOLDER> before proceeding,if ( i < len ) { for ( int j = i ; j < len ; ++ j ) { ch = href . char at ( j ) ; if ( ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) ) { continue ; } if ( xml char . is high surrogate ( ch ) && ++ j < len ) { int ch2 = href . char at ( j ) ; if ( xml char . is low surrogate ( ch2 ) ) { ch2 = xml char . supplemental ( (,remainder contains
test the provider method object itself . this makes sure get instance <PLACE_HOLDER> @$ since gin uses it,list < element > elements = elements . get elements ( module ) ; assert equals ( __num__ @$ elements . size ( ) ) ; element element = elements . get ( __num__ ) ; assert true ( element + __str__ @$ element instanceof provider instance binding ) ; provider instance binding binding = ( provider instance binding ) element ; javax . inject . provider provider = binding . get user supplied provider ( ) ; assert true ( provider instanceof provider method ) ; assert equals ( methods object @$ ( ( provider method ) provider ) . get instance ( ) ) ; assert same ( provider @$ binding . get provider instance ( ) ) ;,test get
the registry now owns native <PLACE_HOLDER> @$ even if registration threw an exception .,s proxy map . set ( i binder @$ result ) ;,registry owns
handle negatives @$ which means last n <PLACE_HOLDER>,if ( start < __num__ ) { start = str . length ( ) + start ; },which means
nb : in this configuration each thread creates a <PLACE_HOLDER> every 4 txs provided it no other tx creates a <PLACE_HOLDER> at the same time,int threads = __num__ ; load entities concurrently with specific sharding threshold ( __num__ @$ __num__ @$ __num__ @$ threads @$ __num__ / threads ) ;,thread creates
firsts contains at least one entry @$ always contains everything else . let 's combine <PLACE_HOLDER> into the stream to form a unified set of capabilities . woohoo !,from oss = firsts . stream ( ) . map ( first -> immutable map . < string @$ object > builder ( ) . put all ( always ) . put all ( first ) . build ( ) ) . map ( this :: apply transforms ) . map ( map -> map . entry set ( ) . stream ( ) . filter ( entry -> accepted_w3c_patterns . test ( entry . get key ( ) ) ) . collect ( immutable map . to immutable map ( map . entry :: get key @$ map . entry :: get value ) ) ) ; from oss = stream . of ( ) ;,"s" combine
if one reader has not processed any <PLACE_HOLDER> @$ its 'last offset ' will be null . in this case @$ it must the be the lagging reader .,if ( a offset == null ) { a reader leading = false ; } else if ( b offset == null ) { a reader leading = true ; } else { document a document = source info . create document from offset ( a offset ) ; document b document = source info . create document from offset ( b offset ) ; a reader leading = source info . is position at or before ( b document @$ a document @$ binlog readera . context . gtid source filter ( ) ) ; } if ( a reader leading ) { logger . info ( __str__ ) ; } else { logger . info ( __str__ ) ; },reader processed
assume this can be done ; if not throw <PLACE_HOLDER> as per javadoc,@ suppress warnings ( __str__ ) final comparator < object > comparator2 = ( comparator < object > ) comparator ;,not throw
some tables do not use dynamic <PLACE_HOLDER>,if ( ! ( cm instanceof g table column model ) ) { super . create default columns from model ( ) ; return ; },tables use
this value must not exceed 4 <PLACE_HOLDER>,list buffer . add ( encode ( default_timecode_scale @$ true ) ) ;,value exceed
nt <PLACE_HOLDER> will call a <PLACE_HOLDER> that does n't exist,system . out . flush ( ) ; system . out . println ( __str__ ) ; try { client response impl cri = ( client response impl ) client . call procedure ( __str__ @$ run on all partitionsnt proc . call_missing_proc ) ; system . err . println ( cri . tojson string ( ) ) ; fail ( ) ; } catch ( proc call exception e ) { response = ( client response impl ) e . get client response ( ) ; assert equals ( client response . unexpected_failure @$ response . get status ( ) ) ; system . out . println ( __str__ ) ; system . out . println ( response . tojson string ( ) ) ; },proc call
get post put head <PLACE_HOLDER> patch delete trace connect,return magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ || magic1 == __str__ && magic2 == __str__ ;,post put
a proper exit will change this <PLACE_HOLDER> .,this . s exit = crawl status . finished_abnormal ; if ( get pause at start ( ) ) { complete pause ( ) ; } else { get frontier ( ) . run ( ) ; },exit change
response impl will make the headers <PLACE_HOLDER>,final map < string @$ string > modifiable headers = new tree map < string @$ string > ( string . case_insensitive_order ) ; modifiable headers . put all ( raw response . get headers ( ) ) ;,impl make
extra element to make sure we have the same <PLACE_HOLDER> to compute the length of each element of the array .,start position = new int [ fields . length + __num__ ] ;,element have
charset encoder icu has taken ownership ; its finalizer will do the <PLACE_HOLDER> .,address = __num__ ;,finalizer do
namespaces can present some <PLACE_HOLDER> @$ so just punt if we 're looking for these .,if ( is set ( analysis @$ bit_namespace ) ) return false ;,namespaces present
the xsd requires a cmmn <PLACE_HOLDER> to be there @$ even though the spec text says it 's optional,xtw . write start element ( cmmndi_prefix @$ element_di_label @$ cmmndi_namespace ) ; xtw . write end element ( ) ; xtw . write end element ( ) ;,xsd requires
verify that the specified packages matches the provided <PLACE_HOLDER> .,int user id = user handle . get user id ( uid ) ; try { application info app info = mi package manager . get application info ( package name @$ __num__ @$ user id ) ; if ( app info == null ) { log . w ( log_tag @$ string . format ( __str__ @$ package name ) ) ; return false ; } else if ( uid != app info . uid ) { string message = string . format ( __str__ @$ package name @$ app info . uid @$ uid ) ; log . w ( log_tag @$ message ) ; throw new security exception ( message ) ; } } catch ( remote exception e ) { log . e (,packages matches
not all supported andorid version have this <PLACE_HOLDER>,if ( m selector position field == null ) { for ( int i = __num__ ; i < get child count ( ) ; i ++ ) { if ( get child at ( i ) . get bottom ( ) == m selector rect . bottom ) { return i + get fixed first visible item ( ) ; } } } else { try { return m selector position field . get int ( this ) ; } catch ( illegal argument exception e ) { e . print stack trace ( ) ; } catch ( illegal access exception e ) { e . print stack trace ( ) ; } },version have
while processing an admin request @$ hdfs failed lock could take long <PLACE_HOLDER> because of multiple hdfs operations @$ especially when the name node is in a different data center . so extend <PLACE_HOLDER>out to 5 minutes .,admin client config admin config = new admin client config ( ) . set max connections per node ( cluster . get number of nodes ( ) ) . set max backoff delay ms ( max backoff delay ms ) . set admin socket timeout sec ( __num__ * __num__ ) ;,lock take
get jpa managed <PLACE_HOLDER> through the repository,message message = message repository . find one ( __num__ ) ; assert not null ( message ) ; assert equals ( __str__ @$ message . get text ( ) ) ;,jpa managed
there is a case that can lead us here . the caller is moving the top activity that is in a <PLACE_HOLDER> that has multiple activities to pip mode . for that the caller is creating a new <PLACE_HOLDER> to host the activity so that we only move the top activity to pip mode and keep other activities in the previous <PLACE_HOLDER> . there,if ( root == null ) { return result_skip ; },caller creating
pointers to polyphase <PLACE_HOLDER>,int [ ] polyp = new int [ ilbc_constants . enh_ups0 ] ;,pointers polyphase
use random ids so that one app can not know that another app creates <PLACE_HOLDER>,int session id ; int tries = __num__ ; do { tries ++ ; if ( tries > max_session_id_create_tries ) { slog . w ( tag @$ __str__ + max_session_id_create_tries + __str__ ) ; return null ; } session id = math . abs ( s random . next int ( ) ) ; } while ( session id == __num__ || session id == no_session || m sessions . index of key ( session id ) >= __num__ ) ; assert caller locked ( component name @$ compat mode ) ;,app creates
check correctness of cross if udf returns right input <PLACE_HOLDER>,final execution environment env = execution environment . get execution environment ( ) ; data set < tuple3 < integer @$ long @$ string > > ds = collection data sets . get small3 tuple data set ( env ) ; data set < tuple5 < integer @$ long @$ integer @$ string @$ long > > ds2 = collection data sets . get small5 tuple data set ( env ) ; data set < tuple5 < integer @$ long @$ integer @$ string @$ long > > cross ds = ds . cross ( ds2 ) . with ( new tuple5 return right ( ) ) ; list < tuple5 < integer @$ long @$ integer @$ string @$ long > > result = cross ds .,udf returns
foo singleton impl was implicitly bound @$ it is an error to call get instance or get <PLACE_HOLDER> @$ it is ok to call get binding for introspection @$ but an error to get the <PLACE_HOLDER> of the binding,ensure fails ( injector @$ allow_binding @$ scoped foo impl . class ) ;,error get
prefer entry destroyed <PLACE_HOLDER> over statistics disabled <PLACE_HOLDER>,check entry destroyed ( ) ; checktx ( ) ; if ( ! this . local region . statistics enabled ) { throw new statistics disabled exception ( string . format ( __str__ @$ this . local region . get full path ( ) ) ) ; },entry destroyed
this makes the dialog take up the full <PLACE_HOLDER>,window manager . layout params lp = new window manager . layout params ( ) ; lp . copy from ( get window ( ) . get attributes ( ) ) ; lp . width = window manager . layout params . match_parent ; lp . height = window manager . layout params . wrap_content ; get window ( ) . set attributes ( lp ) ; account = new account ( get intent ( ) . get string extra ( key_account_name ) @$ get intent ( ) . get string extra ( key_account_type ) ) ; package name = get intent ( ) . get string extra ( key_android_package_name ) ; service = get intent ( ) . get string extra ( key_authtoken ) ; if (,dialog take
required features expect activity launch <PLACE_HOLDER> in,fail ( __str__ ) ;,features expect
snap to edge which has no <PLACE_HOLDER> nodes,iter = expl . set base node ( __num__ ) ; iter . next ( ) ; res = create location result ( __num__ @$ __num__ @$ iter @$ __num__ @$ edge ) ; query graph query graph6 = lookup ( res ) ; assert equals ( new gh point ( __num__ @$ __num__ ) @$ res . get snapped point ( ) ) ; assert equals ( __num__ @$ res . get closest node ( ) ) ; assert equals ( __num__ @$ get points ( query graph6 @$ __num__ @$ __num__ ) . get size ( ) ) ; assert equals ( __num__ @$ get points ( query graph6 @$ __num__ @$ __num__ ) . get size ( ) ) ;,which has
the leak detector should report no leaked <PLACE_HOLDER> as the query is still running,leak detector . check for memory leaks ( ( ) -> immutable list . of ( create query info ( test query . get id ( ) @$ running ) ) @$ immutable map . of ( test query @$ __num__ ) ) ; assert equals ( leak detector . get number of leaked queries ( ) @$ __num__ ) ;,detector report
if the resource returns a <PLACE_HOLDER> we do n't have a separate status <PLACE_HOLDER> in the batch response,assert . assert equals ( entity . get statuses ( ) @$ collections . empty map ( ) ) ;,resource returns
system tables have no <PLACE_HOLDER>,execute ( __str__ ) ; for ( object [ ] row : response . rows ( ) ) { assert null ( row [ __num__ ] ) ; } execute ( __str__ ) ; for ( object [ ] row : response . rows ( ) ) { assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( (,tables have
update total <PLACE_HOLDER> @$ if necessary .,if ( m total duration != duration_infinite ) { m total duration += delta ; },update total
it would be better if the on site changed provided the <PLACE_HOLDER> of changed sites .,if ( get selected site ( ) == null && m site store . has site ( ) ) { set selected site ( m site store . get sites ( ) . get ( __num__ ) ) ; } if ( get selected site ( ) == null ) { return ; } site model site = m site store . get site by local id ( get selected site ( ) . get id ( ) ) ; if ( site != null ) { m selected site = site ; } if ( get my site fragment ( ) != null ) { get my site fragment ( ) . on site changed ( site ) ; },site provided
how many header <PLACE_HOLDER> ?,int argnr = rep . count nr job entry attributes ( id_jobentry @$ __str__ ) ; allocate ( argnr ) ; for ( int a = __num__ ; a < argnr ; a ++ ) { header name [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; header value [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; },many header
this case should pass on <PLACE_HOLDER> 6 and later .,if ( ! system . get property ( __str__ ) . starts with ( __str__ ) ) { fail ( ) ; } assert equals ( __str__ @$ e . get message ( ) ) ; assert equals ( __str__ @$ e . get format name ( ) ) ;,case pass
set the configuration back @$ so that tool can configure <PLACE_HOLDER>,tool . set conf ( conf ) ;,tool configure
user 1 sends a <PLACE_HOLDER> to the bare jid of user 0,chat chat = get connection ( __num__ ) . get chat manager ( ) . create chat ( get barejid ( __num__ ) @$ null ) ; chat . send message ( __str__ ) ; chat . send message ( __str__ ) ;,user sends
top horizontal non croppped <PLACE_HOLDER>,height = this . crop zone rect . y - this . image rect . y ; if ( height > __num__ ) { g . fill rect ( this . crop zone rect . x @$ this . image rect . y @$ this . crop zone rect . width @$ height ) ; },non croppped
do n't let the user click restore if the <PLACE_HOLDER> area contains the current wallet <PLACE_HOLDER> @$ or are an invalid set @$ or if the date field is n't set @$ or if it 's in the future .,restore button . disable property ( ) . bind ( or ( or ( not ( validator . valid ) @$ equal ( orig words @$ words area . text property ( ) ) ) @$ date picker is invalid ) ) ;,area contains
check that pause resume wo n't call the end <PLACE_HOLDER> prematurely,resp . pause ( ) ; resp . resume ( ) ;,resume call
for single partition key column table @$ we can merge multiple partitions into a single split by using in clause in a single select query if the partitions have the same host <PLACE_HOLDER> . for multiple partition key columns table @$ we ca n't merge them into a single select query @$ so keep them in a separate split .,boolean single partition key column = true ; string partition key column name = null ; if ( ! partitions . is empty ( ) ) { single partition key column = partitions . get ( __num__ ) . get tuple domain ( ) . get domains ( ) . get ( ) . size ( ) == __num__ ; if ( single partition key column ) { string partition id = partitions . get ( __num__ ) . get partition id ( ) ; partition key column name = partition id . substring ( __num__ @$ partition id . last index of ( __str__ ) - __num__ ) ; } } map < set < string > @$ set < string > > hosts to partition keys,partitions have
the connection should fail since it the dry <PLACE_HOLDER>,return ;,connection fail
test case 3 set io permission granted to code that was signed by first signer set factory permission granted to code that was signed by second signer keystore that contains only first keypairs code was singed by first signer and second signer expect access control <PLACE_HOLDER> for set factory permission,system . out . println ( __str__ ) ; cmd = constructcmd ( __str__ @$ policy2 @$ __str__ @$ __str__ ) ; process tools . execute test jvm ( cmd ) . should have exit value ( __num__ ) ;,signer expect
due how xml pull parser <PLACE_HOLDER> @$ the xml is fully loaded on the ram,super ( false @$ true @$ algorithm_ttml_converter ) ;,xml pull
if enable exceed throttle <PLACE_HOLDER> @$ make sure that region server throttle <PLACE_HOLDER>s are in seconds time unit . because once previous requests exceed their <PLACE_HOLDER> and consume region server <PLACE_HOLDER> @$ <PLACE_HOLDER> in other time units may be refilled in a long time @$ this may affect later requests .,list < pair < boolean @$ timed quota > > list = arrays . as list ( pair . new pair ( throttle . has req num ( ) @$ throttle . get req num ( ) ) @$ pair . new pair ( throttle . has read num ( ) @$ throttle . get read num ( ) ) @$ pair . new pair ( throttle . has write num ( ) @$ throttle . get write num ( ) ) @$ pair . new pair ( throttle . has req size ( ) @$ throttle . get req size ( ) ) @$ pair . new pair ( throttle . has read size ( ) @$ throttle . get read size ( ) ) @$ pair,requests exceed
second split contains the <PLACE_HOLDER> 3 and <PLACE_HOLDER> 4 @$ however @$ the locations is undetermined .,if ( split . equals ( splits . get ( __num__ ) ) ) { assert equals ( __num__ @$ file split . get num paths ( ) ) ; expected . clear ( ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ __num__ ) ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ blocksize ) ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ blocksize * __num__ ) ) ; expected . add ( new split ( file4 . get name ( ) @$ blocksize @$ __num__ ) ) ; expected . add ( new split ( file4 . get,split contains
some already compressed <PLACE_HOLDER>,return arrays . stream ( new object [ ] [ ] { { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip,some compressed
notification payload <PLACE_HOLDER>,path scheme path = scheme generator . write scheme ( ) ; document builder factory db factory = document builder factory . new instance ( ) ; document builder d builder = db factory . new document builder ( ) ; document scheme = d builder . parse ( project filesystem . new file input stream ( scheme path ) ) ; x path factory xpath factory = x path factory . new instance ( ) ; x path buildable referencex path = xpath factory . newx path ( ) ; x path expression buildable reference expr = buildable referencex path . compile ( __str__ ) ; node list buildable references = ( node list ) buildable reference expr . evaluate ( scheme @$ x path constants .,notification payload
second key should log an <PLACE_HOLDER> .,return new object [ ] [ ] { { __str__ @$ all protocol versions . restli_protocol_1_0_0 . get protocol version ( ) @$ __str__ @$ null @$ resource method . batch_get @$ __str__ @$ new hash set < > ( arrays . as list ( __num__ @$ __num__ @$ __num__ ) ) } @$ { __str__ @$ all protocol versions . restli_protocol_1_0_0 . get protocol version ( ) @$ __str__ @$ null @$ resource method . batch_get @$ __str__ @$ new hash set < > ( arrays . as list ( __num__ @$ __num__ ) ) } @$ { __str__ @$ all protocol versions . restli_protocol_2_0_0 . get protocol version ( ) @$ __str__ @$ null @$ resource method . batch_get @$ __str__ @$ new hash set <,key log
version of refine which wrote the <PLACE_HOLDER>,reader . read line ( ) ;,which wrote
ok @$ one or both lists had extra <PLACE_HOLDER>,if ( message == null ) { message = __str__ + __str__ + __str__ + expected copy + __str__ + actual copy + __str__ ; } fail ( message ) ;,ok had
function will always processes the first <PLACE_HOLDER>,dictionary block ineffective block = create dictionary block ( __num__ @$ __num__ ) ; test project range ( ineffective block @$ dictionary block . class @$ projection @$ force yield @$ produce lazy block ) ; test project fast return ignore yield ( ineffective block @$ projection @$ produce lazy block ) ;,function processes
test trusted channel resolver does not use encryption <PLACE_HOLDER> .,if ( resolver clazz != null ) { return ; },resolver use
each job gets its own metric log <PLACE_HOLDER>,path metrics log dir = new path ( properties . get property ( configuration keys . metrics_log_dir_key ) @$ this . get name ( ) ) ; if ( ! fs . exists ( metrics log dir ) && ! fs . mkdirs ( metrics log dir ) ) { logger . error ( __str__ + this . get name ( ) ) ; return ; },job gets
this routine is to process the plus sign in the dial string by loop through the network portion @$ post dial portion 1 @$ post dial portion 2 ... <PLACE_HOLDER> . if applied,do { string network dial str ; if ( use nanp ) { network dial str = extract network portion ( temp dial str ) ; } else { network dial str = extract network portion alt ( temp dial str ) ; } network dial str = process plus code ( network dial str @$ use nanp ) ; if ( ! text utils . is empty ( network dial str ) ) { if ( ret str == null ) { ret str = network dial str ; } else { ret str = ret str . concat ( network dial str ) ; } } else { rlog . e ( __str__ @$ network dial str ) ; return dial str ; } post dial,routine is
keys in record schema @$ string @$ boolean @$ bytes <PLACE_HOLDER>,object inputs [ ] [ ] = { { as map ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __str__ @$ __str__ @$ true @$ __str__ @$ byte string . copy avro string ( __str__ @$ false ) ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __str__ @$ __str__ @$ true @$ __str__ @$ byte string . copy avro string ( __str__ @$ false ) @$ __str__ @$,keys bytes
skip gc 'd weak <PLACE_HOLDER>,for ( hash entry < k @$ v > p = e ; p != last run ; p = p . next ) { k key = p . key ( ) ; if ( key == null ) { reduce ++ ; continue ; } int k = p . hash & size mask ; hash entry < k @$ v > n = new table [ k ] ; new table [ k ] = new hash entry ( key @$ p . hash @$ n @$ p . value ( ) ) ; },gc "d"
asif : a <PLACE_HOLDER> level of zero means exact <PLACE_HOLDER> .,if ( index expression . equals ( other expression ) && index type == other type ) { match level = get match level ( other definitions @$ index definitions @$ mapping ) ; },level means
a filter with a start date of now should not accept the <PLACE_HOLDER>,filter = new log filter ( level . info @$ local date time . now ( ) @$ null ) ; assert that ( filter . accepts file ( path ) ) . is false ( ) ;,filter accept
learn that a specific host has accessed a specific <PLACE_HOLDER>,access path = __str__ ;,host accessed
onload will set appropriate <PLACE_HOLDER> later,if ( icon instanceof image icon ) { icon . set width ( __str__ ) ; icon . set height ( __str__ ) ; },onload set
spinner model did n't like new <PLACE_HOLDER> @$ reset,try { ( ( j formatted text field ) source ) . set value ( last value ) ; } catch ( illegal argument exception iae2 ) { },model like
user can see these <PLACE_HOLDER>,index for user ( user1 @$ new doc ( ) . set languages ( singleton list ( __str__ ) ) @$ new doc ( ) . set languages ( as list ( __str__ @$ __str__ ) ) ) ;,user see
make sure the user has not requested an insane <PLACE_HOLDER> of txns .,int max txns = metastore conf . get int var ( conf @$ conf vars . txn_max_open_batch ) ; if ( num txns > max txns ) num txns = max txns ; stmt = db conn . create statement ( ) ; list < long > txn ids = open txns ( db conn @$ stmt @$ rqst ) ; log . debug ( __str__ ) ; db conn . commit ( ) ; return new open txns response ( txn ids ) ; close ( null @$ stmt @$ db conn ) ; unlock internal ( ) ;,user requested
should never happen @$ dumpstate always set the <PLACE_HOLDER> .,if ( bugreport file == null ) { log . wtf ( tag @$ __str__ + extra_bugreport + __str__ + intent ) ; return ; },dumpstate set
group has no <PLACE_HOLDER> @$ mapping has been disabled or no role mappers were found : simply return the group contents .,if ( group contents . is empty ( ) ) { for ( principal role principal : group principals ) { group contents . add ( role principal . get name ( ) ) ; } },group has
ensure calculation type getter returns correct <PLACE_HOLDER> and value,cal . set calculation type ( calculation type . islamic_umalqura ) ; object ct obj = cal . get calculation type ( ) ; if ( ct obj instanceof calculation type ) { calculation type ct = ( calculation type ) ct obj ; if ( ct != calculation type . islamic_umalqura ) { errln ( __str__ ) ; } } else { errln ( __str__ ) ; } date now = new date ( ) ; cal . set time ( now ) ; date then = cal . get time ( ) ; if ( ! now . equals ( then ) ) { errln ( __str__ ) ; } logln ( then . to string ( ) ) ; cal . add ( calendar .,returns correct
whether device owner enforces camera <PLACE_HOLDER> .,boolean disallow camera globally = false ; if ( is device owner ) { final active admin device owner = get device owner admin locked ( ) ; if ( device owner == null ) { return ; } user restrictions = device owner . user restrictions ; disallow camera globally = device owner . disable camera ; } else { final active admin profile owner = get profile owner admin locked ( user id ) ; user restrictions = profile owner != null ? profile owner . user restrictions : null ; },owner enforces
forward any remaining selected <PLACE_HOLDER> .,if ( batch . size > __num__ ) { forward big table batch ( batch ) ; },any selected
if the merged anomaly 's create <PLACE_HOLDER> is before last notify <PLACE_HOLDER> @$ discard,if ( snapshot . contains key ( snapshot key ) ) { long last notify time = alert snap shot . get latest status ( snapshot @$ snapshot key ) . get last notify time ( ) ; if ( merged anomaly . get created time ( ) < last notify time ) { iterator . remove ( ) ; } },last notify
the new one should have the current @$ max and total <PLACE_HOLDER> that we had when we wrote it,assert equals ( __num__ @$ full . get current duration ms locked ( __num__ ) ) ; assert equals ( __num__ @$ full . get max duration ms locked ( __num__ ) ) ; assert equals ( __num__ @$ full . get total duration ms locked ( __num__ ) ) ;,one have
there should never be more than 1 intersecting vertex . but if it happens as a fallback simply skip <PLACE_HOLDER> .,m arr temp vertices . add ( n ) ; m arr temp vertices . add all ( intersections ) ;,fallback skip
if frame queue is full @$ let 's drop everything . if frame queue accepts this frame @$ let 's recycle the <PLACE_HOLDER> as well .,if ( m frame queue . offer ( frame ) ) { int curr size = buffer . length ; int req size = m buffer size ; if ( curr size == req size ) { if ( m buffer mode == buffer_mode_dispatch ) { m buffer callback . on buffer available ( buffer ) ; } else { m buffer queue . offer ( buffer ) ; } } },"s" recycle
all log <PLACE_HOLDER> whose transaction id is not less than provided transaction id,if ( segment . get first tx id ( ) >= transaction id ) { final first tx id not less than selector selector = new first tx id not less than selector ( transaction id ) ; async read record from entries ( log name @$ reader @$ segment @$ executor service @$ new single entry scan context ( __num__ ) @$ selector ) . when complete ( new future event listener < log record withdlsn > ( ) { @ override public void on success ( log record withdlsn value ) { promise . complete ( optional . of ( selector . result ( ) ) ) ; } @ override public void on failure ( throwable cause ) { promise . complete exceptionally ( cause,all log
length of row is different @$ copy <PLACE_HOLDER> except family,if ( current . last common prefix < bytes . sizeof_short ) { int old row length with size = current . row length with size ; current buffer . get ( current . key buffer @$ current . last common prefix @$ bytes . sizeof_short - current . last common prefix ) ; current . row length with size = bytes . to short ( current . key buffer @$ __num__ ) + bytes . sizeof_short ; system . arraycopy ( current . key buffer @$ old row length with size @$ current . key buffer @$ current . row length with size @$ current . family length with size ) ; current buffer . get ( current . key buffer @$ bytes . sizeof_short @$ current,length different
parent window can be null if the child is detached from it 's parent already @$ but someone still has a <PLACE_HOLDER> to access it . so @$ we return the top parent value we already have instead of null .,if ( current != null ) { top parent = current ; },someone has
instrument a logger to block the <PLACE_HOLDER> of a remove sub advisory simulate a slow thread,slow down appender = new default test appender ( ) { @ override public void do append ( logging event logging event ) { if ( level . debug . equals ( logging event . get level ( ) ) ) { string message = logging event . get message ( ) . to string ( ) ; if ( message . starts with ( __str__ ) && message . contains ( __str__ ) ) { try { consumer demand exists . count down ( ) ; system . err . println ( __str__ + message ) ; time unit . seconds . sleep ( __num__ ) ; } catch ( exception ignored ) { } } } } } ;,logger block
this user has the email <PLACE_HOLDER> and could be stored @$ but the schema is still incompatible so the entire stream is rejected,record incompatible user = new record ( incompatible ) ; incompatible user . put ( __str__ @$ __num__ ) ; incompatible user . put ( __str__ @$ __str__ ) ; incompatible user . put ( __str__ @$ __str__ ) ; test runner runner = test runners . new test runner ( store in kite dataset . class ) ; runner . set property ( store in kite dataset . kite_dataset_uri @$ dataset uri ) ; runner . assert valid ( ) ; runner . enqueue ( stream for ( incompatible user ) ) ; runner . run ( ) ; runner . assert all flow files transferred ( __str__ @$ __num__ ) ;,user has
1 | 1 not yet visible @$ should n't immediately expand <PLACE_HOLDER>,assert equals ( __num__ @$ grid . get row count ( ) ) ; assert cell texts ( __num__ @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ } ) ; select menu path ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,1 expand
stolen from sp scheduler . need to update the duplicate <PLACE_HOLDER> associated with any every partition tasks cleanup duplicate <PLACE_HOLDER> and collect done <PLACE_HOLDER> in this list for further processing .,return new long [ __num__ ] ;,counters duplicate
a list that is expanded with builder methods has the added <PLACE_HOLDER> at the end,p collection list < long > with one create = pc list . and ( create two ) ; assert that ( with one create . get all ( ) @$ contains ( bounded count @$ max read time count @$ unbounded count @$ create two ) ) ;,list has
and finally @$ let 's allow <PLACE_HOLDER> to be converted too,if ( t == json token . value_string ) { string text = p . get text ( ) . trim ( ) ; if ( __str__ . equals ( text ) || __str__ . equals ( text ) ) { _verify string for scalar coercion ( ctxt @$ text ) ; return boolean . true ; } if ( __str__ . equals ( text ) || __str__ . equals ( text ) ) { _verify string for scalar coercion ( ctxt @$ text ) ; return boolean . false ; } if ( text . length ( ) == __num__ ) { return ( boolean ) _coerce empty string ( ctxt @$ _primitive ) ; } if ( _has textual null ( text ) ) { return,"s" allow
start the modify procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new modify table procedure ( proc exec . get environment ( ) @$ new td ) ) ; int last step = __num__ ;,procedure kill
copy the source <PLACE_HOLDER> to cmroot . as the client will move the source <PLACE_HOLDER> to another location @$ we should make a copy of the <PLACE_HOLDER> to cmroot instead of moving it .,if ( need cm recycle ) { cm . recycle ( source path @$ recycle type . copy @$ true ) ; },client move
registry has an empty <PLACE_HOLDER>,return __str__ ;,registry has
if a and b have different <PLACE_HOLDER> or a has the same sign as the result then there was no overflow @$ return .,if ( ( a ^ b ) < __num__ | ( a ^ naive sum ) >= __num__ ) { return naive sum ; },a have
if any aggregate is distinct @$ bail out if any aggregate is the grouping id @$ bail out if any aggregate call has a filter @$ bail out if any aggregate functions do not support <PLACE_HOLDER> @$ bail out,final immutable bit set bottom aggregate group set = aggregate . get group set ( ) ; final list < aggregate call > top aggregate calls = new array list < > ( ) ; for ( int i = __num__ ; i < aggregate . get agg call list ( ) . size ( ) ; i ++ ) { aggregate call aggregate call = aggregate . get agg call list ( ) . get ( i ) ; if ( aggregate call . is distinct ( ) ) { return ; } if ( aggregate call . get aggregation ( ) . equals ( hive groupingid . instance ) ) { return ; } if ( aggregate call . filter arg >= __num__ ) { return,functions support
this will block until zk comes back @$ at <PLACE_HOLDER> point @$ shutdown will complete,store . shutdown ( new callback < none > ( ) { @ override public void on error ( throwable e ) { warn ( _log @$ __str__ ) ; } @ override public void on success ( none result ) { info ( _log @$ __str__ ) ; } } ) ;,point complete
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( testsql features suite . class ) ;,suite using
let the gui reflect the <PLACE_HOLDER> .,set pro guard configuration ( configuration ) ; parser . close ( ) ;,gui reflect
for single click @$ we handle editing file <PLACE_HOLDER>,if ( evt . get click count ( ) == __num__ && source instanceof j list ) { if ( ( ! fc . is multi selection enabled ( ) || fc . get selected files ( ) . length <= __num__ ) && index >= __num__ && list selection model . is selected index ( index ) && get edit index ( ) == index && edit file == null ) { edit file name ( index ) ; } else { if ( index >= __num__ ) { set edit index ( index ) ; } else { reset edit index ( ) ; } } } else if ( evt . get click count ( ) == __num__ ) { reset edit index ( ),editing file
end else : decode end else : get <PLACE_HOLDER>,throw new java . io . io exception ( __str__ ) ;,end get
in case of multithreaded evaluation the composite partition aware object sink adapter used by the ot ns will take <PLACE_HOLDER> of enqueueing this inseretion on the propagation queues of the different agendas,if ( partitions enabled ) { propagation entry . insert . execute ( handle @$ context @$ working memory @$ object type conf ) ; } else { working memory . add propagation ( new propagation entry . insert ( handle @$ context @$ working memory @$ object type conf ) ) ; },adapter take
return no more than latest detail metrics sz <PLACE_HOLDER> .,if ( detail metrics sz > __num__ ) { if ( detail metrics . size ( ) > detail metrics sz ) { grid bounded priority queue < grid cache query detail metrics adapter > latest metrics = new grid bounded priority queue < > ( detail metrics sz @$ qry_detail_metrics_priority_new_cmp ) ; latest metrics . add all ( detail metrics . values ( ) ) ; return latest metrics ; } return new array list < > ( detail metrics . values ( ) ) ; },metrics sz
we just return all dependencies in the days that fall within end ts and lookback as dependency links themselves do n't have <PLACE_HOLDER> .,list < string > indices = index name formatter . format type and range ( type_dependency @$ begin millis @$ end ts ) ; if ( indices . is empty ( ) ) return call . empty list ( ) ; return search . new call ( search request . create ( indices ) @$ body converters . dependency_links ) ;,themselves have
getting actions for the null <PLACE_HOLDER> @$ which in this case means the body <PLACE_HOLDER>,for ( handler ah : action handlers ) { final action [ ] actions = ah . get actions ( null @$ this ) ; if ( actions != null ) { for ( action action : actions ) { action set . add ( action ) ; keys . add ( action mapper . key ( action ) ) ; } } },which means
if its all in one read then we can just take it all @$ otherwise take only the current frame size and the next iteration starts a new <PLACE_HOLDER> .,if ( current buffer != null ) { if ( current buffer . remaining ( ) >= plain . remaining ( ) ) { current buffer . put ( plain ) ; } else { byte [ ] fill = new byte [ current buffer . remaining ( ) ] ; plain . get ( fill ) ; current buffer . put ( fill ) ; } if ( current buffer . has remaining ( ) ) { return ; } else { current buffer . flip ( ) ; object command = wire format . unmarshal ( new data input stream ( new nio input stream ( current buffer ) ) ) ; do consume ( command ) ; next frame size = - __num__ ; current,size starts
if the incoming vector to copy is random @$ then adding items from the iterator can degrade <PLACE_HOLDER> dramatically if the number of elements is large as this vector tries to stay in order as items are added @$ so it 's better to sort the other vector 's elements by index and then add them to this,copy sorted random access sparse vector ( other ) ;,vector degrade
disable the usual optimizations for <PLACE_HOLDER>ing join output by outer table only . in case of full join @$ the unmatched inner table tuples get appended to the end of the join 's output table thus invalidating the outer table join <PLACE_HOLDER> .,if ( m_join type == join type . full ) { m_sort direction = sort direction type . invalid ; return ; },table join
if we explicitly name the <PLACE_HOLDER> with a name which is both a hash name and a tree name @$ we always get a tree index . this is true even if the column type is hashable .,list < pair < string @$ index type > > passing = arrays . as list ( pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of,which name
if old table satisfies the <PLACE_HOLDER> @$ but the new table does not @$ then the old table should be dropped . this should be done @$ only if the old table is not in the list of tables to be bootstrapped which is a multi rename case . in case of multi rename @$ only the first rename should do the drop .,if ( ! repl utils . table included in repl scope ( within context . repl scope @$ new name ) ) { if ( old table in bootstrap list ) { return false ; } else { scenario = scenario . drop ; log . info ( __str__ + old name + __str__ + new name ) ; return true ; } },table satisfies
if permissions need a <PLACE_HOLDER> before any of the app components can run @$ we launch the <PLACE_HOLDER> activity and pass a pending intent to start the activity we are to launching now after the <PLACE_HOLDER> is completed .,if ( a info != null ) { if ( m service . get package manager internal locked ( ) . is permissions review required ( a info . package name @$ user id ) ) { i intent sender target = m service . get intent sender locked ( activity manager . intent_sender_activity @$ calling package @$ calling uid @$ user id @$ null @$ null @$ __num__ @$ new intent [ ] { intent } @$ new string [ ] { resolved type } @$ pending intent . flag_cancel_current | pending intent . flag_one_shot @$ null ) ; intent new intent = new intent ( intent . action_review_permissions ) ; int flags = intent . get flags ( ) ; flags |= intent . flag_activity_exclude_from_recents ;,permissions need
an empty query to read <PLACE_HOLDER> .,query query = query . new builder ( ) . set limit ( int32 value . new builder ( ) . set value ( num entities ) ) . build ( ) ;,query read
test escaped <PLACE_HOLDER>,assert equals ( do json extract ( __str__ @$ __str__ ) @$ __str__ ) ;,test escaped
wait till the socket has sent the <PLACE_HOLDER>,barrier . await ( __num__ @$ time unit . milliseconds ) ; return sent packet ;,socket sent
if the action bar did n't provide an action <PLACE_HOLDER> @$ start the emulated window one,if ( m action mode == null ) { m action mode = start support action mode from window ( wrapped callback ) ; },bar provide
we can only operate on encodings map because ` fr ` could not have target <PLACE_HOLDER> at all,double global mean for target class = calculate prior mean ( encoding map ) ;,fr have
remove & save client return <PLACE_HOLDER> and insert the protocol header and service name @$ then rewrap <PLACE_HOLDER> .,z frame client = msg . unwrap ( ) ; msg . add first ( service frame . duplicate ( ) ) ; msg . add first ( mdp . c_client . new frame ( ) ) ; msg . wrap ( client ) ; msg . send ( socket ) ;,& save
a sync vector only has one <PLACE_HOLDER> if not recovered from persistence @$ no valid version for the vector 's owner,if ( ! my id . is disk store id ( ) ) { return true ; },vector has
list views will use the bright <PLACE_HOLDER> @$ but buttons use the medium <PLACE_HOLDER> .,last view . set background resource ( last light ? ( has buttons ? bottom medium : bottom bright ) : bottom dark ) ; full bright = a . get resource id ( r . styleable . alert dialog_full bright @$ full bright ) ; full dark = a . get resource id ( r . styleable . alert dialog_full dark @$ full dark ) ; last view . set background resource ( last light ? full bright : full dark ) ;,views use
we need the table and region to determine if this is from a mob region we do n't need to worry about hfilelink back references @$ because the hfilelink cleaner will retain <PLACE_HOLDER> .,path family = file . get parent ( ) ; path region = family . get parent ( ) ; path table = region . get parent ( ) ; table name table name = fs utils . get table name ( table ) ; string mob region = mob_regions . get ( table name ) ; if ( mob region == null ) { string tmp = mob utils . get mob region info ( table name ) . get encoded name ( ) ; if ( tmp == null ) { log . error ( __str__ @$ table name ) ; return false ; } mob region = mob_regions . put if absent ( table name @$ tmp ) ; if ( mob region == null,cleaner retain
limiter should delay <PLACE_HOLDER> by one buffer @$ and there should almost no different in output v.s . input,for ( int i = __num__ ; i < n1a . length ; i ++ ) { if ( math . abs ( out1a [ i ] ) > __num__ ) throw new exception ( __str__ ) ; if ( math . abs ( out2a [ i ] ) > __num__ ) throw new exception ( __str__ ) ; } synth . close ( ) ;,limiter delay
box drawings light horizontal @$ so box drawings light <PLACE_HOLDER> @$ so box drawings light down and right @$ so box drawings light down and left @$ so box drawings light up and left @$ so box drawings light up and right @$ so box drawings light <PLACE_HOLDER> and right @$ so box drawings light down and horizontal @$ so box drawings light <PLACE_HOLDER>,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,drawings light
one property block can only contains at most <PLACE_HOLDER> x 8 byte parts @$ one for header and 3 for coordinates,if ( coordinate . length > geometry type . get max supported dimensions ( ) ) { throw new unsupported operation exception ( __str__ + geometry type . get max supported dimensions ( ) + __str__ ) ; },block contains
minimal delay until the java process object <PLACE_HOLDER> that the process is gone this will not let the test fail predictably if the process is actually in fact going away @$ but it would create frequent failures . not ideal @$ but the best we can do without severely prolonging the test,thread . sleep ( __num__ ) ;,process object
at this point there should be : 1 k flow <PLACE_HOLDER> in the active queue 9@$001 flow <PLACE_HOLDER> in the swap queue 10 k flow <PLACE_HOLDER> swapped to disk,for ( int i = __num__ ; i < __num__ ; i ++ ) { final flow file record flow file = queue . poll ( exp ) ; assert not null ( flow file ) ; assert equals ( __num__ @$ queue . get queue diagnostics ( ) . get local queue partition diagnostics ( ) . get unacknowledged queue size ( ) . get object count ( ) ) ; assert equals ( __num__ @$ queue . get queue diagnostics ( ) . get local queue partition diagnostics ( ) . get unacknowledged queue size ( ) . get byte count ( ) ) ; queue . acknowledge ( collections . singleton ( flow file ) ) ; assert equals ( __num__ @$ queue . get,files files
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
something crashed . stop the <PLACE_HOLDER> .,common utils . sleep ms ( __num__ ) ;,something stop
has next call above sets <PLACE_HOLDER> up,return m_inner itr . next ( ) ;,call sets
validate requested <PLACE_HOLDER>,for ( extension config req ext : request . get extensions ( ) ) { if ( ! extension registry . is available ( req ext . get name ( ) ) ) { throw new illegal argument exception ( __str__ + req ext . get name ( ) + __str__ ) ; } } if ( log . is debug enabled ( ) ) log . debug ( __str__ @$ websocket @$ to uri ) ; init ( ) ; web socket upgrade request ws req = new web socket upgrade request ( this @$ http client @$ request ) ; ws req . set upgrade listener ( upgrade listener ) ; return ws req . send async ( ) ;,validate requested
if a vm bug has caused the <PLACE_HOLDER> to get freed without the reference getting cleared @$ looking it up @$ assigning it to a local and doing a gc should cause some sort of exception .,try { string s = string ref . get ( ) ; system . gc ( ) ; test object finalized = true ; } catch ( throwable t ) { error = new assertion failed error ( __str__ + t + __str__ ) ; },bug caused
iterate through all the transitions until first millis is reached . use the name key and savings for whatever rule reaches the <PLACE_HOLDER> .,long millis = long . min_value ; int save millis = __num__ ; transition first = null ; transition next ; while ( ( next = next transition ( millis @$ save millis ) ) != null ) { millis = next . get millis ( ) ; if ( millis == first millis ) { first = new transition ( first millis @$ next ) ; break ; } if ( millis > first millis ) { if ( first == null ) { for ( rule rule : copy ) { if ( rule . get save millis ( ) == __num__ ) { first = new transition ( first millis @$ rule @$ i standard offset ) ; break ; } } } if (,key reaches
execute binding . notify <PLACE_HOLDER> that the object is now unlocked,fire property change ( __str__ @$ false @$ true ) ;,binding notify
dispatching to empty statement will not call back <PLACE_HOLDER>or @$ must call our <PLACE_HOLDER> empty statement explicitly,if ( else block instanceof empty statement ) { visit empty statement ( ( empty statement ) else block ) ; } else { else block . visit ( this ) ; },statement call
mark known lc makes <PLACE_HOLDER> 1 longer,int sb len = ( knownlc words != null ) ? len + __num__ : len ;,lc makes
see if our node matches the given key <PLACE_HOLDER> according to the match attribute on xsl : key .,x path match expr = kd . get match ( ) ; double score = match expr . get match score ( xctxt @$ test node ) ; if ( score == kd . get match ( ) . match_score_none ) continue ; return dtm iterator . filter_accept ;,node matches
the device is receiving <PLACE_HOLDER> from the wireless charger . update the rest position asynchronously .,if ( is powered && plug type == battery manager . battery_plugged_wireless ) { m powered wirelessly = true ; m must update rest position = true ; start detection locked ( ) ; } else { m powered wirelessly = false ; if ( m at rest ) { if ( plug type != __num__ && plug type != battery manager . battery_plugged_wireless ) { m must update rest position = false ; clear at rest locked ( ) ; } else { start detection locked ( ) ; } } },device receiving
logger tree should not influnce <PLACE_HOLDER>,assert true ( __str__ + sizea + __str__ + sizeb @$ ( sizea - sizeb ) < __num__ ) ;,tree influnce
exception is thrown if the context is a receiver restricted context object . as receiver restricted context is not public @$ the context type can not be checked before calling register receiver . the most likely scenario in which the exception would be thrown would be when a broadcast receiver creates a <PLACE_HOLDER> to show the user .,m screen off receiver = null ;,receiver creates
if the dto does n't have an <PLACE_HOLDER> for the metric @$ add with a value of 0 .,final map < string @$ long > dto metrics = snapshot dto . get status metrics ( ) ; final string field = descriptor . get field ( ) ; if ( ! dto metrics . contains key ( field ) ) { dto metrics . put ( field @$ __num__ ) ; },dto have
enter the started <PLACE_HOLDER> .,final completable future < string > start future = start stop . start ( true ) ; assert that ( start future . join ( ) ) . is equal to ( __str__ ) ;,the started
this will disable stack trace <PLACE_HOLDER> inside jul . if someone wants location info @$ they can use their own formatter or use a different logging framework like sl 4 j @$ or log 4 j,record . set source class name ( null ) ; record . set parameters ( params ) ; logger . log ( record ) ;,stack trace
read the keep <PLACE_HOLDER> .,while ( true ) { read next word ( __str__ + configuration constants . class_keyword + __str__ + java constants . acc_interface + __str__ + java constants . acc_enum + __str__ @$ false @$ true ) ; if ( ! configuration constants . argument_separator_keyword . equals ( next word ) ) { break ; } read next word ( __str__ + configuration constants . allow_shrinking_suboption + __str__ + configuration constants . allow_optimization_suboption + __str__ + configuration constants . allow_obfuscation_suboption + __str__ ) ; if ( configuration constants . include_descriptor_classes_suboption . starts with ( next word ) ) { mark descriptor classes = true ; } else if ( configuration constants . allow_shrinking_suboption . starts with ( next word ) ) { allow shrinking = true ; } else,the keep
check that method under test throws <PLACE_HOLDER>,try { epki . get key spec ( ( key ) null @$ ( provider ) null ) ; fail ( get name ( ) + __str__ ) ; } catch ( null pointer exception ok ) { },method throws
the process will throw an escalation <PLACE_HOLDER> @$ which is caught and escalated by a user org.flowable.task.service.task,assert equals ( __num__ @$ task service . create task query ( ) . task definition key ( __str__ ) . count ( ) ) ; task task = task service . create task query ( ) . single result ( ) ; assert equals ( __str__ @$ task . get name ( ) ) ;,process throw
so let 's start thge <PLACE_HOLDER>,this . start block ( ) ; try { return get result set ( ) ; } catch ( sql exception e ) { throw new dbc exception ( e @$ connection . get data source ( ) ) ; } finally { this . end block ( ) ; },"s" start
two labels : positive or negative because we are dealing with reviews of different lengths and only one output at the final time step : use padding arrays mask arrays contain <PLACE_HOLDER> if data is present at that time step for that example @$ or 0 if data is just padding,ind array features mask = nd4j . zeros ( reviews . size ( ) @$ max length ) ; ind array labels mask = nd4j . zeros ( reviews . size ( ) @$ max length ) ; int [ ] temp = new int [ __num__ ] ; for ( int i = __num__ ; i < reviews . size ( ) ; i ++ ) { list < string > tokens = all tokens . get ( i ) ; temp [ __num__ ] = i ; for ( int j = __num__ ; j < tokens . size ( ) && j < max length ; j ++ ) { string token = tokens . get ( j ) ; ind array vector = word,arrays contain
if cluster management service is enabled and user did not specify a member id @$ then we will find the applicable members based on the what <PLACE_HOLDER> this region is on,if ( cc service != null && member name orid == null ) { region name = get valid region name ( region path @$ cms ) ; set < string > calculated groups = get groups containing region ( cms @$ region name ) ; if ( calculated groups . is empty ( ) ) { return result model . create error ( __str__ + region name + __str__ ) ; } if ( groups != null && ! calculated groups . contains all ( arrays . as list ( groups ) ) ) { return result model . create error ( __str__ + region name + __str__ ) ; } if ( groups == null ) { groups = calculated groups . stream ( ) .,region on
the bounds should equal the defined app <PLACE_HOLDER> and height,assert equals ( info . app width @$ app bounds . width ( ) ) ; assert equals ( info . app height @$ app bounds . height ( ) ) ;,bounds equal
we can not determine <PLACE_HOLDER> method is the most specific because one parameter of the first candidate was more specific and another parameter of the second candidate was more specific .,if ( best match != null && ! potential match . equals ( best match ) ) { return null ; } else { best match = potential match ; },parameter specific
the only way we can find out if we need to quote the properties is by checking an object <PLACE_HOLDER> that we 've constructed .,if ( object name . is domain pattern ( ) ) { domain = object name . quote ( domain ) ; } if ( object name . is property value pattern ( __str__ ) ) { properties . put ( __str__ @$ object name . quote ( name ) ) ; } if ( object name . is property value pattern ( __str__ ) ) { properties . put ( __str__ @$ object name . quote ( type ) ) ; } object name = new object name ( domain @$ properties ) ; return object name ;,way checking
we assume that only a switch to available and chat indicates user <PLACE_HOLDER> since other mode changes could be also a result of some sort of automatism,reset idle time ( ) ; break ; default : break ;,switch indicates
this one will overflow <PLACE_HOLDER> .,wseg = buf . offer ( size ) ; bbuf = wseg . buffer ( ) ; bbuf . put long ( __num__ ) ; wseg . release ( ) ;,one overflow
since the source ca n't be split @$ the first subtask index will read <PLACE_HOLDER>,if ( should have readers ) { mockito . when ( mock . get index of this subtask ( ) ) . then return ( __num__ ) ; } else { mockito . when ( mock . get index of this subtask ( ) ) . then return ( parallelism - __num__ ) ; },index read
even if ' v ' is a dom node @$ it always derive from object @$ so the get bean info <PLACE_HOLDER> bean info for object,if ( bi . jaxb type == object . class && dom handler != null ) w . write dom ( v @$ dom handler @$ o @$ field name ) ; else bi . serialize root ( v @$ w ) ;,info get
verify that thread identifier does not remove <PLACE_HOLDER> as data is lying,assert not null ( __str__ @$ events map . get ( thread id ) ) ;,identifier remove
register an error handler which detects validation <PLACE_HOLDER>,if ( is validating ( ) ) { result . set error handler ( new default handler ( ) { @ override public void error ( sax parse exception ex ) throws sax exception { throw ex ; } } ) ; },which detects
in each case get file link status returns the same file status as get file status since we 're not calling it on a link and file status <PLACE_HOLDER> are compared by path .,assert equals ( wrapper . get file status ( file ) @$ wrapper . get file link status ( file ) ) ; assert equals ( wrapper . get file status ( file via link ) @$ wrapper . get file link status ( file via link ) ) ; assert equals ( wrapper . get file status ( file via link ) @$ wrapper . get file link status ( file ) ) ;,status file
stopship do proper <PLACE_HOLDER> in split user mode,return code_ok ; if ( ! m injector . user manager is split system user ( ) ) { if ( device owner user id != user handle . user_system ) { return code_not_system_user ; } if ( has user setup completed ( user handle . user_system ) ) { return code_user_setup_completed ; } } else { },stopship do
entry has no <PLACE_HOLDER>,if ( attributes == null ) { return null ; } array list < certificate [ ] > cert chains = new array list < certificate [ ] > ( ) ; iterator < map . entry < string @$ hash map < string @$ attributes > > > it = signatures . entry set ( ) . iterator ( ) ; while ( it . has next ( ) ) { map . entry < string @$ hash map < string @$ attributes > > entry = it . next ( ) ; hash map < string @$ attributes > hm = entry . get value ( ) ; if ( hm . get ( name ) != null ) { string signature file = entry .,entry has
disable all this stuff which kills <PLACE_HOLDER>,hmd tracked device pose reference . set auto read ( false ) ; hmd tracked device pose reference . set auto write ( false ) ; hmd tracked device pose reference . set auto synch ( false ) ; for ( int i = __num__ ; i < j openvr library . k_un max tracked device count ; i ++ ) { hmd tracked device poses [ i ] . set auto read ( false ) ; hmd tracked device poses [ i ] . set auto write ( false ) ; hmd tracked device poses [ i ] . set auto synch ( false ) ; },which kills
superclass does not use <PLACE_HOLDER>,return sample result . class . equals ( arg0 ) ;,superclass use
the parsed <PLACE_HOLDER> represents the <PLACE_HOLDER> @$ the process definitions @$ and the bpmn resource @$ parse @$ and model associated with each process definition .,parsed deployment parsed deployment = parsed deployment builder factory . get builder for deployment and settings ( deployment @$ deployment settings ) . build ( ) ; bpmn deployment helper . verify process definitions do not share keys ( parsed deployment . get all process definitions ( ) ) ; bpmn deployment helper . copy deployment values to process definitions ( parsed deployment . get deployment ( ) @$ parsed deployment . get all process definitions ( ) ) ; bpmn deployment helper . set resource names on process definitions ( parsed deployment ) ;,deployment represents
load anyway since listeners should not throw <PLACE_HOLDER>,logger . error ( __str__ @$ e ) ;,listeners throw
this should be impossible given the can rewrite hudson war <PLACE_HOLDER> @$ but let 's be defensive,if ( dest == null ) throw new io exception ( __str__ ) ;,the rewrite
mark all inferred concepts that are required for the insert for persistence explicitly can avoid this potentially expensive <PLACE_HOLDER> if there are n't any inferred concepts to start with,stream < concept map > explicitly persisted = inserted . peek ( concept map -> { if ( transaction cache . get inferred instances ( ) . find any ( ) . is present ( ) ) { mark concepts for persistence ( concept map . concepts ( ) ) ; } } ) ;,concepts avoid
doubling from the expected size will cause exactly one <PLACE_HOLDER> @$ except near minimum capacity .,if ( size > __num__ ) { grow using put all ( m @$ size ) ; assert capacity ( m @$ __num__ * initial capacity ) ; },doubling cause
failed to create <PLACE_HOLDER> registry ...,system . out . println ( __str__ + __str__ + port ) ;,failed create
on windows @$ java must keep the environment <PLACE_HOLDER>ed . order is random on unix @$ so this test does the <PLACE_HOLDER> .,if ( ! windows . is ( ) ) output = sort by lines windowsly ( output ) ; equal ( output @$ expected ) ;,test does
one dimension got <PLACE_HOLDER> @$ but not the other . recheck time !,if ( vertical scroll needed != horizontal scroll needed ) { if ( ! vertical scroll needed && horizontal scroll needed ) { vertical scroll needed = scroll content height > table wrapper height + widget util . pixel_epsilon - header . get height of section ( ) - footer . get height of section ( ) - horizontal scrollbar . get scrollbar thickness ( ) ; } else { horizontal scroll needed = scroll content width > table wrapper width + widget util . pixel_epsilon - vertical scrollbar . get scrollbar thickness ( ) ; } },dimension got
note that we ca n't use 'instanceof ' because this class has a <PLACE_HOLDER> .,if ( obj . get class ( ) != different real path file value with stored chain . class ) { return false ; } different real path file value with stored chain other = ( different real path file value with stored chain ) obj ; return real rooted path . equals ( other . real rooted path ) && real file state value . equals ( other . real file state value ) && logical chain during resolution . equals ( other . logical chain during resolution ) ;,note has
purge on empty with reset clears <PLACE_HOLDER> .,assert equals ( __num__ @$ queue view . get queue size ( ) ) ; assert equals ( __num__ @$ queue view . get enqueue count ( ) ) ; assert equals ( __num__ @$ queue view . get dequeue count ( ) ) ; produce ( __num__ ) ; consume ( __num__ ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ; assert equals ( __num__ @$ queue view . get enqueue count ( ) ) ; assert equals ( __num__ @$ queue view . get dequeue count ( ) ) ; execute purge ( __str__ + get destination name ( ) ) ;,purge clears
this kinda assumes the right <PLACE_HOLDER>,for ( plan fragment f : select stmt . get fragments ( ) ) { if ( i != __num__ ) { select bottom frag = f ; } i ++ ; },kinda assumes
interrupted exception can not be thrown by runnable.run @$ so we must wrap it . interrupts can be caught by both the evaluator and the abstract queue visitor . the former will unwrap the <PLACE_HOLDER> and propagate it as is ; the latter will throw a new <PLACE_HOLDER> .,throw scheduler exception . of interruption ( ie @$ sky key ) ;,former unwrap
the first update is capturing the start <PLACE_HOLDER> @$ the second update is capturing the error,verify ( workspace dao @$ times ( __num__ ) ) . update ( workspace captor . capture ( ) ) ; workspace ws = workspace captor . get all values ( ) . get ( __num__ ) ; assert not null ( ws . get attributes ( ) . get ( stopped_attribute_name ) ) ; assert true ( boolean . value of ( ws . get attributes ( ) . get ( stopped_abnormally_attribute_name ) ) ) ; assert equals ( ws . get attributes ( ) . get ( error_message_attribute_name ) @$ __str__ ) ;,update capturing
let gc do its <PLACE_HOLDER>,element data [ k ] = null ;,gc do
restart so that new config takes <PLACE_HOLDER>,after class ( ) ; before class ( ) ; final table name table name = table name . value of ( name . get method name ( ) ) ; try ( admin admin = connection . get admin ( ) ; region locator rl = connection . get region locator ( table name ) ) { util . create table ( table name @$ __str__ ) ; h region location loc = rl . get all region locations ( ) . get ( __num__ ) ; region info parent = loc . get region ( ) ; long rid = __num__ ; byte [ ] split key = bytes . to bytes ( __str__ ) ; region info splita = region info builder . new builder,config takes
second message will flip the <PLACE_HOLDER> but will be stored before the future task,msg = get message ( __num__ ) ; messages [ __num__ ] = msg ; msg . set memory usage ( system usage . get memory usage ( ) ) ; msg . get message id ( ) . set future or sequence long ( __num__ ) ; under test . add message last ( msg ) ; assert true ( __str__ @$ ! under test . is cache enabled ( ) ) ; assert equals ( __str__ @$ __num__ @$ queue message store . batch . get ( ) ) ; int dequeue count = __num__ ; under test . set max batch size ( __num__ ) ; under test . reset ( ) ; while ( under test . has next ( ) && dequeue count,message flip
let 's give a namespace to our application window . this way @$ if someone uses the same <PLACE_HOLDER> for different applications @$ we do n't get unwanted style conflicts .,listing . set column alignment ( __str__ @$ table . align_right ) ;,someone uses
use jni handle <PLACE_HOLDER> as id,write objectid ( get address value ( handle addr ) ) ;,jni handle
only the owning user can change the <PLACE_HOLDER> .,if ( owning user id != calling user id ) { return false ; },user change
this method exists because heap is the place clients should ask this <PLACE_HOLDER> @$ and to aggregate all the reasons allocation might be disallowed .,return no allocation verifier . is active ( ) || gc impl . collection in progress . get state ( ) ;,clients ask
os gi class loaders must implement bundle <PLACE_HOLDER>,final class loader class loader = info . get class loader ( ) ; settings . put ( org . hibernate . cfg . available settings . scanner @$ new osgi scanner ( ( ( bundle reference ) class loader ) . get bundle ( ) ) ) ; osgi class loader . add class loader ( class loader ) ; final class loader prevcl = thread . current thread ( ) . get context class loader ( ) ; try { thread . current thread ( ) . set context class loader ( class loader ) ; return bootstrap . get entity manager factory builder ( info @$ settings @$ new os gi class loader service impl ( osgi class loader @$ osgi service util ) ),loaders implement
toolbar buttons pass a null <PLACE_HOLDER> to this method . tool bar or function compare panel .,if ( is toolbar button action || function comparison panel . is ancestor of ( source component ) ) { listing code comparison panel dual listing panel = function comparison panel . get dual listing panel ( ) ; boolean is showing dual listing = ( dual listing panel != null ) && dual listing panel . is visible ( ) ; boolean source isa dual field panel = is showing dual listing && dual listing panel . is ancestor of ( source component ) && ( source component instanceof field panel ) ; listing panel listing panel = null ; if ( source isa dual field panel ) { listing panel = dual listing panel . get listing panel ( ( field panel ) source component ),buttons pass
public first set @$ only the non durable sub should get <PLACE_HOLDER> .,publish to topic ( session @$ topic ) ; log . debug ( __str__ ) ;,set get
now turn back on and get data . here we also get the previously sent <PLACE_HOLDER> of urgent data as it is still in the urgent buffer,received string = new string ( my bytes @$ __num__ @$ total bytes read ) ;,previously sent
at this point we are ready to add another rowset to 'this ' object check the size of vec join type and vec row <PLACE_HOLDER> in join,vec row sets injoin . add ( c rowset ) ;,size join
note to translators : the substitution text is the name of a function . a literal string here means a constant string <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note means
update already notified <PLACE_HOLDER> .,synchronized ( this ) { if ( observed attribute != null && observed attribute . equals ( attribute ) ) return ; observed attribute = attribute ; cleanup is complex type attribute ( ) ; int index = __num__ ; for ( observed object o : observed objects ) { reset already notified ( o @$ index ++ @$ observed_attribute_error_notified | observed_attribute_type_error_notified ) ; } },update notified
the content provider does not support canonical <PLACE_HOLDER> so we backup the default,if ( canonical sound == null ) { return settings . system . default_notification_uri ; },provider support
entities now prefers shorted <PLACE_HOLDER> if aliased,assert equals ( __str__ @$ custom out ) ;,entities prefers
fetch the multi auto complete text <PLACE_HOLDER> and set an adapter and tokenizer,multi auto complete text view mactv = find view by id ( r . id . widgets_multiautocompletetextview ) ; mactv . set tokenizer ( new multi auto complete text view . comma tokenizer ( ) ) ; mactv . set adapter ( new array adapter < > ( this @$ android . r . layout . simple_dropdown_item_1line @$ cheeses . s cheese strings ) ) ;,auto complete
order matters @$ as the cluster <PLACE_HOLDER> modifies the event bus <PLACE_HOLDER> .,if ( allow clustering ) { set event bus options ( conf @$ options ) ; initialize cluster options ( conf @$ options ) ; },options modifies
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,scheduler captures
no restrictions to default <PLACE_HOLDER> or vr 2 d <PLACE_HOLDER> .,if ( display id == default_display || ( display id != invalid_display && display id == m service . m vr2d display id ) ) { return true ; },restrictions display
note to translators : an element in the stylesheet requires a particular <PLACE_HOLDER> named by the substitution text @$ but that <PLACE_HOLDER> was not specified in the stylesheet .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note requires
allocation of array may have caused <PLACE_HOLDER> @$ which may have caused additional entries to go stale . removing these entries from the reference queue will make them eligible for reclamation .,while ( queue . poll ( ) != null ) ;,which caused
the original dsl test returns a <PLACE_HOLDER> while the exec model returns an integer,assert equals ( ( ( number ) results . iterator ( ) . next ( ) . get value ( ) ) . int value ( ) @$ __num__ ) ;,model returns
display a toast message indicate the selected <PLACE_HOLDER>,image view . set on click listener ( new on click listener ( ) { @ override public void on click ( view v ) { toast . make text ( get activity ( ) @$ m pos @$ toast . length_short ) . show ( ) ; } } ) ;,message indicate
all locks clients should be stopped at this point @$ and all all locks should be released because none of the clients entered the prepare <PLACE_HOLDER>,lock count visitor lock count visitor = new lock count visitor ( ) ; locks . accept ( lock count visitor ) ; assert equals ( __num__ @$ lock count visitor . get lock count ( ) ) ;,none entered
credentials were verified . verify that the principal has all allowed <PLACE_HOLDER> for authentication,if ( ! has allowed authentication rules ( info . get principals ( ) @$ ldap context factory ) ) { throw new naming exception ( __str__ ) ; } return info ;,principal allowed
no name @$ kids carry <PLACE_HOLDER>,break ;,name carry
setters also accept unknown enum value <PLACE_HOLDER> .,builder . set field ( optional nested enum field @$ unknown6543 ) ; builder . set repeated field ( repeated nested enum field @$ __num__ @$ unknown4321 ) ; builder . set repeated field ( packed nested enum field @$ __num__ @$ unknown5432 ) ; message = builder . build ( ) ;,setters accept
have the factory build the <PLACE_HOLDER>,final object object = this . expression object factory . build object ( this . context @$ name ) ;,factory build
request to allocate two reduce priority <PLACE_HOLDER>,final string [ ] locations = new string [ ] { host } ; allocator . send request ( create request ( job id @$ __num__ @$ resource . new instance ( __num__ @$ __num__ ) @$ locations @$ false @$ true ) ) ; allocator . schedule all reduces ( ) ; allocator . make remote request ( ) ; nm . node heartbeat ( true ) ; rm . drain events ( ) ; allocator . send request ( create request ( job id @$ __num__ @$ resource . new instance ( __num__ @$ __num__ ) @$ locations @$ false @$ false ) ) ; int assigned container ; for ( assigned container = __num__ ; assigned container < __num__ ; ) { assigned container +=,two reduce
step 3 : determine final launch bounds based on resolved windowing mode and activity requested orientation . we set bounds to empty for fullscreen mode and keep bounds as is for all other windowing modes that 's not freeform mode . one can read <PLACE_HOLDER> in relevant methods to further understand this step . we skip making adjustments if the params are fully resolved,final int resolved mode = ( launch mode != windowing_mode_undefined ) ? launch mode : display . get windowing mode ( ) ; if ( fully resolved current param ) { if ( resolved mode == windowing_mode_freeform ) { if ( current params . m preferred display id != display id ) { adjust bounds to fit in display ( display @$ out params . m bounds ) ; } adjust bounds to avoid conflict in display ( display @$ out params . m bounds ) ; } } else { if ( source != null && source . in freeform windowing mode ( ) && resolved mode == windowing_mode_freeform && out params . m bounds . is empty ( ) && source . get display id (,one read
variable column table models have default <PLACE_HOLDER> and 'found ' <PLACE_HOLDER> . we only want to create a key based upon the default <PLACE_HOLDER>,return variable table model . get default column count ( ) ;,models have
all four static methods must get distinct <PLACE_HOLDER> @$ so that moo.a resolves correctly to foo.a,test ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,methods get
if the object is externalizable @$ call write external . else do serializable <PLACE_HOLDER> .,if ( current class desc . is externalizable ( ) ) { orb stream . write_octet ( stream format version ) ; externalizable ext = ( externalizable ) obj ; ext . write external ( this ) ; } else { if ( current class desc . for class ( ) . get name ( ) . equals ( __str__ ) ) { this . writeutf ( ( string ) obj ) ; return ; } int stack mark = class desc stack . size ( ) ; try { object stream class next ; while ( ( next = current class desc . get superclass ( ) ) != null ) { class desc stack . push ( current class desc ) ; current class desc =,external do
after the initial data encryption <PLACE_HOLDER> expires @$ <PLACE_HOLDER> manager should regenerate a valid data encryption <PLACE_HOLDER> using the current block <PLACE_HOLDER> .,final data encryption key dek after expiration = key manager . new data encryption key ( ) ; assert not equals ( __str__ @$ dek @$ dek after expiration ) ; assert true ( __str__ @$ dek after expiration . expiry date > fake timer . now ( ) ) ;,manager regenerate
start the delete procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new delete table procedure ( proc exec . get environment ( ) @$ table name ) ) ;,procedure kill
only one write <PLACE_HOLDER> at a time @$ but there is a chance two writers race after commit : thus synchronize,synchronized ( tx commit count lock ) { commit count ++ ; if ( debug tx write ) { system . out . println ( __str__ + commit count + __str__ + ( entity type ids affected != null ? entity type ids affected . length : __num__ ) ) ; } },one write
the project manager maintains the <PLACE_HOLDER> of the projects with the most recent being first in the list,for ( url project view : recent views ) { string url path = ghidraurl . get display string ( project view ) ; docking action action = new recent view plugin action ( url path ) ; reopen views list . add ( new view info ( action @$ project view ) ) ; tool . add action ( action ) ; },manager maintains
no movie @$ so create <PLACE_HOLDER>,if ( movie == null ) { movie = new philm movie ( ) ; },movie create
reading the merged bag gets both the <PLACE_HOLDER>,assert that ( bag1 . read ( ) @$ contains in any order ( __str__ @$ __str__ @$ __str__ ) ) ; assert that ( bag2 . read ( ) @$ matchers . empty iterable ( ) ) ;,bag gets
verify the successful flow file has the expected <PLACE_HOLDER>,final mock flow file mock flow file = test runner . get flow files for relationship ( putorc . rel_success ) . get ( __num__ ) ; mock flow file . assert attribute equals ( putorc . absolute_hdfs_path_attribute @$ orc file . get parent ( ) . to string ( ) ) ; mock flow file . assert attribute equals ( core attributes . filename . key ( ) @$ filename ) ; mock flow file . assert attribute equals ( putorc . record_count_attr @$ __str__ ) ;,file has
broken glass fish @$ which call modify handshake before get endpoint <PLACE_HOLDER> !,if ( end point . get ( ) == null ) { h request . set ( request ) ; } else { end point . get ( ) . handshake request ( request ) ; end point . set ( null ) ; },fish get
dns ur is never have a dns <PLACE_HOLDER>,if ( curi . getuuri ( ) . get scheme ( ) . equals ( __str__ ) ) { curi . set prerequisite ( true ) ; return false ; } else if ( curi . getuuri ( ) . get scheme ( ) . equals ( __str__ ) ) { return false ; },ur have
check that start captive portal app sends the expected <PLACE_HOLDER> to network monitor .,m cm . start captive portal app ( wifi network ) ; wait for idle ( ) ; verify ( m wi fi network agent . m network monitor ) . launch captive portal app ( ) ;,check sends
db must have <PLACE_HOLDER> . older versions of anki droid did n't create them for new collections .,fix integrity progress ( progress callback @$ current task ++ @$ total tasks ) ; int ixs = m db . query scalar ( __str__ ) ; if ( ixs < __num__ ) { problems . add ( __str__ ) ; storage . add indices ( m db ) ; } m db . get database ( ) . end transaction ( ) ;,db have
schedule delayed <PLACE_HOLDER> killing if graceful stopping will be not finished within timeout .,executor . schedule ( new runnable ( ) { @ override public void run ( ) { if ( state ( name ) == ignite state . started ) { u . error ( null @$ __str__ + timeout ms + __str__ ) ; runtime . get runtime ( ) . halt ( ignition . kill_exit_code ) ; } } } @$ timeout ms @$ time unit . milliseconds ) ;,schedule delayed
slow down the producer @$ this will make the queue mostly empty encouraging visibility <PLACE_HOLDER> .,thread . yield ( ) ;,queue encouraging
check that all requests had set response or set <PLACE_HOLDER> invoked in case 'map response to requests ' was implemented poorly,exception e = null ; for ( collapsed request < response type @$ request argument type > request : shard requests ) { try { e = ( ( collapsed request subject < response type @$ request argument type > ) request ) . set exception if response not received ( e @$ __str__ + command collapser . get collapser key ( ) . name ( ) + __str__ ) ; } catch ( illegal state exception e2 ) { logger . debug ( __str__ @$ e2 ) ; } },requests set
close the fd via the parent stream 's close <PLACE_HOLDER> . the parent will reinvoke our close <PLACE_HOLDER> @$ which is defined in the superclass abstract interruptible channel @$ but the is open logic in that <PLACE_HOLDER> will prevent this <PLACE_HOLDER> from being reinvoked .,if ( parent != null ) { ( ( java . io . closeable ) parent ) . close ( ) ; } else { nd . close ( fd ) ; },method prevent
every one of our providers will call this <PLACE_HOLDER> @$ so only execute the logic once .,if ( initialization state != initialization state . uninitialized ) { return initialization state != initialization state . has_errors ; },one call
need to talk about include all <PLACE_HOLDER> .,inc tag list . set prototype cell value ( __str__ ) ; inc tag scroller = new j scroll pane ( inc tag list ) ; inc tag scroller . set horizontal scroll bar policy ( javax . swing . j scroll pane . horizontal_scrollbar_as_needed ) ; inc tag scroller . set vertical scroll bar policy ( javax . swing . j scroll pane . vertical_scrollbar_as_needed ) ;,need include
response account <PLACE_HOLDER>,m ams . add account as user ( m mock account manager response @$ null @$ __str__ @$ null @$ true @$ null @$ user handle . user_system ) ;,response account
there is one adjp unary rewrite to ad but otherwise all have <PLACE_HOLDER> or adjp,non terminal info . put ( __str__ @$ new string [ ] [ ] { { right @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } ) ;,all have
app 1 gets all <PLACE_HOLDER> in default partition,assert . assert equals ( __num__ @$ scheduler node2 . get num containers ( ) ) ;,app gets
let explicit command line override the <PLACE_HOLDER> .,m_port = ports . next client ( ) ; m_admin port = ports . next admin ( ) ; m_internal port = ports . next ( ) ; m_zk interface = __str__ + ports . next ( ) ;,line override
upadate <PLACE_HOLDER> resource should n't be called before start <PLACE_HOLDER> @$ otherwise @$ node manager can not find the <PLACE_HOLDER>,try { client . update container resource ( container ) ; fail ( __str__ ) ; } catch ( yarn exception e ) { assert true ( __str__ @$ e . get message ( ) . contains ( __str__ ) ) ; },resource find
this should be a relatively uncommon case @$ window for this happening is a few milliseconds when an admin explicitly creates a <PLACE_HOLDER> @$ after the index has been populated . we can improve this later on by replicating the <PLACE_HOLDER> validation logic down here @$ or rethinking where we validate <PLACE_HOLDER>s . for now @$ we just kill these transactions .,throw new transaction failure exception ( status . transaction . constraints changed @$ __str__ + __str__ + __str__ @$ latest constraint introducing tx @$ last committed tx when transaction started ) ;,admin creates
else : no new winner @$ ergo no reason to send <PLACE_HOLDER> to neighbors,messenger . send message ( incident message scope @$ shortest distance seen on this iteration ) ;,reason send
style rules should always have the lowest <PLACE_HOLDER> .,return - __num__ ;,rules have
if the stream is sorted then it should also be <PLACE_HOLDER>ed so the following will also preserve the sort <PLACE_HOLDER>,return new reference pipeline . stateful op < t @$ t > ( upstream @$ stream shape . reference @$ stream op flag . is_distinct | stream op flag . not_sized ) { < p_in > node < t > reduce ( pipeline helper < t > helper @$ spliterator < p_in > spliterator ) { terminal op < t @$ linked hash set < t > > reduce op = reduce ops . < t @$ linked hash set < t > > make ref ( linked hash set :: new @$ linked hash set :: add @$ linked hash set :: add all ) ; return nodes . node ( reduce op . evaluate parallel ( helper @$ spliterator ) ) ; } @ override public,following preserve
write 1000 lines and then save @$ this will reduce disk writing frequency and at the same time will keep <PLACE_HOLDER> under control,if ( ( line count % __num__ ) == __num__ ) { output . flush ( ) ; },time keep
this overload has no uncompression <PLACE_HOLDER>,w . write object ( __num__ @$ value ) ;,overload has
invocation in the code under test uses different <PLACE_HOLDER> and should fail immediately this helps with debugging and is essential for mockito strictness,production code . simple method ( mock @$ __num__ ) ;,invocation uses
did n't get m unthemed entries <PLACE_HOLDER> @$ bail out ...,if ( s themed resource cache_m unthemed entries field == null ) { return ; },m unthemed
add an expect <PLACE_HOLDER> to make sure that the item does n't already exist @$ since it 's supposed to be new,if ( get local save behavior ( ) != save behavior . clobber && ! internal expected value assertions . contains key ( field . name ( ) ) && field . get generate strategy ( ) != dynamodb auto generate strategy . always ) { internal expected value assertions . put ( field . name ( ) @$ new expected attribute value ( ) . with exists ( false ) ) ; },an expect
check system properties for the default <PLACE_HOLDER> then use secure settings <PLACE_HOLDER> @$ if any .,string default delay = system properties . get ( __str__ + settings . global . pac_change_delay @$ default_delays ) ; string val = settings . global . get string ( cr @$ settings . global . pac_change_delay ) ; return ( val == null ) ? default delay : val ;,properties use
the user asked for stats to be collected . some stats like number of rows require a <PLACE_HOLDER> of the data however @$ some other stats @$ like number of files @$ do not require a complete <PLACE_HOLDER> update the stats which do not require a complete <PLACE_HOLDER> .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },which require
if relationship label is null @$ then <PLACE_HOLDER> label is mentioned at both ends @$ use the respective end 's <PLACE_HOLDER> label as relationship label,if ( relationship label == null ) { relationship label = get legacy edge label ( entity type @$ attr name ) ; } if ( attribute == null ) { cardinality cardinality = end def . get cardinality ( ) ; boolean is optional = true ; atlas constraint def constraint = null ; if ( cardinality == cardinality . set ) { attr type name = atlas base type def . get array type name ( attr type name ) ; } if ( relationship def . get relationship category ( ) == relationship category . composition ) { if ( end def . get is container ( ) ) { constraint = new atlas constraint def ( constraint_type_owned_ref ) ; } else { is optional,label use
event listener registry defines 3 <PLACE_HOLDER> to register listeners :,event listener registry . add duplication strategy ( new custom duplication strategy ( ) ) ;,listener defines
when opearting with groups . the group must have <PLACE_HOLDER> so changes to take effect . otherwise group will be lost after loggingout,try { this . op set pers presence1 . subscribe ( group @$ this . fixture . userid2 ) ; synchronized ( o ) { o . wait ( __num__ ) ; } } catch ( exception ex ) { fail ( __str__ + group . get group name ( ) + __str__ + ex . get message ( ) ) ; },group have
last file should have 1 <PLACE_HOLDER>,mff = runner . get flow files for relationship ( select hiveql . rel_success ) . get ( __num__ ) ; in = new byte array input stream ( mff . to byte array ( ) ) ; assert equals ( __num__ @$ get number of records from stream ( in ) ) ; mff . assert attribute exists ( __str__ ) ; assert equals ( integer . to string ( __num__ ) @$ mff . get attribute ( __str__ ) ) ; assert equals ( __str__ @$ mff . get attribute ( __str__ ) ) ; runner . clear transfer state ( ) ;,file have
filter does not change the <PLACE_HOLDER> ordering . filter rel does not permute the <PLACE_HOLDER> . all cor vars produced by filter will have the same output positions in the <PLACE_HOLDER> rel .,return register ( rel @$ rel builder . build ( ) @$ frame . old to new outputs @$ frame . cor def outputs ) ;,rel permute
we return the first found <PLACE_HOLDER> .,if ( paths . size ( ) > __num__ ) { final_symlink = paths . get ( __num__ ) + symlink [ symlink . length - __num__ ] ; } else { final_symlink = symlink [ symlink . length - __num__ ] ; },first found
note : this test encodes the <PLACE_HOLDER> that limit spec sorts numbers like strings ; we might want to change this in the future .,assert . assert equals ( immutable list . of ( test rows list . get ( __num__ ) @$ test rows list . get ( __num__ ) ) @$ limit fn . apply ( sequences . simple ( test rows list ) ) . to list ( ) ) ;,test encodes
this estimate will not take into account the <PLACE_HOLDER> saved by inlining the keys .,return versioned stats disklru region entry off heap object key . class ;,estimate take
it does not sound like it makes a lot of sense to fail the getting of the presence status of the specified signin name just because one of the possibly many operation set presence instances has failed . additionally @$ the native counterpart will swallow any java <PLACE_HOLDER> anyway .,if ( t instanceof thread death ) throw ( thread death ) t ; else t . print stack trace ( system . err ) ;,counterpart swallow
this loop has child <PLACE_HOLDER> . loop peeling could explode graph size .,if ( loop . loop ( ) . get children ( ) . size ( ) > __num__ ) { return false ; },loop has
loopback produces empty <PLACE_HOLDER> .,if ( hw addr != null && hw addr . length > __num__ ) { string mac = byte array2 hex string ( hw addr ) ; if ( ! macs . contains ( mac ) ) macs . add ( mac ) ; },loopback produces
ensure that the schema has the virtual <PLACE_HOLDER> added,virtual column provider factory . add built in virtual columns to schema ( schema ) ;,schema has
note : a null search name will match the first pu <PLACE_HOLDER> found,string search name = null ;,name match
other users may already have a <PLACE_HOLDER> on this combining value .,accum = combine fn . create accumulator ( ) ; is cleared = true ;,users have
this object always contains a <PLACE_HOLDER> to the configuration value for the maximum size of the threadpool it only gets applied if allow maximum size to diverge from core size is true,this . maximum pool size = get property ( property prefix @$ key @$ __str__ @$ builder . get maximum size ( ) @$ default_maximum size ) ; this . keep alive time = get property ( property prefix @$ key @$ __str__ @$ builder . get keep alive time minutes ( ) @$ default_keep alive time minutes ) ; this . max queue size = get property ( property prefix @$ key @$ __str__ @$ builder . get max queue size ( ) @$ default_max queue size ) ; this . queue size rejection threshold = get property ( property prefix @$ key @$ __str__ @$ builder . get queue size rejection threshold ( ) @$ default_queue size rejection threshold ) ; this . thread pool rolling,object contains
prevent npe and make the unit test go equals ordinal <PLACE_HOLDER>,other . set indextype ( index type . normal ) ; index meta . set indextype ( index type . normal ) ; assertions . assert not equals ( index meta @$ other ) ; other = new index meta ( ) ; other . set indextype ( index type . normal ) ; index meta . set indextype ( index type . normal ) ; assertions . assert equals ( index meta @$ other ) ;,test go
note to translators : the option name and the parameter name do not need to be translated . only translate the messages in parentheses . note also that leading whitespace in the messages is used to indent the usage information for each option in the english messages . do not translate the <PLACE_HOLDER> : xsltc @$ sax @$ dom and dtm .,return new object [ ] [ ] { { __str__ @$ __str__ } @$ { er_no_curlybrace @$ __str__ } @$ { er_illegal_attribute @$ __str__ } @$ { er_null_sourcenode_applyimports @$ __str__ } @$ { er_cannot_add @$ __str__ } @$ { er_null_sourcenode_handleapplytemplates @$ __str__ } @$ { er_no_name_attrib @$ __str__ } @$ { er_template_not_found @$ __str__ } @$ { er_cant_resolve_name_avt @$ __str__ } @$ { er_requires_attrib @$ __str__ } @$ { er_must_have_test_attrib @$ __str__ } @$ { er_bad_val_on_level_attrib @$ __str__ } @$ { er_processinginstruction_name_cant_be_xml @$ __str__ } @$ { er_processinginstruction_notvalid_ncname @$ __str__ } @$ { er_need_match_attrib @$ __str__ } @$ { er_need_name_or_match_attrib @$ __str__ } @$ { er_cant_resolve_nsprefix @$ __str__ } @$ { er_illegal_value @$ __str__ } @$ { er_no_ownerdoc @$ __str__ } @$ { er_elemtemplateelem_err @$ __str__ } @$,note translate
ensure a timer throws a null pointer <PLACE_HOLDER> if the task is null,t = new timer ( ) ; d = new date ( system . current time millis ( ) + __num__ ) ; try { t . schedule ( null @$ d ) ; fail ( __str__ ) ; } catch ( null pointer exception expected ) { } t . cancel ( ) ;,timer throws
user certificate store @$ does not bypass static <PLACE_HOLDER> .,if ( info . target sdk version <= build . version_codes . m && ! info . is privileged app ( ) ) { builder . add certificates entry ref ( new certificates entry ref ( user certificate source . get instance ( ) @$ false ) ) ; },store bypass
use this setting to improve performance if you know that changes in content do not change the layout <PLACE_HOLDER> of the recycler view,m recycler view . set has fixed size ( true ) ; recycler view . layout manager layout manager = new linear layout manager ( get activity ( ) ) ; m recycler view . set layout manager ( layout manager ) ; m recycler view . set adapter ( adapter ) ; m recycler view . add item decoration ( new spaces item decoration ( quick return utils . dp2px ( get activity ( ) @$ __num__ ) ) ) ; int header height = get resources ( ) . get dimension pixel size ( r . dimen . facebook_header_height ) ; int footer height = get resources ( ) . get dimension pixel size ( r . dimen . facebook_footer_height ) ; int header translation =,changes change
note : scala and wsdl have no <PLACE_HOLDER>,return arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,scala have
end probe phase @$ iterator build side <PLACE_HOLDER> .,collector . collect ( build iter . get row ( ) ) ; while ( build iter . advance next ( ) ) { collector . collect ( build iter . get row ( ) ) ; },iterator build
the highest one bit should be 1 @$ which means the <PLACE_HOLDER> as a number is negative,return word < __num__ ;,which means
<PLACE_HOLDER> these elements using insertion <PLACE_HOLDER>,a [ e1 ] = t ; a [ e2 ] = a [ e1 ] ;,elements using
dispatch the callback with empty arrays which means a <PLACE_HOLDER> .,on request permissions result ( request code @$ new string [ __num__ ] @$ new int [ __num__ ] ) ; return ;,which means
a volt db <PLACE_HOLDER> to avoid using exceptions for flow control .,if ( ! is option ) { if ( ! prefer to throw ) { return unexpected token ( ) ; } throw unexpected token ( ) ; },volt db
user has specified a <PLACE_HOLDER> @$ we can continue ...,if ( connection != null ) { database db = new database ( this @$ connection ) ; db . share variables with ( this ) ; try { db . connect ( parent job . get transaction id ( ) @$ null ) ; string real schemaname = environment substitute ( schemaname ) ; string real tablename = environment substitute ( tablename ) ; if ( db . check table exists ( real tablename ) ) { if ( log . is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ ) + real tablename + base messages . get string ( pkg @$ __str__ ) ) ; } if ( schemaname != null ) { real tablename =,user specified
also append sub 1 file <PLACE_HOLDER>,dfs test util . append file ( hdfs @$ sub1file2 @$ blocksize ) ; hdfs . create snapshot ( dir @$ __str__ ) ; out . close ( ) ;,1 file
add all added <PLACE_HOLDER> to this graph,for ( filter filter : added filters ) { add filter ( filter ) ; },all added
the application has both 64 and 32 bit bundled libraries . we check here that the app declares multi arch support @$ and warn if it does n't . we will be lenient here and record both ab is . the primary will be the <PLACE_HOLDER> that 's higher on the list @$ i.e @$ a device that 's configured to prefer 64 bit,if ( has32 bit libs && has64 bit libs ) { if ( ( pkg . application info . flags & application info . flag_multiarch ) == __num__ ) { slog . e ( package manager service . tag @$ __str__ + pkg + __str__ ) ; } if ( vm runtime . is64 bit instruction set ( get preferred instruction set ( ) ) ) { primary cpu abi = build . supported_64_bit_abis [ __num__ ] ; secondary cpu abi = build . supported_32_bit_abis [ __num__ ] ; } else { primary cpu abi = build . supported_32_bit_abis [ __num__ ] ; secondary cpu abi = build . supported_64_bit_abis [ __num__ ] ; } } else { primary cpu abi = null ; secondary cpu abi =,device see
if the next sublist has no <PLACE_HOLDER> @$ add this one and then break @$ otherwise just break,if ( next index - curr index == __num__ ) { next index ++ ; },sublist has
do not succeed the callback so the server will block <PLACE_HOLDER> .,scenario . client . new request ( scenario . newuri ( ) ) . on response content async ( ( response @$ content @$ callback ) -> { callbacks . offer ( callback ) ; } ) . send ( result -> { if ( result . is failed ( ) ) result latch . count down ( ) ; } ) ;,server block
since the db is running @$ exclude the <PLACE_HOLDER> files,return ! __str__ . equals ( sub path ) && ! sub path . ends with ( __str__ ) ;,files exclude
this assignment should n't cause <PLACE_HOLDER> ' to be inferred as undefined above .,test types with common externs ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,assignment cause
note to translators : the stylesheet referred to an extension to the xsl syntax and indicated that it was defined by xsltc @$ but xstlc does not recognized the particular extension <PLACE_HOLDER>d . the substitution text gives the extension <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text gives
bind and start to accept incoming <PLACE_HOLDER> .,server bootstrap . bind ( new inet socket address ( address @$ port ) ) ;,bind accept
clients have no <PLACE_HOLDER> to iud acid tables,table . set access type ( accesstype_none ) ;,clients have
the following boolean logic implements the <PLACE_HOLDER> above,if ( left == left2 && right == right2 && key length <= r2 . key length && r2 . pattern . region matches ( __num__ @$ pattern @$ __num__ @$ len ) ) { return ( flags == r2 . flags ) || ( ! ( ( flags & anchor_start ) != __num__ ) && ! ( ( flags & anchor_end ) != __num__ ) ) || ( ( ( r2 . flags & anchor_start ) != __num__ ) && ( ( r2 . flags & anchor_end ) != __num__ ) ) ; },logic implements
close all the given file media <PLACE_HOLDER> on error case .,if ( error string != null ) { for ( media item item : playlist ) { if ( item instanceof file media item ) { ( ( file media item ) item ) . increase ref count ( ) ; ( ( file media item ) item ) . decrease ref count ( ) ; } } throw new illegal argument exception ( error string ) ; },given file
must iterate over every identifier @$ since we do n't know which ones will match any given <PLACE_HOLDER>,for ( identifier id : identifiers . values ( ) ) { if ( ! id . is class ) continue ; for ( int i = __num__ ; i < starting classes . size ( ) ; ++ i ) { if ( starting patterns [ i ] . matcher ( id . name ) . matches ( ) ) { dep queue . add last ( id ) ; closure . add ( id ) ; matched [ i ] = true ; if ( verbose ) { log . info ( __str__ + id . name ) ; } break ; } } } for ( int i = __num__ ; i < starting classes . size ( ) ; ++ i ) { if,ones match
reset things for a new event <PLACE_HOLDER> @$ just in case we did n't get the whole previous <PLACE_HOLDER> .,if ( action == motion event . action_down ) { cancel ( ) ; },things get
effective host name minus domain must not contain any <PLACE_HOLDER>,string effective host without domain = host . substring ( __num__ @$ host . length ( ) - cookie domain . length ( ) ) ; return effective host without domain . index of ( __str__ ) == - __num__ ;,name contain
if all of the algorithms failed and one of the algorithms threw an <PLACE_HOLDER> @$ throw the last planning <PLACE_HOLDER>,if ( planning exception != null ) { throw planning exception ; },one threw
visit the class type after interface types which is the order the obj c compiler visits the <PLACE_HOLDER> .,type mirror class type = null ; for ( type mirror super type : direct supertypes ( type ) ) { if ( ! result ) { return false ; } if ( is class ( super type ) ) { class type = super type ; } else { visit type hierarchy objc order ( super type @$ visitor ) ; } } if ( class type != null && result ) { result = visit type hierarchy objc order ( class type @$ visitor ) ; } return result ;,compiler visits
! f create entity ref nodes move <PLACE_HOLDER> of entity ref before the entity ref . remove entity ref .,f current node index = f deferred document impl . get parent node ( f current node index @$ false ) ;,ref nodes
remove core attributes since the user does not want <PLACE_HOLDER> @$ unless they are in the attribute list . attribute list always wins,for ( string core attribute : core attributes ) { if ( ! attributes . contains ( core attribute ) ) { result . remove ( core attribute ) ; } },user want
unit test version does not handle launch wake <PLACE_HOLDER>,do nothing ( ) . when ( this ) . acquire launch wakelock ( ) ; do return ( m keyguard controller ) . when ( this ) . get keyguard controller ( ) ; m launching activity wake lock = mock ( power manager . wake lock . class ) ; initialize ( ) ;,version handle
if padding <PLACE_HOLDER> equals background <PLACE_HOLDER> @$ return null to indicate we do n't have to paint it .,if ( padding color == background color ) { return null ; } return padding color ;,color equals
process height points reversed <PLACE_HOLDER> @$ becase for same pos x @$ we always want the highest point,priority queue < integer > queue = new priority queue < integer > ( __num__ @$ collections . reverse order ( ) ) ; queue . offer ( __num__ ) ; int prev = queue . peek ( ) ; for ( height point p : height points ) { if ( p . height < __num__ ) { queue . offer ( - p . height ) ; } else { queue . remove ( p . height ) ; } int curr peak = queue . peek ( ) ; if ( curr peak != prev ) { rst . add ( new int [ ] { p . x @$ curr peak } ) ; prev = curr peak ; } } return rst ;,points reversed
find the place the new entry should go @$ ensuring an entry with the same name does n't already exist along the <PLACE_HOLDER>,directory entry prev = null ; directory entry curr = table [ index ] ; while ( curr != null ) { if ( curr . name ( ) . equals ( entry . name ( ) ) ) { if ( overwrite existing ) { if ( prev != null ) { prev . next = entry ; } else { table [ index ] = entry ; } entry . next = curr . next ; curr . next = null ; entry . file ( ) . increment link count ( ) ; return ; } else { throw new illegal argument exception ( __str__ + entry . name ( ) + __str__ ) ; } } prev = curr ; curr = curr .,entry exist
check if font has correct <PLACE_HOLDER> of letters,if ( font . length != __num__ ) { system . out . println ( __str__ + font . length + __str__ ) ; system . out . println ( __str__ ) ; } for ( int i = __num__ ; i < font . length ; i ++ ) { letter = font [ i ] ; b = __num__ ; if ( ( letter . length != n ) && ( n == - __num__ ) ) { n = letter . length ; } else if ( letter . length != n ) { system . out . println ( __str__ + i + __str__ + letter . length + __str__ + n + __str__ ) ; system . out . println ( __str__ ),font has
first see if the file matches the regular <PLACE_HOLDER> !,if ( pattern != null ) { matcher matcher = pattern . matcher ( selectedfile ) ; get it = matcher . matches ( ) ; },file matches
delete <PLACE_HOLDER> is complete before <PLACE_HOLDER> so not relevant @$ get next delete <PLACE_HOLDER>,delete range = delete range it . has next ( ) ? delete range it . next ( ) : null ; break ; case range1_completely_after_range2 :,range get
at this point either m whitelisted country <PLACE_HOLDER> or m blacklisted country <PLACE_HOLDER> is null . we assume no countries are to be excluded . here @$ we correct this assumption based on the contents of either lists .,set < string > excluded countries = new hash set < > ( ) ; if ( m whitelisted country isos == null ) { excluded countries . add all ( m blacklisted country isos ) ; } else { excluded countries . add all ( country info map . key set ( ) ) ; excluded countries . remove all ( m whitelisted country isos ) ; },m whitelisted
broker requires a concrete member <PLACE_HOLDER> to be allowed to join the group . update member <PLACE_HOLDER> and send another join group request in next cycle .,if ( error == errors . member_id_required ) { synchronized ( abstract coordinator . this ) { abstract coordinator . this . generation = new generation ( offset commit request . default_generation_id @$ join response . data ( ) . member id ( ) @$ null ) ; abstract coordinator . this . reset state and rejoin ( ) ; } future . raise ( error ) ; } else { log . error ( __str__ @$ error . message ( ) ) ; future . raise ( new kafka exception ( __str__ + error . message ( ) ) ) ; },broker requires
args can contain <PLACE_HOLDER> like location macros @$ so extract any inputs we find .,for ( arg arg : preprocessor flags . get other flags ( ) . get all flags ( ) ) { buildable support . derive inputs ( arg ) . for each ( input consumer ) ; },args contain
this is not the last <PLACE_HOLDER> @$ so our offset should record the next <PLACE_HOLDER> to be used ...,if ( event row number < ( total number of rows - __num__ ) ) { this . current row number = event row number ; this . restart rows to skip = this . current row number + __num__ ; return offset using position ( this . restart rows to skip ) ; },offset record
the users of this method do not need a nano <PLACE_HOLDER>,return clock . system ( zone ) ;,users need
this method will be called on rotation suggestion changes even if the proposed rotation is not valid for the top app . use invalid rotation <PLACE_HOLDER> as a signal to remove the rotate button if shown .,if ( ! is valid ) { set rotate suggestion button state ( false ) ; return ; },method use
and it is not always true that a bigger array uses more <PLACE_HOLDER> than a smaller one,return ( ) -> arrays . stream ( value type . array types ( ) ) . filter ( t -> t != value type . string_array && t != value type . string_alphanumeric_array && t != value type . string_ascii_array && t != value type . string_bmp_array ) . iterator ( ) ;,array uses
we expect two dependencies @$ because the dependencies are annotated with element @$ which has a unique <PLACE_HOLDER> @$ it 's difficult to directly compare them . instead we will manually compare all the fields except the unique <PLACE_HOLDER>,assert equals ( __num__ @$ actual dependencies . size ( ) ) ; for ( dependency < ? > dependency : actual dependencies ) { key < ? > key = dependency . get key ( ) ; assert equals ( new type literal < string > ( ) { } @$ key . get type literal ( ) ) ; annotation annotation = dependency . get key ( ) . get annotation ( ) ; assert true ( annotation instanceof element ) ; element element = ( element ) annotation ; assert equals ( __str__ @$ element . set name ( ) ) ; assert equals ( element . type . mapbinder @$ element . type ( ) ) ; assert equals ( __str__ @$ element .,which has
the test program creates a class file from the header stored above and adding the content of a source debug extension attribute made of the character 0 x 02 repeated 68000 times . this attribute does n't follow the <PLACE_HOLDER> specified in jsr 45 but it 's fine because this test just checks that the jvm is able to load a class file with,byte [ ] buf = new byte [ header . length + attr size ] ; for ( int i = __num__ ; i < header . length ; i ++ ) { buf [ i ] = header [ i ] ; } for ( int i = __num__ ; i < attr size ; i ++ ) { buf [ header . length + i ] = ( byte ) __num__ ; } class c = loader . define class ( __str__ @$ buf @$ __num__ @$ buf . length ) ; system . out . println ( __str__ ) ;,attribute follow
we hold the wake lock as long as the service is processing <PLACE_HOLDER> .,synchronized ( this ) { if ( ! m service processing ) { m service processing = true ; try { m run wake lock . acquire ( __num__ * __num__ * __num__ ) ; m launch wake lock . release ( ) ; } catch ( throwable e ) { file log . e ( e ) ; m service processing = false ; } } },service processing
customers do n't see this <PLACE_HOLDER>,if ( ! ( boolean ) result ) { throw new entry not found exception ( __str__ ) ; },customers see
check if the workers go <PLACE_HOLDER> ...,assert true ( appender . log contains ( __str__ ) ) ; assert true ( appender . log contains ( __str__ ) ) ; assert true ( appender . log contains ( __str__ ) ) ;,workers go
tests throwable proxy util.ste <PLACE_HOLDER> to step <PLACE_HOLDER>,new throwable proxy ( t ) ;,tests throwable
only labels changed . since we do n't know which properties this entity has let 's include all <PLACE_HOLDER> for the changed labels .,if ( properties . length == __num__ ) { set . matching descriptors ( descriptors @$ changed entity tokens ) ; } else if ( changed entity tokens . length == __num__ ) { set . matching descriptors for partial list of properties ( descriptors @$ unchanged entity tokens @$ properties ) ; } else { set . matching descriptors ( descriptors @$ changed entity tokens ) ; set . matching descriptors for partial list of properties ( descriptors @$ unchanged entity tokens @$ properties ) ; },"s" include
we catch everything because right now the masters do not require authentication . so delay reporting errors to the user until the servers return 401 <PLACE_HOLDER> .,log . debug ( __str__ + __str__ + __str__ @$ e ) ;,servers return
negative offset means <PLACE_HOLDER>,return cursor offset >= __num__ ;,offset means
open a new region which uses this <PLACE_HOLDER>,table descriptor htd = table descriptor builder . new builder ( table name . value of ( __str__ ) ) . set column family ( column family descriptor builder . of ( b ) ) . build ( ) ; region info hri = region info builder . new builder ( htd . get table name ( ) ) . build ( ) ; chunk creator . initialize ( mem storelab impl . chunk_size_default @$ false @$ __num__ @$ __num__ @$ __num__ @$ null ) ; test_util . create localh region ( hri @$ htd @$ wal ) . close ( ) ; region server services rs services = mock ( region server services . class ) ; when ( rs services . get server name ( ),which uses
number start <PLACE_HOLDER>,case __str__ : case __str__ : case __str__ : case __str__ : case __str__ : case __str__ : case __str__ : case __str__ : case __str__ : case __str__ :,number start
rank 2 denotes the second largest <PLACE_HOLDER> .,i2 = rankx >= __num__ ? __num__ : __num__ ; j2 = ranky >= __num__ ? __num__ : __num__ ; k2 = rankz >= __num__ ? __num__ : __num__ ; l2 = rankw >= __num__ ? __num__ : __num__ ;,rank denotes
script object mirror has some object equality <PLACE_HOLDER> for entries,if ( o instanceof map ) { if ( depth > __num__ || ! seen . add ( o ) ) { return true ; } map map = ( map ) o ; map . for each ( ( k @$ v ) -> { if ( recurse cyclic ( depth + __num__ @$ v @$ seen ) ) { map . put ( k @$ __str__ + v . get class ( ) . get name ( ) ) ; } } ) ; } else if ( o instanceof list ) { if ( depth > __num__ || ! seen . add ( o ) ) { return true ; } list list = ( list ) o ; int count = list . size,mirror has
json stringer can cause an <PLACE_HOLDER> when the json to handle is too big .,response . result = null ; response . error = m object mapper . convert value ( e . get message ( ) @$ json object . class ) ; json object = m object mapper . convert value ( response @$ json object . class ) ; response string = json object . to string ( ) ;,stringer cause
harmony expected 1<PLACE_HOLDER> @$ but the ri and android report <PLACE_HOLDER> .,assert equals ( __num__ @$ f . get channel ( ) . position ( ) ) ;,ri report
should flush latch 1 and latch <PLACE_HOLDER> and then run latch 3 immediately,assert true ( latch3 . await ( __num__ @$ time unit . milliseconds ) ) ; assert equals ( __num__ @$ latch1 . get count ( ) ) ; assert equals ( __num__ @$ latch2 . get count ( ) ) ;,flush latch
<PLACE_HOLDER> processing can reuse the last <PLACE_HOLDER> in this case @$ we do n't even check yield signal ; make yield force to false,test project list ( ineffective block @$ dictionary block . class @$ projection @$ false @$ produce lazy block ) ;,processing reuse
noinspection placeholder <PLACE_HOLDER> matches argument <PLACE_HOLDER>,logger . debug ( __str__ @$ instance . to string ( ) ) ; return instance ;,count matches
the map reduce tokens are provided so that tasks can spawn <PLACE_HOLDER> if they wish to . the tasks authenticate to the job tracker via the map reduce delegation tokens .,if ( system . getenv ( __str__ ) != null ) { conf . set ( __str__ @$ system . getenv ( __str__ ) ) ; } return conf ;,tasks spawn
add a custom equality assertion because the resulting geometry may have its constituent <PLACE_HOLDER> in a different order,bi function < object @$ object @$ boolean > equality function = ( left @$ right ) -> { if ( left == null && right == null ) { return true ; } if ( left == null || right == null ) { return false ; } ogc geometry left geometry = ogc geometry . from text ( left . to string ( ) ) ; ogc geometry right geometry = ogc geometry . from text ( right . to string ( ) ) ; return left geometry . difference ( right geometry ) . is empty ( ) && right geometry . difference ( left geometry ) . is empty ( ) ; } ;,geometry have
base length next line handles both positive <PLACE_HOLDER> and also scale mismatch,if ( scale != - lhs . exp ) reqdig = ( reqdig + scale ) + lhs . exp ; reqdig = ( reqdig - ( ( rhs . mant . length - __num__ ) ) ) - rhs . exp ;,length handles
need to set the system property that the log 4 j 2 configuration reads in order to determine the script log file <PLACE_HOLDER> . once that 's set @$ the log configuration must be 'kicked ' to pick up the change .,system . set property ( __str__ @$ file . get absolute path ( ) ) ; if ( initialized ) { ( ( logger context ) log manager . get context ( false ) ) . reconfigure ( ) ; },log file
stream still has <PLACE_HOLDER> . buffer starvation occurred . audio decoder thread will fill the <PLACE_HOLDER> and start the channel again .,reclaim channel = true ;,stream has
stream must not receive <PLACE_HOLDER> sent by server after reset .,assert false ( stream1 data latch . await ( __num__ @$ time unit . seconds ) ) ;,stream receive
gms join leave mistakenly uses an old view <PLACE_HOLDER> when joining @$ making it a rogue member,gms join leave member id . set vm view id ( - __num__ ) ; member identifier previous member id = services . get member factory ( ) . create ( member data builder . new builder ( gms join leave member id . get inet address ( ) @$ gms join leave member id . get host name ( ) ) . set membership port ( gms join leave member id . get membership port ( ) ) . build ( ) ) ; previous member id . set vm view id ( __num__ ) ; previous member id . get member data ( ) . setuuid ( gms join leave member id . get member data ( ) . getuuid ( ) ) ; gms membership,member uses
make sure the master has <PLACE_HOLDER> of the reports,waiter . wait for ( test_util . get configuration ( ) @$ __num__ * __num__ @$ new predicate < exception > ( ) { @ override public boolean evaluate ( ) throws exception { map < region info @$ long > region sizes = quota manager . snapshot region sizes ( ) ; log . trace ( __str__ + region sizes ) ; return num regions == count regions for table ( tn @$ region sizes ) && table size <= get table size ( tn @$ region sizes ) ; } } ) ; map < table name @$ long > sizes = test_util . get admin ( ) . get space quota table sizes ( ) ; long size = sizes . get ( tn ),master has
get namespace edits dirs removes <PLACE_HOLDER> across edits and shared.edits,collection < uri > edits dirs = fs namesystem . get namespace edits dirs ( conf ) ; assert equals ( __num__ @$ edits dirs . size ( ) ) ;,dirs removes
now do an unbounded request . this will read all of the data from cache and then try to read more from upstream which will cause to a 416 so cds will store the <PLACE_HOLDER> .,cache data source cache data source = create cache data source ( true @$ true @$ cache key factory ) ;,cds store
user 3 has the <PLACE_HOLDER>,user dto user1 = db . users ( ) . insert user ( ) ; user dto user2 = db . users ( ) . insert user ( ) ; user dto user3 = db . users ( ) . insert user ( ) ; organization dto organization = db . organizations ( ) . insert ( ) ; group dto group1 = db . users ( ) . insert group ( organization ) ; db . users ( ) . insert permission on group ( group1 @$ administer ) ; db . users ( ) . insert permission on group ( group1 @$ provision_projects ) ; db . users ( ) . insert member ( group1 @$ user1 ) ; db . users ( ) . insert,user has
class d has a directly applied parameter <PLACE_HOLDER> whose value includes to an <PLACE_HOLDER> class that is missing .,test parameter annotation ( d . class . get declared method ( __str__ @$ object . class ) @$ true ) ;,d has
slow down the producer @$ this will make the queue mostly empty encouraging visibility <PLACE_HOLDER> .,thread . yield ( ) ; return ! stop . get ( ) ;,queue encouraging
seems awkward to have the stream wrap <PLACE_HOLDER> .,super ( wrapped @$ wrapped . get http request ( ) ) ; this . wrapped = wrapped ;,stream wrap
if there are more new <PLACE_HOLDER> than how many new dom <PLACE_HOLDER> got added @$ the top row logical index necessarily gets shifted down by that difference because recycling does n't replace any logical <PLACE_HOLDER> @$ just shifts them off the visual range @$ and the inserted <PLACE_HOLDER> that do n't fit to the visual range also push the other <PLACE_HOLDER> down . if,if ( number of rows > added row count ) { update top row logical index ( number of rows - added row count ) ; },rows push
call filter all <PLACE_HOLDER> in project to request cached rules .,filter all targets in project ( parser @$ parsing context ) ;,call filter
this situation is possible only if a callee throws an <PLACE_HOLDER> which type extends throwable directly,if ( cause instanceof command action execution exception ) { command action execution exception command action execution exception = ( command action execution exception ) cause ; cause = command action execution exception . get cause ( ) ; },callee throws
the response should contain the exact <PLACE_HOLDER> that the schema registry has .,assert . assert equals ( __num__ @$ metadata schemas list . size ( ) ) ; for ( register response metadata entry r : metadata schemas list ) { if ( r . get version ( ) == __num__ ) { assert . assert equals ( metadata schema1 @$ r . get schema ( ) ) ; assert . assert true ( arrays . equals ( meta schema digest1 @$ r . get crc32 ( ) ) ) ; } else { assert . assert equals ( metadata schema2 @$ r . get schema ( ) ) ; assert . assert true ( arrays . equals ( meta schema digest2 @$ r . get crc32 ( ) ) ) ; } } easy mock . verify ( mock,response contain
unlike status.from throwable which returns the unknown <PLACE_HOLDER> for these .,status st = status proto . from throwable ( e ) ; return st != null ? st : status . new builder ( ) . set code ( code . internal . get number ( ) ) . set message ( e . get message ( ) ) . build ( ) ;,which returns
list includes all <PLACE_HOLDER>,try ( byte provider byte provider = get byte provider ( __str__ @$ __str__ + __str__ + __str__ + __str__ ) ) { check validxml load spec ( byte provider @$ loader . find supported load specs ( byte provider ) @$ __str__ @$ endian . little @$ __str__ @$ null @$ __str__ ) ; },list includes
this assumes the adapter has already cleaned up any partially created <PLACE_HOLDER> .,if ( overlay ) { check remove address space ( start . get address space ( ) ) ; } throw new cancelled exception ( ) ; program . db error ( e ) ;,adapter cleaned
this branch coordinates fragment task or completed transaction task @$ holds the <PLACE_HOLDER> until all the sites on the node receive the task . task with newer sp handle will,if ( task . need coordination ( ) && m_scoreboard enabled ) { coordinated task queue offer ( task ) ; } else { task queue offer ( task ) ; },task holds
failed to read the file . there are two possibilities . either this file is in old format which does not have a magic <PLACE_HOLDER> in the beginning or this is not a valid file at all . try reading it as a file in old format,fis . close ( ) ; fis = new file input stream ( f ) ; dis = new data input stream ( new buffered input stream ( fis @$ __num__ * __num__ ) ) ; read disk store record ( dis @$ f ) ;,which have
only let the system discover remote display <PLACE_HOLDER> for now .,if ( client record != null ) { if ( ! client record . m trusted ) { route types &= ~ media router . route_type_remote_display ; } if ( client record . m route types != route types || client record . m active scan != active scan ) { if ( debug ) { slog . d ( tag @$ client record + __str__ + integer . to hex string ( route types ) + __str__ + active scan ) ; } client record . m route types = route types ; client record . m active scan = active scan ; client record . m user record . m handler . send empty message ( user handler . msg_update_discovery_request ) ; } },system discover
set the low threshold to 50 instead of 60 set the high threshold to 60 instead of 70 node 2 now should not have new <PLACE_HOLDER> allocated to it @$ and <PLACE_HOLDER> can not remain,disk settings = settings . builder ( ) . put ( disk threshold settings . cluster_routing_allocation_disk_threshold_enabled_setting . get key ( ) @$ true ) . put ( disk threshold settings . cluster_routing_allocation_low_disk_watermark_setting . get key ( ) @$ __str__ ) . put ( disk threshold settings . cluster_routing_allocation_high_disk_watermark_setting . get key ( ) @$ __str__ ) . put ( disk threshold settings . cluster_routing_allocation_disk_flood_stage_watermark_setting . get key ( ) @$ __str__ ) . build ( ) ; deciders = new allocation deciders ( new hash set < > ( arrays . as list ( new same shard allocation decider ( settings . empty @$ cluster settings ) @$ make decider ( disk settings ) ) ) ) ; strategy = new allocation service ( deciders @$ new test,2 have
verify append called or <PLACE_HOLDER>,verify ( wal @$ expect append ? times ( __num__ ) : never ( ) ) . append data ( ( h region info ) any ( ) @$ ( wal key impl ) any ( ) @$ ( wal edit ) any ( ) ) ;,append called
if the text wrapper does n't have a start <PLACE_HOLDER>,if ( quotes . get ( i ) . start index == - __num__ ) { if ( closing quote == - __num__ ) { closing quote = i ; } else { if ( quotes . get ( i ) . end index < quotes . get ( closing quote ) . end index ) { closing quote = i ; } } },wrapper have
table mutation requires a current <PLACE_HOLDER> to mutate,assert ( change type == change type . create || this . old schema != null ) ;,mutation requires
insert the video . the command sends three arguments . the first specifies which <PLACE_HOLDER> the api request is setting and which <PLACE_HOLDER> the api response should return . the second argument is the video resource that contains metadata about the new video . the third argument is the actual video content .,you tube . videos . insert video insert = youtube . videos ( ) . insert ( __str__ @$ video object defining metadata @$ media content ) ;,request setting
make sure the preferences has only one sorted <PLACE_HOLDER>,table sort state saved sort state = get sort state from preference state ( preference state @$ column model state ) ; assert equals ( __num__ @$ saved sort state . get sorted column count ( ) ) ; column sort state column sort state = saved sort state . get column sort state ( column zero ) ; assert not null ( column sort state ) ; sort direction sort direction = column sort state . get sort direction ( ) ; assert true ( sort direction . is ascending ( ) ) ;,preferences sorted
this can only happen if someone uses unchecked <PLACE_HOLDER> .,if ( extension . get containing type default instance ( ) != get default instance for type ( ) ) { throw new illegal argument exception ( __str__ + __str__ ) ; },someone uses
open the first file & read the required rows in the buffer @$ stop if it fails @$ exception will stop process <PLACE_HOLDER>,open next file ( ) ;,exception stop
while wbmp reader does not support ext wbmp <PLACE_HOLDER>,if ( type != __num__ || fix header field != __num__ ) { return false ; },reader support
go through each message and make sure the unmarshalled fields are null then call the getters which will unmarshall the <PLACE_HOLDER> again to show the marshalled <PLACE_HOLDER> exists,for ( message reference ref : messages ) { activemq text message message = ( activemq text message ) ref . get message ( ) ; field properties field = message . class . get declared field ( __str__ ) ; properties field . set accessible ( true ) ; field text field = activemq text message . class . get declared field ( __str__ ) ; text field . set accessible ( true ) ; assert null ( text field . get ( message ) ) ; assert null ( properties field . get ( message ) ) ; assert not null ( message . get properties ( ) ) ; assert not null ( message . get text ( ) ) ; } consumer . close,which unmarshall
no need to sync because noone has <PLACE_HOLDER> to new info yet,new info . policy entries . add ( pe ) ;,noone has
list applicable <PLACE_HOLDER>s may return <PLACE_HOLDER> which was already added explicitly above .,return stream . concat ( roles @$ list applicable roles ( principal @$ list role grants ) . map ( role grant :: get role name ) . filter ( predicate . is equal ( admin_role_name ) . negate ( ) ) ) . distinct ( ) ;,roles return
make sure the new comment has the same <PLACE_HOLDER> as the old one,new comment . level = this . get ( index ) . level ; this . set ( index @$ new comment ) ; return true ;,comment has
the new tree does n't have this <PLACE_HOLDER> . we only need to remove all its children from the list .,if ( new root is null ) { final int current items count = current root . get count ( ) ; removed components . add ( current root ) ; final change set change set = change set . acquire change set ( current root . get count ( ) @$ new root @$ enable stats ) ; for ( int i = __num__ ; i < current items count ; i ++ ) { change set . add change ( change . remove ( __num__ ) ) ; } return change set ; },tree have
optional validation if the response contains a <PLACE_HOLDER>,if ( has text ( saml response . get destination ( ) ) && ! recipient . equals ( saml response . get destination ( ) ) ) { throw auth exception ( invalid_destination @$ __str__ + saml response . get destination ( ) ) ; } string issuer = saml response . get issuer ( ) . get value ( ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ + issuer ) ; } if ( ! has text ( issuer ) || ( ! issuer . equals ( token . get idp entity id ( ) ) ) ) { string message = string . format ( __str__ @$ issuer @$ token . get idp entity id,response contains
put them in and also roll them up while we 're at it a field contains a field <PLACE_HOLDER> which in turn contains a <PLACE_HOLDER>,for ( int i = __num__ ; i < jjt get num children ( ) ; i ++ ) { dynamic ser de field mt = get field ( i ) ; dynamic ser de type base type = mt . get field type ( ) . get my type ( ) ; type . initialize ( ) ; type . fieldid = mt . fieldid ; type . name = mt . name ; types_by_id . put ( integer . value of ( mt . fieldid ) @$ type ) ; types_by_column_name . put ( mt . name @$ type ) ; ordered_types [ i ] = type ; ordered_column_id_by_name . put ( mt . name @$ i ) ; },field contains
slow mode starts an async <PLACE_HOLDER>,if ( mode == getrss mode . ps ) { if ( thread != null ) { if ( thread . is alive ( ) ) return ; else thread = null ; } thread = new thread ( new runnable ( ) { @ override public void run ( ) { sample system now ( medium @$ large ) ; } } ) ; thread . start ( ) ; } else { sample system now ( medium @$ large ) ; },mode starts
<PLACE_HOLDER> nodes will contain the both our <PLACE_HOLDER> @$ if it exists @$ and any nested stat <PLACE_HOLDER>s . ours will always be first,if ( description nodes . get length ( ) > __num__ ) { element description node = ( element ) description nodes . item ( __num__ ) ; if ( description node . get parent node ( ) . get node name ( ) . equals ( type node . get node name ( ) ) ) { description = extract description ( description node ) ; } },nodes contain
key is not a string @$ can not apply <PLACE_HOLDER> to this entry,if ( ! ( entry key instanceof string ) ) { continue ; },key apply
the user scheduler captures <PLACE_HOLDER>,assert true ( results . is context initialized . get ( ) ) ;,scheduler captures
both added global <PLACE_HOLDER>,check symbol ( symbols [ __num__ ] @$ __str__ @$ true ) ;,both added
put connect controls into environment . copy them first since caller owns the <PLACE_HOLDER> .,if ( conn ctls != null ) { control [ ] copy = new control [ conn ctls . length ] ; system . arraycopy ( conn ctls @$ __num__ @$ copy @$ __num__ @$ conn ctls . length ) ; env . put ( bind_controls_property @$ copy ) ; },caller owns
n has <PLACE_HOLDER> and m has 1,if ( ! is bitn && is bitm ) { int mask = ~ ( m << k ) ; m = m & mask ; },n has
any user can execute table <PLACE_HOLDER> ; queries like ` select 1 ` might be used to do simple connection checks,return null ;,user execute
one of the 2 threads should get an optimistic lock <PLACE_HOLDER>,assert equals ( __num__ @$ exceptions . size ( ) ) ; assert true ( exceptions . get ( __num__ ) instanceof activiti optimistic locking exception ) ;,one get
note that we can not do any size checking here @$ as the correct component count depends on the drawing step . gl should catch such <PLACE_HOLDER> then @$ and we will report them to the user .,if ( m values != null ) { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ __num__ ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m values ) ; } else { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ m vbo ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m offset ) ; },note catch
multiload 3 items and ensure that multiload pulls <PLACE_HOLDER> from the database & 1 from the cache .,list < simple entity > entities = session . by multiple ids ( simple entity . class ) . with ( cache mode . normal ) . enable session check ( true ) . enable ordered return ( true ) . enable return of deleted entities ( true ) . multi load ( ids ( __num__ ) ) ; assert equals ( __num__ @$ entities . size ( ) ) ; simple entity deleted entity = entities . get ( __num__ ) ; assert not null ( deleted entity ) ; final entity entry entry = ( ( shared session contract implementor ) session ) . get persistence context ( ) . get entry ( deleted entity ) ; assert true ( entry . get status ( ),multiload pulls
keep glyph cache panel width from ever going down so the canvas game container does n't change <PLACE_HOLDER> and flicker .,glyph cache panel = new j panel ( ) { private int max width ; public dimension get preferred size ( ) { dimension size = super . get preferred size ( ) ; max width = math . max ( max width @$ size . width ) ; size . width = max width ; return size ; } } ;,container change
create server validator eagerly so that we can conveniently get the trusted certificates clients need it anyway eventually @$ and servers will not mind the little extra <PLACE_HOLDER>,validator v = get validator ( validator . var_tls_server ) ; trusted certs = v . get trusted certificates ( ) ; server validator = v ; if ( debug != null && debug . is on ( __str__ ) ) { show trusted certs ( ) ; },servers mind
make sure constantinople config is not using parent 's block <PLACE_HOLDER>,return new constants adapter ( super . get constants ( ) ) { @ override public big integer getblock_reward ( ) { return big integer . ten ; } } ;,config using
for delayed binding step partitioning meta does not achieve schema <PLACE_HOLDER> when using in constructor so we have to set it explicitly . see equals implementation for step partitioning meta .,step partitioning meta part meta = new step partitioning meta ( __str__ @$ schema ) ;,meta achieve
to prevent task stack resize <PLACE_HOLDER> may flicking when playing app transition <PLACE_HOLDER> & ime window enter <PLACE_HOLDER> in parallel @$ make sure app transition is done and then start to animate for ime .,if ( m animating for ime && ! m display content . m app transition . is running ( ) ) { return animate for ime ( now ) ; },task stack
empty input produces empty <PLACE_HOLDER> .,string [ ] [ ] [ ] inputs outputs = { { { } @$ { } } @$ { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__,input produces
one write only @$ valid <PLACE_HOLDER> ; space in the beginning @$ transform quoted,list < object [ ] > parameters = arrays . < object [ ] > as list ( new object [ ] { new string item ( __str__ ) @$ __str__ @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . state ) } @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . command ) } @$ null } @$ new object [ ] { string item with state ( __str__ @$ new string type ( __str__ ) ) @$ __str__ @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . state ) } @$ new itemio connection [ ] { new itemio,one write
queue c 111 has <PLACE_HOLDER> and aq @$ both from parent,assert true ( c111 . has access ( queueacl . administer_queue @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . administer_queue @$ __str__ ) ) ; assert true ( c111 . has access ( queueacl . submit_applications @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . submit_applications @$ __str__ ) ) ; reset ( c ) ;,111 has
column q 1 covers version at 123 fr which user 2 do not have <PLACE_HOLDER>,p . add column ( test_family1 @$ q1 @$ __num__ @$ value ) ; p . add column ( test_family1 @$ q2 @$ value ) ; t . put ( p ) ; fail ( user . get short name ( ) + __str__ ) ;,user have
param types will not include <PLACE_HOLDER> ',std type list param types = desc . get parameter types ( ) ; int sz param types = param types . size ( ) ;,types include
unless this is the last replacment <PLACE_HOLDER>s @$ we pop the used <PLACE_HOLDER> . the lastreplacement <PLACE_HOLDER> applies any additional entries .,if ( names . size ( ) > __num__ ) { names . remove ( __num__ ) ; } callback . copy ( null ) ;,names pop
canceling one instance triggers <PLACE_HOLDER> for all other instances :,task service . set variable ( task . get id ( ) @$ __str__ @$ false ) ; task service . complete ( task . get id ( ) ) ; assert equals ( __num__ @$ create event subscription query ( ) . count ( ) ) ; assert equals ( __num__ @$ runtime service . get variable ( process instance . get id ( ) @$ __str__ ) ) ; assert equals ( __num__ @$ runtime service . get variable ( process instance . get id ( ) @$ __str__ ) ) ; runtime service . trigger ( runtime service . create execution query ( ) . activity id ( __str__ ) . single result ( ) . get id ( ) ) ; assert process ended,instance triggers
say xml has a type <PLACE_HOLDER> that is removed in the new version,string existingxml = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; cache config cache config = service . un marshall ( existingxml ) ; list elements = cache config . get custom cache elements ( ) ; assert that ( elements . get ( __num__ ) ) . is instance of ( element one . class ) ;,xml has
let handle timeout take <PLACE_HOLDER> of finishing the page,if ( ! timeout flag ) handler . post delayed ( new web view status checker ( ) @$ __num__ ) ;,timeout take
add document level entity mentions <PLACE_HOLDER>,if ( doc . contains key ( mentions annotation . class ) ) { for ( core map mention : doc . get ( mentions annotation . class ) ) { builder . add mentions ( to proto mention ( mention ) ) ; } keys to serialize . remove ( mentions annotation . class ) ; builder . set has entity mentions annotation ( true ) ; } else { builder . set has entity mentions annotation ( false ) ; },entity mentions
ensure a task throws an illegal state <PLACE_HOLDER> after cancelled,try { t = new timer ( ) ; timer test task test task = new timer test task ( ) ; t . cancel ( ) ; try { t . schedule ( test task @$ __num__ @$ __num__ ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } t = new timer ( ) ; test task = new timer test task ( ) ; t . schedule ( test task @$ __num__ @$ __num__ ) ; await run ( test task ) ; t . cancel ( ) ; synchronized ( sync ) { sync . wait ( __num__ ) ; } assert equals ( __str__ @$ __num__ @$ test task . was run ( ) ) ;,task throws
assume that attributes immediately follow the <PLACE_HOLDER> .,if ( dtm . element_node == type ) { int identity = make node identity ( node handle ) ; while ( dtm . null != ( identity = get next node identity ( identity ) ) ) { type = _type ( identity ) ; if ( type == dtm . attribute_node || type == dtm . namespace_node ) { node node = lookup node ( identity ) ; string nodeuri = node . get namespaceuri ( ) ; if ( null == nodeuri ) nodeuri = __str__ ; string nodelocalname = node . get local name ( ) ; if ( nodeuri . equals ( namespaceuri ) && name . equals ( nodelocalname ) ) return make node handle ( identity ) ; } else {,attributes follow
get the actual object from the script engine @$ versus the proxy stored in action handler . the object may have additional <PLACE_HOLDER> @$ where action handler is a proxied interface,try { final object obj = script engine . get ( __str__ ) ; if ( obj != null ) { try { invocable . invoke method ( obj @$ __str__ @$ context ) ; } catch ( final no such method exception nsme ) { if ( get logger ( ) . is debug enabled ( ) ) { get logger ( ) . debug ( __str__ ) ; } } } else { throw new script exception ( __str__ ) ; } } catch ( script exception se ) { throw new process exception ( __str__ @$ se ) ; },object have
instead of expiring the session directly here @$ accumulate a list of session ids that need to be expired . this is an efficiency measure : as the expiration involves the session data <PLACE_HOLDER> doing a delete @$ it is most efficient if it can be done as a bulk operation to eg reduce roundtrips to the persistent <PLACE_HOLDER> . only do this if,if ( session . is expired at ( now ) ) { if ( _session id manager . get session house keeper ( ) != null && _session id manager . get session house keeper ( ) . get interval sec ( ) > __num__ ) { _candidate session ids for expiry . add ( session . get id ( ) ) ; if ( log . is debug enabled ( ) ) log . debug ( __str__ @$ session . get id ( ) ) ; } } else { _session cache . check inactive session ( session ) ; },expiration involves
anchor changed @$ qualified name of children needs an <PLACE_HOLDER>,update child categories ( store object @$ store object . get children categories ( ) @$ impacted categories @$ false ) ;,name needs
initialize the input method manager and start a daemon thread if the user has multiple input <PLACE_HOLDER> to choose from . otherwise @$ just keep the instance .,if ( imm . has multiple input methods ( ) ) { imm . initialize ( ) ; thread imm thread = new thread ( imm @$ thread name ) ; imm thread . set daemon ( true ) ; imm thread . set priority ( thread . norm_priority + __num__ ) ; imm thread . start ( ) ; } input method manager = imm ;,user has
removing any quad <PLACE_HOLDER> to that <PLACE_HOLDER> will remove the <PLACE_HOLDER>,final quad item < t > quad item = new quad item < > ( item ) ; synchronized ( m quad tree ) { m items . remove ( quad item ) ; m quad tree . remove ( quad item ) ; },item remove
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default remote profile = new crawl profile ( crawl_profile_remote @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ null @$ - __num__ @$ true @$ true @$ true @$ false @$ true @$ true @$ false @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . iffresh @$ __str__ + crawl_profile_remote @$ client identification . yacy internet crawler agent name @$ null @$ null @$ __num__ ) ;,ip match
both items use the same <PLACE_HOLDER>,return __num__ ;,items use
no incoming flow file containing a query @$ and an exception causes no outbound <PLACE_HOLDER> . there should be no flow files on either relationship,runner . assert all flow files transferred ( abstract executesql . rel_failure @$ __num__ ) ; runner . assert all flow files transferred ( abstract executesql . rel_success @$ __num__ ) ;,exception causes
this is strange xml stream reader throws xml stream <PLACE_HOLDER> xml event reader does n't throw xml stream <PLACE_HOLDER>,boolean next = false ; try { next = fxml reader . has next ( ) ; } catch ( xml stream exception ex ) { return false ; } return next ;,reader throw
check that the vertices received the trigger checkpoint <PLACE_HOLDER> for the second checkpoint,verify ( vertex1 . get current execution attempt ( ) @$ times ( __num__ ) ) . trigger checkpoint ( eq ( checkpoint2 id ) @$ eq ( timestamp + __num__ ) @$ any ( checkpoint options . class ) ) ; verify ( vertex2 . get current execution attempt ( ) @$ times ( __num__ ) ) . trigger checkpoint ( eq ( checkpoint2 id ) @$ eq ( timestamp + __num__ ) @$ any ( checkpoint options . class ) ) ;,vertices received
control multicast advertise <PLACE_HOLDER> to inject proxy,agent = multicast discovery agent factory . create discovery agent ( new uri ( discovery address ) ) ; agent . register service ( proxy . get url ( ) . to string ( ) ) ; agent . start ( ) ; do reconnect ( ) ;,multicast advertise
get the calculated base dirs which includes the <PLACE_HOLDER>,str = props . get property ( __str__ ) ; if ( ! string util . is blank ( str ) ) { resource collection bases = new resource collection ( string util . csv split ( str ) ) ; web app . set war ( null ) ; web app . set base resource ( bases ) ; },which includes
the <PLACE_HOLDER> of call gets the <PLACE_HOLDER> of the input callback argument .,size += size of ( get original callback argument ( ) ) ; return size ;,size gets
since byte buffers can point all <PLACE_HOLDER> of crazy places it 's harder to keep track of which blocks are kept alive by what byte buffer . so we make a guess .,if ( c instanceof byte buffer extended cell ) { byte buffer extended cell bb cell = ( byte buffer extended cell ) c ; byte buffer bb = bb cell . get value byte buffer ( ) ; if ( bb != last block ) { context . increment response block size ( bb . capacity ( ) ) ; last block = bb ; } } else { byte [ ] value array = c . get value array ( ) ; if ( value array != last block ) { context . increment response block size ( value array . length ) ; last block = value array ; } },buffers point
this is not needed @$ the underlying functor does not have <PLACE_HOLDER> to this,throw new unsupported operation exception ( __str__ ) ;,functor have
record job reply <PLACE_HOLDER> .,if ( ! internal && ctx . event ( ) . is recordable ( evt_job_failed ) ) evts = add event ( evts @$ evt_job_failed @$ __str__ + job ) ; try { byte [ ] res bytes = null ; byte [ ] ex bytes = null ; byte [ ] attr bytes = null ; boolean loc = ctx . local node id ( ) . equals ( snd node . id ( ) ) && ! ctx . config ( ) . is marshal local jobs ( ) ; map < object @$ object > attrs = job ctx . get attributes ( ) ; if ( ! loc ) { try { res bytes = u . marshal ( marsh @$ res ) ;,job reply
test that a null element throws null pointer <PLACE_HOLDER>,caught = false ; try { f inst . redefine classes ( new class definition [ ] { null } ) ; } catch ( null pointer exception npe ) { caught = true ; } assert true ( caught ) ;,element throws
single selection model should n't have selection <PLACE_HOLDER> to begin with,assert false ( __str__ @$ get grid element ( ) . get cell ( __num__ @$ __num__ ) . is element present ( by . tag name ( __str__ ) ) ) ; set selection model single ( ) ; header = get grid element ( ) . get header cell ( __num__ @$ __num__ ) ; assert false ( __str__ @$ header . is element present ( by . tag name ( __str__ ) ) ) ;,model have
return to the last opened <PLACE_HOLDER> if triggered from the content area .,if ( m gaia service type != account management screen helper . gaia_service_type_none ) { if ( is added ( ) ) get activity ( ) . finish ( ) ; } return true ;,return opened
big query encodes numeric values to avro using the bytes type with the decimal logical type . avro coder ca n't apply logical <PLACE_HOLDER> to schemas directly @$ so we need to get the schema for the bird class defined below @$ then replace the field used to test numeric with a field that has the appropriate schema .,big decimal birthday money = new big decimal ( __str__ ) ; schema birthday money schema = schema . create ( type . bytes ) ; logical type birthday money logical type = logical types . decimal ( birthday money . precision ( ) @$ birthday money . scale ( ) ) ;,coder apply
test user 1 can not get user 2 's <PLACE_HOLDER>,authorization service . set owners ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set readers ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set runners ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set writers ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; list < paragraph info > paragraph list1 = null ; try { paragraph list1 = notebook server . get paragraph list ( user1 id,user get
process command line property definitions these can potentially occur multiple <PLACE_HOLDER>,list < cl option > cl options = parser . get arguments ( ) ; for ( cl option option : cl options ) { string name = option . get argument ( __num__ ) ; string value = option . get argument ( __num__ ) ; switch ( option . get descriptor ( ) . get id ( ) ) { case cl option . text_argument : throw new illegal argument exception ( __str__ + option . get argument ( ) ) ; case propfile2_opt : log . info ( __str__ @$ name ) ; try ( file input stream fis = new file input stream ( new file ( name ) ) ) { properties tmp = new properties ( ) ; tmp . load (,these occur
represents separator @$ which has no temporal <PLACE_HOLDER>,verify pattern parsing ( __str__ @$ new array list < > ( list . of ( null @$ chrono field . year @$ null @$ chrono field . month_of_year @$ null @$ chrono field . day_of_month @$ null ) ) ) ;,which has
at this point @$ the given <PLACE_HOLDER> does not match any session <PLACE_HOLDER>,throw new api exception ( api exception . type . illegal_parameter @$ action_param_session ) ; case action_unset_active_session : site = extension . get http sessions site ( api utils . get authority ( params . get string ( action_param_site ) ) @$ false ) ; if ( site == null ) { throw new api exception ( api exception . type . illegal_parameter @$ action_param_site ) ; } site . unset active session ( ) ; return api response element . ok ; case action_add_session_token : extension . add http session token ( api utils . get authority ( params . get string ( action_param_site ) ) @$ params . get string ( action_param_token_name ) ) ; return api response element . ok ; case action_remove_session_token : extension,name match
create boring attribute which creates a relation <PLACE_HOLDER>,attribute type < string > attribute type = tx . put attribute type ( __str__ @$ attribute type . data type . string ) ; attribute < string > attribute = attribute type . create ( __str__ ) ; entity type entity type = tx . put entity type ( __str__ ) . has ( attribute type ) ; entity entity = entity type . create ( ) ; entity . has ( attribute ) ; relation impl relation = relation impl . from ( entity . relations ( ) . iterator ( ) . next ( ) ) ;,which creates
set parent calls setup <PLACE_HOLDER> ...,set parent ( parent ) ;,parent calls
create back buffer when not printing @$ and its graphics 2 d then set drawing <PLACE_HOLDER> for that graphics 2 d object,if ( is printing ) g2 = ( graphics2d ) g ; else { back buffer = ( buffered image ) this . create image ( w @$ h ) ; g2 = back buffer . create graphics ( ) ; g2 . set color ( color . white ) ; g2 . fill rect ( __num__ @$ __num__ @$ w @$ h ) ; g2 . set color ( color . black ) ; },printing set
only one <PLACE_HOLDER> must match the local <PLACE_HOLDER>,if ( found > __num__ ) { string msg = __str__ + __str__ + dfs_nameservice_id + __str__ + dfs_ha_namenode_id_key ; throw new hadoop illegal argument exception ( msg ) ; },address match
table has <PLACE_HOLDER>,t capabilities = t capabilities . replace all ( __str__ @$ __str__ ) . to upper case ( ) ;,table has
the <PLACE_HOLDER> in the linear axis will not have <PLACE_HOLDER> after the decimal point .,vertical axis . set label format ( __str__ ) ; categorical axis horizontal axis = new categorical axis ( ) ; horizontal axis . set label interval ( __num__ ) ; horizontal axis . set label fit mode ( axis label fit mode . multi_line ) ; area series . set vertical axis ( vertical axis ) ; area series . set horizontal axis ( horizontal axis ) ;,values have
query success @$ get the right <PLACE_HOLDER> @$ bandwidth and the account name .,account query result = query account ( test key002 @$ blocking stub full ) ;,success get
this view ddl ca n't be used if the table already contains <PLACE_HOLDER> @$ only use it on empty tables,if ( this . new schema . view rep != null && this . start == __num__ ) { batch . add ( this . new schema . view rep . ddl for view ( ) ) ; } this . start = system . nano time ( ) ; if ( change type == change type . create ) { log . info ( __str__ ) ; } else { log . info ( __str__ ) ; } return batch . execute ( ) ;,table contains
some side effect @$ otherwise the jit will remove the <PLACE_HOLDER>,c ++ ;,effect remove
note to translators : xsltc could not find the stylesheet <PLACE_HOLDER> with the name specified by the substitution text .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,note find
this handler reads a single <PLACE_HOLDER> from the message and prints it to the standard output .,try { string string = text message . get text ( ) ; system . out . println ( string ) ; } catch ( javax . jms . jms exception jmse ) { jmse . print stack trace ( ) ; } system . out . println ( __str__ + __str__ ) ;,handler reads
check the json itself makes <PLACE_HOLDER>,json object json obj = new json object ( responsejson ) ; json array results = json obj . getjson array ( __str__ ) ; assert equals ( __num__ @$ response . results . length ) ; json object table = results . getjson object ( __num__ ) ; json array data = table . getjson array ( __str__ ) ; assert equals ( __num__ @$ data . length ( ) ) ; json array row = data . getjson array ( __num__ ) ; assert equals ( __num__ @$ row . length ( ) ) ; long value = row . get long ( __num__ ) ; assert equals ( __num__ @$ value ) ;,itself makes
make sure the data field has the correct <PLACE_HOLDER>,if ( matrix . values [ __num__ ] . length != this . values [ __num__ ] . length || matrix . values . length != this . values . length ) { this . values = new double [ matrix . values . length ] [ matrix . values [ __num__ ] . length ] ; },field has
the compressed bzip 2 stream should start with the identifying <PLACE_HOLDER> bz . caller of cb zip 2 output stream i.e . this class must write these <PLACE_HOLDER> .,if ( super . out != null ) { out . write ( header . get bytes ( standard charsets . utf_8 ) ) ; },bzip write
now let 's build an identical <PLACE_HOLDER> for get,binary object builder bldr = grid ( __num__ ) . binary ( ) . builder ( __str__ ) ; bldr . set field ( __str__ @$ __num__ ) ; bldr . set field ( __str__ @$ __num__ ) ;,"s" build
provide an empty key <PLACE_HOLDER> since trust manager impl does n't support null key <PLACE_HOLDER>s . trust manager impl will use cert <PLACE_HOLDER> to lookup certificates .,key store store = key store . get instance ( key store . get default type ( ) ) ; store . load ( null ) ; m delegate = new trust manager impl ( store @$ null @$ cert store ) ;,impl use
user has not issue admin <PLACE_HOLDER> on these 2 issues,issue dto not authorized issue1 = db . issues ( ) . insert ( rule @$ project2 @$ project2 @$ i -> i . set type ( bug ) . set status ( status_open ) . set resolution ( null ) ) ; issue dto not authorized issue2 = db . issues ( ) . insert ( rule @$ project2 @$ project2 @$ i -> i . set type ( bug ) . set status ( status_open ) . set resolution ( null ) ) ; bulk change ws response response = call ( builder ( ) . set issues ( as list ( authorized issue1 . get key ( ) @$ not authorized issue1 . get key ( ) @$ not authorized issue2 . get key (,user has
create a verifier but do not require exact verification because the verifier sockets may receive <PLACE_HOLDER> out of order @$ due to the fact that socket exporter acknowledges <PLACE_HOLDER> when written on the sending socket @$ not when received on the destination host .,m_verifier = new export test expected data ( m_server sockets @$ false @$ false @$ kfactor + __num__ ) ;,sockets receive
note to translators : the following represents an internal <PLACE_HOLDER> . dom adapter is a java class in xsltc .,return new object [ ] [ ] { { basis library . run_time_internal_err @$ __str__ } @$ { basis library . run_time_copy_err @$ __str__ } @$ { basis library . data_conversion_err @$ __str__ } @$ { basis library . external_func_err @$ __str__ } @$ { basis library . equality_expr_err @$ __str__ } @$ { basis library . invalid_argument_err @$ __str__ } @$ { basis library . format_number_err @$ __str__ } @$ { basis library . iterator_clone_err @$ __str__ } @$ { basis library . axis_support_err @$ __str__ } @$ { basis library . typed_axis_support_err @$ __str__ } @$ { basis library . stray_attribute_err @$ __str__ } @$ { basis library . stray_namespace_err @$ __str__ } @$ { basis library . namespace_prefix_err @$ __str__ } @$ { basis library,the represents
get user inputs and set the map source this bit gets the <PLACE_HOLDER> from the manifest,alert dialog builder . set cancelable ( false ) . set positive button ( __str__ @$ new dialog interface . on click listener ( ) { public void on click ( dialog interface dialog @$ int id ) { map box tile source b = new map box tile source ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ ) ; b . set mapbox mapid ( user input box id . get text ( ) . to string ( ) ) ; b . set access token ( user input token . get text ( ) . to string ( ) ) ; m map view . set tile source ( b ) ; } } ) . set negative button ( __str__ @$ new dialog,bit gets
start editing when a key is typed . ui classes can disable this <PLACE_HOLDER> by setting the client property j table.auto starts edit to boolean.false .,if ( ! ret value && condition == when_ancestor_of_focused_component && is focus owner ( ) && ! boolean . false . equals ( get client property ( __str__ ) ) ) { component editor component = get editor component ( ) ; if ( editor component == null ) { if ( e == null || e . getid ( ) != key event . key_pressed ) { return false ; } int code = e . get key code ( ) ; if ( code == key event . vk_shift || code == key event . vk_control || code == key event . vk_alt ) { return false ; } int lead row = get selection model ( ) . get lead selection index ( ) ;,classes disable
de serialize the <PLACE_HOLDER>,process instance migration document migration document = process instance migration document impl . from json ( serialized document ) ; assert null ( migration document . get migrate to process definition id ( ) ) ; assert equals ( definition key @$ migration document . get migrate to process definition key ( ) ) ; assert equals ( definition ver @$ migration document . get migrate to process definition version ( ) ) ; assert equals ( definition tenant id @$ migration document . get migrate to process definition tenant id ( ) ) ; assert that ( migration document . get activity migration mappings ( ) ) . using field by field element comparator ( ) . contains exactly ( one to one1 @$ one to one2,de serialize
concurrent hash map does not need <PLACE_HOLDER> . here,for ( abstract thread group thread group : groups ) { stopped all = stopped all && thread group . verify threads stopped ( ) ; } return stopped all ;,map need
pdx and tx will not use <PLACE_HOLDER>,user bits = entry bits . set with versions ( user bits @$ true ) ;,pdx use
a @$ should retrieve b 's conflicting <PLACE_HOLDER>,clienta . down ( ) ; assert file list equals ( clienta . get local files exclude locked and no read ( ) @$ clientb . get local files exclude locked and no read ( ) ) ; assert sql database equals ( clienta . get database file ( ) @$ clientb . get database file ( ) ) ;,a retrieve
this is tough for hotspot @$ but graal eliminates all <PLACE_HOLDER> .,consume ( el ) ;,graal eliminates
data set <PLACE_HOLDER>,final list < adapter item > data set one = new array list < > ( ) ; adapter item data set one0 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one1 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one2 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one3 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one4 = new adapter item ( __num__ @$ __str__ ) ; data set one . add ( data set one0 ) ; data set one . add ( data set one1 ) ; data set one . add ( data set one2 ) ; data set,data set
a filter with a start date of now minus an hour should not accept the <PLACE_HOLDER>,filter = new log filter ( level . info @$ local date time . now ( ) . minus hours ( __num__ ) @$ null ) ; assert that ( filter . accepts file ( path ) ) . is true ( ) ;,filter accept
expression consumed <PLACE_HOLDER>,if ( state . backtracking == __num__ && input . index ( ) > first ) { string expr = input . to string ( first @$ input . lt ( - __num__ ) . get token index ( ) ) ; return expr ; },expression consumed
create the exception here @$ so that the call stack reflects the failed <PLACE_HOLDER> . if you set the exception in on shutdown @$ the resulting stack is not of interest .,if ( shutdown ) { reply exception re = new reply exception ( new distributed system disconnected exception ( __str__ ) ) ; this . exception = re ; return false ; },stack reflects
case when 2 txn have same <PLACE_HOLDER> and keys,string key = __str__ ; string old val = __str__ ; map < string @$ string > key vals1 = new hash map < string @$ string > ( ) ; key vals1 . put ( key @$ old val ) ; map < string @$ string > key vals2 = new hash map < string @$ string > ( ) ; string new val = __str__ ; key vals2 . put ( key @$ new val ) ; list < transaction state . per source transactional update > old db updates = generate updates for schema2 ( source ids @$ key vals1 @$ scn ) ; list < transaction state . per source transactional update > new db updates = generate updates for schema2 ( source ids,txn have
if the class is null @$ the driver has no user <PLACE_HOLDER>,if ( user code function type != null ) { this . stub = init stub ( user code function type ) ; },driver has
the map will represent the request <PLACE_HOLDER>,map < string @$ string > request = new hash map < > ( ) ; request . put ( __str__ @$ _sessionid ) ; int random = _server . get random ( __num__ ) ; int uri = random % __num__ ; boolean blocking = ( random / __num__ ) > __num__ ; int delay = ( blocking && uri % __num__ == __num__ ) ? random / __num__ : __num__ ; request . put ( __str__ @$ uri + __str__ ) ;,map represent
if the user 's settng a <PLACE_HOLDER> @$ warn if they do n't allow conditions .,if ( null != query ) { if ( query . index of ( substitute_token ) == - __num__ ) { log . warn ( __str__ + substitute_token + __str__ + query + __str__ ) ; } },user settng
destroy is called on each member . if the <PLACE_HOLDER> destroy is successful on one member @$ we deem the destroy action successful @$ since if one member destroy successfully @$ the subsequent destroy on a another member would probably throw <PLACE_HOLDER> destroyed exception,list < cli function result > results list = execute and get function result ( region destroy function . instance @$ region path @$ region members list ) ; result model result = result model . create member status result ( results list ) ; xml entity xml entity = find xml entity ( results list ) ;,destroy throw
modern apps always support <PLACE_HOLDER> .,application density = display metrics . density_device ; application scale = __num__ ; application inverted scale = __num__ ; final int expandable = __num__ ; final int large_screens = __num__ ; final int xlarge_screens = __num__ ; int size info = __num__ ;,apps support
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; rule key input rule key = new rule key ( __str__ ) ; build rule rule = new failing input rule key build rule ( target @$ filesystem @$ params ) ; graph builder . add to index ( rule ) ;,which writes
m total length contains the <PLACE_HOLDER> already,child left = get padding left ( ) + right - left - m total length ; break ; case gravity . center_horizontal :,length contains
ensure id <PLACE_HOLDER> is not blocking id 2 @$ and id 2 is following id <PLACE_HOLDER>,twitter1 . destroy block ( rw private . id ) ; rw private message . create friendship ( id1 . id ) ; string message = __str__ + new date ( ) . to string ( ) ; direct message sent = twitter1 . send direct message ( rw private . id @$ message ) ; assert equals ( rw private . id @$ sent . get recipient id ( ) ) ; assert equals ( id1 . id @$ sent . get sender id ( ) ) ; assert equals ( message @$ sent . get text ( ) ) ; assert equals ( sent @$ twitter object factory . create direct message ( twitter object factory . get rawjson ( sent ) ) ) ;,id following
these should only affect the given <PLACE_HOLDER>,session s = open session ( ) ; transaction t = s . begin transaction ( ) ; int count = s . create query ( __str__ ) . set string ( __str__ @$ __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; count = s . create query ( __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; t . commit ( ) ; s . close ( ) ; data . cleanup ( ) ;,these affect
node 0 has 2 <PLACE_HOLDER>,assert equals ( __num__ @$ node0 . get leaf count ( ) ) ;,node has
the partition <PLACE_HOLDER> field stores the table <PLACE_HOLDER> .,_table name with type = message . get partition name ( ) ; _logger = logger factory . get logger ( _table name with type + __str__ + timeboundary refresh message handler . class ) ;,field stores
recursively collect <PLACE_HOLDER> for all superinterfaces .,for ( class doc superintf : intf . interfaces ( ) ) { if ( ! collect remote methods ( superintf @$ table ) ) { errors = true ; } } return ! errors ;,recursively collect
now activate the address mask tool @$ which should mask out the last <PLACE_HOLDER> of the add instruction .,press button by name ( component @$ __str__ ) ;,which mask
if user typed the 'bye ' command @$ wait until the server closes the <PLACE_HOLDER> .,if ( __str__ . equals ( line . to lower case ( ) ) ) { channel . get close future ( ) . await uninterruptibly ( ) ; break ; },server closes
somone killed this <PLACE_HOLDER> @$ behave as if timer cancelled,synchronized ( queue ) { new tasks may be scheduled = false ; queue . clear ( ) ; },somone killed
create a number of consumers to read of the messages and start them with a handler which simply stores the message <PLACE_HOLDER> in a map and checks for a duplicate,for ( int i = __num__ ; i < consumer_count ; i ++ ) { receivers [ i ] = new threaded message receiver ( test_queue_name @$ new i message handler ( ) { @ override public void on message ( message message ) throws exception { synchronized ( lock ) { int current = message count . increment and get ( ) ; if ( current % __num__ == __num__ ) { logger . info ( __str__ + message . getjms messageid ( ) + __str__ + ( ( text message ) message ) . get text ( ) ) ; } if ( messages . contains key ( message . getjms messageid ( ) ) ) { duplicate signal . count down ( ) ; logger,which stores
verify that the text field has the min <PLACE_HOLDER> since the current offset is 0,assert equals ( program . get min address ( ) @$ ai . get address ( ) ) ; run swing ( ( ) -> ai . set value ( __str__ ) ) ; press button by text ( d . get component ( ) @$ __str__ ) ;,field has
ensure there is a collection entry for manies ee one state <PLACE_HOLDER> and that the role @$ persister @$ and key are set properly .,collection entry ce manies = get collection entry ( s @$ maniesee one state orig ) ; assert not null ( ce manies ) ; assert equals ( role @$ ce manies . get role ( ) ) ; assert same ( session factory ( ) . get collection persister ( role ) @$ ce manies . get loaded persister ( ) ) ; assert equals ( one . get id ( ) @$ ce manies . get key ( ) ) ;,manies ee
the following will add an <PLACE_HOLDER> into the spark preferences window,preference mypreference = new otr preferences ( ) ; spark manager . get preference manager ( ) . add preference ( mypreference ) ;,following add
we had a previous <PLACE_HOLDER> of who had focus . clear it .,if ( m focused != child ) { if ( m focused != null ) { m focused . un focus ( focused ) ; } m focused = child ; } if ( m parent != null ) { m parent . request child focus ( this @$ focused ) ; },who had
if application was stopped and execution services did not finish <PLACE_HOLDER> @$ these codes will be executed .,platform . run later ( ( ) -> { if ( controllers . get stage ( ) != null ) { controllers . get stage ( ) . close ( ) ; emit status ( loading state . done ) ; } } ) ;,services finish
should trigger work <PLACE_HOLDER> of worker 5,worker1 . dispose ( ) ;,trigger work
apps can set the interception <PLACE_HOLDER> other than the direct parent .,final view group parent ; if ( m touch interception view group == null ) { parent = ( view group ) get parent ( ) ; } else { parent = m touch interception view group ; },apps set
we documented that using distance in the where clause utilizes the <PLACE_HOLDER> which is n't precise so treating lte & lt the same should be acceptable,switch ( parent operator name ) { case lte operator . name : case lt operator . name : return lat lon point . new distance query ( column name @$ lon lat . gety ( ) @$ lon lat . getx ( ) @$ distance ) ; case gte operator . name : if ( distance - geo utils . tolerance <= __num__ ) { return queries . new match all query ( ) ; } case gt operator . name : return queries . not ( lat lon point . new distance query ( column name @$ lon lat . gety ( ) @$ lon lat . getx ( ) @$ distance ) ) ; case eq operator . name : return eq distance ( parent,clause utilizes
kindle soho fire tv gen 2 use the default value for cases where platform limitations may prevent <PLACE_HOLDER> of the calculated maximum input size from being allocated .,return format . no_value ;,limitations prevent
the new result must not have an associated local <PLACE_HOLDER> .,register spec new reg spec = register spec . make ( ssa meth . make new ssa reg ( ) @$ reg . get type bearer ( ) ) ; ssa insn to add = ssa insn . make from rop ( new plain insn ( rops . op move ( new reg spec . get type ( ) ) @$ source position . no_info @$ new reg spec @$ register spec list . make ( reg ) ) @$ block ) ; insns . add ( insn index @$ to add ) ; int new reg = new reg spec . get reg ( ) ;,result have
first do a sanity check and see if this address has any <PLACE_HOLDER> .,if ( ! has conflict ( addr ) ) { return ; } monitor . set message ( __str__ ) ; boolean ask user = chosen conflict option == ask_user ;,address has
if the current cluster does n't contain the <PLACE_HOLDER> @$ fallback to something machine local and then rack local .,list < node > rack nodes = get network topology ( ) . get datanodes in rack ( network location ) ; if ( rack nodes != null ) { for ( node rack node : rack nodes ) { if ( ( ( datanode descriptor ) rack node ) . get ip addr ( ) . equals ( host ) ) { node = ( datanode descriptor ) rack node ; break ; } } if ( node == null && ! rack nodes . is empty ( ) ) { node = ( datanode descriptor ) ( rack nodes . get ( thread local random . current ( ) . next int ( rack nodes . size ( ) ) ) ) ; } },cluster contain
check that the job name contains only allowed <PLACE_HOLDER>,if ( ! name_version_pattern . matcher ( job id name ) . matches ( ) ) { errors . add ( format ( __str__ @$ recomputed id . get name ( ) ) ) ; },name contains
ensure comments are stripped . these cause syntax <PLACE_HOLDER> if not stripped .,final source file src file1 = source file . from code ( __str__ @$ line_joiner . join ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,these cause
there will be one proposed item but the tree will refuse the <PLACE_HOLDER> because of the bad state on disk,try { final list < invoice item > generated items = fixed and recurring invoice item generator . generate items ( account @$ uuid . randomuuid ( ) @$ events @$ existing invoices @$ start date @$ account . get currency ( ) @$ new hash map < uuid @$ subscription future notification dates > ( ) @$ internal call context ) . get items ( ) ; fail ( ) ; } catch ( final invoice api exception e ) { assert equals ( e . get code ( ) @$ error code . unexpected_error . get code ( ) ) ; assert true ( e . get cause ( ) . get message ( ) . starts with ( __str__ ) ) ; },tree refuse
free and that values can move past them . we do n't need to be concerned with exposing the getter or setter here but the decomposer does not have a <PLACE_HOLDER> of exposing properties @$ only variables .,helper move expression ( __str__ @$ __str__ @$ __str__ ) ; helper move expression ( __str__ @$ __str__ @$ __str__ ) ;,decomposer have
legacy does n't report min frame <PLACE_HOLDER>,if ( m static info . is hardware level legacy ( ) ) { frame duration = new long ( __num__ ) ; },legacy report
page transformers can do complex <PLACE_HOLDER> that benefit from hardware layers .,if ( m page transformer != null ) { enable layers ( new state != scroll_state_idle ) ; },transformers do
figure out <PLACE_HOLDER> class loader to use for loading the provider class . if there is a context class loader then use it .,class loader context = security support . get context class loader ( ) ; class loader system = security support . get system class loader ( ) ; class loader chain = system ; while ( true ) { if ( context == chain ) { class loader current = object factory . class . get class loader ( ) ; chain = system ; while ( true ) { if ( current == chain ) { return system ; } if ( chain == null ) { break ; } chain = security support . get parent class loader ( chain ) ; } return current ; } if ( chain == null ) { break ; } chain = security support . get parent class loader,loader use
now @$ <PLACE_HOLDER>ck should show healthy <PLACE_HOLDER> and should not show any open files,out str = run fsck ( conf @$ __num__ @$ true @$ open file . to string ( ) @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ; assert true ( out str . contains ( namenode fsck . healthy_status ) ) ; assert false ( out str . contains ( __str__ ) ) ; assert false ( out str . contains ( __str__ ) ) ; assert false ( out str . contains ( __str__ + num all units ) ) ; assert true ( out str . contains ( __str__ + num all units ) ) ; util . cleanup ( fs @$ top dir ) ;,fsck show
this collation implies that this serial aggregate requires its <PLACE_HOLDER> to be sorted in an order that is one of permutations of the fields from this collation,immutable bit set group by = aggr . get group set ( ) ; list < rel data type field > row type list = aggr . get row type ( ) . get field list ( ) ; list < rel field collation > collation fields = new array list < > ( ) ; for ( int index = group by . next set bit ( __num__ ) ; index != - __num__ ; index = group by . next set bit ( index + __num__ ) ) { preconditions . check state ( index < row type list . size ( ) ) ; collation fields . add ( new rel field collation ( index ) ) ; } return rel collations . of (,aggregate requires
p 1 has primary key <PLACE_HOLDER> on column a : group by c should not use its <PLACE_HOLDER> to speed up .,check seq scan ( pn @$ __str__ @$ __str__ @$ __str__ ) ; assert not null ( aggregate plan node . get inline aggregation node ( pn ) ) ; assert null ( pn . get inline plan node ( plan node type . limit ) ) ;,group use
check only for <PLACE_HOLDER> @$ that 's enough because default ca n't exceed <PLACE_HOLDER>,if ( maximum application lifetime <= __num__ ) { return ( lifetime requested by app <= __num__ ) ? default application lifetime : lifetime requested by app ; } if ( lifetime requested by app <= __num__ ) { return default application lifetime ; } else if ( lifetime requested by app > maximum application lifetime ) { return maximum application lifetime ; } return lifetime requested by app ; read lock . unlock ( ) ;,default exceed
if this is a bulk op @$ and concurrency checks are enabled @$ we need to save the version <PLACE_HOLDER> in case we retry . make record bulk op version <PLACE_HOLDER> after record sequence number @$ so that record bulk op start in a retry bulk op would not incorrectly remove the saved version <PLACE_HOLDER> in recorded bulk op version <PLACE_HOLDER>s,if ( lr . get concurrency checks enabled ( ) && ( event . get operation ( ) . is put all ( ) || event . get operation ( ) . is remove all ( ) ) && lr . get server proxy ( ) == null ) { record bulk op event ( event @$ membershipid ) ; },start remove
note to translators : this message is reported if the stylesheet that is being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,return new object [ ] [ ] { { __str__ @$ __str__ } @$ { er_no_curlybrace @$ __str__ } @$ { er_illegal_attribute @$ __str__ } @$ { er_null_sourcenode_applyimports @$ __str__ } @$ { er_cannot_add @$ __str__ } @$ { er_null_sourcenode_handleapplytemplates @$ __str__ } @$ { er_no_name_attrib @$ __str__ } @$ { er_template_not_found @$ __str__ } @$ { er_cant_resolve_name_avt @$ __str__ } @$ { er_requires_attrib @$ __str__ } @$ { er_must_have_test_attrib @$ __str__ } @$ { er_bad_val_on_level_attrib @$ __str__ } @$ { er_processinginstruction_name_cant_be_xml @$ __str__ } @$ { er_processinginstruction_notvalid_ncname @$ __str__ } @$ { er_need_match_attrib @$ __str__ } @$ { er_need_name_or_match_attrib @$ __str__ } @$ { er_cant_resolve_nsprefix @$ __str__ } @$ { er_illegal_value @$ __str__ } @$ { er_no_ownerdoc @$ __str__ } @$ { er_elemtemplateelem_err @$ __str__ } @$,text specifies
method name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( operation id ) ) { logger . warn ( operation id + __str__ + camelize ( sanitize name ( __str__ + operation id ) ) ) ; operation id = __str__ + operation id ; } return camelize ( sanitize name ( operation id ) ) ;,name use
there was a bug where iteration of entries caused the underlying <PLACE_HOLDER> to change resulting in a subsequent equals changing,while ( iter . has next ( ) ) { map . entry < string @$ object > entry = iter . next ( ) ; } assert equals ( obj2 @$ obj1 ) ;,iteration caused
be aware the legacy parser is not signaling truncate <PLACE_HOLDER>,parser . signal truncate table ( table id @$ ctx ) ; super . enter truncate table ( ctx ) ;,parser signaling
form should not have a description <PLACE_HOLDER>,check tooltip ( $ ( form element . class ) . first ( ) @$ null ) ;,form have
for some reason ` test 0 <PLACE_HOLDER> ` and ` test 1 <PLACE_HOLDER> ` are n't equal . this is good @$ but unexpected .,assert that ( test1 type ) . is not same instance as ( test0 type ) ;,reason test
note : no need to call inc failed accept here because it will be done in our caller . no need to log error here since caller will log <PLACE_HOLDER>,if ( connection != null && ! finished connecting ) { close con ( __str__ @$ connection ) ; connection = null ; },caller log
end of the post fork <PLACE_HOLDER> .,trace . trace end ( trace . trace_tag_activity_manager ) ; if ( parsed args . m invoke with != null ) { wrapper init . exec application ( parsed args . m invoke with @$ parsed args . m nice name @$ parsed args . m target sdk version @$ vm runtime . get current instruction set ( ) @$ pipe fd @$ parsed args . m remaining args ) ; throw new illegal state exception ( __str__ ) ; } else { if ( ! is zygote ) { return zygote init . zygote init ( parsed args . m target sdk version @$ parsed args . m remaining args @$ null ) ; } else { return zygote init . child zygote init ( parsed args,end fork
second insert will violate the primary key <PLACE_HOLDER>,volt queuesql ( insertp1 @$ __num__ @$ __num__ ) ; volt executesql ( ) ;,insert violate
check that a field of type string does not accept <PLACE_HOLDER>,try { completed form . set answer ( __str__ @$ true ) ; fail ( __str__ ) ; } catch ( illegal argument exception e ) { },field accept
make sure the subject has a <PLACE_HOLDER>,assert false ( client principals . is empty ( ) ) ;,subject has
to do : handle if user does n't provide <PLACE_HOLDER> best,if ( flags . usek best ) { int k = flags . k best ; crf . classify and write answersk best ( test file @$ k @$ reader and writer ) ; } else if ( flags . print label value ) { crf . print label information ( test file @$ reader and writer ) ; } else { log . info ( __str__ ) ; crf . classify and write answers ( test file @$ reader and writer @$ true ) ; },user provide
allow that balance is not exact . fyi @$ get region server <PLACE_HOLDER>s does not include master <PLACE_HOLDER> though it is a regionserver so we have to check master and then below the regionservers .,for ( jvm cluster util . region server thread rst : cluster . get region server threads ( ) ) { regions = rst . get region server ( ) . get regions ( ) ; int rs actual count = regions . size ( ) ; check count ( rs actual count @$ rs count ) ; } h master old master = cluster . get master ( ) ; cluster . kill master ( old master . get server name ( ) ) ; old master . join ( ) ; while ( cluster . get master ( ) == null || cluster . get master ( ) . get server name ( ) . equals ( old master . get server name ( ) ),threads include
null means get the first <PLACE_HOLDER> .,return get next oid ( null @$ user data ) ;,means get
if no restrictions were saved @$ m user manager.get application restrictions returns null @$ but dpm method should return an empty <PLACE_HOLDER> as per java doc,return bundle != null ? bundle : bundle . empty ; m injector . binder restore calling identity ( id ) ;,method return
partition node <PLACE_HOLDER> from node 2 .,partition ( member1 @$ member2 ) ;,partition node
this test case generates a relation <PLACE_HOLDER> where multiple positions share the same symbol,assert query ( __str__ ) ;,case generates
extended relocations add an addition <PLACE_HOLDER>,if ( nreloc > __num__ ) { nreloc ++ ; } file offset += ( nreloc * image_relocation . totalsize ) ;,relocations add
schedule an event immediately to ensure the <PLACE_HOLDER> are being honored .,handler . post ( new runnable ( ) { @ override public void run ( ) { events . add ( __str__ ) ; } } ) ; assert true ( latch . await ( __num__ @$ seconds ) ) ; assert equals ( as list ( __str__ @$ __str__ ) @$ events ) ;,event ensure
also the follower may have just sent a leader check @$ which receives no <PLACE_HOLDER>,cluster . stabilise ( math . max ( default millis ( follower_check_timeout_setting ) + default millis ( follower_check_interval_setting ) + default_delay_variability + default_cluster_state_update_delay + default_cluster_state_update_delay @$ default millis ( leader_check_timeout_setting ) + default millis ( leader_check_interval_setting ) + default_delay_variability ) ) ;,which receives
authorization header must have a <PLACE_HOLDER>,if ( auth header base64 string == null || auth header base64 string . is empty ( ) ) { throw new http authentication exception ( __str__ + __str__ ) ; } return auth header base64 string ;,header have
instance on which to get the <PLACE_HOLDER>,mv . visit var insn ( aload @$ __num__ ) ;,which get
finally @$ the default overall toolchain <PLACE_HOLDER> .,return toolchain . get default platform ( ) ;,default toolchain
if we have a <PLACE_HOLDER> for a compressed apk @$ copy it to the reference location . note that copying the <PLACE_HOLDER> here will cause it to override the reference <PLACE_HOLDER> every ota even though the existing reference <PLACE_HOLDER> may have more data . we ca n't copy during decompression since the directories are not set up at that point .,if ( profile file . exists ( ) ) { try { if ( ! m installer . copy system profile ( profile file . get absolute path ( ) @$ pkg . application info . uid @$ pkg . package name @$ art manager . get profile name ( null ) ) ) { log . e ( tag @$ __str__ ) ; } else { use profile for dexopt = true ; } } catch ( exception e ) { log . e ( tag @$ __str__ + profile file . get absolute path ( ) + __str__ @$ e ) ; } },profile have
validate that other vm no longer hosts colocated <PLACE_HOLDER>,host . get host ( __num__ ) . getvm ( othervm ) . invoke ( new serializable runnable ( ) { @ override public void run ( ) { for ( int i = __num__ ; i < region path . length ; i ++ ) { partitioned region pr = ( partitioned region ) get cache ( ) . get region ( region path [ i ] ) ; bucket bucket = pr . get region advisor ( ) . get bucket ( __num__ ) ; assert false ( __str__ @$ bucket . is hosting ( ) ) ; bucket region bucket region = bucket . get bucket advisor ( ) . get proxy bucket region ( ) . get hosted bucket region ( ) ; assert,vm hosts
check whether the server returned the status <PLACE_HOLDER> that we 've set .,assert equals ( __str__ @$ __num__ @$ status event collector . collected stat msg events . size ( ) ) ; assert equals ( __str__ @$ old status message @$ ( ( property change event ) status event collector . collected stat msg events . get ( __num__ ) ) . get old value ( ) ) ; assert equals ( __str__ @$ new status message @$ ( ( property change event ) status event collector . collected stat msg events . get ( __num__ ) ) . get new value ( ) ) ;,server returned
call into chatty asserts test <PLACE_HOLDER> to get the nice formatting .,if ( ! actual . starts with ( expected ) ) { assert that ( actual ) . is equal to ( expected ) ; },call asserts
no reports right after we created this <PLACE_HOLDER> .,assert equals ( __num__ @$ get region reports for table ( quota manager . snapshot region sizes ( ) @$ tn ) ) ;,reports created
we need to strip 'delete ' or else jpa operations.delete will generate the wrong <PLACE_HOLDER>,string delete query string = query string . substring ( __str__ . length ( ) ) ; result handle delete count ; if ( use named params ) { result handle parameters = generate parameters object ( named parameter to index @$ method creator ) ; delete count = method creator . invoke static method ( method descriptor . of method ( jpa operations . class @$ __str__ @$ long . class @$ class . class @$ string . class @$ parameters . class ) @$ method creator . read instance field ( entity class field descriptor @$ method creator . get this ( ) ) @$ method creator . load ( delete query string ) @$ parameters ) ; } else { result handle params array =,"delete" generate
restart cluster and verify that file exists . then take another <PLACE_HOLDER> to verify that the namenode restart accounted for the rolled edit logs .,try { cluster = new minidfs cluster . builder ( conf ) . num data nodes ( num datanodes ) . format ( false ) . build ( ) ; cluster . wait active ( ) ; file sys = cluster . get file system ( ) ; check file ( file sys @$ file1 @$ replication ) ; cleanup file ( file sys @$ file1 ) ; secondary = start secondary name node ( conf ) ; secondary . do checkpoint ( ) ; secondary . shutdown ( ) ; } finally { file sys . close ( ) ; cleanup ( secondary ) ; secondary = null ; cleanup ( cluster ) ; cluster = null ; },then take
editor overlay has no <PLACE_HOLDER> if it 's not active @$ effectively making this loop a noop .,element e = editor overlay . get first child element ( ) ; while ( e != null ) { e . remove class name ( error_class_name ) ; e = e . get next sibling element ( ) ; } column errors . clear ( ) ;,overlay has
by default @$ we should see no restriction ; the unicode set should allow all <PLACE_HOLDER> .,set = sc . get allowed chars ( ) ; tmp set = new unicode set ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ tmp set @$ set ) ;,set allow
if the context url has a different <PLACE_HOLDER> @$ discard it because we ca n't use it .,if ( protocol != null && context != null && ! protocol . equals ( context . protocol ) ) { context = null ; },url has
note to translators : this message is used to indicate the severity of another message . the substitution text contains two error <PLACE_HOLDER> . the spacing before the second substitution text indents it the same amount as the first in english .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text contains
note that chunk <PLACE_HOLDER> only changes chunk <PLACE_HOLDER> in case we actually save chunks otherwise there is a risk file and chunks are not compatible,if ( ! saved chunks ) { try { save chunks ( chunk size ) ; } catch ( io exception ioe ) { throw new mongo exception ( __str__ @$ ioe ) ; } } super . save ( ) ;,size changes
open a connection @$ thus we can not set <PLACE_HOLDER> here !,key chain . choose private key alias ( m activity @$ new key chain alias callback ( ) { @ override public void alias ( string alias ) { timber . d ( __str__ @$ alias ) ; set alias ( alias ) ; } } @$ null @$ null @$ null @$ - __num__ @$ get alias ( ) ) ;,connection set
end else : decode end else : get <PLACE_HOLDER>,throw new java . io . io exception ( __str__ ) ;,end get
the above method will perform the <PLACE_HOLDER> as long as the user does not cancel the request,if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . save_policy_file ) == __num__ ) { string filename = ( ( j text field ) tw . get component ( tool window . mw_filename_textfield ) ) . get text ( ) ; if ( filename == null || filename . length ( ) == __num__ ) { tool dialog td = new tool dialog ( policy tool . get message ( __str__ ) @$ tool @$ tw @$ true ) ; td . display save as dialog ( tool dialog . noaction ) ; } else { try { tool . save policy ( filename ) ; message format form = new message format ( policy tool .,method perform
the test is just checking to make sure thaat the <PLACE_HOLDER> and consumer does not hang due to the network hops take to route the message form the <PLACE_HOLDER> to the consumer .,assert true ( __str__ @$ r > __num__ ) ; assert true ( __str__ @$ p > __num__ ) ;,message form
the following 'deltas ' includes all <PLACE_HOLDER> of delta files including insert & delete deltas .,final list < parsed delta > deltas = new array list < parsed delta > ( ) ; list < parsed delta > working = new array list < parsed delta > ( ) ; list < path > original directories = new array list < > ( ) ; final list < path > obsolete = new array list < > ( ) ; final list < path > aborted directories = new array list < > ( ) ; list < hdfs file status with id > children with id = try list located hdfs status ( use file ids @$ fs @$ candidate directory ) ; txn base best base = new txn base ( ) ; final list < hdfs file status with id,"deltas" includes
rename tables to make them satisfy the <PLACE_HOLDER> and enable acid tables .,primary . run ( __str__ + primary db name ) . run ( __str__ ) . run ( __str__ ) . run ( __str__ ) ; dump with clause = arrays . as list ( __str__ + hive conf . conf vars . repl_bootstrap_acid_tables . varname + __str__ @$ __str__ + repl utils . repl_dump_include_acid_tables + __str__ ) ; replicated tables = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; bootstrap tables = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } ; replicate and verify ( repl policy @$ null @$ last repl id @$ dump with clause @$ null @$ bootstrap tables @$ replicated tables ) ;,them satisfy
if none of the above patterns match @$ use the standard <PLACE_HOLDER>,if ( mother cat . equals ( __str__ ) || mother cat . equals ( __str__ ) ) { if ( ! make copula head ) { for ( tregex pattern pattern : head of copula tregex ) { tregex matcher matcher = pattern . matcher ( t ) ; if ( matcher . matches at ( t ) ) { return matcher . get node ( __str__ ) ; } } } },none match
if the left index has not reached the right <PLACE_HOLDER> of array must now sort the right partition .,if ( lo <= hi0 ) quick sort ( lo @$ hi0 @$ key @$ is ascending ) ;,index reached
processor has <PLACE_HOLDER>,capabilities . clear ( ) ; capabilities . add ( __str__ ) ; sethms client ( __str__ @$ ( string [ ] ) ( capabilities . to array ( new string [ __num__ ] ) ) ) ; parts = client . get partitions by names ( db name @$ tbl name @$ part values @$ false @$ null ) ; for ( partition part : parts ) { assert equals ( __str__ @$ - __num__ @$ part . get sd ( ) . get num buckets ( ) ) ; } tbl name = __str__ ; properties = new string builder ( ) ; properties . append ( __str__ ) ; properties . append ( __str__ ) ; properties . append ( capabilities_key ) . append (,processor has
now format first nn and copy the storage <PLACE_HOLDER> from that node to the others .,int nn index = nn counter ; collection < uri > prevnn dirs = null ; for ( nn conf nn : nameservice . getn ns ( ) ) { init name node conf ( conf @$ ns id @$ ns counter @$ nn . get nn id ( ) @$ manage name dfs dirs @$ manage name dfs dirs @$ nn index ) ; collection < uri > namespace dirs = fs namesystem . get namespace dirs ( conf ) ; if ( format ) { for ( uri name dir uri : namespace dirs ) { file name dir = new file ( name dir uri ) ; if ( name dir . exists ( ) && ! file util . fully delete ( name dir,format nn
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ ) ;,tools |
ensure an empty file is created if there is no string map . this avoids confusing some build <PLACE_HOLDER> that expect to see the file @$ even if it is empty .,if ( compiler . get string map ( ) == null ) { if ( ! ( new file ( config . string map output path ) . create new file ( ) ) ) { throw new io exception ( __str__ + config . string map output path ) ; } } else { compiler . get string map ( ) . save ( config . string map output path ) ; },some build
verify that just changing the build <PLACE_HOLDER> <PLACE_HOLDER> key changes the calculated <PLACE_HOLDER> key .,assert not equals ( build result ( create builder ( graph builder ) . set reflectively ( __str__ @$ explicit build target source path . of ( fake1 . get build target ( ) @$ paths . get ( __str__ ) ) ) ) @$ build result ( create builder ( graph builder ) . set reflectively ( __str__ @$ explicit build target source path . of ( fake2 . get build target ( ) @$ paths . get ( __str__ ) ) ) ) ) ;,the build
validate that the first savepoint does not discard its private <PLACE_HOLDER> .,verify ( subtask state1 @$ never ( ) ) . discard state ( ) ; verify ( subtask state2 @$ never ( ) ) . discard state ( ) ;,savepoint discard
test empty group id and use site url that causes uri syntax <PLACE_HOLDER>,url = __str__ ; mockito . when ( mock result . get site url ( ) ) . then return ( url ) ; mockito . when ( mock result . get group id ( ) ) . then return ( __str__ ) ; assert . assert equals ( url @$ utils . get group id ( mock result ) ) ;,causes uri
validate property has <PLACE_HOLDER> that can not be resolved,object [ ] [ ] input = { { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,property has
list contains an <PLACE_HOLDER> from another thread .,list . add ( obj from another thread . get ( ) ) ;,list contains
inner loop tries all 9 compression <PLACE_HOLDER> .,for ( int i = __num__ ; i <= __num__ ; i ++ ) { system . out . println ( __str__ + j + __str__ + i + __str__ ) ; byte [ ] zipped = new byte [ __num__ * data_size ] ; deflater deflater = new deflater ( i ) ; deflater . set input ( input ) ; deflater . finish ( ) ; deflater . deflate ( zipped ) ; deflater . end ( ) ; byte [ ] output = new byte [ data_size ] ; inflater inflater = new inflater ( ) ; inflater . set input ( zipped ) ; inflater . finished ( ) ; inflater . inflate ( output ) ; inflater . end ( ) ; assert,loop tries
verify that interim <PLACE_HOLDER> mask includes one or more <PLACE_HOLDER> not included in the <PLACE_HOLDER> mask,boolean one or more = false ; for ( int i = __num__ ; i < interim reasons mask . length && ! one or more ; i ++ ) { if ( interim reasons mask [ i ] && ! ( i < reasons mask . length && reasons mask [ i ] ) ) { one or more = true ; } } if ( ! one or more ) { return false ; },mask includes
if an user used customizations for projects he perhaps just used the key <PLACE_HOLDER> for project without a name but the code expects a name for the project . therefore we fill the name according to the project key which is the same .,for ( entry < string @$ project > entry : cfg . get projects ( ) . entry set ( ) ) { if ( entry . get value ( ) . get name ( ) == null ) { entry . get value ( ) . set name ( entry . get key ( ) ) ; } },user used
blocks to avoid a <PLACE_HOLDER> while the <PLACE_HOLDER>or is used to bind the server connection .,synchronized ( update lock ) { },blocks avoid
if scope is limited to this iterator @$ then do n't check any more <PLACE_HOLDER> in this scope,if ( scope . get limit ( ) == itr ) { continue next_scope ; },then check
keep first row in deduplicate will not send <PLACE_HOLDER>,list < object > expected output = new array list < > ( ) ; expected output . add ( record ( __str__ @$ __num__ @$ __num__ ) ) ; expected output . add ( record ( __str__ @$ __num__ @$ __num__ ) ) ; assertor . assert output equals sorted ( __str__ @$ expected output @$ test harness . get output ( ) ) ; test harness . close ( ) ;,row send
regular expression check <PLACE_HOLDER> .,assert button state ( __str__ @$ true @$ false ) ; @ suppress warnings ( __str__ ) j combo box < charset > combo box = ( j combo box < charset > ) find component by name ( pane @$ __str__ ) ; assert not null ( combo box ) ;,expression check
user trigger <PLACE_HOLDER> to tr<PLACE_HOLDER>nsfer token to b,final string trigger txid = public methed . trigger contract ( transfer token contract address @$ __str__ @$ param @$ false @$ __num__ @$ __num__ @$ __str__ @$ __num__ @$ user001 address @$ user001 key @$ blocking stub full ) ; public methed . wait produce next block ( blocking stub full ) ; account infoafter = public methed . query account ( user001 address @$ blocking stub full ) ; account resource message resource infoafter = public methed . get account resource ( user001 address @$ blocking stub full ) ; long after balance = infoafter . get balance ( ) ; long after energy used = resource infoafter . get energy used ( ) ; long after asset issue count = public methed . get asset,user trigger
reset namenode backup address because windows does not release <PLACE_HOLDER> used previously properly .,backup_config . set ( dfs config keys . dfs_namenode_backup_address_key @$ this_host ) ;,windows release
if a meta key is active but the lookup with the meta key did not produce <PLACE_HOLDER> @$ try some other meta keys @$ because the user might have pressed shift when they meant alt @$ or vice versa .,if ( meta != __num__ ) { key data kd = new key data ( ) ; char [ ] accepted = get accepted chars ( ) ; if ( event . get key data ( kd ) ) { for ( int i = __num__ ; i < kd . meta . length ; i ++ ) { if ( ok ( accepted @$ kd . meta [ i ] ) ) { return kd . meta [ i ] ; } } } },lookup produce
set <PLACE_HOLDER> should cover the previous <PLACE_HOLDER> @$ so we call clear here .,if ( assigned != null && assigned . length > __num__ ) { actions . clear ( ) ; actions . add all ( arrays . as list ( assigned ) ) ; },actions cover
compiler options also validates <PLACE_HOLDER> @$ but uses preconditions and therefore wo n't generate a useful exception .,if ( flags . defines != null ) { validate primitive types ( flags . defines ) ; options . set define replacements ( to map ( flags . defines ) ) ; },options validates
zipkin did n't store the <PLACE_HOLDER> @$ as they should n't have been readable @$ due to the error,assert that ( zipkin . get traces ( ) ) . is empty ( ) ;,zipkin store
still an override might change the <PLACE_HOLDER> of the repository .,if ( rule == null ) { repository delegator function . repository_overrides . get ( env ) ; return ; },override change
yes @$ we are assuming that test queries do n't contain quoted question <PLACE_HOLDER> .,int param count = string utils . count matches ( sql @$ __str__ ) ; boolean m_infer = m_by default infer partitioning ; boolean m_forcesp = m_by default infer partitioning ; m_by default infer partitioning = false ; m_by default plan for single partition = true ; abstract plan node pn = compilesp with join order ( sql @$ param count @$ null ) ; m_by default infer partitioning = m_infer ; m_by default plan for single partition = m_forcesp ; return pn ;,queries contain
the last 4 bytes of the file encode the major and minor <PLACE_HOLDER> universally,baos dos . write int ( materialize version ( major version @$ minor version ) ) ; baos . write to ( output stream ) ;,bytes encode
compute coordinates of point in box coordinate <PLACE_HOLDER>,temp vars vars = temp vars . get ( ) ; vector3f closest = vars . vect1 ; point . subtract ( center @$ closest ) ;,coordinates coordinate
available everywhere . let 's note different <PLACE_HOLDER> .,super public . public method ( ) ;,"s" note
will call create blob <PLACE_HOLDER> on the underlying connection,method m = connection . class . get method ( __str__ @$ new class [ ] { } ) ;,call create
in role editor each object may have different privilege <PLACE_HOLDER>,if ( is role editor ( ) ) { permission table . remove all ( ) ; if ( ! common utils . is empty ( objects ) ) { class < ? > object type = objects . get ( __num__ ) . get class ( ) ; for ( postgre privilege type pt : postgre privilege type . values ( ) ) { if ( ! pt . is valid ( ) || ! pt . supports type ( object type ) ) { continue ; } table item priv item = new table item ( permission table @$ swt . left ) ; priv item . set text ( __num__ @$ pt . name ( ) ) ; priv item . set data ( pt,object have
we know check if both member received <PLACE_HOLDER>,assert equals ( __str__ + __str__ @$ __num__ @$ op set1 room collector . collected events . size ( ) ) ; assert equals ( __str__ + __str__ @$ __num__ @$ op set2 room collector . collected events . size ( ) ) ; chat room member presence change event member event = ( chat room member presence change event ) op set1 room collector . collected events . get ( __num__ ) ; assert equals ( __str__ @$ chat room member presence change event . member_joined @$ member event . get event type ( ) ) ; assert equals ( __str__ @$ fixture . userid2 @$ member event . get chat room member ( ) . get contact address ( ) ) ; assert equals ( __str__,member received
verify that this connection supports <PLACE_HOLDER>,connection . verify can update ( ) ;,connection supports
inspect the memstore contents to see whether the memstore contains only <PLACE_HOLDER> with seq id smaller than the flush seq id . if so @$ we can discard those <PLACE_HOLDER> .,drop mem store contents for seq id ( flush . get flush sequence number ( ) @$ null ) ;,memstore contains
illegal access error is expected note : logback stops context <PLACE_HOLDER> after shutdown initiated . it is problematic to see log output system out could help,logger . warn ( __str__ + e . get message ( ) ) ;,logback stops
the replica sets have different <PLACE_HOLDER> ...,if ( ! this . replica sets by name . key set ( ) . equals ( prior state . replica sets by name . key set ( ) ) ) { return true ; },sets have
test that toggling does n't notify the <PLACE_HOLDER> .,parole listener . rearm latch ( ) ; set app idle enabled ( m controller @$ true ) ; parole listener . await on latch ( stable_charging_threshold * __num__ / __num__ ) ; assert true ( parole listener . m on parole ) ; assert equals ( last update time @$ parole listener . get last parole change time ( ) ) ; parole listener . rearm latch ( ) ; set app idle enabled ( m controller @$ false ) ; parole listener . await on latch ( stable_charging_threshold * __num__ / __num__ ) ; assert true ( parole listener . m on parole ) ; assert equals ( last update time @$ parole listener . get last parole change time ( ) ) ;,toggling notify
update the input stack <PLACE_HOLDER> of the successor blocks of basic block in the control flow graph @$ and add these blocks to the list of blocks to process @$ if not already done .,edge outgoing edge = basic block . outgoing edges ; if ( ( basic block . flags & label . flag_subroutine_caller ) != __num__ ) { outgoing edge = outgoing edge . next edge ; },input stack
both output pages must have the same <PLACE_HOLDER>,assert equals ( dictionary block2 . get dictionary ( ) @$ dictionary block . get dictionary ( ) ) ;,pages have
note to translators : an xml document can specify the <PLACE_HOLDER> of the xml specification to which it adheres . this message indicates that the <PLACE_HOLDER> specified for the output document was not valid .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,document specify
use properties file <PLACE_HOLDER>,use properties file = false ; properties file = __str__ ;,properties file
client received the tcp <PLACE_HOLDER> from server .,assert equals ( - __num__ @$ client . get input stream ( ) . read ( ) ) ;,client received
check if first column contains <PLACE_HOLDER>,if ( first col zero ) { for ( int i = __num__ ; i < matrix . length ; ++ i ) { matrix [ i ] [ __num__ ] = __num__ ; } },column contains
ensure process owner creates queued command <PLACE_HOLDER>,if ( ! cmd dir . exists ( ) ) { cmd dir . mkdir ( ) ; return ; },owner creates
inline fragments may not have a type <PLACE_HOLDER>,string type condition = type name == null ? __str__ : wrap ( __str__ @$ type ( type name ) @$ __str__ ) ; string directives = directives ( node . get directives ( ) ) ; string selection set = node ( node . get selection set ( ) ) ; out . printf ( __str__ @$ comments ( node ) ) ; out . printf ( __str__ @$ spaced ( __str__ @$ type condition @$ directives @$ selection set ) ) ;,fragments have
nested groupby only requires time <PLACE_HOLDER> for inner most query,test query ( planner_config_require_time_condition @$ __str__ + __str__ + __str__ + __str__ @$ calcite tests . regular_user_auth_result @$ immutable list . of ( group by query . builder ( ) . set data source ( new query data source ( group by query . builder ( ) . set data source ( calcite tests . datasource1 ) . set interval ( query segment spec ( intervals . utc ( date times . of ( __str__ ) . get millis ( ) @$ joda utils . max_instant ) ) ) . set granularity ( granularities . all ) . set dimensions ( dimensions ( new default dimension spec ( __str__ @$ __str__ ) ) ) . set aggregator specs ( aggregators ( new long sum aggregator factory ( __str__,groupby requires
no more room to display the comments below ; do n't process <PLACE_HOLDER>,if ( total comments found > max display lines ) { return ; },comments process
the test table has 500 <PLACE_HOLDER> @$ so total query time should be ~ 2500 ms,try { stmt . execute query ( __str__ + __str__ + table name + __str__ + table name + __str__ ) ; fail ( __str__ ) ; } catch ( sql timeout exception e ) { assert not null ( e ) ; system . err . println ( e . to string ( ) ) ; } catch ( sql exception e ) { fail ( __str__ + e ) ; e . print stack trace ( ) ; },table has
if user enters the <PLACE_HOLDER> of the project manually and leaves off the extension @$ try to open or create using the extension,if ( ! create && filename . last index of ( __str__ ) > path . last index of ( file . separator ) ) { msg . show error ( get class ( ) @$ tool . get tool frame ( ) @$ __str__ @$ __str__ + file . get name ( ) + __str__ ) ; continue ; },user enters
optional union field @$ field value is custom any <PLACE_HOLDER>,object [ ] [ ] inputs = { { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ + __str__ @$ __str__ + __str__ @$ __str__ + __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__,field custom
the decor view does not have <PLACE_HOLDER> when the 'done ' extract edit text button is pressed . since it is the view ancestor 's m view @$ it requests <PLACE_HOLDER> before extract edit text clears <PLACE_HOLDER> @$ which gives <PLACE_HOLDER> to the extract edit text . this special case ensure that we keep current selection in that case . it would be better,if ( ( ( m text view . is in extracted mode ( ) ) || m selection moved ) && sel start >= __num__ && sel end >= __num__ ) { selection . set selection ( ( spannable ) m text view . get text ( ) @$ sel start @$ sel end ) ; },view have
need constantly invalidate view in order to get max redraw <PLACE_HOLDER> .,m layout . get view tree observer ( ) . add on pre draw listener ( this ) ;,max redraw
rollback should n't be called before start <PLACE_HOLDER> @$ otherwise @$ node manager can not find the <PLACE_HOLDER>,try { client . rollback last re initialization ( container . get id ( ) ) ; fail ( __str__ ) ; } catch ( yarn exception e ) { assert true ( __str__ @$ e . get message ( ) . contains ( __str__ ) ) ; },rollback find
the dummy protocol suite has the nice <PLACE_HOLDER> that it can be run by just one player .,int no of parties = __num__ ; run test ( f @$ eval strategy @$ log performance @$ no of parties ) ;,suite has
we want to avoid looking into the future . so @$ if the cells of the operation specify a <PLACE_HOLDER> @$ or the operation itself specifies a <PLACE_HOLDER> @$ then we use the maximum ts found . otherwise @$ we bound the get to the current server time . we add 1 to the timerange since the upper bound of a timerange is exclusive,long latest ts = math . max ( op ts @$ latest cell ts ) ; if ( latest ts == __num__ || latest ts == h constants . latest_timestamp ) { latest ts = environment edge manager . current time ( ) ; } get . set time range ( __num__ @$ latest ts + __num__ ) ;,itself specifies
store exists default ssl <PLACE_HOLDER> to restore after test .,final ssl context dflt ssl ctx = ssl context . get default ( ) ;,store exists
if we could n't find the region because the cache is closed @$ throw a cache closed <PLACE_HOLDER>,if ( rgn == null ) { if ( cache . is closed ( ) ) { throw new cache closed exception ( ) ; } throw new region not found exception ( string . format ( __str__ @$ this . region path ) ) ; },cache closed
some problems in key conversion @$ so the params do not match the key <PLACE_HOLDER>,continue ;,params match
seed hash of the target peer @$ needed for network stability check if we are the right target and requester has correct <PLACE_HOLDER> about this peer,if ( ( sb . peers . my seed ( ) == null ) || ( ! ( sb . peers . my seed ( ) . hash . equals ( youare ) ) ) ) { return prop ; },hash has
the super implementation does not handle the following <PLACE_HOLDER>,identification protocol = params . get endpoint identification algorithm ( ) ; algorithm constraints = params . get algorithm constraints ( ) ; prefer local cipher suites = params . get use cipher suites order ( ) ; collection < sni matcher > matchers = params . getsni matchers ( ) ; if ( matchers != null ) { sni matchers = params . getsni matchers ( ) ; },implementation handle
reset pig <PLACE_HOLDER> @$ otherwise you may get the pig <PLACE_HOLDER> of last job in the same thread because pig <PLACE_HOLDER> is thread local variable,pig stats . start ( pig server . get pig context ( ) . get execution engine ( ) . instantiate pig stats ( ) ) ; pig script listener script listener = new pig script listener ( ) ; script state . get ( ) . register listener ( script listener ) ; listener map . put ( context . get paragraph id ( ) @$ script listener ) ; pig server . register script ( tmp script file . get absolute path ( ) ) ; schema schema = pig server . dump schema ( alias ) ; boolean schema known = ( schema != null ) ; if ( schema known ) { for ( int i = __num__ ; i < schema . size,stats get
binding is used to group related field setters together . it is essential for action insert fact col and action set field col 52 columns as these represent single fields and need to be grouped together it is not essential for i action 's as these contain their own list of fields . if a brl fragment does not set the binding use a,if ( binding == null ) { binding = action . to string ( ) ; } final labelled action a = new labelled action ( ) ; a . bound name = binding ; a . action = action ; a . is update = is update ; actions . add ( a ) ;,fragment set
cnxns typically have many <PLACE_HOLDER> @$ so use default cap here,if ( paths == null ) { paths = new hash set < > ( ) ; watch2 paths . put ( watcher @$ paths ) ; },cnxns have
next tag does n't correctly handle dt <PLACE_HOLDER>,if ( event == xml stream constants . start_document ) { while ( ! stax stream reader . is start element ( ) ) event = stax stream reader . next ( ) ; },tag handle
this code is only synchronously calling a single native <PLACE_HOLDER> to trigger and asynchronous sync cycle @$ so 5 minutes is generous .,try { if ( ! semaphore . try acquire ( __num__ @$ time unit . minutes ) ) { log . w ( tag @$ __str__ ) ; sync result . stats . num io exceptions ++ ; } } catch ( interrupted exception e ) { log . w ( tag @$ __str__ @$ e ) ; sync result . stats . num io exceptions ++ ; },code calling
add num null <PLACE_HOLDER> null <PLACE_HOLDER> .,if ( i <= num null values ) { pf . pkid = null ; pf . status = __str__ + i ; } r1 . put ( i + __str__ @$ pf ) ;,null values
the line matched the regular <PLACE_HOLDER> .,string class name = null ; string source file = null ; int line number = __num__ ; string type = null ; string field name = null ; string method name = null ; string arguments = null ;,line matched
make sure browser context menu does not block the <PLACE_HOLDER>,get command executor ( ) . execute script ( __str__ @$ e @$ x @$ y ) ; new actions ( get driver ( ) ) . move to element ( e @$ getx offset ( e @$ x coord ) @$ gety offset ( e @$ y coord ) ) . context click ( ) . move by offset ( - __num__ @$ - __num__ ) . click ( ) . perform ( ) ;,menu block
bug : diagnostic contains : missing disposable handling : apply auto dispose or cache the disposable <PLACE_HOLDER> manually and enable lenient mode .,single . just ( __num__ ) . subscribe ( new test observer < > ( ) ) ;,auto dispose
could try comparing exact message @$ but since it 's informational try <PLACE_HOLDER> :,if ( ! desc . contains ( __str__ ) ) { fail ( __str__ + desc ) ; } if ( ! desc . contains ( __str__ ) ) { fail ( __str__ + desc ) ; },informational try
some required local <PLACE_HOLDER> @$ which are matched exactly,global properties gp = new global properties ( ) ; local properties lp = local properties . for grouping ( new field list ( __num__ @$ __num__ ) ) ; requested local properties req lp = new requested local properties ( ) ; req lp . set grouped fields ( new field list ( __num__ @$ __num__ ) ) ; to map1 . set required global props ( null ) ; to map1 . set required local props ( req lp ) ; to map2 . set required global props ( null ) ; to map2 . set required local props ( null ) ; feedback properties meet requirements report report = map2 . check partial solution properties met ( target @$ gp @$ lp ) ; assert,some required
note to translators : a <PLACE_HOLDER> can be given to a particular style to be used to format decimal values . the substitution text gives the <PLACE_HOLDER> of such a style for which more than one declaration was encountered .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text gives
export the data @$ which causes a file <PLACE_HOLDER> to be shown,execute on swing without blocking ( ( ) -> key binding utils . export key bindings ( options ) ) ; file selected file = find and test file chooser ( null @$ test_filename ) ; return selected file ;,which causes
enable a system ime . no need to show a security warning <PLACE_HOLDER> @$ but we might need to prompt if it 's not direct boot aware . tv does n't does n't need to worry about this @$ but other platforms should show a warning .,if ( m imi . is system ( ) ) { if ( m imi . get service info ( ) . direct boot aware || is tv ( ) ) { set checked internal ( true ) ; } else if ( ! is tv ( ) ) { show direct boot warn dialog ( ) ; } } else { show security warn dialog ( ) ; },platforms show
calculator corner radius <PLACE_HOLDER>,float corner radius top left ; float corner radius top right ; float corner radius bottoml right ; float corner radius bottom left ; if ( skeleton attribute child . get corner radius ( ) != integer . min_value ) { corner radius top left = corner radius top right = corner radius bottoml right = corner radius bottom left = get corner radius ( rectangle rect @$ skeleton attribute child . get corner radius ( ) ) ; } else { corner radius top left = skeleton attribute child . get corner radius top left ( ) != integer . min_value ? get corner radius ( rectangle rect @$ skeleton attribute child . get corner radius top left ( ) ) : __num__ ; corner radius top,corner radius
preserve any <PLACE_HOLDER> already associated with the video . if the video does not have any <PLACE_HOLDER> @$ create a new array . append the provided localization to the list of <PLACE_HOLDER> associated with the video .,map < string @$ video localization > localizations = video . get localizations ( ) ; if ( localizations == null ) { localizations = new array map < string @$ video localization > ( ) ; video . set localizations ( localizations ) ; } video localization video localization = new video localization ( ) ; video localization . set title ( title ) ; video localization . set description ( description ) ; localizations . put ( language @$ video localization ) ;,video have
work correctly @$ so let 's indicate <PLACE_HOLDER> right away,java type value type = _target type ;,let indicate
stack trace of the cause should contain three <PLACE_HOLDER> : test error do fn.nested function beta test error do fn.nested function alpha test error do fn.start bundle,assert that ( stack trace frame strings ( exn . get cause ( ) ) @$ contains ( contains string ( __str__ ) @$ contains string ( __str__ ) @$ contains string ( __str__ ) ) ) ; assert that ( exn . to string ( ) @$ contains string ( __str__ ) ) ;,trace contain
starting a pulse while docking should suppress wakeup <PLACE_HOLDER>,m status bar . m doze service host . pulse while dozing ( mock ( doze host . pulse callback . class ) @$ doze log . pulse_reason_docking ) ; verify ( m status bar window view ) . suppress wake up gesture ( eq ( true ) ) ;,docking suppress
the ri explicitly guarantees this <PLACE_HOLDER> in the socket options.set option documentation .,if ( on && timeout < __num__ ) { throw new illegal argument exception ( __str__ ) ; } if ( on ) { impl . set option ( socket options . so_linger @$ integer . value of ( timeout ) ) ; } else { impl . set option ( socket options . so_linger @$ boolean . false ) ; },ri guarantees
chunked encoding only makes <PLACE_HOLDER> to do when the payload is signed,if ( ! is payload signing enabled ( request ) || is chunked encoding disabled ( request ) ) { return false ; } if ( request . get original request object ( ) instanceof put object request || request . get original request object ( ) instanceof upload part request ) { return true ; } return false ;,encoding makes
let the gridmix record fill <PLACE_HOLDER> .,return new record writer < k @$ gridmix record > ( ) { @ override public void write ( k ignored @$ gridmix record value ) throws io exception { value . write ( file out ) ; } @ override public void close ( task attempt context ctxt ) throws io exception { file out . close ( ) ; } } ;,record fill
if the value is not yes @$ true or 1 then do n't add the <PLACE_HOLDER> to the command line,if ( ! ( __str__ . equals ignore case ( value ) || __str__ . equals ignore case ( value ) || __str__ . equals ignore case ( value ) ) ) { continue ; },true add
support the xa rollback to do to write xa recover log and judge xa <PLACE_HOLDER> to judge if send xa end,if ( session . get xatxid ( ) != null && conn instanceof mysql connection ) { mysql connection mysql con = ( mysql connection ) conn ; string xa tx id = session . get xatxid ( ) ; coordinator log entry coordinator log entry = multi node coordinator . in memory repository . get ( xa tx id ) ; if ( coordinator log entry != null ) { write check point = true ; for ( int i = __num__ ; i < coordinator log entry . participants . length ; i ++ ) { if ( coordinator log entry . participants [ i ] . resource name . equals ( conn . get schema ( ) ) ) { coordinator log entry . participants,xa recover
ensure that record provided <PLACE_HOLDER> to a data db record,if ( rec != null ) { if ( ! rec . has same schema ( datadb adapter . data_schema ) ) { return true ; } dt = code mgr . get data type ( rec ) ; if ( dt == null ) { msg . error ( this @$ __str__ + address ) ; } } else { dt = code mgr . get data type ( addr ) ; },record provided
only proxy cache implementation needs a json <PLACE_HOLDER> that has reference to user attributes,return new json formatter ( ) ;,implementation needs
partition ca n't have this <PLACE_HOLDER>,reserved partition values . add ( hive conf . get var ( conf @$ conf vars . defaultpartitionname ) ) ; reserved partition values . add ( hive conf . get var ( conf @$ conf vars . default_zookeeper_partition_name ) ) ;,partition have
<PLACE_HOLDER> 6 : dest content changed @$ source content changed <PLACE_HOLDER> 7 : dest name change & content changed @$ source name changed & content changed <PLACE_HOLDER> 8 : dest name changed & content changed @$ source content changed <PLACE_HOLDER> 9 : dest content changed @$ source name changed & content changed,if ( tree structure changed ( orig root @$ latest root ) && tree structure changed ( orig root @$ my root ) ) { keep other or create tree ( orig root @$ my root @$ result root @$ i + __num__ ) ; } else if ( name changed ( orig root @$ latest tree name ) && tree structure changed ( orig root @$ latest root ) && name changed ( orig root @$ my tree name ) ) { names content changed ( my root @$ my tree name @$ result tree name @$ orig root @$ i + __num__ ) ; } else if ( name changed ( orig root @$ latest tree name ) && name changed ( orig root @$ my,content changed
finally check if either sender threw an <PLACE_HOLDER>,exception e ; e = s1 . get exception ( ) ; if ( e != null ) throw e ; e = s2 . get exception ( ) ; if ( e != null ) throw e ;,sender threw
old clients do n't send the <PLACE_HOLDER> .,checksum = new byte [ __num__ ] ;,clients send
uh oh ... it looks like the provider 's <PLACE_HOLDER> has been killed on us . we need to wait for a new <PLACE_HOLDER> to be started @$ and make sure its death does n't kill our <PLACE_HOLDER> .,if ( ! success ) { slog . i ( tag @$ __str__ + cpr . name . flatten to short string ( ) + __str__ + r ) ; boolean last ref = dec provider count locked ( conn @$ cpr @$ token @$ stable ) ; check time ( start time @$ __str__ ) ; app died locked ( cpr . proc ) ; check time ( start time @$ __str__ ) ; if ( ! last ref ) { return null ; } provider running = false ; conn = null ; } else { cpr . proc . verified adj = cpr . proc . set adj ; },death kill
check get namespace <PLACE_HOLDER> as xml @$ json and protobuf .,string namespace path = __str__ + ns name ; response = client . get ( namespace path ) ; assert equals ( __num__ @$ response . get code ( ) ) ; response = client . get ( namespace path @$ constants . mimetype_xml ) ; assert equals ( __num__ @$ response . get code ( ) ) ; namespaces instance model model = fromxml ( response . get body ( ) ) ; check namespace properties ( model . get properties ( ) @$ ns properties ) ; response = client . get ( namespace path @$ constants . mimetype_json ) ; assert equals ( __num__ @$ response . get code ( ) ) ; model = json mapper . read value ( response . get body,check get
set up key for wikidict ; if caseless use lower case <PLACE_HOLDER> of surface form,string mention surface form key ; if ( wikidict caseless ) mention surface form key = surface form . to lower case ( ) ; else mention surface form key = surface form ;,caseless use
only consider entries with absolute path names . this allows storing ur is in the database without the media scanner removing <PLACE_HOLDER> .,if ( path != null && path . starts with ( __str__ ) ) { boolean exists = false ; try { exists = os . access ( path @$ android . system . os constants . f_ok ) ; } catch ( errno exception e1 ) { } if ( ! exists && ! mtp constants . is abstract object ( format ) ) { string mime type = media file . get mime type for file ( path ) ; if ( ! media file . is play list mime type ( mime type ) ) { deleter . delete ( row id ) ; if ( path . to lower case ( locale . us ) . ends with ( __str__ ) ) { deleter,ur removing
we capture and set the context once the user provided observable <PLACE_HOLDER>,assert true ( results . is context initialized observe on . get ( ) ) ;,user provided
items to test <PLACE_HOLDER>,item = new privacy item ( privacy item . type . subscription . name ( ) @$ true @$ i ) ; item . set value ( privacy rule . subscription_both ) ; original privacy items [ i ] = item ; i = i + __num__ ; item = new privacy item ( privacy item . type . subscription . name ( ) @$ false @$ i ) ; item . set value ( privacy rule . subscription_from ) ; original privacy items [ i ] = item ; i = i + __num__ ; item = new privacy item ( privacy item . type . subscription . name ( ) @$ true @$ i ) ; item . set value ( privacy rule . subscription_to ),items test
partitioned tables do n't have table desc set on the fetch task . instead they have a <PLACE_HOLDER> of partition desc objects @$ each with a table desc . let 's try to fetch the desc for the first partition and use it 's deserializer .,if ( td == null && ft . get work ( ) != null && ft . get work ( ) . get part desc ( ) != null ) { if ( ft . get work ( ) . get part desc ( ) . size ( ) > __num__ ) { td = ft . get work ( ) . get part desc ( ) . get ( __num__ ) . get table desc ( ) ; } } if ( td == null ) { log . info ( __str__ ) ; } else { string table name = __str__ ; list < field schema > lst = null ; try { lst = hive meta store utils . get fields from deserializer ( table,tables have
given on lock screen and stack scroller has a nonzero <PLACE_HOLDER>,given lock screen ( ) ; m notification stack height = __num__ ; m keyguard status height = empty_height ;,scroller has
if the string does n't include the year <PLACE_HOLDER> for some reason @$ then return the gregorian string .,if ( p == - __num__ ) { return s ; } p += year field . length ( ) ; string builder sb = new string builder ( s . substring ( __num__ @$ p ) ) ;,string include
two located block <PLACE_HOLDER>,int lbs count = __num__ ;,two located
now we check if the rectangle of this actor in screen coordinates is in the rectangle spanned by the camera 's view . this assumes that the camera has no <PLACE_HOLDER> and is not rotated !,actor rect . set ( stagex @$ stagey @$ get width ( ) @$ get height ( ) ) ; cam rect . set ( camera . position . x - camera . viewport width / __num__ @$ camera . position . y - camera . viewport height / __num__ @$ camera . viewport width @$ camera . viewport height ) ; visible = cam rect . overlaps ( actor rect ) ; return ! visible ;,camera has
this block of code can include <PLACE_HOLDER> if it 's parent <PLACE_HOLDER> is included,if ( annotation type matches ( element @$ parent element @$ include annotations @$ false @$ include jackson annotations @$ seen annotations @$ lines @$ imports resolver @$ element type @$ nullability ) ) { return true ; },block include
<PLACE_HOLDER> time requested <PLACE_HOLDER>,final plan phase effective plan phase = catalog . find phase ( plan phase . get name ( ) @$ effective date @$ last change plan date ) ; return compute usages ( is cancelled or blocked @$ effective plan phase ) ;,time requested
getter also creates the rigid <PLACE_HOLDER>,physics . get rigid body ( entity ) ;,getter creates
firing the signal should now trigger the <PLACE_HOLDER> with the boundary event too,runtime service . start process instance by key ( __str__ ) ; assert equals ( __num__ @$ runtime service . create process instance query ( ) . count ( ) ) ; assert equals ( __num__ @$ task service . create task query ( ) . count ( ) ) ; assert equals ( __num__ @$ task service . create task query ( ) . task name ( __str__ ) . count ( ) ) ;,signal trigger
result will have our <PLACE_HOLDER> @$ and elapsed time between snapshots,final entry entry = new entry ( ) ; final network stats result ; if ( recycle != null && recycle . capacity >= left . size ) { result = recycle ; result . size = __num__ ; result . elapsed realtime = delta realtime ; } else { result = new network stats ( delta realtime @$ left . size ) ; } for ( int i = __num__ ; i < left . size ; i ++ ) { entry . iface = left . iface [ i ] ; entry . uid = left . uid [ i ] ; entry . set = left . set [ i ] ; entry . tag = left . tag [ i ] ; entry .,result have
this captures all logged <PLACE_HOLDER> @$ allowing us to verify log message was written .,final log interceptor log interceptor = new log interceptor ( ) ; test helper . drop all schemas ( ) ; test helper . executeddl ( __str__ ) ; configuration . builder config builder = test helper . default config ( ) . with ( postgres connector config . snapshot_mode @$ snapshot mode . initial_only . get value ( ) ) ; start ( postgres connector . class @$ config builder . build ( ) ) ; assert connector is running ( ) ; wait for available records ( __num__ @$ time unit . milliseconds ) ; stop connector ( value -> assert that ( log interceptor . contains warn message ( no_monitored_tables_warning ) ) . is false ( ) ) ;,all logged
the following two 'applys ' create multiple <PLACE_HOLDER> to our pipeline @$ one for each of our two input sources .,p collection < table row > events table = p . apply ( big queryio . read table rows ( ) . from ( gdelt_events_table ) ) ; p collection < table row > country codes = p . apply ( big queryio . read table rows ( ) . from ( country_codes ) ) ; p collection < string > formatted results = join events ( events table @$ country codes ) ; formatted results . apply ( textio . write ( ) . to ( options . get output ( ) ) ) ; p . run ( ) . wait until finish ( ) ;,"applys" create
something is n't right . since we rely on being attached to get data set change <PLACE_HOLDER> @$ do n't risk doing anything where we might try to resync and find things in a bogus state .,if ( m is detaching || ! is attached to window ( ) ) { return false ; },data set
this level has no more set <PLACE_HOLDER> @$ pop back up a level .,if ( next active slot bit number > __num__ ) { int index shift above = index shift + __num__ ; virtual index += __num__ << index shift above ; virtual index &= ~ ( ( __num__ << index shift above ) - __num__ ) ; return - virtual index ; },level set
optionally check the byte after this frame matches sync <PLACE_HOLDER> .,if ( ! try read ( pes buffer @$ adts scratch . data @$ __num__ ) ) { return true ; } adts scratch . set position ( __num__ ) ; int frame size = adts scratch . read bits ( __num__ ) ; if ( frame size <= __num__ ) { return false ; },frame matches
the method contains <PLACE_HOLDER> which need to be canonicalized,clause buffer . insert ( __num__ @$ __str__ ) ; compiled value cv = null ; for ( int j = this . args . size ( ) ; j > __num__ ; ) { cv = ( compiled value ) this . args . get ( -- j ) ; cv . generate canonicalized expression ( clause buffer @$ context ) ; clause buffer . insert ( __num__ @$ __str__ ) ; } clause buffer . delete char at ( __num__ ) . insert ( __num__ @$ __str__ ) . insert ( __num__ @$ this . method name ) ;,method contains
vpn is using cell | wi <PLACE_HOLDER> .,m service . set underlying networks for vpn ( new network [ ] { m cell network agent . get network ( ) @$ m wi fi network agent . get network ( ) } ) ; wait for idle ( ) ;,vpn using
client should close the <PLACE_HOLDER> @$ but let 's hold it open .,assert equals ( - __num__ @$ client . get input stream ( ) . read ( ) ) ;,client close
straighten coords if rotations flipped <PLACE_HOLDER>,if ( r . right < r . left ) { final float f = r . right ; r . right = r . left ; r . left = f ; } if ( r . bottom < r . top ) { final float f = r . top ; r . top = r . bottom ; r . bottom = f ; },rotations flipped
check that corrupting our array does n't affect other <PLACE_HOLDER> .,parameters [ __num__ ] = string . class ; parameters = method . get parameter types ( ) ; assert equals ( __num__ @$ parameters . length ) ; assert equals ( expected parameters [ __num__ ] @$ parameters [ __num__ ] ) ;,check affect
the script wrapper has overridden the usual scripting <PLACE_HOLDER>,if ( iface != null ) { return iface ; },wrapper overridden
<PLACE_HOLDER> is call not permitted <PLACE_HOLDER>,assert that ( result . failed ( ) . get ( ) ) . is instance of ( call not permitted exception . class ) ;,exception call
verify that a second call should still return absolute <PLACE_HOLDER>,final long [ ] [ ] times2 = increase time ( times1 ) ; write to file ( uid lines ( m uids @$ times2 ) ) ; m reader . read absolute ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; i ++ ) { m callback . verify ( m uids [ i ] @$ times2 [ i ] ) ; } m callback . verify no more interactions ( ) ; m callback . clear ( ) ; assert true ( m test file . delete ( ) ) ;,call return
the contact already exist its <PLACE_HOLDER>,try { op set presence2 . subscribe ( fixture . userid1 ) ; } catch ( operation failed exception ex ) { if ( ex . get error code ( ) != operation failed exception . subscription_already_exists ) { throw ex ; } else { } },contact exist
ensure payload generation does n't throw an <PLACE_HOLDER>,byte [ ] serialized = new exec checking security manager ( ) . call wrapped ( new callable < byte [ ] > ( ) { public byte [ ] call ( ) throws exception { final string command = args . length > __num__ && args [ __num__ ] != null ? args [ __num__ ] : get default test cmd ( ) ; system . out . println ( __str__ + command + __str__ ) ; object payload < ? > payload = clazz . new instance ( ) ; final object obj before = payload . get object ( command ) ; system . out . println ( __str__ ) ; byte [ ] ser = serializer . serialize ( obj before ) ; utils,generation throw
make sure the fs and the found root dir have the same <PLACE_HOLDER>,log . debug ( __str__ + file system . get default uri ( fs . get file system ( ) . get conf ( ) ) ) ; log . debug ( __str__ + file system . get default uri ( fs . get configuration ( ) ) ) ;,fs have
lastly @$ the behavior as defined by deployment of a webapp add the servlet <PLACE_HOLDER>,servlet context handler app = new servlet context handler ( servlet context handler . sessions ) ; app . set context path ( __str__ ) ; contexts . add handler ( app ) ;,behavior add
cocoa dragged event has the <PLACE_HOLDER> about which mouse button is being dragged . use it to determine the peer that should receive the dragged event .,if ( id == mouse event . mouse_dragged ) { target peer = mouse down target [ target idx ] ; mouse click buttons &= ~ modifiers ; } else if ( id == mouse event . mouse_released ) { target peer = mouse down target [ target idx ] ; if ( ( modifiers & event button mask ) == __num__ ) { mouse down target [ target idx ] = null ; } },event has
image alt attribute numeric attachment <PLACE_HOLDER> of the image in the site 's media library,add metadata property ( metadata @$ __str__ @$ __str__ ) ; add metadata property ( metadata @$ __str__ @$ __str__ ) ;,alt attribute
create the base list of classes which have possible <PLACE_HOLDER> to be overloaded,this . class list = new linked hash set < class > ( ) ; this . class list . add ( super class ) ; if ( generate delegate field ) { class list . add ( delegate class ) ; collections . add all ( this . class list @$ delegate class . get interfaces ( ) ) ; } if ( interfaces != null ) { collections . add all ( this . class list @$ interfaces ) ; } this . proxy name = proxy name ( ) ; this . empty body = empty body ;,which have
test <PLACE_HOLDER> builder constructs test runner <PLACE_HOLDER> with a 'null ' shell path only when we use the native test wrapper . something clearly went wrong .,if ( os . get current ( ) == os . windows && ! action . is using test wrapper instead of test setup script ( ) ) { preconditions . check not null ( action . get sh executable maybe ( ) @$ __str__ @$ action ) ; args . add ( action . get sh executable maybe ( ) . get path string ( ) ) ; args . add ( __str__ ) ; args . add ( __str__ ) ; },constructs test
for mvcc caches we need to wait until updated value becomes visible for consequent readers . when mvcc <PLACE_HOLDER> completes @$ it 's updates are not visible immediately for the new <PLACE_HOLDER>s . this is caused by the lag between <PLACE_HOLDER> completes on the node and mvcc coordinator removes this <PLACE_HOLDER> from the active list .,grid test utils . run async ( new runnable ( ) { @ override public void run ( ) { string v ; while ( ! thread . current thread ( ) . is interrupted ( ) ) { v = cache . get ( key ) ; if ( v == null ) do sleep ( __num__ ) ; else { log . info ( __str__ + id ) ; ( ( ( grid future adapter ) ( ( ignite future impl ) promise ) . internal future ( ) ) ) . on done ( __str__ ) ; break ; } } } } ) ;,coordinator removes
if a table has no <PLACE_HOLDER> 's it does n't get checked,desc . add family ( hcd ) ;,table has
no node of depth i has a valid <PLACE_HOLDER>,if ( ( policy mapping > __num__ ) || ( policy mapping == - __num__ ) ) { set < policy node impl > valid any nodes = root node . get policy nodes valid ( cert index @$ any_policy ) ; for ( policy node impl cur any node : valid any nodes ) { policy node impl cur any node parent = ( policy node impl ) cur any node . get parent ( ) ; set < string > exp pols = new hash set < > ( ) ; exp pols . add ( subject domain ) ; policy node impl cur node = new policy node impl ( cur any node parent @$ issuer domain @$ any quals @$ policies critical @$ exp,node has
we assume that an event set contains only vm death <PLACE_HOLDER> or no vm death <PLACE_HOLDER> .,if ( ! ( my event instanceof vm death event ) ) { break ; },set contains
prev stmt should throw <PLACE_HOLDER>,fail ( ) ;,stmt throw
& & ! special named arg <PLACE_HOLDER>,if ( no arg && ! super list . is empty ( ) && ! has no arg constructor ( c node ) ) { create no arg constructor ( c node @$ modifiers ) ; },& named
hash at height 0 is just the regular tx hash <PLACE_HOLDER> .,if ( height == __num__ ) { return hashes . get ( pos ) ; },tx hash
adding null where the complete tasks header simplifies a <PLACE_HOLDER> of logic for us,m tasks . add ( null ) ;,header simplifies
which segment contains the <PLACE_HOLDER>,bucket segment index = bucket > > > num buckets per segment bits ;,segment contains
dont do actual <PLACE_HOLDER> in check mode .,boolean configcheck = boolean . parse boolean ( config . get property ( export manager . config_check_only @$ __str__ ) ) ; if ( configcheck ) { return ; } server array = host . split ( __str__ ) ; m_executor service = executors . new fixed thread pool ( server array . length ) ; setup connection ( ) ;,dont do
while we still have <PLACE_HOLDER> which have no outgoing edges,while ( no outgoing . size ( ) > __num__ ) { final graph . vertex < integer > current = no outgoing . remove ( __num__ ) ; sorted . add ( current ) ; int i = __num__ ; while ( i < edges . size ( ) ) { final graph . edge < integer > e = edges . get ( i ) ; final graph . vertex < integer > from = e . get from vertex ( ) ; final graph . vertex < integer > to = e . get to vertex ( ) ; if ( to . equals ( current ) ) { edges . remove ( e ) ; from . get edges ( ) . remove (,which have
delete the rows inserted from cache utils create <PLACE_HOLDER> @$ otherwise conflict in pk 's,jta obj . delete rows ( this . tbl name ) ; context ctx = cache . getjndi context ( ) ; user transaction ta = ( user transaction ) ctx . lookup ( __str__ ) ; connection conn = null ; try { ta . begin ( ) ; data source da = ( data source ) ctx . lookup ( __str__ ) ; conn = da . get connection ( ) ; statement stmt = conn . create statement ( ) ; string sqlstr = __str__ + this . tbl name + __str__ + this . tblid fld + __str__ + __str__ + this . tbl name fld + __str__ + __str__ ; stmt . execute update ( sqlstr ) ; ta . commit ( ),rows create
then check if total estimated file size exceeds user specified <PLACE_HOLDER>,if ( total estimated export size > user specified limit ) { string builder sb = new string builder ( ) ; sb . append ( __str__ ) . append ( total estimated export size ) . append ( __str__ ) . append ( cli strings . export_logs__filesizelimit ) . append ( __str__ ) . append ( user specified limit ) . append ( __str__ ) ; return result model . create error ( sb . to string ( ) ) ; },size exceeds
determine if this update is changing the <PLACE_HOLDER> for the processor,if ( ! existing coordinate . equals ( incoming coordinate ) ) { if ( ! existing coordinate . get group ( ) . equals ( incoming coordinate . get group ( ) ) || ! existing coordinate . get id ( ) . equals ( incoming coordinate . get id ( ) ) ) { throw new illegal argument exception ( string . format ( __str__ @$ get identifier ( ) @$ existing coordinate . get coordinate ( ) @$ incoming coordinate . get coordinate ( ) ) ) ; } },update changing
test database meta data queries which do not have a parent <PLACE_HOLDER>,database meta data md = this . con . get meta data ( ) ; assert true ( md . get connection ( ) == this . con ) ; rs = md . get catalogs ( ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get columns ( null @$ null @$ null @$ null ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get functions ( null @$ null @$ null ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get imported keys (,which have
parse named <PLACE_HOLDER> .,int color start = start ; for ( int i = start + __num__ ; i < end ; i ++ ) { char ch = str . char at ( i ) ; if ( ch != __str__ ) continue ; color named color = colors . get ( str . sub sequence ( color start @$ i ) . to string ( ) ) ; if ( named color == null ) return - __num__ ; color color = color pool . obtain ( ) ; color stack . add ( color ) ; color . set ( named color ) ; return i - start ; },parse named
no handlers left so close the actual <PLACE_HOLDER> the done handler needs to be executed on the context that calls close @$ not the context of the actual <PLACE_HOLDER>,actual server . actual close ( context @$ completion ) ;,handlers left
if it is no longer pending someone called unschedule async write so we do n't need to write the <PLACE_HOLDER> @$ but if we have a version tag we need to record the operation to update the rvv,if ( tag != null ) { disk entry . helper . do async flush ( tag @$ region ) ; },async write
device idle controller adds <PLACE_HOLDER> to local services in the constructor @$ so we have to remove them after each test @$ otherwise @$ subsequent tests will fail .,local services . remove service for test ( app state tracker . class ) ; local services . remove service for test ( device idle controller . local service . class ) ;,controller adds
fully recovery needs much longer <PLACE_HOLDER>,assert equals ( load balancer simulator . get point ( __str__ @$ default partition accessor . default_partition_id @$ uri1 ) @$ __num__ ) ; assert equals ( load balancer simulator . get point ( __str__ @$ default partition accessor . default_partition_id @$ uri2 ) @$ __num__ ) ;,recovery needs
gana ya gana small ya kana ya kana small ya gana yu gana small yu kana yu kana small yu gana <PLACE_HOLDER> gana small <PLACE_HOLDER> kana <PLACE_HOLDER> kana small <PLACE_HOLDER>,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,kana yo
check for a vertex hit first @$ otherwise @$ we get edge <PLACE_HOLDER> when we are hovering over a vertex @$ due to how edges are interpreted as existing all the way to the center point of a vertex,v vertex = get pick support ( ) . get vertex ( viewer layout @$ p . getx ( ) @$ p . gety ( ) ) ; if ( vertex != null ) { return new vertex tool tip info ( vertex @$ event ) ; } e edge = get pick support ( ) . get edge ( viewer layout @$ p . getx ( ) @$ p . gety ( ) ) ; if ( edge != null ) { return new edge tool tip info ( edge @$ event ) ; },check get
does the package have <PLACE_HOLDER> ? if not @$ there wo n't be any artifacts .,if ( ! package dex optimizer . can optimize package ( pkg ) ) { continue ; } if ( pkg . code path == null ) { slog . w ( tag @$ __str__ + pkg + __str__ ) ; continue ; },package have
this can happen when the set method throw a checked <PLACE_HOLDER> or something like that,throw new web service exception ( e ) ;,method throw
empty bundles do n't impact <PLACE_HOLDER> and should n't trigger downstream execution @$ so filter them out,if ( ! iterables . is empty ( committed . get elements ( ) ) ) { completed . add ( committed ) ; },bundles impact
if load balancing is configured but client entity indicates that <PLACE_HOLDER> is not being transferred @$ we need to check if any other node is actively transferring <PLACE_HOLDER> . if client entity is transferring <PLACE_HOLDER> @$ we already know the correct value for the status @$ and if the connection is not configured for load balancing @$ then we also know the correct value,if ( client entity . get component ( ) != null && connectiondto . load_balance_inactive . equals ( client entity . get component ( ) . get load balance status ( ) ) ) { final boolean any active = entity map . values ( ) . stream ( ) . map ( connection entity :: get component ) . filter ( objects :: non null ) . map ( connectiondto :: get load balance status ) . any match ( status -> status . equals ( connectiondto . load_balance_active ) ) ; if ( any active ) { client entity . get component ( ) . set load balance status ( connectiondto . load_balance_active ) ; } } final set < string > available relationships = client,entity transferring
ie does n't fire popstate correctly with certain hash changes . simulate the missing <PLACE_HOLDER> with history handler .,if ( browser info . get ( ) . isie ( ) ) { history . add value change handler ( evt -> { final string new location = browser . get window ( ) . get location ( ) . to string ( ) ; if ( ! new location . equals ( current location ) ) { current location = new location ; get rpc proxy ( ui server rpc . class ) . popstate ( browser . get window ( ) . get location ( ) . to string ( ) ) ; } } ) ; current location = browser . get window ( ) . get location ( ) . to string ( ) ; },ie simulate
should return empty <PLACE_HOLDER>,resp = post ( __str__ @$ collections . empty_map ) ; assert not null ( resp ) ; map m = ( map ) resp . get ( __str__ ) ; assert true ( m . is empty ( ) ) ; resp = post ( __str__ @$ immutable map . of ( __str__ @$ immutable list . of ( __str__ @$ test pipeline impl . class . get name ( ) ) ) ) ; assert not null ( resp ) ; m = ( map ) resp . get ( __str__ ) ; assert not null ( m ) ; assert equals ( __num__ @$ m . size ( ) ) ; map v = ( map ) m . get ( __str__ ) ; assert,return empty
determine if we are working with a dlgtemplate or dlgtemplateex structure . the first 4 bytes will have specific <PLACE_HOLDER> if it 's a dlgtemplateex .,try { boolean ex = mem buffer . get short ( __num__ ) == __num__ && mem buffer . get short ( __num__ ) == - __num__ ; temp offset = add dlg template structure ( mem buffer @$ comps @$ temp offset @$ ex ) ; temp offset = add dialog menu array ( mem buffer @$ comps @$ temp offset ) ; temp offset = add dialog class array ( mem buffer @$ comps @$ temp offset ) ; temp offset = add dialog title array ( mem buffer @$ comps @$ temp offset ) ; byte get style = mem buffer . get byte ( __num__ ) ; if ( ( get style & ds_setfont ) > __num__ ) { temp offset = add dialog,bytes have
now @$ iterate over all the discovered <PLACE_HOLDER>,for ( class < ? > iface : interfaces ) { for ( membert member : getter . get members ( iface ) ) { if ( member . is annotation present ( anno ) ) { matches . add ( member ) ; } } } return matches ;,iterate discovered
this may happen if an app has a recorded <PLACE_HOLDER> @$ but has been uninstalled .,continue ;,app has
this methods monitors a <PLACE_HOLDER> for new files to read in for streaming .,javad stream < string > log data = jssc . text file stream ( flags . get instance ( ) . get logs directory ( ) ) ; javad stream < apache access log > access logsd stream = log data . map ( new functions . parse from log line ( ) ) . cache ( ) ; final log analyzer total log analyzer total = new log analyzer total ( ) ; final log analyzer windowed log analyzer windowed = new log analyzer windowed ( ) ;,methods monitors
default generator does n't provide any <PLACE_HOLDER> for emitting supporting files other than by a mustache template @$ so we 're obliged to serialize the caches to json strings and use templates to write them .,if ( load test data from file ) { try { if ( test data cache . root ( ) . is dirty ( ) ) { byte array output stream out = new byte array output stream ( ) ; test data cache . root ( ) . flush ( out ) ; string test data json = new string ( out . to byte array ( ) @$ __str__ ) ; objs . put ( __str__ @$ test data json ) ; supporting files . add ( new supporting file ( __str__ @$ test data file . get absolute path ( ) ) ) ; } } catch ( cache exception | unsupported encoding exception e ) { logger . error ( __str__ + test data,generator provide
virtual or physical memory over limit . fail the <PLACE_HOLDER> and remove the corresponding process tree,if ( is memory over limit ) { log . warn ( msg ) ; if ( ! p tree . check pid pgrpid for match ( ) ) { log . error ( __str__ + __str__ @$ p id ) ; } event dispatcher . get event handler ( ) . handle ( new container kill event ( container id @$ container exit status @$ msg ) ) ; tracking containers . remove ( container id ) ; log . info ( __str__ @$ p id ) ; },memory fail
expect that the worker will create converters and will not initially find them using the current <PLACE_HOLDER> ...,assert not null ( task key converter ) ; assert not null ( task value converter ) ; assert not null ( task header converter ) ; expect task key converters ( class loader usage . current_classloader @$ null ) ; expect task key converters ( class loader usage . plugins @$ task key converter ) ; expect task value converters ( class loader usage . current_classloader @$ null ) ; expect task value converters ( class loader usage . plugins @$ task value converter ) ; expect task header converter ( class loader usage . current_classloader @$ null ) ; expect task header converter ( class loader usage . plugins @$ task header converter ) ; easy mock . expect ( executor service . submit ( worker,them using
130 hz <PLACE_HOLDER> .,if ( eq values [ __num__ ] == __num__ ) { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) __num__ ) ; } else if ( eq values [ __num__ ] < __num__ ) { if ( eq values [ __num__ ] == __num__ ) { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) - __num__ ) ; } else { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) ( - ( __num__ - eq values [ __num__ ] ) * __num__ ) ) ; } } else if,130 hz
test the same data and <PLACE_HOLDER> with prior @$ should get the same <PLACE_HOLDER> except for the intercept,glm = new glm ( params ) ; model3 = glm . train model ( ) . get ( ) ; double lambda = model3 . _output . _submodels [ model3 . _output . _best_lambda_idx ] . lambda_value ; params . _lambda_search = false ; params . _lambda = new double [ ] { lambda } ; model metrics mm3 = model metrics . get fromdkv ( model3 @$ fr ) ; assert equals ( __str__ + model3 . _output . _training_metrics . _mse + __str__ + mm3 . _mse @$ model3 . _output . _training_metrics . _mse @$ mm3 . _mse @$ __num__ ) ; assert equals ( __str__ + ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev + __str__,data get
fails with primitive <PLACE_HOLDER>es ; need the wrapper <PLACE_HOLDER> . thanks @$ java .,a = ( e [ ] ) array . new instance ( afclz @$ splits . length ) ;,fails need
fragment factory needs to be set before calling the super.on create @$ otherwise the activity crashes when it is recreating and there is a fragment which has no default <PLACE_HOLDER> .,super . on create ( saved instance state ) ;,which has
catch <PLACE_HOLDER>s since set daemon can cause a security <PLACE_HOLDER> to be thrown under netscape in the applet mode,set daemon ( true ) ;,daemon cause
to avoid race condition of testcase @$ atleast check 40 <PLACE_HOLDER> with sleep of 50 ms,verify entity for timelinev2 ( app entity file @$ application metrics constants . finished_event_type @$ __num__ @$ __num__ @$ __num__ @$ false ) ;,atleast check
reached end of string . always a break <PLACE_HOLDER> .,if ( p2 >= f text . length ( ) ) { break ; },a break
if the app is undergoing <PLACE_HOLDER> @$ tell the <PLACE_HOLDER> manager about it,final backup record backup target = m backup targets . get ( app . user id ) ; if ( backup target != null && app . pid == backup target . app . pid ) { if ( debug_backup || debug_cleanup ) slog . d ( tag_cleanup @$ __str__ + backup target . app info + __str__ ) ; m handler . post ( new runnable ( ) { @ override public void run ( ) { try { i backup manager bm = i backup manager . stub . as interface ( service manager . get service ( context . backup_service ) ) ; bm . agent disconnected for user ( app . user id @$ app . info . package name ) ; } catch,app undergoing
if the metafile references a <PLACE_HOLDER> @$ remove it from the map .,try { dbus event buffer meta info mi = new dbus event buffer meta info ( f ) ; mi . load meta info ( ) ; if ( mi . is valid ( ) ) { string session id = mi . get session id ( ) ; session file map . remove ( session id ) ; } } catch ( dbus event buffer meta info . dbus event buffer meta info exception e ) { log . warn ( __str__ + f . get name ( ) + __str__ @$ e ) ; return ; },metafile references
handler does n't set a <PLACE_HOLDER> on profile,assert false ( response . to string ( ) @$ response . has defined ( result ) ) ;,handler set
page views . do this once we have the right padding <PLACE_HOLDER> from above .,for ( int i = __num__ ; i < count ; i ++ ) { final view child = get child at ( i ) ; if ( child . get visibility ( ) != gone ) { final layout params lp = ( layout params ) child . get layout params ( ) ; item info ii ; if ( ! lp . is decor && ( ii = info for child ( child ) ) != null ) { int loff = ( int ) ( child width * ii . offset ) ; int child left = padding left + loff ; int child top = padding top ; if ( lp . needs measure ) { lp . needs measure = false ; final,views have
remove the receive <PLACE_HOLDER> and the order by <PLACE_HOLDER> and replace them with a merge receive <PLACE_HOLDER> . leave the order by <PLACE_HOLDER> inline in the merge receive <PLACE_HOLDER> @$ since we need it to calculate the merge .,plan . clear children ( ) ; receive node . remove from graph ( ) ; merge receive plan node mrnode = new merge receive plan node ( ) ; mrnode . add inline plan node ( onode ) ; mrnode . add and link child ( send node ) ; plan . add and link child ( mrnode ) ; return plan ;,node receive
for newly added components who recognize this <PLACE_HOLDER> but did not offer a default value @$ we need to make sure these components will get an opportunity to read the value before parsing begins .,f config updated = true ;,who recognize
skip due to android devices that have broken scrolltop will may get odd <PLACE_HOLDER> here .,if ( browser info . get ( ) . is touch device ( ) ) { return ; },will get
this is only available after the backend initialized the <PLACE_HOLDER>,type serializer < string > value serializer = kv id . get serializer ( ) ;,backend initialized
set automatic accept <PLACE_HOLDER>,if ( args [ i ] . equals ( __str__ ) && args . length > ( i + __num__ ) ) { opt_accept_time = integer . parse int ( args [ ++ i ] ) ; continue ; },automatic accept
note : we shouldnt ' have to do this @$ since j file chooser adds the <PLACE_HOLDER> to the choosable <PLACE_HOLDER>s list when the <PLACE_HOLDER> is set . lets be paranoid just in case someone overrides set file <PLACE_HOLDER> in j file chooser .,file filter current filter = get file chooser ( ) . get file filter ( ) ; boolean found = false ; if ( current filter != null ) { for ( file filter filter : filters ) { if ( filter == current filter ) { found = true ; } } if ( ! found ) { get file chooser ( ) . add choosable file filter ( current filter ) ; } } return get file chooser ( ) . get file filter ( ) ;,chooser adds
case 12 : true if either an attribute named primitive int att name of type int has the <PLACE_HOLDER> int <PLACE_HOLDER> or an attribute named primitive long att name of type long has the <PLACE_HOLDER> long <PLACE_HOLDER> we cover javax.management.or query exp,queries . add ( query . or ( query . eq ( query . attr ( primitive int att name ) @$ query . value ( int value ) ) @$ query . eq ( query . attr ( primitive long att name ) @$ query . value ( long value ) ) ) ) ;,attribute has
query must produce only <PLACE_HOLDER> from single region .,validate clients ( region id @$ clients2 ) ; if ( region id == unmapped_region ) fail ( ) ;,query produce
this test verifies the baseline <PLACE_HOLDER> used in subsequent tests . if this fails @$ the rest will fail .,project dependency graph graph = three projects depending ona single ( ) ; final list < maven project > sorted projects = graph . get sorted projects ( ) ; assert equals ( a project @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender1 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender2 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender3 @$ sorted projects . get ( __num__ ) ) ;,test verifies
reopen implies the <PLACE_HOLDER> of the reopened session for the same query that we gave it out for ; so @$ as we would have failed an active query @$ fail the <PLACE_HOLDER>r before it 's started .,future . set exception ( new runtime exception ( __str__ + session . get reason for kill ( ) ) ) ; return ;,reopen implies
for some reason @$ the <PLACE_HOLDER> is not stored @$ so the restored values use the <PLACE_HOLDER> from the default jvm timezone .,if ( time as timestamp remappingh2 dialect . class . equals ( get remapping dialect class ( ) ) ) { return get original property value ( ) . with offset same local ( offset date time . now ( ) . get offset ( ) ) ; } else { return get original property value ( ) . with nano ( __num__ ) . with offset same local ( offset date time . now ( ) . get offset ( ) ) ; },values use
remove all items using <PLACE_HOLDER>,iterator iterator = observable collection . iterator ( ) ; while ( iterator . has next ( ) ) { iterator . next ( ) ; iterator . remove ( ) ; },items using
this will obtain any waiting outbound <PLACE_HOLDER> @$ or will process the outbound app <PLACE_HOLDER> .,try { synchronized ( write lock ) { hs status = write record ( output record @$ ea ) ; } } catch ( ssl exception e ) { throw e ; } catch ( io exception e ) { throw new ssl exception ( __str__ @$ e ) ; },any waiting
removing these calls and the three location assertions above will cause the test to fail due to the strict stubs . without the alterations to stack trace cleaner provider @$ the failure messages will contain an <PLACE_HOLDER> in the stack trace inside the powermock libraries .,assert that ( something with static method . do static one ( ) @$ is ( __str__ ) ) ; assert that ( something with static method . do static two ( ) @$ is ( __str__ ) ) ; something with static method . do static void ( ) ;,messages contain
producer will do it 's own destination <PLACE_HOLDER> so always use the destination based send method otherwise we might violate a jms rule .,message producer . send ( destination @$ message @$ delivery mode @$ priority @$ time to live ) ;,producer do
check if the caller has enough <PLACE_HOLDER> to embed activities and launch to private displays .,final int start any perm = m service . check permission ( internal_system_window @$ calling pid @$ calling uid ) ; if ( start any perm == permission_granted ) { if ( debug_tasks ) slog . d ( tag @$ __str__ + __str__ ) ; return true ; },caller has
trigger the creation of a bucket @$ which should trigger the <PLACE_HOLDER> of this vm .,assert that thrown by ( ( ) -> region . put ( __str__ @$ __str__ ) ) . is instance of ( cancel exception . class ) ;,which trigger
if the user touches the screen invalidate the current running delay by incrementing the valid message id . so this delay wo n't hide the undo <PLACE_HOLDER> anymore,m undo button . set on touch listener ( new on touch listener ( ) { @ override public boolean on touch ( view v @$ motion event event ) { m valid delayed msg id ++ ; return false ; } } ) ;,delay hide
first see if the file matches the regular <PLACE_HOLDER> !,if ( pattern != null ) { matcher matcher = pattern . matcher ( item . get name ( ) . geturi ( ) ) ; get it = matcher . matches ( ) ; } if ( patternexclude != null ) { matcher matcherexclude = patternexclude . matcher ( item . get name ( ) . geturi ( ) ) ; get itexclude = matcherexclude . matches ( ) ; } boolean take = take this file ( item @$ new file name ) ; if ( get it && ! get itexclude && take ) { if ( log . is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ item . get name ( ) .,file matches
since file download task wraps the actual <PLACE_HOLDER> with download <PLACE_HOLDER> . we should extract it letting the error message clearer .,if ( ! is dependents succeeded ( ) ) { exception t = task . get exception ( ) ; if ( t instanceof download exception ) throw new library download exception ( library @$ t . get cause ( ) ) ; else throw new library download exception ( library @$ t ) ; } else { if ( xz ) unpack library ( jar @$ files . read all bytes ( xz file . to path ( ) ) ) ; if ( ! checksum valid ( jar @$ library . get checksums ( ) ) ) { jar . delete ( ) ; throw new io exception ( __str__ + library ) ; } },task wraps
unmatched files contains any files that had no association to a program give the user a <PLACE_HOLDER> to search the project for it @$ and import it if not found,if ( ! unmatched files . is empty ( ) ) { unmatched files = search project for matching files or fail ( unmatched files @$ program manager @$ monitor @$ programs opened ) ; } return unmatched files ;,files give
the new code can collapse extra separator <PLACE_HOLDER>,compare ( __str__ @$ __str__ ) ; compare ( __str__ @$ __str__ ) ;,code collapse
if the client supplied connection credentials @$ the <PLACE_HOLDER> filter will perform a normal <PLACE_HOLDER> @$ so we should exit immediately :,if ( credentials available ( conn ) ) { return ; },filter perform
wait a minute and you should get other 6 <PLACE_HOLDER> executed,wait minute quota ( ) ;,minute get
did n't enclose table pattern within single quotes . table name and include list not allowed . table name and exclude list not allowed . abrubtly ended <PLACE_HOLDER> . with square brackets two <PLACE_HOLDER>s with empty list more than two list,string [ ] invalid repl policies = new string [ ] { primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ } ;,name ended
finally check account notes did get <PLACE_HOLDER>,assert . assert null ( retrieved account . get notes ( ) ) ;,notes get
the test passed @$ so just <PLACE_HOLDER> from main and harness will interepret this <PLACE_HOLDER> as a pass,return ;,return interepret
the handler should remove <PLACE_HOLDER> .,assert null ( channel . pipeline ( ) . context ( elb proxy protocol channel handler . name ) ) ; assert equals ( ha proxy protocol version . v1 @$ channel . attr ( elb proxy protocol channel handler . attr_haproxy_version ) . get ( ) ) ;,handler remove
cooked string can be null only for tagged template literals . a tagged template literal would hit the default <PLACE_HOLDER> below .,if ( node . has one child ( ) ) { return check not null ( node . get first child ( ) . get cooked string ( ) ) ; } else { throw new malformed exception ( __str__ @$ node ) ; },literal hit
any <PLACE_HOLDER> but primary & proxy can contain any <PLACE_HOLDER> :,if ( m != primary && m != proxy ) for ( igfs mode n : igfs mode . values ( ) ) assert true ( igfs utils . can contain ( m @$ n ) ) ;,primary contain
async write the <PLACE_HOLDER> back .,member . remote . send string ( data @$ null ) ;,async write
crawl job needs to be sure all beans have received finished <PLACE_HOLDER> before teardown,this . is stop complete = true ; app ctx . publish event ( new stop complete event ( this ) ) ;,beans received
any errors here should let the <PLACE_HOLDER> come to a halt and be recognized by the writer,flush all ( ) ;,errors let
we keep a separate list of <PLACE_HOLDER> for each object newly obtained from a view @$ and perform a shallow copy during get clone . that way the list of <PLACE_HOLDER> performed contains all <PLACE_HOLDER> performed on the view by the tree of nodes initialized from it . note that initializing two nodes with the same view will not merge the two lists @$,shadow obtained . performed action and args list = new array list < > ( ) ; shadow obtained . view = view ; s allocation count ++ ; if ( shadow obtained . m origin node id == __num__ ) { shadow obtained . m origin node id = s allocation count ; } strict equality node wrapper wrapper = new strict equality node wrapper ( obtained instance ) ; obtained instances . put ( wrapper @$ thread . current thread ( ) . get stack trace ( ) ) ; ordered instances . put ( s allocation count @$ wrapper ) ; return obtained instance ;,list contain
we should recalculate geometry just before calculation of the thumb movement direction . it is important for the case @$ when j slider is a cell editor in j table . see <PLACE_HOLDER> .,calculate geometry ( ) ; final boolean first click = ( current mousex == - __num__ ) && ( current mousey == - __num__ ) ; current mousex = e . getx ( ) ; current mousey = e . gety ( ) ; if ( slider . is request focus enabled ( ) ) { slider . request focus ( ) ; } boolean is mouse event in thumb = thumb rect . contains ( current mousex @$ current mousey ) ;,calculation see
skip non union r dot <PLACE_HOLDER>,fake buildable context buildable context = new fake buildable context ( ) ; list < step > steps = dummyr dot java . get build steps ( fake build context . noop_context @$ buildable context ) ; assert equals ( __str__ @$ __num__ @$ steps . size ( ) ) ; path r dot java src folder = dummyr dot java . getr dot java src folder ( dummyr dot java . get build target ( ) @$ filesystem ) ; path r dot java bin folder = compiler output paths . get classes dir ( dummyr dot java . get build target ( ) @$ filesystem ) ; path r dot java output folder = dummyr dot java . get path to output dir ( dummyr dot,r dot
the immersive mode confirmation took the focus from m last focused <PLACE_HOLDER> which was controlling the system ui visibility . so if m last focused <PLACE_HOLDER> can still receive keys @$ we let it keep controlling the visibility .,if ( win candidate . get attrs ( ) . token == m immersive mode confirmation . get window token ( ) ) { final boolean last focus can receive keys = ( m last focused window != null && m last focused window . can receive keys ( ) ) ; win candidate = is status bar keyguard ( ) ? m status bar : last focus can receive keys ? m last focused window : m top fullscreen opaque window state ; if ( win candidate == null ) { return __num__ ; } },focus focused
reset the complex type attribute <PLACE_HOLDER> such that it is recalculated again .,cleanup is complex type attribute ( ) ; int index = __num__ ; for ( observed object o : observed objects ) { reset already notified ( o @$ index ++ @$ observed_attribute_error_notified | observed_attribute_type_error_notified ) ; },type attribute
verify ask after registration but before launch . do n't kill @$ should be null . now put a <PLACE_HOLDER> with the id,listener . register pending task ( task @$ wid ) ; result = listener . get task ( context ) ; assert null ( result ) ;,registration put
string object 2 <PLACE_HOLDER> and a reference on a string,int string size = jvm size utils . get object header size ( ) + ( __num__ * __num__ ) + jvm size utils . get reference size ( ) ;,string object
src 1 has no <PLACE_HOLDER> to accommodate new rename node,fs . set quota ( src1 . get parent ( ) @$ __num__ @$ hdfs constants . quota_dont_set ) ; create file ( src1 ) ; fs . set quota ( src1 . get parent ( ) @$ __num__ @$ hdfs constants . quota_dont_set ) ; rename ( dst1 @$ src1 @$ false @$ true @$ rename . overwrite ) ;,src has
figure out <PLACE_HOLDER> class loader to use for loading the provider class . if there is a context class loader then use it .,class loader context = ss . get context class loader ( ) ; class loader system = ss . get system class loader ( ) ; class loader chain = system ; while ( true ) { if ( context == chain ) { class loader current = object factory . class . get class loader ( ) ; chain = system ; while ( true ) { if ( current == chain ) { return system ; } if ( chain == null ) { break ; } chain = ss . get parent class loader ( chain ) ; } return current ; } if ( chain == null ) { break ; } chain = ss . get parent class loader ( chain ) ;,loader use
the rest li response data <PLACE_HOLDER> . this can potentially be refactored .,throwable error ; try { rest li service exception service exception = response data . get response envelope ( ) . get exception ( ) ; final rest li response response = _response handler . build partial response ( _method @$ response data ) ; error = new rest li response exception ( service exception @$ response ) ; } catch ( throwable throwable ) { logger . error ( __str__ @$ response data . get response envelope ( ) . get exception ( ) ) ; error = throwable ; } _wrapped callback . on error ( error ) ;,rest li
for every pixel in this row get the <PLACE_HOLDER>,while ( off < row limit ) { pixels [ off ++ ] = index into gradients arrays ( g ) ; g += dgdx ; },pixel get
foo impl was implicitly bound @$ it is an error to call get instance or get <PLACE_HOLDER> @$ it is ok to call get binding for introspection @$ but an error to get the <PLACE_HOLDER> of the binding,ensure fails ( injector @$ allow_binding @$ foo impl . class ) ;,error get
note : other parent readers init <PLACE_HOLDER> in ctor @$ but union does it in start stripe .,this . tags = new run length byte reader ( data stream ) ;,readers init
generating portfolio object <PLACE_HOLDER> to be populated across the pr 's & local regions,portfolio [ ] portfolio = create portfolios and positions ( cnt dest ) ; new portfolio [ ] new portfolio = create new portfolios and positions ( cnt dest ) ; vm0 . invoke ( pr queryd unit helper . get cache serializable runnable forpr puts ( local name @$ portfolio @$ cnt @$ cnt dest ) ) ; vm0 . invoke ( pr queryd unit helper . get cache serializable runnable forpr puts ( colo local name @$ new portfolio @$ cnt @$ cnt dest ) ) ;,portfolio object
check if no view has <PLACE_HOLDER>,view view = get current focus ( ) ; if ( view != null ) { input method manager input manager = ( input method manager ) get system service ( context . input_method_service ) ; if ( input manager != null ) input manager . hide soft input from window ( view . get window token ( ) @$ input method manager . hide_not_always ) ; },view has
sql server and oracle does n't normally support <PLACE_HOLDER> by in subqueries ...,if ( ! ( get dialect ( ) instanceof sql server dialect ) && ! ( get dialect ( ) instanceof oracle8i dialect ) ) { dc4 . get executable criteria ( session ) . add order ( order . asc ( __str__ ) ) . list ( ) ; } session . create criteria ( enrolment . class @$ __str__ ) . add ( subqueries . eq ( __str__ @$ dc4 ) ) . list ( ) ; session . delete ( enrolment2 ) ; session . delete ( gavin ) ; session . delete ( course ) ; t . commit ( ) ; session . close ( ) ;,server support
track the keys the last time set called set extras . this way @$ the next time set extras is called we can see if the caller has removed any extras <PLACE_HOLDER> .,if ( m previous extra keys == null ) { m previous extra keys = new array set < string > ( ) ; } m previous extra keys . clear ( ) ; if ( extras != null ) { m previous extra keys . add all ( extras . key set ( ) ) ; },caller removed
ca n't use high watermark directly @$ as the partition map may have different <PLACE_HOLDER> . for example @$ high watermark may be specified to seconds @$ but partition map could be specified to hour or date .,long highest watermark = collections . max ( partition map . values ( ) ) ; for ( map . entry < long @$ long > entry : partition map . entry set ( ) ) { long partition high watermark = entry . get value ( ) ; if ( partition high watermark . equals ( highest watermark ) ) { partitions . add ( new partition ( entry . get key ( ) @$ partition high watermark @$ true @$ has user specified high watermark ) ) ; } else { partitions . add ( new partition ( entry . get key ( ) @$ partition high watermark @$ false ) ) ; } },map have
preserve the same order that github org api <PLACE_HOLDER>,map < string @$ scm organization > org map = new linked hash map < > ( ) ;,github org
parse float allows an <PLACE_HOLDER> or a sign,if ( s . matches ( __str__ ) ) { throw new number format exception ( __str__ ) ; } try { float value = float . parse float ( s ) ; if ( value < __num__ || value > __num__ ) { throw new number format exception ( __str__ ) ; } return value ; } catch ( number format exception e ) { throw new number format exception ( __str__ ) ; },float allows
if n has 1 and m has <PLACE_HOLDER>,if ( is bitn && ! is bitm ) { m = m | ( m << k ) ; } else if ( ! is bitn && is bitm ) { int mask = ~ ( m << k ) ; m = m & mask ; },n has
deep copy the matchers and logs @$ which allows the <PLACE_HOLDER> to be reused .,for ( matcher and error m : matchers and logs ) { this . matchers and logs . add ( m ) ; },which allows
now generate another <PLACE_HOLDER> @$ but without having updated the package . the package tracker should recognize the last <PLACE_HOLDER> failed and trigger again .,simulate package installation ( package versions ) ;,tracker recognize
fancier parser would give line <PLACE_HOLDER> and column ...,if ( ! actual str . equals ignore case ( expected str ) ) { throw new io exception ( __str__ + printable ( expected str ) + __str__ + printable ( actual str ) ) ; },parser give
if this edit is creating the <PLACE_HOLDER> for the first time @$ every index must have a value .,if ( success && ! entry . readable ) { for ( int i = __num__ ; i < value count ; i ++ ) { if ( ! editor . written [ i ] ) { editor . abort ( ) ; throw new illegal state exception ( __str__ + i ) ; } if ( ! entry . get dirty file ( i ) . exists ( ) ) { editor . abort ( ) ; return ; } } } for ( int i = __num__ ; i < value count ; i ++ ) { file dirty = entry . get dirty file ( i ) ; if ( success ) { if ( dirty . exists ( ) ) { file clean =,edit creating
no instance has correct <PLACE_HOLDER> configured,try { driver . assign instances ( instance partitions type . offline @$ instance configs ) ; fail ( ) ; } catch ( illegal state exception e ) { assert equals ( e . get message ( ) @$ __str__ ) ; } for ( int i = __num__ ; i < num instances ; i ++ ) { instance config instance config = instance configs . get ( i ) ; if ( i < num instances / __num__ ) { instance config . get record ( ) . set map field ( instance . pool_key @$ collections . singleton map ( offline_tag @$ __str__ ) ) ; } else { instance config . get record ( ) . set map field ( instance . pool_key,instance has
traverse the children which can be either just include tables list or both include and exclude tables <PLACE_HOLDER> .,tree old policy tables list node = old repl policy tree . get child ( __num__ ) ; assert ( old policy tables list node . get type ( ) == tok_repl_tables ) ; set repl dump tables list ( old policy tables list node @$ old repl scope ) ;,list include
construct container resource increase <PLACE_HOLDER> @$,list < token > increase tokens = new array list < > ( ) ;,resource increase
this function needs its stack looked at . set its stack <PLACE_HOLDER> to invalid @$ so wo n't be added to list again if called recursively .,func . set stack purge size ( function . invalid_stack_depth_change ) ; func list . add ( __num__ @$ func ) ;,stack set
instead of getting this directly from the lp @$ use reflection so that a package which uses grammatical <PLACE_HOLDER> does n't necessarily have to use lexicalized parser,try { method method = lp . get class ( ) . get method ( __str__ ) ; params = ( treebank lang parser params ) method . invoke ( lp ) ; params . set generate original dependencies ( generate original dependencies ) ; } catch ( exception cnfe ) { throw new runtime exception ( cnfe ) ; },which uses
we can now utilize shader program 's hash map which may be better than gl get uniform <PLACE_HOLDER>,shader program . set strict mode ( false ) ;,gl get
if specified enable <PLACE_HOLDER>,try { if ( ! kerberos . is empty ( ) ) { config . enable kerberos authentication ( kerberos ) ; } m_client = get client ( config @$ servers @$ port ) ; } catch ( exception exc ) { system . err . println ( exc . get message ( ) ) ; return - __num__ ; },specified enable
and will throw exception . for cases when users will do <PLACE_HOLDER> as result of that as well we need to support chain of <PLACE_HOLDER> calls but still fail on <PLACE_HOLDER> @$ commit,if ( ! is open ( ) && failure ) { return ; } failure ( ) ; close transaction ( ) ;,users do
this special create table is called locally to master . therefore @$ no rpc means no <PLACE_HOLDER> to use nonce to detect duplicated rpc call .,long proc id = this . procedure executor . submit procedure ( new create table procedure ( procedure executor . get environment ( ) @$ table descriptor @$ new regions ) ) ; return proc id ;,rpc means
download attributes and return file <PLACE_HOLDER> only if the blob exists .,if ( null != blob && blob . exists ( get instrumented context ( ) ) ) { log . debug ( __str__ @$ key ) ; try { blob . download attributes ( get instrumented context ( ) ) ; blob properties properties = blob . get properties ( ) ; if ( retrieve folder attribute ( blob ) ) { log . debug ( __str__ @$ key ) ; return new file metadata ( key @$ properties . get last modified ( ) . get time ( ) @$ get permission status ( blob ) @$ blob materialization . explicit @$ hadoop block size ) ; } else { log . debug ( __str__ @$ key ) ; return new file metadata ( key @$ get,attributes file
verify new file is using it 's own <PLACE_HOLDER> @$ with new keyversions @$ and can be decrypted correctly .,assert key version changed ( recreated @$ fei orig ) ; final string content = dfs test util . read file ( fs @$ recreated ) ; assert equals ( content orig @$ content ) ;,file using
this call not only toggles the error <PLACE_HOLDER> on @$ but also wires our special error display wrapper .,set errorgui enabled ( true ) ;,call toggles
server socket will accept <PLACE_HOLDER>,socket base server = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( server @$ not null value ( ) ) ; zmq . set socket option ( server @$ zmq . zmq_curve_server @$ true ) ; zmq . set socket option ( server @$ zmq . zmq_curve_secretkey @$ server secret ) ; zmq . set socket option ( server @$ zmq . zmq_identity @$ __str__ ) ; rc = zmq . bind ( server @$ host ) ; assert that ( rc @$ is ( true ) ) ; host = ( string ) zmq . get socket option ext ( server @$ zmq . zmq_last_endpoint ) ; int port = test utils . port ( host ) ;,socket accept
unmanaged a ms do <PLACE_HOLDER> amrm token,assert . assert not null ( rm client . getamrm token ( app id ) ) ; return app id ;,ms do
create second expired certificate whose key usage extension does not allow code <PLACE_HOLDER>,create alias ( second_key_alias ) ; issue cert ( second_key_alias @$ __str__ @$ __str__ @$ __str__ @$ __str__ + validity * __num__ + __str__ @$ __str__ @$ integer . to string ( validity ) ) ;,extension allow
in case default constructor someday does <PLACE_HOLDER>,this ( ) ;,constructor does
we allow one fewer output buffer due to the way that media codec r<PLACE_HOLDER>erer and the underlying decoders handle the <PLACE_HOLDER> of stream . this should be tightened up in the future .,decoder counters util . assert total buffer count ( tag + audio_tag_suffix @$ audio counters @$ audio counters . input buffer count - __num__ @$ audio counters . input buffer count ) ; decoder counters util . assert total buffer count ( tag + video_tag_suffix @$ video counters @$ video counters . input buffer count - __num__ @$ video counters . input buffer count ) ;,renderer handle
make sure vm 1 did n't create the <PLACE_HOLDER>,assert that ( vm1 . invoke ( ( ) -> get bucket list ( partitioned region name ) ) ) . is empty ( ) ; vm1 . invoke ( ( ) -> get cache ( ) . close ( ) ) ; async invocation < void > create partitioned region onvm0 = vm0 . invoke async ( ( ) -> create partitioned region ( __num__ @$ - __num__ @$ __num__ @$ true ) ) ; create partitioned region onvm0 . join ( seconds . to millis ( __num__ ) ) ;,vm create
if there is no existing named view add <PLACE_HOLDER>,add to cache ( named view detail @$ false ) ; return ;,view add
if we have found the account <PLACE_HOLDER> for this protocol provider return this <PLACE_HOLDER>,if ( index != null ) { return integer . parse int ( index ) ; } else { return create account index ( protocol provider @$ account root prop name ) ; },index return
support for automatic static linking of standard libraries . this works because all of the jdk uses system.load <PLACE_HOLDER> or jdk.internal.loader.boot loader with literal string arguments . if such a <PLACE_HOLDER> is in our list of static standard libraries @$ add the <PLACE_HOLDER> to the linker command .,if ( libname != null && native library support . singleton ( ) . is preregistered builtin library ( libname ) && registered libraries . put if absent ( libname @$ boolean . true ) != boolean . true ) { native libraries . add library ( libname @$ true ) ; },all uses
mock suspend @$ which stops <PLACE_HOLDER> 1 and sets suspended state in metadata @$ flipping to <PLACE_HOLDER> 2 in test <PLACE_HOLDER> spec implementation of create suspended spec,reset all ( ) ; metadata supervisor manager . insert ( easy mock . eq ( __str__ ) @$ easy mock . capture ( captured insert ) ) ; supervisor2 . start ( ) ; supervisor1 . stop ( true ) ; replay all ( ) ; manager . suspend or resume supervisor ( __str__ @$ true ) ; assert . assert equals ( __num__ @$ manager . get supervisor ids ( ) . size ( ) ) ; assert . assert equals ( captured insert . get value ( ) @$ manager . get supervisor spec ( __str__ ) . get ( ) ) ; assert . assert true ( captured insert . get value ( ) . suspended ) ; verify all ( ) ;,which stops
nm 1 do 50 <PLACE_HOLDER>,capacity scheduler cs = ( capacity scheduler ) rm1 . get resource scheduler ( ) ; rm node rm node1 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ; rm node rm node2 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm2 . get node id ( ) ) ; scheduler node scheduler node1 = cs . get scheduler node ( nm1 . get node id ( ) ) ;,nm do
if <PLACE_HOLDER> is declared to be thrown by the proxy method @$ then no catch blocks are necessary @$ because the invoke can @$ at most @$ throw <PLACE_HOLDER> anyway .,if ( ex . is assignable from ( throwable . class ) ) { unique list . clear ( ) ; break ; } else if ( ! throwable . class . is assignable from ( ex ) ) { continue ; },invoke throw
check size will open a new <PLACE_HOLDER> if we exceeded the max bytes setting,writer . check size ( ) ; if ( writer . get position ( ) != position ) { position = writer . get position ( ) ; },size open
use in alert prevents <PLACE_HOLDER> of x,test same ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,use prevents
one of the two does n't have a <PLACE_HOLDER> . only return true if the other one also does n't have a <PLACE_HOLDER> .,if ( ( last dot1 == - __num__ ) || ( last dot2 == - __num__ ) ) { return ( last dot1 == last dot2 ) ; } else { int idx1 = __num__ ; int idx2 = __num__ ; if ( name1 . char at ( idx1 ) == __str__ ) { do { idx1 ++ ; } while ( name1 . char at ( idx1 ) == __str__ ) ; if ( name1 . char at ( idx1 ) != __str__ ) { throw new internal error ( __str__ + name1 ) ; } } if ( name2 . char at ( idx2 ) == __str__ ) { do { idx2 ++ ; } while ( name2 . char at ( idx2 ) == __str__,one have
asif asif : the iter operands passed are null @$ as a not null value can exists only if there exists a single <PLACE_HOLDER> operand in original group junction,filter results = filter . filter evaluate ( context @$ ! is conditioning needed ? intermediate results : null @$ this . complete expansion @$ null @$ this . indpndnt itr @$ _operator == literal_and @$ is conditioning needed @$ false ) ;,value exists
a media player created by a video view should already have its m subtitle controller <PLACE_HOLDER> .,if ( m subtitle controller == null ) { set subtitle anchor ( ) ; } if ( ! m subtitle controller . has renderer for ( f format ) ) { context context = activity thread . current application ( ) ; m subtitle controller . register renderer ( new srt renderer ( context @$ m event handler ) ) ; },player have
the time we accessed wrapper.reference count @$ the wrapper was tombstoned by a pending release task . this race condition is highly unlikely to happen as there is no systematic coding practice which can cause this error because of ttl . however @$ even in very unlikely case when it happen we have the retry which get a valid <PLACE_HOLDER> . note : there,for ( int retry = __num__ ; retry < max_retry ; retry ++ ) { wrapped context wrapper = get cache ( ) . compute if absent ( job info . job id ( ) @$ job id -> { try { return new wrapped context ( job info @$ creator . apply ( job info ) ) ; } catch ( exception e ) { throw new runtime exception ( __str__ + job info . job id ( ) @$ e ) ; } } ) ; synchronized ( wrapper ) { if ( wrapper . reference count != null ) { wrapper . reference count . increment and get ( ) ; return wrapper ; } } },which get
calls to functions that have no side effects have the no side effect <PLACE_HOLDER> set .,if ( ! function call has side effects ( n ) ) { break ; },calls have
mp 3 live streams commonly have seekable <PLACE_HOLDER> @$ despite being unseekable .,if ( icy headers != null && extractor instanceof mp3 extractor ) { ( ( mp3 extractor ) extractor ) . disable seeking ( ) ; } if ( pending extractor seek ) { extractor . seek ( position @$ seek time us ) ; pending extractor seek = false ; } while ( result == extractor . result_continue && ! load canceled ) { load condition . block ( ) ; result = extractor . read ( input @$ position holder ) ; if ( input . get position ( ) > position + continue loading check interval bytes ) { position = input . get position ( ) ; load condition . close ( ) ; handler . post ( on continue loading requested runnable ),streams have
redex access the constructed proguard command line and then goes and opens a <PLACE_HOLDER> of the files listed there .,return immutable list . < source path > builder ( ) . add all ( classpath entries to dex source paths ) . add all ( rich stream . from ( proguard config ) . collect ( collectors . to list ( ) ) ) . add all ( proguard configs ) . build ( ) ;,access goes
test read 0 <PLACE_HOLDER> .,test array . set position ( __num__ ) ; result [ __num__ ] = __num__ ; test array . read bits ( result @$ __num__ @$ __num__ ) ; assert that ( result [ __num__ ] ) . is equal to ( ( byte ) __num__ ) ;,test read
successful connect request <PLACE_HOLDER> in a response with empty body .,if ( status code == __num__ ) { if ( http method . connect . equals ( method ) ) { done = true ; queue . clear ( ) ; return true ; } },successful connect
killed jobs might not have <PLACE_HOLDER>,if ( total counters != null ) { json object j groups = new json object ( ) ; for ( counter group counter group : total counters ) { string group name = counter group . get name ( ) ; counter group total group = total counters . get group ( group name ) ; counter group map group = map counters . get group ( group name ) ; counter group reduce group = reduce counters . get group ( group name ) ; iterator < counter > ctr itr = total group . iterator ( ) ; json array j group = new json array ( ) ; while ( ctr itr . has next ( ) ) { json object j counter =,jobs have
we are tolerant here because frameworks such as avro accept a boxed <PLACE_HOLDER> even though the field is primitive,if ( ! box primitive ( parameter type ) . equals ( box primitive ( field type ) ) ) { return null ; },frameworks accept
note to translators : this message is used to indicate the severity of another message . the substitution text contains two error <PLACE_HOLDER> . the spacing before the second substitution text indents it the same amount as the first in english .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text contains
start task c which will prevent the <PLACE_HOLDER> from being completed directly @$ if stage a completes,cmmn runtime service . start plan item instance ( get plan item instance id by name ( plan item instances @$ __str__ ) ) ;,which prevent
this is too long @$ and the ri just returns the input <PLACE_HOLDER> ...,string long input = make puny string ( __num__ ) ; assert equals ( long input @$ idn . to unicode ( long input ) ) ;,ri returns
simple date format has this <PLACE_HOLDER>,date time formatter test = fmt . with locale ( locale . english ) . with decimal style ( decimal style . standard ) ; format format = test . to format ( ) ; parse position pos = new parse position ( __num__ ) ; format . parse object ( ( string ) null @$ pos ) ;,format has
so that each bucket has <PLACE_HOLDER> splits so that each bucket has <PLACE_HOLDER> splits so that there 's no minimum split size for the file so that the file is splittable,map < string @$ string > hive bucketed properties = immutable map . < string @$ string > builder ( ) . put all ( hive properties ) . put ( __str__ @$ __str__ ) . put ( __str__ @$ __str__ ) . put ( __str__ @$ __str__ ) . put ( __str__ @$ __str__ ) . build ( ) ;,bucket has
the healthy path will never see this synchronized <PLACE_HOLDER>,synchronized ( resource holder . bad_filters ) { long last time = resource holder . bad_filters . get ( f ) ; if ( last time == null || last time + time unit . minutes . to millis ( interval ) < system . current time millis ( ) ) { resource holder . bad_filters . put ( f @$ system . current time millis ( ) ) ; return level . warning ; } else { return level . fine ; } },path see
let basic date click handler handle calendar <PLACE_HOLDER> @$ and update only the other parts of ui here,calendar component . set handler ( new basic date click handler ( ) { @ override public void date click ( date click event event ) { super . date click ( event ) ; switch to day view ( ) ; } } ) ;,handler handle
this will actually mark the parent as a dir @$ so that lists of that dir will pick up the <PLACE_HOLDER>,put file ( child @$ now @$ null ) ;,lists pick
check whether a provider can provide an implementation that 's closer to the requested locale than <PLACE_HOLDER> the java runtime itself can provide .,try { return new simple date format ( time style @$ date style @$ loc ) ; } catch ( missing resource exception e ) { return new simple date format ( __str__ ) ; },java runtime
some tests only run on linux @$ those wo n't create a <PLACE_HOLDER> on other os,if ( client != null ) client . close ( ) ;,tests create
now new job requests should succeed as list operation has no cancel <PLACE_HOLDER> .,job runnable = concurrent list jobs ( __num__ @$ config @$ false @$ false @$ list job helper . get delayed resonse answer ( __num__ @$ new array list < job item bean > ( ) ) ) ; assert true ( job runnable . exception == null ) ;,operation has
we bypass the standard equals <PLACE_HOLDER> that resolve the hostname,assert that ( slave . get channel ( ) . call ( new url builder callable ( __str__ ) ) @$ not ( equal to ( slave . get channel ( ) . call ( new url builder callable ( __str__ ) ) ) ) ) ;,standard equals
expressions propagate linked <PLACE_HOLDER> ; statements do not .,return node util . is statement ( parent ) ? empty : propagate ( false ) ;,expressions propagate
for joda time <PLACE_HOLDER>s @$ return sql <PLACE_HOLDER> for java.util.date .,if ( raw type instanceof class && abstract instant . class . is assignable from ( ( class < ? > ) raw type ) ) { return type factory . create java type ( date . class ) ; } else if ( raw type instanceof class && byte string . class . is assignable from ( ( class < ? > ) raw type ) ) { return type factory . create java type ( byte [ ] . class ) ; } return type factory . create java type ( ( class ) raw type ) ;,return sql
check any changes do not break already encoded <PLACE_HOLDER>,string password = __str__ ; string obfuscate = __str__ ; assert equals ( password @$ password . deobfuscate ( obfuscate ) ) ;,changes break
assert that for each now produces no <PLACE_HOLDER>,spliterator . try advance ( boxing adapter . apply ( e -> fail ( __str__ + e ) ) ) ; spliterator < t > split = spliterator . try split ( ) ; assert null ( split ) ; dest . add all ( leaf dest ) ;,each produces
at this point all sub<PLACE_HOLDER>s are destroyed and this <PLACE_HOLDER> has been marked as destroyed and post destroy <PLACE_HOLDER> has been called for each <PLACE_HOLDER> . the only detail left is unhooking this <PLACE_HOLDER> from the parent sub<PLACE_HOLDER> map @$ and sending listener events,assert . assert true ( is destroyed ) ;,left unhooking
return the nicer break length which does n't split a <PLACE_HOLDER> up,return temp length + __num__ ;,which split
put records into the queues using the queue <PLACE_HOLDER> . each worker will pull and process .,try { record record = records . next ( ) ; id distributor . distribute ( record @$ record consumer ) ; progress . add ( __num__ ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; break ; },queues using
this <PLACE_HOLDER> to shutdown will eventually make a <PLACE_HOLDER> to population cancelled on the monitor below,dbms . shutdown ( ) ;,call make
make sure the fragment either has no <PLACE_HOLDER> or dependencies have been dealt with,set < node > node with fragment = connected nodes . stream ( ) . filter ( node -> ! node . get fragments without dependency ( ) . is empty ( ) || ! node . get fragments with dependency visited ( ) . is empty ( ) ) . collect ( collectors . to set ( ) ) ;,fragment has
cut off to maximum queue <PLACE_HOLDER> if update <PLACE_HOLDER> is exceeding queue <PLACE_HOLDER> .,new expire time . put ( application timeout type . lifetime @$ updatedlifetime in millis ) ; new timeout iniso8601 format . put ( application timeout type . lifetime @$ times . formatiso8601 ( updatedlifetime in millis . long value ( ) ) ) ;,lifetime exceeding
if the expanded child has the same <PLACE_HOLDER> as the collapsed one we hide it .,if ( m expanded child != null && m expanded child . get height ( ) != __num__ ) { if ( ( ! m is heads up && ! m heads up animating away ) || m heads up child == null || ! m containing notification . can show heads up ( ) ) { if ( m expanded child . get height ( ) <= m contracted child . get height ( ) ) { expandable = false ; } } else if ( m expanded child . get height ( ) <= m heads up child . get height ( ) ) { expandable = false ; } } if ( m expanded child != null ) { m expanded wrapper . update expandability,child has
create a job launcher instance depending on the configuration . the same <PLACE_HOLDER> object is used for both system and job configuration <PLACE_HOLDER> because azkaban puts configuration <PLACE_HOLDER> in the .job file and in the .<PLACE_HOLDER> file into the same <PLACE_HOLDER> object .,this . job launcher = this . closer . register ( job launcher factory . new job launcher ( job props @$ job props @$ null @$ metadata tags ) ) ;,azkaban puts
nor the admin have that <PLACE_HOLDER>,jenkins rule . web client wc = j . create web client ( ) ; wc . login ( __str__ ) ; wc . get options ( ) . set throw exception on failing status code ( false ) ; page page = wc . go to ( __str__ + __num__ + __str__ ) ; assert equals ( __num__ @$ page . get web response ( ) . get status code ( ) ) ; assert request was blocked and reset flag ( ) ;,admin have
we 'd like to have 20 buckets per member @$ but what we 'll find is that member 1 will have 15 and 2 and 3 will have <PLACE_HOLDER> and 18 .,for ( partition member info details : model . get partitioned member details ( __str__ ) ) { assert equals ( __num__ @$ details . get bucket count ( ) ) ; },member have
server did not perform the <PLACE_HOLDER> @$ so do n't leave an invalid entry here,if ( ! force new entry && event . no version received from server ( ) ) { return false ; },server perform
constructing an a should trigger an <PLACE_HOLDER>,injector . get instance ( a . class ) ; assert equals ( __num__ @$ injections . get and set ( __num__ ) ) ;,a trigger
rotate the queue so that each filter gets the <PLACE_HOLDER> in a different order,for ( filter filter : filters ) { mutating descriptions . add last ( mutating descriptions . poll first ( ) ) ; for ( description description : descriptions ) { if ( filter . should run ( description ) ) { add description for filter to map ( descriptions run @$ filter @$ description ) ; } } },filter gets
server socket will accept <PLACE_HOLDER>,socket base req = zmq . socket ( ctx @$ bind type ) ; assert that ( req @$ not null value ( ) ) ; boolean rc = zmq . bind ( req @$ address ) ; assert that ( rc @$ is ( true ) ) ; socket base router = zmq . socket ( ctx @$ connect type ) ; assert that ( router @$ not null value ( ) ) ; string host = ( string ) zmq . get socket option ext ( req @$ zmq . zmq_last_endpoint ) ; assert that ( host @$ not null value ( ) ) ; rc = zmq . connect ( router @$ host ) ; assert that ( rc @$ is ( true ) ),socket accept
tag change from 'unresolved ' to 'string ' does not happen <PLACE_HOLDER> . we just look at the object at the corresponding index and decide based on the oop type .,if ( ctag . is string ( ) ) { symbol sym = cpool . get unresolved string at ( cp index ) ; return __str__ + sym . as string ( ) + __str__ ; } else if ( ctag . is klass ( ) || ctag . is unresolved klass ( ) ) { constant pool . cp slot obj = cpool . get slot at ( cp index ) ; if ( obj . is resolved ( ) ) { klass k = obj . get klass ( ) ; return __str__ + k . get name ( ) . as string ( ) + __str__ + k . get address ( ) + __str__ ; } else if ( obj . is unresolved (,change happen
<PLACE_HOLDER> null means no <PLACE_HOLDER> @$ so the tag is irrelevant since it is used to reset a settings subset their <PLACE_HOLDER>s . also it is irrelevant if the system set the canonical <PLACE_HOLDER> .,if ( default value == null ) { tag = null ; default from system = false ; },system set
parse need convert model names @$ if not set @$ we will convert all <PLACE_HOLDER> in input directory,string need convert model names = conf . get ( __str__ ) ; string [ ] model names = null ; if ( need convert model names == null ) { log . info ( __str__ + model load dir ) ; path model load path = new path ( model load dir ) ; file system fs = model load path . get file system ( conf ) ; file status [ ] file status = fs . list status ( model load path ) ; if ( file status == null || file status . length == __num__ ) { log . error ( __str__ + model load dir ) ; return __str__ ; } list < string > model name list = new array list,parse convert
make sure the <PLACE_HOLDER> that we send to the peer does have an accurate region version <PLACE_HOLDER> for our local version,return create copy ( this . my id @$ cloned holders @$ this . local version . get ( ) @$ gc versions @$ this . localgc version . get ( ) @$ false @$ cloned local holder ) ;,holder have
need to figure out whether space defines the <PLACE_HOLDER> of attribute value or not,cur = end ; pos ++ ; chars [ end ++ ] = __str__ ; for ( ; pos < length && chars [ pos ] == __str__ ; pos ++ ) { chars [ end ++ ] = __str__ ; } if ( pos == length || chars [ pos ] == __str__ || chars [ pos ] == __str__ || chars [ pos ] == __str__ ) { return new string ( chars @$ beg @$ cur - beg ) ; },space defines
white rectangle detector returns points <PLACE_HOLDER> of the rectangle . i want points on the edges .,float centerx = ( pointa . getx ( ) + pointb . getx ( ) + pointc . getx ( ) + pointd . getx ( ) ) / __num__ ; float centery = ( pointa . gety ( ) + pointb . gety ( ) + pointc . gety ( ) + pointd . gety ( ) ) / __num__ ; pointa = move away ( pointa @$ centerx @$ centery ) ; pointb = move away ( pointb @$ centerx @$ centery ) ; pointc = move away ( pointc @$ centerx @$ centery ) ; pointd = move away ( pointd @$ centerx @$ centery ) ; result point point bs ; result point point ds ;,returns points
check if the statements contains a <PLACE_HOLDER> to check not null or require non null,if ( is expression ) { expression expression = ( expression ) stat ; if ( expression instanceof assignment ) expression = ( ( assignment ) expression ) . expression ; if ( ! ( expression instanceof message send ) ) return null ; message send invocation = ( message send ) expression ; if ( ! arrays . equals ( invocation . selector @$ check_not_null ) && ! arrays . equals ( invocation . selector @$ require_non_null ) ) return null ; if ( invocation . arguments == null || invocation . arguments . length == __num__ ) return null ; expression first argument = invocation . arguments [ __num__ ] ; if ( ! ( first argument instanceof single name reference ) ) return null ;,statements contains
then we should be able to start a transaction @$ though perhaps not be able to finish it . this is <PLACE_HOLDER> the individual test methods will be doing . the test passes when transaction.close completes within the test timeout @$ that is @$ it did n't deadlock .,start log rotation latch . await ( ) ;,methods doing
let 's add a <PLACE_HOLDER> of constant rows to test the rle,row . set field value ( __num__ @$ null ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; row . set field value ( __num__ @$ null ) ; for ( int i = __num__ ; i < __num__ ; ++ i ) { writer . add row ( row ) ; } union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; writer . add row ( row ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; writer . add row ( row ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ),"s" add
noop build rule implements supports <PLACE_HOLDER> based rule key @$ which can not add build rule for rule keys .,return false ;,implements supports
make copy of ground items because we are going to modify them here @$ and the array list supports our desired <PLACE_HOLDER> here,if ( plugin . is hot key pressed ( ) ) { ground item list = new array list < > ( ground item list ) ; final java . awt . point awt mouse pos = new java . awt . point ( mouse pos . getx ( ) @$ mouse pos . gety ( ) ) ; ground item ground item = null ; for ( ground item item : ground item list ) { item . set offset ( offset map . compute ( item . get location ( ) @$ ( k @$ v ) -> v != null ? v + __num__ : __num__ ) ) ; if ( ground item != null ) { continue ; } if ( plugin . get,list supports
do n't let focus issues hide the popup <PLACE_HOLDER>,panel . set ignore focus ( true ) ; return p ;,issues hide
this member has a lower <PLACE_HOLDER> that ca n't fit buckets of <PLACE_HOLDER> 30,partition member info impl details3 = build details ( member3 @$ __num__ @$ __num__ @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ) ; model . add region ( __str__ @$ arrays . as list ( details1 @$ details2 @$ details3 ) @$ new fake offline details ( ) @$ true ) ; assert equals ( __num__ @$ do moves ( new composite director ( false @$ false @$ true @$ true ) @$ model ) ) ; assert equals ( collections . empty list ( ) @$ bucket operator . creates ) ;,member has
minimum two level stored artifact <PLACE_HOLDER>,try ( in memory artifact cache in memory artifact cache = new in memory artifact cache ( ) ; two level artifact cache decorator two level cache = new two level artifact cache decorator ( in memory artifact cache @$ test project filesystems . create project filesystem ( tmp . get root ( ) ) @$ buck event bus for tests . new instance ( ) @$ true @$ __num__ @$ optional . empty ( ) ) ) { lazy path dummy file = lazy path . of instance ( tmp . new file ( ) ) ; string test metadata key = __str__ ; two level cache . store ( artifact info . builder ( ) . add rule keys ( dummy rule key ) . set,level stored
treat this as deterministic for reporting purposes : delete statements produce just one <PLACE_HOLDER> that is the number of <PLACE_HOLDER>s affected,boolean order is deterministic = true ; boolean has limit or offset = m_parsed delete . has limit or offset ( ) ;,statements produce
<PLACE_HOLDER> start and <PLACE_HOLDER> end are equivalent positions so to be consistent we translate them to the same integer value . that way we can check whether a run covers the entire <PLACE_HOLDER> by just checking if the start equals the end position .,if ( current run end == range end ) { current run end = range start ; },run covers
wait until the scheduler kills the <PLACE_HOLDER>,generic test utils . wait for ( ( ) -> { try { nm . node heartbeat ( true ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; } scheduler node report report = scheduler . get node report ( nm id ) ; return report . get available resource ( ) . get memory size ( ) > __num__ ; } @$ __num__ @$ __num__ * __num__ ) ;,scheduler kills
<PLACE_HOLDER> set change when new set excludes current <PLACE_HOLDER> and success ful response,int old server index = relay puller . get current server idx ( ) ; server info old server = relay puller . get curent server ( ) ; assert . assert equals ( relay puller . get servers ( ) @$ exp server info2 @$ __str__ ) ; do execute and change state ( relay puller @$ create set server message ( false @$ relay puller ) ) ; assert . assert equals ( relay puller . get current server idx ( ) @$ old server index @$ __str__ ) ; assert . assert equals ( relay puller . get curent server ( ) @$ old server @$ __str__ ) ; assert . assert equals ( relay puller . get servers ( ) @$ exp server info3,set excludes
if this thread created mid key @$ block until the other thread adds a <PLACE_HOLDER> on it .,if ( order == order . before && thread . current thread ( ) . equals ( first thread . get ( ) ) ) { tracking awaiter . instance . await latch and track exceptions ( other thread winning @$ __str__ ) ; } else if ( order == order . after && ! thread . current thread ( ) . equals ( first thread . get ( ) ) ) { other thread winning . count down ( ) ; },thread adds
if primitive just use the primitive <PLACE_HOLDER> not the boxed version,if ( type mirror . get kind ( ) . is primitive ( ) ) { class type = type name . get ( type mirror ) ; },primitive use
another thread already locked this <PLACE_HOLDER> and is processing it . wait for the other thread to finish @$ by locking the existing read lock .,if ( existing lock != null ) { write lock . unlock ( ) ; write lock = null ; read lock = existing lock . read lock ( ) ; read lock . lock ( ) ; if ( m cache . get if present ( alluxio uri . get path ( ) ) != null ) { return false ; } } else { mount table . resolution resolution = m mount table . resolve ( alluxio uri ) ; if ( resolution . get mount id ( ) != mount info . get mount id ( ) ) { return false ; } boolean exists in ufs ; try ( closeable resource < under file system > ufs resource = resolution . acquire ufs resource,thread locked
queue level max allocation ca n't exceed the cluster <PLACE_HOLDER>,capacity scheduler cs = new capacity scheduler ( ) ; cs . set conf ( new yarn configuration ( ) ) ; cs . setrm context ( resource manager . getrm context ( ) ) ; capacity scheduler configuration conf = new capacity scheduler configuration ( ) ; setup queue configuration ( conf ) ; set max alloc mb ( conf @$ yarn configuration . default_rm_scheduler_maximum_allocation_mb ) ; set max alloc vcores ( conf @$ yarn configuration . default_rm_scheduler_maximum_allocation_vcores ) ; long larger mem = yarn configuration . default_rm_scheduler_maximum_allocation_mb + __num__ ; long larger vcores = yarn configuration . default_rm_scheduler_maximum_allocation_vcores + __num__ ; cs . init ( conf ) ; cs . start ( ) ; cs . reinitialize ( conf @$ mock context ) ; check queue,allocation exceed
the output stream must be flushed before the <PLACE_HOLDER> is obtained as the flush can move the <PLACE_HOLDER> forward .,section output stream . flush ( ) ; long length = file channel . position ( ) - sub section offset ; if ( length == __num__ ) { log . warn ( __str__ + __str__ @$ name . to string ( ) ) ; return ; } summary . add sections ( file summary . section . new builder ( ) . set name ( name . name ) . set length ( length ) . set offset ( sub section offset ) ) ; sub section offset += length ;,flush move
java ca n't have a root whose value is null . instead of setting null @$ the method sets user <PLACE_HOLDER> so that other methods are able to know the root should be nil .,if ( new_root == context . nil ) { get document ( ) . get document element ( ) . set user data ( nokogiri helpers . root_node_invalid @$ boolean . true @$ null ) ; return new_root ; } xml node new root = as xml node ( context @$ new_root ) ; i ruby object root = root ( context ) ; if ( root . is nil ( ) ) { node new root node ; if ( get document ( ) == new root . get owner document ( ) ) { new root node = new root . node ; } else { new root node = get document ( ) . import node ( new root . node @$ true ) ;,method sets
if this attribute used an <PLACE_HOLDER> @$ then we ca n't validate at this time .,if ( ! avt . is simple ( ) ) return avt ;,attribute used
backslash and double quote need double the <PLACE_HOLDER> for both java and haskell,special char replacements . remove ( __str__ ) ; special char replacements . remove ( __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ;,backslash double
here we explicitly forget to do the ack or fail it would trigger fail on this <PLACE_HOLDER> on spout end after message timeout seconds,if ( n items % __num__ == __num__ ) { collector . fail ( tuple ) ; } else { collector . ack ( tuple ) ; },seconds fail
the primary zygote did n't match . try the <PLACE_HOLDER> .,if ( m zygote secondary socket address != null ) { attempt connection to secondary zygote ( ) ; if ( secondary zygote state . matches ( abi ) ) { return secondary zygote state ; } },primary try
incorrect for not yet ported 3 rd party storage <PLACE_HOLDER> .,return call . empty list ( ) ;,incorrect ported
output reduce sleep count num reduce <PLACE_HOLDER> of random values @$ so that each reducer will get reduce sleep count <PLACE_HOLDER> of keys .,int k = key . get ( ) ; for ( int i = __num__ ; i < value . get ( ) ; ++ i ) { context . write ( new int writable ( k + i ) @$ null writable . get ( ) ) ; },output reduce
authorization check for kill query will be in kill query impl as both admin or <PLACE_HOLDER> owner can perform the <PLACE_HOLDER> . which is not directly supported in authorizer,if ( driver context . get query state ( ) . get hive operation ( ) != hive operation . kill_query ) { command authorizer . do authorization ( driver context . get query state ( ) . get hive operation ( ) @$ sem @$ context . get cmd ( ) ) ; } session state . get perf logger ( ) . perf log end ( class_name @$ perf logger . do_authorization ) ;,owner perform
dont do actual <PLACE_HOLDER> in check mode .,boolean configcheck = boolean . parse boolean ( config . get property ( export manager . config_check_only @$ __str__ ) ) ; if ( configcheck ) { return ; } m_is krb = boolean . parse boolean ( config . get property ( __str__ @$ __str__ ) ) ; if ( m_is krb ) { m_context = new login context ( __str__ ) ; m_context . login ( ) ; } connect ( ) ; if ( m_is hdfs && endpoint expander . has date conversion ( m_endpoint ) ) { runnable rotator = new runnable ( ) { @ override public void run ( ) { try { roll ( ) ; } catch ( throwable t ) { m_logger . error ( __str__ + throwables .,dont do
wait for the present complete semaphore to be signaled to ensure that the image wo n't be rendered to until the presentation engine has fully released <PLACE_HOLDER> to the application @$ and it is okay to render to the image .,demo_draw_build_cmd ( ) ; long buffer lp2 = stack . malloc long ( __num__ ) ; vk submit info submit_info = vk submit info . malloc stack ( stack ) . s type ( vk_structure_type_submit_info ) . p next ( null ) . wait semaphore count ( __num__ ) . p wait semaphores ( lp . put ( __num__ @$ image acquired semaphore ) ) . p wait dst stage mask ( ip . put ( __num__ @$ vk_pipeline_stage_bottom_of_pipe_bit ) ) . p command buffers ( pp . put ( __num__ @$ draw_cmd ) ) . p signal semaphores ( lp2 . put ( __num__ @$ draw complete semaphore ) ) ; check ( vk queue submit ( queue @$ submit_info @$ vk_null_handle ) ) ; vk present,engine released
this flow covers instance not found exception . actual <PLACE_HOLDER> just eating the exception . i.e actual <PLACE_HOLDER> just printing the stacktrace @$ whenever an exception of type instance not found exception occurs .,mbean container . bean removed ( null @$ managed ) ;,exception found
the entries in the original logs are alternating <PLACE_HOLDER> considering the sequence file header @$ the middle corruption should affect at least half of the entries,int good entries = ( num_writers - __num__ ) * entries ; int first half entries = ( int ) math . ceil ( entries / __num__ ) - __num__ ; int all regions count = split and count ( num_writers @$ - __num__ ) ; assert true ( __str__ @$ regions . size ( ) * ( good entries + first half entries ) <= all regions count ) ;,entries alternating
we can assume here that the request contains all the <PLACE_HOLDER> needed for authentication etc .,try { o auth2 access token token = retrieve token ( request @$ resource @$ get parameters for token request ( resource @$ request ) @$ get headers for token request ( request ) ) ; if ( token == null ) { throw new user redirect required exception ( resource . get user authorization uri ( ) @$ request . to single value map ( ) ) ; } return token ; } catch ( user redirect required exception e ) { throw new user redirect required exception ( e . get redirect uri ( ) @$ request . to single value map ( ) ) ; },request contains
test that set 2 digit year start takes a <PLACE_HOLDER> .,date new date = new date ( ) ; sdf . set2 digit year start ( new date ) ; assert not same ( sdf . get2 digit year start ( ) @$ new date ) ; assert equals ( sdf . get2 digit year start ( ) @$ new date ) ; new date . set time ( __num__ ) ; assert false ( sdf . get2 digit year start ( ) . equals ( new date ) ) ;,test takes
get tunnel interface record ; if no such interface is found @$ will throw illegal argument <PLACE_HOLDER>,tunnel interface record tunnel interface info = user record . m tunnel interface records . get resource or throw ( tunnel resource id ) ; try { m srv config . get netd instance ( ) . interface add address ( tunnel interface info . m interface name @$ local addr . get address ( ) . get host address ( ) @$ local addr . get prefix length ( ) ) ; } catch ( remote exception e ) { throw e . rethrow from system server ( ) ; },record throw
this drop target does n't accept any <PLACE_HOLDER> .,return false ;,target accept
fast source : source produce 8 before @$ and 8 after invocation . buffered source should return all <PLACE_HOLDER> at once .,mock source = new mock split source ( ) . set batch size ( __num__ ) ; try ( split source source = new buffering split source ( mock source @$ __num__ ) ) { mock source . increase available splits ( __num__ ) ; listenable future < next batch result > next batch future = get next batch ( source @$ __num__ ) ; assert false ( next batch future . is done ( ) ) ; mock source . increase available splits ( __num__ ) ; require future value ( next batch future ) . assert size ( __num__ ) . assert no more splits ( false ) ; },source return
check that cluster status reports the correct <PLACE_HOLDER> and backup masters,assert not null ( active ) ; cluster metrics status = active . get cluster metrics ( ) ; assert true ( status . get master name ( ) . equals ( active name ) ) ; assert equals ( __num__ @$ status . get backup master names ( ) . size ( ) ) ;,status reports
bug : diagnostic contains : missing disposable handling : apply auto dispose or cache the disposable <PLACE_HOLDER> manually and enable lenient mode .,maybe . just ( __num__ ) . subscribe ( new test observer < > ( ) ) ;,auto dispose
the wait is necessary to have the poll function complete and propagate the <PLACE_HOLDER> from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one after = database onecode node . get comments ( ) . get local code node comment ( ) ; final list < i comment > two after = database twocode node . get comments ( ) . get local code node comment ( ) ; assert not null ( one after ) ; assert not null ( two after ) ; assert equals ( one before . size ( ) + __num__ @$ one after . size ( ) ) ; assert equals ( two before . size ( ) + __num__ @$ two after . size ( ) ) ; assert equals,function complete
although the streaming ops support multiple <PLACE_HOLDER> @$ we only query one key here,byte array key ; try { if ( key format . equals ( admin parser utils . opt_json ) ) { object key object ; string key serializer name = key serializer def . get name ( ) ; if ( is avro schema ( key serializer name ) ) { schema key schema = schema . parse ( key serializer def . get current schema info ( ) ) ; json decoder decoder = new json decoder ( key schema @$ key string ) ; generic datum reader < object > datum reader = new generic datum reader < object > ( key schema ) ; key object = datum reader . read ( null @$ decoder ) ; } else if ( key serializer name .,ops support
sorani kurdish <PLACE_HOLDER>,if ( __str__ . equals ignore case ( language ) ) { return new sorani stem filter ( token stream ) ; } else if ( __str__ . equals ignore case ( language ) ) { return new snowball filter ( token stream @$ new swedish stemmer ( ) ) ; } else if ( __str__ . equals ignore case ( language ) || __str__ . equals ignore case ( language ) ) { return new swedish light stem filter ( token stream ) ; } else if ( __str__ . equals ignore case ( language ) ) { return new snowball filter ( token stream @$ new turkish stemmer ( ) ) ; },sorani kurdish
verify that the configuration now has three <PLACE_HOLDER> .,configuration = s3 client . get bucket lifecycle configuration ( bucket name ) ; system . out . println ( __str__ + configuration . get rules ( ) . size ( ) ) ;,configuration has
validate router failure <PLACE_HOLDER> matches nn failure <PLACE_HOLDER> .,method m = client protocol . class . get method ( __str__ @$ string . class @$ string . class @$ enum set writable . class ) ; string bad path = __str__ ; enum set writable < create flag > create flag writable = new enum set writable < create flag > ( create flag ) ; compare responses ( router protocol @$ nn protocol @$ m @$ new object [ ] { bad path @$ __str__ @$ create flag writable } ) ;,response matches
allocation of array may have caused gc @$ which may have caused additional <PLACE_HOLDER> to go stale . removing these <PLACE_HOLDER> from the reference queue will make them eligible for reclamation .,while ( queue . poll ( ) != null ) { },which caused
does the handler type map have a <PLACE_HOLDER> and address .,if ( handler type count == __num__ || handler type map address == null || ( is relative ( ) && image base address . equals ( handler type map address ) ) ) { throw new invalid data type exception ( get name ( ) + __str__ ) ; },map have
this is needed to make sure that the policy adds the <PLACE_HOLDER> to the linked list as <PLACE_HOLDER> ready to be evicted,if ( buffer . is locked ( ) ) { buffer . dec ref ( ) ; } cache policy . notify unlock ( buffer ) ;,policy adds
next task takes less than 10 <PLACE_HOLDER> and should be only aggregated,try ( silent closeable c2 = profiler . profile ( profiler task . action_check @$ __str__ ) ) { profiler . log simple task ( blaze clock . instance ( ) . nano time ( ) @$ profiler task . vfs_stat @$ __str__ ) ; long start time = blaze clock . instance ( ) . nano time ( ) ; clock . advance millis ( __num__ ) ; profiler . log simple task ( start time @$ profiler task . vfs_stat @$ __str__ ) ; },task takes
agg spout <PLACE_HOLDER>,if ( is spout ) { map mm = new hash map ( ) ; map acked = client stats util . get map by key ( stats @$ acked ) ; for ( object win : acked . key set ( ) ) { mm . put ( win @$ agg spout lat and count ( ( map ) comp lat stats . get ( win ) @$ ( map ) acked . get ( win ) ) ) ; } mm = swap map order ( mm ) ; w2comp lat wgt avg = client stats util . get map by key ( mm @$ comp_lat_total ) ; w2acked = client stats util . get map by key ( mm @$ acked ) ; } else,agg spout
disable opening assist <PLACE_HOLDER> during setup,if ( ! is user setup complete ( ) ) { return ; },opening assist
if the value is 1 byte and the byte represents null @$ <PLACE_HOLDER> to create the entry . this test needs to be moved to data serializer or data serializer.null needs to be publicly accessible .,boolean result = false ; if ( value == null ) { result = region . basic bridge create ( key @$ null @$ true @$ callback arg @$ server connection . get proxyid ( ) @$ true @$ new eventid holder ( event id ) @$ false ) ; } else { result = region . basic bridge put ( key @$ value @$ null @$ is object @$ callback arg @$ server connection . get proxyid ( ) @$ true @$ new eventid holder ( event id ) ) ; },byte represents
in the worst possible case all the unique values come in consecutive order & hence only 5 iterations will yield the <PLACE_HOLDER>,query observer old = query observer holder . set instance ( new query observer adapter ( ) { @ override public void after iteration evaluation ( object result ) { num [ __num__ ] += __num__ ; } @ override public void before iteration evaluation ( compiled value ritr @$ object curr object ) { if ( data . contains ( curr object ) ) { num repeat [ __num__ ] += __num__ ; } else { data . add ( curr object ) ; } } } ) ; string query string = __str__ ; query = qs . new query ( query string ) ; result = ( select results ) query . execute ( ) ; assert equals ( ( __num__ + num repeat [,values yield
we do n't have any idea how big we are yet and should n't have any pages either . just set <PLACE_HOLDER> up and let the pending layout handle <PLACE_HOLDER> .,if ( m first layout ) { m cur item = item ; if ( dispatch selected && m on page change listener != null ) { m on page change listener . on page selected ( item ) ; } if ( dispatch selected && m internal page change listener != null ) { m internal page change listener . on page selected ( item ) ; } request layout ( ) ; } else { populate ( item ) ; scroll to item ( item @$ smooth scroll @$ velocity @$ dispatch selected ) ; },layout handle
photos and albums have i <PLACE_HOLDER> starting from 1,get album summary ( resp writer @$ ( long ) new random ( ) . next int ( __num__ ) + __num__ ) ; purge all photos ( resp writer ) ; try { latch . await ( ) ; } catch ( interrupted exception e ) { resp writer . println ( e . get message ( ) ) ; },photos i
now that we know its not a primitive @$ lets just allow the passed <PLACE_HOLDER> to handle the request .,return class . for name ( class name @$ false @$ cl ) ;,lets allow
parse root object <PLACE_HOLDER>,document mapper . builder doc builder = new document mapper . builder ( ( root object mapper . builder ) root object type parser . parse ( type @$ mapping @$ parser context ) @$ mapper service ) ; iterator < map . entry < string @$ object > > iterator = mapping . entry set ( ) . iterator ( ) ;,root object
generate entities that their string a field has a <PLACE_HOLDER> over the length limitation,if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__ ; i ++ ) { validation demo . union field with inline record union = new validation demo . union field with inline record ( ) ; union . set my enum ( my enum . foofoo ) ; validation demos . add ( new validation demo ( ) . set stringa ( __str__ ) . set inta ( current criteria . get inta ( ) ) . set stringb ( __str__ ) . set union field with inline record ( union ) ) ; } } else if ( current criteria . get inta ( ) == __num__ ) { for ( int i,string has
put i ps in h in such a way that we believe the mostfront have more <PLACE_HOLDER> to get connected,h . add ( domains . chop zoneid ( ipx ) ) ;,mostfront have
the query should return all <PLACE_HOLDER> in region .,assert equals ( region . size ( ) @$ results . size ( ) ) ; query observer holder . reset ( ) ;,query return
this version of hadoop does not support ec <PLACE_HOLDER> for mr,return ( field != null ) ;,version support
releasing composite should release the remaining <PLACE_HOLDER>,new composite . release ( ) ; assert equals ( __num__ @$ new composite . ref cnt ( ) ) ; assert equals ( __num__ @$ s1 . ref cnt ( ) ) ; assert equals ( __num__ @$ s2 . ref cnt ( ) ) ; assert equals ( __num__ @$ s3 . ref cnt ( ) ) ; assert equals ( __num__ @$ b1 . ref cnt ( ) ) ;,composite release
space at the end allows the remaining <PLACE_HOLDER> ' to be kept intact .,list < string > lines = html line splitter . split ( __str__ @$ __num__ ) ; assert equals ( __num__ @$ lines . size ( ) ) ; assert equals ( __str__ @$ lines . get ( __num__ ) ) ; assert equals ( __str__ @$ lines . get ( __num__ ) ) ;,space allows
given that we do not delete @$ an empty slot means no <PLACE_HOLDER> .,if ( value ref == __num__ ) { return - __num__ ; },slot means
optionally leave the <PLACE_HOLDER> truncated between early truncates @$ but always restore the rows towards the end of the iterations .,if ( leave truncated -- <= __num__ ) { volt queuesql ( renewbase0 ) ; volt executesql ( ) ; volt queuesql ( captureview1 ) ; volt queuesql ( captureview2 ) ; after views = volt executesql ( ) ; validate same ( before views [ __num__ ] @$ after views [ __num__ ] ) ; validate same ( before views [ __num__ ] @$ after views [ __num__ ] ) ; } else { volt queuesql ( captureview1 ) ; volt queuesql ( captureview2 ) ; after views = volt executesql ( ) ; validate purged ( after views ) ; },optionally leave
let 's use whatever is currently thrown exception ... may change <PLACE_HOLDER>,verify exception ( e @$ __str__ ) ;,use change
create a new x path result <PLACE_HOLDER> reuse result <PLACE_HOLDER> passed in ? the constructor will check the compatibility of type and xobj and throw an exception if they are not compatible .,return new x path result impl ( type @$ xobj @$ context node @$ m_xpath ) ;,result object
if both match @$ take <PLACE_HOLDER>,if ( got positive && got negative ) { if ( positive suffix . length ( ) > negative suffix . length ( ) ) { got negative = false ; } else if ( positive suffix . length ( ) < negative suffix . length ( ) ) { got positive = false ; } },both match
paging metadata can not be null at this point so we skip the null check . notice here that we are using automatic intentionally since resource methods can not explicitly project paging . however @$ it should be noted that client resource methods have the option of selectively setting the total to null . this happens if a client decides that they want the,final collection metadata projected paging = new collection metadata ( rest utils . project fields ( paging . data ( ) @$ projection mode . automatic @$ resource context . get paging projection mask ( ) ) ) ;,method see
avoid having to make sure that the array has <PLACE_HOLDER> below .,if ( stop times . length == __num__ ) return collections . empty list ( ) ;,array has
move all the if node 's following <PLACE_HOLDER> .,move all following ( if node @$ if node . get parent ( ) @$ new dest block ) ; report change to enclosing scope ( if node ) ;,node following
problem when retrieving a simple role : either role not found or not readable @$ so raises a role not found <PLACE_HOLDER> .,if ( ! ( multi role flg ) ) { try { relation service . throw role problem exception ( pb type @$ role name ) ; return null ; } catch ( invalid role value exception exc ) { throw new runtime exception ( exc . get message ( ) ) ; } } else { result = new role unresolved ( role name @$ null @$ pb type ) ; },role found
the request should n't affect the key event <PLACE_HOLDER> .,new thread ( new runnable ( ) { public void run ( ) { try { thread . sleep ( __num__ ) ; } catch ( exception ex ) { } system . out . println ( __str__ + t2 ) ; t2 . request focus ( ) ; } } ) . start ( ) ; f . set visible ( true ) ; util . wait for idle ( robot ) ; test ( ) ; if ( passed ) system . out . println ( __str__ ) ;,request affect
when we get to here the new cache has a use <PLACE_HOLDER> of 1 and when setting a bunch of values on the same node sequence @$ such as when sorting @$ we will keep setting values in that same copy which has a use <PLACE_HOLDER> of 1 .,vec . set element at ( node @$ index ) ; m_last = vec . size ( ) ;,which has
actual data does n't matter . just send more than 4 k <PLACE_HOLDER>,byte b = __str__ ; for ( int i = __num__ ; i < ( __num__ + __num__ ) ; i ++ ) os . write ( b ) ; os . close ( ) ; t . close ( ) ;,data send
empty response from the resource manager . blacklist this <PLACE_HOLDER> for this request .,blacklist . add ( sub cluster id ) ;,response blacklist
one last sync whose transactions are not expected to be seen in the input streams because the journal nodes have not updated their <PLACE_HOLDER> of the committed transaction id yet,write txns ( stm @$ __num__ @$ __num__ ) ; future throws ( new io exception ( ) ) . when ( spies . get ( __num__ ) ) . get journaled edits ( __num__ @$ quorum journal manager . qjm_rpc_max_txns_default ) ; future throws ( new io exception ( ) ) . when ( spies . get ( __num__ ) ) . get journaled edits ( __num__ @$ quorum journal manager . qjm_rpc_max_txns_default ) ; list < edit log input stream > streams = new array list < > ( ) ; qjm . select input streams ( streams @$ __num__ @$ true @$ true ) ;,nodes updated
find which one supplies type <PLACE_HOLDER> and return it,type variable < ? > tv = base class . get type parameters ( ) [ __num__ ] ; while ( ! superclasses . is empty ( ) ) { current type = superclasses . pop ( ) ; if ( current type instanceof parameterized type ) { parameterized type pt = ( parameterized type ) current type ; class < ? > raw type = ( class < ? > ) pt . get raw type ( ) ; int arg index = arrays . as list ( raw type . get type parameters ( ) ) . index of ( tv ) ; if ( arg index > - __num__ ) { type type arg = pt . get actual type arguments ( ) [ arg,one supplies
important ? if source has native <PLACE_HOLDER> @$ need to store,if ( ! _has native type ids ) { _has native type ids = other . can write type id ( ) ; } if ( ! _has native object ids ) { _has native object ids = other . can write object id ( ) ; } _may have native ids = _has native type ids | _has native object ids ; json parser p = other . as parser ( ) ; while ( p . next token ( ) != null ) { copy current structure ( p ) ; } return this ;,source has
interceptor changes return <PLACE_HOLDER> .,interceptor . ret interceptor = new before remove interceptor ( new ignite bi tuple ( false @$ __num__ ) ) ;,changes return
end if : got four <PLACE_HOLDER>,if ( i == __num__ ) { return - __num__ ; } else { throw new java . io . io exception ( __str__ ) ; },end got
events can start coming in the moment the input initializer is created . the pruner must be setup and initialized here so that it sets up it 's structures to start accepting events . setting it up in initialize leads to a window where events may come in before the pruner is initialized @$ which may cause it to drop events . no dynamic,pruner = null ;,events start
verify that the comparison gives the correct <PLACE_HOLDER> for all values in both directions .,for ( int i = __num__ ; i < test_keys . length ; ++ i ) { for ( int j = __num__ ; j < test_keys . length ; ++ j ) { byte key left = test_keys [ i ] ; byte key right = test_keys [ j ] ; int cmp = left . compare to ( right ) ; if ( i < j && ! ( cmp < __num__ ) ) { fail ( string . format ( __str__ @$ left @$ right @$ cmp @$ i @$ j ) ) ; } else if ( i == j && ! ( cmp == __num__ ) ) { fail ( string . format ( __str__ @$ left @$ right @$ cmp @$ i,comparison gives
registers a callback to be invoked whenever a user changes a <PLACE_HOLDER> .,get preference screen ( ) . get shared preferences ( ) . register on shared preference change listener ( this ) ;,user changes
when the target process is reset @$ the breakpoint deleted message will never arrive . it is therefore necessary to delete the deleting breakpoints manually . see <PLACE_HOLDER> 2109 for an example of what can happen .,if ( manager . get breakpoint status ( address @$ breakpoint type . regular ) == breakpoint status . breakpoint_deleting ) { addresses to remove . add ( address ) ; } else if ( manager . get breakpoint status ( address @$ breakpoint type . regular ) != breakpoint status . breakpoint_disabled ) { addresses to disable . add ( address ) ; },message see
mark set possible <PLACE_HOLDER> to true for all events in this bucket before it becomes primary on the node,while ( itr . has next ( ) ) { object key = itr . next ( ) ; object sender event = get nolru ( key @$ true @$ false @$ false ) ; if ( sender event != null ) { ( ( gateway sender event impl ) sender event ) . set possible duplicate ( true ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ sender event ) ; } } i ++ ; },mark set
validate the xml file and return the <PLACE_HOLDER>,boolean status = validator . validate ( ) ; if ( ! status ) { log . log error ( validator . get error message ( ) ) ; result . set result ( false ) ; result . set nr errors ( validator . get nr errors ( ) ) ; result . set log text ( validator . get error message ( ) ) ; },xml file
cancellation exception <PLACE_HOLDER> missing when root <PLACE_HOLDER> not found <PLACE_HOLDER> missing when <PLACE_HOLDER> not found,assert that ( exception . to string ( ) @$ any of ( contains string ( __str__ ) @$ contains string ( __str__ ) @$ contains string ( __str__ ) ) ) ;,task found
do n't add the original policy if it was an expansion flag @$ which have no <PLACE_HOLDER> @$ but do add it if there was either no expansion or if it was a <PLACE_HOLDER>d flag with implicit requirements .,if ( ! is expansion ) { expanded policies . add ( original policy ) ; } return expanded policies ;,which have
fourth document has some nested <PLACE_HOLDER> next to more complex datatypes,assert equals ( __num__ @$ json utils . query ( __str__ ) . get ( __str__ ) . get ( __num__ ) . get ( __str__ ) . get ( __num__ ) . apply ( scroll . get hits ( ) . get ( __num__ ) [ __num__ ] ) ) ; assert equals ( iso date time format . date parser ( ) . parse date time ( __str__ ) . to date ( ) @$ json utils . query ( __str__ ) . get ( __str__ ) . get ( __num__ ) . get ( __str__ ) . apply ( scroll . get hits ( ) . get ( __num__ ) [ __num__ ] ) ) ; assert equals ( __num__ @$ json utils . query,document has
get twitter created a flow <PLACE_HOLDER> @$ then it 's sent via s 2 s,tc . add lineage ( create lineage ( prs @$ __num__ @$ __num__ @$ __num__ ) ) ; test ( tc ) ; wait notifications get delivered ( ) ; final lineage lineage = get lineage ( ) ; final node flow = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patha = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathb = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathc = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patht = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathi = lineage . find node (,twitter created
assuming the default configuration has the correct <PLACE_HOLDER> set . users can specify a particular factory by providing a configuration .,if ( conf == null ) { conf = default conf ; },configuration has
aet should override any <PLACE_HOLDER> set unless it is zero,message . set time to live ( __num__ ) ; message . set text ( __str__ ) ; sender . send ( message ) ; sender . close ( ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ; thread . sleep ( __num__ ) ;,aet override
now we want to save the normalizer to a binary file . for doing this @$ one can use the normalizer <PLACE_HOLDER> .,normalizer serializer serializer = normalizer serializer . get default ( ) ;,one use
turn off persistent ipc @$ so that the dfs client can survive nn <PLACE_HOLDER>,conf . set int ( common configuration keys public . ipc_client_connection_maxidletime_key @$ __num__ ) ; minidfs cluster cluster = null ; fs data output stream stream ; try { cluster = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) . build ( ) ; file system fs = cluster . get file system ( ) ; dfs util client . getnn address ( conf ) . get port ( ) ; stream = fs . create ( file_path @$ true @$ block_size @$ ( short ) __num__ @$ block_size ) ; stream . write ( data_before_restart ) ; stream . write ( ( byte ) __num__ ) ; stream . hflush ( ) ; cluster . restart name node ( ),client survive
let the hash builder operators reduce their accounted <PLACE_HOLDER>,partitions no longer needed . set ( null ) ; lock . write lock ( ) . lock ( ) ; try { arrays . fill ( partitions @$ null ) ; lookup source supplier = null ; close cached lookup sources ( ) ; } finally { lock . write lock ( ) . unlock ( ) ; },operators reduce
delete items that do have a keychain item <PLACE_HOLDER> .,for ( enumeration e = deleted entries . keys ( ) ; e . has more elements ( ) ; ) { string alias = ( string ) e . next element ( ) ; object entry = deleted entries . get ( alias ) ; if ( entry instanceof trusted cert entry ) { if ( ( ( trusted cert entry ) entry ) . cert ref != __num__ ) { _remove item from keychain ( ( ( trusted cert entry ) entry ) . cert ref ) ; _release keychain item ref ( ( ( trusted cert entry ) entry ) . cert ref ) ; } } else { certificate cert elem ; key entry key entry = ( key entry ) entry ; if,items have
if a build file has been added or removed @$ reconstruct the build file <PLACE_HOLDER> .,build file trees . invalidate ( cell ) ;,the build
if a topic has k partitions @$ and in the previous run @$ each partition recorded its avg <PLACE_HOLDER> to pull a record @$ then use the geometric mean of these k numbers as the estimated avg <PLACE_HOLDER> to pull a record in this run .,double est avg millis for topic = geometric mean ( prev avg millis for partitions ) ; this . est avg millis . put ( topic @$ est avg millis for topic ) ; log . info ( string . format ( __str__ @$ topic @$ est avg millis for topic ) ) ; all est avg millis . add ( est avg millis for topic ) ;,partition recorded
just pressing the button does not change the <PLACE_HOLDER> ...,set button pressed ( true ) ; assert equals ( empty border button . get border ( ) @$ empty border button . no_button_border ) ;,button change
every iteration will signal the boundary <PLACE_HOLDER> of the previous iteration !,for ( int i = __num__ ; i < __num__ ; i ++ ) { runtime service . start process instance by message and tenant id ( __str__ @$ tenant_id ) ; },iteration signal
does the cell contain special <PLACE_HOLDER> which indicates that the replicated cell visiblilty <PLACE_HOLDER>s have been modified,if ( is system or super user ( ) ) { tag modified tag = null ; iterator < tag > tags iterator = private cell util . tags iterator ( cell ) ; while ( tags iterator . has next ( ) ) { tag tag = tags iterator . next ( ) ; if ( tag . get type ( ) == tag type . string_vis_tag_type ) { modified tag = tag ; break ; } } pair . set first ( true ) ; pair . set second ( modified tag ) ; return pair ; },cell contain
since the system will disconnect and attempt to reconnect a new system the old reference to dtc.system can cause <PLACE_HOLDER> @$ so we first null it out .,serializable callable fd = new serializable callable ( __str__ ) { @ override public object call ( ) throws exception { null system ( ) ; final locator old locator = locator . get locator ( ) ; final distributed system msys = cache . get distributed system ( ) ; membership manager helper . crash distributed system ( msys ) ; if ( old locator != null ) { wait criterion wc = new wait criterion ( ) { @ override public boolean done ( ) { return msys . is reconnecting ( ) || msys . get reconnected system ( ) != null ; } @ override public string description ( ) { return __str__ + old locator ; } } ; geode awaitility . await,reference cause
to do . this can happen when we are restoring the entire pager from its saved state @$ where the fragment manager has already taken <PLACE_HOLDER> of restoring the fragments we previously had instantiated .,if ( m fragments . size ( ) > position ) { fragment f = m fragments . get ( position ) ; if ( f != null ) { return f ; } } if ( m cur transaction == null ) { m cur transaction = m fragment manager . begin transaction ( ) ; } fragment fragment = get item ( position ) ; if ( debug ) log . v ( tag @$ __str__ + position + __str__ + fragment ) ; if ( m saved state . size ( ) > position ) { fragment . saved state fss = m saved state . get ( position ) ; if ( fss != null ) { fragment . set initial saved state (,manager taken
now flush messages and message pull <PLACE_HOLDER> .,for ( command msg : message cache . values ( ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + ( msg . is message ( ) ? ( ( message ) msg ) . get message id ( ) : msg ) ) ; } transport . oneway ( msg ) ; },messages pull
although we are in doze and would normally allow the device to suspend @$ the doze service has explicitly requested the <PLACE_HOLDER> to remain in the on state which means we should hold the <PLACE_HOLDER> suspend blocker .,if ( m display power request . policy == display power request . policy_doze && m display power request . doze screen state == display . state_on ) { return true ; },service requested
framework classes should be loaded from smalivm 's generated framework jar . this is because the object instantiator will expect an empty default <PLACE_HOLDER> and one is added whenever classes are built .,if ( class manager . get framework class names ( ) . contains ( internal name ) ) { class < ? > klazz = cached classes . get ( name ) ; if ( klazz != null ) { return klazz ; } klazz = find class ( name ) ; cached classes . put ( name @$ klazz ) ; return klazz ; } return super . load class ( name @$ resolve ) ;,instantiator expect
interceptor disables remove and changes return <PLACE_HOLDER> .,interceptor . ret interceptor = new before remove interceptor ( new ignite bi tuple ( true @$ __num__ ) ) ;,disables remove
make sure decompiler receives <PLACE_HOLDER>,native out . flush ( ) ;,decompiler receives
open ssl only supports pkcs 5 <PLACE_HOLDER> .,native crypto . evp_cipher_ctx_set_padding ( cipher ctx . get context ( ) @$ padding == padding . pkcs5padding ) ; mode block size = native crypto . evp_cipher_ctx_block_size ( cipher ctx . get context ( ) ) ; called update = false ;,ssl supports
force line <PLACE_HOLDER> force line <PLACE_HOLDER>,scratch . file ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,break force
the message indicated some error trying to start : do call handle stop <PLACE_HOLDER> .,handle stop keepalive ( nai @$ slot @$ reason ) ;,call handle
we take the max of the default and whatever the user put in here . each node 's <PLACE_HOLDER> can be the sum of several operations @$ so the simplest thing to do is get the max . the situation we want to avoid is that the user sets low <PLACE_HOLDER> on one node @$ and when that node is combined with a bunch,if ( on heap == null ) { on heap = on heap default ; } else { on heap = math . max ( on heap . double value ( ) @$ on heap default . double value ( ) ) ; } if ( off heap == null ) { off heap = off heap default ; } else { off heap = math . max ( off heap . double value ( ) @$ off heap default . double value ( ) ) ; } if ( cpu load == null ) { cpu load = cpu load default ; } else { cpu load = math . max ( cpu load . double value ( ) @$ cpu load default . double value (,user sets
this security realm does n't support <PLACE_HOLDER> registration . just report an error,req . get view ( this @$ __str__ ) . forward ( req @$ rsp ) ;,realm support
calls coalesce validator <PLACE_HOLDER> .,sink . process ( ) ;,calls coalesce
the decision table does not define type <PLACE_HOLDER> for the single output clause @$ but neither the enclosing decision define type <PLACE_HOLDER> for its variable,assert that ( messages . get ( __num__ ) . get source id ( ) @$ is ( __str__ ) ) ; assert that ( messages . get ( __num__ ) . get message type ( ) @$ is ( dmn message type . missing_type_ref ) ) ;,decision define
note to translators : this message is reported if the stylesheet that is being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,return new object [ ] [ ] { { __str__ @$ __str__ } @$ { er_no_curlybrace @$ __str__ } @$ { er_illegal_attribute @$ __str__ } @$ { er_null_sourcenode_applyimports @$ __str__ } @$ { er_cannot_add @$ __str__ } @$ { er_null_sourcenode_handleapplytemplates @$ __str__ } @$ { er_no_name_attrib @$ __str__ } @$ { er_template_not_found @$ __str__ } @$ { er_cant_resolve_name_avt @$ __str__ } @$ { er_requires_attrib @$ __str__ } @$ { er_must_have_test_attrib @$ __str__ } @$ { er_bad_val_on_level_attrib @$ __str__ } @$ { er_processinginstruction_name_cant_be_xml @$ __str__ } @$ { er_processinginstruction_notvalid_ncname @$ __str__ } @$ { er_need_match_attrib @$ __str__ } @$ { er_need_name_or_match_attrib @$ __str__ } @$ { er_cant_resolve_nsprefix @$ __str__ } @$ { er_illegal_value @$ __str__ } @$ { er_no_ownerdoc @$ __str__ } @$ { er_elemtemplateelem_err @$ __str__ } @$,text specifies
parquet has <PLACE_HOLDER> logging at info level,parquet logger = logger . get logger ( __str__ ) ; parquet logger . set level ( level . warning ) ;,parquet has
compilation mutates the <PLACE_HOLDER> . this is unrelated to testing recoverable js ast .,return new recoverable js ast ( ast @$ true ) ;,compilation mutates
we need to make sure the section goes all the <PLACE_HOLDER> to the shelf,if ( section == last section ) { min bottom position = ( int ) ( view state . get final translationy ( m shelf ) + m shelf . get intrinsic height ( ) ) ; },section goes
if this method did not throw remote <PLACE_HOLDER> as required @$ generate the error but continue @$ so that multiple such errors can be reported .,if ( ! has remote exception ) { env . error ( __str__ @$ intf . qualified name ( ) @$ method . name ( ) + method . signature ( ) ) ; errors = true ; continue next method ; },method throw
0 x 10018 a 6 : op 0 has reg <PLACE_HOLDER> to esi . 0 x 100295 a : op 1 has reg <PLACE_HOLDER> to cx . 0 x 1002 d 0 b : op 0 has reg <PLACE_HOLDER> to edi ; op 1 has reg <PLACE_HOLDER> to eax . 0 x 10033 fe : op 0 both have reg <PLACE_HOLDER> to edi .,mtf . initialize ( __str__ @$ new program modifier listener ( ) { @ override public void modify latest ( programdb program ) { int tx id = program . start transaction ( __str__ ) ; boolean commit = false ; try { program context context = program . get program context ( ) ; register esi reg = context . get register ( __str__ ) ; register cx reg = context . get register ( __str__ ) ; register eax reg = context . get register ( __str__ ) ; register edi reg = context . get register ( __str__ ) ; reference manager ref mgr = program . get reference manager ( ) ; reference [ ] refs ; refs = ref mgr . get references,op has
because entries collection is empty @$ remove and decrement <PLACE_HOLDER>,if ( entries . is empty ( ) ) { synchronized ( entries ) { if ( entries . is empty ( ) ) { if ( value to entries map . remove ( new key @$ entries ) ) { num index keys . decrement and get ( ) ; internal index stats . inc num keys ( - __num__ ) ; } } } },collection empty
if the left index has not reached the right side of array must now sort the right <PLACE_HOLDER>,if ( hi + __num__ < hi0 ) { qsort ( a @$ hi + __num__ @$ hi0 @$ comp ) ; },side sort
this should never happen but the api has marked <PLACE_HOLDER> as nullable,iterable < field > fields new = iterables . transform ( fields @$ new function < field @$ field > ( ) { @ override public schema . field apply ( field input ) { if ( null == input ) { return null ; } field field = new field ( input . name ( ) @$ input . schema ( ) @$ input . doc ( ) @$ input . default value ( ) @$ input . order ( ) ) ; return field ; } } ) ;,api marked
set the mode to streaming first so relay will inspect the <PLACE_HOLDER>,cp . set consumption mode ( dbus client mode . online_consumption ) ;,relay inspect
a volt db extension to customize the sql function set <PLACE_HOLDER>,volt disabled = disabled_in_functioncustom_constructor ;,extension customize
if slider has number <PLACE_HOLDER>,if ( number indicator != null ) { number indicator . indicator . x = x ; number indicator . indicator . finaly = utils . get relative top ( this ) - get height ( ) / __num__ ; number indicator . indicator . final size = get height ( ) / __num__ ; number indicator . number indicator . set text ( __str__ ) ; },slider has
return null since neither dm nor dls are shutting down can not call <PLACE_HOLDER> in progress because it 's abstract,return null ;,null call
vp 9 introduced profiles around 2016 @$ so some vp 9 codecs may not advertise <PLACE_HOLDER> supported profiles . determine the level for them using the info they provide .,if ( prof levs . length == __num__ && m mime . equals ignore case ( media format . mimetype_video_vp9 ) ) { codec profile level prof lev = new codec profile level ( ) ; prof lev . profile = codec profile level . vp9 profile0 ; prof lev . level = video capabilities . equivalentvp9 level ( info ) ; prof levs = new codec profile level [ ] { prof lev } ; } profile levels = prof levs ; if ( m mime . to lower case ( ) . starts with ( __str__ ) ) { m audio caps = audio capabilities . create ( info @$ this ) ; m audio caps . get default format ( m default format ) ;,codecs advertise
cdh uses different <PLACE_HOLDER> for parquet,if ( __str__ . equals ( input format name ) || __str__ . equals ( input format name ) ) { return mapred parquet input format . class ; } class < ? > clazz = conf . get class by name ( input format name ) ; return ( class < ? extends input format < ? @$ ? > > ) clazz . as subclass ( input format . class ) ;,cdh uses
do a get file <PLACE_HOLDER> with empty dir flag,s3a file status status = get status with empty dir flag ( fs @$ base ) ; assert non empty dir ( status ) ; if ( isddb ) { list metric . assert diff equals ( __str__ @$ __num__ ) ; get metric . assert diff equals ( __str__ @$ __num__ ) ; log . info ( __str__ ) ; },a get
audio track.release can take some <PLACE_HOLDER> @$ so we call it on a background thread .,final audio track to release = audio track ; audio track = null ; if ( pending configuration != null ) { configuration = pending configuration ; pending configuration = null ; } audio track position tracker . reset ( ) ; releasing condition variable . close ( ) ; new thread ( ) { @ override public void run ( ) { try { to release . flush ( ) ; to release . release ( ) ; } finally { releasing condition variable . open ( ) ; } } } . start ( ) ;,track.release take
complete task c @$ which will start task <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name and state ( plan item instances @$ __str__ @$ active ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert no plan item instance ( plan item instances @$ __str__ ) ;,which start
now new job requests should succeed as status operation has no cancel <PLACE_HOLDER> .,job runnable = concurrent jobs status ( __num__ @$ config @$ false @$ false @$ status job helper . get delayed resonse answer ( __num__ @$ status bean ) ) ; assert true ( job runnable . exception == null ) ;,operation has
sets max <PLACE_HOLDER> to 1 so cache metrics have correct <PLACE_HOLDER> .,try ( ignite data streamer < integer @$ long > ldr = g . data streamer ( default_cache_name ) ) { ldr . per node parallel operations ( __num__ ) ; ldr . receiver ( new incrementing updater ( ) ) ; for ( int i = __num__ ; i < cnt ; i ++ ) ldr . add data ( i % ( cnt / __num__ ) @$ __num__ ) ; },sets max
initializes the amazon pinpoint <PLACE_HOLDER> .,amazon pinpoint pinpoint client = amazon pinpoint client builder . standard ( ) . with region ( regions . us_east_1 ) . build ( ) ; system . out . format ( __str__ @$ endpoints file name @$ application id ) ; try { create import job result import result = pinpoint client . create import job ( create import job request ) ; string job id = import result . get import job response ( ) . get id ( ) ; get import job result get import job result = null ; string job status = null ; do { get import job result = pinpoint client . get import job ( new get import job request ( ) . with job id ( job id,amazon pinpoint
junit do n't provide any <PLACE_HOLDER> to order tests,pre auth ( ) ; missing auth ( ) ; valid auth ( ) ; valid auth2 ( ) ;,junit provide
a little bit of paranoid checking here ? most drivers will throw batch update <PLACE_HOLDER> perhaps ?,if ( batch count != last result . length ) { log . warning ( __str__ + batch count + __str__ + last result . length ) ; } else if ( found error ) { log . warning ( __str__ + default groovy methods . to list ( last result ) ) ; } else { log . fine ( __str__ + last result . length + __str__ ) ; },drivers throw
fetch the certificates via ldap . ldap cert store has its own caching <PLACE_HOLDER> @$ see the class description for more info . safe cast since xsel is an x 509 certificate selector .,return ( collection < x509 certificate > ) ldap cert store . get certificates ( xsel ) ;,store has
if new available range contains the <PLACE_HOLDER> @$ try again,if ( range . with length ( first row index @$ number of rows ) . contains ( row index ) ) { registration . get ( ) . remove ( ) ; wait until visible ( row index @$ destination @$ when visible ) ; },range contains
figure tabs use 8 <PLACE_HOLDER>,if ( end char == __str__ ) { line length += __num__ ; },tabs use
there is possibility that we 'll replay txns for a node which was created and then deleted in the fuzzy range @$ and it 's not exist in the snapshot @$ so replay the creation might revert the <PLACE_HOLDER> and pzxid @$ need to check and only update when it 's larger .,if ( parentc version > parent . stat . get cversion ( ) ) { parent . stat . set cversion ( parentc version ) ; parent . stat . set pzxid ( zxid ) ; } data node child = new data node ( data @$ longval @$ stat ) ; parent . add child ( child name ) ; nodes . post change ( parent name @$ parent ) ; node data size . add and get ( get node size ( path @$ child . data ) ) ; nodes . put ( path @$ child ) ; ephemeral type ephemeral type = ephemeral type . get ( ephemeral owner ) ; if ( ephemeral type == ephemeral type . container ) { containers .,replay revert
make an empty path match <PLACE_HOLDER> .,if ( path == null ) { path = __str__ ; },path match
test new configuration should clear other <PLACE_HOLDER>,conf = new configuration ( ) ; conf . set ( default_key_acl_prefix + __str__ @$ __str__ ) ; acls . set keyac ls ( conf ) ; assert default key acl ( acls @$ key op type . decrypt_eek @$ __str__ ) ; assert . assert true ( acls . key acls . is empty ( ) ) ; assert . assert true ( acls . whitelist key acls . is empty ( ) ) ; assert . assert equals ( __str__ + acls . default key acls @$ __num__ @$ acls . default key acls . size ( ) ) ;,configuration clear
can specify orientation if app does n't fill <PLACE_HOLDER> .,assert equals ( screen_orientation_landscape @$ m token . get orientation ( ) ) ; m token . set fills parent ( true ) ; m token . set hidden ( true ) ; m token . sending to bottom = true ;,app fill
list contains an unmanaged <PLACE_HOLDER>,list . add ( unmanaged ) ;,list contains
as we do n't have a bind we need a unique name . let 's use the <PLACE_HOLDER> as the generated invoker,identifier = generated class name ; context . add declaration ( generated class name @$ object . class ) ;,name use
first see if the file matches the regular <PLACE_HOLDER> !,if ( pattern != null ) { matcher matcher = pattern . matcher ( selectedfile ) ; get it = matcher . matches ( ) ; },file matches
we decr index jj by 8 as we go along to not recompute indices using multiplication every <PLACE_HOLDER> inside the loop .,for ( int ii = __num__ @$ jj = ascii . length - __num__ ; ii < l_raw . length ; ii ++ @$ jj -= __num__ ) { for ( int bits = __num__ ; bits < bits . length ; ++ bits ) { if ( ascii [ jj - bits ] == __str__ ) { l_raw [ ii ] |= bits [ bits ] ; } } } return l_raw ;,indices using
nn should not bind the wildcard <PLACE_HOLDER> by default .,j cluster = new mini journal cluster . builder ( conf ) . format ( true ) . num journal nodes ( num_jn ) . build ( ) ; jn = j cluster . get journal node ( __num__ ) ; string address = get rpc server address ( jn ) ; assert that ( __str__ @$ address @$ not ( __str__ + wildcard_address ) ) ; log . info ( __str__ + dfs_journalnode_rpc_bind_host_key ) ;,nn bind
note to translators : this message is reported if the stylesheet being processed attempted to construct an xml document with an attribute in a place other than on an element . the substitution text specifies the <PLACE_HOLDER> of the attribute .,object [ ] [ ] contents = new object [ ] [ ] { { msg key . bad_msgkey @$ __str__ } @$ { msg key . bad_msgformat @$ __str__ } @$ { msg key . er_serializer_not_contenthandler @$ __str__ } @$ { msg key . er_resource_could_not_find @$ __str__ } @$ { msg key . er_resource_could_not_load @$ __str__ } @$ { msg key . er_buffer_size_lessthan_zero @$ __str__ } @$ { msg key . er_invalid_utf16_surrogate @$ __str__ } @$ { msg key . er_oierror @$ __str__ } @$ { msg key . er_illegal_attribute_position @$ __str__ } @$ { msg key . er_namespace_prefix @$ __str__ } @$ { msg key . er_stray_attribute @$ __str__ } @$ { msg key . er_stray_namespace @$ __str__ } @$ { msg key . er_could_not_load_resource @$,text specifies
set last saved <PLACE_HOLDER> or string by default :,if ( saved type != - __num__ && saved type < type combo box . get item count ( ) ) { type combo box . set selected index ( saved type ) ; } else { type combo box . set selected item ( new supported column type wrapper ( string . class ) ) ; },set saved
cipher uses <PLACE_HOLDER>,if ( password != null ) { passwd = string . copy value of ( password ) ; java . util . arrays . fill ( password @$ __str__ ) ; },cipher uses
activity scopes often include transient <PLACE_HOLDER> like task id . make sure bundler service runner is n't stymied by that .,new process ( __str__ ) ; activity = new faux activity ( ) ; activity . create ( bundle ) ; assert that ( activity . root bundler . last loaded ) . is not null ( ) ; assert that ( activity . child bundler . last loaded ) . is not null ( ) ;,scopes include
remain only first item as exo player does n't support adaptive <PLACE_HOLDER> for url list,return list . sub list ( __num__ @$ __num__ ) ;,player support
add all nodes from the follow pos of the start node to the follow pos set of the end node @$ which will have the <PLACE_HOLDER> of letting matches transition from a match state at end node to the second char of a match starting with start node .,end node . f follow pos . add all ( start node . f follow pos ) ;,which have
special case : java.lang.annotation.target must not have repeated <PLACE_HOLDER> in its value member,if ( a . annotation type . type . tsym != syms . annotation target type . tsym || a . args . tail == null ) return is valid ; if ( ! a . args . head . has tag ( assign ) ) return false ;,java.lang.annotation.target have
control case : admin can use default <PLACE_HOLDER> .,j . submit ( wc . with basic api token ( admin ) . get page ( p @$ __str__ ) . get form by name ( __str__ ) ) ; j . wait until no activity ( ) ; free style build b1 = p . get last build ( ) ; assert equals ( __num__ @$ b1 . get number ( ) ) ; j . assert log contains ( __str__ @$ j . assert build status success ( b1 ) ) ;,admin use
foo loads bar own secondary <PLACE_HOLDER> .,list < string > bar secondaries = m bar user0 . get secondary dex paths ( ) ; notify dex load ( m foo user0 @$ bar secondaries @$ m user0 ) ;,loads bar
lower android versions have a reference <PLACE_HOLDER> with 1024 entries only,for ( int i = __num__ ; i < __num__ ; i ++ ) { assert true ( jni test . create and delete int array ( ) ) ; system . out . print ( i ) ; },versions have
do this so that the editor is referencing the current renderer from the tree . the renderer can potentially change each time laf <PLACE_HOLDER> .,set default editor ( tree table model . class @$ new tree table cell editor ( ) ) ;,renderer change
if the appliation did n't set the status code @$ set the default <PLACE_HOLDER> .,if ( con . get status ( ) == __num__ ) { con . set status ( response message . is fault ( ) ? httpurl connection . http_internal_error : httpurl connection . http_ok ) ; },appliation set
the currently inactive histogram is reset each <PLACE_HOLDER> we flip . so flipping twice resets both :,perform interval sample ( ) ; perform interval sample ( ) ;,histogram reset
ghost components will have a null <PLACE_HOLDER>,if ( existing node . get logger ( ) != null ) { existing node . get logger ( ) . debug ( __str__ @$ new object [ ] { id @$ new type @$ bundle coordinate } ) ; } final extension manager extension manager = flow controller . get extension manager ( ) ;,components have
if the short std matches the long <PLACE_HOLDER> @$ or the long std matches the short <PLACE_HOLDER> @$ it probably means we have a time zone that icu 4 c does n't believe has ever observed <PLACE_HOLDER> .,if ( short std . equals ( long dst ) ) { failures . append ( string . format ( __str__ @$ id @$ short std @$ long dst ) ) ; } if ( long std . equals ( short dst ) ) { failures . append ( string . format ( __str__ @$ id @$ long std @$ short dst ) ) ; },std matches
this function calls <PLACE_HOLDER> recursively,iso run p last iso run = bd . iso runs [ bd . iso run last ] ; opening q opening ; int k @$ opening position @$ closing position ; for ( k = opening index + __num__ ; k < p last iso run . limit ; k ++ ) { q opening = bd . openings [ k ] ; if ( q opening . match >= __num__ ) continue ; if ( new prop position < q opening . context pos ) break ; if ( new prop position >= q opening . position ) continue ; if ( new prop == q opening . context dir ) break ; opening position = q opening . position ; dir props [ opening position,function calls
there are 12 issues in total @$ with 10 issues per page @$ the page 2 should only contain 2 <PLACE_HOLDER>,search response result = under test . search ( query . build ( ) @$ new search options ( ) . set page ( __num__ @$ __num__ ) ) ; assert that ( result . get hits ( ) . get hits ( ) ) . has size ( __num__ ) ; assert that ( result . get hits ( ) . get total hits ( ) ) . is equal to ( __num__ ) ; result = under test . search ( issue query . builder ( ) . build ( ) @$ new search options ( ) . set offset ( __num__ ) . set limit ( __num__ ) ) ; assert that ( result . get hits ( ) . get hits ( ) ),issues contain
the application will call <PLACE_HOLDER> when waiting for a message @$ which will in turn call this on the transport thread .,stream . transport state ( ) . request messages from deframer ( __num__ ) ;,application call
the <PLACE_HOLDER> session was aborted @$ clean up the pending <PLACE_HOLDER> .,if ( ! success ) { delete rollback ( rollback ) ; return null ; },session clean
check if the given spark <PLACE_HOLDER> has a child spark <PLACE_HOLDER> that contains the target map work if it does not @$ then remove the target from dpp op,if ( ! task contains dependent map work ( task @$ target map work ) ) { to remove . add ( target info ) ; pruning sink op . remove from source event ( target map work @$ target info . part key @$ target info . column name @$ target info . column type ) ; log . info ( __str__ + target map work . get name ( ) + __str__ + base work . get name ( ) + __str__ ) ; },child spark
getting action for the null <PLACE_HOLDER> @$ which in this case means the body <PLACE_HOLDER>,for ( handler ah : action handlers ) { final action [ ] aa = ah . get actions ( null @$ this ) ; if ( aa != null ) { for ( int ai = __num__ ; ai < aa . length ; ai ++ ) { final string akey = action mapper . key ( aa [ ai ] ) ; action set . add ( aa [ ai ] ) ; keys . add ( akey ) ; } } },which means
fs dir <PLACE_HOLDER> because fs stats uses fs dir <PLACE_HOLDER>,final service metric event . builder builder = builder ( ) . set dimension ( __str__ @$ dir ) ;,stats uses
empty clozes use first <PLACE_HOLDER>,if ( ords . is empty ( ) && allow empty ) { return new array list < > ( arrays . as list ( new integer [ ] { __num__ } ) ) ; },clozes use
emit max uncommitted offsets messages @$ and fail all of <PLACE_HOLDER> . then ensure that the spout will retry <PLACE_HOLDER> when the retry backoff has passed,try ( simulated time simulated time = new simulated time ( ) ) { kafka spout < string @$ string > spout = spout with mocked consumer setup helper . setup spout ( spout config @$ conf @$ context mock @$ collector mock @$ consumer mock @$ partition ) ; map < topic partition @$ list < consumer record < string @$ string > > > records = new hash map < > ( ) ; int num records = spout config . get max uncommitted offsets ( ) ; records . put ( partition @$ spout with mocked consumer setup helper . create records ( partition @$ __num__ @$ num records ) ) ; when ( consumer mock . poll ( any long ( ) ) ),spout retry
any change to an input file may affect program <PLACE_HOLDER> @$ even if only by changing line numbers in error messages .,path fragment extension file = extension label . to path fragment ( ) ; try ( mutability mutability = mutability . create ( __str__ @$ extension file ) ) { starlark thread thread = rule class provider . create rule class starlark thread ( extension label @$ mutability @$ starlark semantics @$ event handler @$ file . get content hash code ( ) @$ import map @$ package factory . get native module ( in workspace ) @$ repository mapping ) ; exec and export ( file @$ extension label @$ event handler @$ thread ) ; event . replay events on ( env . get listener ( ) @$ event handler . get events ( ) ) ; for ( postable post : event handler . get,change affect
this version gives a <PLACE_HOLDER> that rest li runs,request < void > req = builders . < void > action ( __str__ ) . build ( ) ; get client ( ) . send request ( req ) . get response ( ) ;,version gives
utf requires java string <PLACE_HOLDER>,buffered string tmp str = new buffered string ( ) ; for ( int i = __num__ ; i < chk . _len ; i ++ ) { if ( chk . isna ( i ) ) new chk . addna ( ) ; else { string str = chk . at str ( tmp str @$ i ) . to string ( ) ; new chk . add num ( calc entropy ( str ) ) ; } },utf requires
checks the status of the rpc call @$ throws an <PLACE_HOLDER> in case of error,check status ( resp . get status ( ) ) ; operation state op state = operation state . get operation state ( resp . get operation state ( ) ) ; hivesql exception op exception = null ; if ( op state == operation state . error ) { op exception = new hivesql exception ( resp . get error message ( ) @$ resp . get sql state ( ) @$ resp . get error code ( ) ) ; } return new operation status ( op state @$ resp . get task status ( ) @$ resp . get operation started ( ) @$ resp . get operation completed ( ) @$ resp . is set has result set ( ) ? resp . is,checks throws
those following presences would have leak <PLACE_HOLDER> that there is some file satisfying that pattern inside,assert that ( workspace content @$ all of ( not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) ) ) ;,presences have
capacity . if the eviction fails due to a concurrent removal of the victim @$ that removal may cancel out the <PLACE_HOLDER> that triggered this eviction . the victim is eagerly unlinked before the removal task so that if an eviction is still required then a new victim will be chosen for removal .,while ( has overflowed ( ) ) { final node < k @$ v > node = eviction deque . poll ( ) ; if ( node == null ) { return ; } if ( data . remove ( node . key @$ node ) ) { pending notifications . add ( node ) ; } make dead ( node ) ; },removal cancel
noinspection object <PLACE_HOLDER> in loop,data cache . add ( new o raw pair < > ( entry . key @$ entry . value ) ) ;,noinspection object
read until eof is meet or read buf <PLACE_HOLDER> is full .,byte buffer [ ] read buf array = { byte buffer . allocate ( buffer_size ) @$ byte buffer . allocate ( buffer_size ) } ; long total count = __num__ ; do { long count = source . read ( read buf array @$ __num__ @$ __num__ ) ; if ( count < __num__ ) { break ; } if ( __num__ == count && buffer_size == read buf array [ __num__ ] . position ( ) ) { break ; } total count += count ; } while ( total count != __num__ ) ;,eof meet
and if still not found @$ let 's choose <PLACE_HOLDER> ?,if ( name == null ) { name = _default type id ( cls ) ; },"s" choose
this check is required because <PLACE_HOLDER> impl currently implements soap body <PLACE_HOLDER>,if ( ( element instanceof soap body element ) && ! ( element . get class ( ) . equals ( element impl . class ) ) ) { return ( soap element ) element ; } else { return replace element withsoap element ( element @$ ( element impl ) create body element ( name impl . copy element name ( element ) ) ) ; },impl implements
now write the attributes representing the configuration <PLACE_HOLDER> .,configuration . write xml attrs ( xml @$ config stats . m configuration ) ; xml . end tag ( null @$ config_tag ) ;,attributes representing
unusual code : we @$ as a plugin @$ do n't have any <PLACE_HOLDER> . our transient tables do have <PLACE_HOLDER> . we need a way to have keybindings shared for all the different <PLACE_HOLDER> . further @$ we need to register them now @$ not when the transient providers are created @$ as they would only appear in the options at that point,delete table row action . register dummy ( tool ) ;,tables have
expected exception because the boundary timer event created a timer <PLACE_HOLDER> to be executed after 10 minutes,wait for job executor to process all jobs ( __num__ @$ __num__ ) ;,event created
wait for all non cancelled <PLACE_HOLDER> to be completed,try { boolean acquired = sem . try acquire ( max_waiting_time @$ time unit . milliseconds ) ; assert . assert true ( __str__ @$ acquired ) ; assert . assert equals ( __str__ @$ list . size ( ) @$ nb_add ) ; for ( int i = __num__ ; i < nb_add ; i ++ ) { assert . assert true ( __str__ @$ i < nb_add ) ; } } catch ( interrupted exception e ) { assert . assert false ( __str__ @$ true ) ; },non cancelled
if the id is a generated unique id then this could affect .q file golden <PLACE_HOLDER> for tests that run explain queries .,return __str__ + id + __str__ ;,.q file
check if client reconnected . if so @$ notify apps . ralam <PLACE_HOLDER> 35 @$ 2018,log . debug ( __str__ @$ username + __str__ + user id + __str__ @$ client id ) ; sip peer manager . hangup ( peer id @$ client id @$ notify apps ) ;,client ralam
if the number of posts on this blog that use this <PLACE_HOLDER> is higher than previous @$ set this as the most popular <PLACE_HOLDER> @$ and set the second most popular <PLACE_HOLDER> to the current most popular <PLACE_HOLDER>,int post count = json this tag . opt int ( __str__ ) ; if ( post count > popular count ) { next most popular tag = most popular tag ; most popular tag = this tag name ; popular count = post count ; } else if ( next most popular tag == null ) { next most popular tag = this tag name ; },number set
now round robin assignment with the modified servers list should return the <PLACE_HOLDER> as the regionserver assignee,assignment map = balancer . round robin assignment ( regions @$ servers ) ; set < server name > server with primary = assignment map . key set ( ) ; assert true ( server before . contains all ( server with primary ) ) ;,assignment return
verify that the includes file contains all <PLACE_HOLDER>,path file resource = new path ( config ) ; conf . add resource ( file resource ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; tear down ( ) ;,file contains
assume indirect definitions references use the <PLACE_HOLDER>,for ( node n : refs ) { if ( reference map . is call target ( n ) ) { node call node = reference map . get call or new node for target ( n ) ; if ( node util . is expression result used ( call node ) ) { return false ; } seen use = true ; } else if ( is candidate definition ( n ) ) { seen candidate definiton = true ; } else { if ( ! optimize calls . is allowed reference ( n ) ) { return false ; } } },references use
since hot swap passes run one <PLACE_HOLDER> at a time @$ namespaces seen will not include any provides earlier than this current <PLACE_HOLDER> .,if ( ! in hot swap && import type . must be ordered ( ) && ! namespaces seen . contains ( namespace ) ) { t . report ( call @$ late_provide_error @$ namespace ) ; },passes run
expected ip matches the given ip <PLACE_HOLDER> @$ return,if ( inet address . get by name ( expectedip ) . equals ( inet address . get by name ( ip address ) ) ) { return ; },ip matches
first run <PLACE_HOLDER> 1 @$ <PLACE_HOLDER> 3 are able to run <PLACE_HOLDER> 2 is blocked by <PLACE_HOLDER> 1,wait and assert timestamp ( p1keya @$ __num__ @$ __num__ ) ; wait and assert timestamp ( p2keya @$ __num__ @$ - __num__ ) ; wait and assert timestamp ( p3keyb @$ __num__ @$ __num__ ) ; assert equals ( true @$ locka . get ( ) ) ; assert equals ( true @$ lockb . get ( ) ) ;,1 run
we need to pass on input method events since some host input method adapters send them through the java event queue instead of directly to the component @$ and the input context also handles the java composition <PLACE_HOLDER>,if ( are input methods enabled ( ) ) { if ( ( ( e instanceof input method event ) && ! ( this instanceof composition area ) ) || ( e instanceof input event ) || ( e instanceof focus event ) ) { input context input context = get input context ( ) ; if ( input context != null ) { input context . dispatch event ( e ) ; if ( e . is consumed ( ) ) { if ( ( e instanceof focus event ) && focus log . is loggable ( platform logger . level . finest ) ) { focus log . finest ( __str__ + e ) ; } return ; } } } } else { if (,context handles
note : semgrex can match <PLACE_HOLDER> named nodes to the same node . in this case @$ we simply @$ check the named nodes @$ and if there are any collisions @$ we throw out this match .,while ( matcher . find ( ) ) { set < string > node names = matcher . get node names ( ) ; set < indexed word > seen = generics . new hash set ( ) ; for ( string name : node names ) { indexed word curr = matcher . get node ( name ) ; if ( seen . contains ( curr ) ) break next match ; seen . add ( curr ) ; } if ( predicate test != null ) { if ( ! predicate test . test ( matcher ) ) continue ; } semantic graph tgt = semantic graph factory . duplicate keep nodes ( sg ) ; node map = generics . new hash map ( ),semgrex match
easiest ways is using json <PLACE_HOLDER>,json object json to encode = new json object ( ) ; json to encode . put ( __str__ @$ custom message . get status code ( ) ) ; json to encode . put ( __str__ @$ custom message . get result code ( ) ) ; json to encode . put ( __str__ @$ custom message . get summary ( ) ) ;,ways using
register a pre commit <PLACE_HOLDER> that will touch the collection and delete the entity,( ( event source ) s ) . get action queue ( ) . register process ( new before transaction completion process ( ) { @ override public void do before transaction completion ( session implementor session ) { q to delete . get fums ( ) . size ( ) ; } } ) ; s . delete ( q to delete ) ; boolean ok = false ; try { s . get transaction ( ) . commit ( ) ; } catch ( lazy initialization exception e ) { ok = true ; s . get transaction ( ) . rollback ( ) ; } catch ( transaction exception te ) { if ( te . get cause ( ) instanceof lazy initialization exception ),pre commit
we used to report an error here if the <PLACE_HOLDER> was not resolved . having moved the call to 'check supers ' from 'basic check ' into 'resolve type structure ' @$ the errors reported here should have already been reported . furthermore @$ error recovery can null out the <PLACE_HOLDER> @$ which would cause a spurious error from the test here .,env . dt exit ( __str__ + this ) ;,recovery null
if permissions need a <PLACE_HOLDER> before any of the app components can run @$ we drop the broadcast and if the calling app is in the foreground and the broadcast is explicit we launch the <PLACE_HOLDER> ui passing it a pending intent to send the skipped broadcast .,if ( ! request start target permissions review if needed locked ( r @$ filter . package name @$ filter . owning user id ) ) { r . delivery [ index ] = broadcast record . delivery_skipped ; return ; } r . delivery [ index ] = broadcast record . delivery_delivered ;,permissions need
custom one ! also @$ this one has <PLACE_HOLDER> so ca n't use the singleton .,service initiators . add ( new quarkus mutable identifier generator factory initiator ( ) ) ;,one has
check types so we can make sure our exported externs have type <PLACE_HOLDER> .,options . set check symbols ( true ) ; return options ;,types have
default transfer handler does n't support <PLACE_HOLDER> so we do n't want <PLACE_HOLDER> handling,if ( tree . get drop target ( ) instanceof ui resource ) { tree . set drop target ( null ) ; },handler support
eat the exception as it does n't affect the state of existing tables . expect @$ user to manually drop this path when exception and so logging a <PLACE_HOLDER> .,log . warn ( __str__ @$ delete old data loc @$ dbname + __str__ + name ) ;,exception logging
reset attempts count and update cumulative wait <PLACE_HOLDER> .,backoff = reset backoff ( duration @$ nano clock @$ start nanos ) ;,attempts count
last removed column was frozen @$ meaning that all removed columns were frozen . just decrement the <PLACE_HOLDER> of frozen columns accordingly .,if ( index + number of columns < frozen columns ) { frozen columns -= number of columns ; } else { frozen columns = index ; },column decrement
load address <PLACE_HOLDER> of the mm .,load address ( msg id @$ headers ) ; int msg type = headers . get octet ( pdu headers . message_type ) ; pdu body body = new pdu body ( ) ;,load address
use a label if it is marked with one trailing star @$ even if the label string sorts the <PLACE_HOLDER> when all contractions are suppressed .,if ( item . char at ( item . length ( ) - __num__ ) == __str__ && item . char at ( item . length ( ) - __num__ ) != __str__ ) { item = item . substring ( __num__ @$ item . length ( ) - __num__ ) ; check distinct = false ; } else { check distinct = true ; },string sorts
the answer does n't match the <PLACE_HOLDER> . that 's not good .,if ( ! query . get question ( ) . equals ( response . get question ( ) ) ) { badresponse = true ; badresponse_error = __str__ ; log manager . i ( this @$ __str__ + badresponse_error ) ; return ; },answer match
this function does n't use 'att index ' . we are adding the attribute later after we have figured out that current attribute is not namespace declaration since scan attribute value does n't use att index <PLACE_HOLDER> therefore we can safely add the attribute later..,xml string tmp str = get string ( ) ; string localpart = f attributeq name . localpart ; string prefix = f attributeq name . prefix != null ? f attributeq name . prefix : xml symbols . empty_string ; boolean isns decl = f bind namespaces & ( prefix == xml symbols . prefix_xmlns || prefix == xml symbols . empty_string && localpart == xml symbols . prefix_xmlns ) ; scan attribute value ( tmp str @$ f temp string2 @$ f attributeq name . rawname @$ attributes @$ attr index @$ isvc @$ f current element . rawname @$ isns decl ) ; string value = null ;,value use
note to translators : the message indicates that the encoding requested for the output document was on that requires <PLACE_HOLDER> that is not available from the java virtual machine being used to execute the program .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,document requires
init logic get <PLACE_HOLDER>,local < member > method = code . get parameter ( __num__ @$ member type id ) ; local < xposed bridge . additional hook info > hook info = code . get parameter ( __num__ @$ hook info type id ) ;,logic get
some drivers do not report an incorrect <PLACE_HOLDER> . then the name is '\0 ' terminated .,if ( location == shader program constants . location_invalid ) { length = __num__ ; while ( length < shader program . name_container_size && shader program . name_container [ length ] != __str__ ) { length ++ ; } name = new string ( shader program . name_container @$ __num__ @$ length ) ; location = gles20 . gl get attrib location ( this . m programid @$ name ) ; if ( location == shader program constants . location_invalid ) { throw new shader program link exception ( __str__ + name + __str__ ) ; } },drivers report
login to ftps <PLACE_HOLDER> ...,connection . connect ( ) ;,login ftps
write out the keep <PLACE_HOLDER> and keep method options @$ if any .,if ( class specification . field specifications != null || class specification . method specifications != null ) { writer . print ( __str__ ) ; writer . println ( configuration constants . open_keyword ) ; write field specification ( class specification . field specifications ) ; write method specification ( class specification . method specifications ) ; writer . println ( configuration constants . close_keyword ) ; } else { writer . println ( ) ; },the keep
see if clazz extends <PLACE_HOLDER> of the configured base classes for this method,for ( string baseclass name : preserve overrides . get ( method name ) ) { class < ? > baseclass = load from internal ( baseclass name ) ; check state ( ! baseclass . is interface ( ) @$ __str__ @$ baseclass name ) ; if ( ! baseclass . is assignable from ( clazz ) ) { continue ; } for ( method m : clazz . get superclass ( ) . get methods ( ) ) { if ( method name . equals ( m . get name ( ) ) && descriptor . equals ( type . get method descriptor ( m ) ) && baseclass . equals ( m . get declaring class ( ) ) ) { return true ; },clazz extends
trigger check to see if parent size has changed @$ recalculate <PLACE_HOLDER>,resize timer = new timer ( ) { @ override public void run ( ) { perform size check ( ) ; resize timer . schedule ( monitor_parent_timer_interval ) ; } } ;,size changed
only first or second operator contains dpp <PLACE_HOLDER>,if ( dpps op1 . size ( ) != dpps op2 . size ( ) ) { return false ; },first contains
output will reflect default <PLACE_HOLDER>,verify ( message output ) . write ( expected ) ;,output reflect
firing timer should n't cancel <PLACE_HOLDER> @$ but create new task,jobs = management service . create timer job query ( ) . list ( ) ; assert equals ( __num__ @$ jobs . size ( ) ) ; management service . move timer to executable job ( jobs . get ( __num__ ) . get id ( ) ) ; management service . execute job ( jobs . get ( __num__ ) . get id ( ) ) ; tasks = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . order by task name ( ) . asc ( ) . list ( ) ; assert equals ( __num__ @$ tasks . size ( ) ) ; assert equals ( __str__ @$ tasks . get,timer cancel
using random uuid ensures that multiple clusters can be launched by a same test @$ if it stops & starts <PLACE_HOLDER>,path test dir = get data test dir ( __str__ + get randomuuid ( ) . to string ( ) ) ; cluster test dir = new file ( test dir . to string ( ) ) . get absolute file ( ) ;,& starts
now do the 1 big <PLACE_HOLDER> .,file . seek ( next write batch . offset ) ; if ( max stat > __num__ ) { if ( stat idx < max stat ) { stats [ stat idx ++ ] = sequence . get length ( ) ; } else { long all = __num__ ; for ( ; stat idx > __num__ ; ) { all += stats [ -- stat idx ] ; } log . trace ( __str__ @$ all / max stat ) ; } } file . write ( sequence . get data ( ) @$ sequence . get offset ( ) @$ sequence . get length ( ) ) ; replication target replication target = journal . get replication target ( ) ; if ( replication target !=,now do
action client env contains the <PLACE_HOLDER> where values from action <PLACE_HOLDER> are overridden .,action client env . put all ( client env ) ; if ( command . builds ( ) ) { for ( map . entry < string @$ string > entry : options . get options ( core options . class ) . action environment ) { if ( entry . get value ( ) == null ) { visible action env . add ( entry . get key ( ) ) ; } else { visible action env . remove ( entry . get key ( ) ) ; action client env . put ( entry . get key ( ) @$ entry . get value ( ) ) ; } } for ( map . entry < string @$ string > entry : options . get,env contains
currently project <PLACE_HOLDER> can not have derived <PLACE_HOLDER> .,if ( view . get configuration ( ) . get module ( ) == null ) { return views ; } final string query = __str__ ; try ( prepared statement statement = provider . get connection ( ) . get connection ( ) . prepare statement ( query ) ) { final list < i navi view > module views = view . get configuration ( ) . get module ( ) . get content ( ) . get view container ( ) . get views ( ) ; statement . set int ( __num__ @$ view . get configuration ( ) . get id ( ) ) ; final result set result set = statement . execute query ( ) ; if ( result set ==,views derived
if the mapping list contains the generic type <PLACE_HOLDER> @$ begin to check this generic type <PLACE_HOLDER> class .,if ( lite pal attr . get instance ( ) . get class names ( ) . contains ( generic type name ) ) { class < ? > reverse dynamic class = class . for name ( generic type name ) ; field [ ] reverse fields = reverse dynamic class . get declared fields ( ) ; boolean reverse associations = false ; for ( field reverse field : reverse fields ) { if ( ! modifier . is static ( reverse field . get modifiers ( ) ) ) { class < ? > reverse field type class = reverse field . get type ( ) ; if ( class name . equals ( reverse field type class . get name ( ) ) ),list contains
now that the store is sane @$ try and get all the <PLACE_HOLDER> sent,return receive messages ( messages expected @$ session @$ one phase ) ;,store sane
replication causes many implicit alter database <PLACE_HOLDER> @$ so metastore will see some alter table events as well .,events map . put ( alter database event . class . get name ( ) @$ null ) ; events map . put ( create table event . class . get name ( ) @$ new hash set < > ( arrays . as list ( __str__ ) ) ) ; events map . put ( alter table event . class . get name ( ) @$ new hash set < > ( arrays . as list ( __str__ @$ __str__ @$ __str__ ) ) ) ; events map . put ( drop table event . class . get name ( ) @$ new hash set < > ( arrays . as list ( __str__ ) ) ) ; return events map ;,some alter
we have to use do <PLACE_HOLDER> invoke here instead of simply invoke @$ because get meta <PLACE_HOLDER> may provide a <PLACE_HOLDER> that can not be called without further argument transformation @$ which is done only in do <PLACE_HOLDER> invoke,if ( meta method != null ) { return meta method . do method invoke ( this @$ args arr ) ; },method provide
the two approaches should produce the same hash <PLACE_HOLDER>,assert false ( seq add tokens . is empty ( ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert equals ( batch add tokens @$ seq add tokens ) ;,approaches produce
should not work : <PLACE_HOLDER> on string,try { tuple ds . aggregate ( aggregations . sum @$ __num__ ) ; assert . fail ( ) ; } catch ( unsupported aggregation type exception iae ) { },not work
use the record <PLACE_HOLDER> processor list if it is configured . this list can contain both all record <PLACE_HOLDER> processor types,if ( ! this . record stream processors . is empty ( ) ) { for ( record stream processor stream processor : this . record stream processors ) { stream = stream processor . process stream ( stream @$ this . task state ) ; } } else { if ( this . converter instanceof multi converter ) { for ( converter cverter : ( ( multi converter ) this . converter ) . get converters ( ) ) { stream = cverter . process stream ( stream @$ this . task state ) ; } } else { stream = this . converter . process stream ( stream @$ this . task state ) ; } },list contain
should use root entity <PLACE_HOLDER> by default,criteria executor criteria executor aliased1 = new criteria executor ( ) { protected criteria get criteria ( session s ) { return s . create criteria ( student . class @$ __str__ ) . create alias ( __str__ @$ __str__ @$ criteria . left_join ) . set fetch mode ( __str__ @$ fetch mode . join ) . add order ( order . asc ( __str__ ) ) ; } } ;,use root
make the user agent tester change its <PLACE_HOLDER> and make sure we do n't get notifications as we 're now unsubscribed .,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = supported status set2 . get ( jabber status enum . free_for_chat ) ;,tester change
the second time should contain the <PLACE_HOLDER>,has certificate = info . get transport context ( ) instanceof x509 certificate [ ] ;,time contain
some systems send update <PLACE_HOLDER> before call is established to update call screening @$ if we do not process them and just send 501 we end the dialog and the call will fail to establish @$ so we just send ok,return false ;,systems send
new chunk requires a new <PLACE_HOLDER>,statement dsg = new declaration statement group ( this @$ module ) ; dsg stack . push ( statement dsg ) ;,chunk requires
vm <PLACE_HOLDER> locks and frees key <PLACE_HOLDER>,vm1 . invoke ( new serializable runnable ( ) { @ override public void run ( ) { logger . info ( __str__ ) ; connect distributed system ( ) ; d lock service dls = ( d lock service ) d lock service . create ( dls name @$ get system ( ) @$ true @$ true @$ false ) ; assert that ( dls . lock ( key1 @$ - __num__ @$ - __num__ ) ) . is true ( ) ; logger . info ( __str__ ) ; dls . unlock ( key1 ) ; assert that ( dls . get token ( key1 ) ) . is not null ( ) ; dls . free resources ( key1 ) ; d lock token token,vm locks
this table has no <PLACE_HOLDER> .,return null ;,table has
fails for e.g . broken sockets silently swallow <PLACE_HOLDER> and just record the failed path,fails . add ( path ) ;,sockets swallow
noinspection unchecked ca n't know target entity <PLACE_HOLDER> .,if ( relation info . to one getter != null ) { to one to one = relation info . to one getter . get to one ( entity ) ; if ( to one != null ) { to one . get target ( ) ; } } else { if ( relation info . to many getter == null ) { throw new illegal state exception ( __str__ + relation info ) ; } list to many = relation info . to many getter . get to many ( entity ) ; if ( to many != null ) { to many . size ( ) ; } },unchecked know
let the visitor visit the <PLACE_HOLDER> referenced in the local variable .,local variable info . referenced class accept ( class visitor ) ;,visitor visit
if we found ourselves closed after the <PLACE_HOLDER> increment @$ decrement the <PLACE_HOLDER> again and do not forward the request,if ( this . closed || this . request queue . is closed ( ) ) { this . requests not returned . decrement and get ( ) ; final notification listener listener ; synchronized ( listener lock ) { listener = all requests processed listener ; all requests processed listener = null ; } if ( listener != null ) { listener . on notification ( ) ; } throw new io exception ( __str__ + request ) ; },request decrement
we have finished processing this element . decrement the <PLACE_HOLDER> .,int new depth = depth . decrement and get ( ) ;,element decrement
in all chunks except the last this chunk also takes <PLACE_HOLDER> of the detection in the seam @$ but for the last one there 's no seam at the end .,long end = last ? to exclusive - __num__ : to exclusive ; for ( long i = from inclusive ; i < end ; i ++ ) { detect ( same group detector @$ i ) ; if ( ++ local progress == __num__ ) { progress . add ( local progress ) ; local progress = __num__ ; } } progress . add ( local progress ) ;,chunk takes
sensors do n't generate <PLACE_HOLDER> .,m_manifold . point count = __num__ ; evaluate ( m_manifold @$ xfa @$ xfb ) ; touching = m_manifold . point count > __num__ ;,sensors generate
another wrapping panel so the borderlayout 'd source list <PLACE_HOLDER> panel does n't get forced to take up the full east cell of the containing panel .,j panel button wrapper panel = new j panel ( ) ; button wrapper panel . add ( source list buttons panel ) ; source list panel . add ( button wrapper panel @$ border layout . east ) ; source list model . add list data listener ( new list data listener ( ) { @ override public void interval removed ( list data event e ) { contents changed ( e ) ; } @ override public void interval added ( list data event e ) { contents changed ( e ) ; } @ override public void contents changed ( list data event e ) { boolean has selection = source list . get selected indices ( ) . length > __num__ ; remove source,panel "d"
we explicitly do n't close 'out ' because we must not close the backing <PLACE_HOLDER> . the file output stream will not close it implicitly .,@ suppress warnings ( __str__ ) data output stream out = new data output stream ( fos ) ; out . write int ( m current blob version ) ; final int n = ( state != null ) ? state . size ( ) : __num__ ; out . write int ( n ) ; for ( int i = __num__ ; i < n ; i ++ ) { final string key = state . key at ( i ) ; final long checksum = state . value at ( i ) . long value ( ) ; if ( debug ) { log . i ( tag @$ __str__ + key + __str__ + checksum ) ; } out . writeutf ( key ) ;,stream close
wait till client gets the <PLACE_HOLDER>,test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { int events = counting consumer . get num data events ( ) ; log . info ( __str__ + events + __str__ ) ; return events == __num__ ; } } @$ __str__ @$ __num__ * __num__ @$ log ) ;,client gets
do the reduce <PLACE_HOLDER>,t res = reducer . reduce ( match @$ record ) ;,the reduce
bring up wifi @$ then validate it . previous versions would immediately tear down <PLACE_HOLDER> @$ but it 's arguably correct to linger it @$ since it was the default network before it validated .,m wi fi network agent = new test network agent wrapper ( transport_wifi ) ; m wi fi network agent . connect ( true ) ; callback . expect available callbacks unvalidated ( m wi fi network agent ) ;,versions tear
no item @$ so create <PLACE_HOLDER>,if ( item == null ) { item = new philm person ( ) ; },item create
target tier <PLACE_HOLDER> :,assert equals ( input tier1 . size ( ) @$ __num__ ) ; assert equals ( input tier1 . get ( __num__ ) . get amount ( ) @$ new big decimal ( __str__ ) ) ;,target tier
most qjmha cluster tests do n't need data <PLACE_HOLDER> @$ so we 'll make this the default,this . dfs builder = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) ;,tests need
host @$ port @$ ssl @$ ssl socket factory @$ ssl parameters @$ hostname <PLACE_HOLDER>,jedis utils . add set end point interceptor ( target @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ; return target . to bytecode ( ) ;,factory hostname
if the arc identifies a readable <PLACE_HOLDER> @$ then two cases :,if ( is readable ( arc ) ) { if ( pos == ( length - __num__ ) ) { result = new long [ depth + __num__ ] ; result [ depth + __num__ ] = __num__ ; result [ depth ] = arc ; checker . add ( depth @$ result @$ depth @$ __num__ ) ; try { checker . check current oid ( ) ; } catch ( snmp status exception e ) { throw new snmp status exception ( snmp status exception . no such object ) ; } finally { checker . remove ( depth @$ __num__ ) ; } handlers . add ( this @$ depth @$ varbind ) ; return result ; } } else if ( is nested arc,arc identifies
start the two trigger <PLACE_HOLDER> . they will wait at the barrier,t1 . start ( ) ; t2 . start ( ) ;,two trigger
parse fail <PLACE_HOLDER> @$ if present,if ( status info . data . available ( ) > __num__ ) { this . failure info = status info . data . get unaligned bit string ( ) . to boolean array ( ) ; },parse fail
say hello to equinox who has its own <PLACE_HOLDER> . we use introspection like there is no tomorrow to get access to the file,if ( url . get protocol ( ) . equals ( __str__ ) ) { url connection con = url . open connection ( ) ; con . set use caches ( resource . get default use caches ( ) ) ; if ( bundle_entry_field == null ) { bundle_entry_field = con . get class ( ) . get declared field ( __str__ ) ; bundle_entry_field . set accessible ( true ) ; } object bundle entry = bundle_entry_field . get ( con ) ; if ( match ( bundle entry . get class ( ) . get name ( ) @$ file_bundle_entry_classes ) ) { if ( file_field == null ) { file_field = bundle entry . get class ( ) . get declared field ( __str__,who has
foo impl was implicitly bound @$ it is an error to call get instance or get <PLACE_HOLDER> @$ it is ok to call get binding for introspection @$ but an error to get the <PLACE_HOLDER> of the binding,ensure fails ( injector @$ allow_binding @$ foo impl . class ) ;,error get
we do n't have permission to read <PLACE_HOLDER> creds we should get an empty set even though the private creds has an <PLACE_HOLDER> cred . no security exception either !,try { set priv cred set1 = s . get private credentials ( integer . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { system . out . println ( __str__ ) ; } system . out . println ( __str__ ) ; set priv cred set2 = s . get private credentials ( ) ; if ( priv cred set2 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set2 . size ( ) ) ; },creds has
a volt db extension to disable variable scale <PLACE_HOLDER>,scale = __num__ ;,extension disable
if input has <PLACE_HOLDER>,if ( text . matches ( __str__ ) ) { assert equals ( pos . get error index ( ) >= __num__ @$ true ) ; assert equals ( pos . get index ( ) @$ __num__ ) ; assert equals ( parsed @$ null ) ; } else { assert equals ( pos . get index ( ) @$ expected index @$ __str__ + lc text ) ; assert equals ( pos . get error index ( ) @$ expected error index @$ __str__ + lc text ) ; if ( expected != null ) { assert equals ( parsed . query ( temporal queries . zone id ( ) ) @$ expected ) ; assert equals ( parsed . query ( temporal queries . offset (,input has
let the dom organize <PLACE_HOLDER>,thread . sleep ( __num__ ) ;,dom organize
server <PLACE_HOLDER> should not retrieve local <PLACE_HOLDER> .,verify no more interactions ( local logs ) ;,logs retrieve
when <PLACE_HOLDER> is on @$ do <PLACE_HOLDER> based user auth check,try { if ( authorization enabled && access controller available && ! is system or super user ( ) ) { user user = visibility utils . get active user ( ) ; throw new access denied exception ( __str__ + ( user != null ? user . get short name ( ) : __str__ ) + __str__ ) ; } if ( authorization enabled ) { check calling user auth ( ) ; } for ( byte string authbs : auths ) { label auths . add ( authbs . to byte array ( ) ) ; } operation status [ ] op status = this . visibility label service . clear auths ( request user @$ label auths ) ; log result ( true @$ __str__,check do
the above connection request should trigger a 'connecting ' state <PLACE_HOLDER> to be replicated,while ( node statuses . is empty ( ) ) { thread . sleep ( __num__ ) ; } final node connection status connecting status = node statuses . get ( __num__ ) ; assert equals ( node connection state . connecting @$ connecting status . get state ( ) ) ; assert equals ( requested node id @$ connecting status . get node identifier ( ) ) ;,request trigger
start get <PLACE_HOLDER> .,executors . execute ( new runnable ( ) { @ override public void run ( ) { try { list < string > keys = new array list < string > ( test entries . key set ( ) ) ; while ( ! rebalancing token . get ( ) ) { int index = ( int ) ( math . random ( ) * keys . size ( ) ) ; try { list < versioned < byte [ ] > > values = server side routing storerw . get ( new byte array ( byte utils . get bytes ( keys . get ( index ) @$ __str__ ) ) @$ null ) ; assert equals ( __str__ @$ __num__ @$ values . size ( ),start get
simultaneously null implies we are not connected to the db @$ implies undesirable <PLACE_HOLDER> so throw exception,if ( conn == null && ps == null && rs == null ) { throw new sql exception ( res bundle . handle get object ( __str__ ) . to string ( ) ) ; },exception implies
the user specified an empty <PLACE_HOLDER> to skip the copy,if ( s . trim ( ) . ends with ( __str__ ) || s . trim ( ) . length ( ) == __num__ ) { continue ; },user specified
any fully contained <PLACE_HOLDER> beats a partially contained <PLACE_HOLDER>,if ( view is fully contained ) { focus candidate = view ; found fully contained focusable = true ; } else if ( view is closer to boundary ) { focus candidate = view ; },view beats
ensure root has the correct inode <PLACE_HOLDER> last inode <PLACE_HOLDER> should be root inode <PLACE_HOLDER> and inode map size should be 1,int inode count = __num__ ; long expected last inode id = i node id . root_inode_id ; assert equals ( fsn . dir . root dir . get id ( ) @$ i node id . root_inode_id ) ; assert equals ( expected last inode id @$ last id ) ; assert equals ( inode count @$ fsn . dir . get inode map size ( ) ) ;,root has
try to load the <PLACE_HOLDER> as a pem encoded public <PLACE_HOLDER>,if ( pem reader . is pem ( data ) ) { try { return new loaded key ( pem reader . load public key ( data ) ) ; } catch ( runtime exception | general security exception e ) { throw new signature exception ( __str__ @$ e ) ; } },pem encoded
chrome always allows insertion <PLACE_HOLDER> .,return true ;,chrome allows
update permission on an outdated acl @$ retry should keep <PLACE_HOLDER> going,principal sid user1 = new principal sid ( __str__ ) ; mutable acl record child acl2 = acl service . upsert ace ( child acl outdated @$ user1 @$ acl permission . administration ) ; assert . assert equals ( parent oid @$ child acl2 . get acl record ( ) . get parent domain object info ( ) ) ; assert . assert equals ( acl permission . administration @$ child acl2 . get acl record ( ) . get permission ( user1 ) ) ;,retry keep
note that 'rollback ' closes the <PLACE_HOLDER> .,if ( writer != null ) { writer . rollback ( ) ; },"rollback" closes
we do n't want the packager closing the <PLACE_HOLDER> . v 1 creates a tar output <PLACE_HOLDER> @$ which then gets closed @$ which in turn closes the underlying output <PLACE_HOLDER> @$ and we want to protect ourselves against that .,try ( final output stream buffered out = new buffered output stream ( raw out ) ) { final output stream out = new non closeable output stream ( buffered out ) ; for ( final flow file flow file : contents ) { bin . get session ( ) . read ( flow file @$ false @$ new input stream callback ( ) { @ override public void process ( final input stream raw in ) throws io exception { try ( final input stream in = new buffered input stream ( raw in ) ) { final map < string @$ string > attributes = new hash map < > ( flow file . get attributes ( ) ) ; attributes . put ( __str__ @$,which closes
let 's take the <PLACE_HOLDER> of item geopoint x field ...,data . index of fielditempointx = data . input row meta . index of value ( meta . get geo point lat ( ) ) ; if ( data . index of fielditempointx < __num__ ) { log error ( base messages . get string ( pkg @$ __str__ @$ meta . get geo point lat ( ) ) ) ; throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ meta . get geo point lat ( ) ) ) ; },"s" take
the abort slow consumer strategy kicks in here and sends a message down to the client to close itself . the client does not close because it is in the middle of the transaction . meanwhile the failover transport detects the close <PLACE_HOLDER> and removes the consumer from its state .,assert true ( __str__ @$ wait . wait for ( new wait . condition ( ) { @ override public boolean is satisified ( ) throws exception { return aborting slow consumer ; } } @$ __num__ * __num__ ) ) ;,transport detects
convert deoptimize to guard phase reduces the <PLACE_HOLDER> of merges in the graph @$ so that fewer frame states will be created . this significantly reduces the <PLACE_HOLDER> of nodes in the initial graph .,try ( debug context . scope scope = debug . scope ( __str__ @$ graph to encode ) ) { new convert deoptimize to guard phase ( ) . apply ( graph to encode @$ providers ) ; } catch ( throwable t ) { throw debug . handle ( t ) ; } encoded graph encoded graph = graph encoder . encode single graph ( graph to encode @$ architecture ) ; graph cache . put ( method @$ encoded graph ) ; return encoded graph ;,phase reduces
bk commit time safety <PLACE_HOLDER>,int thread sleep time = idle reader error threshold - __num__ - __num__ ;,bk commit
query the superclass @$ which triggers argument <PLACE_HOLDER> .,final path p = make qualified ( path ) ; switch ( validate path capability args ( p @$ capability ) ) { case common path capabilities . fs_acls : case common path capabilities . fs_append : case common path capabilities . fs_concat : case common path capabilities . fs_permissions : case common path capabilities . fs_snapshots : case common path capabilities . fs_storagepolicy : case common path capabilities . fs_xattrs : return true ; case common path capabilities . fs_symlinks : return false ; default : return super . has path capability ( p @$ capability ) ; },which triggers
add pending remote input <PLACE_HOLDER> after starting work challenge @$ as starting work challenge will clear all previous pending re<PLACE_HOLDER> <PLACE_HOLDER>,m pending work remote input view = clicked ;,challenge clear
a<PLACE_HOLDER><PLACE_HOLDER>ert that root doe<PLACE_HOLDER> not contain <PLACE_HOLDER> 1,assert true ( ! root . contains ( frag one ) ) ;,root contain
does not search <PLACE_HOLDER>,if ( scope activity . get activities ( ) . contains ( current activity ) || scope activity . equals ( current activity ) ) { candiadate execution = execution ; } else if ( current activity != null && current activity . contains ( ( activity impl ) scope activity ) ) { break ; },not search
we have backed up this app before . check whether the <PLACE_HOLDER> of the backup matches the <PLACE_HOLDER> of the current app ; if they do n't match @$ the app has been updated and we need to store its metadata again . in either case @$ take it out of m existing so that we do n't consider it deleted later .,if ( m existing . contains ( pack name ) ) { m existing . remove ( pack name ) ; if ( info . get long version code ( ) == m state versions . get ( pack name ) . version code ) { continue ; } },version matches
basic split pane divider.get preferred <PLACE_HOLDER> for the reason . i leave it in hopes of having this used at some point .,if ( c != basic split pane divider . this || split pane == null ) { return new dimension ( __num__ @$ __num__ ) ; } dimension button min size = null ; if ( split pane . is one touch expandable ( ) && left button != null ) { button min size = left button . get minimum size ( ) ; } insets insets = get insets ( ) ; int width = get divider size ( ) ; int height = width ; if ( orientation == j split pane . vertical_split ) { if ( button min size != null ) { int size = button min size . height ; if ( insets != null ) { size += insets .,split pane
otherwise @$ no display text @$ just use the executable <PLACE_HOLDER>,string program info = text [ __num__ ] ; return get display text for file path or name ( program info ) ;,text use
skip pass all the finally <PLACE_HOLDER> because both the fall through and return will also trigger all the finally <PLACE_HOLDER> .,node prefinally follows = follow ; follow = skip finally nodes ( follow ) ; if ( prefinally follows != follow ) { if ( ! is pure ( exit expr ) ) { return n ; } },return trigger
the selector returned prematurely many <PLACE_HOLDER> in a row . rebuild the selector to work around the problem .,if ( selector_auto_rebuild_threshold > __num__ && select cnt >= selector_auto_rebuild_threshold ) { logger . warn ( __str__ @$ select cnt @$ selector ) ; rebuild selector ( ) ; return true ; },selector returned
test what happens if a user acc<PLACE_HOLDER>entally uses the same <PLACE_HOLDER> in multiple layouts too .,parsed android data direct = android data builder . of ( source ) . add resource ( __str__ @$ android data builder . resource type . layout @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . layout @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . value @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . value @$ __str__ ) . create manifest ( __str__ @$ __str__ @$ __str__ ) . build parsed ( ) ;,user uses
this should be 0 because the broker needs 2 buffers and the queryable node needs <PLACE_HOLDER> .,assert . assert equals ( __num__ @$ merge_buffer_pool . get min remain buffer num ( ) ) ; assert . assert equals ( __num__ @$ merge_buffer_pool . get pool size ( ) ) ;,buffers needs
a volt db extension to avoid a <PLACE_HOLDER> when a is not an instance of double,if ( ! ( a instanceof double ) ) { a = new double ( a . to string ( ) ) ; },extension avoid
j rockit throws this exception instead of returning null as the javadocs say it should . see <PLACE_HOLDER> 36348,this . set collection usage unsupported ( mp ) ;,36348 see
an inactive context may not have a <PLACE_HOLDER> so we use our <PLACE_HOLDER> to call all of the context 's listeners instead,if ( context instanceof abstract application context ) { for ( application listener < ? > listener : ( ( abstract application context ) context ) . get application listeners ( ) ) { this . initial multicaster . add application listener ( listener ) ; } } this . initial multicaster . set error handler ( new logging error handler ( ) ) ; this . initial multicaster . multicast event ( event ) ;,context have
check the locale first @$ in case the locale has the same <PLACE_HOLDER> .,locale data locale data = locale data . get ( locale ) ; if ( locale data . international currency symbol . equals ( currency code ) ) { return locale data . currency symbol ; },locale has
if the proxy requires basic <PLACE_HOLDER> :,if ( wuse basic authentication . get selection ( ) ) { conn . set proxy data ( new http proxy data ( job meta . environment substitute ( whttp proxy host . get text ( ) ) @$ const . to int ( whttp proxy port . get text ( ) @$ __num__ ) @$ job meta . environment substitute ( whttp proxy username . get text ( ) ) @$ job meta . environment substitute ( whttp proxy password . get text ( ) ) ) ) ; } else { conn . set proxy data ( new http proxy data ( job meta . environment substitute ( whttp proxy host . get text ( ) ) @$ const . to int ( whttp proxy port,proxy requires
only one geometry @$ let 's not wrap it in another node unless the node has <PLACE_HOLDER> .,if ( primitives . length == __num__ && children == null ) { spatial = primitives [ __num__ ] ; } else { node node = new node ( ) ; for ( geometry primitive : primitives ) { node . attach child ( primitive ) ; } spatial = node ; },node has
where the code forgot the <PLACE_HOLDER> !,assert equals ( expected @$ format information . decode format information ( unmasked_test_format_info @$ masked_test_format_info ) ) ;,code forgot
this will trigger end callback which should set <PLACE_HOLDER> to their target values .,view . animate ( ) . cancel ( ) ;,which set
create all connectors before adding @$ so a broken connector does not leave the system half <PLACE_HOLDER>,materialized connector connector = new materialized connector ( catalog name @$ create connector ( catalog name @$ factory @$ properties ) ) ; connector handle resolver connector handle resolver = connector . get connector ( ) . get handle resolver ( ) . or else get ( factory . get connector factory ( ) :: get handle resolver ) ; check argument ( connector handle resolver != null @$ __str__ @$ factory ) ; materialized connector information schema connector = new materialized connector ( create information schema catalog name ( catalog name ) @$ new information schema connector ( catalog name . get catalog name ( ) @$ node manager @$ metadata manager @$ access control manager ) ) ; catalog name system id = create system tables,connector leave
we can only set up the real icq test suites when the <PLACE_HOLDER>.properties file defines the two test <PLACE_HOLDER>,if ( icq test agent name != null ) { icq slick fixture . tester agent = new icq tester agent ( icq test agent name ) ; string icq test agent pwd = system . get property ( testing_impl_pwd_prop_name @$ null ) ; if ( icq slick fixture . tester agent . register ( icq test agent pwd ) ) { if ( ! icq slick fixture . online testing disabled ) { icq slick fixture . tester agent . set authorization required ( ) ; try { initialize tested contact list ( ) ; } catch ( exception ex ) { logger . error ( __str__ @$ ex ) ; } string offline msg body = __str__ + __str__ ; icq slick fixture . offline msg,file defines
note to translators : the stylesheet referred to an extension to the xsl syntax and indicated that it was defined by xsltc @$ but xstlc does not recognized the particular extension <PLACE_HOLDER>d . the substitution text gives the extension <PLACE_HOLDER> .,return new object [ ] [ ] { { error msg . multiple_stylesheet_err @$ __str__ } @$ { error msg . template_redef_err @$ __str__ } @$ { error msg . template_undef_err @$ __str__ } @$ { error msg . variable_redef_err @$ __str__ } @$ { error msg . variable_undef_err @$ __str__ } @$ { error msg . class_not_found_err @$ __str__ } @$ { error msg . method_not_found_err @$ __str__ } @$ { error msg . argument_conversion_err @$ __str__ } @$ { error msg . file_not_found_err @$ __str__ } @$ { error msg . invalid_uri_err @$ __str__ } @$ { error msg . file_access_err @$ __str__ } @$ { error msg . missing_root_err @$ __str__ } @$ { error msg . namespace_undef_err @$ __str__ } @$ { error msg,text gives
wait a little bit to let the delete take <PLACE_HOLDER> .,thread . sleep ( __num__ ) ;,delete take
note the api requires a source file <PLACE_HOLDER> because the html and xml formatters display a page of code annotated with coverage information . having the source files is not actually needed for generating the lcov report ...,visitor . visit bundle ( bundle coverage @$ new i source file locator ( ) { @ override public reader get source file ( string package name @$ string file name ) throws io exception { return null ; } @ override public int get tab width ( ) { return __num__ ; } } ) ;,source file
class cast <PLACE_HOLDER>,try { set perms = new hash set ( ) ; perms . add ( new object ( ) ) ; view . set permissions ( perms ) ; throw new runtime exception ( __str__ ) ; } catch ( class cast exception x ) { },class cast
more complicated . vh 2 has a higher version @$ but has some <PLACE_HOLDER> that vh 1 does not have .,region version holder vh1 = new region version holder ( member ) ; region version holder vh2 = new region version holder ( member ) ; bit set bs1 = new bit set ( ) ; bs1 . set ( __num__ @$ __num__ ) ; bs1 . set ( __num__ @$ __num__ ) ; record versions ( vh1 @$ bs1 ) ; bit set bs2 = new bit set ( ) ; bs2 . set ( __num__ @$ __num__ ) ; bs2 . set ( __num__ @$ __num__ ) ; record versions ( vh2 @$ bs2 ) ;,vh has
before conf object is passed in @$ rm has already processed it and used rm specific <PLACE_HOLDER> to overwrite hadoop common ones . hence we just need to source hadoop.proxyuser <PLACE_HOLDER> here .,map < string @$ string > filter config = authentication filter initializer . get filter config map ( conf @$ config prefix ) ;,ones rm
matching legacy i pv 4 <PLACE_HOLDER> and path matching legacy i pv 4 <PLACE_HOLDER> and path matching second <PLACE_HOLDER> of i pv 4 addresses and path matching second <PLACE_HOLDER> of i pv 4 addresses and path matching i pv 6 <PLACE_HOLDER> and path matching i pv 6 <PLACE_HOLDER> and path matching i pv 4 cidr notation <PLACE_HOLDER> and path matching i pv 4 cidr,final string [ ] [ ] allowed = { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } ;,range matching
in order to allow programs to use a single <PLACE_HOLDER> as the display for multiple tabs @$ we will not change the visible compnent if the currently selected tab has a null <PLACE_HOLDER> . this is a bit dicey @$ as we do n't explicitly state we support this in the spec @$ but since programs are now depending on this @$ we 're,if ( selected component != null ) { if ( selected component != visible component && visible component != null ) { if ( swing utilities . find focus owner ( visible component ) != null ) { should change focus = true ; } } set visible component ( selected component ) ; } final rectangle bounds = tab pane . get bounds ( ) ; final int num children = tab pane . get component count ( ) ; if ( num children > __num__ ) { switch ( tab placement ) { case left : total tab width = calculate tab area width ( tab placement @$ run count @$ max tab width ) ; cx = insets . left + total tab width +,tab has
only class object type can use property <PLACE_HOLDER>,if ( ! ( object type instanceof class object type ) ) { left declared mask = all set bit mask . get ( ) ; return ; },type use
middle of an existing chain download . create a <PLACE_HOLDER> of peers .,peer group . start ( ) ;,middle create
nb : if rule acts as a sub @$ do not include type <PLACE_HOLDER>,boolean sub head = head . get atom ( ) . is type ( ) ; if ( sub head ) { body . get atoms ( ) . stream ( ) . filter ( atomic :: is type ) . filter ( at -> at . get var name ( ) . equals ( head . get atom ( ) . get var name ( ) ) ) . for each ( all atoms :: remove ) ; } all atoms . add ( head . get atom ( ) ) ; return reasoner query factory . create ( all atoms ) ;,nb include
set group 2 bool state to true @$ will trigger the <PLACE_HOLDER>,g2i1 . switch state ( ) ;,state trigger
if <PLACE_HOLDER>loading failed @$ other thread may have called <PLACE_HOLDER> loader which will null out <PLACE_HOLDER> @$ hence we check for it .,synchronized ( this ) { if ( image != null ) { if ( ( new state & width_flag ) == width_flag || width == __num__ ) { width = new width ; } if ( ( new state & height_flag ) == height_flag || height == __num__ ) { height = new height ; } } else { create text = true ; if ( ( new state & width_flag ) == width_flag ) { width = new width ; } if ( ( new state & height_flag ) == height_flag ) { height = new height ; } } state = state | new state ; state = ( state | loading_flag ) ^ loading_flag ; },which null
pairwise : init ouput col has <PLACE_HOLDER> @$ init ouput col is repeating @$ column 1 has <PLACE_HOLDER> @$ column 1 is repeating @$ column 2 has <PLACE_HOLDER> @$ column 2 is repeating,for ( boolean [ ] test matrix : new boolean [ ] [ ] { { true @$ true @$ false @$ true @$ true @$ true } @$ { false @$ false @$ true @$ false @$ false @$ false } @$ { true @$ false @$ true @$ false @$ true @$ true } @$ { true @$ true @$ true @$ true @$ false @$ false } @$ { false @$ false @$ false @$ true @$ true @$ false } @$ { false @$ true @$ false @$ false @$ false @$ true } } ) { string test case = template string ; test case = test case . replace all ( __str__ @$ __str__ + vector exp class name + create null,column has
singles have special <PLACE_HOLDER>,object [ ] [ ] should fail = { { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ @$ parse exception . class } @$ { __str__ } @$ { __str__ } @$ { __str__ @$ parse exception . class } @$ { __str__ @$ parse exception . class } @$ { __str__ } @$,singles have
desired schema does not include virtual <PLACE_HOLDER> or partition <PLACE_HOLDER> .,type description result = type description . create struct ( ) ; for ( int i = __num__ ; i < schema evolution type descrs . size ( ) ; i ++ ) { result . add field ( schema evolution column names . get ( i ) @$ schema evolution type descrs . get ( i ) ) ; } return result ;,schema include
hmmm @$ did this member have a restart ? determine <PLACE_HOLDER> member dir might be a match for us,if ( ! baseline dir . exists ( ) ) { baseline dir = find baseline for this member ( member backup location dir . get parent ( ) @$ disk store ) ; },dir match
thread a 1 gets <PLACE_HOLDER> a with thread a 1,if ( threada1 . in event loop ( ) ) { await ( arrival barrier ) ; return poola1 ; } else if ( threada2 . in event loop ( ) ) { await ( arrival barrier ) ; await ( release barrier ) ; return poola2 ; },1 gets
visiting the configuration page should n't change <PLACE_HOLDER>,html page pg = wc . go to ( __str__ ) ; j . submit ( pg . get form by name ( __str__ ) ) ; p = u . get property ( last granted authorities property . class ) ; assert authorities ( p @$ __str__ ) ; assert authorities ( u . impersonate ( ) @$ __str__ ) ;,page change
end is not important since the match crosses code <PLACE_HOLDER>,assert equals ( __num__ @$ h [ __num__ ] . get start ( ) ) ;,match crosses
string <PLACE_HOLDER> follows str index <PLACE_HOLDER>,int st off = sit off + numb strings * __num__ ;,table follows
the test succeeds if we get the no class def found <PLACE_HOLDER> .,try { in . read object ( ) ; } catch ( no class def found error e ) { if ( e == ncdfe ) { system . err . println ( __str__ + e . to string ( ) ) ; } else { throw e ; } },def found
semijoins may have created task level <PLACE_HOLDER> @$ examine those,connect terminal ops ( proc ctx . parse context ) ; boolean cycle free = false ; while ( ! cycle free ) { cycle free = true ; set < set < operator < ? > > > components = get components ( proc ctx ) ; for ( set < operator < ? > > component : components ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ ) ; for ( operator < ? > co : component ) { log . debug ( __str__ + co . get name ( ) + __str__ + co . get identifier ( ) ) ; } } if ( component . size ( ) != __num__ ) { log,semijoins created
should n't happen if this is only being called by a single thread . plumber.add should be swapping out <PLACE_HOLDER> before they fill up .,throw new ise ( e @$ __str__ ) ;,plumber.add swapping
assert that the total sockets quota has a reasonable <PLACE_HOLDER> .,assert true ( __str__ @$ ! open udp encap sockets . is empty ( ) ) ; assert true ( __str__ @$ open udp encap sockets . size ( ) < max_num_encap_sockets ) ;,quota has
this member has a base type that itself has multiple <PLACE_HOLDER> : we need to go deeper !,final type member tree node nested node = new type member tree node ( member ) ; member nodes . put ( member @$ nested node ) ; nested struct nodes . put ( member . get base type ( ) @$ nested node ) ; current node . add ( nested node ) ; create type nodes ( nested node @$ member . get base type ( ) ) ; break ; default : navi logger . warning ( __str__ @$ member . get base type ( ) . get category ( ) ) ; break ;,itself has
also non colored cumulatives have <PLACE_HOLDER> !,if ( m has colored cumulative ) { used color = new color wrap ( themes . get color from attr ( m chart view . get context ( ) @$ m colors [ i - __num__ ] ) ) ; } else { if ( m chart type == stats . chart type . intervals ) { name = m chart view . get resources ( ) . get string ( r . string . stats_cumulative_percentage ) ; } },cumulatives have
if the dto is null @$ it is an indication that the user does not have <PLACE_HOLDER> . however @$ we do n't want to just throw an access denied exception because we would rather ensure that all of the appropriate actions are taken by the pluggable authorizer . as a result @$ we attempt to find the component as a processor and fall,if ( dto == null ) { authorizable authorizable ; try { authorizable = lookup . get processor ( entity . get id ( ) ) . get authorizable ( ) ; } catch ( final resource not found exception rnfe ) { authorizable = lookup . get controller service ( entity . get id ( ) ) . get authorizable ( ) ; } if ( require read ) { authorizable . authorize ( authorizer @$ request action . read @$ user ) ; } if ( require write ) { authorizable . authorize ( authorizer @$ request action . write @$ user ) ; } } else if ( affected componentdto . component_type_processor . equals ( dto . get reference type ( ) ) ) {,user have
if there are remote deps @$ block on them fragment <PLACE_HOLDER>s indicating failure will throw an exception which will propagate out of handle received frag <PLACE_HOLDER> and cause procedure runner to do the right thing and cause rollback .,while ( ! check done receiving frag responses ( ) ) { fragment response message msg = poll for responses ( ) ; if ( trace log != null ) { final int batch idx = m_remote work . get current batch index ( ) ; trace log . add ( ( ) -> volt trace . end async ( __str__ @$ misc utils . hs id pair txn id to string ( m_mbox . geths id ( ) @$ msg . m_sourcehs id @$ txn id @$ batch idx ) @$ __str__ @$ byte . to string ( msg . get status code ( ) ) ) ) ; } boolean expected msg = handle received frag response ( msg ) ; if ( expected msg ),block received
else the initial subsystem install is not complete ; messaging subsystem add will add a handler that calls add queue <PLACE_HOLDER>,sb . install ( ) ;,calls add
for distinct count @$ because multiple groups have same <PLACE_HOLDER> @$ so there is no guarantee on the order of groups @$ just check the <PLACE_HOLDER>,group by result distinct count group by result = distinct count group by result list . get ( i ) ; assert . assert equals ( distinct count group by result . get value ( ) @$ expected group index / ( num_groups / max_size_of_set ) + __num__ @$ error_message ) ;,groups have
dom level <PLACE_HOLDER> : namespace uri is never empty string .,this . namespaceuri = namespaceuri ; if ( namespaceuri != null ) { this . namespaceuri = ( namespaceuri . length ( ) == __num__ ) ? null : namespaceuri ; } int colon1 = qname . index of ( __str__ ) ; int colon2 = qname . last index of ( __str__ ) ; owner document . check namespacewf ( qname @$ colon1 @$ colon2 ) ; if ( colon1 < __num__ ) { local name = qname ; if ( owner document . error checking ) { owner document . checkq name ( null @$ local name ) ; if ( qname . equals ( __str__ ) && ( namespaceuri == null || ! namespaceuri . equals ( namespace context . xmlns_uri ) ) || (,dom level
first call will return original <PLACE_HOLDER> in container @$ then second call will return a new one to simulate the key being replaced by a new one in a different thread,do return ( mock originalha event wrapper in container ) . do return ( mock newha event wrapper in container ) . when ( mockha container ) . get key ( mock remoteha event wrapper ) ; region queue . ha container = mockha container ; region queue . dec and remove fromha container ( mock remoteha event wrapper ) ; verify ( mockha container @$ times ( __num__ ) ) . remove ( mock originalha event wrapper in container ) ; verify ( mock originalha event wrapper in container @$ times ( __num__ ) ) . dec and get reference count ( ) ; verify ( mockha container @$ times ( __num__ ) ) . remove ( mock newha event wrapper in container ) ; verify (,call return
assume that if the get string threw this <PLACE_HOLDER> then the column is not representable by a string @$ e.g . it is a blob .,value = __str__ ;,string threw
disable quota checks <PLACE_HOLDER> in standby .,dir . disable quota checks ( ) ; edit log tailer = new edit log tailer ( this @$ conf ) ; edit log tailer . start ( ) ; if ( ! is observer && standby should checkpoint ) { standby checkpointer = new standby checkpointer ( conf @$ this ) ; standby checkpointer . start ( ) ; },quota checks
inspect contained <PLACE_HOLDER> to see if we need to migrate extras . we do n't promote clip data to the parent @$ since chooser activity will already start the picked item as the caller @$ and we ca n't combine the flags in a safe way .,if ( action_chooser . equals ( action ) ) { boolean migrated = false ; try { final intent intent = get parcelable extra ( extra_intent ) ; if ( intent != null ) { migrated |= intent . migrate extra stream to clip data ( ) ; } } catch ( class cast exception e ) { } try { final parcelable [ ] intents = get parcelable array extra ( extra_initial_intents ) ; if ( intents != null ) { for ( int i = __num__ ; i < intents . length ; i ++ ) { final intent intent = ( intent ) intents [ i ] ; if ( intent != null ) { migrated |= intent . migrate extra stream to clip data,inspect contained
the iterator already returned it 's load next batch <PLACE_HOLDER> @$ we must complete it exceptionally,current page . complete exceptionally ( t ) ; return ;,iterator returned
the root object can omit the surrounding <PLACE_HOLDER> . this token should be the first field 's key @$ or part of it @$ so put it back .,put back ( t ) ; missing curly = true ; result = parse object ( false ) ;,object omit
internally @$ the code does n't permit nullable <PLACE_HOLDER>s @$ so we lazily initialize dummy instances if the developer did n't supply a real <PLACE_HOLDER> .,m on drag initiated listener = ( m on drag initiated listener != null ) ? m on drag initiated listener : new on drag initiated listener ( ) { @ override public boolean on drag initiated ( @ non null motion event e ) { return false ; } } ; m on item activated listener = ( m on item activated listener != null ) ? m on item activated listener : new on item activated listener < k > ( ) { @ override public boolean on item activated ( @ non null item details lookup . item details < k > item @$ @ non null motion event e ) { return false ; } } ; m on context click listener = (,developer supply
stream buffer size <PLACE_HOLDER> in model .,igfs cfg props . add ( __str__ ) ;,buffer size
remove the stack trace so we do not pollute the build <PLACE_HOLDER> .,return exceptions . clear trace ( new file service exception ( ) ) ;,the build
if the list accepts the key events and the key event was a click @$ the text view gets the selected <PLACE_HOLDER> from the drop down as its content,switch ( key code ) { case key event . keycode_enter : case key event . keycode_dpad_center : case key event . keycode_tab : if ( event . has no modifiers ( ) ) { perform completion ( ) ; } return true ; },view gets
now scheduler health records last <PLACE_HOLDER> allocated @$ aggregated allocation account will not be changed,assert . assert equals ( __num__ @$ sh . get allocation count ( ) . long value ( ) ) ; assert . assert equals ( resource . new instance ( __num__ * __num__ @$ __num__ ) @$ sh . get resources allocated ( ) ) ; assert . assert equals ( __num__ @$ sh . get aggregate allocation count ( ) . long value ( ) ) ; assert . assert equals ( __str__ @$ sh . get last allocation details ( ) . get node id ( ) . to string ( ) ) ; assert . assert equals ( __str__ @$ sh . get last allocation details ( ) . get queue ( ) ) ; task task_0_2 = new task ( application_0 @$ priority_0,health records
exception could occur if a symbol element is missing an important <PLACE_HOLDER> such as address or length,string message = e . get message ( ) ; if ( message == null ) { message = e . get class ( ) . get simple name ( ) ; } message = __str__ + message ; msg . error ( this @$ message @$ e ) ; throw new io exception ( message @$ e ) ;,element missing
a list of all available logback log <PLACE_HOLDER> .,final list < string > log levels = arrays . as list ( level . all . to string ( ) @$ level . off . to string ( ) @$ level . error . to string ( ) @$ level . warn . to string ( ) @$ level . info . to string ( ) @$ level . debug . to string ( ) @$ level . trace . to string ( ) ) ; final argument parser parser = argument parsers . new argument parser ( __str__ ) . default help ( true ) . description ( __str__ + __str__ ) ; parser . add argument ( __str__ @$ __str__ ) . type ( string . class ) . nargs ( __str__ ) . required,list log
make sure the env supports bulk <PLACE_HOLDER> with generated ids ...,if ( ! supports bulk insert id generation ( integer versioned . class ) ) { skip log . report skip ( __str__ @$ __str__ ) ; return ; } session s = open session ( ) ; transaction t = s . begin transaction ( ) ; integer versioned entity = new integer versioned ( __str__ ) ; s . save ( entity ) ; s . create query ( __str__ ) . list ( ) ; t . commit ( ) ; s . close ( ) ; long initial id = entity . get id ( ) ; int initial version = entity . get version ( ) ; s = open session ( ) ; t = s . begin transaction ( ) ;,env supports
no costs known . use the same <PLACE_HOLDER> as above on the heuristic costs,final long sampled = ( long ) ( heuristic_cost_base * __num__ ) ; costs . add heuristic network cost ( heuristic_cost_base + sampled ) ; costs . add heuristic disk cost ( __num__ * sampled ) ;,costs use
single interface enabled <PLACE_HOLDER> .,string single interface enabled prop = util activator . get resources ( ) . get settings string ( single_window_interface_enabled ) ; boolean is enabled = false ; if ( single interface enabled prop != null ) is enabled = boolean . parse boolean ( single interface enabled prop ) ; else is enabled = boolean . parse boolean ( util activator . get resources ( ) . get settings string ( __str__ ) ) ;,interface enabled
verify that a second call will only return <PLACE_HOLDER> .,m callback . clear ( ) ; final long [ ] [ ] new times1 = increase time ( times ) ; write to file ( m headline + uid lines ( m uids @$ new times1 ) ) ; m reader . read delta ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; ++ i ) { m callback . verify ( m uids [ i ] @$ get active time ( new times1 [ i ] ) - get active time ( times [ i ] ) ) ; } m callback . verify no more interactions ( ) ;,call return
add all the <PLACE_HOLDER>s with proper context add first <PLACE_HOLDER>,outputqueue = chain . create blocking queue ( ) ; chain . add mapper ( context @$ outputqueue @$ __num__ ) ;,mappers add
safe to delegate since this reader does not alter the <PLACE_HOLDER>,return in . get reader cache helper ( ) ;,reader alter
the text lines total height is larger than this view @$ snap <PLACE_HOLDER> to the top and bottom of the view .,if ( line1 height + line2 height > height ) { if ( child != m text line1 ) { vertical offset = height - line2 height ; } } else { vertical offset = ( height - line1 height - line2 height ) / __num__ ; if ( child == m text line2 ) { vertical offset += line1 height ; if ( m suggestion . has answer ( ) && m suggestion . get answer ( ) . get second line ( ) . has image ( ) ) { vertical offset += get resources ( ) . get dimension pixel offset ( r . dimen . omnibox_suggestion_answer_line2_vertical_spacing ) ; } } if ( line1 height != line2 height ) { vertical offset += ( line2,lines snap
current dest has no distinct <PLACE_HOLDER> .,if ( current distinct keys . is empty ( ) ) { list < expr node desc > combined list = combine expr node lists ( target spray keys @$ target distinct keys ) ; if ( ! match expr lists ( combined list @$ current spray keys ) ) { continue ; } } else { if ( target distinct keys . is empty ( ) ) { list < expr node desc > combined list = combine expr node lists ( current spray keys @$ current distinct keys ) ; if ( ! match expr lists ( combined list @$ target spray keys ) ) { continue ; } else { new distinct key lists . remove ( i ) ; new spray key lists .,dest has
uri may contain <PLACE_HOLDER> @$ so make sure we check it too by converting ours @$ if necessary,string to match host = to match . get host ( ) ;,uri contain
regis<PLACE_HOLDER>ra<PLACE_HOLDER>ion requires <PLACE_HOLDER>ha<PLACE_HOLDER> sub<PLACE_HOLDER>ype ex<PLACE_HOLDER>ends <PLACE_HOLDER>,@ suppress warnings ( __str__ ) type adapter < r > delegate = ( type adapter < r > ) subtype to delegate . get ( src type ) ;,subtype extends
we set 0 this time as the last updated : can happen <PLACE_HOLDER> . when we use an old dynamo table,pm . set last updated ( __num__ ) ; metadata store ms = mock ( metadata store . class ) ; when ( ms . get ( path @$ false ) ) . then return ( pm ) ; i ttl time provider time provider = mock ( i ttl time provider . class ) ; when ( time provider . get now ( ) ) . then return ( __num__ ) ; when ( time provider . get metadata ttl ( ) ) . then return ( __num__ ) ;,updated happen
the classic translator does not know this <PLACE_HOLDER>,return null ;,translator know
ok then @$ let 's collect the whole <PLACE_HOLDER> ; name and value,if ( unknown == null ) { unknown = token buffer . for input buffering ( p @$ ctxt ) ; } unknown . write field name ( prop name ) ; unknown . copy current structure ( p ) ;,"s" collect
we called close on the underlying socket above to make doubly sure all resources got released . we do n't finalize self in the case of overlain sockets @$ that 's a different object <PLACE_HOLDER> the gc will finalize separately .,super . finalize ( ) ;,gc finalize
no ties top <PLACE_HOLDER>,arrays . fill ( hits @$ __num__ ) ; actual_label = __num__ ; pred_dist = new double [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ; update hits ( __num__ @$ actual_label @$ pred_dist @$ hits ) ; assert . assert true ( arrays . equals ( hits @$ new double [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) ) ;,ties top
packing saves <PLACE_HOLDER>,long test packed date time = packed local date time . pack ( test date ) ;,packing saves
assert that the tab layout did not update and add the new <PLACE_HOLDER>,assert not equals ( __str__ @$ new item count @$ tab layout . get tab count ( ) ) ;,layout update
checks that a <PLACE_HOLDER>et contains a primary key @$ except when it contains only public <PLACE_HOLDER> .,if ( ! has primary key && ! contains only public key material ) { throw new general security exception ( __str__ ) ; },checks contains
list result may contain continuation <PLACE_HOLDER>,if ( ( answer . status != ldap client . ldap_success ) || ( answer . referrals != null ) ) { process return code ( answer @$ name ) ; } return new ldap naming enumeration ( this @$ answer @$ name @$ cont ) ;,result contain
motion events get the raw float <PLACE_HOLDER> :,final float x = is horizontal ? m requested drag distance : __num__ ; final float y = is horizontal ? __num__ : m requested drag distance ; m recycler view . scroll by ( offsetx @$ offsety ) ; add fake motion event ( time @$ motion event . action_move @$ x @$ y ) ; return true ;,events get
computing the canonical path of a win 32 file should expose the true <PLACE_HOLDER> of filenames @$ rather than just using the input <PLACE_HOLDER>,file y = new file ( user dir @$ f . get path ( ) ) ; string ans = y . get path ( ) ; check ( ans @$ __str__ ) ; check ( ans @$ __str__ ) ; check ( ans @$ __str__ ) ;,file expose
now reptable has 1000 <PLACE_HOLDER> and parttable has 126 <PLACE_HOLDER>,save tables with path ( client @$ testnonce @$ snapshots path ) ;,reptable has
if a topic was not pulled in the previous run @$ use this.avg est avg millis as the estimated avg time to pull a <PLACE_HOLDER> in this run @$ which is the geometric mean of all topics whose avg times to pull a <PLACE_HOLDER> in the previous run are known .,this . avg est avg millis = geometric mean ( all est avg millis ) ;,times pull
find all the methods with the specific name and which accept just 1 <PLACE_HOLDER> .,if ( methods == null || methods . is empty ( ) ) { methods = class reflection index util . find all methods ( reflection index @$ class index @$ name @$ __num__ ) ; },which accept
sometimes the former <PLACE_HOLDER> and the newer <PLACE_HOLDER> are execute in same millisecond which causes the later <PLACE_HOLDER> to fail with obsolete version exception . add 5 ms sleep .,thread . sleep ( __num__ ) ;,which causes
initial method of data file <PLACE_HOLDER>,set property ( hsqldb_nio_data_file @$ true ) ;,method file
code browser expands <PLACE_HOLDER> to the code unit .,expected addresses . add ( addr ( __num__ ) @$ addr ( __num__ ) ) ; expected addresses . add ( addr ( __num__ ) @$ addr ( __num__ ) ) ; program selection expected selection = new program selection ( expected addresses ) ; perform action ( select all flows from action @$ get action context ( ) @$ true ) ; program selection current selection = code browser plugin . get current selection ( ) ; assert equals ( new my selection ( expected selection ) @$ new my selection ( current selection ) ) ;,browser expands
table has <PLACE_HOLDER>,tbl name = __str__ ; table_params = new string builder ( ) ; table_params . append ( capabilities_key ) . append ( __str__ ) . append ( __str__ ) ; t props . put ( __str__ @$ tbl name ) ; t props . put ( __str__ @$ table_params . to string ( ) ) ; try { table = create table with capabilities ( t props ) ; log . info ( __str__ ) ; } catch ( exception e ) { log . info ( __str__ ) ; fail ( __str__ + e . get message ( ) + __str__ ) ; },table has
instrumentation can kill and relaunch even persistent <PLACE_HOLDER>,force stop package locked ( ii . target package @$ - __num__ @$ true @$ false @$ true @$ true @$ false @$ user id @$ __str__ ) ;,instrumentation kill
decode gif pixel <PLACE_HOLDER> .,datum = bits = count = first = top = pi = bi = __num__ ; for ( i = __num__ ; i < npix ; ) { if ( top == __num__ ) { if ( bits < code_size ) { if ( count == __num__ ) { count = read block ( ) ; if ( count <= __num__ ) { break ; } bi = __num__ ; } datum += ( ( ( int ) block [ bi ] ) & __num__ ) << bits ; bits += __num__ ; bi ++ ; count -- ; continue ; } code = datum & code_mask ; datum >>= code_size ; bits -= code_size ; if ( ( code > available ) || ( code == end_of_information,decode gif
retrieve the replication configuration and verify that the configuration matches the <PLACE_HOLDER> we just set .,bucket replication configuration replication config = s3 client . get bucket replication configuration ( source bucket name ) ; replication rule rule = replication config . get rule ( __str__ ) ; system . out . println ( __str__ + rule . get destination config ( ) . get bucketarn ( ) ) ; system . out . println ( __str__ + rule . get priority ( ) ) ; system . out . println ( __str__ + rule . get status ( ) ) ;,configuration matches
we found counter objects which imply <PLACE_HOLDER>,if ( ! verify unexpected values ( counters ) ) { success = false ; },which imply
we need to make a copy here since get previous work unit <PLACE_HOLDER> returns immutable work unit <PLACE_HOLDER> for which add all is not supported,if ( state . get prop as boolean ( configuration keys . overwrite_configs_in_statestore @$ configuration keys . default_overwrite_configs_in_statestore ) ) { work unit state work unit state copy = new work unit state ( work unit state . get workunit ( ) @$ state ) ; work unit state copy . add all ( work unit state ) ; work unit state copy . override with ( state ) ; previous work unit states . add ( work unit state copy ) ; } else { previous work unit states . add ( work unit state ) ; },states returns
for a list @$ the client can either supply an <PLACE_HOLDER> of items or an adapter or a cursor,if ( ( m items != null ) || ( m cursor != null ) || ( m adapter != null ) ) { create list view ( dialog ) ; } if ( m view != null ) { if ( m view spacing specified ) { dialog . set view ( m view @$ m view spacing left @$ m view spacing top @$ m view spacing right @$ m view spacing bottom ) ; } else { dialog . set view ( m view ) ; } } else if ( m view layout res id != __num__ ) { dialog . set view ( m view layout res id ) ; },client supply
read message length will throw stream corrupted <PLACE_HOLDER> if the marker bytes incorrect,int msg size = tcp transport . read message length ( new bytes array ( minimal header ) ) ; if ( msg size == - __num__ ) { socket . get output stream ( ) . flush ( ) ; } else { final byte [ ] buffer = new byte [ msg size ] ; input . read fully ( buffer ) ; int expected size = tcp header . marker_bytes_size + tcp header . message_length_size + msg size ; try ( bytes stream output output = new releasable bytes stream output ( expected size @$ big arrays ) ) { output . write ( minimal header ) ; output . write ( buffer ) ; consume network reads ( mock channel @$ output . bytes,length throw
<PLACE_HOLDER> new <PLACE_HOLDER> never returns <PLACE_HOLDER> null <PLACE_HOLDER> .,result = new byte [ len ] ;,` returns
although this is the same size as the <PLACE_HOLDER> that is likely already loaded @$ the lifecycle is different and interactions are on a different thread . thus to simplify @$ this source will decode its own <PLACE_HOLDER> .,bitmap preview = decode preview ( source @$ preview size ) ; if ( preview . get width ( ) <= gl_size_limit && preview . get height ( ) <= gl_size_limit ) { m preview = new bitmap texture ( preview ) ; } else { log . w ( tag @$ string . format ( __str__ + __str__ @$ m width @$ m height @$ preview . get width ( ) @$ preview . get height ( ) ) ) ; },source decode
check that iterator always contains all the <PLACE_HOLDER> .,try { int iter cnt = __num__ ; for ( int i = __num__ ; i < iter cnt ; i ++ ) { collection < integer > cp = new hash set < > ( original ) ; cp . remove all ( linked map . key set ( ) ) ; assert true ( __str__ + cp @$ cp . is empty ( ) ) ; } info ( __str__ + ( system . current time millis ( ) - start ) + __str__ ) ; } finally { run . set ( false ) ; fut . get ( ) ; },iterator contains
if the app is honeycomb mr 1 or earlier @$ switch its async task <PLACE_HOLDER> to use the pool executor . normally @$ we use the serialized executor as the default . this has to happen in the main thread so the main looper is set right .,if ( data . app info . target sdk version <= android . os . build . version_codes . honeycomb_mr1 ) { async task . set default executor ( async task . thread_pool_executor ) ; },executor switch
only has 10 <PLACE_HOLDER> .,buffered writer . write ( string . format ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; buffered writer . write ( string . format ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; buffered writer . close ( ) ;,only has
mini hs 2 cluster is up .. let it run until someone kills the <PLACE_HOLDER>,while ( true ) { thread . sleep ( __num__ ) ; },someone kills
flags @$ currently we never calculate the crc and if we dont calculate them cant keep orig <PLACE_HOLDER> . tags are not experimental and we never create extended header to keep things simple .,extended = false ; experimental = false ; footer = false ;,them keep
this means that the embedded finder could not parse a <PLACE_HOLDER> .,if ( version == null ) { return null ; },finder parse
this check prevent <PLACE_HOLDER> when removal already happened during finishing animation,if ( j < additions . size ( ) ) { additions . remove ( j ) ; } if ( additions . is empty ( ) ) { m additions list . remove ( additions ) ; },check prevent
test that it emits when time passed the time <PLACE_HOLDER>,consumer . accept ( value in global window ( new byte [ __num__ ] ) ) ;,time passed
calendar should have undefined <PLACE_HOLDER>,assert true ( calendar . get width ( ) < __num__ ) ; assert true ( calendar . get height ( ) < __num__ ) ;,calendar have
the container for this dialog takes up the entire <PLACE_HOLDER> . as a result @$ need to manually listen for clicks and dismiss the dialog when necessary .,window . find view by id ( r . id . container ) . set on click listener ( v -> handle touch outside ( ) ) ; initialize title ( ) ; initialize body text ( ) ; initialize list ( ) ; initialize buttons ( ) ;,container takes
the main thread 's context class <PLACE_HOLDER> is set to the current thread 's context class <PLACE_HOLDER> which is a native image class <PLACE_HOLDER> . the class <PLACE_HOLDER> feature object replacer will unwrap the original app class <PLACE_HOLDER> from the native image class <PLACE_HOLDER> .,main thread = new thread ( main group @$ __str__ ) ; main thread . set daemon ( false ) ;,replacer unwrap
<PLACE_HOLDER> and private classes must share <PLACE_HOLDER> to be visible to each other,if ( ! objects . equals ( root invoker class . get package ( ) @$ root target class . get package ( ) ) ) { return false ; },package share
model name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( camelized name ) ) { final string model name = __str__ + camelized name ; logger . warn ( camelized name + __str__ + model name ) ; return model name ; },name use
mark unterminated line in case fill buf throws eof <PLACE_HOLDER> or io <PLACE_HOLDER> .,end = - __num__ ; fill buf ( ) ;,line throws
verify set <PLACE_HOLDER> on the stores as well .,for ( quota type quota type : new quota type [ ] { quota type . storage_space @$ quota type . get_throughput } ) { long new quota = __num__ + new random ( ) . next int ( __num__ ) ; admin client . quota mgmt ops . set quota ( new store name @$ quota type @$ new quota @$ service ) ; for ( integer node id : cluster . get node ids ( ) ) { long retrieved quota = get quota for node ( new store name @$ quota type @$ node id ) ; assert equals ( __str__ @$ new quota @$ retrieved quota . long value ( ) ) ; } },verify set
the variable reference violates a declared module <PLACE_HOLDER> .,if ( module graph . depends on ( var module @$ curr module ) ) { t . report ( n @$ violated_module_dep_error @$ curr module . get name ( ) @$ var module . get name ( ) @$ var name ) ; } else { t . report ( n @$ missing_module_dep_error @$ curr module . get name ( ) @$ var module . get name ( ) @$ var name ) ; },reference violates
values defined in contacts contract only affect very old <PLACE_HOLDER> of quick contacts .,intent intent = compose quick contacts intent ( context @$ target @$ lookup uri @$ mode_default @$ exclude mimes ) ; intent . put extra ( extra_prioritized_mimetype @$ prioritized mime type ) ; contacts internal . start quick contact with error toast ( context @$ intent ) ;,values affect
calculate max number of indices this class will add to the dex file . the possibility of overloading means that we ca n't easily know how many constant are needed for declared methods and fields . we therefore make the simplifying <PLACE_HOLDER> that all constants are external method or field references .,if ( args . multi dex ) { int constant pool size = cf . get constant pool ( ) . size ( ) ; max method ids in class = constant pool size + cf . get methods ( ) . size ( ) + max_method_added_during_dex_creation ; max field ids in class = constant pool size + cf . get fields ( ) . size ( ) + max_field_added_during_dex_creation ; synchronized ( dex rotation lock ) { int num method ids ; int num field ids ; synchronized ( output dex ) { num method ids = output dex . get method ids ( ) . items ( ) . size ( ) ; num field ids = output dex . get field ids ( ) .,number make
if the version is 0 then we are upgrading from a file format that did not know about periodic syncs . in that case do n't clear the <PLACE_HOLDER> since we want the default @$ which is a daily periodic sync . otherwise clear out this default <PLACE_HOLDER> since we will populate it later with the periodic sync descriptions that are read from the,if ( version > __num__ ) { authority . periodic syncs . clear ( ) ; } event log . write event ( __num__ @$ __str__ @$ - __num__ @$ __str__ + info . account + __str__ + authority name + __str__ + user id ) ;,case clear
release app 2 's am <PLACE_HOLDER> on node 2 .,scheduler . handle ( app removed event2 ) ; assert equals ( __str__ @$ __num__ @$ queue1 . get am resource usage ( ) . get memory size ( ) ) ; scheduler . update ( ) ;,release app
the track reselection did n't affect any <PLACE_HOLDER> that has been read .,if ( period holder == reading period holder ) { selections changed for read period = false ; },reselection affect
calculate the current radius at which to place the selection <PLACE_HOLDER> .,m line length = ( int ) ( m circle radius * m numbers radius multiplier * m animation radius multiplier ) ; int pointx = mx center + ( int ) ( m line length * math . sin ( m selection radians ) ) ; int pointy = my center - ( int ) ( m line length * math . cos ( m selection radians ) ) ;,which place
do not check the port number 0 because a user may want his or her <PLACE_HOLDER> to be bound on multiple arbitrary ports .,if ( p . local address ( ) . get port ( ) > __num__ ) { for ( int i = __num__ ; i < distinct ports . size ( ) ; i ++ ) { final server port port = distinct ports . get ( i ) ; if ( port . local address ( ) . equals ( p . local address ( ) ) ) { final server port merged = new server port ( port . local address ( ) @$ sets . union ( port . protocols ( ) @$ p . protocols ( ) ) ) ; distinct ports . set ( i @$ merged ) ; found = true ; break ; } } } if ( ! found ),user want
need to breakdown commandline into parts @$ as spaces in command line will cause <PLACE_HOLDER> .,list < string > exec commands = split and unescape command line ( cmd line ) ; system . out . printf ( __str__ @$ cmd line ) ; system . out . printf ( __str__ @$ jetty home dir . get absolute path ( ) ) ; pb cmd = new process builder ( exec commands ) ; pid = pb cmd . start ( ) ; console parser parser = new console parser ( ) ; list < string [ ] > jmx list = parser . new pattern ( __str__ @$ __num__ ) ; list < string [ ] > conn list = parser . new pattern ( __str__ @$ __num__ ) ;,spaces cause
expect errors . only create <PLACE_HOLDER> of two necessary configurability rules :,scratch . file ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ; write hello rules ( true ) ;,errors create
for ssl the exception may not come since the server can close <PLACE_HOLDER> before handshake message is sent from client . however exception should come in any region operations .,if ( gen . class code ( ) . equals ( credential generator . class code . ssl ) ) { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ no_exception ) ) ; client2 . invoke ( ( ) -> do puts ( __num__ @$ other_exception ) ) ; } else { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ authreq_exception ) ) ; },server close
record the sequence of seeks and reads which trigger a <PLACE_HOLDER> .,int [ ] seeks = new int [ __num__ ] ; int [ ] reads = new int [ __num__ ] ; try ( fs data input stream stm = get file system ( ) . open ( random seek file ) ) { for ( int i = __num__ ; i < limit ; i ++ ) { int seek off = r . next int ( buf . length ) ; int to read = r . next int ( math . min ( buf . length - seek off @$ __num__ ) ) ; seeks [ i % seeks . length ] = seek off ; reads [ i % reads . length ] = to read ; verify read ( stm @$ buf @$,which trigger
source produces some different <PLACE_HOLDER> .,if ( testing mode . equals ( mode_source_based ) ) { return ; },source produces
the activity that we are finishing may be over the lock screen . in this case @$ we do not want to consider activities that can not be shown on the lock screen as running and should proceed with finishing the activity if there is no valid next top running activity . note that if this finishing activity is floating <PLACE_HOLDER> @$ we do,final activity display display = get display ( ) ; final activity record next = display . top running activity ( true ) ;,activity floating
shared libraries get a null <PLACE_HOLDER> : this has the side effect of having canonicalized shared libraries using application loaders cache @$ which is the behavior we want .,return application loaders . get default ( ) . get shared library class loader with shared libraries ( jars @$ m application info . target sdk version @$ is bundled app @$ library search path @$ library permitted path @$ null @$ null @$ shared libraries ) ;,libraries get
my sql connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support
let 's actually create <PLACE_HOLDER> on non affinity server .,srv3 . cache ( query utils . create table cache name ( query utils . dflt_schema @$ __str__ ) ) ;,let create
parent name only makes <PLACE_HOLDER> when there is a single obvious parent,if ( refed parent names . size ( ) == __num__ ) { logger . warn ( __str__ + __str__ @$ composed schema . get name ( ) ) ; return refed parent names . get ( __num__ ) ; } return null ;,name makes
cleanup flavor c : the user does not want any buffered <PLACE_HOLDER> to persist between panes .,if ( should discard ) { reduce fn . clear state ( renamed context ) ; },user want
template location . this is the location which templates will be read from . the generator will use the resource <PLACE_HOLDER> to attempt to read the templates .,embedded template dir = template dir = template directory ;,generator use
frequency with which to collect <PLACE_HOLDER> for later plotting,int plot everyn minibatches = __num__ ;,which collect
this version gives a <PLACE_HOLDER> that rest li runs,request < void > req = builders . < void > action ( __str__ ) . build ( ) ; get client ( ) . send request ( req ) . get response ( ) ;,version gives
