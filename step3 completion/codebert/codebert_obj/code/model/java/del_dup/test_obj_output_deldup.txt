head requests should not set a content length by passing it through this api @$ but should instead manually set the required <PLACE_HOLDER> .,if ( is head request ( ) ) { if ( content len >= __num__ ) { final logger logger = server . get logger ( ) ; string msg = __str__ ; logger . warning ( msg ) ; } no content to send = true ; content len = __num__ ; } else { if ( content len == __num__ ) { if ( http10 ) { o . set wrapped stream ( new undef length output stream ( this @$ ros ) ) ; close = true ; } else { rsp hdrs . set ( __str__ @$ __str__ ) ; o . set wrapped stream ( new chunked output stream ( this @$ ros ) ) ; } } else { if ( content,requests set value
peer 2 advertises the <PLACE_HOLDER> but does not receive it yet .,inbound ( p2 @$ inv ) ; assert true ( outbound ( p2 ) instanceof get data message ) ; assert equals ( __num__ @$ tx . get confidence ( ) . num broadcast peers ( ) ) ; assert null ( event [ __num__ ] ) ;,peer advertises tx
when partition could not be fetched from metastore @$ it is not known whether the partition was added . deleting the partition when aborting commit has the <PLACE_HOLDER> of deleting partition not added in this transaction . not deleting the partition may leave garbage behind . the former is much more dangerous than the latter . therefore @$ the partition is not added to,batch completely added = false ;,commit has affect
if we have parents combine their <PLACE_HOLDER>,while ( parent node != null ) { if ( parent == current spatial ) { transform trans = new transform ( ) ; trans . set scale ( current spatial . get local scale ( ) ) ; shape transform . combine with parent ( trans ) ; parent node = null ; } else { shape transform . combine with parent ( current spatial . get local transform ( ) ) ; parent node = current spatial . get parent ( ) ; current spatial = parent node ; } },parents combine shapes
do n't allow partitioned tables in subqueries . this restriction stems from the lack of confidence that the planner can reliably identify all cases of adequate and inadequate partition key join <PLACE_HOLDER> across different levels of correlated subqueries .,if ( ! m_partitioning . was specified as single ( ) ) { for ( abstract expression e : subquery exprs ) { assert ( e instanceof select subquery expression ) ; select subquery expression sub expr = ( select subquery expression ) e ; if ( ! sub expr . get subquery scan ( ) . get is replicated ( ) ) { m_recent error msg = in_exists_scalar_error_message ; return null ; } } },cases join strategy
bn has a trailing <PLACE_HOLDER> : the web sphere class loader would return null for a raw directory name without it .,try { enumeration < url > urls = root . get resources ( bn ) ; if ( urls == null ) { return null ; } url visitor v = new url visitor ( ) { @ override public void visit ( string s ) { if ( s . ends with ( __str__ ) ) { string locstr = s . substring ( __num__ @$ s . length ( ) - __num__ ) ; names . add ( locstr ) ; } } } ; while ( urls . has more elements ( ) ) { url url = urls . next element ( ) ; url handler handler = url handler . get ( url ) ; if ( handler != null ) { handler,bn has slash
if this project has correlated <PLACE_HOLDER> @$ create value generator and produce the correlated variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel ) ; },project correlated reference
when visibility changes and the user has a <PLACE_HOLDER> selected @$ unselect it and make sure their callback gets called .,if ( changed view == this && visibility != visible && m grabbed state != on trigger listener . no_handle ) { cancel grab ( ) ; },changes has popup
match the container @$ to reduce the risk of issues . the preview should never be drawn while the surface has this <PLACE_HOLDER> .,if ( surface rect == null ) { surface view . layout ( __num__ @$ __num__ @$ get width ( ) @$ get height ( ) ) ; } else { surface view . layout ( surface rect . left @$ surface rect . top @$ surface rect . right @$ surface rect . bottom ) ; },surface has surface
a window frame may contain just the start <PLACE_HOLDER> or in the between style of expressing a window frame both boundaries are specified .,boundary spec start = process boundary ( ( ast node ) node . get child ( __num__ ) ) ; if ( node . get child count ( ) > __num__ ) { end = process boundary ( ( ast node ) node . get child ( __num__ ) ) ; },frame contain part
create a walker which walks the <PLACE_HOLDER> in a dfs manner while maintaining the operator stack .,map < rule @$ node processor > op rules = new linked hash map < rule @$ node processor > ( ) ; op rules . put ( new rule reg exp ( __str__ @$ app master event operator . get operator name ( ) + __str__ ) @$ new remove dynamic pruning by size ( ) ) ;,which walks tree
see if the avro schema has any fields that are n't in the record schema @$ and if those fields have a default <PLACE_HOLDER> then we want to populate it in the generic record being produced,for ( final field field : avro schema . get fields ( ) ) { final optional < record field > record field = record schema . get field ( field . name ( ) ) ; if ( ! record field . is present ( ) && rec . get ( field . name ( ) ) == null && field . default val ( ) != null ) { rec . put ( field . name ( ) @$ field . default val ( ) ) ; } } return rec ;,fields have value
zip files have 2 second <PLACE_HOLDER> .,cal . set ( calendar . second @$ rand . next int ( __num__ ) * __num__ ) ;,files have digits
we 'll never have two dots nor will a type name end or begin with dot . so no need to consider <PLACE_HOLDER> at the beginning @$ end @$ or adjacent to dots .,if ( index == __num__ || index == class name . length ( ) - __num__ || current . char at ( index - __num__ ) == __str__ || current . char at ( index + __num__ ) == __str__ ) { search start = index + __num__ ; continue ; } return index ;,need consider whitespace
when layout is frozen @$ rv does not intercept the motion event . a child view e.g . a button may still get the <PLACE_HOLDER> .,if ( m layout frozen ) { return false ; },button get click
if execute <PLACE_HOLDER>ong is used for the initia<PLACE_HOLDER> execution of the node and the uninitia<PLACE_HOLDER>ized case is not checked then this execute <PLACE_HOLDER>ong method might return 0 <PLACE_HOLDER> instead of 2 <PLACE_HOLDER>. this test verifies that this particu<PLACE_HOLDER>ar case does not happen .,assert . assert equals ( __num__ @$ node . execute long ( truffle . get runtime ( ) . create virtual frame ( new object [ ] { __num__ } @$ new frame descriptor ( ) ) ) ) ;,method return l
table may or may not be using <PLACE_HOLDER> . only the ser de can tell us .,abstract ser de deserializer = null ; try { class < ? > clazz = conf . get class by name ( serde lib ) ; if ( ! abstract ser de . class . is assignable from ( clazz ) ) { return true ; } deserializer = reflection util . new instance ( conf . get class by name ( serde lib ) . as subclass ( abstract ser de . class ) @$ conf ) ; } catch ( exception ex ) { log . warn ( __str__ + serde lib + __str__ @$ ex ) ; return true ; },table using synchronization
get base xml file result set <PLACE_HOLDER>,j2d analyzer . read results ( basexml file name ) ; j2d analyzer . single result set holder basesrsh = ( j2d analyzer . single result set holder ) j2d analyzer . results . element at ( __num__ ) ; enumeration base enum_ = basesrsh . get key enumeration ( ) ; vector base keyvector = new vector ( ) ; while ( base enum_ . has more elements ( ) ) { base keyvector . add ( base enum_ . next element ( ) ) ; } string base keys [ ] = new string [ base keyvector . size ( ) ] ; base keyvector . copy into ( base keys ) ; j2d analyzer . sort ( base keys ) ;,xml file list
client can still write <PLACE_HOLDER> even though server is closed ? ? ?,client . get output stream ( ) . write ( __num__ ) ;,client write data
gathering geometries in the sub graph . this must be done in the update phase as the gathering might add a matparam <PLACE_HOLDER>,targets . clear ( ) ; this . spatial . depth first traversal ( target locator ) ;,gathering add target
if the <PLACE_HOLDER> has no requested minimal size @$ we 'd like to enforce a minimal size so that the user can not render the <PLACE_HOLDER> too small to manipulate . we do n't need to do this for the pinned stack as the bounds are controlled by the system .,if ( ! in pinned windowing mode ( ) && m stack != null ) { final int default min size dp = m service . m root activity container . m default min size of resizeable task dp ; final activity display display = m service . m root activity container . get activity display ( m stack . m display id ) ; final float density = ( float ) display . get configuration ( ) . density dpi / display metrics . density_default ; final int default min size = ( int ) ( default min size dp * density ) ; if ( min width == invalid_min_size ) { min width = default min size ; } if ( min height == invalid_min_size ),user render task
set up an empty panel to be used as a gui place holder when a conflict in another merger already made a <PLACE_HOLDER> that resolved the conflict .,empty conflict panel = new vertical choices panel ( ) ; empty conflict panel . clear ( ) ;,conflict made change
only map owned ref and relationship <PLACE_HOLDER> when follow references is set to true,if ( entity with ext info == null ) { entity with ext info = entity graph retriever . to atlas entity with ext info ( guid @$ ! follow references ) ; if ( entity with ext info != null ) { context . cache ( entity with ext info ) ; if ( log . is debug enabled ( ) ) { log . debug ( __str__ @$ guid ) ; } } },map owned information
intern @$ as most entities have multiple surface forms add the <PLACE_HOLDER>,dictionary . put ( surface form @$ link ) ; i += __num__ ;,forms add link
test that the module map action contains the header tree <PLACE_HOLDER> as both the public header and part of the action inputs .,assert that ( module map action . get public headers ( ) ) . contains ( headers ) ; assert that ( module map action . get inputs ( ) ) . contains ( headers ) ; action execution context dummy action execution context = new action execution context ( null @$ null @$ action input prefetcher . none @$ action key context @$ null @$ null @$ null @$ immutable map . of ( ) @$ immutable map . of ( ) @$ dummy_artifact_expander @$ null @$ null ) ; byte array output stream module map stream = new byte array output stream ( ) ; byte array output stream umbrella header stream = new byte array output stream ( ) ; module map action . new,action contains object
blob got <PLACE_HOLDER>,assert equals ( constants . sqlite_blob @$ st . column_type ( __num__ ) ) ;,blob got interrupted
count timer fired <PLACE_HOLDER>,int timer fired count = __num__ ; list < flowable event > events received = listener . get events received ( ) ; for ( flowable event event received : events received ) { if ( flowable engine event type . timer_fired == event received . get type ( ) ) { timer fired count ++ ; } } listener . clear events received ( ) ; assert equals ( __num__ @$ timer fired count ) ; process engine configuration . reset clock ( ) ;,timer fired events
maven turns empty <PLACE_HOLDER> into null entries @$ and its possible to have empty <PLACE_HOLDER> in jib.to.<PLACE_HOLDER>,session properties . put ( __str__ @$ __str__ ) ; try { test plugin configuration . get target image additional tags ( ) ; assert . fail ( ) ; } catch ( illegal argument exception ex ) { assert . assert equals ( __str__ @$ ex . get message ( ) ) ; },maven turns tags
this is used to set data type @$ which defines a python <PLACE_HOLDER> of classes,string full suffix = suffix ; if ( __str__ . equals ( suffix ) ) { full suffix = __str__ + suffix ; } if ( model utils . is nullable ( p ) ) { full suffix = __str__ + suffix ; } if ( model utils . is free form object ( p ) && model utils . get additional properties ( p ) == null ) { return prefix + __str__ + full suffix ; } if ( ( model utils . is map schema ( p ) || p . get type ( ) == __str__ ) && model utils . get additional properties ( p ) != null ) { schema inner = model utils . get additional properties ( p ) ;,which defines list
the code is written so that even completely incorrect approximations will still yield the correct <PLACE_HOLDER> eventually @$ but in practice this branch should almost never be entered @$ and even then the loop should not run more than once .,if ( approx cmp > __num__ ) { do { approx log10 -- ; approx pow = approx pow . divide ( big integer . ten ) ; approx cmp = approx pow . compare to ( x ) ; } while ( approx cmp > __num__ ) ; } else { big integer next pow = big integer . ten . multiply ( approx pow ) ; int next cmp = next pow . compare to ( x ) ; while ( next cmp <= __num__ ) { approx log10 ++ ; approx pow = next pow ; approx cmp = next cmp ; next pow = big integer . ten . multiply ( approx pow ) ; next cmp = next pow . compare to (,approximations yield result
no awb modes <PLACE_HOLDER> ? that 's unpossible !,if ( awb avail == null || awb avail . size ( ) == __num__ ) { log . w ( tag @$ __str__ ) ; awb avail = new array list < integer > ( __num__ ) ; awb avail . add ( control_awb_mode_auto ) ; },awb modes supported
set the text to an invalid value @$ which will trigger an <PLACE_HOLDER> and an error message,set offset ( offset @$ __str__ ) ; check offset fields enabled state ( info fields @$ true ) ; check offset fields enabled state ( offset buttons @$ false ) ;,which trigger animation
if the table has property external <PLACE_HOLDER> @$ update table type accordingly,string table type = tbl . get table type ( ) ; boolean is external = boolean . parse boolean ( tbl . get parameters ( ) . get ( __str__ ) ) ; if ( table type . managed_table . to string ( ) . equals ( table type ) ) { if ( is external ) { table type = table type . external_table . to string ( ) ; } } if ( table type . external_table . to string ( ) . equals ( table type ) ) { if ( ! is external ) { table type = table type . managed_table . to string ( ) ; } } principal type owner principal type = tbl . get owner type ( ),table has value
first line : sentence <PLACE_HOLDER> for all nodes ; we recover the words from the original tokens the first two tokens in this line indicate : docid @$ sentence <PLACE_HOLDER>,for ( indexed word node : graph . vertex set ( ) ) { if ( ! output header ) { string doc id = node . get ( core annotations . docid annotation . class ) ; if ( doc id != null && doc id . length ( ) > __num__ ) pw . print ( doc id ) ; else pw . print ( __str__ ) ; pw . print ( __str__ ) ; pw . print ( node . get ( core annotations . sentence index annotation . class ) ) ; output header = true ; } pw . print ( __str__ ) ; pw . print ( node . index ( ) ) ; if ( node . copy count ( ),tokens indicate position
server has sent <PLACE_HOLDER> since last check,if ( last bytes written . compare and set ( previous bytes @$ current bytes ) ) { return false ; },server sent data
we are only running remotely if both the distribution is there and if the distribution is actually contains <PLACE_HOLDER> .,clustered partitioning = trans meta . get slave step copy partition distribution ( ) != null && ! trans meta . get slave step copy partition distribution ( ) . get distribution ( ) . is empty ( ) ;,distribution contains partitions
to change body of generated methods @$ choose tools | <PLACE_HOLDER> .,return super . load symlinks ( ) ;,tools | templates
composition needs one <PLACE_HOLDER> to be the container .,if ( relationship category == relationship category . composition ) { throw new atlas base exception ( atlas error code . relationshipdef_composition_no_container @$ name ) ; } else if ( relationship category == relationship category . aggregation ) { throw new atlas base exception ( atlas error code . relationshipdef_aggregation_no_container @$ name ) ; },composition needs end
thread polling every 5 seconds to update the <PLACE_HOLDER> set seconds which is used in filter <PLACE_HOLDER>s bolt to filter the <PLACE_HOLDER>s,try { if ( ! poll ) { word set = parse file ( file name ) ; poll time = system . current time millis ( ) ; poll = true ; } else { if ( ( system . current time millis ( ) - poll time ) > __num__ ) { word set = parse file ( file name ) ; poll time = system . current time millis ( ) ; } } } catch ( io exception exp ) { throw new runtime exception ( exp ) ; } if ( word set != null && ! word set . contains ( word ) ) { collector . emit ( new values ( word ) ) ; },seconds update message
rows to skip <PLACE_HOLDER>,label wl rows to skip = new label ( shell @$ swt . right ) ; wl rows to skip . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl rows to skip ) ; form data fdl rows to skip = new form data ( ) ; fdl rows to skip = new form data ( ) ; fdl rows to skip . left = new form attachment ( __num__ @$ __num__ ) ; fdl rows to skip . top = new form attachment ( last control @$ margin ) ; fdl rows to skip . right = new form attachment ( middle @$ - margin ) ; wl rows to skip . set layout,rows skip line
use the messages hostname as default domain when generating set <PLACE_HOLDER>,for ( http cookie c : msg . get response header ( ) . get http cookies ( msg . get request header ( ) . get host name ( ) ) ) session . get http state ( ) . add cookie ( convert cookie ( c ) ) ; return session ;,generating set cookies
original has changed @$ but snapshot still has old <PLACE_HOLDER> .,do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ; restart ( false ) ; do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ; restart ( true ) ; do snapshot contents removal assertions ( file path @$ file snapshot path @$ subdir path @$ subdir snapshot path ) ;,snapshot has acl
unix domain socket address . create the underlying <PLACE_HOLDER> for the unix domain socket .,if ( value . starts with ( unix_domain_socket_prefix ) ) { string file path = value . substring ( unix_domain_socket_prefix . length ( ) ) ; file file = new file ( file path ) ; if ( ! file . is absolute ( ) ) { throw new illegal argument exception ( __str__ + file path ) ; } try { if ( file . create new file ( ) ) { file . delete on exit ( ) ; } } catch ( io exception ex ) { throw new runtime exception ( ex ) ; } return new domain socket address ( file ) ; } else { string [ ] parts = value . split ( __str__ @$ __num__ ) ; if ( parts,address create file
new file @$ should preserve <PLACE_HOLDER>,string r = test col . get media ( ) . add file ( path ) ; assert equals ( __str__ @$ r ) ;,new preserve sequence
since b and c have the same <PLACE_HOLDER> @$ we can expect them to appear in either order,assertion . has table section ( ) . has column ( __str__ ) . contains exactly ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,b have columns
schedule low pri first . when high pri is scheduled @$ it takes away the <PLACE_HOLDER> from the low pri task . when the high pri finishes @$ low pri gets the <PLACE_HOLDER> back .,try { priority high pri = priority . new instance ( __num__ ) @$ low pri = priority . new instance ( __num__ ) ; tez task attemptid task1 = test task scheduler service wrapper . generate task attempt id ( ) @$ task2 = test task scheduler service wrapper . generate task attempt id ( ) ; ts wrapper . ts . update guaranteed count ( __num__ ) ; ts wrapper . control scheduler ( true ) ; ts wrapper . allocate task ( task1 @$ null @$ low pri @$ new object ( ) ) ; ts wrapper . await total task allocations ( __num__ ) ; task info ti1 = ts wrapper . ts . get task info ( task1 ) ; assert true (,pri gets attempt
now @$ fsck should show healthy fs and should not show any open <PLACE_HOLDER>,out str = run fsck ( conf @$ __num__ @$ true @$ top dir ) ; system . out . println ( out str ) ; assert true ( out str . contains ( namenode fsck . healthy_status ) ) ; assert false ( out str . contains ( __str__ ) ) ; assert false ( out str . contains ( __str__ ) ) ; util . cleanup ( fs @$ top dir ) ;,fsck show files
the full <PLACE_HOLDER> should not contain the organization folder <PLACE_HOLDER>,map links ; for ( map map : pipelines ) { assert . assert equals ( __str__ @$ map . get ( __str__ ) ) ; if ( map . get ( __str__ ) . equals ( __str__ ) ) { map . get ( __str__ ) . equals ( __str__ ) ; map . get ( __str__ ) . equals ( __str__ ) ; check links ( ( map ) map . get ( __str__ ) @$ __str__ ) ; } else if ( map . get ( __str__ ) . equals ( __str__ ) ) { map . get ( __str__ ) . equals ( __str__ ) ; map . get ( __str__ ) . equals ( __str__ ) ; check links ( ( map ),name contain map
distributed system.disconnect may have already destroyed the <PLACE_HOLDER>,distributed lock service . destroy ( abstract gateway sender . lock_service_name ) ;,system.disconnect destroyed dls
begin @$ do and if increases block <PLACE_HOLDER>,if ( ( __str__ . equals ( keyword . get text ( ) ) || __str__ . equals ( keyword . get text ( ) ) || __str__ . equals ( keyword . get text ( ) ) && ! __str__ . equals ( previous keyword . get text ( ) ) ) ) { context . increase block depth ( ) ; } else if ( __str__ . equals ( keyword . get text ( ) ) ) { context . decrease block depth ( ) ; },increases block depth
rvv does not contain this <PLACE_HOLDER> so it is retained,result . add ( id ) ;,rvv contain id
the added node adds a new <PLACE_HOLDER> .,return true ;,node adds node
nobody understood this <PLACE_HOLDER>,throw new web service exception ( __str__ + lexical ) ;,nobody understood exception
normalization will remove <PLACE_HOLDER> @$ so no uplevel references left,assert that ( create ( __str__ ) . contains uplevel references ( ) ) . is false ( ) ;,normalization remove everything
update the switches insode the <PLACE_HOLDER> 200 switch program service,e service . update errors ( node root ) ;,switches insode km
id types can use int <PLACE_HOLDER> .,if ( type == scalars . graphqlid && string value . matches ( __str__ ) ) { return int value . new int value ( ) . value ( new big integer ( string value ) ) . build ( ) ; },types use literals
without the metadata the status and health check ur ls will not be set and the status page and health check url paths will not include the context path so set <PLACE_HOLDER> here,if ( string utils . has text ( management context path ) ) { instance . set health check url path ( management context path + instance . get health check url path ( ) ) ; instance . set status page url path ( management context path + instance . get status page url path ( ) ) ; },here set them
quick answers requires the js <PLACE_HOLDER> .,return contextual search field trial . is quick answers enabled ( ) ;,answers requires doc
legacy security behavior : setup the security context if a sas current is available and invoke the component . one of the ejb security interceptors will authenticate and authorize the <PLACE_HOLDER> .,security context legacy context = null ; if ( this . legacy security domain != null && ( identity principal != null || principal != null ) ) { final object final credential = identity principal != null ? this . sas current : credential ; final principal final principal = identity principal != null ? identity principal : principal ; if ( wild fly security manager . is checking ( ) ) { legacy context = access controller . do privileged ( ( privileged exception action < security context > ) ( ) -> { security context sc = security context factory . create security context ( this . legacy security domain ) ; sc . get util ( ) . create subject info ( final principal,behavior authenticate authentication
case insense . should include root @$ html @$ <PLACE_HOLDER>,elements p2 = doc . select ( __str__ ) ;,case include document
note that this class does not support forward <PLACE_HOLDER> .,if ( ! forward ) { if ( trusted pub key != null ) { prev pub key = trusted pub key ; } else { prev pub key = null ; } } else { throw new cert path validator exception ( __str__ ) ; },class support digests
if the current wi is a quantity @$ we add it to the collector . if its the first word in a quantity @$ we record <PLACE_HOLDER> before it,if ( quantifiable . contains ( curr ner tag ) ) { if ( collector . is empty ( ) ) { before index = i - __num__ ; } collector . add ( wi ) ; } prev ner tag = curr ner tag ;,word record stuff
server and cluster has the same <PLACE_HOLDER>,cluster props . clear ( ) ; server props . clear ( ) ; cluster props . set property ( __str__ @$ __str__ ) ; server props . set property ( __str__ @$ __str__ ) ; assert that ( gem fire cache impl . is mis configured ( cluster props @$ server props @$ __str__ ) ) . is false ( ) ; cluster props . set property ( __str__ @$ __str__ ) ; server props . set property ( __str__ @$ __str__ ) ; assert that ( gem fire cache impl . is mis configured ( cluster props @$ server props @$ __str__ ) ) . is false ( ) ;,server has value
make sure the given union <PLACE_HOLDER> has a corresponding tuple <PLACE_HOLDER> in the schema .,int union tag = value . get union tag ( ) ; if ( schema . size ( ) <= union tag ) { throw new illegal state exception ( __str__ + union tag + __str__ ) ; } list < object > value list = ( list < object > ) value map . get ( union tag ) ; value list . add ( value . get value ( ) ) ;,tag has tag
end playback @$ as we did n't manage to find a valid seek <PLACE_HOLDER> .,if ( period position us == c . time_unset ) { set state ( player . state_ended ) ; reset internal ( false @$ false @$ true @$ false @$ true ) ; } else { long new period position us = period position us ; if ( period id . equals ( playback info . period id ) ) { media period holder playing period holder = queue . get playing period ( ) ; if ( playing period holder != null && playing period holder . prepared && new period position us != __num__ ) { new period position us = playing period holder . media period . get adjusted seek position us ( new period position us @$ seek parameters ) ; } if (,playback seek position
repl load during migration @$ commits the explicit <PLACE_HOLDER> and start some internal <PLACE_HOLDER>s . call release locks and commit or rollback to do the clean up .,if ( ! driver context . get txn manager ( ) . is txn open ( ) && driver context . get query state ( ) . get hive operation ( ) == hive operation . replload ) { release locks and commit or rollback ( false ) ; } else { },load commits txn
other task will now finish the process <PLACE_HOLDER>,org . flowable . task . api . task task = task service . create task query ( ) . process instance id ( pi . get id ( ) ) . task definition key ( __str__ ) . single result ( ) ; map < string @$ object > variables = new hash map < string @$ object > ( ) ; variables . put ( __str__ @$ __num__ ) ; task service . complete ( task . get id ( ) @$ variables ) ; assert equals ( __num__ @$ runtime service . create process instance query ( ) . process instance id ( pi . get id ( ) ) . count ( ) ) ;,task finish instance
a volt db <PLACE_HOLDER> to support indexed expressions and assume unique attribute .,c . set assume unique ( assume unique ) ; if ( has non column exprs ) { c = c . with expressions ( index exprs . to array ( new expression [ index exprs . size ( ) ] ) ) ; },volt db extension
magic numbers @$ if anybody knows why @$ please tell <PLACE_HOLDER>,datfiles . put ( dat file name @$ new byte [ ] { ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$,numbers tell others
if rm is complaining about response id out of sync @$ force reset next <PLACE_HOLDER>,if ( t instanceof invalid application master request exception ) { int response id = amrm client utils . parse expected response id from exception ( t . get message ( ) ) ; if ( response id != - __num__ ) { this . reset response id = response id ; log . info ( __str__ + response id + __str__ + allocate request . get response id ( ) + __str__ + this . app id + __str__ ) ; } else { log . warn ( __str__ + this . app id ) ; } } throw t ;,rm complaining time
special case @$ a collection with only a read <PLACE_HOLDER> we assume we can just add to the connection,if ( collection . class . is assignable from ( i . get property type ( ) ) ) { handled properties . add ( i . get name ( ) ) ; collection property value = ( collection ) i . read ( param ) ; if ( ! property value . is empty ( ) ) { list < deferred parameter > params = new array list < > ( ) ; for ( object c : property value ) { deferred parameter to add = load object instance ( c @$ existing @$ object . class ) ; params . add ( to add ) ; } setup steps . add ( new serialzation step ( ) { @ override public void handle ( method,case read method
some binary files have <PLACE_HOLDER>,while ( c != __str__ ) { if ( c != __str__ ) sb . append ( c ) ; c = ( char ) buffer . get ( ) ; },files have newline
after all class information has been read @$ now we can safely inspect import information for errors . if we did this before all parsing was finished @$ we could get vicious circularities @$ since files can import each others ' <PLACE_HOLDER> .,checked = - __num__ ;,files import classes
to find the existing room created with op set muc 1 @$ we will use op set muc <PLACE_HOLDER> to be sure the room will not be retrieved from op set muc 1 cache,try { found room = op setmuc2 . find room ( test room name ) ; } catch ( exception ex ) { logger . warn ( ex ) ; } assert not null ( __str__ @$ found room ) ; assert equals ( __str__ @$ op set1 room . get name ( ) @$ found room . get name ( ) ) ;,cache set 2
rename lambda <PLACE_HOLDER> to reflect the new owner . not doing so confuses lambda desugaring if it 's run over this class again . lambda desugaring has already renamed the <PLACE_HOLDER> from its original name to include the interface name at this point .,if ( is lambda ) { return name + dependency collector . interface_companion_suffix ; },lambda renamed method
this should typically never happen . cancelling work should remove alarms @$ but if an alarm has already fired @$ then fire a stop work request to remove the pending <PLACE_HOLDER> met command handler .,if ( work spec == null ) { stop work ( ) ; return ; },work remove state
for sake of simplicity transform any <PLACE_HOLDER> in array,if ( result instanceof iterable < ? > ) { final list < object > list = new array list < object > ( ) ; for ( object o : ( iterable < object > ) result ) { list . add ( o ) ; } result = list . to array ( ) ; },sake transform objects
is this a returning <PLACE_HOLDER> ?,if ( branch target finder . is subroutine ( offset ) && branch target finder . is subroutine returning ( offset ) ) { if ( debug ) { system . out . println ( __str__ + instruction . to string ( offset ) ) ; } code attribute composer . append label ( offset ) ; } else { instruction . accept ( clazz @$ method @$ code attribute @$ offset @$ this ) ; },a returning method
which will cause the main <PLACE_HOLDER> to recreate this fragment,( new sign out word press com async ( get activity ( ) ) ) . execute on executor ( async task . thread_pool_executor ) ;,which cause thread
check if send mono triggers all <PLACE_HOLDER> off,channel . note on ( __num__ @$ __num__ ) ; soft . read ( __num__ ) ; assert true ( soft . find voice ( __num__ @$ __num__ ) != null ) ; smsg . set message ( short message . control_change @$ __num__ @$ __num__ @$ __num__ ) ; receiver . send ( smsg @$ - __num__ ) ; soft . read ( __num__ ) ; assert true ( soft . find voice ( __num__ @$ __num__ ) == null ) ; soft . close ( ) ;,mono triggers notes
these colors should get <PLACE_HOLDER> .,intent intent = new custom tabs intent . builder ( ) . set toolbar color ( __num__ ) . set secondary toolbar color ( __num__ ) . set navigation bar color ( __num__ ) . set color scheme params ( color_scheme_light @$ light params ) . set color scheme params ( color_scheme_dark @$ dark params ) . build ( ) . intent ;,colors get updated
step 2 : optional . dynamic runtime property change option we would like to utilize the benefits of obtaining dynamic property changes initialize the dynamic property <PLACE_HOLDER> with our configuration source,dynamic property factory . init with configuration source ( my configuration ) ;,changes initialize factory
if we 're at this point in the method 's execution @$ we could n't reconstitute the original hash . so @$ we need to hash the submitted plaintext using current hash service configuration and then compare the formatted <PLACE_HOLDER> with the saved string . this will correctly compare passwords @$ but does not allow changing the hash service configuration without breaking previously saved,return passwords match ( submitted plaintext @$ saved hash ) ;,then compare result
if filter.g does date <PLACE_HOLDER> for quoted strings @$ we 'd need to verify there 's no type mismatch when string col is filtered by a string that looks like date .,if ( col type == filter type . date && val type == filter type . string ) { try { node value = meta store utils . partition_date_format . get ( ) . parse ( ( string ) node value ) ; val type = filter type . date ; } catch ( parse exception pe ) { } },filter.g does validation
make sure no function at current and new location ca n't change <PLACE_HOLDER> if it is there .,if ( is func ) { function manager . remove function ( mem addr ) ; function manager . remove function ( new addr ) ; },function change primary
prev state <PLACE_HOLDER> state expected global counter exptected <PLACE_HOLDER> proc state seq expected block state expect notify,verify seq counter and interactions ( uid rec @$ process_state_foreground_service @$ process_state_service @$ __num__ @$ __num__ @$ network_state_unblock @$ true ) ;,state expected cur
if <PLACE_HOLDER> of the bits have been cleared in copy @$ that means bit set 1 had at least <PLACE_HOLDER> of the bits set that were set in bs 2,return copy . is empty ( ) ;,1 had some
legacy herb flavors might hit this <PLACE_HOLDER> before the caching logic corrects it @$ so treat this as disabled .,return false ;,flavors hit limit
we found a simplification . remove the old <PLACE_HOLDER> and add new ones .,if ( range set . as ranges ( ) . size ( ) < filter list . size ( ) ) { for ( final bound dim filter bound : filter list ) { if ( ! new children . remove ( bound ) ) { throw new ise ( __str__ ) ; } } if ( range set . as ranges ( ) . is empty ( ) ) { new children . add ( filtration . match nothing ( ) ) ; } for ( final range < bound value > range : range set . as ranges ( ) ) { if ( ! range . has lower bound ( ) && ! range . has upper bound ( ) ) { new children .,simplification remove filters
a single instance of hbase checksum failure causes the <PLACE_HOLDER> to switch off hbase checksum verification for the next 100 read requests . verify that this is correct .,for ( int i = __num__ ; i < h file block . checksum_verification_num_io_threshold + __num__ ; i ++ ) { b = hbr . read block data ( __num__ @$ - __num__ @$ pread @$ false @$ true ) ; assert true ( b . get buffer read only ( ) instanceof single byte buff ) ; assert equals ( __num__ @$ h file . get and reset checksum failures count ( ) ) ; },instance causes reader
special case : if there were no files to measure @$ use the containing j scroll pane 's <PLACE_HOLDER>,if ( d . width == __num__ && get parent ( ) != null ) { if ( get parent ( ) . get parent ( ) instanceof j scroll pane ) { j scroll pane parent = ( j scroll pane ) get parent ( ) . get parent ( ) ; dimension parent size = parent . get size ( ) ; insets insets = parent . get insets ( ) ; d . width = parent size . width - ( insets != null ? insets . right + insets . left : __num__ ) ; } } else { d . width += default_icon_size + width_padding ; },case use width
should print further diagnostic <PLACE_HOLDER> ?,return __num__ ;,print further message
data should get <PLACE_HOLDER>,watcher . wait for change ( ) ; assert equals ( __str__ @$ new string ( o . zk . getzk database ( ) . get data ( __str__ @$ stat @$ null ) ) ) ; assert equals ( __str__ @$ new string ( o . zk . getzk database ( ) . get data ( __str__ @$ stat @$ null ) ) ) ;,data get updated
required for mdc based routing appender so that child threads can inherit the mdc <PLACE_HOLDER>,system . set property ( __str__ @$ __str__ ) ; configurator . initialize ( __str__ @$ llap_l4j2 . to string ( ) ) ; long end = system . current time millis ( ) ; log . debug ( __str__ @$ llap_l4j2 @$ ( end - start ) @$ async ) ; throw new runtime exception ( __str__ + __str__ + llap constants . log4j2_properties_file + __str__ ) ;,threads inherit properties
record elasticsearch version and cluster <PLACE_HOLDER> .,if ( target instanceof cluster info accessor ) { recorder . record attribute ( elasticsearch constants . args_version_annotation_key @$ ( ( cluster info accessor ) target ) . _$pinpoint$_get cluster info ( ) ) ; },record elasticsearch info
gesture selection helper provides <PLACE_HOLDER> that interprets a combination of motions and gestures in order to provide gesture driven selection support when used in conjunction with recycler view .,final gesture selection helper gesture helper = gesture selection helper . create ( tracker @$ m selection predicate @$ m recycler view @$ scroller @$ m monitor ) ;,helper provides implementation
verify the widget will now display white <PLACE_HOLDER>,attack plugin . on varbit changed ( new varbit changed ( ) ) ; warned skills = attack plugin . get warned skills ( ) ; assert true ( warned skills . contains ( skill . attack ) ) ; assert false ( attack plugin . is warned skill selected ( ) ) ;,widget display tags
bits consumed range : 0 to 7 @$ where 0 indicates last <PLACE_HOLDER> fully consumed,if ( bits consumed == __num__ || bits consumed == __num__ ) { bits consumed = __num__ ; ++ offset ; },0 indicates packet
someone modified the components tree <PLACE_HOLDER> while we were computing this range . we can just bail as another range will be computed .,synchronized ( this ) { if ( tree holders size != m component tree holders . size ( ) ) { return false ; } holder = m component tree holders . get ( index ) ; if ( holder . get render info ( ) . renders view ( ) ) { return true ; } children width spec = get actual children width spec ( holder ) ; children height spec = get actual children height spec ( holder ) ; },someone modified holder
turn the screen off . a black surface is already hiding the <PLACE_HOLDER> of the screen .,if ( m power state . get color fade level ( ) == __num__ ) { set screen state ( display . state_off ) ; m pending screen off = false ; m power state . dismiss color fade resources ( ) ; } else if ( perform screen off transition && m power state . prepare color fade ( m context @$ m color fade fades config ? color fade . mode_fade : color fade . mode_cool_down ) && m power state . get screen state ( ) != display . state_off ) { m color fade off animator . start ( ) ; } else { m color fade off animator . end ( ) ; },surface hiding area
since this link includes object <PLACE_HOLDER> from another library @$ we know that library must be statically linked @$ so we need to look at include link static in lto indexing to decide whether to include its objects in the lto indexing for this target .,if ( include link static in lto indexing ) { for ( linker inputs . library to link lib : unique libraries ) { if ( ! lib . contains object files ( ) ) { continue ; } for ( artifact object file : lib . get object files ( ) ) { if ( compiled . contains ( object file ) ) { all bitcode . put ( object file . get exec path ( ) @$ object file ) ; } } } } for ( linker input input : object files ) { if ( this . lto compilation context . contains bitcode file ( input . get artifact ( ) ) ) { all bitcode . put ( input . get artifact (,link includes files
check cm when <PLACE_HOLDER>ifier predicts only one <PLACE_HOLDER>,checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ; checkcm one obs ( __num__ @$ __num__ ) ;,classifier predicts observer
create one file then delete it to trigger the file not found <PLACE_HOLDER> when closing the file .,file sys . create ( new path ( __str__ ) ) ; file sys . delete ( new path ( __str__ ) @$ true ) ; dfs client dfs client = file sys . get client ( ) ;,file found exception
first operator should return final operator <PLACE_HOLDER>,assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; operator stats = pipeline operator . get operator context ( ) . get nested operator stats ( ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ; assert equals ( get testing operator info ( operator stats . get ( __num__ ) ) . count @$ __num__ ) ;,operator return stats
create one build rule to generate build <PLACE_HOLDER> .,build rule params build config params = params ; optional < build rule > values file rule = values file . flat map ( graph builder :: get rule ) ; if ( values file rule . is present ( ) ) { build config params = build config params . copy appending extra deps ( values file rule . get ( ) ) ; } android build config android build config = new android build config ( build config build target @$ project filesystem @$ build config params @$ java package @$ values @$ values file @$ use constant expressions ) ; graph builder . add to index ( android build config ) ;,one build configs
if the user provided a <PLACE_HOLDER> then we have to make sure the current realm has an events looper to deliver the results .,if ( ( on success != null || on error != null ) ) { shared realm . capabilities . check can deliver notification ( __str__ ) ; },user provided notification
should not work : <PLACE_HOLDER> on basic type,long ds . group by ( __num__ ) ;,not work groups
bind and start to accept incoming <PLACE_HOLDER> .,try { bootstrap server . register shutdown hook ( ) ; bootstrap server . start and block ( ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; } log . info ( __str__ ) ;,bind accept connections
create and set iam role so that firehose service has <PLACE_HOLDER> to the s 3 buckets to put data . please check the trust policy document.json and permissions policy document.json files for the trust and permissions policies set for the role .,string iam role arn = create iam role ( s3 object prefix ) ; redshifts3 configuration . set rolearn ( iam role arn ) ; copy command copy command = new copy command ( ) ; copy command . with copy options ( copy options ) . with data table name ( data table name ) ; redshift destination configuration redshift destination configuration = new redshift destination configuration ( ) ; redshift destination configuration . with clusterjdbcurl ( clusterjdbc url ) . with rolearn ( iam role arn ) . with username ( username ) . with password ( password ) . with copy command ( copy command ) . withs3 configuration ( redshifts3 configuration ) ; create delivery stream request . set redshift destination configuration ( redshift,service has access
test that a match set with no tags applied has no <PLACE_HOLDER> filtered,vt session session = controller . get session ( ) ; list < vt match set > match sets = session . get match sets ( ) ; vt match set match set = match sets . get ( __num__ ) ; collection < vt match > matches = match set . get matches ( ) ; for ( vt match match : matches ) { assert true ( __str__ @$ tag filter . passes filter ( match ) ) ; } filter state filter state = tag filter . get filter state ( ) ; map < string @$ vt match tag > excluded tags = ( map < string @$ vt match tag > ) filter state . get ( tag filter . excluded_tags_key ) ;,match has results
each test <PLACE_HOLDER> .,int start = __num__ @$ total = __num__ @$ remain = __num__ ; for ( int i = __num__ ; i < gas . length ; ++ i ) { remain += gas [ i ] - cost [ i ] ; total += gas [ i ] - cost [ i ] ; if ( remain < __num__ ) { start = i + __num__ ; remain = __num__ ; } },each test case
call the api 's channels.list method to retrieve the resource that represents the authenticated user 's channel . in the api response @$ only include channel information needed for this use case . the channel 's content details part contains playlist i ds relevant to the channel @$ including the id for the list that contains <PLACE_HOLDER> uploaded to the channel .,you tube . channels . list channel request = youtube . channels ( ) . list ( __str__ ) ; channel request . set mine ( true ) ; channel request . set fields ( __str__ ) ; channel list response channel result = channel request . execute ( ) ; list < channel > channels list = channel result . get items ( ) ; if ( channels list != null ) { string upload playlist id = channels list . get ( __num__ ) . get content details ( ) . get related playlists ( ) . get uploads ( ) ; list < playlist item > playlist item list = new array list < playlist item > ( ) ; you tube . playlist items,part contains data
extract character list @$ gold quote speaker and mention <PLACE_HOLDER> from the xml document .,document doc = xml utils . read document from file ( file name ) ; node text = doc . get document element ( ) . get elements by tag name ( __str__ ) . item ( __num__ ) ; string doc text = get just text ( text ) ; annotation document = get annotated file ( doc text @$ file name @$ get processed corenlp properties ( ) ) ; list < core map > quotes = document . get ( core annotations . quotations annotation . class ) ; list < core label > tokens = document . get ( core annotations . tokens annotation . class ) ; list < gold quote info > gold list = new array list < > ( ),gold quote tags
a new directory also receives a <PLACE_HOLDER> of the parent 's default acl .,list < acl entry > default entries = child . is directory ( ) ? parent default entries : collections . < acl entry > empty list ( ) ; final fs permission new perm ; if ( ! acl util . is minimal acl ( access entries ) || ! default entries . is empty ( ) ) { child . add acl feature ( create acl feature ( access entries @$ default entries ) ) ; new perm = create fs permission for extended acl ( access entries @$ child perm ) ; } else { new perm = create fs permission for minimal acl ( access entries @$ child perm ) ; },directory receives copy
0 means no <PLACE_HOLDER> on master .,if ( master count == __num__ || master count == system_regions ) { assert equals ( master count @$ m actual count ) ; } else { check count ( master count @$ m actual count ) ; },0 means data
compare to make sure the created job has the expected <PLACE_HOLDER> @$ i.e . the <PLACE_HOLDER> resulting from a merge of the json file and cli args .,final job actual job = json . read ( actual job config json @$ job . class ) ; final job . builder actual job builder = actual job . to builder ( ) ; builder . set name ( test job name ) . set version ( test job version ) . set image ( busybox ) . set env ( immutable map . of ( redundant env key @$ __str__ ) ) ; assert job equals ( builder . build ( ) @$ actual job builder . build ( ) ) ;,job has configuration
these values always start non <PLACE_HOLDER> .,while ( callback != null ) { future callback internal callback = this . callback ; exception e = this . e ; object result = this . result ; this . callback = null ; this . e = null ; this . result = null ; callback . on completed ( e @$ result @$ this ) ; },values start processing
some exceptions do not have a <PLACE_HOLDER> ; fall back to the string value .,if ( message == null ) { message = thrown . to string ( ) ; },exceptions have message
d 2 has pkeys <PLACE_HOLDER> to 5<PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; ++ i ) { volt queuesql ( loadd2 @$ i @$ __str__ + string . value of ( i ) ) ; } volt executesql ( ) ;,d pkeys 0
stop the inbound bridges when the foreign connection is dropped since the bridge has no <PLACE_HOLDER> and needs to be restarted once a new connection to the foreign side is made .,if ( this . foreign connection . compare and set ( connection @$ null ) ) { for ( destination bridge bridge : inbound bridges ) { try { bridge . stop ( ) ; } catch ( exception e ) { } } this . connection service . execute ( new runnable ( ) { @ override public void run ( ) { try { do initialize connection ( false ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; } } } ) ; } else if ( this . local connection . compare and set ( connection @$ null ) ) { for ( destination bridge bridge : outbound bridges ) { try { bridge . stop,bridge has consumer
parens force <PLACE_HOLDER> as an expression .,node script = parse ( __str__ + js + __str__ ) ;,parens force rendering
force default source if any param has unassigned <PLACE_HOLDER>,if ( ! get return ( ) . is valid ( ) ) { return source type . default ; } for ( parameter param : get parameters ( ) ) { if ( ! param . is valid ( ) ) { return source type . default ; } } return get stored signature source ( ) ; manager . lock . release ( ) ;,param unassigned value
mock unregister m bean to throw the instance not found <PLACE_HOLDER> @$ indicating that the m bean has already been unregistered,do throw ( new instance not found exception ( ) ) . when ( mockm bean server ) . unregisterm bean ( object name ) ; m beanjmx adapter m beanjmx adapter = spy ( new m beanjmx adapter ( dist member ) ) ; m beanjmx adapter . mbean server = mockm bean server ; m beanjmx adapter . unregisterm bean ( object name ) ;,m found exception
if passed integer and in list use numeric form else use original <PLACE_HOLDER>,try { int genre id = integer . parse int ( value ) ; if ( genre id < genre types . get max genre id ( ) ) { return bracket wrap ( string . value of ( genre id ) ) ; } else { return value ; } } catch ( number format exception nfe ) { integer genre id = genre types . get instance of ( ) . get id for name ( value ) ; if ( genre id != null ) { return bracket wrap ( string . value of ( genre id ) ) ; } if ( value . equals ignore case ( id3v2 extended genre types . rx . get description ( ) ) ) { value =,form use string
get the set of component <PLACE_HOLDER>s to make sure the config only specifies valid component <PLACE_HOLDER>,set < string > component names = get component parallelism ( topology ) . key set ( ) ;,config specifies parallelism
these array types still need to be implemented . the superclass wo n't handle <PLACE_HOLDER> so we return null here until we can code schema implementations for <PLACE_HOLDER> .,return null ; default : if ( oid value == type registry . geometry oid ( ) ) { return geometry . builder ( ) ; } else if ( oid value == type registry . geography oid ( ) ) { return geography . builder ( ) ; } else if ( oid value == type registry . citext oid ( ) ) { return schema builder . string ( ) ; } else if ( oid value == type registry . geometry array oid ( ) ) { return schema builder . array ( geometry . builder ( ) . optional ( ) . build ( ) ) ; } else if ( oid value == type registry . hstore oid ( ) ) { return,superclass handle ii
create a long from the first 8 bytes of the digest this is fine as md 5 has the avalanche <PLACE_HOLDER> . paranoids could have xor folded the other 8 bytes in too .,long seed = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { seed = ( seed << __num__ ) + ( ( int ) digest [ i ] + __num__ ) ; } return seed ;,md has buffer
up to here @$ an error can give a good <PLACE_HOLDER>,log . error ( __str__ @$ client socket . get inet address ( ) @$ t ) ; try { write error to stream ( output stream @$ t ) ; } catch ( io exception e ) { },error give error
a field of executable stage which includes the p <PLACE_HOLDER> goes to worker side .,set < p collection node > executable stage outputs = new hash set < > ( ) ;,which includes collection
<PLACE_HOLDER> 1 reads all local <PLACE_HOLDER> 2 reads 10 local and 10 remote <PLACE_HOLDER> 3 reads all remote,try { final string [ ] hosts = { __str__ @$ __str__ @$ __str__ } ; final int num_local_host1_splits = __num__ ; final int num_local_host2_splits = __num__ ; final int num_remote_splits = __num__ ; final int num_local_splits = num_local_host1_splits + num_local_host2_splits ; int split cnt = __num__ ; set < locatable input split > splits = new hash set < locatable input split > ( ) ; for ( int i = __num__ ; i < num_local_host1_splits ; i ++ ) { splits . add ( new locatable input split ( split cnt ++ @$ __str__ ) ) ; } for ( int i = __num__ ; i < num_local_host2_splits ; i ++ ) { splits . add ( new locatable input split ( split cnt ++ @$,remote reads host
owner can add internal system <PLACE_HOLDER>,return create window ( parent @$ type @$ token @$ name @$ owner id @$ false ) ;,owner add window
filter actually did some <PLACE_HOLDER> @$ set the new chunk in and release the old chunk .,if ( ( filtered chunk != null ) && ( filtered chunk != orig chunk ) ) { body chunks . set ( i @$ filtered chunk ) ; final int ref cnt = orig chunk . ref cnt ( ) ; if ( ref cnt > __num__ ) { orig chunk . release ( ref cnt ) ; } },filter did work
we only check for incomplete multi statement procedures right now add a mandatory <PLACE_HOLDER>,incomplete stmt offset = i start ; incomplete stmt = string . copy value of ( buf @$ i start @$ i cur - i start ) ;,procedures add point
make sure that the metadata was refreshed during the rebalance and thus subscriptions now contain two <PLACE_HOLDER> .,final set < string > updated subscription set = new hash set < > ( arrays . as list ( topic1 @$ topic2 ) ) ; assert equals ( updated subscription set @$ subscriptions . subscription ( ) ) ;,subscriptions contain topics
propagating the amrm client nm token cache <PLACE_HOLDER>,nm client . setnm token cache ( rm client . getnm token cache ( ) ) ; nm client . init ( conf ) ; nm client . start ( ) ; assert not null ( nm client ) ; assert equals ( state . started @$ nm client . get service state ( ) ) ;,client nm object
for <PLACE_HOLDER>s @$ the computed <PLACE_HOLDER> equals the symbol 's <PLACE_HOLDER> @$ except for two situations :,owntype = sym . type ; if ( owntype . has tag ( class ) ) { chk . check for bad auxiliary class access ( tree . pos ( ) @$ env @$ ( class symbol ) sym ) ; type own outer = owntype . get enclosing type ( ) ; if ( owntype . tsym . type . get type arguments ( ) . non empty ( ) ) { owntype = types . erasure ( owntype ) ; } else if ( own outer . has tag ( class ) && site != own outer ) { type norm outer = site ; if ( norm outer . has tag ( class ) ) { norm outer = types . as enclosing super (,type equals type
create four wheels and add them at their locations note that our fancy car actually goes <PLACE_HOLDER>,vector3f wheel direction = new vector3f ( __num__ @$ - __num__ @$ __num__ ) ; vector3f wheel axle = new vector3f ( - __num__ @$ __num__ @$ __num__ ) ; geometry wheel_fr = find geom ( car node @$ __str__ ) ; wheel_fr . center ( ) ; box = ( bounding box ) wheel_fr . get model bound ( ) ; wheel radius = box . gety extent ( ) ; float back_wheel_h = ( wheel radius * __num__ ) - __num__ ; float front_wheel_h = ( wheel radius * __num__ ) - __num__ ; player . add wheel ( wheel_fr . get parent ( ) @$ box . get center ( ) . add ( __num__ @$ - front_wheel_h @$ __num__ ) @$ wheel direction @$,car goes backwards
omitting sha <PLACE_HOLDER> for the moment ; there seems to be no reason to allow it .,if ( params . get hash ( ) != hash type . sha256 && params . get hash ( ) != hash type . sha512 ) { throw new general security exception ( __str__ ) ; },omitting sha 256
a corrupt mob file does n't abort the <PLACE_HOLDER> of regions @$ so we can enable the table .,admin . enable table ( table ) ; h base fsck res = hbck testing util . doh file quarantine ( conf @$ table ) ; assert equals ( __num__ @$ res . get ret code ( ) ) ; h file corruption checker hfcc = res . geth filecorruption checker ( ) ; assert equals ( __num__ @$ hfcc . geth files checked ( ) ) ; assert equals ( __num__ @$ hfcc . get corrupted ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get failures ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get quarantined ( ) . size ( ) ) ; assert equals ( __num__ @$ hfcc . get missing,file abort check
second try should hit the <PLACE_HOLDER> again,try { metastore . get all databases ( ) ; } catch ( runtime exception ignored ) { } assert equals ( mock client . get access count ( ) @$ __num__ ) ;,try hit server
the other one has multiple <PLACE_HOLDER> @$ but we do n't . expand to multiple <PLACE_HOLDER> and copy over .,if ( other src . m durations != null ) { my src . make durations ( ) ; my src . m durations . add durations ( other src . m durations ) ; if ( my src . m active duration != __num__ ) { my src . m durations . add duration ( my src . m active proc state @$ my src . m active duration ) ; my src . m active duration = __num__ ; my src . m active proc state = process stats . state_nothing ; } } else if ( my src . m active duration != __num__ ) { if ( my src . m active proc state == other src . m active proc state ) {,one has durations
date @$ note : these ms counts all presume pst jan @$ feb @$ mar @$ apr 2014 may @$ jun @$ jul @$ aug 2014 <PLACE_HOLDER> @$ oct @$ nov @$ dec 2014 jan 2016 @$ mar 2017 @$ jun 2018 @$ <PLACE_HOLDER> 2019 @$ dec 2020 jan @$ feb @$ mar @$ apr 2014 may @$ jun @$ jul @$ aug 2014,long [ ] exp = new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ;,date aug 19
not a comma . stop parsing the keep <PLACE_HOLDER> .,if ( ! configuration constants . argument_separator_keyword . equals ( next word ) ) { break ; },the keep thing
kick off any lingering app transitions form the move <PLACE_HOLDER> to front operation @$ but only consider the top <PLACE_HOLDER> and stack on that display .,if ( display . is top stack ( stack ) && top running activity . is state ( resumed ) ) { stack . execute app transition ( target options ) ; } else { resumed on display |= top running activity . make active if needed ( target ) ; },transitions form task
required for mdc based routing appender so that child threads can inherit the mdc <PLACE_HOLDER>,system . set property ( default thread context map . inheritable_map @$ __str__ ) ; configurator . initialize ( null @$ log4j file name ) ; log config location ( conf ) ; return __str__ + log4j config file + __str__ + async ;,threads inherit context
restarted rm has the failed app <PLACE_HOLDER> too .,rm2 . wait for state ( app1 . get application id ( ) @$ rm app state . failed ) ;,rm has state
character functions returning number <PLACE_HOLDER> :,add functions ( arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$,functions returning values
note : complex structure already has a single undefined <PLACE_HOLDER> .,complex structure . add ( new byte data type ( ) ) ; complex structure . add ( new word data type ( ) ) ; complex structure . add ( new pointer32 data type ( data type . default ) @$ __num__ ) ;,structure has type
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( test update deployment . class ) ;,suite using tests
consuming segment should have all <PLACE_HOLDER> in consuming state,for ( map . entry < string @$ string > entry : consuming segment instance state map . entry set ( ) ) { assert equals ( entry . get value ( ) @$ realtime segment online offline state model . consuming ) ; entry . set value ( realtime segment online offline state model . offline ) ; },segment have instances
else can we handle iterator types if context node does n't support event <PLACE_HOLDER> ? ?,add event listener ( ) ;,node support listeners
then : the getters should return properly parsed <PLACE_HOLDER>,assert that ( builder . get bind address ( ) ) . is null ( ) ; assert that ( builder . get command ( ) ) . is equal to ( command . start ) ; assert that ( builder . get debug ( ) ) . is false ( ) ; assert that ( builder . get force ( ) ) . is false ( ) ; assert that ( builder . get help ( ) ) . is false ( ) ; assert that ( builder . get hostname for clients ( ) ) . is null ( ) ; assert that ( builder . get member name ( ) ) . is equal to ( __str__ ) ; assert that ( builder . get,getters return values
src inode and its subtree can not contain snapshottable <PLACE_HOLDER> with snapshots,fs dir snapshot op . check snapshot ( fsd @$ srciip @$ snapshottable dirs ) ;,inode contain directories
the current item has higher <PLACE_HOLDER>,final int current index = play queue . get index ( ) ; final play queue item current item = play queue . get item ( current index ) ; if ( current item == null ) return null ;,item has priority
if spell <PLACE_HOLDER> is disabled @$ just return . the user should explicitly enable the spell <PLACE_HOLDER> .,if ( ! tsd . is spell checker enabled ( ) ) return ; if ( sci == null ) { sci = find avail system spell checker locked ( null @$ tsd ) ; set current spell checker locked ( sci @$ tsd ) ; } else { final string package name = sci . get package name ( ) ; final int change = is package disappearing ( package name ) ; if ( dbg ) slog . d ( tag @$ __str__ + package name ) ; if ( change == package_permanent_change || change == package_temporary_change ) { spell checker info avail sci = find avail system spell checker locked ( package name @$ tsd ) ; if ( avail sci == null || (,user enable check
add a test . this test runs the select <PLACE_HOLDER> and expects the result to be order by output .,dbc . add test ( new test config ( __str__ @$ __str__ @$ false @$ order by output ) ) ;,test runs statement
we do n't yet know the next <PLACE_HOLDER>imum value so get the <PLACE_HOLDER> of the address ranges .,if ( max == null ) { for ( int i = __num__ ; i < addr ranges . length ; i ++ ) { if ( addr ranges [ i ] != null ) { address check max addr = addr ranges [ i ] . get max address ( ) ; if ( ( max == null ) || ( max . compare to ( check max addr ) < __num__ ) ) { max = check max addr ; } } } },value get max
someone really wants this <PLACE_HOLDER> to die off .,thread . current thread ( ) . interrupt ( ) ;,someone wants thread
the last one should have gotten a stream <PLACE_HOLDER>,for ( int i = __num__ ; i < single partmime reader callbacks . size ( ) - __num__ ; i ++ ) { final single partmime exception reader callback impl current callback = single partmime reader callbacks . get ( i ) ; final body part current expected part = multi part mime body . get body part ( i ) ; final map < string @$ string > expected headers = new hash map < string @$ string > ( ) ; @ suppress warnings ( __str__ ) final enumeration < header > all headers = current expected part . get all headers ( ) ; while ( all headers . has more elements ( ) ) { final header header = all headers . next,one gotten catch
check that the backgrounds have the same <PLACE_HOLDER> of the components to which they are associated,assert that ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) . is equal to ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) ; assert that ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) . is equal to ( layout state . get mountable output at ( __num__ ) . get bounds ( ) ) ; final rect text layout bounds = layout state . get mountable output at ( __num__ ) . get bounds ( ) ; final rect text background bounds = layout state . get mountable output at ( __num__ ) . get bounds ( ) ; assert that,backgrounds have dimensions
note down that the process has finished an <PLACE_HOLDER> and is in background <PLACE_HOLDER> starts grace period,if ( r . app != null ) { r . app . set last activity finish time if needed ( system clock . uptime millis ( ) ) ; } final long orig id = binder . clear calling identity ( ) ; try { boolean res ; final boolean finish with root activity = finish task == activity . finish_task_with_root_activity ; if ( finish task == activity . finish_task_with_activity || ( finish with root activity && r == rootr ) ) { res = m stack supervisor . remove task by id locked ( tr . task id @$ false @$ finish with root activity @$ __str__ ) ; if ( ! res ) { slog . i ( tag @$ __str__ ) ; } r,process finished activity
use print writer.println @$ which uses correct platform <PLACE_HOLDER> ending .,byte array output stream baos = new byte array output stream ( ) ; print writer node report str = new print writer ( new output stream writer ( baos @$ charset . for name ( __str__ ) ) ) ; node report node report = null ; for ( node report report : nodes report ) { if ( ! report . get node id ( ) . equals ( node id ) ) { continue ; } node report = report ; node report str . println ( __str__ ) ; node report str . print ( __str__ ) ; node report str . println ( node report . get node id ( ) ) ; node report str . print ( __str__ ) ; node,which uses line
initialize stats publishing table for noscan which has only stats <PLACE_HOLDER> the rest of mr <PLACE_HOLDER> following stats <PLACE_HOLDER> initializes it in exec driver.java,stats publisher stats publisher = factory . get stats publisher ( ) ; if ( ! stats publisher . init ( scc ) ) { throw new hive exception ( error msg . statspublisher_initialization_error . get error coded msg ( ) ) ; },which has producer
generate for the get <PLACE_HOLDER>,super . abstract method ( symbol table @$ a @$ stream ) ;,the get method
a correct run should n't deliver any <PLACE_HOLDER>,var counter = __num__ ; for ( var ex : res . get exception list ( ) ) { counter ++ ; logger . info ( ex ) ; } return counter ;,run deliver exceptions
buck config should allow nonexistent <PLACE_HOLDER> without throwing .,buck config test utils . create with default filesystem ( temporary folder @$ reader ) ;,config allow files
variants only contain <PLACE_HOLDER> .,if ( variants contain audio codecs ) { format [ ] audio formats = new format [ selected variants . size ( ) ] ; for ( int i = __num__ ; i < audio formats . length ; i ++ ) { format variant format = variants [ i ] . format ; audio formats [ i ] = derive audio format ( variant format @$ master playlist . muxed audio format @$ true ) ; } muxed track groups . add ( new track group ( audio formats ) ) ; } else { throw new illegal argument exception ( __str__ + codecs ) ; },variants contain audio
catch the original exception which sounds like : java.lang.illegal argument exception : comparison method violates its general <PLACE_HOLDER> !,swing utilities . invoke later ( ( ) -> { thread . current thread ( ) . set uncaught exception handler ( new thread . uncaught exception handler ( ) { public void uncaught exception ( thread t @$ throwable e ) { e . print stack trace ( ) ; if ( e instanceof illegal argument exception ) { passed = false ; latch . count down ( ) ; } } } ) ; test dialog d = new test dialog ( ) ; d . add window focus listener ( new window adapter ( ) { public void window gained focus ( window event e ) { latch . count down ( ) ; } } ) ; d . set visible ( true ),method violates contract
byte argument in gram key breaks <PLACE_HOLDER> between equal grams,byte [ ] empty = new byte [ __num__ ] ; gram key [ ] input = { new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ empty ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ empty ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . unigram ) @$ foo ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . ngram ) @$ foo ) @$ new gram key ( new gram ( __str__ @$ __num__ @$ gram . type . ngram ) @$ empty ) @$ new gram key ( new,argument breaks distribution
health checker <PLACE_HOLDER> .,if ( is health checker configured ( ) ) { int sleep time = this . conf . get int ( h constants . health_chore_wake_freq @$ h constants . default_thread_wake_frequency ) ; health check chore = new health check chore ( sleep time @$ this @$ get configuration ( ) ) ; },health checker configuration
lazily initialized since most selectors do n't survive module <PLACE_HOLDER> .,if ( set name == null ) { set name = annotations . name of ( set key ) ; } return set name ;,selectors survive creation
in case when remote tx has updated the current <PLACE_HOLDER> before .,ignore ( ) ;,tx updated transaction
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( testtpcc suite . class ) ;,suite using tests
we did not demand online loading @$ therefore a failure does not mean that the missing snippet causes a <PLACE_HOLDER> of this result this may happen during a remote search @$ because snippet loading is omitted to retrieve results faster,if ( cache strategy . must be offline ( ) ) { return page . make result entry ( this . query . get segment ( ) @$ this . peers @$ null ) ; } else { if ( this . snippet fetch words . contains ( segment . catchall string ) ) { return page . make result entry ( this . query . get segment ( ) @$ this . peers @$ null ) ; } final string reason = __str__ + snippet . get error code ( ) ; if ( this . delete if snippet fail ) { this . work tables . failur ls register missing word ( this . query . get segment ( ) . term index ( ) @$,snippet causes reload
the output stream of this fs does n't support <PLACE_HOLDER> @$ so the below test will fail,conf . set var ( hive conf . conf vars . hive_blobstore_supported_schemes @$ __str__ ) ; strict delimited input writer writer = strict delimited input writer . new builder ( ) . with field delimiter ( __str__ ) . build ( ) ; try { hive streaming connection . new builder ( ) . with database ( db name ) . with table ( tbl name ) . with agent info ( __str__ + thread . current thread ( ) . get name ( ) ) . with record writer ( writer ) . with transaction batch size ( __num__ ) . with hive conf ( conf ) . connect ( ) ; assert . fail ( ) ; } catch ( connection error e ) { assert,stream support protocols
should not trigger and block <PLACE_HOLDER>,partial blocks < test element > partial blocks = block . get partial blocks ( ) ; assert not null ( partial blocks ) ; assert not null ( partial blocks . get block ranges ( ) ) ; assert equals ( __num__ @$ partial blocks . get block ranges ( ) . length ) ; assert equals ( group size @$ partial blocks . get block ranges ( ) [ __num__ ] ) ; assert not null ( partial blocks . get block targets ( ) ) ; assert equals ( __num__ @$ partial blocks . get block targets ( ) . length ) ; assert true ( partial blocks . get block targets ( ) [ __num__ ] . is valid ( ) ) ; assert,not trigger compilation
corresponding method <PLACE_HOLDER> with the current stack may exceeds the stack trace . trims the <PLACE_HOLDER>,if ( method count + stack offset > trace . length ) { method count = trace . length - stack offset - __num__ ; } for ( int i = method count ; i > __num__ ; i -- ) { int stack index = i + stack offset ; if ( stack index >= trace . length ) { continue ; } string builder builder = new string builder ( ) ; builder . append ( horizontal_line ) . append ( __str__ ) . append ( level ) . append ( get simple class name ( trace [ stack index ] . get class name ( ) ) ) . append ( __str__ ) . append ( trace [ stack index ] . get method name,trace trims count
check choose server <PLACE_HOLDER> for rsa keys .,results rsa = km . choose server alias ( __str__ @$ null @$ null ) ; if ( results rsa == null ) { throw new exception ( __str__ ) ; } system . out . println ( __str__ ) ;,check choose alias
new children added @$ qualified name needs <PLACE_HOLDER>,update child categories ( store object @$ new children . values ( ) @$ impacted categories @$ false ) ; break ;,name needs recomputation
see if the file matches the regular <PLACE_HOLDER> !,try { if ( pattern != null ) { matcher matcher = pattern . matcher ( file . get name ( ) ) ; get it = matcher . matches ( ) ; } if ( get it ) { string local filename = return target filename ( file . get name ( ) ) ; if ( ( ! only getting new files ) || ( only getting new files && needs download ( local filename ) ) ) { if ( is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ file . get name ( ) @$ target directory ) ) ; } connection . download file ( file @$ return target filename ( file .,file matches expression
define the do <PLACE_HOLDER> that logs the value provider value .,p . apply ( create . of ( __num__ ) ) . apply ( par do . of ( new do fn < integer @$ integer > ( ) { @ process element public void process ( process context c ) { my options ops = c . get pipeline options ( ) . as ( my options . class ) ; log . info ( __str__ @$ ops . get string value ( ) ) ; } } ) ) ;,the do fn
d tx 2 <PLACE_HOLDER> @$ but t xs with lower ids are still open,buffer . offer ( __num__ ) ; tx opened . increment and get ( ) ;,d tx finishes
note that this class does not support forward <PLACE_HOLDER> .,if ( ! forward ) { if ( trusted pub key != null ) { prev pub key = trusted pub key ; } else { prev pub key = null ; } } else { throw new cert path validator exception ( __str__ ) ; },class support digests
note that we do not need to update requires old <PLACE_HOLDER> in events because that flag is only used during region initialization . otherwise we would need to,if ( ! ( this . region instanceof partitioned region ) ) { return ; },note requires value
noinspection object <PLACE_HOLDER> in loop,if ( is composite ( current index ) ) { new keys = new tree set < > ( ) ; for ( comparable current key : current keys ) { final list < orid > current result = get from composite index ( current key @$ current index ) ; new keys . add all ( prepare keys ( next index @$ current result ) ) ; } } else { final list < o identifiable > keys ; try ( stream < o raw pair < object @$ orid > > stream = current index . get internal ( ) . stream entries ( current keys @$ true ) ) { keys = stream . map ( ( pair ) -> pair . second ) .,noinspection object allocation
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; rule key input rule key = new rule key ( __str__ ) ; path output = paths . get ( __str__ ) ; build rule rule = new input rule key build rule ( target @$ filesystem @$ params ) { @ override public immutable list < step > get build steps ( build context context @$ buildable context buildable context ) { buildable context . record artifact ( output ) ; return immutable list . of ( new write file step ( filesystem @$ __str__ @$ output @$ false ) ) ; } @ override public source path get,which writes file
add ejb suspend handler <PLACE_HOLDER>,boolean enable graceful shutdown = ejb3 subsystem root resource definition . enable_graceful_txn_shutdown . resolve model attribute ( context @$ model ) . as boolean ( ) ; final ejb suspend handler service ejb suspend handler service = new ejb suspend handler service ( enable graceful shutdown ) ; context . get service target ( ) . add service ( ejb suspend handler service . service_name @$ ejb suspend handler service ) . add dependency ( suspend controller service name @$ suspend controller . class @$ ejb suspend handler service . get suspend controller injected value ( ) ) . add dependency ( txn services . jboss_txn_local_transaction_context @$ local transaction context . class @$ ejb suspend handler service . get local transaction context injected value ( ) ) .,ejb suspend service
we need to keep the admin <PLACE_HOLDER> instance separate in each callable @$ so that a refresh of the <PLACE_HOLDER> in one callable does not refresh the admin <PLACE_HOLDER> used by another callable .,admin client current admin client = admin store swapper . this . admin client ; int attempt = __num__ ; while ( attempt <= max_fetch_attempts ) { if ( attempt > __num__ ) { logger . info ( __str__ + attempt + __str__ + max_fetch_attempts + __str__ + node . brief to string ( ) + __str__ + wait_time_between_fetch_attempts + __str__ ) ; try { thread . sleep ( wait_time_between_fetch_attempts ) ; } catch ( interrupted exception e ) { throw new voldemort exception ( e ) ; } } logger . info ( __str__ + node . brief to string ( ) + __str__ + hadoop store dir to fetch ) ; try { return current admin client . readonly ops . fetch store ( node .,refresh refresh client
holds an <PLACE_HOLDER> in a long register ? the callee has no clue whether the register holds an <PLACE_HOLDER> @$ long or is unused . he always saves a long . here we know a long was saved @$ but we only want an <PLACE_HOLDER> back . narrow the saved long to the <PLACE_HOLDER> that the jvm wants .,if ( loc . holds int ( ) ) { if ( assert . asserts_enabled ) { assert . that ( loc . is register ( ) @$ __str__ ) ; } return new stack value ( value addr . getj long at ( __num__ ) & __num__ ) ; } else if ( loc . holds narrow oop ( ) ) { if ( loc . is register ( ) && vm . getvm ( ) . is big endian ( ) ) { return new stack value ( value addr . get comp oop handle at ( vm . getvm ( ) . get int size ( ) ) @$ __num__ ) ; } else { return new stack value ( value addr . get comp,register holds int
log the input value which caused <PLACE_HOLDER> so that it 's available for debugging . but when exposed through an error message it can leak sensitive information @$ even to the client application .,log . trace ( __str__ + tag + __str__ + utilities . format binary string ( value writable . get bytes ( ) @$ __num__ @$ value writable . get length ( ) ) + __str__ + value table desc [ tag ] . get properties ( ) ) ; throw new hive exception ( __str__ @$ e ) ;,which caused exception
then @$ already tried this credential . remove any rfc 2617 credential since presence of a rfc 2617 credential serves as flag to frontier to requeue this curi and let the curi die a natural <PLACE_HOLDER> .,if ( extant != null ) { extant . detach all ( curi ) ; logger . warning ( __str__ + realm + __str__ + curi . to string ( ) ) ; } else { string server key = get server key ( curi ) ; crawl server server = server cache . get server for ( server key ) ; set < credential > store rfc2617 credentials = get credential store ( ) . subset ( curi @$ http authentication credential . class @$ server . get name ( ) ) ; if ( store rfc2617 credentials == null || store rfc2617 credentials . size ( ) <= __num__ ) { logger . fine ( __str__ + curi ) ; } else { http authentication credential,curi die place
make sure that the nested custom layouts do not render <PLACE_HOLDER>,for ( element child : root node . children ( ) ) { assert that ( child . children ( ) . size ( ) @$ is ( __num__ ) ) ; },layouts render them
remote node has newer im<PLACE_HOLDER> <PLACE_HOLDER>,if ( bn reg . get layout version ( ) < storage . get layout version ( ) || ( bn reg . get layout version ( ) == storage . get layout version ( ) && bn reg . getc time ( ) > storage . getc time ( ) ) ) msg = __str__ + bn reg . get address ( ) + __str__ + bn reg . get layout version ( ) + __str__ + bn reg . getc time ( ) + __str__ + storage . get layout version ( ) + __str__ + storage . getc time ( ) ;,node has id
lower half of the threads insert keys 0 @$ 1 @$2 ... uper half of the threads insert <PLACE_HOLDER> 0 @$ 1 @$ 2 @$ ...,for ( int thread no = __num__ ; thread no < threads ; thread no ++ ) { graql insert query = thread no < threads / __num__ ? graql . parse ( __str__ + thread no + __str__ ) . as insert ( ) : graql . parse ( __str__ + ( thread no - threads / __num__ ) + __str__ ) . as insert ( ) ; completable future < void > async insert = completable future . supply async ( ( ) -> { transaction impl tx = ( transaction impl ) session . write transaction ( ) ; tx . execute ( query ) ; try { barrier . await ( ) ; } catch ( exception e ) { e . print stack,half keys keys
node only has left <PLACE_HOLDER> which will be the minimum .,if ( right child index >= size ) { min index = left child index ; } else { t left child value = _values . get ( left child index ) ; t right child value = _values . get ( right child index ) ; if ( compare ( left child value @$ right child value ) <= __num__ ) { min index = left child index ; } else { min index = right child index ; } },node left child
mem sql uses <PLACE_HOLDER> instead of schemas,return result set . get string ( __str__ ) ;,sql uses catalogs
if we do not update content of the record we should keep version of the record the same otherwise we would have <PLACE_HOLDER> when two records may have the same version but different content,final int new record version ; if ( update content ) { new record version = ppos . record version ; } else { new record version = version ; } if ( callback != null ) { callback . call ( rid @$ new record version ) ; } if ( o log manager . instance ( ) . is debug enabled ( ) ) { o log manager . instance ( ) . debug ( this @$ __str__ @$ rid @$ new record version @$ content . length ) ; } record updated . increment ( ) ; if ( content modified ) { return new o storage operation result < > ( new record version @$ content @$ false ) ; } else { return,records have deadlock
sak : not calling this means that mouse and mouse motion listeners do n't get <PLACE_HOLDER> . not a problem because the menu manager handles tracking for us .,( ( screen menu itemui ) ui ) . update listeners for screen menu item ( ) ;,listeners get updated
k.o . criteria @$ start tag is not svg @$ fail <PLACE_HOLDER> on none svg,default :,criteria fail search
be more forgiving of not finding the get listener <PLACE_HOLDER> .,method method = introspector . find method ( source class @$ get listener method name @$ __num__ ) ; if ( method != null ) { set get listener method ( method ) ; },the get method
send enough coin to the apply account to make that account has <PLACE_HOLDER> to apply become witness .,grpcapi . witness list witnesslist = blocking stub full . list witnesses ( grpcapi . empty message . new builder ( ) . build ( ) ) ; optional < witness list > result = optional . of nullable ( witnesslist ) ; grpcapi . witness list witness list = result . get ( ) ; if ( result . get ( ) . get witnesses count ( ) < __num__ ) { assert . assert true ( public methed . sendcoin ( low bal address @$ cost for create witness @$ from address @$ test key002 @$ blocking stub full ) ) ; assert . assert false ( create witness not broadcast ( low bal address @$ wrong url @$ test update witness key ) ) ;,account has ability
does lfs see the correct file <PLACE_HOLDER> ?,assert equals ( lfs . get file status ( pathtotestfile1 ) . get len ( ) @$ testfile1 . length ( ) ) ;,lfs see size
eof scanline write the <PLACE_HOLDER>,if ( j == scanline bytes - __num__ ) { if ( abs val == - __num__ ) { stream . write byte ( run count ) ; stream . write byte ( run val ) ; inc comp image size ( __num__ ) ; run count = __num__ ; } else { if ( abs val >= __num__ ) { stream . write byte ( __num__ ) ; stream . write byte ( abs val + __num__ ) ; inc comp image size ( __num__ ) ; for ( int a = __num__ ; a <= abs val ; a ++ ) { stream . write byte ( abs buf [ a ] ) ; inc comp image size ( __num__ ) ; } if ( !,scanline write bytes
by the a 2 dp spec @$ srcs must indicate the capture <PLACE_HOLDER> . however if some device that do not @$ we try to match on some other class bits .,switch ( get device class ( ) ) { case device . audio_video_hifi_audio : case device . audio_video_set_top_box : case device . audio_video_vcr : return true ; default : return false ; },srcs indicate feature
handle negatives @$ which means last n <PLACE_HOLDER>,if ( start < __num__ ) { start = str . length ( ) + start ; },which means text
nodes must have the same <PLACE_HOLDER>,if ( node1 . get class ( ) != node2 . get class ( ) ) { return false ; } string image1 = node1 . get image ( ) ; string image2 = node2 . get image ( ) ;,nodes have image
jdk 11 added the <PLACE_HOLDER> to the error message @$ we can do that for all java versions to be consistent .,return new array index out of bounds exception ( __str__ + index + __str__ + length ) ;,jdk added path
check this ps is the master ps for this location @$ only master ps can accept the <PLACE_HOLDER>,if ( ! request . is come from ps ( ) && ! is part master ps ( part loc ) ) { string log = __str__ + request + __str__ + request . get part key ( ) ; log . error ( log ) ; return new updater response ( response type . server_handle_failed @$ log ) ; } else { try { class < ? extends update func > func class = ( class < ? extends update func > ) class . for name ( request . get updater func class ( ) ) ; constructor < ? extends update func > constructor = func class . get constructor ( ) ; constructor . set accessible ( true ) ; update func func =,ps accept request
stop the datanode contains b <PLACE_HOLDER>,data node dn = cluster . get data node ( dn list [ decommission nodes num ] . get ipc port ( ) ) ; cluster . stop data node ( dn list [ decommission nodes num ] . get xfer addr ( ) ) ; cluster . set data node dead ( dn . get datanode id ( ) ) ;,datanode contains bucket
to work around the fact that pmd does not yet do full type <PLACE_HOLDER> when it does @$ delete this,set < method name declaration > unique = new hash set < > ( ) ; set < string > sigs = new hash set < > ( ) ; for ( method name declaration mnd : methods . key set ( ) ) { string sig = mnd . get image ( ) + mnd . get parameter count ( ) + mnd . is varargs ( ) ; if ( ! sigs . contains ( sig ) ) { unique . add ( mnd ) ; } sigs . add ( sig ) ; } return unique ;,pmd do matching
it is expected that every map processes map sleep count <PLACE_HOLDER> of records .,try { context . set status ( __str__ + ( map sleep duration * ( map sleep count - count ) ) + __str__ ) ; long sleep time = sleep calc . calc sleep duration ( context . get task attemptid ( ) @$ count @$ map sleep count @$ map sleep duration ) ; thread . sleep ( sleep time ) ; } catch ( interrupted exception ex ) { throw ( io exception ) new io exception ( __str__ ) . init cause ( ex ) ; } ++ count ;,processes map number
android resources @$ views and extras require special <PLACE_HOLDER>,if ( has injection points for annotation ( inject resource . class ) ) { bind listener ( matchers . any ( ) @$ resource listener ) ; } if ( has injection points for annotation ( inject extra . class ) ) { final extras listener extras listener = new extras listener ( context provider ) ; bind listener ( matchers . any ( ) @$ extras listener ) ; },resources require handling
this creates lock contention for the delay met command <PLACE_HOLDER> inside the command handler . so move the actual execution of the post completion callbacks on the command executor thread .,post on main thread ( new add runnable ( this @$ command handler . create execution completed intent ( m context @$ work spec id @$ needs reschedule ) @$ default_start_id ) ) ;,contention met execution
password match <PLACE_HOLDER>,inputerror = __num__ ;,password match check
only add to lexer if the function actually contains <PLACE_HOLDER>,if ( ! raf . actions . is empty ( ) && ! lexer . action funcs . contains key ( r ) ) { lexer . action funcs . put ( r @$ raf ) ; },function contains actions
just return @$ stats gathering should not block the main <PLACE_HOLDER>,if ( stats publisher == null ) { log . error ( __str__ ) ; if ( is stats reliable ) { throw new hive exception ( error msg . statspublisher_not_obtained . get error coded msg ( ) ) ; } return ; },return block query
note : token only copied if the recipient does n't already have <PLACE_HOLDER> .,if ( token == null ) { token = o . token ; },recipient have one
the two columns have different data <PLACE_HOLDER>,if ( ! expected data type . get class ( ) . is instance ( actual data type ) ) { if ( expected data type instanceof unknown data type ) { return actual data type ; } if ( actual data type instanceof unknown data type ) { return expected data type ; } if ( hack ignore int big int mismatch ) { if ( expected data type instanceof integer data type && actual data type instanceof big integer data type ) return actual data type ; } string msg = __str__ + table name + __str__ + expected column . get column name ( ) + __str__ ; throw failure handler . create failure ( msg @$ string . value of ( expected data type,columns have type
apk does not contain merkle tree root <PLACE_HOLDER> .,return false ;,apk contain node
the set of <PLACE_HOLDER> dirs includes device protected <PLACE_HOLDER> dir and credential protected <PLACE_HOLDER> dir which might be null for shared libraries . currently we do n't track these but be lenient and check in case we ever decide to store their usage <PLACE_HOLDER> .,for ( string data dir : data dirs ) { if ( data dir != null ) { pcl . merge app data dirs ( data dir @$ user id ) ; } },dir protected data
if no partition <PLACE_HOLDER> registered for the cluster @$ simply use the default <PLACE_HOLDER> this can happen when the customized <PLACE_HOLDER> implementation library has not been deployed to the client,if ( partition accessors == null || partition accessors . is empty ( ) ) { _log . error ( __str__ + cluster name + __str__ ) ; return default partition accessor . get instance ( ) ; },accessor use accessor
redis has three chunks <PLACE_HOLDER> last chunk only has entries,navigable map < byte [ ] @$ byte [ ] > chunk map = get binary tree map ( ) ; put encoded key value to map ( chunk map @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) ; put encoded key value to map ( chunk map @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) ; scan result < map . entry < byte [ ] @$ byte [ ] > > scan result first = new scan result < > ( __str__ . get bytes ( ) @$ new array list < map . entry < byte [ ] @$ byte [ ] > > ( ) ) ; scan result <,redis has which
if the client acked an index <PLACE_HOLDER> than the current event sequence number since we know the client must have received it from another server .,if ( complete index > context . current index ( ) ) { return ; },client acked greater
initializes bluetooth <PLACE_HOLDER> .,if ( m bluetooth adapter == null ) { final bluetooth manager bluetooth manager = ( bluetooth manager ) m context . get application context ( ) . get system service ( context . bluetooth_service ) ; m bluetooth adapter = bluetooth manager . get adapter ( ) ; if ( m bluetooth adapter == null ) { log manager . w ( tag @$ __str__ ) ; } },initializes bluetooth adapter
first deployment should only have two available xml <PLACE_HOLDER>,validate job xml names ( deployment_name_1 @$ __str__ @$ __str__ ) ;,deployment have descriptors
the immersive mode confirmation should never affect the system bar <PLACE_HOLDER> @$ otherwise it will unhide the navigation bar and hide itself .,if ( win candidate . get attrs ( ) . token == m immersive mode confirmation . get window token ( ) ) { final boolean last focus can receive keys = ( m last focused window != null && m last focused window . can receive keys ( ) ) ; win candidate = is status bar keyguard ( ) ? m status bar : last focus can receive keys ? m last focused window : m top fullscreen opaque window state ; if ( win candidate == null ) { return __num__ ; } },confirmation affect type
we know lo should n't have a hardware <PLACE_HOLDER> or an i pv 4 broadcast <PLACE_HOLDER> .,network interface lo = network interface . get by name ( lo ) ; assert null ( lo . get hardware address ( ) ) ; for ( interface address ia : lo . get interface addresses ( ) ) { assert null ( ia . get broadcast ( ) ) ; },address have address
javax mail incorrectly adds the <PLACE_HOLDER> for the first boundary to the end of the preamble @$ so we trim,assert . assert equals ( javax mail multi partmime reader . _preamble != null ? javax mail multi partmime reader . _preamble . trim ( ) : null @$ expected preamble ) ;,mail adds crlf
currently @$ we get one join <PLACE_HOLDER> @$ but it is easy to change the hard coded number to get more join <PLACE_HOLDER>s for large table joins .,assert ( join order queue . size ( ) == __num__ ) ; assert ( m_join order list . size ( ) == __num__ ) ; m_join order list . add all ( join order queue ) ;,one join order
no way to push the current token <PLACE_HOLDER> ...,tokens = new string tokenizer ( l ) ;,way push tree
verify that balancer runs <PLACE_HOLDER> .,ugi . do as ( new privileged exception action < void > ( ) { @ override public void run ( ) throws exception { test unknown datanode ( conf ) ; assert true ( user group information . is login keytab based ( ) ) ; return null ; } } ) ;,balancer runs ok
check if the zone actually uses daylight saving <PLACE_HOLDER> around the <PLACE_HOLDER>,if ( tz instanceof basic time zone ) { basic time zone btz = ( basic time zone ) tz ; time zone transition before = btz . get previous transition ( date @$ true ) ; if ( before != null && ( date - before . get time ( ) < dst_check_range ) && before . get from ( ) . getdst savings ( ) != __num__ ) { use standard = false ; } else { time zone transition after = btz . get next transition ( date @$ false ) ; if ( after != null && ( after . get time ( ) - date < dst_check_range ) && after . get to ( ) . getdst savings ( ) != __num__ ),zone uses zone
if keys exist : update <PLACE_HOLDER>,if ( index >= __num__ ) { values . set ( index @$ values . get ( index ) + val ) ; } else { set ( val @$ keys ) ; },keys exist values
not currently used @$ as lengths can extend over more than one <PLACE_HOLDER> i think,int section three length ; int section four length ; int section five length ; int section six length ;,lengths extend row
try to insert element that causes file <PLACE_HOLDER> @$ but fail,long file length before expansion = file . length ( ) ; broken random access file braf = new broken random access file ( file @$ __str__ ) ; queue = new queue file ( braf ) ; try { queue . add ( values [ max ] ) ; fail ( ) ; } catch ( io exception e ) { },causes file expansion
send a bluetooth restart <PLACE_HOLDER>,message restart msg = m handler . obtain message ( message_restart_bluetooth_service ) ; m handler . send message delayed ( restart msg @$ get service restart ms ( ) ) ;,bluetooth restart message
read nomad heron executor start up <PLACE_HOLDER> from file,string heron nomad script = get heron nomad script ( this . local config ) ; task . set name ( task name ) ;,executor start script
the output dot <PLACE_HOLDER> for visualization,set < atn state > marked states = new hash set < atn state > ( ) ; st dot = stlib . get instance of ( __str__ ) ; dot . add ( __str__ @$ start state . state number ) ; dot . add ( __str__ @$ rankdir ) ; list < atn state > work = new linked list < atn state > ( ) ; work . add ( start state ) ; while ( ! work . is empty ( ) ) { atn state s = work . get ( __num__ ) ; if ( marked states . contains ( s ) ) { work . remove ( __num__ ) ; continue ; } marked states . add ( s ) ;,output dot text
adders should add all <PLACE_HOLDER>,set < value meta interface > metas = new hash set < value meta interface > ( row meta . get value meta list ( ) ) ; for ( adder adder : adders ) { execution result < list < value meta interface > > result = ( execution result < list < value meta interface > > ) results . get ( adder ) ; for ( value meta interface meta : result . get result ( ) ) { assert true ( meta . get name ( ) @$ metas . remove ( meta ) ) ; } } assert equals ( searchers amount @$ metas . size ( ) ) ;,adders add meta
python 2 has <PLACE_HOLDER> and python 3 do n't have <PLACE_HOLDER>,assert true ( result . message ( ) . get ( __num__ ) . get data ( ) . contains ( __str__ ) || result . message ( ) . get ( __num__ ) . get data ( ) . contains ( __str__ ) ) ;,python has u
no rules fired this <PLACE_HOLDER> @$ so we know this is 0,returned fire count = __num__ ;,rules fired time
the subscription should receive a <PLACE_HOLDER> with the reply to property set .,stomp frame received = responder . receive ( ) ; assert not null ( received ) ; string remote reply to = received . get headers ( ) . get ( stomp . headers . send . reply_to ) ; assert not null ( remote reply to ) ; assert true ( remote reply to . starts with ( string . format ( __str__ @$ type ) ) ) ; log . info ( string . format ( __str__ @$ received . get action ( ) @$ stomp . headers . send . reply_to @$ remote reply to ) ) ;,subscription receive response
in order to avoid going over max <PLACE_HOLDER> i may need to steal from myself even though other pools have free <PLACE_HOLDER> . so figure out how much each group should provide,int nodes needed from others = math . min ( math . min ( max nodes - used nodes @$ nodes from others available ) @$ nodes needed ) ; int nodes needed from us = nodes needed - nodes needed from others ; log . debug ( __str__ @$ nodes needed from us @$ nodes needed from others ) ; if ( nodes needed from us > nodes from us available ) { cluster . set status ( top id @$ __str__ ) ; return __num__ ; },pools have nodes
append the work unit file <PLACE_HOLDER> to the job input file,throw closer . rethrow ( t ) ;,unit file path
descend from the solution set delta . check that it depends on both the workset and the solution set . if it does depend on both @$ this descend should create both <PLACE_HOLDER>,iter . get solution set delta ( ) . accept ( recursive creator ) ; final workset node workset node = ( workset node ) recursive creator . con2node . get ( iter . get workset ( ) ) ; if ( workset node == null ) { throw new compiler exception ( __str__ + __str__ ) ; } iter . get next workset ( ) . accept ( recursive creator ) ; solution set node solution set node = ( solution set node ) recursive creator . con2node . get ( iter . get solution set ( ) ) ; if ( solution set node == null || solution set node . get outgoing connections ( ) == null || solution set node . get outgoing connections,descend create nodes
check the <PLACE_HOLDER> of the first buffer and record it . all further buffers must have the same <PLACE_HOLDER> . the <PLACE_HOLDER> must also be a power of 2,this . total num buffers = memory . size ( ) ; if ( this . total num buffers < min_required_buffers ) { throw new illegal argument exception ( __str__ + min_required_buffers + __str__ ) ; } this . segment size = memory . get ( __num__ ) . size ( ) ; this . record size = serializer . get length ( ) ; this . num key bytes = this . comparator . get normalize key len ( ) ;,buffers have size
the set contains the incorrect <PLACE_HOLDER> @$ i.e . the ones without hyphen,line = line . replace ( __str__ @$ __str__ ) ;,set contains letters
allocate the trie 2 index array . if the data width is 16 bits @$ the array also includes the <PLACE_HOLDER> for the data .,int index array size = this . index length ; if ( width == value width . bits_16 ) { index array size += this . data length ; },array includes length
triggering the evaluation twice will satisfy the entry <PLACE_HOLDER> for b,assert equals ( __num__ @$ runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state active ( ) . count ( ) ) ; string url = build url ( cmmn rest urls . url_case_instance @$ case instance . get id ( ) ) ; http put http put = new http put ( url ) ; http put . set entity ( new string entity ( __str__ ) ) ; execute request ( http put @$ http status . sc_ok ) ; assert equals ( __num__ @$ runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ),evaluation satisfy criterion
withdrawing <PLACE_HOLDER> 1 's suggestion should leave <PLACE_HOLDER> 2 as the new winner . since the zone id is different @$ the time zone setting should be updated if the score is high enough .,script . suggest phone time zone ( empty phone1 suggestion ) ; if ( test case . expected score >= phone_score_usage_threshold ) { script . verify time zone set and reset ( zone phone2 suggestion ) ; } else { script . verify time zone not set ( ) ; },suggestion leave phone
insert core <PLACE_HOLDER> @$ a @$ b @$ thing . update the <PLACE_HOLDER> on don b,assert equals ( __num__ @$ cwm . getdeletes ( ) ) ; assert equals ( __num__ @$ cwm . get inserts ( ) ) ; assert equals ( __num__ @$ cwm . get updates ( ) ) ; cwm . reset ( ) ; fact handle handle = ksession . insert ( __str__ ) ; ksession . fire all rules ( ) ;,bean update bean
this is an open id connect authentication request so return <PLACE_HOLDER> and let oidc authorization code authentication provider handle it instead,if ( authorization code authentication . get authorization exchange ( ) . get authorization request ( ) . get scopes ( ) . contains ( __str__ ) ) { return null ; },request return null
res file p ctx root tasks fetch task analyzer explain config cbo <PLACE_HOLDER> @$,explain work work = new explain work ( null @$ null @$ plan . get root tasks ( ) @$ plan . get fetch task ( ) @$ null @$ null @$ config @$ plan . get cbo info ( ) @$ plan . get optimized query string ( ) @$ plan . get optimizedcbo plan ( ) ) ;,analyzer explain plan
we make sure the main fragment observes the <PLACE_HOLDER> . this will also trigger the <PLACE_HOLDER> to update itself .,usernames repository . add updatable ( this ) ;,fragment observes listener
we need to copy before iterating because <PLACE_HOLDER> can add other <PLACE_HOLDER>,array list < lifecycle > lifecycles = new array list < > ( deployment . get lifecycle objects ( ) ) ; for ( lifecycle object : lifecycles ) { object . start ( ) ; } http handler root = deployment . get handler ( ) ; final tree map < integer @$ list < managed servlet > > load on startup = new tree map < > ( ) ; for ( map . entry < string @$ servlet handler > entry : deployment . get servlets ( ) . get servlet handlers ( ) . entry set ( ) ) { managed servlet servlet = entry . get value ( ) . get managed servlet ( ) ; integer load on startup number = servlet,listeners add resources
if the source node and result node are the same use the dom result augmentor . otherwise use the dom result <PLACE_HOLDER> .,if ( source . get node ( ) == node result ) { fdom validator handler = fdom result augmentor ; fdom result augmentor . setdom result ( result ) ; f schema validator . set document handler ( fdom result augmentor ) ; return ; } if ( result . get node ( ) == null ) { try { document builder factory factory = jdk xml utils . getdom factory ( f component manager . get feature ( jdk xml utils . override_parser ) ) ; document builder builder = factory . new document builder ( ) ; result . set node ( builder . new document ( ) ) ; } catch ( parser configuration exception e ) { throw new sax exception ( e,node use handler
in this case first call to end offsets returns correct <PLACE_HOLDER> @$ but a second thread has updated the source topic but since it 's a source topic @$ the second check should not fire hence no exception,consumer . add end offsets ( collections . singleton map ( topic partition @$ __num__ ) ) ; changelog reader . register ( new state restorer ( topic partition @$ restore listener @$ null @$ __num__ @$ true @$ __str__ @$ identity ( ) ) ) ; expect ( active . restoring task for ( topic partition ) ) . and return ( task ) ; replay ( active ) ; changelog reader . restore ( active ) ;,offsets returns values
find all configurations where the key contains any <PLACE_HOLDER> from hidden set,iterable < map . entry < string @$ string > > matching = iterables . filter ( conf @$ conf entry -> { for ( string name : hidden set ) { if ( conf entry . get key ( ) . starts with ( name ) ) { return true ; } } return false ; } ) ;,key contains characters
hregion calls this <PLACE_HOLDER>,verify ( wal @$ times ( __num__ ) ) . sync ( any long ( ) ) ;,hregion calls one
check if the dataset version contains the correct modified <PLACE_HOLDER> of the underlying data location,assert . assert true ( dataset version . get date time ( ) . get millis ( ) == long . value of ( timestamp ) ) ; system . out . println ( dataset version ) ;,version contains timestamp
force verify that the destination fs matches the <PLACE_HOLDER>,fs . make qualified ( output ) ; sequence file . sorter sorter = new sequence file . sorter ( fs @$ text . class @$ copy listing file status . class @$ conf ) ; fs . delete ( output @$ false ) ; sorter . sort ( source listing @$ output ) ;,fs matches input
a document with canonical tag should not get a webgraph <PLACE_HOLDER> @$ because that belongs to the canonical document,if ( webgraph != null && ( ! contains canonical || ( canonical_equal_sku != null && ( canonical_equal_sku . boolean value ( ) ) ) ) ) { list < solr input document > edges = webgraph . get edges ( subgraph @$ digesturl @$ response header @$ collections @$ crawldepth @$ process types @$ document . get hyperlinks ( ) . key set ( ) @$ source name ) ; doc . webgraph documents . add all ( edges ) ; } else { if ( all attr || contains ( collection schema . inboundlinks_protocol_sxt ) || contains ( collection schema . inboundlinks_urlstub_sxt ) || contains ( collection schema . inboundlinks_anchortext_txt ) || contains ( collection schema . outboundlinks_protocol_sxt ) || contains ( collection schema . outboundlinks_urlstub_sxt,document get edge
all honest parties should detect <PLACE_HOLDER>,boolean thrown = false ; try { run application ( test application ) ; } catch ( exception e ) { assert true ( e . get cause ( ) instanceof malicious exception ) ; thrown = true ; } assert true ( __str__ @$ thrown ) ;,parties detect issues
the second byte buffer set to null will throw an <PLACE_HOLDER>,client . new request ( __str__ @$ connector . get local port ( ) ) . scheme ( scenario . get scheme ( ) ) . content ( new content provider ( ) { @ override public long get length ( ) { return - __num__ ; } @ override public iterator < byte buffer > iterator ( ) { return new iterator < byte buffer > ( ) { @ override public boolean has next ( ) { return true ; } @ override public byte buffer next ( ) { throw new no such element exception ( __str__ ) ; } @ override public void remove ( ) { throw new unsupported operation exception ( ) ; } } ; } } ) . send (,buffer throw exception
compute the new escaped value if the new property value does n't match the previous <PLACE_HOLDER>,if ( g user diruri != null && user dir . equals ( g user dir ) ) { return g user diruri ; },value match one
get drop location will throw an illegal state <PLACE_HOLDER> .,if ( ! support . is drop ( ) || ! ( support . get component ( ) instanceof types tree ) ) { return true ; } final j tree . drop location location = ( j tree . drop location ) support . get drop location ( ) ;,location throw exception
whether the epoch is still valid . note that the runtime guarantees sufficient <PLACE_HOLDER> of java thread pointers to allow age to be placed into low bits .,final word biasable lock bits = mark . and ( biased lock mask in place ( injected_vmconfig ) ) ;,runtime guarantees use
the retransform native code that called this method does not propagate <PLACE_HOLDER> . instead of getting an uninformative generic error @$ catch problems here and print it @$ then exit .,e . print stack trace ( ) ; system . exit ( __num__ ) ;,method propagate exceptions
reopen ec file @$ get the new <PLACE_HOLDER> .,lb = dfs . get client ( ) . get located blocks ( ec file . to string ( ) @$ __num__ ) . get ( __num__ ) ; lsb = ( located striped block ) lb ; datanode info [ ] new dn locs = lsb . get locations ( ) ;,ec file nodes
all tiers have <PLACE_HOLDER> so the evictor will evict till low watermark for all tiers,long unreserved tier1 = math . round ( capacity_bytes * low_watermark ) ; long unreserved tier2 = math . round ( capacity_bytes * low_watermark ) ; long unreserved tier3 = math . round ( capacity_bytes * low_watermark ) ; for ( int i = __num__ ; i < files_per_tier ; i ++ ) { write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ i * file_size @$ i * file_size ) ; write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ ( i + __num__ ) * file_size @$ i * file_size ) ; write file and check usage ( __num__ @$ ( i + __num__ ) * file_size @$ ( i + __num__,tiers have capacity
the query returns 4 @$ index 1 has the revision <PLACE_HOLDER> which we do n't test here,assert equals ( __num__ @$ row . length ) ;,4 has number
these return a json <PLACE_HOLDER> which describes if and where the query was found . this api may break or disappear at any time in the future . since this is an api call rather than a website @$ we do n't use locale manager to change the tld .,try { string the query = args [ __num__ ] ; string the isbn = args [ __num__ ] ; string uri ; if ( locale manager . is book search url ( the isbn ) ) { int equals = the isbn . index of ( __str__ ) ; string volume id = the isbn . substring ( equals + __num__ ) ; uri = __str__ + volume id + __str__ + the query ; } else { uri = __str__ + the isbn + __str__ + the query ; } char sequence content = http helper . download via http ( uri @$ http helper . content type . json ) ; return new json object ( content . to string ( ) ) ; },these return map
might go out of sync with queue here @$ but should be minor slippage . will not accumulate <PLACE_HOLDER> either @$ but reset on every clear .,size . set ( __num__ ) ;,slippage accumulate capacity
if the remote object implements the unreferenced <PLACE_HOLDER> @$ invoke its unreferenced callback in a separate thread .,remote obj = get impl ( ) ; if ( obj instanceof unreferenced ) { final unreferenced unref obj = ( unreferenced ) obj ; access controller . do privileged ( new new thread action ( ( ) -> { thread . current thread ( ) . set context class loader ( ccl ) ; access controller . do privileged ( ( privileged action < void > ) ( ) -> { unref obj . unreferenced ( ) ; return null ; } @$ acc ) ; } @$ __str__ + next thread num ++ @$ false @$ true ) ) . start ( ) ; },object implements interface
expected exception . we used to expect that there would be un<PLACE_HOLDER>ed appends but this not reliable now that <PLACE_HOLDER> plays a roll in wall rolling . the above puts also now call <PLACE_HOLDER> .,log . error ( h base markers . fatal @$ __str__ @$ t ) ;,puts call coll
alice can discover <PLACE_HOLDER>,jenkins rule . web client wc = j . create web client ( ) . login ( __str__ ) ; try { wc . go to ( __str__ ) ; assert . fail ( __str__ ) ; } catch ( failing http status code exception e ) { assert . assert equals ( __str__ @$ __num__ @$ e . get status code ( ) ) ; },alice discover 
write out the keep field and keep method <PLACE_HOLDER> @$ if any .,if ( class specification . field specifications != null || class specification . method specifications != null ) { writer . print ( __str__ ) ; writer . println ( configuration constants . open_keyword ) ; write field specification ( class specification . field specifications ) ; write method specification ( class specification . method specifications ) ; writer . println ( configuration constants . close_keyword ) ; } else { writer . println ( ) ; },the keep lists
the listener returns the new set of <PLACE_HOLDER>s to listen to . because 0 means no <PLACE_HOLDER> @$ the listener gets unregistered .,return __num__ ;,0 means listener
make sure eviction thread has entered run <PLACE_HOLDER>,while ( ! eviction thread . is entering run ( ) ) { thread . sleep ( __num__ ) ; },thread entered phase
we should not check the hash code of asset index <PLACE_HOLDER> since this <PLACE_HOLDER> is not consistent and mojang will modify this <PLACE_HOLDER> anytime . so asset index.hash might be outdated .,dependencies . add ( new file download task ( network utils . tourl ( dependency manager . get download provider ( ) . injecturl ( asset index info . get url ( ) ) ) @$ asset index file ) . set cache repository ( dependency manager . get cache repository ( ) ) ) ;,mojang modify file
use the caller passed <PLACE_HOLDER> in erased indexes positions,for ( int output idx = __num__ @$ i = __num__ ; i < decoding state . erased indexes . length ; i ++ ) { boolean found = false ; for ( int j = __num__ ; j < erased or not to read indexes . length ; j ++ ) { if ( decoding state . erased indexes [ i ] == erased or not to read indexes [ j ] ) { found = true ; adjusted direct buffer outputs parameter [ j ] = coder util . reset buffer ( decoding state . outputs [ output idx ++ ] @$ data len ) ; } } if ( ! found ) { throw new hadoop illegal argument exception ( __str__ ) ; } },caller passed buffers
only return true if the list items cover the entire <PLACE_HOLDER> of the view,if ( ret value ) { final int list top = m list padding != null ? m list padding . top : m padding top ; view first = get child at ( __num__ ) ; if ( first == null || first . get top ( ) > list top ) { return false ; } final int list bottom = get height ( ) - ( m list padding != null ? m list padding . bottom : m padding bottom ) ; view last = get child at ( get child count ( ) - __num__ ) ; if ( last == null || last . get bottom ( ) < list bottom ) { return false ; } },items cover width
interceptor is null @$ call super <PLACE_HOLDER> .,code . mark ( interceptor null case ) ; code . invoke super ( super method @$ null @$ local this @$ local canvas ) ; code . return void ( ) ; final method id < g @$ void > calls super method = generated type . get method ( void_type @$ view method . get invoke name ( ) @$ canvas_type ) ; final code super code = dex maker . declare ( calls super method @$ public ) ; final local < g > super this = super code . get this ( generated type ) ; final local < canvas > super local canvas = super code . get parameter ( __num__ @$ canvas_type ) ; super code . invoke super ( super method,interceptor call method
these nodes have no interesting type behavior . these nodes require data flow <PLACE_HOLDER> .,case param_list : case string_key : case member_function_def : case computed_prop : case label : case label_name : case switch : case break : case catch : case try : case script : case module_body : case export : case export_spec : case export_specs : case import : case import_spec : case import_specs : case import_star : case expr_result : case block : case root : case empty : case default_case : case continue : case debugger : case throw : case do : case if : case while : case for : case templatelit_sub : case iter_rest : case object_rest : case destructuring_lhs : typeable = false ; break ; case array_pattern : ensure typed ( n ) ; validator . expect autoboxes to iterable ( n,nodes require policy
test that an applied filter will include only those <PLACE_HOLDER> chosen to pass the filter,vt match tag foo tag = new test match tag ( __str__ ) ; vt match tag bar tag = new test match tag ( __str__ ) ; vt match tag baz tag = new test match tag ( __str__ ) ; vt session session = controller . get session ( ) ; list < vt match set > match sets = session . get match sets ( ) ; vt match set match set = match sets . get ( __num__ ) ; collection < vt match > matches = match set . get matches ( ) ; list < vt match > matches list = new array list < > ( matches ) ; vt match foo tag match1 = matches list . get ( __num__,filter include matches
this account type does n't have <PLACE_HOLDER> lock provider . lock resources by current account,if ( resource lock key provider == null ) { lock key = account id ; } else { lock key = resource lock key provider . get lock key ( account id ) ; },type have resource
all fields may not be comparable so only compare the ones that can be compared completion time is set when a node is finalized @$ so that can not be compared if the node is inprogress @$ do n't compare the last tx <PLACE_HOLDER> either,if ( this . get log segment sequence number ( ) != other . get log segment sequence number ( ) || this . log segment id != other . log segment id || this . first tx id != other . first tx id ) { ret val = false ; } else if ( this . inprogress ) { ret val = other . inprogress ; } else { ret val = ( ! other . inprogress && ( this . last tx id == other . last tx id ) ) ; } if ( ! ret val ) { log . warn ( __str__ @$ this @$ other ) ; } return ret val ;,ones compare id
if we have <PLACE_HOLDER> regexes @$ the first one has 1 match @$ second has 12 matches @$ third has <PLACE_HOLDER> matches @$ then we have <PLACE_HOLDER>6 combinations of matches @$ thus <PLACE_HOLDER>6 outlinks to extracted .,int num outlinks = __num__ ; for ( match list match list : match lists . values ( ) ) { num outlinks *= match list . size ( ) ; } string [ ] regex names = match lists . key set ( ) . to array ( new string [ __num__ ] ) ; for ( int i = __num__ ; i < num outlinks ; i ++ ) { map < string @$ object > bindings = make bindings ( match lists @$ regex names @$ i ) ; build and add outlink ( curi @$ bindings ) ; },one has 3
no sub<PLACE_HOLDER> is open @$ so click only once to open the file <PLACE_HOLDER> .,menu bar . click item ( __str__ ) ; assert true ( is item visible ( __str__ ) ) ;,submenu open menu
explicit support for auto 'this ' parameter must inject pointer <PLACE_HOLDER> to obtain storage assignment,if ( add auto params && has this ) { inject auto this param = true ; data type [ ] ammended types = new data type [ data types . length + __num__ ] ; ammended types [ __num__ ] = data types [ __num__ ] ; ammended types [ __num__ ] = new pointer data type ( program . get data type manager ( ) ) ; if ( data types . length > __num__ ) { system . arraycopy ( data types @$ __num__ @$ ammended types @$ __num__ @$ data types . length - __num__ ) ; } data types = ammended types ; },support inject type
note : just calling this method may trigger the awt <PLACE_HOLDER> to get created,return swing utilities . is event dispatch thread ( ) ;,method trigger thread
these might trigger <PLACE_HOLDER> .,if ( ! activemq client header . get sampled ( message @$ true ) ) { return trace context . disable sampling ( ) ; } final trace id trace id = populate trace id from request ( message ) ; final trace trace = trace id == null ? trace context . new trace object ( ) : trace context . continue trace object ( trace id ) ; if ( trace . can sampled ( ) ) { span recorder recorder = trace . get span recorder ( ) ; record root span ( recorder @$ target @$ args ) ; } return trace ;,these trigger samples
we may have changed size @$ so let 's constrain the <PLACE_HOLDER> and bottom offset correctly @$ just in case we 're out of the bounds,set top and bottom offset ( math utils . clamp ( get top and bottom offset ( ) @$ - abl . get total scroll range ( ) @$ __num__ ) ) ;,'s constrain top
override the reporter with our own which collates the allocation <PLACE_HOLDER> .,close guard . set reporter ( new reporter ( ) { @ override public void report ( string message @$ throwable allocation site ) { close guard allocation sites . add ( allocation site ) ; } } ) ;,which collates sites
when opearting with groups . the group must have entries so changes to take <PLACE_HOLDER> . otherwise group will be lost after loggingout,try { this . op set pers presence1 . subscribe ( group @$ this . fixture . userid2 ) ; synchronized ( o ) { o . wait ( __num__ ) ; } } catch ( exception ex ) { fail ( __str__ + group . get group name ( ) + __str__ + ex . get message ( ) ) ; },changes take effect
get new display metrics based on the display adjustments given to the resources impl . update a <PLACE_HOLDER> if the compatibility info changed @$ because the resources impl object will handle the update internally .,display adjustments daj = r . get display adjustments ( ) ; if ( compat != null ) { daj = new display adjustments ( daj ) ; daj . set compatibility info ( compat ) ; } dm = get display metrics ( display id @$ daj ) ; if ( ! is default display ) { apply non default display metrics to configuration ( dm @$ tmp config ) ; } if ( has override configuration ) { tmp config . update from ( key . m override configuration ) ; } r . update configuration ( tmp config @$ dm @$ compat ) ; r . update configuration ( config @$ dm @$ compat ) ;,metrics update configuration
calling traverse node here would create infinite <PLACE_HOLDER> for a function declaration,if ( node . is function ( ) ) { traverse function ( node @$ scope ) ; } else { traverse node ( node @$ scope ) ; },node create recursion
the typed setters for these can not accept <PLACE_HOLDER> as input .,realm . cancel transaction ( ) ;,setters accept null
this weirdness of setting it in our conf and then reading back does two <PLACE_HOLDER> . one @$ it handles the conversion of the time unit . two @$ it keeps the value around for later in case we need it again .,if ( key . equals ( conf vars . event_db_listener_clean_interval . to string ( ) ) || key . equals ( conf vars . event_db_listener_clean_interval . get hive name ( ) ) ) { long time = metastore conf . convert time str ( table event . get new value ( ) @$ time unit . seconds @$ time unit . seconds ) ; metastore conf . set time var ( get conf ( ) @$ metastore conf . conf vars . event_db_listener_clean_interval @$ time @$ time unit . seconds ) ; cleaner . set cleanup interval ( metastore conf . get time var ( get conf ( ) @$ metastore conf . conf vars . event_db_listener_clean_interval @$ time unit . milliseconds ) ) ; },weirdness does things
user can specify a <PLACE_HOLDER> to be used to ex<PLACE_HOLDER> rmi object @$ in order to simplify firewall rules if <PLACE_HOLDER> is not specified random one will be allocated .,int rmi port = __num__ ; string rmi port str = props . get property ( property names . rmi_port ) ; try { if ( rmi port str != null ) { rmi port = integer . parse int ( rmi port str ) ; } } catch ( number format exception x ) { throw new agent configuration error ( invalid_jmxremote_rmi_port @$ x @$ rmi port str ) ; } if ( rmi port < __num__ ) { throw new agent configuration error ( invalid_jmxremote_rmi_port @$ rmi port str ) ; },user specify port
all location settings are satisfied . the client can initialize location <PLACE_HOLDER> here .,task . add on success listener ( this @$ new on success listener < location settings response > ( ) { @ override public void on success ( location settings response location settings response ) { if ( check self permission ( android . manifest . permission . access_fine_location ) == package manager . permission_granted ) { m location callback = get location callback ( ) ; m location client . request location updates ( m location request @$ m location callback @$ null ) ; executors . new scheduled thread pool ( __num__ ) . schedule ( new runnable ( ) { @ override public void run ( ) { m location client . remove location updates ( m location callback ) ; } } @$ measure_time,client initialize information
only one web node should do the <PLACE_HOLDER> in db to avoid any collision,if ( ! lock manager . try lock ( lock_name @$ lock_duration_in_second ) ) { return ; } db client . internal properties dao ( ) . save ( db session @$ projects_in_warning_internal_property @$ long . to string ( nb projects in warning ) ) ; db session . commit ( ) ;,node do locks
index name <PLACE_HOLDER> .,final int len = page utils . get unsigned byte ( page addr @$ off ) & __num__ ; off ++ ;,index name length
the wallpaper has real ultimate <PLACE_HOLDER> @$ but we want to tell it about the overscan area .,df . set ( display frames . m overscan ) ; pf . set ( display frames . m overscan ) ; cf . set ( display frames . m unrestricted ) ; of . set ( display frames . m unrestricted ) ;,wallpaper has power
create one build <PLACE_HOLDER> to generate build config.java .,build rule params build config params = params ; optional < build rule > values file rule = values file . flat map ( graph builder :: get rule ) ; if ( values file rule . is present ( ) ) { build config params = build config params . copy appending extra deps ( values file rule . get ( ) ) ; } android build config android build config = new android build config ( build config build target @$ project filesystem @$ build config params @$ java package @$ values @$ values file @$ use constant expressions ) ; graph builder . add to index ( android build config ) ;,one build rule
even if some members have fallen behind . the config offset used to generate the <PLACE_HOLDER> is included in the response so members that have fallen behind will not use the <PLACE_HOLDER> until they have caught up .,long max offset = null ; for ( map . entry < string @$ extended worker state > state entry : member configs . entry set ( ) ) { long member root offset = state entry . get value ( ) . offset ( ) ; if ( max offset == null ) max offset = member root offset ; else max offset = math . max ( max offset @$ member root offset ) ; } log . debug ( __str__ @$ max offset @$ coordinator . config snapshot ( ) . offset ( ) ) ; return max offset ;,members use log
test compatibility with old uri properties bytes : client can understand <PLACE_HOLDER> published by old servers,string old uri json = __str__ ; uri properties from old bytes = json serializer . from bytes ( old uri json . get bytes ( ) ) ; uri properties created new = get instance with old arguments ( __str__ @$ uri weights ) ; assert equals ( from old bytes @$ created new ) ;,client understand rows
used when the parent view intercepts <PLACE_HOLDER> for things like scrolling,case motion event . action_cancel :,view intercepts events
incremental publish should happen every 3 <PLACE_HOLDER>,max rows per segment = integer . max_value ; max total rows = __num__ ;,publish happen records
sure we need to expand collection only if collection size more than <PLACE_HOLDER> @$ otherwise collection of composite keys already contains original composite key,if ( ! contains collection ) for ( int i = __num__ ; i < collection size ; i ++ ) { final o composite key composite key = new o composite key ( first key . get keys ( ) ) ; composite keys . add ( composite key ) ; } else throw new o index exception ( __str__ ) ;,collection size 1
checkstyle counts line <PLACE_HOLDER> from 0 but ide from 1,for ( integer line no : empty lines to log ) { log ( line no + __num__ @$ msg_multiple_lines_inside ) ; },checkstyle counts 0
add singleton getter adds a get instance <PLACE_HOLDER> to a class .,test same ( __str__ ) ;,a get method
no messages @$ do n't continue <PLACE_HOLDER>,return false ;,messages continue loop
increment a counter again and check that the existing view was not modified @$ but a new view shows the updated <PLACE_HOLDER> .,increment counter ( startup progress @$ loading_edits @$ loading edits file @$ __num__ ) ; startup progress . end step ( loading_edits @$ loading edits file ) ; startup progress . end phase ( loading_edits ) ; assert equals ( __num__ @$ view . get count ( loading_edits @$ loading edits file ) ) ; view = startup progress . create view ( ) ; assert not null ( view ) ; assert equals ( __num__ @$ view . get count ( loading_edits @$ loading edits file ) ) ;,view shows edit
downgrade priority as user is disconnecting the hearing <PLACE_HOLDER> .,if ( m service . get priority ( device ) > bluetooth profile . priority_on ) { m service . set priority ( device @$ bluetooth profile . priority_on ) ; } return m service . disconnect ( device ) ;,priority disconnecting power
one host matches this name <PLACE_HOLDER>,final string name pattern = __str__ ; when ( model . list hosts ( name pattern ) ) . then return ( immutable list . of ( __str__ ) ) ; final job id job id1 = job id . parse ( __str__ ) ; final job id job id2 = job id . parse ( __str__ ) ; final job job1 = job . new builder ( ) . build ( ) ; final job job2 = job . new builder ( ) . build ( ) ;,host matches patterns
if the final number of <PLACE_HOLDER>s to scroll ends up being 0 @$ the view should still scroll at least one <PLACE_HOLDER> .,return capped scroll step != __num__ ? capped scroll step : direction ;,view scroll step
we always convert back to byte array @$ since we store it and field only supports <PLACE_HOLDER> so @$ we might as well do it here @$ and improve the performance of working with direct byte arrays,this . source = new bytes array ( objects . require non null ( source ) . to bytes ref ( ) ) ; this . x content type = objects . require non null ( x content type ) ; this . routing = routing ;,field supports maps
case 1 : ensure that workers that are backed off are only executed when they are supposed to . greedy scheduler can schedule work <PLACE_HOLDER> that have already been backed off because it is holding on to snapshots of work <PLACE_HOLDER> . so worker wrapper needs to determine if the listenable worker is actually eligible to execute at this point in time .,return ;,case schedule specs
no <PLACE_HOLDER>s between the focused <PLACE_HOLDER> and the window is actually interested by the key event . let 's try the other j <PLACE_HOLDER> in this window .,if ( parent != null ) { return j component . process key bindings for all components ( e @$ parent @$ pressed ) ; } return false ;,'s try component
values of lists must be accumulated as object <PLACE_HOLDER> objects under the value key . will return as a array <PLACE_HOLDER> . called recursively to traverse the entire object graph of each item in the array .,if ( type . equals ( graphson tokens . type_list ) ) { array node list = ( array node ) value ; array node value array = value and type . put array ( graphson tokens . value ) ; for ( int ix = __num__ ; ix < list . size ( ) ; ix ++ ) { add object ( value array @$ get value ( get typed value from json node ( list . get ( ix ) ) @$ include type ) ) ; } } else if ( type . equals ( graphson tokens . type_map ) ) { object node converted map = json node factory . object node ( ) ; object node json object = ( object node ),values return node
using property would change <PLACE_HOLDER> :,composite drawable . child drawable . bottom_absolute . set ( parent drawable . get child at ( __num__ ) @$ __num__ ) ; composite drawable . child drawable . bottom_fraction . set ( parent drawable . get child at ( __num__ ) @$ __num__ ) ; parent drawable . update bounds ( bounds ) ; adjusted bounds = drawable . get bounds ( ) ; expected bounds = new rect ( bounds ) ; expected bounds . top = - __num__ ; expected bounds . bottom = ( int ) ( __num__ * height ) ; assert equals ( expected bounds @$ adjusted bounds ) ;,property change size
same document content . leave the data as is in tracking <PLACE_HOLDER> @$ and continue tracking previous attempts .,if ( array utils . slice equals ( document . bytes ( ) @$ document . offset ( ) @$ document . length ( ) @$ retry data buffer @$ __num__ @$ retry data buffer . length ) ) { retries . add ( previous attempt ) ; tracking bytes position ++ ; } else { bytes ref new entry = validate edited entry ( retry data buffer ) ; data . remove ( tracking bytes position ) ; data . copy from ( new entry ) ; if ( ba . available ( ) < new entry . length ( ) ) { tracking array expanded = true ; } previous attempt . attempt number = __num__ ; new document retries . add ( previous attempt ),content tracking buffer
indentation should probably be dealt with before because an indentation has <PLACE_HOLDER> also on the following lines,if ( ! ( element instanceof csm indent ) && ! ( element instanceof csm unindent ) ) { throw new unsupported operation exception ( element . get class ( ) . get simple name ( ) ) ; },indentation has activity
at least one pattern includes a path definition : we must use the inet path access handler as inet access handler does n't support path <PLACE_HOLDER>,if ( white . contains ( __str__ ) ) { white list handler = new inet path access handler ( ) ; } else { white list handler = new inet access handler ( ) ; },handler support separators
some more horrible functions . drop the passed function and return a <PLACE_HOLDER>,tree = __str__ ; check tree ( tree ) ;,functions function tree
check the buffer has an <PLACE_HOLDER> of the right size .,assert true ( b . has array ( ) ) ; byte [ ] array = b . array ( ) ; assert true ( array . length >= b . capacity ( ) ) ; assert equals ( __num__ @$ b . capacity ( ) ) ;,buffer has array
only at least one metrics exporter implement had imported in pom then need register metrics <PLACE_HOLDER>,if ( exporters . size ( ) != __num__ ) { exporters . for each ( exporter -> exporter . set registry ( registry ) ) ; event bus manager . get ( ) . register ( new metrics subscriber ( registry ) ) ; },metrics register subreporter
sleep for a while so the user can inspect the <PLACE_HOLDER> . once the sleep is finished @$ tool and <PLACE_HOLDER> will be closed .,long minutes in millis = __num__ * __num__ * duration ; sleep ( minutes in millis ) ; throw new assertion failed error ( __str__ ) ; env . dispose ( ) ;,user inspect tool
verify also the blocking states dao adds <PLACE_HOLDER> not on disk,check blocking statesdao ( changed base entitlement @$ add on entitlement @$ base effective cancellation or change date @$ false ) ;,dao adds events
make sure that the tab strips fills this <PLACE_HOLDER>,set fill viewport ( ! tab strip . is indicator always in center ( ) ) ; add view ( tab strip @$ layout params . match_parent @$ layout params . match_parent ) ;,strips fills view
use object equality to see if this status is the root placeholder . see the <PLACE_HOLDER> for root placeholder above for more information .,for ( file status status : candidates ) { if ( status == root placeholder ) { status = get file status ( root placeholder . get path ( ) ) ; if ( status == null ) continue ; } if ( filter . accept ( status . get path ( ) ) ) { results . add ( status ) ; } },equality see comment
this call does the right <PLACE_HOLDER> with a null transaction task queue,if ( ! m_complete msg . is restart ( ) ) { do commonspi complete actions ( ) ; log todr ( site connection . getdr gateway ( ) ) ; } else { m_txn state . set begin undo token ( site . k invalid undo token ) ; },call does thing
verify residual : should contain <PLACE_HOLDER> the current tracker did n't claim .,assert null ( residual . get poll watermark ( ) ) ; assert equals ( __num__ @$ residual . get completed ( ) . size ( ) ) ; assert equals ( __num__ @$ ( int ) residual . get termination state ( ) ) ;,tracker claim what
we are not able to retrieve the exact number of cells <PLACE_HOLDER> result cell meta says us . we have to scan for the same results again . throwing dnrioe as a client retry on the same scanner will result in out of order scanner next exception,if ( cell scanner . advance ( ) == false ) { string msg = __str__ + no of results + __str__ + i + __str__ ; log . error ( msg ) ; throw new do not retryio exception ( msg ) ; },meta says which
null should not impact <PLACE_HOLDER>,current min = double min kudaf . aggregate ( null @$ current min ) ; assert that ( __num__ @$ equal to ( current min ) ) ;,null impact result
if the processed currency is different we return it ; otherwise we return null so that template does not print <PLACE_HOLDER> special,return ( processed currency != get currency ( ) ) ? processed currency : null ;,template print anything
create a stream in which to serialize the <PLACE_HOLDER> .,try { byte array output stream ostream = new byte array output stream ( ) ; object output stream p = new object output stream ( ostream ) ; p . write object ( old obj ) ; byte [ ] byte array = ostream . to byte array ( ) ; byte array input stream istream = new byte array input stream ( byte array ) ; object input stream q = new object input stream ( istream ) ; new obj = q . read object ( ) ; } catch ( exception ex ) { ex . print stack trace ( ) ; },which serialize object
even after the security realm deleted the <PLACE_HOLDER> @$ they can still connect @$ until session invalidation,assert user connected ( wc @$ alice ) ; try { request renew seed for user ( alice ) ; fail ( __str__ ) ; } catch ( failing http status code exception e ) { },realm deleted realm
mix in various ways to specify no catalog . use java <PLACE_HOLDER> here .,string dep bytes = new string ( client utils . file to bytes ( new file ( deploymenturl ) ) @$ constants . utf8encoding ) ; volt table [ ] results = client . call procedure ( __str__ @$ null @$ dep bytes ) . get results ( ) ;,mix use api
only the system can access this <PLACE_HOLDER> .,enforce system only ( ) ; return m network scorer app manager . get all valid scorers ( ) ;,system access data
this flag will relax the <PLACE_HOLDER> that the scoped named in this version pragma directive can not resolve to a module .,parser . is module legal type ( true ) ; symtab entry an error occurred = new symtab entry ( ) ; symtab entry entry = parser . scoped name ( parser . current module @$ an error occurred ) ;,flag relax case
note : this next bit changes the tree <PLACE_HOLDER> @$ rather than creating a new tree node . beware !,ht . set label ( lf . new label ( ht . value ( ) + __str__ ) ) ;,bit changes label
user 2 reject the <PLACE_HOLDER> as planned,op setmuc2 . reject invitation ( invitation @$ invitation . get reason ( ) ) ; op set1 collector . wait for event ( __num__ ) ;,user reject invitation
catching throwable here due to the fact that google app engine raises no class <PLACE_HOLDER> found error for unsafe .,return unsafe ;,engine raises def
renumber used <PLACE_HOLDER> and keep track of the last line .,int id = __num__ ; int max line = __num__ ; for ( mapping m : mappings ) { if ( m . used ) { m . id = id ++ ; int end position line = m . end position . get line ( ) ; max line = math . max ( max line @$ end position line ) ; } },renumber used mappings
the new calc <PLACE_HOLDER> must match the original sort <PLACE_HOLDER>,result = calc . copy ( calc . get trait set ( ) . replace ( orig sort collation ) @$ index scan @$ calc . get program ( ) @$ calc . get split count ( ) ) ;,collation match collation
if a user has set <PLACE_HOLDER> in one test @$ and then selects a different test which supports the same <PLACE_HOLDER> @$ those <PLACE_HOLDER> should have the same values that they did in the original test .,if ( curr args map . contains key ( name ) ) { string new val = curr args map . get ( name ) ; if ( new val != null && new val . length ( ) > __num__ ) { value = new val ; } } new args . add argument ( name @$ value ) ;,which supports parameters
caller wants <PLACE_HOLDER> sent back to them .,if ( receiver != null ) { bundle send bundle = new bundle ( ) ; send bundle . put bundle ( assist_key_receiver_extras @$ pae . receiver extras ) ; try { pae . receiver . on handle assist data ( send bundle ) ; } catch ( remote exception e ) { } },caller wants result
check that connection 2 recevied 3 <PLACE_HOLDER>,message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert not null ( __str__ @$ message1 ) ; message1 = ( message ) collector2 . next result ( smack configuration . get packet reply timeout ( ) ) ; assert null ( __str__ @$ message1 ) ;,connection recevied messages
if prev has <PLACE_HOLDER> than curr @$ the remainder will contain the extra current elements,if ( ! curr remainder . is empty ( ) ) { _info map . add rest spec info ( field . get name ( ) @$ compatibility info . type . array_not_equal @$ _info path @$ prev array ) ; return false ; } return true ;,prev has elements
if coprocessor exposes any <PLACE_HOLDER> @$ register them .,for ( service service : instance . get services ( ) ) { region . register service ( service ) ; } concurrent map < string @$ object > class data ;,coprocessor exposes services
freeze failed when freeze amount less than 1 <PLACE_HOLDER>,ret1 = freeze balance2 ( from address @$ __num__ @$ __num__ @$ test key002 ) ; assert . assert equals ( ret1 . get code ( ) @$ grpcapi . return . response_code . contract_validate_error ) ; assert . assert equals ( ret1 . get message ( ) . to string utf8 ( ) @$ __str__ ) ;,freeze amount trx
sequence file.block compress <PLACE_HOLDER>,write test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ codec ) ; read test ( fs @$ count @$ seed @$ block compressed file ) ; sort test ( fs @$ count @$ megabytes @$ factor @$ false @$ block compressed file ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; sort test ( fs @$ count @$ megabytes @$ factor @$ true @$ block compressed file ) ; check sort ( fs @$ count @$ seed @$ block compressed file ) ; merge test ( fs @$ count @$ seed @$ block compressed file @$ compression type . block @$ false @$ factor @$ megabytes ) ; check sort ( fs,file.block compress writer
complete task g w<PLACE_HOLDER>ic<PLACE_HOLDER> s<PLACE_HOLDER>ould start task <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name ( plan item instances @$ __str__ ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active @$ waiting_for_repetition ) ;,which start a
metadata objects now write their descriptor <PLACE_HOLDER>,if ( cont type != container type . extended_content ) { out . write ( utils . get bytes ( get name ( ) @$ asf header . asf_charset ) ) ; out . write ( asf header . zero_term ) ; },objects write metadata
verify the conflation indexes map equals the <PLACE_HOLDER> of updates,verify conflation indexes size ( __str__ @$ __num__ @$ vm4 @$ vm5 @$ vm6 @$ vm7 ) ; vm4 . invoke ( ( ) -> wan test base . check queue size ( __str__ @$ key values . size ( ) + update key values . size ( ) ) ) ;,map equals number
ensure that evaluation succeeds if <PLACE_HOLDER> key does not throw an <PLACE_HOLDER> .,tester . get or create ( error key ) . set builder ( null ) ; tester . set ( error key @$ new string value ( __str__ ) ) ; tester . invalidate ( ) ; assert that ( tester . eval and get ( __str__ ) ) . is equal to ( new string value ( __str__ ) ) ;,key throw error
items that are not skills do not have an experience <PLACE_HOLDER>,long experience = - __num__ ; if ( record . size ( ) == __num__ ) { experience = long . parse long ( record . get ( __num__ ) ) ; } skill skill = new skill ( rank @$ level @$ experience ) ; hiscore builder . set next skill ( skill ) ;,items have field
the label does not cross a subpackage <PLACE_HOLDER> .,if ( containing pkg . equals ( label . get package identifier ( ) ) ) { return false ; },label cross boundary
j table does n't allow <PLACE_HOLDER>,return __num__ ;,table allow rows
note : assumption @$ the decompiler wo n't get the <PLACE_HOLDER> if there is no guard,final code block jump block at = basic block model . get first code block containing ( location @$ monitor ) ;,assumption get block
these two exceptions trigger an immediate <PLACE_HOLDER>,throw e ; log . debug ( __str__ @$ iterations @$ e ) ; ex = e ;,exceptions trigger run
client has provided a <PLACE_HOLDER>,if ( delegation . get value ( ) != null ) { delegation query = __str__ + delegation ; } else { final token < ? extends token identifier > t = generate delegation token ( ugi @$ null ) ; delegation query = __str__ + new delegation param ( t . encode to url string ( ) ) ; },client provided delegation
if we 're in system server and in a binder transaction we need to clear the calling uid . this works around code in system server that did not call clear calling identity @$ previously this was n't needed because reading settings did not do permission <PLACE_HOLDER> but thats no longer the case . long term this should be removed and callers should properly,if ( settings . is in system server ( ) && binder . get calling uid ( ) != process . my uid ( ) ) { final long token = binder . clear calling identity ( ) ; try { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ) ; } finally { binder . restore calling identity ( token ) ; } } else { b = cp . call ( cr . get package name ( ) @$ m provider holder . m uri . get authority ( ) @$ m call get command @$ name @$ args ),settings do checks
see also regression testing that ensures ee picks up catalog <PLACE_HOLDER> in test sql features new suite,compile limit delete stmt and check catalog ( ddl @$ null @$ __str__ @$ __num__ @$ null ) ;,ee picks changes
server has defined column <PLACE_HOLDER> explicitly,if ( h cell . is defined width ( ) ) { if ( needs indent && w < hierarchy column indent ) { w = hierarchy column indent ; } total explicit columns widths += w ; } else { if ( h cell . get expand ratio ( ) > __num__ ) { expand ratio divider += h cell . get expand ratio ( ) ; w = __num__ ; if ( needs indent && w < hierarchy column indent ) { hierarchy header with expand ratio = h cell ; } } else { int header width = h cell . get natural column width ( i ) ; int footer width = f cell . get natural column width ( i ) ; w,server defined width
let any requests pending a response see an <PLACE_HOLDER>,try { peer . transport listener . on exception ( new transport disposedio exception ( __str__ + this + __str__ ) ) ; } catch ( exception ignore ) { },response see exception
image does not have <PLACE_HOLDER> and they are not required . specify that that the texture has no <PLACE_HOLDER> .,gl . gl tex parameteri ( target @$ gl2 . gl_texture_max_level @$ __num__ ) ;,texture has mipmaps
test server initiated <PLACE_HOLDER>,get server counter start button ( ) . click ( ) ; wait until server counter changes ( ) ;,server initiated push
first time for this <PLACE_HOLDER> . create <PLACE_HOLDER> local,if ( m == null ) { m = new hash map ( ) ; synchronized ( thread conn maps ) { if ( closed ) { owner . get cancel criterion ( ) . check cancel in progress ( null ) ; throw new distributed system disconnected exception ( __str__ ) ; } for ( iterator it = thread conn maps . iterator ( ) ; it . has next ( ) ; ) { reference r = ( reference ) it . next ( ) ; if ( r . get ( ) == null ) { it . remove ( ) ; } } thread conn maps . add ( new weak reference ( m ) ) ; } thread ordered conn map . set,time create graph
staxmapper will just output the <PLACE_HOLDER> without adding newlines if this is used,char [ ] chars = value . to char array ( ) ; writer . write characters ( chars @$ __num__ @$ chars . length ) ;,staxmapper output string
if bolt has not finished <PLACE_HOLDER> or was not exactly once mode @$ just process the tuple immediately,if ( ! init || ( batch cache != null && ! batch cache . is exactly once mode ( ) ) ) { } else { pending batch batch = batch cache . get next pending batch ( last successful batch ) ; if ( batch != null ) { list < byte [ ] > pending msgs = batch . get tuples ( ) ; while ( pending msgs != null ) { for ( byte [ ] msg : pending msgs ) { receiver . deserialize tuple ( deserializer @$ msg @$ queue ) ; } pending msgs = batch . get tuples ( ) ; } } kryo input . set buffer ( data ) ; kryo input . set position ( __num__ ),bolt finished initialization
could define a get outputs <PLACE_HOLDER> instead,double [ ] ret result = new double [ num output ] ;,a get method
decrypt temp 1 using trip lede s in cbc mode using the kek and the iv found in the previous step . call the result <PLACE_HOLDER> .,this . param plusiv = new parameters withiv ( this . param @$ this . iv ) ; this . engine . init ( false @$ this . param plusiv ) ; byte [ ] wkcks = new byte [ temp1 . length ] ; for ( int current byte pos = __num__ ; current byte pos != wkcks . length ; current byte pos += block size ) { engine . process block ( temp1 @$ current byte pos @$ wkcks @$ current byte pos ) ; },iv call alpha
disable ssl hostname <PLACE_HOLDER> .,httpsurl connection . set default hostname verifier ( new hostname verifier ( ) { @ override public boolean verify ( string s @$ ssl session ssl ses ) { return true ; } } ) ; super . before test ( ) ;,ssl hostname verification
attribute value update allows nulls for its <PLACE_HOLDER> @$ since they are semantically meaningful . attribute <PLACE_HOLDER> never have null <PLACE_HOLDER> .,if ( attribute value != null && ! attribute action . delete . to string ( ) . equals ( attribute action ) ) { map . put ( attribute name @$ attribute value ) ; },values have values
unknown host exception happens if we ca n't resolve host<PLACE_HOLDER> into ip address . unknown host exception 's get message method returns just the host<PLACE_HOLDER> which is a useless message @$ so log the exception class <PLACE_HOLDER> to provide more info .,log . debug ( e . to string ( ) ) ; throw new helios exception ( __str__ + uri @$ e ) ; throw new helios exception ( __str__ + uri @$ e ) ;,hostname log name
start spi start <PLACE_HOLDER> .,start stopwatch ( ) ; assert parameter ( cred != null @$ __str__ ) ; if ( log . is debug enabled ( ) ) { log . debug ( config info ( __str__ @$ cred ) ) ; log . debug ( config info ( __str__ @$ cfg ) ) ; log . debug ( config info ( __str__ @$ bucket name suffix ) ) ; log . debug ( config info ( __str__ @$ bucket endpoint ) ) ; log . debug ( config info ( __str__ @$ sse alg ) ) ; } if ( cfg == null ) u . warn ( log @$ __str__ ) ; if ( f . is empty ( bucket name suffix ) ) { u . warn (,spi start stopwatch
the exception transports the actual return <PLACE_HOLDER> .,return ex . get result ( ) ;,exception transports type
do this after the build so that other goals do n't use the <PLACE_HOLDER> if it does n't exist,if ( repository != null ) { write image info ( repository @$ tag ) ; } write metadata ( log ) ; if ( repository == null ) { log . info ( message format . format ( __str__ @$ image id ) ) ; } else { log . info ( message format . format ( __str__ @$ format image name ( repository @$ tag ) ) ) ; },goals use repository
other protocols could cause <PLACE_HOLDER>,if ( url != null && url . starts with ( __str__ ) ) { this . sb . tables . bookmarks . add bookmark ( this . bmk_user @$ bmk @$ true @$ true ) ; if ( this . autotag ) { if ( ! this . empty ) { this . auto tagging queue . put ( url ) ; } else if ( ! bmk . contains key ( y mark entry . bookmark . tags . key ( ) ) || bmk . get ( y mark entry . bookmark . tags . key ( ) ) . equals ( y mark entry . bookmark . tags . deflt ( ) ) ) { this . auto tagging queue . put ( url,protocols cause problems
replace the begin <PLACE_HOLDER> with its formatted string and store the old timestamp,long old timestamp = format timestamp ( report generator . begin_date_consumer_name @$ data context ) ;,the begin date
was the initial port specified ? if so @$ override this property normally is applied for the client side configuration of resolvers . here we are using it to define the server port that the with <PLACE_HOLDER> the resolvers communicate .,if ( args [ i ] . equals ( __str__ ) && i < args . length - __num__ ) { initial port = java . lang . integer . parse int ( args [ i + __num__ ] ) ; },resolvers communicate which
unmanaged realm list does not know actual element <PLACE_HOLDER> .,sb . append ( __str__ ) ;,list know type
if the parent has a default acl @$ copy that default acl an <PLACE_HOLDER> with the umask as the new file 's access acl . if it is a metadata load operation @$ do not consider the umask .,default access control list d acl = current inode directory . get defaultacl ( ) ; short mode = context . is metadata load ( ) ? mode . create full access ( ) . to short ( ) : new file . get mode ( ) ; if ( ! d acl . is empty ( ) ) { access control list acl = d acl . generate child fileacl ( mode ) ; new file . set internal acl ( acl ) ; } if ( file context . is cacheable ( ) ) { new file . set cacheable ( true ) ; } if ( file context . get write type ( ) == write type . async_through ) { new file . set,default acl assemble
create 4 threads @$ each one a daemon thread running the event index <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { final thread t = new thread ( task ) ; t . set daemon ( true ) ; t . start ( ) ; } assert equals ( __num__ @$ commit count . get ( ) ) ;,threads running processor
bind and start to accept incoming <PLACE_HOLDER> .,channel channel = bootstrap . bind ( new inet socket address ( port ) ) ; all channels . add ( channel ) ; log . info ( __str__ @$ port @$ buffer_size @$ max workers ) ; this . is backpressure enable = config extension . is backpressure enable ( storm conf ) ; if ( is backpressure enable ) { flow ctrl handler = new netty server flow ctrl handler ( storm conf @$ all channels @$ worker tasks ) ; flow ctrl handler . start ( ) ; },bind accept connections
if this instruction writes ref <PLACE_HOLDER> @$ do n't need to track back since this is the start of the scope,if ( write ref set . contains ( instr . get address ( ) ) ) { write scope . add range ( instr . get min address ( ) @$ instr . get max address ( ) ) ; } else { set scope before instruction ( instr @$ sub set @$ write checker ) ; } set scope after instruction ( instr @$ sub set @$ write checker ) ;,instruction writes set
try to deserialize using deserialize read our writable row <PLACE_HOLDER> created by ser de .,for ( int i = __num__ ; i < row count ; i ++ ) { object [ ] row = rows [ i ] ; lazy binary deserialize read lazy binary deserialize read = new lazy binary deserialize read ( type infos @$ false ) ; bytes writable bytes writable = serde bytes [ i ] ; lazy binary deserialize read . set ( bytes writable . get bytes ( ) @$ __num__ @$ bytes writable . get length ( ) ) ; for ( int index = __num__ ; index < column count ; index ++ ) { if ( use include columns && ! columns to include [ index ] ) { lazy binary deserialize read . skip next field ( ) ; } else,deserialize read objects
fetch the auto complete text <PLACE_HOLDER> and set an adapter,auto complete text view actv = find view by id ( r . id . widgets_autocompletetextview ) ; actv . set adapter ( new array adapter < > ( this @$ android . r . layout . simple_dropdown_item_1line @$ cheeses . s cheese strings ) ) ;,auto complete view
pointer exceed depth <PLACE_HOLDER> of 2,if ( pointer classification == pointer reference classification . deep ) { return pointer_label_prefix + __str__ + pointer_label_prefix ; },pointer exceed limit
verify that username exists and the associated password matches the <PLACE_HOLDER> supplied by the client .,username = a credentials [ __num__ ] ; password = a credentials [ __num__ ] ; if ( username == null || password == null ) { final string message = __str__ ; authentication failure ( __str__ @$ message ) ; },password matches one
grouping by size . if group <PLACE_HOLDER> size exceeds specified limit @$ then execute transformation and flush group <PLACE_HOLDER> .,if ( trans executor data . group size > __num__ ) { if ( trans executor data . group buffer . size ( ) >= trans executor data . group size ) { execute transformation ( incoming field values ) ; } } return true ;,then execute buffer
copying over empty byte <PLACE_HOLDER> @$ but let 's keep things consistent .,final multi partmime input stream body less body input stream = new multi partmime input stream . builder ( new byte array input stream ( _body less body . get part data ( ) . copy bytes ( ) ) @$ scheduled executor service @$ _body less body . get part headers ( ) ) . with write chunk size ( chunk size ) . build ( ) ; final multi partmime input stream purely empty body input stream = new multi partmime input stream . builder ( new byte array input stream ( _purely empty body . get part data ( ) . copy bytes ( ) ) @$ scheduled executor service @$ _purely empty body . get part headers ( ) ) . with write chunk,copying empty array
<PLACE_HOLDER> not found @$ add this <PLACE_HOLDER> and its hfile paths pair to the list,if ( ! found family ) { add family and itsh file path to table in map ( family @$ path to hfile fromns @$ familyh file paths list ) ; },family add family
<PLACE_HOLDER> while other threads control the lock grantor future result terminate <PLACE_HOLDER> if other thread has already made us lock grantor terminate <PLACE_HOLDER> if this thread gets control of lock grantor future result,while ( ! own lock grantor future result ) { assert . assert holds lock ( this . destroy lock @$ false ) ; synchronized ( this . lock grantor id lock ) { if ( is currently or is making lock grantor ( ) ) { return ; } else if ( this . lock grantor future result != null ) { lock grantor future result ref = this . lock grantor future result ; } else { own lock grantor future result = true ; lock grantor future result ref = new future result ( this . dm . get cancel criterion ( ) ) ; if ( is debug enabled_dls ) { logger . trace ( log marker . dls_verbose @$ __str__ ) ; },grantor terminate loop
record driver so other threads add unpartitioned sources can see the driver note : this must be done before reading unpartitioned sources @$ so we see a consistent <PLACE_HOLDER> of the unpartitioned sources,drivers . add ( new weak reference < > ( driver ) ) ; if ( partitioned split != null ) { driver . update source ( new task source ( partitioned split . get plan node id ( ) @$ immutable set . of ( partitioned split ) @$ true ) ) ; },sources see order
test named <PLACE_HOLDER> .,message format mf = new message format ( __str__ ) ; if ( ! mf . uses named arguments ( ) ) { errln ( __str__ ) ; } mf = new message format ( __str__ ) ; if ( ! mf . uses named arguments ( ) ) { errln ( __str__ ) ; },test named arguments
verify that we have no <PLACE_HOLDER> in the table because neither file should have been loaded even though one of the files could have .,table table = test_util . get connection ( ) . get table ( tn ) ; result scanner scanner = table . get scanner ( new scan ( ) ) ; try { assert null ( __str__ @$ scanner . next ( ) ) ; } finally { scanner . close ( ) ; },one have data
the second time should contain the <PLACE_HOLDER>,has certificate = info . get transport context ( ) instanceof x509 certificate [ ] ;,time contain certificate
<PLACE_HOLDER> wa <PLACE_HOLDER> small wa <PLACE_HOLDER> wi <PLACE_HOLDER> we <PLACE_HOLDER> wo,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,kana wa kana
note : the subclass will release the 'work finished latch ' early @$ before work is actually finished . this means that the test may proceed and perform <PLACE_HOLDER> earlier than anticipated .,work start latch . count down ( ) ; try { work finished latch . await ( __num__ @$ time unit . seconds ) ; } catch ( interrupted exception e ) { assert . fail ( __str__ ) ; } work finished latch = new count down latch ( __num__ ) ;,test proceed work
not sure which thread gets the <PLACE_HOLDER> first so we add them to a map and verify that some thread had 4 threads waiting @$ 3 threads @$ etc .,assert that ( map . size ( ) @$ equal to ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ; assert that ( map @$ has key ( __num__ ) ) ;,thread gets keys
m<PLACE_HOLDER>ke sure b will h<PLACE_HOLDER>ve less digits th<PLACE_HOLDER>n <PLACE_HOLDER>,if ( b digits > a digits ) { temp = a digits ; a digits = b digits ; b digits = temp ; } if ( b digits == a digits ) { return ; } assert true ( a digits + b digits == __num__ && a digits > __num__ && b digits > __num__ ) ; a = new decimal128 ( ) ; sa = make numeric string ( a digits ) ; a . update ( sa @$ ( short ) __num__ ) ; b = new decimal128 ( ) ; sb = make numeric string ( b digits ) ; b . update ( sb @$ ( short ) __num__ ) ; if ( b . is zero ( ) ) {,b have a
durable topic consumer will consume 10 <PLACE_HOLDER> and disconnect,create consumer ( interest @$ initial max msgs ) ; thread . sleep ( reconnect sleep ) ; create consumer ( interest @$ cleanup msg count ) ; string broker version = ( string ) mbean server . get attribute ( new object name ( __str__ ) @$ __str__ ) ; log . info ( __str__ + broker version ) ; final string the jmx object = __str__ + __str__ + __str__ ; assert true ( __str__ @$ wait . wait for ( new wait . condition ( ) { @ override public boolean is satisified ( ) throws exception { integer pending queue size = ( integer ) mbean server . get attribute ( new object name ( the jmx object ) @$ __str__ ) ; log,consumer consume messages
this pattern has an <PLACE_HOLDER> .,pattern pattern1 = verifier component mock factory . create pattern1 ( ) ; restriction r1 = literal restriction . create restriction ( pattern1 @$ __str__ ) ; restriction r2 = literal restriction . create restriction ( pattern1 @$ __str__ ) ; incompatibility i1 = new incompatibility ( r1 @$ r2 ) ; sub pattern pp1 = new sub pattern ( pattern1 @$ __num__ ) ; pp1 . add ( r1 ) ; pp1 . add ( r2 ) ; restriction r3 = new variable restriction ( pattern1 ) ; restriction r4 = new variable restriction ( pattern1 ) ; incompatibility i2 = new incompatibility ( r1 @$ r2 ) ; sub pattern pp2 = new sub pattern ( pattern1 @$ __num__ ) ; pp2 . add ( r1,pattern has error
use cached buffered <PLACE_HOLDER> for now .,return m player . get buffered position ( ) ;,use cached position
just let the user enter their <PLACE_HOLDER>,if ( saved instance state != null || m called ) { return ; },user enter form
first version @$ using union find data <PLACE_HOLDER>,connect ( i @$ j ) ;,version find vector
this is a safety net . if the current subtype ca n't be added to the history and the framework could n't find the last <PLACE_HOLDER> @$ we will make the last <PLACE_HOLDER> be the most applicable enabled keyboard subtype of the system <PLACE_HOLDER>s .,if ( text utils . is empty ( target last imi id ) && ! input method utils . can add to last input method ( m current subtype ) ) { final list < input method info > enabled = m settings . get enabled input method list locked ( ) ; if ( enabled != null ) { final int n = enabled . size ( ) ; final string locale = m current subtype == null ? m res . get configuration ( ) . locale . to string ( ) : m current subtype . get locale ( ) ; for ( int i = __num__ ; i < n ; ++ i ) { final input method info imi = enabled . get,framework find ime
second request should have the same session <PLACE_HOLDER> .,response = client . new request ( __str__ @$ connector . get local port ( ) ) . scheme ( http scheme . https . as string ( ) ) . header ( http header . connection @$ __str__ ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( http status . ok_200 @$ response . get status ( ) ) ; assert true ( server latch . await ( __num__ @$ time unit . seconds ) ) ; assert true ( client latch . await ( __num__ @$ time unit . seconds ) ) ;,request have response
in case we 're not on the first page and the <PLACE_HOLDER> exceeds the page <PLACE_HOLDER> @$ we need to do an additional count for the total,if ( page != __num__ || tasks . size ( ) == size ) { long total count = task info query wrapper . get task info query ( ) . count ( ) ; result . set total ( long . value of ( total count . int value ( ) ) ) ; result . set start ( page * size ) ; } return result ;,size exceeds size
the dispatcher fires the <PLACE_HOLDER> corresponding to the closest matching rule and passes the context along,dispatcher disp = new default rule dispatcher ( op traits rules proc factory . get default rule ( ) @$ op rules @$ annotate ctx ) ; graph walker ogw = new level order walker ( disp @$ __num__ ) ;,dispatcher fires processor
null cache does n't actually retain the <PLACE_HOLDER>,assert false ( cache . contains ( __str__ ) ) ;,cache retain session
a node throwing the exception because of the can take <PLACE_HOLDER> of the attached faulty node property,dumb slave faulty agent = r . create online slave ( label expression . get ( __str__ ) ) ; faulty agent . get node properties ( ) . add ( new faulty node property ( ) ) ;,node take care
shutdown <PLACE_HOLDER> @$ then join <PLACE_HOLDER> @$ hopefully save some shutdown time for tests .,synchronized ( m_all sites ) { for ( mp ro site context site : m_all sites ) { site . shutdown ( ) ; } for ( mp ro site context site : m_all sites ) { site . join thread ( ) ; } },all join threads
update the position now since the mrp will update it only once the video is playing <PLACE_HOLDER> . in particular @$ if the video is paused @$ the mrp does n't send the command until the video is resumed .,m position extrapolator . on seek ( msec ) ; m seeking = true ; intent intent = new intent ( media control intent . action_seek ) ; intent . add category ( media control intent . category_remote_playback ) ; intent . put extra ( media control intent . extra_session_id @$ m current session id ) ; intent . put extra ( media control intent . extra_item_id @$ m current item id ) ; intent . put extra ( media control intent . extra_item_content_position @$ msec ) ; send intent to route ( intent @$ new result bundle handler ( ) { @ override public void on result ( bundle data ) { if ( get media state listener ( ) != null ) get media state listener,video playing itself
throws obsolete version exception if another process has created a new <PLACE_HOLDER> already,store client . put ( new key . map value ( ) @$ new node ) ;,process created node
next call for get buffered will return the buffered <PLACE_HOLDER> that came after the partial boundary match,if ( buffered byte != boundary [ __num__ ] ) { matched count = __num__ ; } else { matched count = __num__ ; buffered byte = - __num__ ; },call return byte
start maintenance for 2 nd dn ; still get 3 <PLACE_HOLDER> .,start maintenance ( cluster . get namesystem ( ) @$ dnm @$ __num__ ) ; verify file location ( file index @$ __num__ ) ; data node dn1 = cluster . get data nodes ( ) . get ( __num__ ) ; data node dn2 = cluster . get data nodes ( ) . get ( __num__ ) ;,maintenance get files
initialize file <PLACE_HOLDER> as the current user .,return file system . get ( uri @$ hadoop conf ) ;,initialize file system
invalidate operations do n't fire cache <PLACE_HOLDER> @$ so do n't assert they were fired .,if ( op != op . invalidate ) { accessor . invoke ( new serializable callable ( ) { @ override public object call ( ) throws exception { region cust = get cache ( ) . get region ( customer ) ; assert false ( ( ( test cache writer ) cust . get attributes ( ) . get cache writer ( ) ) . was fired ) ; region order = get cache ( ) . get region ( order ) ; assert false ( ( ( test cache writer ) order . get attributes ( ) . get cache writer ( ) ) . was fired ) ; region ref = get cache ( ) . get region ( d_reference ) ; assert false (,operations fire writers
<PLACE_HOLDER> is open @$ should n't close underlying <PLACE_HOLDER>,bris . get input stream ( ) ; verify ( bris @$ times ( bris opens ) ) . open input stream ( mockito . any long ( ) ) ; verify ( bris @$ times ( bris closes ) ) . close ( ) ; verify ( mock stream . in @$ times ( is closes ) ) . close ( ) ;,stream close stream
create enough element converters note : we have to have a separate element converter for each element @$ because the element converters can reuse the internal <PLACE_HOLDER> . so it 's not safe to use the same element converter to convert multiple elements .,int size = inputoi . get list length ( input ) ; while ( element converters . size ( ) < size ) { element converters . add ( get converter ( input elementoi @$ output elementoi ) ) ; },converters reuse object
if the entry length field spans a sector <PLACE_HOLDER> @$ write the high order bit of the entry length @$ otherwise write zero for the entry length .,long entry start = log . get file pointer ( ) ; boolean spans boundary = log . check spans boundary ( entry start ) ; write int ( log @$ spans boundary ? __num__ << __num__ : __num__ ) ;,field spans boundary
accumulo connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support comment
steam reader need copy <PLACE_HOLDER> from pread reader,reader . copy fields ( initial reader ) ;,reader need fields
the following block of code calculates and updates the max drop rate if the client had been fully degraded in the past and has not received enough requests since being fully degraded . to increase the chances of the client receiving a request @$ we change the max drop rate @$ which influences the maximum <PLACE_HOLDER> of computed drop rate @$ which is used,if ( call count < degrader control . get min call count ( ) ) { if ( strategy == partition degrader load balancer state . strategy . load_balance ) { double old max drop rate = client updater . get max drop rate ( ) ; double transmission rate = __num__ - old max drop rate ; if ( transmission rate <= __num__ ) { transmission rate = initial recovery level ; } else { transmission rate *= ring ramp factor ; transmission rate = math . min ( transmission rate @$ __num__ ) ; } client updater . set max drop rate ( __num__ - transmission rate ) ; } } else if ( ring ramp factor > fast_recovery_threshold && ! degrader control . is high,which influences value
user will have an unknown <PLACE_HOLDER> .,password authentication a = privileged request password authentication ( host @$ addr @$ port @$ __str__ @$ realm @$ scheme @$ url @$ requestor type . proxy ) ; if ( a != null ) { ret = new basic authentication ( true @$ host @$ port @$ realm @$ a ) ; } break ; case digest : a = privileged request password authentication ( host @$ null @$ port @$ url . get protocol ( ) @$ realm @$ scheme @$ url @$ requestor type . proxy ) ; if ( a != null ) { digest authentication . parameters params = new digest authentication . parameters ( ) ; ret = new digest authentication ( true @$ host @$ port @$ realm @$ scheme @$,user have name
check <PLACE_HOLDER> errors only if automatic <PLACE_HOLDER> is enabled . empty @$ required fields will generate a <PLACE_HOLDER> error containing the required error string . for these fields the exclamation mark will be hidden but the error must still be sent to the client .,validator . invalid value exception validation error = null ; if ( is validation visible ( ) ) { try { validate ( ) ; } catch ( validator . invalid value exception e ) { if ( ! e . is invisible ( ) ) { validation error = e ; } } },fields generate validation
bottom over scroll might not grab all scrolling <PLACE_HOLDER> @$ we have to scroll as well .,if ( anchor_scrolling ) { float scroll amount = new bottom amount < __num__ ? new bottom amount : __num__ ; expandable view first child = get first child not gone ( ) ; float top = first child . get translationy ( ) ; float distance to top = m scroll anchor view . get translationy ( ) - top - m scroll anchor viewy ; if ( distance to top < - scroll amount ) { float current top pixels = get current over scrolled pixels ( true ) ; set over scrolled pixels ( current top pixels + ( - scroll amount - distance to top ) @$ true @$ false ) ; m scroll anchor view = first child ; m scroll anchor viewy,bottom grab motion
cancellation of the job should remove <PLACE_HOLDER>,final completable future < job result > job result future = dispatcher gateway . request job result ( job graph . get jobid ( ) @$ testing_timeout ) ; dispatcher gateway . cancel job ( job graph . get jobid ( ) @$ testing_timeout ) . get ( ) ;,cancellation remove itself
clear references to <PLACE_HOLDER> @$ this should orphan the <PLACE_HOLDER> which should in turn trigger orphan removal logic .,team . set one vone player ( null ) ; team2 . set one vone player ( null ) ; tx . commit ( ) ; s . close ( ) ; s = open session ( ) ; tx = s . begin transaction ( ) ; count = ( ( long ) s . create query ( __str__ ) . iterate ( ) . next ( ) ) . int value ( ) ; assert equals ( __str__ + count @$ count @$ __num__ ) ; tx . commit ( ) ; s . close ( ) ;,references orphan players
empty selection not allowed @$ keep old <PLACE_HOLDER>,if ( ! is null selection allowed ( ) && s . is empty ( ) ) { mark as dirty ( ) ; return ; },selection keep value
failed emit will discard buffer 's <PLACE_HOLDER>,actual . on complete ( ) ;,emit discard content
some carriers will download duplicate mms <PLACE_HOLDER> without this ack . when using the system sending method @$ apparently google does not do this for us . not sure why . you might have to have users manually enter their apn settings if you can not get them from the system somehow .,return null ;,carriers download files
this thread <PLACE_HOLDER>s the <PLACE_HOLDER> in server 1 first @$ then server 2 .,lock the locks ( server1 @$ server2 @$ count down latch ) ;,thread locks lock
no need for a state sets <PLACE_HOLDER> for the child since it only has two states,if ( indicator != null && indicator . is stateful ( ) ) { final int state set [ ] = pos . position . flat list pos == pos . group metadata . last child fl pos ? child_last_state_set : empty_state_set ; indicator . set state ( state set ) ; },need sets list
similar to the test above @$ except the threads will have multiple page <PLACE_HOLDER> opened at a time .,assert timeout preemptively ( of millis ( semi_long_timeout_millis ) @$ ( ) -> { final atomic boolean should stop = new atomic boolean ( ) ; final int cache pages = __num__ ; final int file pages = cache pages * __num__ ; final int thread count = __num__ ; final int page size = thread count * __num__ ; final int max cursors per thread = cache pages / ( __num__ + thread count ) ; assert that ( max cursors per thread * thread count @$ less than ( cache pages ) ) ; get page cache ( fs @$ cache pages @$ page cache tracer . null @$ page cursor tracer supplier . null ) ; try ( paged file paged file = page cache,threads have files
check stop requested inside the sync to prevent a race in which the wait misses the stopper 's <PLACE_HOLDER> .,if ( stop requested ( ) ) { return ; },wait misses event
make sure the home button has an accurate content <PLACE_HOLDER> for accessibility .,update home accessibility ( enable ) ;,button has description
if deletion failed then the directory scanner will cleanup the <PLACE_HOLDER> eventually .,cleanup replica ( bpid @$ replica info ) ;,scanner cleanup files
classify 1 has <PLACE_HOLDER> a as an allowed type,assert true ( classify type1 . can apply to entity type ( entity typea ) ) ;,1 has entity
the edge that leaves the do <PLACE_HOLDER> if the condition fails .,if ( ! cond . is true ( ) ) { create edge ( node @$ branch . on_false @$ compute follow node ( node @$ this ) ) ; },edge do branch
shuffle the list so all clients do n't prefer the same <PLACE_HOLDER>,if ( iter == null ) { list < redis client > clients = new array list < > ( sentinels . values ( ) ) ; collections . shuffle ( clients ) ; iter = clients . iterator ( ) ; },clients prefer referants
exopackage builds <PLACE_HOLDER> to jar @$ otherwise @$ <PLACE_HOLDER> to raw .,dex store default dex store = exopackage mode . enabled for secondary dexes ( exopackage modes ) ? dex store . jar : dex store . raw ; dex split strategy dex split strategy = args . get minimize primary dex size ( ) ? dex split strategy . minimize_primary_dex_size : dex split strategy . maximize_primary_dex_size ; return new dex split mode ( args . get use split dex ( ) @$ dex split strategy @$ args . get dex compression ( ) . or else ( default dex store ) @$ args . get linear alloc hard limit ( ) @$ args . get dex group lib limit ( ) @$ args . get primary dex patterns ( ) @$ args . get primary dex classes file,exopackage builds link
test key store only contain key pair <PLACE_HOLDER> .,if ( is cert entry == true ) { throw new runtime exception ( __str__ + __str__ + __str__ + alias ) ; } boolean is key entry = input key store . is key entry ( alias ) ; key key = null ; if ( is key entry ) { key = input key store . get key ( alias @$ in key pass . to char array ( ) ) ; } else { throw new runtime exception ( __str__ + alias ) ; } output key store . set key entry ( alias @$ key @$ out key pass . to char array ( ) @$ certs ) ;,store contain entries
both threads have terminated and cancel should have cancelled the insert <PLACE_HOLDER> .,result set res = st . execute query ( __str__ ) ; assert false ( res . next ( ) ) ; try { st . close ( ) ; st . cancel ( ) ; fail ( __str__ ) ; } catch ( sql exception e ) { },cancel cancelled statement
subsequent same key presses move the keyboard <PLACE_HOLDER> to the next object that starts with the same letter .,if ( ( prefix . length ( ) == __num__ ) && ( c == prefix . char at ( __num__ ) ) ) { starting row ++ ; } else { prefix = typed string ; },presses move focus
check if application needs more <PLACE_HOLDER> @$ skip if it does n't need more .,if ( reserved container == null ) { if ( ! application . has pending resource request ( candidates . get partition ( ) @$ scheduling mode ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + application . get application attempt id ( ) + __str__ + scheduling mode . name ( ) + __str__ + candidates . get partition ( ) ) ; } activities logger . app . record skipped app activity without allocation ( activities manager @$ node @$ application @$ null @$ activity diagnostic constant . application_do_not_need_resource @$ activity level . app ) ; return cs assignment . skip_assignment ; } for ( scheduler request key scheduler key : application . get scheduler,application needs resource
iterate over all <PLACE_HOLDER>s found in the string @$ expanding each found <PLACE_HOLDER> .,int last end = __num__ ; macro finder automaton matcher = new macro finder automaton ( blob ) ; while ( matcher . has next ( ) ) { macro match result match result = matcher . next ( ) ; combiner . add string ( blob . substring ( last end @$ match result . get start index ( ) ) ) ; if ( match result . is escaped ( ) ) { combiner . add string ( blob . substring ( match result . get start index ( ) + ( resolve escaping ? __num__ : __num__ ) @$ match result . get end index ( ) ) ) ; } else { macro replacer < t > replacer = replacers . get ( match,iterate found macro
record should have a <PLACE_HOLDER> that indicates that the 'child ' is a choice of 2 different record types,assert true ( first outer schema . get data type ( __str__ ) . get ( ) . get field type ( ) == record field type . choice ) ; final list < data type > first sub types = ( ( choice data type ) first outer schema . get data type ( __str__ ) . get ( ) ) . get possible sub types ( ) ; assert equals ( __num__ @$ first sub types . size ( ) ) ; assert equals ( __num__ @$ first sub types . stream ( ) . filter ( type -> type . get field type ( ) == record field type . record ) . count ( ) ) ;,record have schema
the partial solution was on the path here . check whether the channel requires certain <PLACE_HOLDER> that are met @$ or whether the channel introduces new <PLACE_HOLDER>,found = true ;,channel introduces properties
ordinary bmp code point @$ excluding leading surrogates . bmp uses a single level <PLACE_HOLDER> . bmp index starts at offset 0 in the trie 2 index . 32 bit data is stored in the index array itself .,if ( code point < __num__ || ( code point > __num__ && code point <= __num__ ) ) { ix = index [ code point > > utrie2_shift_2 ] ; ix = ( ix << utrie2_index_shift ) + ( code point & utrie2_data_mask ) ; value = data32 [ ix ] ; return value ; },bmp uses lookup
simulate node status updater impl sending c mgr signal <PLACE_HOLDER> event,signal container request signal req = signal container request . new instance ( c id @$ command ) ; list < signal container request > reqs = new array list < > ( ) ; reqs . add ( signal req ) ; container manager . handle ( new c mgr signal containers event ( reqs ) ) ; final argument captor < container signal context > signal context captor = argument captor . for class ( container signal context . class ) ; if ( signal . equals ( signal . null ) ) { verify ( exec @$ never ( ) ) . signal container ( signal context captor . capture ( ) ) ; } else { verify ( exec @$ timeout ( __num__ ),c mgr container
handler which do n't send an <PLACE_HOLDER>,sys prop configa . eth62 = new eth62 ( ) { @ override protected void process get block headers ( get block headers message msg ) { if ( msg . get max headers ( ) == __num__ ) { super . process get block headers ( msg ) ; return ; } list < block header > headers = new array list < > ( ) ; for ( int i = __num__ ; i < mainb1b10 . size ( ) ; i ++ ) { headers . add ( mainb1b10 . get ( i ) . get header ( ) ) ; } block headers message response = new block headers message ( headers ) ; send message ( response ) ; } } ;,which send error
permission bits from server response will have the <PLACE_HOLDER> for accuracy .,if ( this . permission != null ) { perm arg = this . permission ; },bits have same
memory merge may have limited the changed code <PLACE_HOLDER> we are working with .,address set view result set = result pgm . get memory ( ) ; this . latest set = latest changes . get register address set ( ) . intersect ( result set ) ; this . my set = my changes . get register address set ( ) . intersect ( result set ) ; original context = original pgm . get program context ( ) ; latest context = latest pgm . get program context ( ) ; my context = my pgm . get program context ( ) ; result context = result pgm . get program context ( ) ; registers = my context . get registers ( ) ; try { diff original latest = new program diff ( original pgm @$ latest,merge limited units
client configuration setting has a <PLACE_HOLDER> over system property,system . set property ( __str__ @$ __str__ ) ; config = new client configuration ( ) . with non proxy hosts ( __str__ ) ; assert equals ( __str__ @$ config . get non proxy hosts ( ) ) ; config . set protocol ( protocol . http ) ; assert equals ( __str__ @$ config . get non proxy hosts ( ) ) ; system . clear property ( __str__ ) ;,setting has precedence
event bus goes here . this method is only called if one of the binding providers provide a binding for the given 'item <PLACE_HOLDER> ' . see the readme.md for the item formatting for the isy link .,this . logger . debug ( __str__ @$ item name @$ command ) ; isy binding config config = get binding config by name ( item name ) ; if ( config != null ) { process command ( config @$ command ) ; } else { this . logger . warn ( __str__ @$ item name ) ; },one provide name
secs since midnight now represents <PLACE_HOLDER>,secs since midnight -= minutes ;,secs represents time
color <PLACE_HOLDER> always override tint <PLACE_HOLDER> .,final color filter color filter = ( m color filter == null ? m tint filter : m color filter ) ;,filters override filters
wait until vm 1 has sent the <PLACE_HOLDER>,try { pausibletx . class . wait ( ) ; } catch ( interrupted exception ie ) { fail ( __str__ + ie ) ; },vm sent message
analyzer has already handled this <PLACE_HOLDER>,continue ;,analyzer handled case
noinspection object <PLACE_HOLDER> in loop,if ( rid . equals ( new o record id ( configuration1 . get schema record id ( ) ) ) && rid . equals ( new o record id ( configuration2 . get schema record id ( ) ) ) ) continue ; if ( rid . get cluster id ( ) == __num__ && rid . get cluster position ( ) == __num__ ) { if ( ! storage type1 . equals ( storage type2 ) ) continue ; },noinspection object allocation
return true if child views should not receive this <PLACE_HOLDER> .,if ( should consume event ) { return true ; },views receive event
output node local <PLACE_HOLDER>,local properties lp = p . get local properties ( ) ; writer . print ( __str__ ) ; if ( lp . get ordering ( ) != null ) { add property ( writer @$ __str__ @$ lp . get ordering ( ) . to string ( ) @$ true ) ; } else { add property ( writer @$ __str__ @$ __str__ @$ true ) ; } if ( lp . get grouped fields ( ) != null && lp . get grouped fields ( ) . size ( ) > __num__ ) { add property ( writer @$ __str__ @$ lp . get grouped fields ( ) . to string ( ) @$ false ) ; } else { add property ( writer @$ __str__,output node properties
for enumerations run the <PLACE_HOLDER>,if ( collection utils . is not empty ( types def . get enum defs ( ) ) ) { for ( atlas enum def enum def : types def . get enum defs ( ) ) { atlas enum def created def = enum def store . create ( enum def @$ null ) ; ttr . update guid ( created def . get name ( ) @$ created def . get guid ( ) ) ; ret . get enum defs ( ) . add ( created def ) ; } },enumerations run methods
use a local copy in case another thread changes <PLACE_HOLDER>,last mapping < configuration property name > last = this . last mapped configuration property name ; if ( last != null && last . is from ( configuration property name ) ) { return last . get mapping ( ) ; } string converted name = configuration property name . to string ( ) ; property mapping [ ] mapping = { new property mapping ( converted name @$ configuration property name ) } ; this . last mapped configuration property name = new last mapping < > ( configuration property name @$ mapping ) ; return mapping ;,thread changes things
get the width of a task view so that we know how wide to draw the header <PLACE_HOLDER> .,if ( use grid layout ) { task grid layout algorithm grid layout = m dummy stack view . get grid algorithm ( ) ; grid layout . initialize ( window rect ) ; task view width = ( int ) grid layout . get transform ( __num__ @$ stack . get task count ( ) @$ new task view transform ( ) @$ stack layout ) . rect . width ( ) ; } else { rect task view bounds = stack layout . get untransformed task view bounds ( ) ; if ( ! task view bounds . is empty ( ) ) { task view width = task view bounds . width ( ) ; } },wide draw frame
set domain file for newly opened domain object note : some domain object implementations may throw runtime <PLACE_HOLDER> so cleanup is required in those cases,try { domain obj . set domain file ( get domain file ( ) ) ; } catch ( exception e ) { domain obj . release ( consumer ) ; file manager . clear domain object ( get pathname ( ) ) ; throwable cause = e . get cause ( ) ; if ( cause instanceof io exception ) { throw ( io exception ) cause ; } else if ( cause instanceof version exception ) { throw ( version exception ) cause ; } throw new io exception ( e . get message ( ) @$ e ) ; },implementations throw exception
screenshot does not include <PLACE_HOLDER> !,final display display = display content . get display ( ) ; int original rotation = display . get rotation ( ) ; final int original width ; final int original height ; display info display info = display content . get display info ( ) ; if ( fixed to user rotation ) { m force default orientation = true ; original width = display content . m base display width ; original height = display content . m base display height ; } else { original width = display info . logical width ; original height = display info . logical height ; },screenshot include padding
without any read permission the anon have <PLACE_HOLDER> to the user list,jenkins rule . web client wc = j . create web client ( ) ; wc . get options ( ) . set throw exception on failing status code ( false ) ; wc . get options ( ) . set redirect enabled ( false ) ; page page = wc . go to ( __str__ @$ null ) ; check page is redirected to login ( page ) ; assert that ( page . get web response ( ) . get content as string ( ) @$ not ( contains string ( __str__ ) ) ) ; assert request was not blocked ( ) ; page = wc . go to ( __str__ @$ null ) ; assert equals ( __num__ @$ page . get web response,anon have access
because this function allocates a totally random <PLACE_HOLDER> @$ it really should n't ever fail to allocate an <PLACE_HOLDER> ; we simply need this because the exception is checked .,throw new resource unavailable exception ( __str__ ) ;,function allocates resource
no good way to verify that we have an array ... although could i guess check via json parser . so let 's assume <PLACE_HOLDER> is working fine @$ for now .,if ( _injectables != null ) { inject values ( ctxt @$ bean ) ; } final settable bean property [ ] props = _ordered properties ; int i = __num__ ; final int prop count = props . length ; while ( true ) { if ( p . next token ( ) == json token . end_array ) { return bean ; } if ( i == prop count ) { break ; } settable bean property prop = props [ i ] ; if ( prop != null ) { try { prop . deserialize and set ( p @$ ctxt @$ bean ) ; } catch ( exception e ) { throw wrap and throw ( e @$ bean @$ prop . get name,way assume everything
the internal ci adapters report negative connection <PLACE_HOLDER> and are n't included in public stats .,for ( map . entry < long @$ client interface handle manager > e : m_cihm . entry set ( ) ) { if ( e . get key ( ) > __num__ ) { long admin mode = e . get value ( ) . is admin ? __num__ : __num__ ; long read wait = e . get value ( ) . connection . read stream ( ) . data available ( ) ; long write wait = e . get value ( ) . connection . write stream ( ) . get outstanding message count ( ) ; long outstanding txns = e . get value ( ) . get outstanding txns ( ) ; client_stats . put ( e . get key ( ),adapters report counts
if we have n't configured the standard dialect @$ preprocessing inlined expressions makes no <PLACE_HOLDER> and in any case we would be in risk of not knowing what we are doing ...,if ( ! this . dialect set configuration . is standard dialect present ( ) ) { return false ; },expressions makes sense
check to make sure that the runtime defined <PLACE_HOLDER> contain all the template <PLACE_HOLDER> .,set < string > runtime tag keys = new hash set < > ( tags . key set ( ) ) ; runtime tag keys . add all ( config ( ) . tags ( ) . key set ( ) ) ; set < string > template tag keys = template . tags ( ) ; if ( ! runtime tag keys . equals ( template tag keys ) ) { throw new illegal argument exception ( __str__ + template . name ( ) + __str__ + __str__ + runtime tag keys . to string ( ) + __str__ + template tag keys . to string ( ) ) ; } return this . metric name ( template . name ( ) @$ template . group (,tags contain keys
need to put this here @$ since vanilla assumes this <PLACE_HOLDER> after the vignette was rendered .,if ( pre ( vignette ) ) { gl state manager . enable depth test ( ) ; gl state manager . blend func separate ( gl state manager . source factor . src_alpha @$ gl state manager . dest factor . one_minus_src_alpha @$ gl state manager . source factor . one @$ gl state manager . dest factor . zero ) ; return ; },vanilla assumes work
the response object does n't contain any relevant <PLACE_HOLDER> so we have to create a copy of values being sent over the network in case m jp settings is modified while awaiting response,final jetpack settings model sent jp data = new jetpack settings model ( m jp settings ) ; ++ m save request count ; word press . get rest client utilsv1_1 ( ) . set jetpack settings ( m site . get site id ( ) @$ params @$ new rest request . listener ( ) { @ override public void on response ( json object response ) { app log . d ( app log . t . api @$ __str__ ) ; m remote jp settings . monitor active = sent jp data . monitor active ; m remote jp settings . jetpack protect enabled = sent jp data . jetpack protect enabled ; m remote jp settings . jetpack protect whitelist . clear ( ),object contain information
this input reference is part of a join expression that refers an expression that comes from either node . to resolve it we need to find its index in the inner node 's expression list using the inner node <PLACE_HOLDER>,if ( resolving outer || resolving inner ) { preconditions . check state ( expr input indx < m_program . get project list ( ) . size ( ) ) ; final rex local ref input local ref = m_program . get project list ( ) . get ( expr input indx ) ; input idx = input local ref . get index ( ) ; rex node expr = m_program . get expr list ( ) . get ( input idx ) ; if ( expr . isa ( sql kind . cast ) ) { expr = ( ( rex call ) expr ) . get operands ( ) . get ( __num__ ) ; } final abstract expression volt expr = expr . accept (,node using predicate
if we 're restarting this transaction @$ and we only have local <PLACE_HOLDER> @$ add some dummy remote <PLACE_HOLDER> so that we can avoid injecting a borrow task into the local buddy site before the complete transaction message with the restart flag reaches it . right now @$ any read on a replicated table which has no distributed <PLACE_HOLDER> will generate these null fragments,boolean used null fragment = false ; if ( m_is restart && m_remote work == null ) { used null fragment = true ; m_remote work = new fragment task message ( m_local work . get initiatorhs id ( ) @$ m_local work . get coordinatorhs id ( ) @$ m_local work . get txn id ( ) @$ m_local work . get unique id ( ) @$ m_local work . is read only ( ) @$ false @$ false @$ m_n part txn @$ m_restart timestamp ) ; m_remote work . set empty for restart ( get next dependency id ( ) ) ; if ( ! m_have distributed init task && ! is for replay ( ) && ! is read only ( ) ) {,which has work
if user has specified a stopword <PLACE_HOLDER> other than the template,try { if ( ! conf . get ( __str__ ) . equals ( __str__ ) ) { stop words = new array list < string > ( ) ; string stop word ; buffered reader br = new buffered reader ( conf . get conf resource as reader ( ( conf . get ( __str__ ) ) ) ) ; while ( ( stop word = br . read line ( ) ) != null ) { stop words . add ( stop word ) ; } log . info ( __str__ @$ conf . get ( __str__ ) ) ; } int [ ] ngram arr = retrieve ngrams ( conf ) ; int mingram = ngram arr [ __num__ ] ; int maxgram = ngram,user specified string
we need to return the connection back to the jdbc pool . in order to do that we need to close it @$ to keep the remaining code readable one can add a headers end <PLACE_HOLDER> to close the connection .,routing context . add headers end handler ( done -> conn . close ( v -> { } ) ) ; routing context . next ( ) ;,one add handler
in case when remote tx has updated the current <PLACE_HOLDER> before .,ignore ( ) ;,tx updated transaction
create 100 threads @$ each will do its own <PLACE_HOLDER>,putter [ ] all = new putter [ threads100 ] ;,each do put
peek does n't change <PLACE_HOLDER>,assert equals ( __num__ @$ buffer . position ( ) ) ;,peek change position
given that a second user has selected the bubble clock <PLACE_HOLDER>,when ( m mock settings wrapper . get lock screen custom clock face ( secondary_user_id ) ) . then return ( bubble_clock ) ;,user selected channel
fall through to skip since comment has no <PLACE_HOLDER> .,case ls parser filter . filter_skip : default :,comment has comment
first @$ reduce to account asset balance . else ca n't complete this test <PLACE_HOLDER> .,account capsule to account = db manager . get account store ( ) . get ( byte array . from hex string ( to_address ) ) ; to account . reduce asset amount ( byte string . copy from utf8 ( asset_name ) . to byte array ( ) @$ total_supply - __num__ ) ; db manager . get account store ( ) . put ( to account . get address ( ) . to byte array ( ) @$ to account ) ; participate asset issue actuator actuator = new participate asset issue actuator ( ) ; actuator . set chain base manager ( db manager . get chain base manager ( ) ) . set any ( get contract ( __num__ ) ) ; transaction result,first complete case
do n't remove if user specifies <PLACE_HOLDER> or is recorded in previous run,if ( partition . get has user specified high watermark ( ) || this . work unit state . get prop ( configuration keys . work_unit_state_actual_high_water_mark_key ) != null ) { return false ; } return true ;,user specifies watermark
visual index is now the visual <PLACE_HOLDER> minus the bi di controls @$ which should match the <PLACE_HOLDER> of the bidi test.txt ordering .,if ( is ok && ordering count != visual index ) { errln ( __str__ + ordering count + __str__ + visual index ) ; is ok = false ; } if ( ! is ok ) { print error line ( ) ; string builder eord = new string builder ( __str__ ) ; for ( i = __num__ ; i < ordering count ; ++ i ) { eord . append ( __str__ ) . append ( ( char ) ( __str__ + ordering [ i ] ) ) ; } string builder aord = new string builder ( __str__ ) ; for ( i = __num__ ; i < result length ; ++ i ) { int logical index = ubidi . get logical index,which match order
for batch mode @$ the max watermark should force the <PLACE_HOLDER> to close,if ( watermark . is equal ( bounded window . timestamp_max_value ) ) { finish bundle ( emitter ) ; },watermark force window
the block would have been decremented for the scan case as it was wrapped before even the post next hook gets <PLACE_HOLDER> . giving some time for the block to be decremented,thread . sleep ( __num__ ) ; iterator < cached block > iterator = cache . iterator ( ) ; boolean used blocks found = false ; int ref count = __num__ ; while ( iterator . has next ( ) ) { cached block next = iterator . next ( ) ; block cache key cache key = new block cache key ( next . get filename ( ) @$ next . get offset ( ) ) ; if ( cache instanceof bucket cache ) { ref count = ( ( bucket cache ) cache ) . get rpc ref count ( cache key ) ; } else if ( cache instanceof combined block cache ) { ref count = ( ( combined block cache ) cache,hook gets executed
user 2 changes his <PLACE_HOLDER> to away,muc2 . change availability status ( __str__ @$ presence . mode . away ) ; thread . sleep ( __num__ ) ;,user changes state
if this does n't throw then the clues map does n't have duplicate <PLACE_HOLDER>,new coordinate clue ( __str__ @$ new world point ( __num__ @$ __num__ @$ __num__ ) @$ null ) ;,map have positions
if no child found a more specific <PLACE_HOLDER> @$ see if we have a value for the property,if ( ! decision found && property map . contains key ( property name ) ) { property value value = property map . get ( property name ) ; list < string > decision path = get decision path ( ) ; decision set . add decision ( new decision ( value . value @$ decision path @$ value . source ) ) ; decision found = true ; } return decision found ;,child found decision
if an instance of media tray @$ fall thru returning all media printable <PLACE_HOLDER>,if ( ! ( media name instanceof media size name ) ) { media name = null ; },instance printable attributes
should not do <PLACE_HOLDER> when cursor is null,matcher . cursor = null ; matcher . next is ( opcodes . nop ) ;,not do anything
flushing and minor compaction keep delete <PLACE_HOLDER>,region . flush ( true ) ; region . compact ( false ) ; assert equals ( __num__ @$ count delete markers ( region ) ) ; region . compact ( true ) ;,compaction keep marker
all partitioned tables get insert crud <PLACE_HOLDER>,add shim procedure ( prefix + __str__ @$ table @$ null @$ true @$ partition index @$ partitioncolumn @$ false ) ;,tables insert job
let client retry the same <PLACE_HOLDER> @$ add pending commit to sync later,if ( ! from read ) { commit ctx commit ctx = new commit ctx ( commit offset @$ channel @$ xid @$ pre op attr ) ; pending commits . put ( commit offset @$ commit ctx ) ; },client retry operation
method reports <PLACE_HOLDER> in nanoseconds across all processors .,cpu time /= __num__ * os . get available processors ( ) ; double cpu = __num__ ; if ( prev cpu time > __num__ ) { long cpu time diff = cpu time - prev cpu time ; cpu = math . min ( __num__ @$ ( double ) cpu time diff / metrics_update_freq ) ; },method reports cpu
here we restore our wallet from a seed with no passphrase . also have a <PLACE_HOLDER> at the backup to mnemonic seed.java example that shows how to backup a wallet by creating a mnemonic sentence .,string seed code = __str__ ; string passphrase = __str__ ; long creationtime = __num__ ; deterministic seed seed = new deterministic seed ( seed code @$ null @$ passphrase @$ creationtime ) ;,here have look
robolectric does n't like this <PLACE_HOLDER>,if ( ! test util . are robolectric tests running ( ) ) { ok http builder . add interceptor ( new chuck interceptor ( application . get application context ( ) ) ) ; },robolectric like test
client establishes the <PLACE_HOLDER>,socket s1 = new socket ( ) ; s1 . connect ( isa ) ;,client establishes connection
check if the caller has the <PLACE_HOLDER> to invoke 'query m beans ',if ( sm != null ) { checkm bean permission ( ( string ) null @$ null @$ null @$ __str__ ) ; set < object instance > list = querym beans impl ( name @$ null ) ; set < object instance > allowed list = new hash set < object instance > ( list . size ( ) ) ; for ( object instance oi : list ) { try { checkm bean permission ( oi . get class name ( ) @$ null @$ oi . get object name ( ) @$ __str__ ) ; allowed list . add ( oi ) ; } catch ( security exception e ) { } } return filter list of object instances ( allowed list @$ query ),caller has right
x sends x crossing to all hierarchy so if the edge of child equals to ancestor and mouse enters child @$ the ancestor will get an <PLACE_HOLDER> too . from java point the <PLACE_HOLDER> is bogus as ancestor is obscured @$ so if the child can get java <PLACE_HOLDER> itself @$ we skip it on ancestor .,long child wnd = xce . get_subwindow ( ) ; if ( child wnd != x constants . none ) { x base window child = x toolkit . window tox window ( child wnd ) ; if ( child != null && child instanceof x window && ! child . is event disabled ( xev ) ) { return ; } },child get x
if queue has default label <PLACE_HOLDER> @$ and rr does n't have @$ use the default label <PLACE_HOLDER> of queue,if ( label exp == null && queue info != null && resource request . any . equals ( res req . get resource name ( ) ) ) { log . debug ( __str__ @$ queue info . get default node label expression ( ) ) ; label exp = queue info . get default node label expression ( ) ; },queue has expression
has to be tied for best break if we 've found one in ` best arborescence ` at this point all edges in ` candidates ` have equal <PLACE_HOLDER> @$ and if one of them is in ` best arborescence ` it will be first,final exclusive edge best edge = candidates . remove first ( ) ;,edges have results
first renderer handles <PLACE_HOLDER> .,map < string @$ integer > first renderer mapped capabilities = new hash map < > ( ) ; first renderer mapped capabilities . put ( english . id @$ format_handled ) ; first renderer mapped capabilities . put ( german . id @$ format_unsupported_subtype ) ; renderer capabilities first renderer capabilities = new fake mapped renderer capabilities ( c . track_type_text @$ first renderer mapped capabilities ) ;,renderer handles english
split 0 times should throw <PLACE_HOLDER>,try { parts = bytes . split ( low @$ high @$ __num__ ) ; assert true ( __str__ @$ false ) ; } catch ( illegal argument exception iae ) { },split throw exception
higher order <PLACE_HOLDER> return <PLACE_HOLDER> .,return true ; case switch : case case :,functions return functions
share ur is use the content : <PLACE_HOLDER> when able @$ which looks bad when displayed in the url bar .,if ( service . is download openable in browser ( is off the record @$ mime type ) ) { uri file uri = uri . from file ( file ) ; uri share uri = get uri for item ( file ) ; string normalized mime type = intent . normalize mime type ( mime type ) ; intent intent = get media viewer intent for download item ( file uri @$ share uri @$ normalized mime type ) ; intent handler . start activity for trusted intent ( intent @$ context ) ; return true ; },ur use download
each replica could have own end point factory <PLACE_HOLDER>,string end point factory = sub config . has path ( end_point_factory_class ) ? sub config . get string ( end_point_factory_class ) : default_end_point_factory_class ; end point factory factory = end point factory resolver . resolve class ( end point factory ) . new instance ( ) ; this . replicas . add ( factory . build replica ( sub config @$ replica name @$ this . selection config ) ) ;,replica have class
moved region remove <PLACE_HOLDER> to a guarded block . if a region is getting created it wo n't allow it to destroy any region .,synchronized ( region op lock ) { object name object name = m beanjmx adapter . get regionm bean name ( internal cache . get distributed system ( ) . get distributed member ( ) @$ region . get full path ( ) ) ; try { regionm bean regionm bean = ( regionm bean ) service . get local regionm bean ( region . get full path ( ) ) ; if ( regionm bean != null ) { regionm bean . stop monitor ( ) ; } } catch ( management exception e ) { if ( logger . is debug enabled ( ) ) { logger . debug ( e . get message ( ) @$ e ) ; } return ; } service .,region remove link
make all cache elements for this guy go <PLACE_HOLDER> .,remove stale entries ( class value ) ;,elements go poof
card array is cloned in deck task @$ which pays <PLACE_HOLDER> to memory pressure,m undo . add ( new object [ ] { type @$ o [ __num__ ] } ) ; break ; default : timber . e ( __str__ @$ type ) ; break ;,which pays usions
only the main task stack change notification requires a <PLACE_HOLDER> .,m handler . send message delayed ( msg @$ notify_task_stack_change_listeners_delay ) ;,notification requires delivery
java does not do this right <PLACE_HOLDER> . the number of bytes in a file is a long @$ but the length of a string is an int . why ?,line guess = ( int ) ( f . length ( ) / ( long ) __str__ . length ( ) ) ; line guess += ( line guess / __num__ ) ;,java do way
use a flag bc we need to handle is encap nonce get either <PLACE_HOLDER>,boolean failed = false ;,nonce get way
no content <PLACE_HOLDER> check is performed when the md 5 check is enabled @$ since a correct md 5 check would imply a correct content <PLACE_HOLDER> .,try { message digest digest = message digest . get instance ( __str__ ) ; is = new digest validation input stream ( is @$ digest @$ server side hash ) ; } catch ( no such algorithm exception e ) { log . warn ( __str__ + __str__ @$ e ) ; },check imply hash
create a view coming from the locator that <PLACE_HOLDER> with the installed view,member identifier locator member id = new internal distributed member ( __str__ @$ mock members [ mock members . length - __num__ ] . get membership port ( ) + __num__ ) ; locator member id . set vm kind ( cluster distribution manager . locator_dm_type ) ; list < member identifier > new member list = new array list < > ( members ) ; new member list . add ( locator member id ) ; gms membership view locator view = new gms membership view ( locator member id @$ installed view . get view id ( ) + __num__ @$ new member list ) ;,view coming conflicts
client 1 put <PLACE_HOLDER>,client1 . invoke ( ( ) -> { region < string @$ ticker data > region = get cache ( ) . get region ( region name ) ; do put all ( region @$ __str__ @$ one_hundred * __num__ ) ; assert that ( region . size ( ) ) . is equal to ( one_hundred * __num__ ) ; } ) ;,client put all
third volume @$ again with 3 mb free <PLACE_HOLDER> .,volumes . add ( mockito . mock ( fs volume spi . class ) ) ; mockito . when ( volumes . get ( __num__ ) . get available ( ) ) . then return ( __num__ * __num__ * __num__ ) ;,volume mb space
msc.sync all will trigger a <PLACE_HOLDER>,if ( spt == null ) break ;,all trigger sync
force eof if a read takes <PLACE_HOLDER> at this position,curr buf idx -- ; buf position = buffer_size ; curr buf = file . get buffer ( curr buf idx ) ; buf position = __num__ ; long buflen = length - buf start ; buf length = buflen > buffer_size ? buffer_size : ( int ) buflen ;,read takes place
note : <PLACE_HOLDER> format used by date format only uses int <PLACE_HOLDER>s . remainder operation on 32 bit platform using long is significantly slower than int . so @$ this method casts long <PLACE_HOLDER> into int .,int number = ( int ) numberl ; int limit = decimal buf . length < max int digits ? decimal buf . length : max int digits ; int index = limit - __num__ ; while ( true ) { decimal buf [ index ] = digits [ ( number % __num__ ) ] ; number /= __num__ ; if ( index == __num__ || number == __num__ ) { break ; } index -- ; } int padding = min int digits - ( limit - index ) ; for ( ; padding > __num__ ; padding -- ) { decimal buf [ -- index ] = digits [ __num__ ] ; } int length = limit - index ; to append to . append (,method casts part
insert the video . the command sends three <PLACE_HOLDER>s . the first specifies which information the api request is setting and which information the api response should return . the second <PLACE_HOLDER> is the video resource that contains metadata about the new video . the third <PLACE_HOLDER> is the actual video content .,you tube . videos . insert video insert = youtube . videos ( ) . insert ( __str__ @$ video object defining metadata @$ media content ) ;,command sends summary
\u 00 a <PLACE_HOLDER> and \uffe <PLACE_HOLDER> are actually the same symbol @$ just different code points . but the ri returns the \uffe <PLACE_HOLDER> and android returns those with \u 00 a <PLACE_HOLDER>,string [ ] yen = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; string [ ] dollar = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } ;,ri returns word
if error while generating pojo do not try compile <PLACE_HOLDER> as they very likely depends hence fail too .,if ( has errors ( ) ) { return ; },pojo try files
! drained if dispatch queued new <PLACE_HOLDER> on this dispatcher,synchronized ( mutex ) { drained = queue . is empty ( ) ; },dispatch queued messages
we only care about it if the image <PLACE_HOLDER> matches the current base <PLACE_HOLDER>,if ( variable declarator id . has image equal to ( base name ) ) { boolean allocation found = declarator . get first descendant of type ( ast allocation expression . class ) != null ; boolean iterator = is iterator ( ) || is factory ( declarator ) ; boolean for loop = is for loop ( declarator ) ; assignments . add ( new assignment ( declarator . get begin line ( ) @$ allocation found @$ iterator @$ for loop ) ) ; },name matches name
check notification has css <PLACE_HOLDER> which describes notification type,if ( get attribute ( __str__ ) . contains ( notif type ) ) { return entry . get value ( ) ; },notification has attribute
the old request should have no <PLACE_HOLDER> on new tracker,request . record miss ( ) ; assert equals ( __num__ @$ tracker . get total hit count ( ) ) ; assert equals ( __num__ @$ tracker . get total miss count ( ) ) ; assert equals ( __num__ @$ tracker . get total miss match count ( ) ) ; assert equals ( __num__ @$ tracker . get total eviction count ( ) ) ; assert equals ( __num__ @$ tracker . get total invalidation count ( ) ) ; assert equals ( __num__ @$ tracker . get total load success count ( ) ) ; assert equals ( __num__ @$ tracker . get total load exception count ( ) ) ; assert equals ( __num__ @$ tracker . get total retrieval time ( ),request have effect
test the same data and <PLACE_HOLDER> with prior @$ should get the same <PLACE_HOLDER> except for the intercept,glm = new glm ( params ) ; model4 = glm . train model ( ) . get ( ) ; assert equals ( __str__ + model3 . _output . _training_metrics . _mse + __str__ + model4 . _output . _training_metrics . _mse @$ model3 . _output . _training_metrics . _mse @$ model4 . _output . _training_metrics . _mse @$ __num__ ) ; assert equals ( __str__ + ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev + __str__ + ( ( model metrics binomialglm ) model4 . _output . _training_metrics ) . _res dev @$ ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev @$ ( ( model metrics binomialglm ) model4 . _output .,data get model
cancel currently executing tasks wait a <PLACE_HOLDER> for tasks to respond to being cancelled,if ( ! executor . await termination ( __num__ @$ time unit . seconds ) ) system . out . println ( __str__ ) ;,tasks wait while
more complicated race ! ! some client managed to acquire the provider and release it before the <PLACE_HOLDER> was completed . continue the <PLACE_HOLDER> @$ and abort the next remove message .,prc . remove pending = false ; final i binder j binder = prc . holder . provider . as binder ( ) ; provider ref count existing prc = m provider ref count map . get ( j binder ) ; if ( existing prc == prc ) { m provider ref count map . remove ( j binder ) ; } for ( int i = m provider map . size ( ) - __num__ ; i >= __num__ ; i -- ) { provider client record pr = m provider map . value at ( i ) ; i binder my binder = pr . m provider . as binder ( ) ; if ( my binder == j binder ) { m provider map,race continue test
verify resource request sent for map have appropriate node label <PLACE_HOLDER> as per the configuration,validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ; validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ; validate labels requests ( mock scheduler . last ask . get ( __num__ ) @$ false ) ;,request have types
britis<PLACE_HOLDER> pronounce <PLACE_HOLDER> in t<PLACE_HOLDER>is word americans give it ' <PLACE_HOLDER> ' for t<PLACE_HOLDER>e name @$ no ' <PLACE_HOLDER> ' for t<PLACE_HOLDER>e plant,if ( string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) || string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ ) || string at ( ( m_current + __num__ ) @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) { if ( ( m_current == __num__ ) && string at ( m_current @$ __num__ @$ __str__ @$ __str__ ) ) { if ( m_encode vowels ) { metaph add ( __str__ @$ __str__ ) ; } else { metaph add ( __str__ @$ __str__ ) ; } } else if ( ( m_current == __num__ ) || m_encode vowels ) { metaph add ( __str__ ) ; } m_current ++ ;,h give d
all project related messages which changes the <PLACE_HOLDER>,string current etag = string . format ( __str__ @$ objects . hash ( get last modified ( ) @$ get project messages ( ) @$ get env ( ) . get date for last index run ( ) != null ? get env ( ) . get date for last index run ( ) . get time ( ) : __num__ @$ info . get version ( ) ) ) ;,which changes timestamp
mix in various ways to specify no catalog . use empty <PLACE_HOLDER> here .,dep bytes = new string ( client utils . file to bytes ( new file ( deploymenturl ) ) @$ constants . utf8encoding ) ; results = client . call procedure ( __str__ @$ __str__ @$ dep bytes ) . get results ( ) ; assert true ( results . length == __num__ ) ; thread . sleep ( __num__ ) ;,mix use array
assert that the disk has seen <PLACE_HOLDER> of the updates,region version vector rvv = getrvv ( vm1 ) ; region version vector diskrvv = get diskrvv ( vm1 ) ; assert samervv ( rvv @$ diskrvv ) ;,disk seen all
the new tree root node id must match the original <PLACE_HOLDER> to be able to reconnect the subtrees,new tree . set id ( sub tree . get id ( ) ) ; tree permutations . add ( new tree ) ;,id match one
we want to keep track of the last 1000 events in the files so that we can add them to 'ring buffer ' . however @$ we do n't want to add them directly to ring buffer @$ because once they are added to ring buffer @$ they are available in query results . as a result @$ we can have the issue where,final ring buffer < provenance event record > latest records = new ring buffer < > ( __num__ ) ;,records copy times
if already setup return the <PLACE_HOLDER>,if ( null == this . crouton view ) { initialize crouton view ( ) ; } return crouton view ;,setup return view
all done @$ no <PLACE_HOLDER> or rollback from tm,if ( xa resource . xa_rdonly == response . get result ( ) ) { super . on response ( command ) ; },all done operations
conversions on parent would have <PLACE_HOLDER>,attribute conversion info conversion = locate attribute conversion info ( property name ) ; if ( conversion != null ) { return conversion ; } return null ;,conversions have priority
m total length contains the <PLACE_HOLDER> already,child top = m padding top + bottom - top - m total length ; break ;,length contains padding
lie data <PLACE_HOLDER> invaid data <PLACE_HOLDER> .,byte i71 [ ] = { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ;,type invaid type
check if the .job file has an underlying job <PLACE_HOLDER>,if ( job config . has path ( gobblin_job_template_key ) ) { uri job template relative uri = new uri ( job config . get string ( gobblin_job_template_key ) ) ; if ( ! job template relative uri . get scheme ( ) . equals ( fs_scheme ) ) { throw new runtime exception ( __str__ + fs_scheme + __str__ + flow template diruri . get scheme ( ) ) ; } path full job template path = path utils . merge paths ( new path ( template catalog dir ) @$ new path ( job template relative uri ) ) ; job config = job config . with fallback ( load hocon file at path ( full job template path ) ) ; } job templates .,file has template
we do n't care as long as nobody calls the <PLACE_HOLDER> :,compilation helper . set args ( arrays . as list ( __str__ @$ temporary folder . get root ( ) . get absolute path ( ) @$ __str__ ) ) . add source lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . do test ( ) ;,nobody calls method
join to next if parent of p paragraph has another <PLACE_HOLDER> after p paragraph @$ and it is n't a leaf .,element parent = p paragraph . get parent element ( ) ; int p paragraph index = parent . get element index ( offset ) ; if ( ( p paragraph index + __num__ ) < parent . get element count ( ) && ! parent . get element ( p paragraph index + __num__ ) . is leaf ( ) ) { last start spec . set direction ( element spec . join next direction ) ; },parent has element
but they are not . however @$ i bet someone will try and treat it like a target @$ so find the owning cell if necessary @$ and then fully resolve the <PLACE_HOLDER> against the owning cell 's root .,matcher matcher = include_path_pattern . matcher ( include ) ; preconditions . check state ( matcher . matches ( ) ) ; optional < string > cell name = optional . of nullable ( matcher . group ( __num__ ) ) ; string include path = matcher . group ( __num__ ) ; return cell path resolver . get cell path ( cell name ) . map ( cell path -> cell path . resolve ( include path ) ) . or else get ( ( ) -> cell . get filesystem ( ) . resolve ( include path ) ) ;,then resolve path
according to david ackerman @$ the compression type can change the <PLACE_HOLDER> of the document .,if ( compression type . equals ( __str__ ) ) { aiff header . set endian ( aiff audio header . endian . little_endian ) ; } bytes left -= __num__ ; compression name = aiff util . read pascal string ( raf ) ; bytes left -= compression name . length ( ) + __num__ ;,type change length
hope the next local variable would have a smaller <PLACE_HOLDER> .,continue ;,hope have value
if the structure size indicates there are more fields @$ we are dealing with a newer <PLACE_HOLDER> of the structure . each size check represents a new <PLACE_HOLDER> of the structure .,if ( reader . get pointer index ( ) - index < size ) { security cookie = read pointer ( reader ) ; se handler table = read pointer ( reader ) ; se handler count = read pointer ( reader ) ; } if ( reader . get pointer index ( ) - index < size ) { guard cfc check function pointer = read pointer ( reader ) ; guard cf dispatch function pointer = read pointer ( reader ) ; guard cf function table = read pointer ( reader ) ; guard cf function count = read pointer ( reader ) ; guard flags = new guard flags ( reader . read next int ( ) ) ; } if ( reader . get,check represents version
push queries only return value <PLACE_HOLDER> @$ but query metadata schema includes key and meta :,final logical schema stored schema = query metadata . get logical schema ( ) ; final builder actual schema builder = logical schema . builder ( ) . no implicit columns ( ) ; stored schema . value ( ) . for each ( actual schema builder :: value column ) ; return streamed row . header ( no_query_id @$ actual schema builder . build ( ) ) ;,queries return columns
block types other than <PLACE_HOLDER> blocks always have <PLACE_HOLDER> block encoding.none . to avoid false negative cache misses @$ only perform this check if cached block is a <PLACE_HOLDER> block .,if ( cached block . get block type ( ) . is data ( ) && ! actual data block encoding . equals ( expected data block encoding ) ) { if ( ! expected data block encoding . equals ( data block encoding . none ) && ! actual data block encoding . equals ( data block encoding . none ) ) { log . info ( __str__ + cache key + __str__ + __str__ + expected data block encoding + __str__ + actual data block encoding + __str__ + path ) ; return and evict block ( cache @$ cache key @$ cached block ) ; } return null ; },types have data
when amrmp roxy ha is enabled @$ nmss should not have the uam <PLACE_HOLDER> @$ it should be in registry,assert . assert false ( recovered data map . contains key ( sc entry ) ) ;,nmss have key
wait for auth failed <PLACE_HOLDER> from client 's <PLACE_HOLDER> thread .,try ( zoo keeper ignored = create client ( new my watcher ( ) @$ host port ) ) { auth failed . await ( ) ; },wait failed event
find all aliases matching current <PLACE_HOLDER>,if ( ! common utils . is empty ( request . get active query ( ) . get text ( ) ) && ! common utils . is empty ( word part ) ) { if ( word part . index of ( request . get context ( ) . get syntax manager ( ) . get struct separator ( ) ) != - __num__ || word part . equals ( all_columns_pattern ) ) { return ; } sql dialect sql dialect = sql utils . get dialect from data source ( request . get context ( ) . get data source ( ) ) ; string table name pattern = get table name pattern ( sql dialect ) ; string table alias pattern = get table alias,aliases matching query
our protocol does not use an <PLACE_HOLDER> or a preamble .,final multi partmime writer . builder attachments builder = new multi partmime writer . builder ( ) ; for ( final object data source : streaming attachments ) { assert ( data source instanceof rest li attachment data source writer || data source instanceof rest li data source iterator ) ; if ( data source instanceof rest li attachment data source writer ) { attachment utils . append single attachment to builder ( attachments builder @$ ( rest li attachment data source writer ) data source ) ; } else { attachment utils . append multiple attachments to builder ( attachments builder @$ ( rest li data source iterator ) data source ) ; } } final multi partmime writer multi partmime writer = attachment utils . create,protocol use envelope
compiler bug : a strictfp class sets all <PLACE_HOLDER> to strictfp,flags &= ~ code constants . acc_strict ;,class sets flags
we need to log these independently of cards @$ as one side may have more card <PLACE_HOLDER>,_log rem ( ids @$ consts . rem_note ) ; m db . execute ( __str__ + strids ) ;,side have data
only allocate the array if there are enough bytes available . this only works for byte array input stream . the assignment below ensures that buffer has the required <PLACE_HOLDER> .,byte array input stream array input = buffer ; if ( array input . available ( ) < length ) { throw new io exception ( __str__ ) ; } byte [ ] bytes = new byte [ length ] ; array input . read ( bytes ) ; if ( is constructed ( ) ) { der input stream in = new der input stream ( bytes @$ __num__ @$ bytes . length @$ buffer . allowber ) ; bytes = null ; while ( in . available ( ) != __num__ ) { bytes = append ( bytes @$ in . get octet string ( ) ) ; } } return bytes ;,buffer has length
caller might want different <PLACE_HOLDER>,convert temps ( main @$ units ) ;,caller want values
check if user is member of group since api does n't return typed <PLACE_HOLDER>,if ( identity service . create user query ( ) . member of group ( group . get id ( ) ) . user id ( member ship . get user id ( ) ) . count ( ) > __num__ ) { throw new flowable conflict exception ( __str__ + member ship . get user id ( ) + __str__ + group . get id ( ) + __str__ ) ; } identity service . create membership ( member ship . get user id ( ) @$ group . get id ( ) ) ; response . set status ( http status . created . value ( ) ) ; return rest response factory . create membership response ( member ship . get user id ( ),api return exception
remap duplicate <PLACE_HOLDER> to the token id we created for the first instance of any duplicate token name .,if ( remapping indexes . not empty ( ) ) { remapping indexes . for each key value ( ( index @$ creating index ) -> ids [ index ] = ids [ creating index ] ) ; } return created tokens ;,remap duplicate indexes
the fractional seconds may have <PLACE_HOLDER> through 6 digits but no more and no less .,parse date fails ( __str__ ) ; parse date fails ( __str__ ) ;,seconds have 3
google camera adds the preview <PLACE_HOLDER> as well as capture <PLACE_HOLDER> @$ for still capture,still builder . add target ( image reader . get surface ( ) ) ; if ( image reader raw != null ) still builder . add target ( image reader raw . get surface ( ) ) ; capture session . stop repeating ( ) ;,camera adds surface
consider our cached version dirty since app code now has a <PLACE_HOLDER> to it,if ( m drawable == m recycleable bitmap drawable ) { m recycleable bitmap drawable = null ; },version has reference
verify that mock provider 2 contains a new group named the <PLACE_HOLDER> as the parent meta group of the contact we just moved,contact group new grpp2 = mcl slick fixture . mock pres op setp2 . get server stored contact list root ( ) . get group ( mcl slick fixture . metap1 grp1 . get group name ( ) ) ; assert not null ( __str__ + mcl slick fixture . emilp2 . get display name ( ) + __str__ @$ new grpp2 ) ;,group named same
add g 1 and all descendants their <PLACE_HOLDER>,auth entity = auth entity factory . apply ( null ) ; auth entity . set for groups ( new tree set < > ( ) ) ; auth entity . set for groups ( __str__ ) ; auth entity . load ( new tree map < > ( ) ) ; assert equals ( new tree set < > ( arrays . as list ( new string [ ] { __str__ @$ __str__ } ) ) @$ auth entity . for groups ( ) ) ; assert equals ( new tree set < > ( arrays . as list ( new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ) ) @$ auth entity . for projects,g descendants ownership
minimal receive buffer <PLACE_HOLDER> .,final silent server ss = new silent server ( __num__ ) ;,minimal receive size
divide space according to expansion <PLACE_HOLDER>s if any span has a <PLACE_HOLDER>,int total expansion = __num__ ; for ( int i = __num__ ; i < span size ; i ++ ) { int item index = span start index + i ; total expansion += expansion ratios [ item index ] ; } for ( int i = __num__ ; i < span size ; i ++ ) { int item index = span start index + i ; int expansion ; if ( total expansion == __num__ ) { expansion = needed extra space / span size ; } else { expansion = ( int ) ( needed extra space * expansion ratios [ item index ] / total expansion ) ; } dimensions [ item index ] += expansion ; allocated extra space += expansion ;,span has ratio
if state machine instance not exists stop <PLACE_HOLDER>,if ( framework error code . state machine instance not exists . equals ( e . get errcode ( ) ) ) { return branch status . phase two_ committed ; },instance exists retry
create a bitmap of the icon which is <PLACE_HOLDER> the widget 's remoteview requires .,icon . set color filter ( m icon utilities . get disabled color filter ( ) ) ; return m icon utilities . create icon bitmap ( icon ) ;,remoteview requires what
user 10 does n't have <PLACE_HOLDER> 3 @$ so no callback .,verify ( c10 @$ times ( __num__ ) ) . on shortcuts changed ( eq ( calling_package_3 ) @$ any ( list . class ) @$ any ( user handle . class ) ) ; assert shortcut ids ( shortcuts . get value ( ) @$ __str__ ) ; assert equals ( start_time + __num__ @$ find shortcut ( shortcuts . get value ( ) @$ __str__ ) . get last changed timestamp ( ) ) ;,user have package
regression test @$ used to throw a class cast exception because the job launcher created a query <PLACE_HOLDER> instead of row count <PLACE_HOLDER>,long [ ] row counts = execute ( __str__ @$ $$ ( $ ( __str__ ) ) ) ; assert that ( row counts . length @$ is ( __num__ ) ) ;,launcher created parameter
center inside @$ but bitmap bounds exceed the resize <PLACE_HOLDER> ... so change it to fit center .,if ( scale mode == scale mode . center inside ) { if ( resize width <= b . get width ( ) || resize height <= b . get height ( ) ) scale mode = scale mode . fit center ; },inside exceed ratio
when the trailer contains an orca report @$ callbacks for both listeners will be invoked . both listener will receive the same orca report <PLACE_HOLDER> @$ which means deserialization happens only once .,trailer . put ( orca reporting tracer factory . orca_endpoint_load_metrics_key @$ orca load report . get default instance ( ) ) ; child tracer . inbound trailers ( trailer ) ; argument captor < orca load report > parent report cap = argument captor . for class ( null ) ; argument captor < orca load report > child report cap = argument captor . for class ( null ) ; verify ( orca listener1 ) . on load report ( parent report cap . capture ( ) ) ; verify ( orca listener2 ) . on load report ( child report cap . capture ( ) ) ; assert that ( parent report cap . get value ( ) ) . is equal to ( orca load,listener receive type
java serialization only supported <PLACE_HOLDER> if not defaulting to json if java serliazation needed together with json defaulting then add to custom post variable types,if ( ! serializepoj os in variables to json ) { variable types . add type ( new byte array type ( ) ) ; variable types . add type ( new serializable type ( serializable variable type track deserialized objects ) ) ; variable types . add type ( new custom object type ( __str__ @$ item instance . class ) ) ; variable types . add type ( new custom object type ( __str__ @$ message instance . class ) ) ; } if ( custom post variable types != null ) { for ( variable type custom variable type : custom post variable types ) { variable types . add type ( custom variable type ) ; } },serialization supported everything
take from <PLACE_HOLDER> 1 even though mux said <PLACE_HOLDER> 0 @$ since <PLACE_HOLDER> 0 empty,assert equals ( call @$ fcq . take ( ) ) ; assert equals ( __num__ @$ fcq . size ( ) ) ;,mux said queue
the summary section displays up to two text views <PLACE_HOLDER> by <PLACE_HOLDER> .,m summary layout = new linear layout ( get context ( ) ) ; m summary layout . add view ( m summary left text view @$ left layout params ) ; m summary layout . add view ( m summary right text view @$ right layout params ) ; main section layout . add view ( m summary layout @$ new linear layout . layout params ( layout params . match_parent @$ layout params . wrap_content ) ) ; set summary text ( null @$ null ) ; create main section content ( main section layout ) ; return main section layout ;,displays views one
ensure fc 2 has <PLACE_HOLDER>,assert . assert true ( is dir ( fc2 @$ path ) ) ; assert . assert true ( exists ( fc2 @$ path ) ) ; assert . assert false ( is file ( fc2 @$ path ) ) ;,fc has directory
catcher also receives the <PLACE_HOLDER> @$ prepended :,catch args . add ( __num__ @$ thrown ) ; helper . assert called ( __str__ @$ catch args ) ; asserteq ( cast . apply ( catch args ) @$ returned ) ;,catcher receives exception
the acceptor event loop thread needs to be from a different pool otherwise can get <PLACE_HOLDER> in accepted connections under a lot of load,acceptor event loop group = transport . event loop group ( transport . acceptor_event_loop_group @$ __num__ @$ acceptor event loop thread factory @$ __num__ ) ; metrics = initialise metrics ( options ) ; int worker pool size = options . get worker pool size ( ) ; executor service worker exec = new thread pool executor ( worker pool size @$ worker pool size @$ __num__ @$ time unit . milliseconds @$ new linked transfer queue < > ( ) @$ new vertx thread factory ( __str__ @$ checker @$ true @$ options . get max worker execute time ( ) @$ options . get max worker execute time unit ( ) ) ) ; pool metrics worker pool metrics = metrics != null ? metrics .,event get deadlock
skip if statements as they have oplog file exists <PLACE_HOLDER> .,if ( line . starts with ( __str__ ) ) { continue ; } else if ( line . contains ( windows script generator . exit_marker ) ) { break ; } else { begin index = line . last index of ( __str__ ) + __num__ ; end index = line . index of ( windows script generator . robocopy_no_job_header @$ begin index ) - __num__ ; oplog name = line . substring ( begin index @$ end index ) . trim ( ) ; add oplog line ( oplog name @$ line ) ; },statements exists information
do not overwrite interface <PLACE_HOLDER> with instance <PLACE_HOLDER> do not overwrite private <PLACE_HOLDER> note : private <PLACE_HOLDER> from parent classes are not shown here @$ but when doing the multimethod connection step @$ we overwrite <PLACE_HOLDER> of the parent class with <PLACE_HOLDER> of a subclass and in that case we want to keep the private <PLACE_HOLDER>,if ( match . is private ( ) || ( ! is non real method ( match ) && match . get declaring class ( ) . is interface ( ) && ! method . get declaring class ( ) . is interface ( ) ) ) { } else { cached class methodc = method . get declaring class ( ) ; cached class matchc = match . get declaring class ( ) ; if ( methodc == matchc ) { if ( is non real method ( method ) ) { return method ; } } else if ( ! methodc . is assignable from ( matchc . get the class ( ) ) ) { return method ; } },methods overwrite methods
the data matrix will not fit in this size @$ so the matrix should come back <PLACE_HOLDER>,int too small = __num__ ; data matrix writer writer = new data matrix writer ( ) ; bit matrix matrix = writer . encode ( __str__ @$ barcode format . data_matrix @$ too small @$ too small @$ null ) ; assert not null ( matrix ) ; assert true ( too small < matrix . get width ( ) ) ; assert true ( too small < matrix . get height ( ) ) ;,matrix come bigger
if tools not found @$ no point in trying a url class loader so rethrow the original <PLACE_HOLDER> .,if ( ! file . exists ( ) ) throw e ; url [ ] urls = { file . touri ( ) . tourl ( ) } ; trace ( fine @$ urls [ __num__ ] . to string ( ) ) ; cl = url class loader . new instance ( urls ) ; ref tool class loader = new weak reference < class loader > ( cl ) ;,point rethrow exception
replace values if better probability log <PLACE_HOLDER> @$ if same edit distance or one space difference,if ( ( i == max segmentation word length ) || ( ( ( compositions [ circular index ] . distance sum + top ed == compositions [ destination index ] . distance sum ) || ( compositions [ circular index ] . distance sum + separator length + top ed == compositions [ destination index ] . distance sum ) ) && ( compositions [ destination index ] . probability log sum < compositions [ circular index ] . probability log sum + top probability log ) ) || ( compositions [ circular index ] . distance sum + separator length + top ed < compositions [ destination index ] . distance sum ) ) { compositions [ destination index ] . segmented string = compositions [,probability log sum
fail verification which should produce an <PLACE_HOLDER> that is send back to the client .,return new trust manager [ ] { new x509 trust manager ( ) { @ override public void check client trusted ( x509 certificate [ ] x509 certificates @$ string s ) throws certificate exception { throw new certificate exception ( ) ; } @ override public void check server trusted ( x509 certificate [ ] x509 certificates @$ string s ) { } @ override public x509 certificate [ ] get accepted issuers ( ) { return empty arrays . empty_x509_certificates ; } } } ;,which produce exception
this is a code <PLACE_HOLDER> @$ ensure this document uses the same code <PLACE_HOLDER> .,if ( ! character . is digit ( library min sdk version . char at ( __num__ ) ) ) { if ( ! library min sdk version . equals ( get min sdk version ( default_sdk_version ) ) ) { merging report . add message ( get source file ( ) @$ merging report . record . severity . error @$ string . format ( __str__ + __str__ @$ get min sdk version ( default_sdk_version ) @$ library min sdk version @$ lower priority document . get source file ( ) . print ( false ) ) ) ; return ; } },document uses name
sanity check : get only element of a set @$ to ensure all segments have the same data <PLACE_HOLDER> .,final string data source = iterables . get only element ( stream support . stream ( segments . spliterator ( ) @$ false ) . map ( segment id :: get data source ) . collect ( collectors . to set ( ) ) ) ; final multiple specific segment spec query segment spec = new multiple specific segment spec ( stream support . stream ( segments . spliterator ( ) @$ false ) . map ( segment id :: to descriptor ) . collect ( collectors . to list ( ) ) ) ; final segment metadata query segment metadata query = new segment metadata query ( new table data source ( data source ) @$ query segment spec @$ new all column includerator ( ) @$,segments have source
<PLACE_HOLDER> actually generates the <PLACE_HOLDER>,naming context data store impl = ( naming context data store ) this ; synchronized ( impl ) { impl . list ( how_many @$ bl @$ bi ) ; } if ( debug && bl . value != null ) dprint ( __str__ + how_many + __str__ + bl . value . length + __str__ + bi . value ) ;,list generates list
when the node has only one <PLACE_HOLDER>,if ( parent == x ) { x = parent . left == null ? parent . right : parent . left ; } else if ( x . left == null ) { if ( parent . left == x ) parent . left = x . right ; else parent . right = x . right ; } else { if ( parent . left == x ) parent . left = x . left ; else parent . right = x . left ; },node has child
if the incoming uri matches a single note <PLACE_HOLDER> @$ does the delete based on the incoming data @$ but modifies the where clause to restrict it to the particular note <PLACE_HOLDER> .,case main_id :,uri matches id
an estimate of how much extra memory is needed before we can go ahead and expand the hash table . this includes the new <PLACE_HOLDER> for values @$ group ids @$ and values by group id as well as the size of the current page,preallocated memory in bytes = ( new capacity - hash capacity ) * ( long ) ( long . bytes + integer . bytes ) + ( calculate max fill ( new capacity ) - max fill ) * long . bytes + current page size in bytes ; if ( ! update memory . update ( ) ) { return false ; },estimate includes capacity
direct column <PLACE_HOLDER> on a complex column @$ expressions can not operate on complex columns @$ only postaggs wrap the column <PLACE_HOLDER> in a field <PLACE_HOLDER> postagg so that other postaggs can use it,if ( post aggregator complex direct column is ok ( input row signature @$ post aggregator expression @$ post aggregator rex node ) ) { final post aggregator post aggregator = new field access post aggregator ( post aggregator visitor . get output name prefix ( ) + post aggregator visitor . get and increment counter ( ) @$ post aggregator expression . get direct column ( ) ) ; post aggregator visitor . add post agg ( post aggregator ) ; row order . add ( post aggregator . get name ( ) ) ; } else if ( post aggregator direct column is ok ( input row signature @$ post aggregator expression @$ post aggregator rex node ) ) { row order . add ( post,postaggs wrap access
array contains local <PLACE_HOLDER> & rack <PLACE_HOLDER>,test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; test nodes [ __num__ ] = data nodes [ __num__ ] ; cluster . sort by distance ( data nodes [ __num__ ] @$ test nodes @$ test nodes . length ) ; assert true ( test nodes [ __num__ ] == data nodes [ __num__ ] ) ; assert true ( test nodes [ __num__ ] == data nodes [ __num__ ] ) ;,array contains node
if the user configured their <PLACE_HOLDER> we use it even if persistence is disabled since we do n't know anything about their implementation .,if ( job scheduler store == null && ! has start exception ( ) ) { if ( ! is persistent ( ) ) { this . job scheduler store = new in memory job scheduler store ( ) ; configure service ( job scheduler store ) ; return this . job scheduler store ; } try { persistence adapter pa = get persistence adapter ( ) ; if ( pa != null ) { this . job scheduler store = pa . create job scheduler store ( ) ; job scheduler store . set directory ( get scheduler directory file ( ) ) ; configure service ( job scheduler store ) ; return this . job scheduler store ; } } catch ( io exception e ),user configured store
setup object <PLACE_HOLDER> .,message receiver s ; if ( sponge . get game ( ) . is server available ( ) ) { s = sponge . get server ( ) . get console ( ) ; } else { s = new client message reciever ( ) ; },setup object receiver
check proj rel only <PLACE_HOLDER> one expression check this project only <PLACE_HOLDER> one expression @$ i.e . scalar subqueries .,list < rex node > proj exprs = project . get projects ( ) ; if ( proj exprs . size ( ) != __num__ ) { return ; },expression check one
we do not resolve a vanilla name starting with a lower case letter try to resolve against a default import @$ because we know that the default packages do not contain <PLACE_HOLDER> like these,test default imports &= ! ( type instanceof lower case class ) ; if ( test default imports ) { for ( int i = __num__ @$ size = default_imports . length ; i < size ; i ++ ) { string package prefix = default_imports [ i ] ; string name = type . get name ( ) ; constructed class with package tmp = new constructed class with package ( package prefix @$ name ) ; if ( resolve ( tmp @$ false @$ false @$ false ) ) { type . set redirect ( tmp . redirect ( ) ) ; return true ; } } string name = type . get name ( ) ; if ( name . equals ( __str__ ) ),packages contain classes
put the band into the correct location on the <PLACE_HOLDER> . once the band is moved we translate the device transform so that the band will move down the <PLACE_HOLDER> on the next iteration of the loop .,band graphics . set transform ( uniform transform ) ; band graphics . transform ( device transform ) ; device transform . translate ( __num__ @$ - band height ) ;,band move screen
this test will read <PLACE_HOLDER> 0 and 1 from the parent before calling split @$ so we expect the primary read to start at offset 2 .,when ( fake storage client . read rows ( read rows request . new builder ( ) . set read position ( stream position . new builder ( ) . set stream ( stream . new builder ( ) . set name ( __str__ ) ) . set offset ( __num__ ) ) . build ( ) ) ) . then return ( new fake big query server stream < > ( parent responses . sub list ( __num__ @$ __num__ ) ) ) ;,primary read rows
set to un<PLACE_HOLDER>ed if global memstore size already exceeds lower <PLACE_HOLDER>,if ( flush pressure >= __num__ ) { max throughput to set = double . max_value ; } else { max throughput to set = max throughput lower bound + ( max throughput upper bound - max throughput lower bound ) * flush pressure ; },size exceeds limit
while partition has been spilled @$ relocation bloom filter <PLACE_HOLDER> for current bucket @$ and build bloom filter with hashcode .,if ( status == bucket_status_in_filter ) { this . bloom filter . set bits location ( bucket @$ bucket in segment pos + bucket_header_length ) ; this . bloom filter . add hash ( hash code ) ; },relocation bloom its
svn repo has <PLACE_HOLDER> . if there is a cheap test @$ then this code can be refined @$ boosting performance .,return true ;,repo has history
the tables should have the same <PLACE_HOLDER> and be dividable by 3,for ( int i = __num__ ; i < vertex table . length ; i += __num__ ) { vector3f vert = new vector3f ( vertex table [ i ] @$ vertex table [ i + __num__ ] @$ vertex table [ i + __num__ ] ) ; vector3f norm = vert to normal map . get ( vert ) ; if ( norm == null ) { norm = new vector3f ( normal table [ i ] @$ normal table [ i + __num__ ] @$ normal table [ i + __num__ ] ) ; vert to normal map . put ( vert @$ norm ) ; } else { norm . add local ( normal table [ i ] @$ normal table [ i + __num__,tables have size
constant get constructor <PLACE_HOLDER> 2 @$,match get member ( clazz @$ method @$ code attribute @$ offset @$ instruction @$ null @$ get constructor matcher2 @$ false @$ false @$ class constants . method_name_init @$ null ) ;,constant get matcher
populate the new build rule graph <PLACE_HOLDER> with all of the usable rules from the last build rule graph <PLACE_HOLDER> for incremental action graph generation .,action graph cache . populate action graph builder with cached rules ( event bus @$ target graph @$ graph builder ) ;,new build builder
the object that we 're modifying here is a copy of the original ! so let 's change the <PLACE_HOLDER> from relative to absolute by grabbing the file object ... in case the name of the file comes from previous steps @$ forget about this !,try { list < string > new filenames = new array list < string > ( ) ; if ( ! is in fields ( ) ) { file input list file list = get files ( space ) ; if ( file list . get files ( ) . size ( ) > __num__ ) { for ( file object file object : file list . get files ( ) ) { if ( file object . exists ( ) ) { new filenames . add ( file object . get name ( ) . get path ( ) ) ; } } set file name ( new filenames . to array ( new string [ new filenames . size ( ) ] ) ) ; set,let change filename
fire the presence changed <PLACE_HOLDER>,while ( presences . has next ( ) ) { listener . presence changed ( presences . next ( ) ) ; },presence changed event
if the <PLACE_HOLDER> sequence number is greater than the next sequence number @$ that indicates a command is missing . register the command <PLACE_HOLDER> and return a future to be completed once commands are properly sequenced . if the session 's current sequence number is too far beyond the last known sequence number @$ reject the command to force it to be resent by,if ( sequence number > session . next request sequence ( ) ) { if ( session . get commands ( ) . size ( ) < max_pending_commands ) { log . trace ( __str__ @$ sequence number @$ session . next request sequence ( ) ) ; session . register command ( request . sequence number ( ) @$ new pending command ( request @$ future ) ) ; return future ; } else { return completable future . completed future ( log response ( command response . builder ( ) . with status ( raft response . status . error ) . with error ( raft error . type . command_failure ) . with last sequence ( session . get request sequence ( ) ) .,future register request
copy resource into a byte array . this is necessary because several browsers consider class.get resource a security risk since it can be used to load additional classes . class.get resource as stream just returns raw <PLACE_HOLDER> @$ which we can convert to a sound .,byte [ ] buffer = access controller . do privileged ( new privileged action < byte [ ] > ( ) { public byte [ ] run ( ) { try { input stream resource = basic look and feel . this . get class ( ) . get resource as stream ( sound file ) ; if ( resource == null ) { return null ; } buffered input stream in = new buffered input stream ( resource ) ; byte array output stream out = new byte array output stream ( __num__ ) ; byte [ ] buffer = new byte [ __num__ ] ; int n ; while ( ( n = in . read ( buffer ) ) > __num__ ) { out .,stream returns string
only fill in <PLACE_HOLDER> if the argument has a <PLACE_HOLDER> .,code . begin control flow ( __str__ @$ i ) ;,argument has line
record the failure persistently @$ and upload to uma when the library successfully loads next <PLACE_HOLDER> .,precacheuma . record ( precacheuma . event . precache_task_load_library_fail ) ; break ; default : break ;,library loads starts
patch throwing pc into return address so that deoptimization finds the right debug <PLACE_HOLDER>,patch return address ( exception pc ) ; word handler pc = exception handler for pc ( exception_handler_for_pc @$ thread ) ; if ( logging ) { printf ( __str__ @$ word . object to tracked pointer ( exception ) . raw value ( ) @$ exception pc . raw value ( ) @$ handler pc . raw value ( ) ) ; decipher ( handler pc . raw value ( ) ) ; printf ( __str__ ) ; },deoptimization finds message
wait until main thread gets <PLACE_HOLDER> of the semaphore,synchronized ( locka ) { while ( handshake . get waiter count ( ) == __num__ ) { utils . go sleep ( __num__ ) ; } handshake . semav ( ) ; try { blocked thread . wait until blocked ( ) ; system . out . println ( __str__ ) ; utils . check thread state ( itself @$ thread . state . runnable ) ; check stack ( itself @$ examiner stack @$ es depth ) ; system . out . println ( __str__ + __str__ ) ; utils . check thread state ( blocked thread @$ thread . state . blocked ) ; check stack ( blocked thread @$ blocked stack @$ bs depth ) ; } catch ( exception e ) { e,thread gets control
the index population detects <PLACE_HOLDER> on the fly @$ however for updates coming in we 're in a position where we can not detect <PLACE_HOLDER> while applying @$ but instead afterwards .,if ( descriptor . is unique ( ) && can check conflicts without store access ( ) ) { updater = new deferred conflict checking index updater ( updater @$ this :: new reader @$ descriptor ) ; },population detects conflicts
report if multiple devices are matching the <PLACE_HOLDER> .,if ( ! quiet && devices . size ( ) > __num__ ) { print message ( __str__ + devices . size ( ) + __str__ ) ; },devices matching pattern
if the expression does n't contain any <PLACE_HOLDER> @$ we 'll look up the <PLACE_HOLDER> in the imports . if not found there @$ it 's a local <PLACE_HOLDER> .,if ( expression . contains ( dot ) ) { int last dot index = expression . last index of ( __str__ ) ; messages package = expression . substring ( __num__ @$ last dot index ) ; } else { string package name = imported classes . get ( expression ) ; if ( package name == null ) { messages package = class package ; } else { messages package = package name ; } },expression contain package
closing the writable log channel @$ then the underlying channel is <PLACE_HOLDER> physical log file does,channel . close ( ) ; store channel . close ( ) ;,file does what
we overrode this method here to clear the picked state of edges and vertices if we ever get a released event when the user is clicking <PLACE_HOLDER> that is not an edge or vertex,if ( ! is dragging ( ) && vertex == null && edge == null ) { maybe clear picked state ( e ) ; } super . mouse released ( e ) ;,user clicking something
test if gdi can handle the <PLACE_HOLDER>,boolean direct togdi = ( ( transform type != affine transform . type_general_transform ) && ( ( transform type & affine transform . type_flip ) == __num__ ) ) ; if ( ! direct togdi ) { return __num__ ; },gdi handle transform
verify that we can read in all the transactions that we have written . if there were any corruptions @$ it is likely that the reading in of these transactions will throw an <PLACE_HOLDER> .,for ( storage directory sd : fsimage . get storage ( ) . dir iterable ( name node dir type . edits ) ) { file edit file = new file ( sd . get current dir ( ) @$ log file name ) ; system . out . println ( __str__ + edit file ) ; fs edit log loader loader = new fs edit log loader ( namesystem @$ start tx id ) ; long num edits this log = loader . loadfs edits ( new edit log file input stream ( edit file ) @$ start tx id ) ; system . out . println ( __str__ + num edits this log ) ; assert true ( num edits == - __num__ || num edits,the throw exception
we only need to create a customized image @$ if we 're running on linux @$ as docker on mac os and windows does n't map <PLACE_HOLDER> from the host into the container anyway .,if ( os . get current ( ) != os . linux ) { return base image ; } return image map . compute if absent ( base image @$ ( image ) -> { reporter . handle ( event . info ( __str__ + image + __str__ ) ) ; string work dir = path fragment . create ( __str__ ) . get relative ( exec root . get base name ( ) ) . get path string ( ) ; string builder dockerfile = new string builder ( ) ; dockerfile . append ( string . format ( __str__ @$ image ) ) ; dockerfile . append ( string . format ( __str__ @$ work dir ) ) ; if ( gid > __num__ ) {,docker map images
lazy check that we get 5 rows back @$ put off checking <PLACE_HOLDER>,assert equals ( __num__ @$ results [ __num__ ] . get row count ( ) ) ;,check put things
make sure this call happens before check declared application <PLACE_HOLDER> as servlet ! ! !,boolean has boot = has boot classes ( webdata ) ; resteasy deployment data . set boot classes ( has boot ) ; class < ? > declared application class = check declared application class as servlet ( webdata @$ class loader ) ;,check declared class
the call to build may also throw unsupported version <PLACE_HOLDER> @$ if there are essential fields that can not be represented in the chosen version .,do send ( client request @$ is internal request @$ now @$ builder . build ( version ) ) ;,call throw exception
unlike message consumers @$ we try current span before trying extraction . this is the proper order because the span in scope should take <PLACE_HOLDER> over a potentially stale header entry . note : brave instrumentation used properly does not result in stale header entries @$ as we always clear message headers after reading .,span span ; if ( maybe parent == null ) { trace context or sampling flags extracted = kafka tracing . extract and clear headers ( extractor @$ request @$ record . headers ( ) ) ; span = kafka tracing . next messaging span ( sampler @$ request @$ extracted ) ; } else { span = tracer . new child ( maybe parent ) ; },span take precedence
table has these split <PLACE_HOLDER>,byte [ ] [ ] table split keys = new byte [ ] [ ] { bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) } ;,table has keys
before sending the request @$ check if the request requires a global <PLACE_HOLDER> and what we have is a local <PLACE_HOLDER> . if so do an upgrade .,if ( check for upgrade ) { request upgrade request = null ; try { upgrade request = zks . check upgrade session ( request ) ; } catch ( keeper exception ke ) { if ( request . get hdr ( ) != null ) { request . get hdr ( ) . set type ( op code . error ) ; request . set txn ( new error txn ( ke . code ( ) . int value ( ) ) ) ; } request . set exception ( ke ) ; log . warn ( __str__ @$ ke ) ; } catch ( io exception ie ) { log . error ( __str__ @$ ie ) ; } if ( upgrade request != null ),request requires deadlock
test send <PLACE_HOLDER> .,account send account = public methed . query account ( send account key @$ blocking stub full ) ; long send account before balance = send account . get balance ( ) ; assert . assert true ( send account before balance == __num__ ) ; account receipt account = public methed . query account ( receipt account key @$ blocking stub full ) ; long receipt account before balance = receipt account . get balance ( ) ; assert . assert true ( receipt account before balance == __num__ ) ;,test send account
copy the existing queue into the new one . first remove the oldest <PLACE_HOLDER> if no new ones exist .,if ( m event records . remaining capacity ( ) == __num__ ) { event record record = m event records . poll ( ) ; if ( record != null ) { m call event record map . remove ( record . get record entry ( ) ) ; } },one remove record
close the column family handles <PLACE_HOLDER> @$ then the db last,for ( column family handle cfh : column family handles . values ( ) ) { safe close ( cfh @$ exception reference ) ; } logger . info ( __str__ ) ; safe close ( rocksdb @$ exception reference ) ; rocksdb = null ; closed = true ; if ( exception reference . get ( ) != null ) { throw new io exception ( exception reference . get ( ) ) ; } db write lock . unlock ( ) ;,family handles etc
open d trace <PLACE_HOLDER>,cons . open ( ) ;,d trace consumer
the implementation of fragments do n't need to be beans <PLACE_HOLDER>,spring data repository creator repository creator = new spring data repository creator ( class output @$ composite index @$ ( n ) -> { additional beans . produce ( additional bean build item . unremovable of ( n ) ) ; } ) ;,implementation need referenced
load <PLACE_HOLDER> removes loaded <PLACE_HOLDER> from the input set @$ so the remaining ones were not on the node,final int iterator properties with no value = additional properties to load . int iterator ( ) ; while ( properties with no value . has next ( ) ) { put ( properties with no value . next ( ) @$ no_value ) ; },properties removes properties
all existing types should have a dedicated <PLACE_HOLDER>,for ( scenario simulation model . type value : scenario simulation model . type . values ( ) ) { final scenario runner provider retrieved = abstract scenario runner . get specific runner provider ( value ) ; assert not null ( retrieved ) ; },types have loader
specifically only include relevant bits according to the env var <PLACE_HOLDER> @$ rather than strictly overlaying config so that if the config file has a <PLACE_HOLDER> for master endpoints @$ the file does n't override the env var if it 's a domain : as master endpoints takes precedence over domains .,final uri uri = new uri ( master ) ; config config from env var = config factory . empty ( ) ;,file has list
handle possible indirection indirect flow should be to a data pointer which references <PLACE_HOLDER> .,if ( ref type == ref type . indirection ) { instruction dest instr = listing . get instruction containing ( ref . get to address ( ) ) ; int cnt = __num__ ; if ( dest instr == null && follow indirect flows ) { if ( instr == null || ! instr . get min address ( ) . equals ( from addr ) ) { instr = listing . get instruction at ( from addr ) ; } cnt = follow indirection ( block ref queue @$ include externals @$ block @$ ref @$ instr . get flow type ( ) . is call ( ) ? ref type . computed_call : ref type . computed_jump @$ monitor ) ; } if ( cnt,which references something
empty error messages indicate <PLACE_HOLDER>,if ( original host id == cluster save file state . error_code ) { long host id = savefile_data [ __num__ ] . get long ( __str__ ) ; string host name = savefile_data [ __num__ ] . get string ( __str__ ) ; string warning msg = savefile_data [ __num__ ] . get string ( __str__ ) ; warnings . add ( __str__ + host id + __str__ + host name + __str__ + warning msg ) ; },messages indicate failure
second observer will receive only last <PLACE_HOLDER>,second observer . assert value ( __num__ ) ;,observer receive impact
let <PLACE_HOLDER> 1 proceed and cancel <PLACE_HOLDER> 2,task2 handle queue . add ( handle ) ;,task proceed task
the test data is constructed such that the merge join zig zag has an early <PLACE_HOLDER> @$ leaving elements on the dynamic path input unconsumed,data set < path > edges = env . from elements ( new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) @$ new path ( __num__ @$ __num__ ) ) ; iterative data set < path > current paths = edges . iterate ( __num__ ) ; data set < path > new paths = current paths . join ( edges @$ join hint . repartition_sort_merge,zag has out
update the per file <PLACE_HOLDER>,if ( file check sum generator != null ) { file check sum generator . update ( buffer @$ __num__ @$ read ) ; },per file checksum
functions within i fs cause syntax <PLACE_HOLDER> on safari .,assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ; assert print ( __str__ @$ __str__ ) ;,functions fs errors
if comment does n't have a post <PLACE_HOLDER> @$ set it to the passed one and save to comment table,if ( m comment != null && m comment . get post title ( ) == null ) { m comment . set post title ( post title ) ; m dispatcher . dispatch ( comment action builder . new update comment action ( m comment ) ) ; },comment have title
add a listener to know when android has done another measurement pass . the listener automatically removes <PLACE_HOLDER> to prevent triggering the animation multiple times .,m layout . add on layout change listener ( new on layout change listener ( ) { @ override public void on layout change ( view v @$ int left @$ int top @$ int right @$ int bottom @$ int old left @$ int old top @$ int old right @$ int old bottom ) { m layout . remove on layout change listener ( this ) ; start animator ( callback ) ; } } ) ;,listener removes itself
copy & find decimal <PLACE_HOLDER>,int decimal point = end ; for ( int i = start @$ j = offset ; i < end ; i ++ @$ j ++ ) { buf [ i ] = value [ j ] ; if ( buf [ i ] == __str__ && i < decimal point ) { decimal point = i ; } },& find point
test chrome driver is not honoring headless <PLACE_HOLDER> ; using chrome driver instead,super . driver = new chrome driver ( options ) ; driver = ( chrome driver ) super . driver ; driver . get ( pages . clicks page ) ; assert that ( check permission ( driver @$ clipboard_read ) ) . is equal to ( __str__ ) ; assert that ( check permission ( driver @$ clipboard_write ) ) . is equal to ( __str__ ) ; driver . set permission ( clipboard_read @$ __str__ ) ; driver . set permission ( clipboard_write @$ __str__ ) ; assert that ( check permission ( driver @$ clipboard_read ) ) . is equal to ( __str__ ) ; assert that ( check permission ( driver @$ clipboard_write ) ) . is equal to ( __str__ ) ;,driver honoring permissions
produce a message to broker 1 's test queue and verify that broker 1 's memory <PLACE_HOLDER> has increased @$ but broker 2 still has no memory <PLACE_HOLDER> .,send messages ( __str__ @$ test queue @$ __num__ ) ; assert true ( broker1 test queue . get memory usage ( ) . get usage ( ) > __num__ ) ; assert equals ( __num__ @$ broker2 test queue . get memory usage ( ) . get usage ( ) ) ;,broker has usage
do one last placeholder <PLACE_HOLDER> @$ this is useful as we do n't stop the build when a library failed a placeholder <PLACE_HOLDER> @$ but the element might have been overridden so the problem was transient . however @$ with the final document ready @$ all placeholders values must have been provided .,if ( ! m optional features . contains ( invoker . feature . no_placeholder_replacement ) ) { perform place holder substitution ( loaded main manifest info @$ xml document optional . get ( ) @$ merging report builder ) ; if ( merging report builder . has errors ( ) ) { return merging report builder . build ( ) ; } },library failed replacement
the partition must have a current <PLACE_HOLDER>,string consumer = current partition consumer . get ( partition ) ; if ( consumer == null ) log . error ( __str__ @$ partition ) ; if ( prev assignment . contains key ( partition ) && current assignment . get ( consumer ) . size ( ) > current assignment . get ( prev assignment . get ( partition ) . consumer ) . size ( ) + __num__ ) { reassign partition ( partition @$ current assignment @$ sorted current subscriptions @$ current partition consumer @$ prev assignment . get ( partition ) . consumer ) ; reassignment performed = true ; modified = true ; continue ; },partition have consumer
only some of the node memories require special <PLACE_HOLDER> handling so we iterate over all of them and process only those that require it,for ( base node base node : context . sinks . values ( ) ) { memory memory = memories . peek node memory ( base node ) ; if ( memory != null ) { protobuf messages . node memory _node = null ; switch ( memory . get node type ( ) ) { case node type enums . query element node : { _node = write query element node memory ( base node . get id ( ) @$ memory @$ wm ) ; break ; } } if ( _node != null ) { _ksb . add node memory ( _node ) ; } } },some require handling
the actual tested <PLACE_HOLDER>,secu filter . process authentication ( filter context @$ client builder @$ method security @$ tracing . atn tracing ( ) ) ; assert that ( filter context . is should finish ( ) @$ is ( true ) ) ; assert that ( secu context . user ( ) @$ is ( optional . empty ( ) ) ) ;,actual tested method
note : batch update the <PLACE_HOLDER> of dataset if the event traffic spike,dataset config manager . update last refresh time ( dataset @$ event . get high watermark ( ) ) ; thirdeye metrics util . processed trigger event counter . inc ( ) ; log . debug ( __str__ + event . get dataset name ( ) ) ;,batch update timestamp
direct call to the <PLACE_HOLDER> service shall not throw a <PLACE_HOLDER> not found exception for an internal <PLACE_HOLDER>,if ( relation serv call flg ) { try { relation serv . send role update notification ( my rel id @$ new role @$ old role value ) ; } catch ( relation not found exception exc ) { throw new runtime exception ( exc . get message ( ) ) ; } } else { object [ ] params = new object [ __num__ ] ; params [ __num__ ] = my rel id ; params [ __num__ ] = new role ; params [ __num__ ] = old role value ; string [ ] signature = new string [ __num__ ] ; signature [ __num__ ] = __str__ ; signature [ __num__ ] = __str__ ; signature [ __num__ ] = __str__ ; try {,call throw relation
updated golden value since we have a different serial version <PLACE_HOLDER> in open jdk .,string s = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; tree map < string @$ string > map = new tree map < string @$ string > ( string . case_insensitive_order ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; map . put ( __str__ @$ __str__ ) ; sorted map < string @$ string > sub map = map . sub map ( __str__ @$ __str__ ) ; new serialization tester < sorted map < string @$ string > > ( sub map @$ s ) { @,value have uid
check number of transactions @$ should only have <PLACE_HOLDER>,log entry reader log entry reader = new version aware log entry reader ( new test command reader factory ( ) ) ; assert that ( log files . get lowest log version ( ) @$ is ( log files . get highest log version ( ) ) ) ; long version = log files . get highest log version ( ) ; try ( log versioned store channel channel = log files . open for version ( version ) ; read ahead log channel read ahead log channel = new read ahead log channel ( channel ) ; log entry cursor cursor = new log entry cursor ( log entry reader @$ read ahead log channel ) ) { log entry entry ; long number of transactions,number have 5
create the allow option <PLACE_HOLDER> .,final j panel allow option panel = new j panel ( layout ) ; allow option panel . set border ( border factory . create titled border ( etched border @$ msg ( __str__ ) ) ) ; allow option panel . add ( tip ( allow shrinking check box @$ __str__ ) @$ constraints last stretch ) ; allow option panel . add ( tip ( allow optimization check box @$ __str__ ) @$ constraints last stretch ) ; allow option panel . add ( tip ( allow obfuscation check box @$ __str__ ) @$ constraints last stretch ) ;,the allow panel
the front end will know <PLACE_HOLDER> one is the active one ; just use the first one for the test,final workspace active workspace = workspaces [ __num__ ] ; swing utilities . invoke and wait ( new runnable ( ) { @ override public void run ( ) { running tool = active workspace . run tool ( tool config ) ; } } ) ; assert not null ( running tool ) ; swing utilities . invoke and wait ( new runnable ( ) { @ override public void run ( ) { running tool . close ( ) ; } } ) ;,one one which
adapter views contain their <PLACE_HOLDER> in a frame so we need to go one layer deeper here .,if ( parent instanceof adapter view animator ) { vg = ( view group ) vg . get child at ( __num__ ) ; } if ( vg == null ) return ; remote response response = null ; int child count = vg . get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { object tag = vg . get child at ( i ) . get tag ( com . android . internal . r . id . fill in intent ) ; if ( tag instanceof remote response ) { response = ( remote response ) tag ; break ; } } if ( response == null ) return ; response . handle,views contain children
use batch scan <PLACE_HOLDER> when performing seeded operation,input configurator . set batch scan ( accumulo input format . class @$ conf @$ true ) ; add iterators ( accumulo store @$ conf @$ context . get user ( ) @$ operation ) ; add ranges ( accumulo store @$ conf @$ operation ) ; final java pairrdd < element @$ null writable > pairrdd = spark context . newapi hadooprdd ( conf @$ element input format . class @$ element . class @$ null writable . class ) ; final javardd < element > rdd = pairrdd . map ( new first element ( ) ) ; return rdd ;,batch scan option
transfer send some asset issue to default account @$ to test if this transaction use the transaction free <PLACE_HOLDER> .,assert . assert true ( public methed . transfer asset ( to address @$ asset account id . to byte array ( ) @$ __num__ @$ transfer asset address @$ transfer asset create key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ; asset creator net = public methed . get account net ( asset014 address @$ blocking stub full ) ; asset transfer net = public methed . get account net ( transfer asset address @$ blocking stub full ) ; long creator after net used = asset creator net . get net used ( ) ; long transfer after free net used = asset transfer net . get free net used ( ) ; logger,transaction use net
e.g . binary operator extends bi function @$ binary operator contains no abstract <PLACE_HOLDER> @$ but it is really a sam,if ( type . is interface ( ) ) { methods = type . redirect ( ) . get all declared methods ( ) ; } else { methods = type . get methods ( ) ; },operator contains method
dw suggested buffer size specifies the suggested buffer size for reading the file . generally @$ this size should be large enough to contain the largest chunk in the file . if set to zero @$ or if it is too small @$ the playback software will have to reallocate memory during playback @$ which will reduce <PLACE_HOLDER> . for an interleaved file @$,video track vt = null ; int width = __num__ @$ height = __num__ ;,which reduce footprint
csv can not differentiate null and empty <PLACE_HOLDER> .,if ( value == null ) { current line . add ( __str__ ) ; } else if ( value instanceof string ) { current line . add ( ( string ) value ) ; } else { current line . add ( value . to string ( ) ) ; },csv differentiate string
configuration for publishing jars containing sources for generated <PLACE_HOLDER> to the project artifacts for including in the ivy.xml,if ( _generate sources jar task == null ) { configuration container configurations = project . get configurations ( ) ; configuration generated sources = configurations . maybe create ( __str__ ) ; configuration test generated sources = configurations . maybe create ( __str__ ) ; test generated sources . extends from ( generated sources ) ; _generate sources jar task = project . get tasks ( ) . create ( __str__ @$ jar . class @$ jar task -> { jar task . set group ( java base plugin . documentation_group ) ; jar task . set description ( __str__ ) ; jar task . set classifier ( __str__ ) ; } ) ; project . get artifacts ( ) . add ( __str__ @$ _generate sources,configuration generated classes
if the op <PLACE_HOLDER> is null then the implementation does n't offer a presence operation <PLACE_HOLDER> which is unacceptable for gibberish .,if ( operation set presence1 == null ) { throw new null pointer exception ( __str__ + __str__ + __str__ ) ; },implementation offer set
all notes with this template must have at least two <PLACE_HOLDER> @$ or we could end up creating orphaned notes,sql = __str__ + utils . ids2str ( cids ) + __str__ ; if ( m col . get db ( ) . query scalar ( sql ) != __num__ ) { return false ; },notes have ids
keys which have similar <PLACE_HOLDER> as the desired key,list < string > possible matches = new array list < > ( ) ;,which have pattern
set up things @$ the order does <PLACE_HOLDER> as if it is messed up then the set up should fail because of the precondition checks in the respective set up methods for creating a customized task queue see test realtime index task failure test,task storage = set up task storage ( ) ; handoff notifier factory = set up segment hand off notifier factory ( ) ; data segment pusher = set up data segment pusher ( ) ; mdc = set up metadata storage coordinator ( ) ; tb = set up task toolbox factory ( data segment pusher @$ handoff notifier factory @$ mdc ) ; task runner = set up thread pool task runner ( tb ) ; task queue = set up task queue ( task storage @$ task runner ) ;,order does work
this is expected @$ void type options have no <PLACE_HOLDER> .,if ( option definition . get type ( ) . equals ( void . class ) ) { } else if ( next args . has next ( ) ) { unconverted value = next args . next ( ) ; command line form . append ( __str__ ) . append ( unconverted value ) ; } else { throw new options parsing exception ( __str__ + arg ) ; },options have effect
socket timeout exceptions are interrupted io exceptions ; however they do not signify an external interruption @$ but simply a failed download due to some server timing out . so rethrow <PLACE_HOLDER> as ordinary io exceptions .,throw new io exception ( e ) ;,download rethrow them
do not start any application as super dev mode will refresh the <PLACE_HOLDER> once done compiling,if ( super dev mode . enable based on parameter ( ) ) { return ; },mode refresh cache
source and destination images may have different <PLACE_HOLDER> requirements @$ therefore may have different strides . copy row by row for such case .,int src offset = src buffer . position ( ) ; int dst offset = dst buffer . position ( ) ; size effective plane size = get effective plane size for image ( src @$ i ) ; int src byte count = effective plane size . get width ( ) * src planes [ i ] . get pixel stride ( ) ; for ( int row = __num__ ; row < effective plane size . get height ( ) ; row ++ ) { if ( row == effective plane size . get height ( ) - __num__ ) { int remaining bytes = src buffer . remaining ( ) - src offset ; if ( src byte count > remaining bytes ) { src,images have strides
will be null in the get <PLACE_HOLDER> or when we run out opf attempts..,if ( notification time != null ) { try { janitor queue . record future notification ( notification time @$ key @$ user token @$ account record id @$ tenant record id ) ; } catch ( final io exception e ) { log . warn ( __str__ @$ payment transaction id @$ e . get message ( ) ) ; } },the get method
first lets check that a fully opaque foreground has sufficient <PLACE_HOLDER>,double test ratio = contrast calculator . calculate contrast ( foreground @$ background @$ __num__ ) ; if ( test ratio < min contrast ratio ) { return - __num__ ; },foreground has contrast
we hide the <PLACE_HOLDER> label until we know if the chat supports <PLACE_HOLDER> .,sms button . set visible ( false ) ; center panel . add ( sms button @$ constraints ) ;,chat supports buttons
we then use the looked up <PLACE_HOLDER> to assert that it behaves as expected @$ just like in any other test .,map < com . oracle . truffle . api . source . source @$ coverage > coverage map = coverage instrument . get coverage map ( ) ; assert . assert equals ( __num__ @$ coverage map . size ( ) ) ; coverage map . for each ( ( com . oracle . truffle . api . source . source s @$ coverage v ) -> { set < integer > not yet covered line numbers = coverage instrument . non covered line numbers ( s ) ; object [ ] expected = new integer [ ] { __num__ @$ __num__ @$ __num__ } ; assert . assert array equals ( expected @$ not yet covered line numbers . stream ( ) . sorted ( ) .,the looked service
we know an exists only ever has one <PLACE_HOLDER> @$ and the previous algorithm has confirmed the <PLACE_HOLDER> is an or,final group element or = ( group element ) parent . get children ( ) . get ( __num__ ) ; parent . set type ( group element . not ) ; parent . get children ( ) . clear ( ) ; final group element and = group element factory . new and instance ( ) ; for ( rule condition element rule condition element : or . get children ( ) ) { final group element new not = group element factory . new not instance ( ) ; new not . add child ( rule condition element ) ; and . add child ( new not ) ; } parent . add child ( and ) ; parent . pack ( ) ;,exists has child
enable long press to <PLACE_HOLDER> items enable <PLACE_HOLDER> using handle view enable swipe items,m adapter . set long press drag enabled ( true ) . set handle drag enabled ( true ) . set swipe enabled ( true ) ;,view enable drag
the expression must have no <PLACE_HOLDER>,return check parameter count ( expr @$ keywords . none ) ;,expression have children
then the last task should be cleared @$ and the system should quit lock task <PLACE_HOLDER>,verify ( tr1 ) . perform clear task locked ( ) ; assert false ( m lock task controller . is task locked ( tr1 ) ) ; assert equals ( lock_task_mode_none @$ m lock task controller . get lock task mode state ( ) ) ; verify lock task stopped ( times ( __num__ ) ) ;,system quit mode
arc to treats the sweep angle mod 360 @$ so check for that @$ since we think 360 means draw the entire <PLACE_HOLDER>,if ( sweep < __num__ && sweep > - __num__ ) { ring path . set fill type ( path . fill type . even_odd ) ; ring path . move to ( x + radius @$ y ) ; ring path . line to ( x + radius + thickness @$ y ) ; ring path . arc to ( bounds @$ __num__ @$ sweep @$ false ) ; ring path . arc to ( inner bounds @$ sweep @$ - sweep @$ false ) ; ring path . close ( ) ; } else { ring path . add oval ( bounds @$ path . direction . cw ) ; ring path . add oval ( inner bounds @$ path . direction . ccw ) ;,means draw thing
the wait is necessary to have the poll function complete and propagate the <PLACE_HOLDER> from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one three = database onecode node . get comments ( ) . get global code node comment ( ) ; final list < i comment > two three = database twocode node . get comments ( ) . get global code node comment ( ) ; assert equals ( one two size @$ one three . size ( ) ) ; assert equals ( two two size @$ two three . size ( ) ) ; assert equals ( one three @$ two three ) ; assert equals ( __str__ @$ iterables . get last ( one three ) . get comment ( ),function complete changes
user 6 has other global <PLACE_HOLDER>,user dto user6 = db . users ( ) . insert user ( with email ( __str__ ) ) ; db . users ( ) . insert permission on user ( organization1 @$ user6 @$ administer_quality_profiles ) ;,user has permission
create the base list of classes which have possible <PLACE_HOLDER> to be overloaded,this . class list = new linked hash set < class > ( ) ; this . class list . add ( super class ) ; if ( generate delegate field ) { class list . add ( delegate class ) ; collections . add all ( this . class list @$ delegate class . get interfaces ( ) ) ; } if ( interfaces != null ) { collections . add all ( this . class list @$ interfaces ) ; } this . proxy name = proxy name ( ) ; this . empty body = empty body ;,which have methods
grantor acknowledged <PLACE_HOLDER> of lock ...,if ( is debug enabled_dls ) { if ( my reply . reply code == d lock release reply message . ok ) { logger . trace ( log marker . dls_verbose @$ __str__ @$ this . object name @$ my reply . service name ) ; } else if ( my reply . reply code == d lock release reply message . not_grantor ) { logger . trace ( log marker . dls_verbose @$ __str__ @$ my reply . get sender ( ) @$ my reply . service name ) ; } },grantor acknowledged response
indent:0 exp:12 warn indent:8 <PLACE_HOLDER> indent:8 <PLACE_HOLDER> indent:8 <PLACE_HOLDER>,system . get property ( __str__ ) ;,exp:12 warn exp
the <PLACE_HOLDER> can have two peers at the same time and there is no one is conference focus . this is situation when someone has made an attended transfer and has transfered us . we have one <PLACE_HOLDER> with two peers the one we are talking to and the one we have been transfered to . and the first one is been hanged up,return call peers . size ( ) > __num__ ;,call have call
spawn a thread to put on server @$ which will acquire a <PLACE_HOLDER> on entry,set client server observer for before interest recovery ( ) ; server1 . invoke ( ( ) -> start server ( ) ) ; verify dead and live servers ( __num__ @$ __num__ ) ; wait for before interest recovery call back ( ) ;,which acquire lock
always log the <PLACE_HOLDER> here @$ we might not see the <PLACE_HOLDER> in the caller,log . error ( __str__ @$ db err ) ;,error see error
any object literal definition prevents <PLACE_HOLDER> .,used ( lines ( __str__ @$ __str__ ) ) ;,definition prevents warning
they are in different functions @$ find out which encloses the <PLACE_HOLDER>,if ( has larger parent ( outer2 @$ section length ) ) { outer = outer2 ; } else { },which encloses other
if user is scrolling headers support <PLACE_HOLDER> @$ swap to empty <PLACE_HOLDER> and wait scrolling finishes .,if ( is showing headers ( ) && grid view != null && grid view . get scroll state ( ) != recycler view . scroll_state_idle ) { get child fragment manager ( ) . begin transaction ( ) . replace ( r . id . scale_frame @$ new fragment ( ) ) . commit ( ) ; grid view . remove on scroll listener ( m wait scroll finish and commit main fragment ) ; grid view . add on scroll listener ( m wait scroll finish and commit main fragment ) ; } else { commit main fragment ( ) ; },user scrolling fragment
client advertises these <PLACE_HOLDER> .,return protocols ;,client advertises protocols
the input method manager only throws security <PLACE_HOLDER> @$ so let 's log all others .,if ( ! ( e instanceof security exception ) ) { slog . wtf ( tag @$ __str__ @$ e ) ; } throw e ;,manager throws exceptions
axes draw vertical <PLACE_HOLDER>,int x = left margin - __num__ ; int y = top margin ; font metrics fm = g . get font metrics ( ) ; g . draw line ( x @$ y @$ x @$ y + h ) ; int n = __num__ ; if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ) { n = __num__ ; } else if ( ( __str__ + v max ) . starts with ( __str__ ) ),axes draw line
alias can be used in js types . types have node type <PLACE_HOLDER> and not name so we have to use their name as <PLACE_HOLDER> .,string node name = n . is string ( ) || n . is import star ( ) ? n . get string ( ) : preprocessor symbol table . get qualified name ( n ) ;,types have string
the <PLACE_HOLDER> was not sent using async send @$ so we should only ack the local broker when we get confirmation that the remote broker has received the <PLACE_HOLDER> .,response callback callback = new response callback ( ) { public void on completion ( future response future ) { try { response response = future . get result ( ) ; if ( response . is exception ( ) ) { exception response er = ( exception response ) response ; service local exception ( er . get exception ( ) ) ; } else { dequeue counter . increment and get ( ) ; local broker . oneway ( new message ack ( md @$ message ack . standard_ack_type @$ __num__ ) ) ; } } catch ( io exception e ) { service local exception ( e ) ; } } } ; remote broker . async request ( message @$ callback ) ;,broker received message
note param.always on top will be null if the user has n't specified a <PLACE_HOLDER> yet,get check box always on top ( ) . set selected ( param . get always on top ( ) != boolean . false ) ; get check box in scope only ( ) . set selected ( param . is in scope only ( ) ) ; get button mode ( ) . set selected index ( param . get button mode ( ) - __num__ ) ;,user specified state
if we have a callback to call @$ schedule it on a different thread . who knows <PLACE_HOLDER> this thread is doing .,if ( callback != null ) { queue buffer . executor . submit ( new callable < void > ( ) { public void call ( ) throws exception { callback . on success ( result ) ; return null ; } } ) ; },thread doing what
notify the target capture <PLACE_HOLDER> .,notify ( event @$ true ) ; if ( event . is stopped ( ) ) return event . is cancelled ( ) ;,target capture event
no @$ we do n't have a code <PLACE_HOLDER>,if ( code set == __num__ ) { switch ( new code set ) { case code_code_a : pattern index = code_start_a ; break ; case code_code_b : pattern index = code_start_b ; break ; default : pattern index = code_start_c ; break ; } } else { pattern index = new code set ; },no have set
the work spec had <PLACE_HOLDER> . once the execution of the worker is complete @$ we might need to disable constraint proxies which were previously enabled for this work spec . hence @$ trigger a <PLACE_HOLDER> changed command .,if ( m has constraints ) { intent intent = command handler . create constraints changed intent ( m context ) ; m dispatcher . post on main thread ( new system alarm dispatcher . add runnable ( m dispatcher @$ intent @$ m start id ) ) ; },spec had constraints
did some instances lose their <PLACE_HOLDER> ?,if ( last known instance configs . size ( ) != current relevant instance configs . size ( ) ) { logger . info ( __str__ @$ table name @$ last known instance configs . size ( ) @$ current relevant instance configs . size ( ) ) ; return true ; },instances lose state
if there is no container acl but the user can still access the container @$ the only possibility is that the user has the admin role . in this case @$ the user should have 0700 <PLACE_HOLDER> to the swift container .,if ( mode == __num__ && m access . get token ( ) != null ) { mode = ( short ) __num__ ; } m account mode = mode ;,user have access
this ensures that incremental compilation only touches the <PLACE_HOLDER> that 's been swapped out .,typed scope scope = t . get typed scope ( ) ; if ( ! scope . is block scope ( ) && ! scope . is module scope ( ) ) { infer scope ( t . get current node ( ) @$ scope ) ; },compilation touches scope
now set the <PLACE_HOLDER> in the row !,for ( int i = __num__ ; i < row meta . size ( ) ; i ++ ) { value meta interface v = row meta . get value meta ( i ) ; object object = data [ i ] ; try { set value ( ps @$ v @$ object @$ i + __num__ ) ; } catch ( kettle database exception e ) { throw new kettle database exception ( __str__ + row meta @$ e ) ; } },now set values
skip if any aggregation contains a <PLACE_HOLDER> by,if ( node . has orderings ( ) ) { return context . default rewrite ( node @$ optional . empty ( ) ) ; },aggregation contains order
connectivity service should have changed the <PLACE_HOLDER> on lan supported to true,wifi lp . set wake on lan supported ( true ) ; assert equals ( wifi lp @$ m service . get active link properties ( ) ) ;,service changed wakelock
response get gateway <PLACE_HOLDER>,if ( cluster member != null ) { cluster . gateway receiver gateway receiver = cluster member . get gateway receiver ( ) ; boolean is gateway = false ; if ( gateway receiver != null ) { responsejson . put ( __str__ @$ true ) ; responsejson . put ( __str__ @$ gateway receiver . get listening port ( ) ) ; responsejson . put ( __str__ @$ gateway receiver . get link throughput ( ) ) ; responsejson . put ( __str__ @$ gateway receiver . get avg batch processing time ( ) ) ; } else { responsejson . put ( __str__ @$ false ) ; } cluster . gateway sender [ ] gateway senders = cluster member . get member gateway senders ( ),response get receiver
the week crosses a year <PLACE_HOLDER> .,time temp = new time ( this ) ; temp . month day += s thursday offset [ week day ] ; temp . normalize ( true ) ;,week crosses day
process schema included <PLACE_HOLDER> first @$ so that unnamed classes will belong to the defining class instead of the current class,final list < named data schema > includes = schema . get include ( ) ; for ( named data schema included schema : includes ) { process schema ( included schema @$ null @$ null ) ; } final map < custom info spec @$ object > custom info map = new identity hash map < custom info spec @$ object > ( schema . get fields ( ) . size ( ) * __num__ ) ; for ( record data schema . field field : schema . get fields ( ) ) { final class template spec field class = process schema ( field . get type ( ) @$ record class @$ field . get name ( ) ) ; final record template spec .,schema included roots
note that test xml ca n't check if they are same because enabling dynamic <PLACE_HOLDER>s causes a meta <PLACE_HOLDER> to be produced .,test xml ( cache @$ false ) ; assert equals ( true @$ dynamic region factory . get ( ) . is open ( ) ) ; assert equals ( f . get absolute file ( ) @$ dynamic region factory . get ( ) . get config ( ) . get disk dir ( ) ) ; region dr = get cache ( ) . get region ( __str__ ) ; if ( dr != null ) { dr . local destroy region ( ) ; },note causes region
` promise & & whatever ` never returns promise <PLACE_HOLDER> @$ so it is safe . ` whatever & & promise ` may return promise @$ so return outer context .,return first ? link ( parent @$ true ) : this ; case or :,returns promise object
check can not post <PLACE_HOLDER> that already exist .,response = client . post ( namespace path3 @$ null headers @$ new byte [ ] { } ) ; assert equals ( __num__ @$ response . get code ( ) ) ; response = client . post ( namespace path4 @$ constants . mimetype_protobuf @$ model4 . create protobuf output ( ) ) ; assert equals ( __num__ @$ response . get code ( ) ) ;,check post tables
existing write <PLACE_HOLDER> against the caller 's val<PLACE_HOLDER> list .,if ( tbl == null ) { return null ; } m database mdb = null ; string cat name = tbl . is set cat name ( ) ? tbl . get cat name ( ) : get default catalog ( conf ) ; try { mdb = getm database ( cat name @$ tbl . get db name ( ) ) ; } catch ( no such object exception e ) { log . error ( __str__ @$ e ) ; throw new invalid object exception ( __str__ + database name . get qualified ( cat name @$ tbl . get db name ( ) ) + __str__ ) ; },existing write table
transformation must keep the <PLACE_HOLDER>,assert equals ( a @$ and1 . get children ( ) . get ( __num__ ) ) ; assert equals ( c @$ and1 . get children ( ) . get ( __num__ ) ) ; final group element and2 = ( group element ) parent . get children ( ) . get ( __num__ ) ; assert equals ( b @$ and2 . get children ( ) . get ( __num__ ) ) ; assert equals ( c @$ and2 . get children ( ) . get ( __num__ ) ) ;,transformation keep group
do n't send an error response here @$ unlike the base authentication filter implementation . this request did not use <PLACE_HOLDER> auth . instead @$ we will send an error response in pre response authorization check filter to allow other authenticator implementations to check the request .,if ( authentication ex == null ) { filter chain . do filter ( request @$ response ) ; } else { http response . send error ( err code @$ authentication ex . get message ( ) ) ; },request use access
change <PLACE_HOLDER>s and check that the selected date is not lost and that the calendar has the correct <PLACE_HOLDER> .,click ( resolution hour ) ; check header and body ( date time resolution . hour @$ false ) ; click ( resolution year ) ; check header and body ( date time resolution . year @$ false ) ; click ( resolution minute ) ; check header and body ( date time resolution . minute @$ false ) ;,calendar has resolution
print outs <PLACE_HOLDER> that are set using the entry processor above .,print cache entries ( cache ) ;,print outs entries
verify exit status matches exit <PLACE_HOLDER> of script,assert . assert equals ( exit code @$ container status . get exit status ( ) ) ;,status matches code
no apps can bypass <PLACE_HOLDER>,assert equals ( __num__ @$ m helper . get apps bypassing dnd count ( user ) ) ;,apps bypass dnd
app has a visible <PLACE_HOLDER> ; only upgrade adjustment .,if ( adj > process list . visible_app_adj ) { adj = process list . visible_app_adj ; app . adj type = __str__ ; if ( debug_oom_adj_reason || log uid == app uid ) { report oom adj message locked ( tag_oom_adj @$ __str__ + app ) ; } } if ( proc state > process state cur top ) { proc state = process state cur top ; app . adj type = __str__ ; if ( debug_oom_adj_reason || log uid == app uid ) { report oom adj message locked ( tag_oom_adj @$ __str__ + app ) ; } } if ( sched group < process list . sched_group_default ) { sched group = process list . sched_group_default ; } app . cached = false ;,app has id
this method might modify the file on disk . use segment <PLACE_HOLDER> to prevent race condition,lock segment lock = segment locks . get segment lock ( table name with type @$ segment name ) ; try { segment lock . lock ( ) ; final file segment dir = new file ( _fetcher and loader . get segment local directory ( table name with type @$ segment name ) ) ; if ( segment dir . exists ( ) ) { file utils . delete quietly ( segment dir ) ; _logger . info ( __str__ @$ segment dir ) ; } } catch ( final exception e ) { _logger . error ( __str__ + segment name + __str__ + e . get message ( ) @$ e ) ; utils . rethrow exception ( e ) ; } finally { segment,method modify lock
verify that the archive rule has the correct <PLACE_HOLDER> : the object files from our sources .,rule . get native linkable input ( cxx platform @$ linker . linkable dep type . static @$ graph builder @$ unconfigured target configuration . instance ) ; build rule static rule = graph builder . get rule ( cxx description enhancer . create static library build target ( target @$ cxx platform . get flavor ( ) @$ pic type . pdc ) ) ; assert not null ( static rule ) ; assert equals ( immutable set . of ( cxx source rule factorypdc . create compile build target ( __str__ ) @$ cxx source rule factorypdc . create compile build target ( gen source name ) ) @$ static rule . get build deps ( ) . stream ( ) . map ( build rule,rule has deps
node 1 still has node 2 in readers <PLACE_HOLDER> .,assert true ( e1 . readers ( ) . contains ( n2 . id ( ) ) ) ; assert not null ( cache1 . get and put ( __num__ @$ __str__ ) ) ; final grid dht cache entry e1f = e1 ; grid test utils . wait for condition ( new grid abs predicate ( ) { @ override public boolean apply ( ) { try { return ! e1f . readers ( ) . contains ( n2 . id ( ) ) ; } catch ( grid cache entry removed exception ignored ) { return true ; } catch ( exception e ) { throw new runtime exception ( e ) ; } } } @$ __num__ ) ;,node has map
update the key.current symlink . first create tmp symlink and do <PLACE_HOLDER> of tmp to current so that the operation is atomic .,path tmp symlink = tmp symlink location ( ) ; path target of symlink = construct blob with version file name ( base dir @$ get key ( ) @$ version ) ; log . debug ( __str__ @$ tmp symlink @$ target of symlink ) ; files . create symbolic link ( tmp symlink @$ target of symlink ) ; path current sym link = get current symlink path ( ) ; files . move ( tmp symlink @$ current sym link @$ atomic_move ) ;,tmp symlink move
swap in our <PLACE_HOLDER> if the filtered fs is using a different <PLACE_HOLDER>,if ( swap scheme != null ) { try { fq path = new path ( new uri ( swap scheme @$ fq path . to uri ( ) . get scheme specific part ( ) @$ null ) ) ; } catch ( uri syntax exception e ) { throw new illegal argument exception ( e ) ; } },fs using scheme
while still holding the lock cancel the <PLACE_HOLDER> by transitioning . this simulates a race where the callback goes to cancel the <PLACE_HOLDER> while the <PLACE_HOLDER> is trying to run .,ees . become active ( ) ;,lock cancel receiver
otherwise @$ adjust enabled <PLACE_HOLDER> .,cancel caption animator ( ) ; if ( enabled ) { helper text view = new app compat text view ( context ) ; helper text view . set id ( r . id . textinput_helper_text ) ; if ( version . sdk_int >= __num__ ) { helper text view . set text alignment ( view . text_alignment_view_start ) ; } if ( typeface != null ) { helper text view . set typeface ( typeface ) ; } helper text view . set visibility ( view . invisible ) ; view compat . set accessibility live region ( helper text view @$ view compat . accessibility_live_region_polite ) ; set helper text appearance ( helper text text appearance ) ; set helper text view text color ( helper,adjust enabled state
it was <PLACE_HOLDER> @$ try again . the assumption is that it must be a word start if the last one had <PLACE_HOLDER> following it .,word position = words . next ( ) ; if ( word position != break iterator . done ) { offs = line start + word position - seg . offset ; if ( offs != line end ) { return offs ; } } segment cache . release shared segment ( seg ) ; return break iterator . done ;,one had whitespace
check that method under test throws <PLACE_HOLDER>,try { epki . get key spec ( ( key ) null @$ __str__ ) ; fail ( get name ( ) + __str__ ) ; } catch ( null pointer exception ok ) { },method throws npe
the id lists must have the same <PLACE_HOLDER>,this . operatori ds . add ( operatorid . from job vertexid ( this . id ) ) ; this . operator ids alternatives . add ( null ) ;,lists have constraints
both use the same client <PLACE_HOLDER> @$ so there 's no lock conflict . not necessarily <PLACE_HOLDER>eal @$ but how the system currently works .,utils . io result ( future reader1 ) ; utils . io result ( future reader2 ) ; dlm0 . close ( ) ; dlm1 . close ( ) ;,both use latch
the user 1 queue should inherit the <PLACE_HOLDER> from the root queue,fs leaf queue user queue = scheduler . get queue manager ( ) . get leaf queue ( __str__ @$ true ) ; assert equals ( __num__ @$ user queue . get num runnable apps ( ) ) ; assert equals ( __num__ @$ user queue . get min share preemption timeout ( ) ) ; assert equals ( __num__ @$ user queue . get fair share preemption timeout ( ) ) ; assert equals ( __num__ @$ user queue . get fair share preemption threshold ( ) @$ __num__ ) ;,queue inherit precedence
make sure the thread saw the correct <PLACE_HOLDER>,assert true ( thread success . get ( ) ) ;,thread saw callback
the 2 i indexes have to be stored in the appositely created strong consistent <PLACE_HOLDER> bucket type : this however has to be done only if the user actually created it ! so @$ if the latter does n't exist @$ then the scan transactions will not be performed at all .,if ( strong consistency && ! strong consistent scans bucket type . is empty ( ) ) { bucket type2i = strong consistent scans bucket type ; perform strong consistent scans = true ; } else { bucket type2i = bucket type ; perform strong consistent scans = false ; },user created scanations
set character offset <PLACE_HOLDER> for this section,curr section core map . set ( core annotations . character offset begin annotation . class @$ section start token . get ( core annotations . character offset begin annotation . class ) ) ;,character offset info
time of creation we use a do <PLACE_HOLDER> with the access control context that was in place when this was created .,if ( acc == null && system . get security manager ( ) != null ) { throw new security exception ( __str__ ) ; } return access controller . do privileged ( new privileged action < object > ( ) { public object run ( ) { try { class < ? > c ; object cl ; if ( table == null || ! ( ( cl = table . get ( __str__ ) ) instanceof class loader ) ) { cl = thread . current thread ( ) . get context class loader ( ) ; if ( cl == null ) { cl = class loader . get system class loader ( ) ; } } reflect util . check package access ( class,a do privilege
skeleton throws unmarshal <PLACE_HOLDER> if it does not recognize the operation number ; this is consistent with the case of an unrecognized method hash .,p . pln ( __str__ + id unmarshal exception + __str__ ) ; p . p oln ( __str__ ) ;,skeleton throws exception
clearing focus forces the <PLACE_HOLDER> to commit any pending changes @$ e.g . typed text in a number picker .,m time picker . clear focus ( ) ; dismiss ( ) ;,focus forces dialog
full recurse into class hierarchy @$ should engage all available <PLACE_HOLDER> @$ so all internal maps and lists are transformed too,return immutable list . of ( pojoizer . convert to pojo ( rclass . prop1 ) ) ;,recurse engage objects
if we are currently performing a binary search on the input @$ do n't forward the results currently this <PLACE_HOLDER> is set when a query is optimized using a compact index . the map reduce job responsible for scanning and filtering the index sets this <PLACE_HOLDER> . it remains set throughout the binary search executed by the hive binary search record resder until a,if ( io context . is binary searching ( ) ) { return ; } boolean ret = ( boolean ) condition inspector . get primitive java object ( condition ) ; if ( boolean . true . equals ( ret ) ) { forward ( row @$ row inspector ) ; },job sets flag
if <PLACE_HOLDER> does not exists defined for this region then create a <PLACE_HOLDER>,if ( null == member ) { member = new cluster . member ( ) ; member . set name ( member name ) ; cluster . get membersh map ( ) . put ( member name @$ member ) ; },region create member
client <PLACE_HOLDER> required exception is caught by o auth 2 <PLACE_HOLDER> request redirect web filter which initiates <PLACE_HOLDER>,if ( authorization grant type . authorization_code . equals ( context . get client registration ( ) . get authorization grant type ( ) ) && context . get authorized client ( ) == null ) { return mono . error ( ( ) -> new client authorization required exception ( context . get client registration ( ) . get registration id ( ) ) ) ; },which initiates authorization
check whether the broadcast inputs use the same plan <PLACE_HOLDER> at the branching point,for ( int i = __num__ ; i < broadcast channels combination . size ( ) ; i ++ ) { named channel nc = broadcast channels combination . get ( i ) ; plan node bc source = nc . get source ( ) ; if ( ! are branch compatible ( bc source @$ input source ) ) { valid combination = false ; break ; } for ( int k = __num__ ; k < i ; k ++ ) { plan node other bc source = broadcast channels combination . get ( k ) . get source ( ) ; if ( ! are branch compatible ( bc source @$ other bc source ) ) { valid combination = false ; break ; },inputs use candidate
negative ty value means an upward <PLACE_HOLDER> so subtracting ty means expanding the panel .,set clamped panel height ( m initial panel height - ty ) ; request update ( ) ;,ty means movement
the use of 'flat map ' at notification sender does not preserve actual <PLACE_HOLDER> order . use typed qname map to assert regardless of ordering .,final map < string @$ referenceable > entities = message . get entities ( ) . stream ( ) . collect ( collectors . to map ( ref -> atlas utils . to typed qualified name ( ref . get type name ( ) @$ ( string ) ref . get ( attr_qualified_name ) ) @$ ref -> ref ) ) ; boolean has flow path seen = false ; for ( int i = __num__ ; i < expects . length ; i ++ ) { final referenceable expect = expects [ i ] ; final string type name = expect . get type name ( ) ; final referenceable actual = entities . get ( atlas utils . to typed qualified name ( type name @$,use preserve attribute
user trigger <PLACE_HOLDER> to tr<PLACE_HOLDER>nsfer token to b,byte string asset account dev = public methed . query account ( dev001 address @$ blocking stub full ) . get asset issuedid ( ) ; byte string fake token id = byte string . copy from utf8 ( long . to string ( long . value of ( asset account dev . to string utf8 ( ) ) + __num__ ) ) ; string param = __str__ + fake token id . to string utf8 ( ) + __str__ ; final string trigger txid = public methed . trigger contract ( transfer token contract address @$ __str__ @$ param @$ false @$ __num__ @$ __num__ @$ __str__ @$ __num__ @$ dev001 address @$ dev001 key @$ blocking stub full ) ; public methed . wait produce next,user trigger a
we load the url from the tab rather than directly from the content view so the tab has a <PLACE_HOLDER> of using a prerenderer page is any .,int load type = native load url ( m native tab android @$ params . get url ( ) @$ params . get verbatim headers ( ) @$ params . get post data ( ) @$ params . get transition type ( ) @$ params . get referrer ( ) != null ? params . get referrer ( ) . get url ( ) : null @$ params . get referrer ( ) != null ? params . get referrer ( ) . get policy ( ) : __num__ @$ params . get is renderer initiated ( ) @$ params . get should replace current entry ( ) @$ params . get intent received timestamp ( ) @$ params . get has user gesture ( ) ) ;,tab has way
synchronization provided by controller <PLACE_HOLDER> to make sure that only one thread updates the <PLACE_HOLDER>,_controller metrics . set value of table gauge ( table name with type @$ controller gauge . number_of_replicas @$ n replicas external ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . percent_of_replicas @$ ( n replicas ideal max > __num__ ) ? ( n replicas external * __num__ / n replicas ideal max ) : __num__ ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . segments_in_error_state @$ n errors ) ; _controller metrics . set value of table gauge ( table name with type @$ controller gauge . percent_segments_available @$ ( n segments > __num__ ) ? ( __num__ - ( n offline * __num__ / n segments,thread updates metrics
this is solely for testing . it checks if the test has set the looped <PLACE_HOLDER> to false @$ and if so remembers that and then sets it to true at the end . we have to check here first to make sure we go through a complete iteration of the loop before resetting it .,do { boolean set looped = ! looped . get ( ) ; txn store . mutexapi . lock handle handle = null ; long started at = - __num__ ; try { handle = txn handler . get mutexapi ( ) . acquire lock ( txn store . mutex_key . cleaner . name ( ) ) ; started at = system . current time millis ( ) ; long min open txn id = txn handler . find min open txn id ( ) ; for ( compaction info compaction info : txn handler . find ready to clean ( ) ) { clean ( compaction info @$ min open txn id ) ; } } catch ( throwable t ) { log . error ( __str__,test set flag
step 3 : lookup mounted file <PLACE_HOLDER>,byte [ ] dir = path . as byte array ( ) ; for ( unix mount entry entry : fs . get mount entries ( ) ) { if ( arrays . equals ( dir @$ entry . dir ( ) ) ) return entry ; } throw new io exception ( __str__ ) ;,mounted file systems
unrecognized attributes do not cause an <PLACE_HOLDER>,return ;,attributes cause exception
if the user did n't specify a ser <PLACE_HOLDER> @$ we use the <PLACE_HOLDER>fault .,string ser de class name ; if ( get ser name ( ) == null ) { if ( storage handler == null ) { ser de class name = plan utils . get default ser de ( ) . get name ( ) ; log . info ( __str__ + ser de class name + __str__ + table name ) ; } else { ser de class name = storage handler . get ser de class ( ) . get name ( ) ; log . info ( __str__ + ser de class name + __str__ + table name ) ; } } else { ser de class name = get ser name ( ) ; ddl utils . validate ser de ( ser de class name,user specify de
system arraycopy does the boundary checks anyways @$ no <PLACE_HOLDER> to check extra,system . arraycopy ( this . memory @$ index @$ dst @$ offset @$ length ) ;,checks anyways need
open jdk 8 does not support iv parameter <PLACE_HOLDER> for gcm .,system . out . println ( __str__ + ex . to string ( ) ) ; return ;,jdk support generator
these <PLACE_HOLDER> are usually api methods that are templated in such a way as the <PLACE_HOLDER> become too large @$ really to much so to even read . not sure of the best way to trim these <PLACE_HOLDER> without losing the type specificity provided by the template arguments . for now @$ just trim the name to some length that still leaves us with,string builder buffy = new string builder ( ) ; buffy . append ( name . substring ( __num__ @$ symbol utilities . max_symbol_name_length / __num__ ) ) ; buffy . append ( __str__ ) ; buffy . append ( name . substring ( length - __num__ ) ) ;,read trim names
evaluate batch 1 so that temporary arrays in the expression have residual <PLACE_HOLDER> to interfere in later computation,or expr . evaluate ( batch1 ) ;,arrays have values
the timestamp given by golden gate does not have nanoseconds <PLACE_HOLDER> needed by oracle timestamp,string corrected timestamp = time stamp . append ( __str__ ) . to string ( ) ;,timestamp have offsets
we check only permission because igfs client adds user<PLACE_HOLDER> and group <PLACE_HOLDER> explicitly .,assert equals ( props . get ( igfs utils . prop_permission ) @$ igfs . info ( dir ) . properties ( ) . get ( igfs utils . prop_permission ) ) ;,client adds name
set the values which have no <PLACE_HOLDER> and compare,set annotation member value ( impl1 @$ __str__ @$ annotation . boolean1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . byte1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . short1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . char1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . int1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . long1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . float1 ( ) ) ; set annotation member value ( impl1 @$ __str__ @$ annotation . double1 ( ) ) ;,which have annotations
the test is complete when the client sends the other object <PLACE_HOLDER> .,if ( object == server test object . get other object ( ) ) stop end points ( ) ;,client sends eof
path stack has <PLACE_HOLDER> of stack at front,if ( path stack . is empty ( ) ) throw new config exception . bug or broken ( __str__ ) ; else return new path ( path stack . descending iterator ( ) ) ;,stack has head
foo has the <PLACE_HOLDER> and length of ` super `,node super dotg replacement = prototype . get first child ( ) ; assert node ( super dotg replacement ) . matches qualified name ( __str__ ) . has equal source info to ( source super ) ;,foo has position
multiply by 2 because each has a <PLACE_HOLDER> @$ and map.entry,int expected size = ( map results . size ( ) + duplicates ) * __num__ ; assert equals ( __str__ + joiner . on ( __str__ ) . join ( other matches ) @$ expected size @$ other matches size ) ; assert true ( entry set match ) ; assert true ( entry set javax match ) ; assert true ( map provider match ) ; assert true ( map javax provider match ) ; assert true ( collection of providers of entry of provider match ) ; assert true ( collection of javax providers of entry of provider match ) ; assert equals ( allow duplicates @$ map set match ) ; assert equals ( allow duplicates @$ map set provider match ) ; assert,each has value
the above method will perform the <PLACE_HOLDER> as long as the user does not cancel the request,if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . open_policy_file ) == __num__ ) { tool dialog td = new tool dialog ( policy tool . get message ( __str__ ) @$ tool @$ tw @$ true ) ; td . display user save ( tool dialog . open ) ; } else if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . save_policy_file ) == __num__ ) { string filename = ( ( j text field ) tw . get component ( tool window . mw_filename_textfield ) ) . get text ( ) ; if ( filename == null || filename . length ( ) ==,method perform quit
create a native linkable dep and have it list the fake build <PLACE_HOLDER> above as a link time dependency .,native linkable input native linkable input = native linkable input . of ( immutable list . of ( source path arg . of ( fake build rule . get source path to output ( ) ) ) @$ immutable set . of ( ) @$ immutable set . of ( ) ) ; fake native linkable group native linkable = create native linkable ( __str__ @$ native linkable input @$ native linkable input ) ;,fake build rule
check that ot ns ordering is not breaking <PLACE_HOLDER>,ksession = serialization helper . get serialised stateful knowledge session ( ksession @$ true ) ; ksession . fire all rules ( ) ;,ns breaking rules
no requirement to pull an <PLACE_HOLDER> from the distributed cache anymore .,just elected primary node = false ;,requirement pull node
connection acquisition takes more than 0 <PLACE_HOLDER> in a real system,stub data source . set connection acquistion time ( connection acquisition time ms ) ; final executor service thread pool = new fixed thread pool ( thread count ) ; final count down latch all threads done = new count down latch ( iterations ) ; for ( int i = __num__ ; i < iterations ; i ++ ) { thread pool . submit ( ( ) -> { if ( ref . get ( ) == null ) { quietly sleep ( rest time ms ) ; try ( connection c2 = ds . get connection ( ) ) { quietly sleep ( work time ms ) ; } catch ( exception e ) { ref . set ( e ) ; } } all threads,acquisition takes ms
method does not throw an <PLACE_HOLDER> .,verify read xml returns expected signatures ( __str__ @$ package parser . signing details . signature scheme version . unknown ) ;,method throw exception
otherwise @$ the user specified specific test <PLACE_HOLDER> to build and run @$ so build a graph around these .,log . debug ( __str__ @$ get arguments ( ) ) ; target graph creation result = params . get parser ( ) . build target graph without top level configuration targets ( parsing context @$ parse arguments as target node specs ( params . get cell ( ) @$ params . get client working dir ( ) @$ get arguments ( ) @$ params . get buck config ( ) ) @$ params . get target configuration ( ) ) ; explicit build targets = target graph creation result . get build targets ( ) ; log . debug ( __str__ @$ explicit build targets ) ; immutable set . builder < build target > test targets builder = immutable set . builder ( ) ; for,user specified paths
this all could probably be done more elegantly via a group extracted from a more comprehensive regexp . clean up any extra <PLACE_HOLDER> around the remainder of the line @$ which should be a view name .,return statement . substring ( matcher . end ( ) ) . trim ( ) ;,elegantly clean spaces
both columns have same data <PLACE_HOLDER> @$ return any one of them,return expected data type ;,columns have type
user has specified a <PLACE_HOLDER> @$ we can continue ...,try { file object file object = kettlevfs . get file object ( vfs filename @$ this ) ; if ( ! ( file object instanceof local file ) ) { throw new kettle exception ( __str__ + vfs filename + __str__ ) ; } string real filename = kettlevfs . get filename ( file object ) ; file file = new file ( real filename ) ; if ( ( file . exists ( ) && file . can read ( ) ) || is local infile ( ) == false ) { if ( log . is detailed ( ) ) { log detailed ( __str__ + real filename + __str__ ) ; } if ( connection != null ) { database db = new database,user specified file
the first vertex has a duplicate <PLACE_HOLDER> from a vertex in the graph and should not be added to the new graph,vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; vertices . add ( new vertex < > ( __num__ @$ __num__ ) ) ; graph = graph . add vertices ( vertices ) ; data set < vertex < long @$ long > > data = graph . get vertices ( ) ; list < vertex < long @$ long > > result = data . collect ( ) ; expected result = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; compare result as tuples ( result @$ expected result ) ;,vertex has distribution
find out if the class which contains this field <PLACE_HOLDER> has access to the class which declares the public or protected field .,if ( source class == ctx class ) { class definition declarer = field . get class definition ( ) ; if ( declarer . is package private ( ) && ! declarer . get name ( ) . get qualifier ( ) . equals ( source class . get name ( ) . get qualifier ( ) ) ) { field = member definition . make proxy member ( field @$ clazz @$ env ) ; } },which contains declaration
if the path does <PLACE_HOLDER> @$ then pass on to the subclass implementation for specific checks :,if ( path matches ( path pattern @$ requesturi ) ) { if ( log . is trace enabled ( ) ) { log . trace ( __str__ + path pattern + __str__ + encode . for html ( requesturi ) + __str__ + __str__ ) ; } return filter chain manager . proxy ( original chain @$ path pattern ) ; },path does match
check if the node contains the <PLACE_HOLDER> passed as filter,region function context impl rfc = ( region function context impl ) context ; partitioned region pr = ( partitioned region ) rfc . get data set ( ) ; int [ ] bucketi ds = rfc . get local bucket array ( pr ) ; pr . get gem fire cache ( ) . get logger ( ) . fine ( __str__ + bucketi ds ) ; result sender < integer > rs = context . < integer > get result sender ( ) ; if ( ! pr . get data store ( ) . are all buckets hosted ( bucketi ds ) ) { throw new assertion error ( __str__ + bucketi ds + __str__ ) ; } else { for ( int i =,node contains bucket
base connector config has 13 <PLACE_HOLDER> @$ connector 's configs add 2 @$ and 2 producer overrides,assert equals ( __num__ @$ result . values ( ) . size ( ) ) ; assert true ( result . values ( ) . stream ( ) . any match ( config info -> ack config key . equals ( config info . config value ( ) . name ( ) ) && ! config info . config value ( ) . errors ( ) . is empty ( ) ) ) ; assert true ( result . values ( ) . stream ( ) . any match ( config info -> sasl config key . equals ( config info . config value ( ) . name ( ) ) && config info . config value ( ) . errors ( ) . is empty ( ),config has defaults
check that large slices do something <PLACE_HOLDER>,string tree = __str__ ; check tree ( tree ) ;,slices do ok
clear the current suggestions as the server response always includes the new ones . exception is when filtering @$ then we need to retain the value if the user does not select <PLACE_HOLDER> of the options matching the filter .,if ( ! widget . waiting for filtering response ) { widget . current suggestion = null ; widget . suggestion popup . menu . clear items ( ) ; popup open and cleared = widget . suggestion popup . is attached ( ) ; },user select any
the specified network connection is not available . displays error <PLACE_HOLDER> .,web view my web view = find view by id ( r . id . webview ) ; my web view . load data ( get resources ( ) . get string ( r . string . connection_error ) @$ __str__ @$ null ) ;,network displays text
free hash map <PLACE_HOLDER> @$ but not release back to <PLACE_HOLDER> manager,mutable object iterator < tuple2 < binary row @$ binary row > > iterator = sorter . getkv iterator ( ) ; tuple2 < binary row @$ binary row > kv ; while ( ( kv = iterator . next ( ) ) != null ) { binary row key = kv . f0 ; binary row value = kv . f1 ; fallback input . replace ( key @$ value ) ; if ( last key == null ) { last key = key . copy ( ) ; agg sum is null = true ; agg sum = - __num__ ; } else if ( key . get size in bytes ( ) != last key . get size in bytes ( ) || ! (,hash map cache
a match with both pos and word labeled should have both <PLACE_HOLDER> on the same node,string pattern = __str__ ;,match have values
dexopts have to match exactly since aspect only creates <PLACE_HOLDER> for listed ones,return normalize dexopts ( iterables . filter ( tokenized dexopts @$ predicates . in ( get android config ( rule context ) . get dexopts supported in incremental dexing ( ) ) ) ) ;,aspect creates directories
it will show all its tiles . in this case @$ the tiles have to be entered before the container is measured . any change in the tiles @$ should trigger a <PLACE_HOLDER> .,final int num tiles = m records . size ( ) ; final int width = measure spec . get size ( width measure spec ) ; final int available width = width - get padding start ( ) - get padding end ( ) ; final int height mode = measure spec . get mode ( height measure spec ) ; if ( height mode == measure spec . unspecified ) { m rows = ( num tiles + m columns - __num__ ) / m columns ; } m cell width = ( available width - m side padding * __num__ - ( m cell margin horizontal * m columns ) ) / m columns ;,case trigger gap
simple lv hangul simple lvt hangul lvtt @$ last jam<PLACE_HOLDER> expands f<PLACE_HOLDER>r search llvvvtt @$ every jam<PLACE_HOLDER> expands f<PLACE_HOLDER>r search 0 x ac 01 as c<PLACE_HOLDER>nj<PLACE_HOLDER>ining jam<PLACE_HOLDER> 0 x ac 01 as c<PLACE_HOLDER>mpatibility jam<PLACE_HOLDER> 0 x ac 0 f as c<PLACE_HOLDER>nj<PLACE_HOLDER>ining jam<PLACE_HOLDER> ; last expands f<PLACE_HOLDER>r search 0 x afff as c<PLACE_HOLDER>nj<PLACE_HOLDER>ining jam<PLACE_HOLDER> ; all expand f<PLACE_HOLDER>r search small letter ae @$ expands small,string tsce text = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ;,lvtt expands o
is true if either the resources has increased or execution <PLACE_HOLDER> updated from opportunistic to guaranteed,boolean is increase = false ; if ( ! current resource . equals ( target resource ) ) { is resource change = true ; is increase = resources . fits in ( current resource @$ target resource ) && ! resources . fits in ( target resource @$ current resource ) ; } else if ( ! current exec type . equals ( target exec type ) ) { is exec type update = true ; is increase = current exec type == execution type . opportunistic && target exec type == execution type . guaranteed ; } if ( is increase ) { org . apache . hadoop . yarn . api . records . container increased container = null ; if ( is resource change,resources has type
note that this might miss collisions @$ use internal tick <PLACE_HOLDER> to check for collision on every tick . see internal tick test on how to implement it .,contacts . clear ( ) ;,collision use test
do n't send the parsed db name @$ as this method will parse <PLACE_HOLDER> .,ret = get_partitions_ps_with_auth ( db_name @$ tbl_name @$ part_vals @$ max_parts @$ null @$ null ) ; ret = filter utils . filter partitions if enabled ( is server filter enabled @$ filter hook @$ ret ) ; end function ( __str__ @$ ret != null @$ ex @$ tbl_name ) ;,method parse one
otherwise @$ we return the generic placeholder of this library @$ that dependents can use get the real build <PLACE_HOLDER> via querying the action graph .,prebuilt cxx library paths paths = get paths ( build target @$ args ) ; return new prebuilt cxx library ( build target @$ project filesystem @$ params ) { private final transitive cxx preprocessor input cache transitive cxx preprocessor input cache = new transitive cxx preprocessor input cache ( this ) ; cxx deps all exported deps = cxx deps . builder ( ) . add deps ( args . get exported deps ( ) ) . add platform deps ( args . get exported platform deps ( ) ) . build ( ) ; private boolean has headers ( cxx platform cxx platform ) { if ( ! args . get exported headers ( ) . is empty ( ) ) { return true ; },real build rules
pure symbol reference : the data contains the symbol 's <PLACE_HOLDER> @$ load it,if ( data info . is symbol reference ( ) ) { register result register = as register ( result ) ; a arch64 address address = a arch64 address . create scaled immediate address ( result register @$ __num__ ) ; masm . adrp ldr ( __num__ @$ result register @$ address ) ; crb . compilation result . record data patch ( before @$ new c global data reference ( data info ) ) ; } else { a arch64 address address = masm . get placeholder ( before ) ; masm . load address ( as register ( result ) @$ address @$ __num__ ) ; crb . compilation result . record data patch ( before @$ new c global data reference ( data info,data contains address
previous call to read <PLACE_HOLDER> file has already set the <PLACE_HOLDER> to draw,if ( text to use != file_text ) { set text to draw ( text @$ range @$ user @$ null ) ; } set font params ( name @$ size @$ style @$ transform ) ; set transformg2 ( g2transform ) ;,call set text
a special native server to check basic <PLACE_HOLDER>,if ( mode != - __num__ ) { if ( ns ( - __num__ ) . wait for ( ) != __num__ ) { proc . d ( __str__ ) ; mode = - __num__ ; } },server check content
cache size is 3 @$ so 4 th access should remove first <PLACE_HOLDER>,assert equals ( __num__ @$ evicted from cache . size ( ) ) ;,access remove entry
set default enable <PLACE_HOLDER> to cluster only if it is true .,if ( dflt enable val ) baseline auto adjust enabled . propagate ( dflt enable val ) ;,default enable state
if we are in the middle of connecting process do not fire events @$ will do it later when the method connect and login <PLACE_HOLDER> its work,if ( err != null && err . get condition ( ) == stream error . condition . conflict ) { synchronized ( connect and login lock ) { if ( in connect and login ) { event during login = new registration state change event ( protocol provider service jabber impl . this @$ get registration state ( ) @$ registration state . unregistered @$ registration state change event . reason_multiple_logins @$ __str__ ) ; return ; } } fire registration state changed ( get registration state ( ) @$ registration state . unregistered @$ registration state change event . reason_multiple_logins @$ __str__ ) ; disconnect and clean connection ( ) ; return ; },method connect finishes
the handler created a <PLACE_HOLDER> of the <PLACE_HOLDER> and is now done with it .,dup2 . release ( ) ;,handler created slice
black color @$ show a <PLACE_HOLDER> off :,if ( color . equals ( color . black ) ) { set icon ( image utilities . load image icon ( __str__ @$ false ) ) ; } else { set icon ( new color icon ( ) ) ; },color show lightbulb
no averagers returned <PLACE_HOLDER> . all buckets must be empty . skip this row .,return null ;,averagers returned nulls
any exceptions thrown inside an event listener will not stop <PLACE_HOLDER> of the event,timber . w ( e @$ __str__ ) ;,exceptions stop deletion
legacy application listeners only understood one <PLACE_HOLDER> at a time,for ( final file file : files ) { final application event ae = new application event ( toolkit . get default toolkit ( ) @$ file . get absolute path ( ) ) ; send event to each listener until handled ( ae @$ new event dispatcher ( ) { public void dispatch event ( final application listener listener ) { listener . handle open file ( ae ) ; } } ) ; },listeners understood file
else : mtp deleted the <PLACE_HOLDER> as part of cleanup . do n't send an event .,break ; default : return false ;,mtp deleted node
class.for name and class.get constructor are supposed to never return null though a broken class loader could break the <PLACE_HOLDER> . just in case we introduce this specific catch to avoid polluting the logs with np es .,logger . log ( warning @$ string . format ( __str__ @$ pm class name ) ) ; logger . log ( warning @$ string . format ( __str__ @$ pm class name ) ) ; logger . log ( warning @$ string . format ( __str__ @$ pm class name ) @$ e ) ;,loader break test
second cancellation should n't trigger additional <PLACE_HOLDER>,server stream . cancel ( status ) ; do ping pong ( server listener ) ;,cancellation trigger signal
create a display which only contains 2 <PLACE_HOLDER> .,final activity display display = add new activity display at ( activity display . position_top ) ; final activity stack stack1 = create fullscreen stack with simple activity at ( display ) ; final activity stack stack2 = create fullscreen stack with simple activity at ( display ) ;,which contains activities
assume that this zone stopped <PLACE_HOLDER>,if ( tz . getn transitions ( ) > __num__ ) { tz . setdst type ( timezone . x_dst ) ; long time = time . get local time ( max year @$ month . january @$ __num__ @$ __num__ ) ; time -= zrec . get gmt offset ( ) ; tz . add transition ( time @$ tz . get offset index ( gmt offset ) @$ tz . get dst offset index ( __num__ ) ) ; used zone = true ; } else { tz . setdst type ( timezone . no_dst ) ; },zone stopped race
only apps that target older than o sdk can add window without a token @$ after that we require a token so apps can not add <PLACE_HOLDER> directly as the token is added by the notification system . window manager does the checking for this .,out app op [ __num__ ] = op_toast_window ; return add_okay ; case type_dream : case type_input_method : case type_wallpaper : case type_presentation : case type_private_presentation : case type_voice_interaction : case type_accessibility_overlay : case type_qs_dialog :,apps add toasts
add to all instant messaging operation sets the message listener which handles all received <PLACE_HOLDER> .,im . add message listener ( this ) ;,all received messages
have the nn redundancy monitor compute the reconstruction and invalidation commands to send d ns every <PLACE_HOLDER> .,conf . set int ( dfs config keys . dfs_namenode_redundancy_interval_seconds_key @$ __num__ ) ;,d ns second
if no identity was found then the user does not have a reserved room <PLACE_HOLDER>,return null ;,user have privilege
serialized version 0 progressive actions did not contain <PLACE_HOLDER> .,boolean is legacy progressive = version == __num__ && download request . type_progressive . equals ( type ) ; list < stream key > keys = new array list < > ( ) ; if ( ! is legacy progressive ) { int key count = input . read int ( ) ; for ( int i = __num__ ; i < key count ; i ++ ) { keys . add ( read key ( type @$ version @$ input ) ) ; } },version contain keys
update once at end of iteration to reduce heap write <PLACE_HOLDER>,cursor = i ; last ret = i - __num__ ; if ( mod count != expected mod count ) throw new concurrent modification exception ( ) ;,heap write traffic
add key as int here as to have the importer use the token <PLACE_HOLDER>,while ( store property cursor . next ( ) ) { visitor . property ( store property cursor . property key ( ) @$ store property cursor . property value ( ) . as object ( ) ) ; },importer use value
do n't include auto remove <PLACE_HOLDER> that are finished or finishing .,if ( tr . auto remove recents && tr . get top activity ( ) == null ) { if ( debug_recents ) { slog . d ( tag_recents @$ __str__ + tr ) ; } continue ; },auto remove recents
this loop may throw an io <PLACE_HOLDER>,while ( true ) { int bytes read = as . read ( loaded audio @$ loaded audio byte length @$ int len - loaded audio byte length ) ; if ( bytes read <= __num__ ) { as . close ( ) ; break ; } loaded audio byte length += bytes read ; },loop throw exception
randomly choose the app directory the chance of picking a directory is proportional to the available space on the directory . firstly calculate the <PLACE_HOLDER> of all available space on these directories,for ( string local dir : local dirs ) { path cur base = get application dir ( new path ( local dir ) @$ user @$ app id ) ; long space = __num__ ; try { space = get disk free space ( cur base ) ; } catch ( io exception e ) { log . warn ( __str__ @$ cur base @$ e ) ; } available on disk [ i ++ ] = space ; total available += space ; },chance calculate amount
for binary set the response op <PLACE_HOLDER>,if ( this . protocol == protocol . binary ) { reply . rewind ( ) ; reply . put ( position_opcode @$ buffer . get ( position_opcode ) ) ; reply . put int ( position_opaque @$ buffer . get int ( position_opaque ) ) ; if ( connection handler . get logger ( ) . finer enabled ( ) ) { connection handler . get logger ( ) . finer ( __str__ + reply + __str__ + command . bufferto string ( reply ) ) ; } } socket channel channel = this . socket . get channel ( ) ; if ( channel == null || ! channel . is open ( ) ) { throw new illegal state exception ( __str__ ) ; },binary set code
now reload the reloadable top class and introduce that field i which will shadow the <PLACE_HOLDER> in the supertype,r top . load new version ( __str__ @$ retrieve rename ( top @$ top + __str__ ) ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; run on instance ( c top @$ i top @$ __str__ @$ __num__ ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; assert equals ( __num__ @$ run on instance ( c top @$ i top @$ __str__ ) . return value ) ; is mgr fa = get field accessor ( i top ) ; assert contains ( __str__ @$ fa . to string ( ) ) ; assert does,which shadow one
in the absence of sorted by clause @$ the sorted dynamic partition insert should honor the <PLACE_HOLDER> of records provided by order by in select statement,reduce sink operator parentrs op = operator utils . find single operator upstream ( parent @$ reduce sink operator . class ) ; if ( parentrs op != null && parse ctx . get query properties ( ) . has outer order by ( ) ) { string parentrs op order = parentrs op . get conf ( ) . get order ( ) ; string parentrs op null order = parentrs op . get conf ( ) . get null order ( ) ; if ( parentrs op order != null && ! parentrs op order . is empty ( ) && sort positions . is empty ( ) ) { key cols . add all ( parentrs op . get conf ( ) . get key,insert honor order
b is maxed out on capacity @$ so this move should fail the <PLACE_HOLDER>,future = wm . apply move session async ( sessiona3 @$ __str__ ) ; assert not null ( future . get ( ) ) ; assert false ( future . get ( ) ) ; wm . add test event ( ) . get ( ) ; while ( sessiona3 . is open ( ) ) { thread . sleep ( __num__ ) ; } assert null ( sessiona3 . get pool name ( ) ) ; assert equals ( __str__ @$ sessiona3 . get reason for kill ( ) ) ; assert equals ( __num__ @$ all session providers . get ( __str__ ) . get sessions ( ) . size ( ) ) ; assert equals ( __num__ @$ all session providers . get ( __str__,move fail session
does this observer match the target <PLACE_HOLDER> ?,if ( target user handle == user handle . user_all || entry . user handle == user handle . user_all || target user handle == entry . user handle ) { if ( leaf ) { if ( ( flags & content resolver . notify_skip_notify_for_descendants ) != __num__ && entry . notify for descendants ) { if ( debug ) slog . d ( tag @$ __str__ + entry . observer + __str__ ) ; continue ; } } else { if ( ! entry . notify for descendants ) { if ( debug ) slog . d ( tag @$ __str__ + entry . observer + __str__ ) ; continue ; } } if ( debug ) slog . d ( tag @$ __str__ + entry .,observer match state
if neither are present then store empty <PLACE_HOLDER> in parameter slot,final constant pool gen cpg = class gen . get constant pool ( ) ; final instruction list il = method gen . get instruction list ( ) ; il . append ( new push ( cpg @$ constants . emptystring ) ) ;,neither store string
the second stream will see the first <PLACE_HOLDER>,verify ( picker @$ timeout ( __num__ ) ) . pick subchannel ( args2 ) ;,stream see picker
check if this string does not contain any uppercase <PLACE_HOLDER> .,if ( lowercased ) { return this ; } final byte [ ] new value = platform dependent . allocate uninitialized array ( length ( ) ) ; for ( i = __num__ @$ j = array offset ( ) ; i < new value . length ; ++ i @$ ++ j ) { new value [ i ] = to lower case ( value [ j ] ) ; } return new ascii string ( new value @$ false ) ;,string contain characters
if the decl<PLACE_HOLDER>ring cl<PLACE_HOLDER>ss of the field references missing cl<PLACE_HOLDER>sses <PLACE_HOLDER> ` no cl<PLACE_HOLDER>ss def found error ` c<PLACE_HOLDER>n be thrown . we intrinsify ` it here .,method intrinsic = get intrinsic ( analysis @$ hosted @$ b @$ exception synthesizer . throw no class def found error method ) ; if ( intrinsic == null ) { return false ; } throw no class def found error ( b @$ target method @$ e . get message ( ) ) ;,class references a
stack allocation needs an allocation <PLACE_HOLDER> that is a compile time constant @$ so we split the byte array up in multiple chunks and write them separately .,final int chunk size = __num__ ; final c char pointer bytes = stack value . get ( chunk size ) ; int chunk offset = offset ; int input length = length ; while ( input length > __num__ ) { int chunk length = math . min ( input length @$ chunk size ) ; for ( int i = __num__ ; i < chunk length ; i ++ ) { int index = chunk offset + i ; byte b ; if ( value instanceof string ) { b = ( byte ) char at ( ( string ) value @$ index ) ; } else if ( value instanceof char [ ] ) { b = ( byte ) ( ( char [ ],allocation needs index
skip if this consumer already has all the topic <PLACE_HOLDER> it can get,if ( consumer partition count == all subscriptions . get ( consumer ) . size ( ) ) continue ;,consumer has partitions
<PLACE_HOLDER> migration has updated the <PLACE_HOLDER> @$ send the request to the new <PLACE_HOLDER>,m_mailbox . send ( new leader @$ counter . get open message ( ) ) ;,migration updated leader
other topics should not throw <PLACE_HOLDER> @$ but they should clear existing <PLACE_HOLDER>,metadata . update ( metadata response @$ time . milliseconds ( ) ) ; metadata . maybe throw exception for topic ( __str__ ) ; metadata . maybe throw any exception ( ) ;,topics throw exceptions
update recognition locked can possibly update the <PLACE_HOLDER> of models,array list < model data > model datas = new array list < model data > ( m model data map . values ( ) ) ; for ( model data model data : model datas ) { update recognition locked ( model data @$ is allowed @$ notify ) ; },recognition update list
create a dummy select to select all <PLACE_HOLDER>,gen select plan ( pctx @$ map join op ) ; return map join op ;,dummy select columns
32 mb default keep <PLACE_HOLDER> in memory,kahadb persistence adapter . set index cache size ( __num__ ) ; kahadb persistence adapter . set index write batch size ( __num__ ) ; kahadb persistence adapter . set enable index recovery file ( false ) ; kahadb persistence adapter . set enable index disk syncs ( false ) ; broker . add connector ( __str__ ) ; broker . start ( ) ; string options = __str__ ; connection factory = new activemq connection factory ( broker . get transport connectors ( ) . get ( __num__ ) . get connect uri ( ) + options ) ;,default keep size
if the user mouses around the neutral zone @$ then start the close timer . the timer will be reset if the user enters the <PLACE_HOLDER> .,close timer . start ( ) ;,user enters button
hash aggregates and partial aggregates invalidate the <PLACE_HOLDER> ordering . so @$ we will need an orderby node .,if ( number hash aggregates > __num__ ) { return true ; } else if ( number window functions == __num__ ) { if ( index use . get window function uses index ( ) == window function scoreboard . no_index_use ) { return true ; } else { assert ( index use . get window function uses index ( ) == window function scoreboard . statement_level_order_by_index ) ; return number receive nodes > __num__ ; } } else if ( number window functions == __num__ ) { return ! ( index use . get window function uses index ( ) == __num__ && index use . is window function compatible with order by ( ) ) ; } else { return true ; },aggregates invalidate column
check if graphics card does n't support depth <PLACE_HOLDER>,if ( img . get format ( ) . is depth format ( ) && ! caps . contains ( caps . depth texture ) ) { throw new renderer exception ( __str__ ) ; } if ( target == gl . gl_texture_cube_map ) { int cube size = limits . get ( limits . cubemap size ) ; if ( img . get width ( ) > cube size || img . get height ( ) > cube size ) { throw new renderer exception ( __str__ + img + __str__ + cube size ) ; } if ( img . get width ( ) != img . get height ( ) ) { throw new renderer exception ( __str__ ) ; } } else { int,card support texture
pattern has <PLACE_HOLDER> and match,if ( pp < plen && ( s . char at ( ps ) == p . char at ( pp ) || p . char at ( pp ) == __str__ ) ) { ++ pp ; ++ ps ; } else if ( pp < plen && p . char at ( pp ) == __str__ ) { old ps = ps ; old pp = pp ; met star = true ; ++ pp ; } else { if ( met star ) { ++ old ps ; ps = old ps ; pp = old pp ; ++ pp ; } else { return false ; } },pattern has subqueries
no actions @$ hide the row @$ clear out the <PLACE_HOLDER>,if ( m actions == null ) { m action row . set visibility ( view . gone ) ; m current view . set slice actions ( null ) ; m current view . set insets ( get padding start ( ) @$ get padding top ( ) @$ get padding end ( ) @$ get padding bottom ( ) ) ; return ; },actions clear repaint
vector should contain rawname value <PLACE_HOLDER>,int i = __num__ ; while ( i < annotation local attrs . size ( ) ) { string rawname = ( string ) annotation local attrs . element at ( i ++ ) ; int colon index = rawname . index of ( __str__ ) ; string prefix @$ localpart ; if ( colon index == - __num__ ) { prefix = __str__ ; localpart = rawname ; } else { prefix = rawname . substring ( __num__ @$ colon index ) ; localpart = rawname . substring ( colon index + __num__ ) ; } string uri = schema doc . f namespace support . geturi ( f symbol table . add symbol ( prefix ) ) ; local str buffer . append ( rawname ),vector contain pairs
validate that every inherited interface must extend pipeline <PLACE_HOLDER> except for pipeline <PLACE_HOLDER> itself .,validate inherited interfaces extend pipeline options ( iface ) ; @ suppress warnings ( __str__ ) set < class < ? extends pipeline options > > combined pipeline options interfaces = fluent iterable . from ( validated pipeline options interfaces ) . append ( iface ) . to set ( ) ;,interface extend options
request did n't succeed because the token was revoked so we invalidate the token stored in the session and render the index page so that the user can start the o auth <PLACE_HOLDER> again,if ( res2 . failed ( ) ) { ctx . session ( ) . destroy ( ) ; ctx . fail ( res2 . cause ( ) ) ; } else { user info . put ( __str__ @$ res2 . result ( ) . json array ( ) ) ; json object data = new json object ( ) . put ( __str__ @$ user info ) ; engine . render ( data @$ __str__ @$ res3 -> { if ( res3 . succeeded ( ) ) { ctx . response ( ) . put header ( __str__ @$ __str__ ) . end ( res3 . result ( ) ) ; } else { ctx . fail ( res3 . cause ( ) ) ; },user start flow
run the timing <PLACE_HOLDER> first warm up the <PLACE_HOLDER> to make sure it gets compiled,if ( argv [ __num__ ] . equals ( __str__ ) ) { for ( int i = __num__ ; i < warmup_loops ; i ++ ) { timeit ( __num__ @$ __num__ @$ __num__ ) ; } system . out . println ( __str__ ) ; timeit ( timing_trials @$ get_timing_loops @$ set_timing_loops ) ; } else if ( argv [ __num__ ] . equals ( __str__ ) ) { max_milliseconds = stress_milliseconds ; } else { throw new runtime exception ( __str__ + argv [ __num__ ] + __str__ ) ; },method warm steps
test suite uses standalone wal <PLACE_HOLDER> to verify pds content .,grid test utils . add test if needed ( suite @$ ignite wal reader test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds exchange during checkpoint test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds reserve wal segments test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite pds reserve wal segments with compaction test . class @$ ignored tests ) ; grid test utils . add test if needed ( suite @$ ignite wal replaying after restart test . class @$ ignored tests ) ;,suite uses reader
check that active primary has a newer <PLACE_HOLDER> so that peer recovery works,if ( primary != null ) { return is version compatible allocating replica ( allocation . routing nodes ( ) @$ primary . current node id ( ) @$ node @$ allocation ) ; } else { return allocation . decision ( decision . yes @$ name @$ __str__ ) ; },primary has acl
create data with 2 chunks : the 2 nd chunk has <PLACE_HOLDER> of the block size,long first chunk length = block_size * blocks per chunk ; long second chunk length = block_size / __num__ ; dfs test util . create file ( fs @$ src data @$ buffer len @$ first chunk length @$ block_size @$ repl factor @$ src seed ) ; dfs test util . append file new block ( ( distributed file system ) fs @$ src data @$ ( int ) second chunk length ) ; dfs test util . create file ( fs @$ new path ( target base + filename + __str__ + first chunk length ) @$ buffer len @$ first chunk length @$ block_size @$ repl factor @$ dst seed ) ; dfs test util . create file ( fs @$ new path ( target,chunk has half
if a component specifies the file with a bad <PLACE_HOLDER> @$ the corresponding slot will be initialized by default physical <PLACE_HOLDER> . in such case find <PLACE_HOLDER> 2 d may return composite <PLACE_HOLDER> which can not be casted to physical <PLACE_HOLDER> .,if ( ! component names [ slot ] . equals ignore case ( name ) ) { try { components [ slot ] = ( physical font ) fm . find font2d ( component names [ slot ] @$ style @$ font manager . physical_fallback ) ; } catch ( class cast exception cce ) { components [ slot ] = fm . get default physical font ( ) ; } },slot find font
permission controller hosts default permission <PLACE_HOLDER> and role management @$ so it 's a critical part of the core system .,m required permission controller package = get required permission controllerl pr ( ) ;,controller hosts controller
verify that both consumers have a <PLACE_HOLDER> of 10,assert equals ( __num__ @$ brokerb . get destination ( advisory topic ) . get consumers ( ) . get ( __num__ ) . get prefetch size ( ) ) ; assert equals ( __num__ @$ brokerb . get destination ( topic1 ) . get consumers ( ) . get ( __num__ ) . get prefetch size ( ) ) ; assert deq inflight ( __num__ @$ __num__ ) ;,consumers have prefetch
important : first add factory methods ; then constructors @$ so latter can override <PLACE_HOLDER> !,_add factory creators ( ctxt @$ bean desc @$ vchecker @$ intr @$ creators @$ creator defs ) ;,latter override them
each thread gets the <PLACE_HOLDER> from the existing pool . if no <PLACE_HOLDER> is available @$ it requests one and waits for it to be created . but a thread that requests the <PLACE_HOLDER> and before it could wait @$ other thread could steal that <PLACE_HOLDER> . the <PLACE_HOLDER>s in progress tries to avoid these edge cases by not requesting a <PLACE_HOLDER> from the,if ( connections in progress . get ( ) == __num__ ) { resource = non blocking get ( key @$ pool ) ; } if ( resource == null ) { connections in progress . increment and get ( ) ; try { attempt grow ( key @$ this . object factory @$ pool ) ; resource = non blocking get ( key @$ pool ) ; } finally { connections in progress . decrement and get ( ) ; } } return resource ;,thread gets connection
let the fragment handle the back <PLACE_HOLDER> if it implements our on parent back pressed listener,fragment fragment = m bottom nav . get active fragment ( ) ; if ( fragment instanceof on activity back pressed listener ) { boolean handled = ( ( on activity back pressed listener ) fragment ) . on activity back pressed ( ) ; if ( handled ) { return ; } } if ( is task root ( ) && device utils . get instance ( ) . is chromebook ( this ) ) { return ; },fragment handle press
write <PLACE_HOLDER> to disk so that configuration.get password will find <PLACE_HOLDER>,provider . flush ( ) ;,password find something
this maybe not take effect @$ when not every consume queue has extend <PLACE_HOLDER> .,if ( is ext addr ( tags code ) ) { max ext addr = tags code ; },queue extend file
default display should show system <PLACE_HOLDER>,assert true ( m target . should show system decors locked ( m primary display ) ) ;,display show decor
this is the last split in the list @$ the filters define the <PLACE_HOLDER> from the previous split to the current split and also the current split to the end,if ( i == split keys . size ( ) - __num__ ) { range filter = string . format ( __str__ + __str__ @$ lowest bound @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; range filter = string . format ( __str__ @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; } else { range filter = string . format ( __str__ + __str__ @$ lowest bound @$ split key ) ; filters . add ( string . format ( __str__ @$ range filter ) ) ; },filters define range
if we did n't find an existing policy create a new <PLACE_HOLDER>,if ( found policy == null ) { final string uuid seed = resource + action ; final access policy . builder builder = new access policy . builder ( ) . identifier generate from seed ( uuid seed ) . resource ( resource ) . add user ( user identifier ) ; if ( action . equals ( read_code ) ) { builder . action ( request action . read ) ; } else if ( action . equals ( write_code ) ) { builder . action ( request action . write ) ; } else { throw new illegal state exception ( __str__ + action ) ; } final access policy access policy = builder . build ( ) ; final policy jaxb policy = createjaxb,policy create one
the current group has produced all its <PLACE_HOLDER>,if ( current group position == current group size ) { memory size in bytes -= current group size in bytes ; current group position = __num__ ; current rows = next grouped rows ( ) ; continue ; },group produced rows
now resource should have 0 <PLACE_HOLDER> .,assert . assert equals ( __num__ @$ lr . sem . available permits ( ) ) ;,resource have permits
region destroy will destroy dls and manual free <PLACE_HOLDER> only,if ( dlock service == null ) { dlock service = d lock service . create ( get full path ( ) @$ get system ( ) @$ true @$ false @$ false ) ; },destroy destroy stlocks
let 's also read back the result <PLACE_HOLDER> ...,node result rows node = xml handler . get sub node ( node @$ xml_rows_tag ) ; list < node > result nodes = xml handler . get nodes ( result rows node @$ row meta . xml_data_tag ) ; if ( ! result nodes . is empty ( ) ) { row meta row meta = new row meta ( xml handler . get sub node ( result rows node @$ row meta . xml_meta_tag ) ) ; for ( node result node : result nodes ) { object [ ] row data = row meta . get row ( result node ) ; rows . add ( new row meta and data ( row meta @$ row data ) ) ; } },'s read rows
the number is just to debug the <PLACE_HOLDER> of the fieldname,set < string > key set = fields . key set ( ) ; list < string > entries = new array list < string > ( key set ) ; string [ ] field names = entries . to array ( new string [ entries . size ( ) ] ) ; const . sort strings ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ; colinf [ __num__ ] . set combo values ( field names ) ;,number debug number
check that the jstl bundle is not already included in the <PLACE_HOLDER> @$ and include it if it is not because subsequent classes such as os gi web inf configuration use this <PLACE_HOLDER> to determine which jars are considered to be on the container classpath,if ( jstl bundle != null ) { if ( pattern == null ) { pattern = pattern . compile ( jstl bundle . get symbolic name ( ) ) ; deployment manager . set context attribute ( os gi web inf configuration . container_bundle_pattern @$ jstl bundle . get symbolic name ( ) ) ; } else if ( ! ( pattern . matcher ( jstl bundle . get symbolic name ( ) ) . matches ( ) ) ) { string s = tmp + __str__ + jstl bundle . get symbolic name ( ) ; pattern = pattern . compile ( s ) ; deployment manager . set context attribute ( os gi web inf configuration . container_bundle_pattern @$ s ) ; } } for,classes use pattern
the object that we 're modifying here is a copy of the original ! so let 's change the <PLACE_HOLDER> from relative to absolute by grabbing the file object ... in case the name of the file comes from previous steps @$ forget about this !,try { if ( ! accepting filenames ) { for ( int i = __num__ ; i < file name . length ; i ++ ) { file object file object = kettlevfs . get file object ( space . environment substitute ( file name [ i ] ) @$ space ) ; file name [ i ] = resource naming interface . name resource ( file object @$ space @$ utils . is empty ( file mask [ i ] ) ) ; } } return null ; } catch ( exception e ) { throw new kettle exception ( e ) ; },let change filename
check whether the path matches an existing leaf <PLACE_HOLDER> .,for ( string part : parts ) { if ( ! create new branch && node != root && node . children . is empty ( ) ) { return this ; } if ( node . children . contains key ( part ) ) { node = node . children . get ( part ) ; } else { create new branch = true ; node tmp = new node ( ) ; node . children . put ( part @$ tmp ) ; node = tmp ; } },path matches node
grouping shuffle reader produces <PLACE_HOLDER> in empty windows . for now @$ we count the element at least once to keep the current counter behavior .,if ( windows size == __num__ ) { element count . add value ( __num__ ) ; } else { element count . add value ( windows size ) ; },reader produces duplicates
length protocol version authentication result code server host <PLACE_HOLDER> connection <PLACE_HOLDER>,int offset = __num__ + __num__ + __num__ + __num__ + __num__ ;,host id id
unlock managed <PLACE_HOLDER> with unified lock,for ( user info profile : m user manager . get profiles ( user id ) ) { if ( tied managed profile ready to unlock ( profile ) ) { try { unlock child profile ( profile . id @$ false @$ challenge type @$ challenge @$ reset lockouts ) ; } catch ( remote exception e ) { log . d ( tag @$ __str__ @$ e ) ; } } if ( ! already unlocked ) { long ident = clear calling identity ( ) ; try { maybe show encryption notification for user ( profile . id ) ; } finally { restore calling identity ( ident ) ; } } },unlock managed profile
every language equals <PLACE_HOLDER> :,assert true ( new german ( ) . equals consider variants if specified ( new german ( ) ) ) ; assert true ( new germany german ( ) . equals consider variants if specified ( new germany german ( ) ) ) ; assert true ( new english ( ) . equals consider variants if specified ( new english ( ) ) ) ; assert true ( new american english ( ) . equals consider variants if specified ( new american english ( ) ) ) ;,language equals everything
mimic the normal kdc behavior . when a server is not allowed to send s 4 u 2 self @$ do not send an <PLACE_HOLDER> . instead @$ send a ticket which is useless later .,if ( ! names . contains ( cname . to string ( ) ) ) { allow forwardable = false ; },behavior send admin
now we have a <PLACE_HOLDER> . let ' register this <PLACE_HOLDER> .,m upload store . register post model ( post @$ media list ) ;,' register post
actual and values have the same <PLACE_HOLDER> but are they in the same order ?,if ( ! diff . differences found ( ) ) { int i = __num__ ; for ( object element from actual : actual ) { if ( ! are equal ( element from actual @$ values [ i ] ) ) { throw failures . failure ( info @$ elements differ at index ( element from actual @$ values [ i ] @$ i @$ comparison strategy ) ) ; } i ++ ; } return ; },actual have elements
a polygon should have more than 2 <PLACE_HOLDER>,preconditions . check argument ( abscissa . length > __num__ ) ;,polygon have digits
noinspection android lint clickable <PLACE_HOLDER> accessibility,note block holder . m avatar image view . set on touch listener ( m on gravatar touch listener ) ; if ( site id == user id ) { note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_no ) ; } else { note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_yes ) ; } note block holder . m avatar image view . set important for accessibility ( view . important_for_accessibility_no ) ; note block holder . m avatar image view . set content description ( null ) ; note block holder . m avatar image view . set on click listener ( null ) ;,noinspection android view
balancer sends initial <PLACE_HOLDER> .,lb response observer . on next ( build initial response ( ) ) ;,balancer sends response
if we have no record of this environment @$ that means the current rule implicitly uses the <PLACE_HOLDER> for this group . so explicitly opt that group 's <PLACE_HOLDER> into the refined set before trying to remove specific items .,if ( env to prune == null ) { for ( environment with group default env : get defaults ( refined environment to prune @$ dep environments . get refined environments ( ) ) ) { refined environments so far . add ( default env ) ; labels to environments . put ( default env . environment ( ) @$ default env ) ; } env to prune = verify . verify not null ( labels to environments . get ( refined environment to prune ) ) ; },rule opt environment
no space for eo cd <PLACE_HOLDER> in the file .,if ( file size < zip_eocd_rec_min_size ) { return null ; },space cd records
closing window should focus tex <PLACE_HOLDER>,openw . click ( ) ; text field element text field = $ ( text field element . class ) . first ( ) ; window element window = $ ( window element . class ) . first ( ) ; window . close ( ) ; assert equals ( text field . get wrapped element ( ) @$ get focused element ( ) ) ;,window focus text
now lost a finishable <PLACE_HOLDER> .,r2 . set can update finishable ( ) ; task wrapper . finishable state updated ( false ) ; task wrapper2 = task executor service . preemption queue . peek ( ) ; assert not null ( task wrapper2 ) ; assert true ( task wrapper . is in preemption queue ( ) ) ; r2 . complete ( ) ; r2 . await end ( ) ; task executor service . shut down ( false ) ;,now lost future
all plan item instances are created . now activate <PLACE_HOLDER> .,command context util . get agenda ( command context ) . plan activate plan item instance operation ( entry dependent plan item instance @$ satisfied criterion . get id ( ) ) ; for ( int i = parent plan item instances to activate . size ( ) - __num__ ; i >= __num__ ; i -- ) { plan item instance entity parent plan item instance = parent plan item instances to activate . get ( i ) ; if ( parent plan item instance == null ) { command context util . get agenda ( command context ) . plan create plan item instance operation ( parent plan item instance ) ; } command context util . get agenda ( command context ) . plan activate,plan activate them
on windows the system <PLACE_HOLDER> may be different than the user <PLACE_HOLDER> . this is an unsupported configuration @$ but in that case we want to return a dummy <PLACE_HOLDER> that will never cause a match in the usage of this api . this is important because windows documents that the family names of fonts are enumerated using the language of the system <PLACE_HOLDER>,system locale = ( locale ) java . security . access controller . do privileged ( new java . security . privileged action ( ) { public object run ( ) { string file encoding = system . get property ( __str__ @$ __str__ ) ; string sys encoding = system . get property ( __str__ ) ; if ( sys encoding != null && ! sys encoding . equals ( file encoding ) ) { return locale . root ; } string language = system . get property ( __str__ @$ __str__ ) ; string country = system . get property ( __str__ @$ __str__ ) ; string variant = system . get property ( __str__ @$ __str__ ) ; return new locale ( language @$ country,which return locale
since we send the empty <PLACE_HOLDER> to the client @$ if the client has not modified the <PLACE_HOLDER> then we do change it,if ( ! string utils . is empty ( user info . get password ( ) ) ) { user . set password ( user info . get password ( ) ) ; } user . set description ( user info . get description ( ) ) ; return user ;,client modified password
this hook is for testing <PLACE_HOLDER> only .,if ( boolean . get boolean ( cli strings . ignore_interceptors ) ) { return result model . create info ( cli strings . shutdown__msg__shutdown_entire_ds ) ; } response response = read yes no ( cli strings . shutdown__msg__warn_user @$ response . yes ) ; if ( response == response . no ) { return result model . create error ( cli strings . shutdown__msg__aborting_shutdown ) ; } else { return result model . create info ( cli strings . shutdown__msg__shutdown_entire_ds ) ; },hook testing use
so let 's create pipe <PLACE_HOLDER> to make that possible,return new pipe cloner impl ( ) . copy ( p ) ;,'s create clue
note that regular ser de does n't tolerate fewer <PLACE_HOLDER> .,list < object > deserialized row ; if ( do write fewer columns ) { deserialized row = ( list < object > ) serde_fewer . deserialize ( bytes writable ) ; } else { deserialized row = ( list < object > ) serde . deserialize ( bytes writable ) ; } object [ ] row = rows [ i ] ; for ( int index = __num__ ; index < write column count ; index ++ ) { object expected = row [ index ] ; object object = deserialized row . get ( index ) ; if ( expected == null || object == null ) { if ( expected != null || object != null ) { fail ( __str__ ) ; } },de tolerate rows
test when a user single clicks a directory name and clicks the action <PLACE_HOLDER> when the filename text field is empty,set mode ( files_only ) ; file dir = new file ( get test directory path ( ) ) ; set dir ( dir . get parent file ( ) ) ; assert equals ( dir . get parent file ( ) @$ get current directory ( ) ) ; set filename field text ( null ) ; set file ( dir ) ; press ok ( ) ; wait for chooser ( ) ; assert true ( __str__ + __str__ + last selected file @$ chooser . is showing ( ) ) ; assert equals ( dir @$ get current directory ( ) ) ;,user clicks button
i found that crimson does n't show the proper stack trace when a runtime <PLACE_HOLDER> happens inside a schema builder . the following code shows the actual <PLACE_HOLDER> that happened .,if ( e . get cause ( ) instanceof sax exception ) { sax exception se = ( sax exception ) e . get cause ( ) ; if ( se . get exception ( ) != null ) se . get exception ( ) . print stack trace ( ) ; } throw e ;,code shows exception
scale fractional <PLACE_HOLDER> @$ dot @$ integer <PLACE_HOLDER> .,final int scale = fast scale ; final boolean is zero fast1 and fast2 = ( fast1 == __num__ && fast2 == __num__ ) ; final boolean is zero fast2 = ( fast2 == __num__ ) ; int lower longword scale = __num__ ; int middle longword scale = __num__ ; int high longword scale = __num__ ; long long word = fast0 ; if ( scale > __num__ ) { lower longword scale = math . min ( scale @$ longword_decimal_digits ) ; for ( int i = __num__ ; i < lower longword scale ; i ++ ) { scratch buffer [ index -- ] = ( byte ) ( byte_digit_zero + long word % __num__ ) ; long word /= __num__ ; } if (,digits integer rounds
make sure nongreedy mech cut off does n't kill this <PLACE_HOLDER>,lexer grammar lg = new lexer grammar ( __str__ + __str__ + __str__ + __str__ ) ;,mech kill plan
test renaming a source to a path which uses a <PLACE_HOLDER> as a directory .,source file = new file ( test_dir @$ __str__ ) ; assert . assert true ( source file . create new file ( ) ) ; file bad target = new file ( target file @$ __str__ ) ; try { nativeio . rename to ( source file @$ bad target ) ; assert . fail ( ) ; } catch ( nativeio exception e ) { if ( path . windows ) { assert . assert equals ( string . format ( __str__ ) @$ e . get message ( ) ) ; } else { assert . assert equals ( errno . enotdir @$ e . get errno ( ) ) ; } },which uses directory
starting the case instance starts the process task . the process has an async <PLACE_HOLDER> at the beginning,cmmn engine . get cmmn runtime service ( ) . create case instance builder ( ) . case definition key ( __str__ ) . start ( ) ; job job = process engine . get management service ( ) . create job query ( ) . single result ( ) ; assert null ( job . get scope type ( ) ) ; job test helper . wait for job executor to process all jobs ( process engine . get process engine configuration ( ) @$ process engine . get management service ( ) @$ __num__ @$ __num__ ) ;,process has task
total overlap do n't add either <PLACE_HOLDER> @$ get next range 1 and range 2,range1 = range1 it . has next ( ) ? range1 it . next ( ) : null ; range2 = range2 it . has next ( ) ? range2 it . next ( ) : null ; break ; case range1_starts_at_range2_ends_after_range2 :,overlap add range
nested view has scrollable <PLACE_HOLDER> under this point . let it be handled there .,if ( dy != __num__ && ! is gutter drag vertically ( m last motiony @$ dy ) && can scroll vertically ( this @$ false @$ ( int ) dy @$ ( int ) x @$ ( int ) y ) ) { m last motionx = x ; m last motiony = y ; m is unable to drag = true ; return false ; },view has area
already exist . only update allowed <PLACE_HOLDER> .,connected controller record record = m controller records . get ( saved info ) ; record . allowed commands = commands ;,update allowed commands
break @$ next read will support larger <PLACE_HOLDER> .,break ;,read support size
make sure test invoice plugin api will return an additional tax <PLACE_HOLDER>,test invoice plugin api . add tax item ( new tax invoice item ( uuid . randomuuid ( ) @$ null @$ null @$ account . get id ( ) @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ new local date ( __num__ @$ __num__ @$ __num__ ) @$ new local date ( __num__ @$ __num__ @$ __num__ ) @$ __str__ @$ big decimal . one @$ account . get currency ( ) @$ null @$ null ) ) ; test invoice plugin api . add tax item ( new tax invoice item ( uuid . randomuuid ( ) @$ null @$ null @$ account . get id ( ) @$ null @$ null @$,api return item
the touchable region should not exceed the <PLACE_HOLDER> of its container .,apply op to region by bounds ( touchable region @$ this @$ region . op . intersect ) ; final view parent parent = get parent ( ) ; if ( parent != null ) { parent . subtract obscured touchable region ( touchable region @$ this ) ; },region exceed size
we know that the parameters have the same name @$ since this is what the descriptor 's hash code & equality are based on . the only thing that may be different is the <PLACE_HOLDER> . and since the proposed parameter does not have a <PLACE_HOLDER> @$ we want to use whatever is currently set .,return old parameter == null ? descriptor : old parameter . get descriptor ( ) ;,parameter have description
if the comment is lacking <PLACE_HOLDER> @$ offer moderation actions,if ( note . get comment status ( ) == comment status . unapproved ) { if ( note . can moderate ( ) ) { add comment approve action for comment notification ( context @$ builder @$ note id ) ; } } else { if ( note . can like ( ) ) { add comment like action for comment notification ( context @$ builder @$ note id ) ; } },comment lacking permission
error getting active data rethrows <PLACE_HOLDER>,try { mockito . when ( mockzk . get data ( mockito . eq ( zk_lock_name ) @$ mockito . eq ( false ) @$ any ( ) ) ) . then throw ( new keeper exception . auth failed exception ( ) ) ; elector . get active data ( ) ; assert . fail ( __str__ ) ; } catch ( keeper exception . auth failed exception ke ) { mockito . verify ( mockzk @$ mockito . times ( __num__ ) ) . get data ( mockito . eq ( zk_lock_name ) @$ mockito . eq ( false ) @$ any ( ) ) ; },error rethrows exception
generating portfolio object <PLACE_HOLDER> to be populated across the pr 's & local regions,portfolio data [ ] portfolio data = create portfolio data ( start_portfolio_data_index @$ total_data_size ) ;,portfolio object array
count all nodes @$ how many groups <PLACE_HOLDER> node has <PLACE_HOLDER>,configuration group config = configuration . with batch size ( config @$ neo store . get relationship group store ( ) . get records per page ( ) ) ; stats provider memory usage = new memory usage stats provider ( neo store @$ group cache ) ; execute stage ( new count groups stage ( group config @$ from store @$ group cache @$ memory usage ) ) ; long from node id = __num__ ; while ( from node id < high node id ) { long to node id = group cache . prepare ( from node id ) ; monitor . defragmenting node range ( from node id @$ to node id ) ; execute stage ( new scan and cache groups stage (,groups has activity
owner sets <PLACE_HOLDER> and permission,verify set acl ( test_user_1 @$ test_dir_uri @$ null @$ test_user_2 . get group ( ) @$ ( short ) __num__ @$ true ) ; file info file info = m file system master . get file info ( m file system master . get file id ( new alluxiouri ( test_dir_file_uri ) ) ) ; assert equals ( test_user_2 . get group ( ) @$ file info . get group ( ) ) ; assert equals ( ( short ) __num__ @$ file info . get mode ( ) ) ;,owner sets permission
handle the case where the last call to write actually caused an <PLACE_HOLDER> in the log,if ( ( ledger writer . is log segment in error ( ) || force recovery ) && reset on error ) { completable future < void > close future ; if ( ledger writer . is log segment in error ( ) ) { close future = ledger writer . async abort ( ) ; } else { close future = ledger writer . async close ( ) ; } return close future . then compose ( new function < void @$ completion stage < bk log segment writer > > ( ) { @ override public completable future < bk log segment writer > apply ( void result ) { remove cached log writer ( ) ; if ( ledger writer . is log segment in,call caused error
make sure the state we just read in for columns @$ rows @$ and scrollbar visibility has legal <PLACE_HOLDER>,if ( columns < __num__ ) { columns = __num__ ; } if ( rows < __num__ ) { rows = __num__ ; } if ( ( scrollbar visibility < scrollbars_both ) || ( scrollbar visibility > scrollbars_none ) ) { this . scrollbar visibility = scrollbars_both ; } if ( text area serialized data version < __num__ ) { set focus traversal keys ( keyboard focus manager . forward_traversal_keys @$ forward traversal keys ) ; set focus traversal keys ( keyboard focus manager . backward_traversal_keys @$ backward traversal keys ) ; },state has values
the line crosses a paragraph <PLACE_HOLDER>,if ( get paragraph index ( start ) != get paragraph index ( limit - __num__ ) ) { throw new illegal argument exception ( ) ; },line crosses boundary
check that union has only a single <PLACE_HOLDER>,if ( this . get outgoing connections ( ) . size ( ) > __num__ ) { throw new compiler exception ( __str__ ) ; } boolean children skipped due to replicated input = false ;,union has output
we compute <PLACE_HOLDER> the distance between each cluster and its closest neighbor is to set a proportional distance threshold for points that should be involved in calculating the centroid .,closest cluster distances . clear ( ) ; for ( vector center : centroids ) { vector closest other cluster = centroids . search first ( center @$ true ) . get value ( ) ; closest cluster distances . add ( distance measure . distance ( center @$ closest other cluster ) ) ; },distance is what
create a media file <PLACE_HOLDER>,string file name = string . format ( __str__ @$ new simple date format ( __str__ ) . format ( new date ( ) ) ) ; if ( type == media_type_image ) { file name = string . format ( __str__ @$ file name ) ; } else if ( type == media_type_video ) { file name = string . format ( __str__ @$ file name ) ; } else { log . e ( tag @$ __str__ + type ) ; return null ; } return new file ( string . format ( __str__ @$ storage dir . get path ( ) @$ file . separator @$ file name ) ) ;,media file name
reduce to subinterfaces this does not need to be recursive since we make a copy @$ and that copy contains all super <PLACE_HOLDER> for the whole hierarchy,for ( ct class intf : alter map . values ( ) ) { ct class [ ] interfaces ; try { interfaces = intf . get interfaces ( ) ; } catch ( not found exception e ) { throw new runtime exception ( e ) ; } for ( ct class c : interfaces ) alter map . remove ( c . get name ( ) ) ; } return alter map ;,copy contains interfaces
exception will set <PLACE_HOLDER>,new certificate manager ( key @$ cert ) ;,exception set error
the glow depends <PLACE_HOLDER> on the velocity @$ and therefore starts out nearly invisible .,m glow alpha start = __num__ ; m glow scaley start = math . max ( m glow scaley @$ __num__ ) ;,glow depends more
find the actual first which matches the <PLACE_HOLDER> .,boolean bool = super . internal first ( ) ; if ( p == null ) { return bool ; } while ( bool ) { if ( p . evaluate ( this ) ) { break ; } bool = super . internal next ( ) ; } return bool ;,which matches filter
initial board has a <PLACE_HOLDER> like the following in the center : wb bw,int middle row = board . length / __num__ ; int middle column = board [ middle row ] . length / __num__ ; board [ middle row ] [ middle column ] = new piece ( color . white ) ; board [ middle row + __num__ ] [ middle column ] = new piece ( color . black ) ; board [ middle row + __num__ ] [ middle column + __num__ ] = new piece ( color . white ) ; board [ middle row ] [ middle column + __num__ ] = new piece ( color . black ) ; black count = __num__ ; white count = __num__ ;,board has layout
start definitions root <PLACE_HOLDER>,xtw . write start element ( bpmn2_prefix @$ element_definitions @$ bpmn2_namespace ) ; xtw . set default namespace ( bpmn2_namespace ) ; xtw . write default namespace ( bpmn2_namespace ) ; xtw . write namespace ( bpmn2_prefix @$ bpmn2_namespace ) ; xtw . write namespace ( xsi_prefix @$ xsi_namespace ) ; xtw . write namespace ( xsd_prefix @$ schema_namespace ) ; xtw . write namespace ( activiti_extensions_prefix @$ activiti_extensions_namespace ) ; xtw . write namespace ( bpmndi_prefix @$ bpmndi_namespace ) ; xtw . write namespace ( omgdc_prefix @$ omgdc_namespace ) ; xtw . write namespace ( omgdi_prefix @$ omgdi_namespace ) ; for ( string prefix : model . get namespaces ( ) . key set ( ) ) { if ( ! default namespaces . contains ( prefix,definitions root element
let 's give the principal the administration <PLACE_HOLDER> @$ without granting access,mutable acl acl first deny = new acl impl ( identity @$ __num__ @$ acl authorization strategy @$ new console audit logger ( ) ) ; acl first deny . insert ace ( __num__ @$ base permission . administration @$ new principal sid ( auth ) @$ false ) ;,'s give permission
workaround : if the abstract value type does not have any <PLACE_HOLDER> @$ a vector containing value base should be returned instead of an empty vector,if ( v . derived from ( ) . size ( ) == __num__ ) stream . print ( __str__ ) ; else { symtab entry parent ; for ( int i = __num__ ; i < v . derived from ( ) . size ( ) ; i ++ ) { if ( i == __num__ ) stream . print ( __str__ ) ; else stream . print ( __str__ ) ; parent = ( symtab entry ) v . derived from ( ) . element at ( i ) ; stream . print ( util . java name ( parent ) ) ; } },type have methods
an event passes multiple <PLACE_HOLDER>,event filter dimension map . put ( __str__ @$ lists . new array list ( __str__ ) ) ; filtered events = event filter . apply dimension filter ( all events @$ event filter dimension map ) ; assert . assert equals ( filtered events . size ( ) @$ __num__ ) ; assert . assert equals ( filtered events . get ( __num__ ) . get name ( ) @$ __str__ ) ;,event passes filters
asif : do the <PLACE_HOLDER> of the different group junction results .,compiled value iter operands to send = null ; if ( ! iter operands . is empty ( ) ) { int size = iter operands . size ( ) ; compiled value cv [ ] = new compiled value [ size ] ; for ( int k = __num__ ; k < size ; ++ k ) { cv [ k ] = ( compiled value ) this . iter operands . get ( k ) ; } if ( cv . length == __num__ ) { iter operands to send = cv [ __num__ ] ; } else { iter operands to send = new compiled junction ( cv @$ this . operator ) ; } },asif do division
get <PLACE_HOLDER> in this we should try 5 count get the <PLACE_HOLDER>,int count = __num__ ; exception error = null ; while ( count > __num__ ) { if ( command . is cancel ) { if ( command . m listener != null ) command . m listener . on cancel ( ) ; break ; } try { command . m result = i_command . command ( command . m id @$ command . m timeout @$ command . m parameters ) ; if ( command . m listener != null ) command . m listener . on completed ( command . m result ) ; break ; } catch ( exception e ) { error = e ; count -- ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e1,count get command
like many fully successful operations @$ a single row fetch <PLACE_HOLDER> as 2 logical row operations @$ one for locating the row and one for retrieving it .,assert equals ( __num__ @$ m_ee . m_calls fromee ) ; assert equals ( long opthreshold @$ m_ee . m_last tuples accessed ) ; assert true ( __num__ < m_ee . m_curr memory in bytes ) ; assert true ( __num__ > m_ee . m_curr memory in bytes ) ; assert true ( __num__ < m_ee . m_peak memory in bytes ) ; assert true ( __num__ > m_ee . m_peak memory in bytes ) ; assert true ( m_ee . m_peak memory in bytes >= m_ee . m_curr memory in bytes ) ;,row fetch counts
jdbc should return <PLACE_HOLDER> ...,if ( data instanceof string ) { r . deliver ( data ) ; } else if ( data instanceof long ) { long indexes = ( ( long ) data ) . long value ( ) ; r . deliver ( convert set value ( column @$ indexes @$ options ) ) ; },jdbc return string
another call to close should not cause an <PLACE_HOLDER>,zis . close ( ) ;,call cause exception
after the delay the lfo should start <PLACE_HOLDER> let make sure output is accurate enough,double p_step = ( __num__ / control_rate ) * math . exp ( ( freq - __num__ ) * ( math . log ( __num__ ) / __num__ ) ) ; double p = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { p += p_step ; double predicted_output = __num__ + math . sin ( p * __num__ * math . pi ) * __num__ ; if ( math . abs ( predicted_output - lfo_output [ __num__ ] ) > __num__ ) throw new exception ( __str__ + predicted_output + __str__ + lfo_output [ __num__ ] + __str__ ) ; lfo . process control logic ( ) ; },lfo start execution
7900040069 ea 030000000000 8300 c 20003006 aea 03005 c 003800223 aab 5107 a <PLACE_HOLDER> b 41 a 0030 d 506 e 3000414010250 bf 630007000000000000000000000000000000 c 3107209000089050000000000006 bea 03005 c 003800403 aab 5107 a <PLACE_HOLDER> b 41 a 0030 d 506 e 3000414010250 bf 630007000000000000000000000000000000 c 3107209000089050000000000006 cea 03005 c 0038005 e 3 aab 5107 a <PLACE_HOLDER> b 41 a 0030 d 506 e 3000414010250,apel protocol decoder decoder = new apel protocol decoder ( null ) ;,aab 5107 630007000000000000000000000000000000
app : released api 10 dev : released api <PLACE_HOLDER>,verify compute target sdk version ( older_version @$ released @$ true @$ older_version ) ;,app released 2
we used to destroy the buffers on exiting fs mode @$ this is no longer needed since fs change will cause a surface data <PLACE_HOLDER>,synchronized ( peer ) { exit full screen exclusive ( screen @$ peer ) ; },change cause reset
the value of call count can exceed <PLACE_HOLDER> only if the callback thread survives the exception thrown by the first callback .,assert true ( call count . get ( ) > __num__ ) ; if ( watcher != null ) { watcher . stop ( ) ; watcher . wait for state ( file change watcher . state . stopped ) ; },value exceed 4
if still null then the implementation does n't offer a presence operation <PLACE_HOLDER> which is unacceptable for jabber .,if ( op set pers presence2 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ;,implementation offer set
first preference for <PLACE_HOLDER> goes to the task id set in the activity options . use the <PLACE_HOLDER> associated with that if possible .,if ( task id != invalid_task_id ) { options . set launch task id ( invalid_task_id ) ; final task record task = any task for id ( task id @$ match_task_in_stacks_or_recent_tasks_and_restore @$ options @$ on top ) ; options . set launch task id ( task id ) ; if ( task != null ) { return task . get stack ( ) ; } },preference use task
start from relation type go to a role player check the role player 's type go to the other role player check two <PLACE_HOLDER> are different check the role player 's type check the subtypes,assert that ( plan @$ contains in any order ( instance of ( label fragment . class ) @$ instance of ( label fragment . class ) @$ instance of ( label fragment . class ) @$ instance of ( in isa fragment . class ) @$ instance of ( out role player fragment . class ) @$ instance of ( out isa fragment . class ) @$ instance of ( out role player fragment . class ) @$ instance of ( neq fragment . class ) @$ instance of ( out isa fragment . class ) @$ instance of ( out sub fragment . class ) ) ) ;,start go players
this is the actual creation of the file system . return true indicating a <PLACE_HOLDER>,return su . execute ( listener @$ root username @$ root password @$ new create ( listener @$ home @$ uid @$ gid @$ user name ) ) ;,system indicating success
for some reason in some corner cases nodes are n't having sentence <PLACE_HOLDER> set do a pass and make sure all nodes have sentence <PLACE_HOLDER> set,semantic graph sg = sentence . get ( semantic graph core annotations . collapsed dependencies annotation . class ) ; if ( sg != null ) { for ( indexed word iw : sg . vertex set ( ) ) { if ( iw . get ( core annotations . sentence index annotation . class ) == null && sentence . get ( core annotations . sentence index annotation . class ) != null ) { iw . set sent index ( sentence . get ( core annotations . sentence index annotation . class ) ) ; } } },nodes have index
latvian does not use <PLACE_HOLDER> first,string [ ] data = { __str__ @$ __str__ @$ __str__ @$ __str__ } ; generic locale starter ( new locale ( __str__ @$ __str__ ) @$ data ) ;,latvian use spaces
exclude if this bulletin does n't have a source <PLACE_HOLDER> or if it does n't match,if ( bulletin query . get source id pattern ( ) != null ) { if ( bulletin . get source id ( ) == null || ! bulletin query . get source id pattern ( ) . matcher ( bulletin . get source id ( ) ) . find ( ) ) { return false ; } },bulletin have id
if 1 st sibling @$ return <PLACE_HOLDER>,if ( result == null ) { result = node . get parent node ( ) ; return result ; },1 st parent
assert that all threads got back the same <PLACE_HOLDER>,for ( future < map < bounded window @$ long > > result : results ) { assert equals ( value @$ result . get ( ) ) ; for ( map . entry < bounded window @$ long > entry : result . get ( ) . entry set ( ) ) { assert same ( value . get ( entry . get key ( ) ) @$ entry . get value ( ) ) ; } },threads got reference
rejoin does not do paused <PLACE_HOLDER> .,list < string > rejoin cmd ln str = rejoin cmd ln . create command line ( ) ; string cmd line full = __str__ ; for ( string element : rejoin cmd ln str ) { cmd line full += __str__ + element ; } log . info ( cmd line full ) ; m_proc builder . command ( ) . clear ( ) ; m_proc builder . command ( ) . add all ( rejoin cmd ln str ) ; process proc = m_proc builder . start ( ) ; start = system . current time millis ( ) ;,rejoin do commands
make sure we do n't think anyone else is holding the <PLACE_HOLDER>,if ( got ) { if ( blackboard . get is locked ( ) ) { string msg = __str__ + service name + __str__ + object name + __str__ + ( ( d lock service ) service ) . get lock grantor id ( ) + __str__ + service . is lock grantor ( ) ; system . out . println ( __str__ + msg ) ; fail ( msg ) ; } blackboard . set is locked ( true ) ; long count = blackboard . get count ( ) ; system . out . println ( __str__ + count + __str__ + service name + __str__ + object name ) ; thread . sleep ( hold time ) ; blackboard . inc count ( ),anyone holding lock
no service for this class @$ must choose the <PLACE_HOLDER> by selection,p conf . as node list ( ) . get ( ) . stream ( ) . filter ( this :: not reserved provider key ) . for each ( provider specific conf -> { if ( ! provider specific . compare and set ( null @$ provider specific conf ) ) { throw new security exception ( __str__ + __str__ + provider specific . get ( ) . key ( ) + __str__ + provider specific conf . key ( ) ) ; } } ) ;,service choose better
the user has selected a new <PLACE_HOLDER>,if ( node != null ) { this . target = node ; if ( node . get start node ( ) != null ) { session session = model . get singleton ( ) . get session ( ) ; list < context > contexts = session . get contexts for node ( node . get start node ( ) ) ; for ( context context : contexts ) { ctx names . add ( context . get name ( ) ) ; } } else if ( node . get context ( ) != null ) { ctx names . add ( node . get context ( ) . get name ( ) ) ; } },user selected node
invalid <PLACE_HOLDER> is now since the session straddles the window cutoff <PLACE_HOLDER> .,expected stats . expiration time elapsed = now ; expected stats . execution time in window ms = __num__ * minute_in_millis ; expected stats . bg job count in window = __num__ ; expected stats . execution time in max period ms = __num__ * minute_in_millis ; expected stats . bg job count in max period = __num__ ; expected stats . session count in window = __num__ ; m quota controller . update execution stats locked ( __num__ @$ __str__ @$ input stats ) ; assert equals ( expected stats @$ input stats ) ; input stats . window size ms = expected stats . window size ms = __num__ * minute_in_millis ;,session straddles time
exopackage builds <PLACE_HOLDER> to jar @$ otherwise @$ <PLACE_HOLDER> to raw .,dex store default dex store = exopackage mode . enabled for secondary dexes ( exopackage modes ) ? dex store . jar : dex store . raw ; dex split strategy dex split strategy = args . get minimize primary dex size ( ) ? dex split strategy . minimize_primary_dex_size : dex split strategy . maximize_primary_dex_size ; return new dex split mode ( args . get use split dex ( ) @$ dex split strategy @$ args . get dex compression ( ) . or else ( default dex store ) @$ args . get linear alloc hard limit ( ) @$ args . get dex group lib limit ( ) @$ args . get primary dex patterns ( ) @$ args . get primary dex classes file,exopackage builds link
will be null when the feature to use minimized <PLACE_HOLDER> is disabled .,if ( minimized bitcode == null ) { return full bitcode ; } return minimized bitcode ;,feature use bitmap
proto source root . this ensures that protos can reference either the full <PLACE_HOLDER> or the short <PLACE_HOLDER> when including other protos .,command line . add all ( vector arg . of ( transitive imports ) . mapped ( new expand import args fn ( output directory @$ direct proto source roots ) ) ) ; if ( protos in direct dependencies != null ) { if ( ! protos in direct dependencies . is empty ( ) ) { command line . add all ( __str__ @$ vector arg . join ( __str__ ) . each ( protos in direct dependencies ) . mapped ( new expand to path fn with imports ( output directory @$ direct proto source roots ) ) ) ; } else { command line . add ( __str__ ) ; } },protos reference package
start <PLACE_HOLDER> create <PLACE_HOLDER>,event = ( activiti entity event ) listener . get events received ( ) . get ( __num__ ) ; assert equals ( activiti event type . entity_created @$ event . get type ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get process instance id ( ) ) ; assert not equals ( process instance . get id ( ) @$ event . get execution id ( ) ) ; assert equals ( process instance . get process definition id ( ) @$ event . get process definition id ( ) ) ;,event create event
if aod is showing @$ the ime should be hidden . however @$ sometimes the aod is considered hidden because it 's in the process of hiding @$ but it 's still being shown on screen . in that case @$ we want to continue hiding the ime until the windows have completed <PLACE_HOLDER> . this way @$ we know that the ime can,final boolean hide ime = win . is input method window ( ) && ( m aod showing || ! m default display policy . is window manager draw complete ( ) ) ; if ( hide ime ) { return true ; } final boolean show ime over keyguard = ime target != null && ime target . is visible lw ( ) && ( ime target . can show when locked ( ) || ! can be hidden by keyguard lw ( ime target ) ) ;,windows completed processing
sets the peer supported signature <PLACE_HOLDER> to use in km temporarily .,session . set peer supported signature algorithms ( supported sign algs ) ;,sets supported algorithms
if we 're in a chunk and the item spanned <PLACE_HOLDER>,if ( in block && special chunk ) { in block = false ; block size index = - __num__ ; block size position = - __num__ ; start_block ( ) ; special chunk = false ; },item spanned everything
target <PLACE_HOLDER> may not match connected <PLACE_HOLDER>,dc . send ( bb @$ p . get socket address ( ) ) ;,address match address
if we have operation set persistent presence skip sending initial presence while login is executed @$ the operation set will take <PLACE_HOLDER> of it,if ( get operation set ( operation set persistent presence . class ) != null ) conf conn . set send presence ( false ) ;,set take care
weird case : someone has given us their own custom i intent <PLACE_HOLDER> @$ and now they have someone else trying to send to it but of course this is n't really a pending intent @$ so there is no base intent @$ and the caller is n't supplying an intent ... but we never want to dispatch a null intent to a receiver,if ( intent == null ) { slog . wtf ( tag @$ __str__ ) ; intent = new intent ( intent . action_main ) ; },someone given handler
sep ; ascii ' 4 ' single quotes check header <PLACE_HOLDER>,parse setup setup = new parse setup ( xls_info @$ ( byte ) __num__ @$ true @$ parse setup . no_header @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ ctypes @$ null @$ null @$ null ) ;,quotes check length
second call must not log another <PLACE_HOLDER>,assert that ( under test . is operational ( ) ) . is false ( ) ; assert that ( memory appender . events ) . extracting ( i logging event :: get level @$ i logging event :: get message ) . contains only once ( tuple ( level . info @$ __str__ ) ) ;,call log event
fuzzy k means <PLACE_HOLDER>,throw new unsupported operation exception ( __str__ ) ;,k means failure
one shot <PLACE_HOLDER>,if ( timeto measure iterations == __num__ ) { long start = system . current time millis ( ) ; ksession . execute ( command factory . new insert elements ( arrays . as list ( data ) ) ) ; system . out . println ( __str__ + message + __str__ + ( system . current time millis ( ) - start ) ) ; assert true ( list . size ( ) > __num__ ) ; } else { long start = system . current time millis ( ) ; long end = start + timeto measure iterations ; int count = __num__ ; while ( system . current time millis ( ) < end ) { stateless kie session sess2 = create stateless knowledge session,one shot computation
tree indexes have a 40 byte <PLACE_HOLDER> per row .,isize . width min += tree_map_entry_overhead + tuple_ptr_size ; isize . width max += tree_map_entry_overhead + tuple_ptr_size ;,indexes have buffer
then it is an attribute or its not an attribute name nor operation name and the below invocation will throw a attribute not found <PLACE_HOLDER> .,result = do attribute operation ( mbsc @$ instance @$ sub command @$ attribute info ) ;,invocation throw exception
the following should have the same <PLACE_HOLDER> as the previous @$ since it has the same restrictions applied in reverse order .,s = open session ( ) ; s . get transaction ( ) . begin ( ) ; root criteria = s . create criteria ( order . class @$ __str__ ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; root criteria . create alias ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; result = ( order ) root criteria . unique result ( ) ; assert equals ( order1 . get order id ( ) @$ result . get order id ( ) ) ; assert equals ( __num__ @$ result,following have result
res stores the first found abstract <PLACE_HOLDER>,method res = null ; for ( method mi : methods ) { if ( ! modifier . is abstract ( mi . get modifiers ( ) ) ) continue ; if ( mi . get annotation ( traits . implemented . class ) != null ) continue ; try { object . class . get method ( mi . get name ( ) @$ mi . get parameter types ( ) ) ; continue ; } catch ( no such method exception e ) { } if ( res != null ) return null ; res = mi ; },stores found method
only the exception case would have destroyed the proxy <PLACE_HOLDER> . we can safely proceed here .,repo . put entry in monitoring region map ( member @$ proxy monitoring region ) ; repo . put entry in notif region map ( member @$ proxy notification region ) ; try { if ( ! running ) { return ; } proxy factory . create all proxies ( member @$ proxy monitoring region ) ; management cache listener . mark ready ( ) ; notif listener . mark ready ( ) ; } catch ( exception e ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ e ) ; } throw new management exception ( e ) ; },case destroyed region
have one cycle of successful calls to verify valid tracker clients returned . try <PLACE_HOLDER> balancing on this update state @$ need to update state before forcing the strategy .,tracker client resulttc = get tracker client ( strategy @$ request @$ new request context ( ) @$ __num__ @$ clients ) ; strategy . set strategy ( default_partition_id @$ partition degrader load balancer state . strategy . load_balance ) ; resulttc = get tracker client ( strategy @$ request @$ new request context ( ) @$ __num__ @$ clients ) ; assert not null ( resulttc @$ __str__ ) ; for ( int j = __num__ ; j < num_checks ; j ++ ) { cc list . add ( client1 . get call tracker ( ) . start call ( ) ) ; cc list . add ( client2 . get call tracker ( ) . start call ( ) ) ; } clock . add,returned try balancing
drag & drop <PLACE_HOLDER>,recycler view drag drop manager drag drop manager = new recycler view drag drop manager ( ) ; drag drop manager . set dragging item shadow drawable ( ( nine patch drawable ) context . get resources ( ) . get drawable ( r . drawable . shadow_8dp ) ) ; recycler view . adapter adapter = new quick search adapter ( ) ; adapter . set has stable ids ( true ) ; adapter = drag drop manager . create wrapped adapter ( adapter ) ;,& drop manager
as the annotation is legal on fields and methods only @$ javac itself will take <PLACE_HOLDER> of printing an error message for this .,return ;,itself take care
wait @$ the client no longer has <PLACE_HOLDER> to the display .,if ( ! m window manager internal . is uid allowed on display ( cs . self reported display id @$ cs . uid ) ) { return input bind result . invalid_display_id ; },client has access
caution : fist delete tasks then jobs @$ as task has a foreign <PLACE_HOLDER> .,try { int job retention days = monitor task info . get default retention days ( ) ; int deleted tasks = dao_registry . get taskdao ( ) . delete records older than days ( job retention days ) ; int deleted jobs = dao_registry . get jobdao ( ) . delete records older than days ( job retention days ) ; log . info ( __str__ @$ deleted tasks @$ deleted jobs @$ job retention days ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; },tasks has key
if we are using osvr get the eye <PLACE_HOLDER> here,if ( environment . getvr hardware ( ) instanceof osvr ) { ( ( osvr ) environment . getvr hardware ( ) ) . get eye info ( ) ; },osvr get info
see if the qualifier expression is a type or <PLACE_HOLDER> name . javac does not provide the exact method required @$ so we first check if qualifier expression identifies a type @$ and if not @$ then we check to see if it identifies a <PLACE_HOLDER> .,type t = attr . attrib type ( ref . qualifier expression @$ env ) ; if ( t . is erroneous ( ) ) { if ( ref . member name == null ) { package symbol pck = elements . get package element ( ref . qualifier expression . to string ( ) ) ; if ( pck != null ) { return pck ; } else if ( ref . qualifier expression . has tag ( jc tree . tag . ident ) ) { tsym = env . encl class . sym ; member name = ( ( jc ident ) ref . qualifier expression ) . name ; } else return null ; } else { return null ; } } else {,expression identifies member
so that other persistence context can also see new <PLACE_HOLDER>,employee = sfsb1 . get employee notx ( __num__ ) ;,context see objects
verify user from a group which has column family level <PLACE_HOLDER> can read all the data belonging to that family and group which has no <PLACE_HOLDER> ca n't read any data .,grant on table ( test_util @$ testgroup_1_name @$ table name @$ test_family @$ null @$ permission . action . read ) ; verify allowed ( testgroup1_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup1_user1 @$ scan family action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan family action for group with family level access ) ;,which has access
function invoked 2 <PLACE_HOLDER> @$ cancel never,verify ( on next function @$ times ( __num__ ) ) . apply ( any ( ) ) ; verify ( subscription @$ never ( ) ) . cancel ( ) ;,function invoked times
then the repository has a success <PLACE_HOLDER> and usernames are available,assert false ( usernames repository . is error ( ) ) ; assert true ( usernames repository . get ( ) . length == usernames . length ) ;,repository has message
find the old generation which supports low memory <PLACE_HOLDER>,list iterator iter = pools . list iterator ( ) ; while ( iter . has next ( ) ) { memory poolmx bean p = ( memory poolmx bean ) iter . next ( ) ; if ( p . get type ( ) == memory type . heap && p . is usage threshold supported ( ) ) { mpool = p ; if ( trace ) { system . out . println ( __str__ + __str__ ) ; memory util . print memory pool ( mpool ) ; } break ; } } test listener listener = new test listener ( ) ; sensor listener l2 = new sensor listener ( ) ; notification emitter emitter = ( notification emitter ) mm ; emitter .,which supports detection
register the chat window menu bar <PLACE_HOLDER> .,container filter . put ( container . container_id @$ container . container_chat_menu_bar . getid ( ) ) ; bundle context . register service ( plugin component factory . class . get name ( ) @$ new otr plugin component factory ( container . container_chat_menu_bar ) @$ container filter ) ;,menu bar component
but if statements and loops get <PLACE_HOLDER> automagically .,assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ; assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ; assert pretty print ( __str__ @$ __str__ + __str__ + __str__ ) ;,statements get forwarded
last task to exit when shutdown release <PLACE_HOLDER>,int remaining = thread exit ( this @$ replace me ) ; if ( remaining == __num__ && is shutdown ( ) ) impl close ( ) ;,shutdown release resources
release compute engine startup method which will throw startup <PLACE_HOLDER>,compute engine . release startup ( ) ; while ( ce server . get status ( ) == monitored . status . down ) { },which throw exception
deque contains a <PLACE_HOLDER> that when true indicates that the value in that position of the window is missing .,return new aggregate function < t @$ integer > ( ) { private final array deque < boolean > queue = new array deque < > ( ) ; private int missing count = __num__ ; @ override public void remove left most ( ) { boolean removed missing value = queue . remove ( ) ; if ( removed missing value ) { missing count -- ; } } @ override public void add right most ( t new value ) { queue . add ( false ) ; } @ override public void add right most missing ( ) { queue . add ( true ) ; missing count ++ ; } @ override public integer get value ( ) { return queue . size (,deque contains flag
if this is a transactional remove <PLACE_HOLDER> @$ we will not have version information as it is only generated at commit so treat transactional remove <PLACE_HOLDER> as if the server is not versioned . if we have no storage then act as if the server is not versioned .,final boolean server is versioned = proxy result != null && proxy result . region is versioned ( ) && ! istx ( ) && get data policy ( ) . with storage ( ) ; if ( ! server is versioned && ! partial result ) { proxy result = null ; },transactional remove operation
if user typed the 'quit ' command @$ wait until the server closes the <PLACE_HOLDER> .,while ( ( line = command input stream . read line ( ) ) != null ) { if ( __str__ . equals ( line . to lower case ( ) ) ) { log . info ( __str__ ) ; channel . close ( ) ; break ; } message base msg = produce message base on user input ( line @$ ( short ) random . next int ( short . max_value ) ) ; if ( msg == null ) { continue ; } send ( msg ) ; },server closes connection
if the job is python shell job @$ the entry point class name is python gate <PLACE_HOLDER> server . otherwise @$ the entry point class of python job is python driver,if ( entry point class == null ) { entry point class = __str__ ; } if ( jar file path == null ) { throw new illegal argument exception ( __str__ ) ; } jar file = get jar file ( jar file path ) ;,name python ctor
<PLACE_HOLDER>ut that is not known until after a 's creation . a 's creation occurs within <PLACE_HOLDER> @$ which requires <PLACE_HOLDER> to have an outer field in order to access a 's capturing field for i. <PLACE_HOLDER> 's creation therefore requires the outer field to <PLACE_HOLDER>e passed as an outer argument . <PLACE_HOLDER>ecause of the cascading effects of the statements in this test and,resolve source ( __str__ @$ __str__ + __str__ + __str__ ) ; class instance creation b create = ( class instance creation ) nodes by type . get ( kind . class_instance_creation ) . get ( __num__ ) ; expression outer arg = b create . get expression ( ) ; assert true ( outer arg instanceof simple name ) ; variable element var = tree util . get variable element ( outer arg ) ; assert not null ( var ) ; assert equals ( __str__ @$ element util . get name ( var ) ) ;,creation requires b
for remote output port @$ it 's possible that multiple processors are connected . in that case @$ the received flow file is cloned and passed to each connection . so we need to create multiple data set <PLACE_HOLDER> .,final list < connection status > connections = nifi flow . get outgoing connections ( port process id ) ; if ( connections == null || connections . is empty ( ) ) { logger . warn ( __str__ @$ new object [ ] { event } ) ; return ; },data set attributes
owid with the given value found <PLACE_HOLDER> searching now for row id ...,if ( pos >= __num__ ) { key = compressed owids [ pos ] ; if ( row id < row ids [ key . from index ] || row id > row ids [ key . to index - __num__ ] ) { return false ; } if ( arrays . binary search ( row ids @$ key . from index @$ key . to index @$ row id ) >= __num__ ) { return true ; } },owid found first
rather than fight it @$ let root have an <PLACE_HOLDER>,nodes . put ( __str__ @$ root ) ; nodes . put ( root zookeeper @$ root ) ; root . add child ( proc child zookeeper ) ; nodes . put ( proc zookeeper @$ proc data node ) ; proc data node . add child ( quota child zookeeper ) ; nodes . put ( quota zookeeper @$ quota data node ) ;,root have effect
compressed streams do not update <PLACE_HOLDER> until after sync marker @$ so we can hit eof here .,try { close ( ) ; } catch ( final exception e2 ) { log . warn ( e . get message ( ) @$ e2 ) ; } return false ;,streams update counts
means get all <PLACE_HOLDER> @$ so no need to add filter,if ( db name != null && ! __str__ . equals ( db name ) ) { append simple condition ( filter builder @$ __str__ @$ new string [ ] { db name } @$ parameter vals ) ; },means get results
the client must not send the <PLACE_HOLDER> .,assert false ( trailers latch . await ( __num__ @$ time unit . seconds ) ) ;,client send product
comparing with itself returns <PLACE_HOLDER>,assert true ( comp . compare ( p1 @$ p1 ) == __num__ ) ;,itself returns whitespace
if the flushed requests has <PLACE_HOLDER> @$ we should propagate it also and fail the checkpoint,check and propagate async error ( ) ;,requests has errors
functions that are n't ctors or interfaces have no instance <PLACE_HOLDER> .,return null ;,functions have fields
3 regions 3 replicas write some <PLACE_HOLDER> to the table,table ht = test_util . get connection ( ) . get table ( table name ) ; list < put > puts = new array list < > ( ) ; byte [ ] qualifier = bytes . to bytes ( __str__ ) ; put put = new put ( new byte [ ] { ( byte ) __str__ } ) ; put . add column ( cf @$ qualifier @$ bytes . to bytes ( __str__ ) ) ; puts . add ( put ) ; put = new put ( new byte [ ] { ( byte ) __str__ } ) ; put . add column ( cf @$ qualifier @$ bytes . to bytes ( __str__ ) ) ; puts . add ( put ),regions write data
do init transactions calls sender.do once three <PLACE_HOLDER> @$ only two requests are sent @$ so we should only poll twice,verify ( client @$ times ( __num__ ) ) . poll ( eq ( retry_backoff_ms ) @$ any long ( ) ) ;,transactions calls times
the user might cancel the file <PLACE_HOLDER> dialog in this case we should not close the test plan,if ( gui package . is dirty ( ) ) { return false ; },user cancel status
note : the above limiter could block and delay the caller 's <PLACE_HOLDER>,callback . on error ( t ) ; latch . count down ( ) ;,limiter block callback
seek with no offset epoch requires no validation no matter <PLACE_HOLDER> the current leader is,state . seek unvalidated ( tp0 @$ new subscription state . fetch position ( __num__ @$ optional . of ( __num__ ) @$ new metadata . leader and epoch ( broker1 @$ optional . of ( __num__ ) ) ) ) ; assert false ( state . has valid position ( tp0 ) ) ; assert true ( state . awaiting validation ( tp0 ) ) ; state . seek unvalidated ( tp0 @$ new subscription state . fetch position ( __num__ @$ optional . empty ( ) @$ new metadata . leader and epoch ( broker1 @$ optional . of ( __num__ ) ) ) ) ; assert true ( state . has valid position ( tp0 ) ) ; assert false ( state . awaiting validation,leader is what
this rename never overwrites the <PLACE_HOLDER> so files deleted and collected are irrelevant .,return create rename result ( fsd @$ renameiip @$ false @$ null ) ;,rename overwrites remote
the empty class has a default <PLACE_HOLDER> .,assert that ( class info . get ( ) . declared members ( ) ) . has size ( __num__ ) ; assert that ( class info . get ( ) . declared members ( ) . iterator ( ) . next ( ) . member name ( ) ) . is equal to ( __str__ ) ;,class has constructor
if text field equals only one <PLACE_HOLDER> @$ use that <PLACE_HOLDER> .,equate row object match = get match from table ( equate from filter ) ; if ( match != null ) { return match ; } return null ;,field equals alias
measure the component with undefined width spec @$ as the contents of the hscroll have unlimited horizontal <PLACE_HOLDER> .,child component tree . set root and size spec ( content props @$ size spec . make size spec ( __num__ @$ unspecified ) @$ height spec @$ content size ) ; content props . measure ( context @$ size spec . make size spec ( __num__ @$ unspecified ) @$ height spec @$ content size ) ; measured width = content size . width ; measured height = content size . height ; measured component width . set ( measured width ) ; measured component height . set ( measured height ) ;,contents have width
when errors are not stored alongside values @$ <PLACE_HOLDER> errors that are recovered from do not make the parent <PLACE_HOLDER>,if ( supports transient exceptions ) { assert that error info ( error info ) . is transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is not null ( ) ; } else { assert that error info ( error info ) . is not transient ( ) ; assert that error info ( error info ) . has exception that ( ) . is null ( ) ; },errors make exception
comment must immeiately precede the parameter <PLACE_HOLDER>,parm entry . comment ( token . comment ) ; param attribute ( parm entry ) ; parm entry . type ( param type spec ( entry ) ) ; parm entry . name ( token . name ) ; match ( token . identifier ) ; if ( isnt in list ( entry . parameters ( ) @$ parm entry . name ( ) ) ) entry . add parameter ( parm entry ) ;,comment precede attribute
when generating transform <PLACE_HOLDER> @$ the authority of the output path will be determined based on this hostname configuration .,server configuration . set ( property key . master_hostname @$ __str__ ) ; list < transform plan > plans = m catalog . get transform plan ( db name @$ table name @$ transform_definition ) ; map < string @$ layout > transformed layouts = maps . new hash map with expected size ( plans . size ( ) ) ; plans . for each ( plan -> transformed layouts . put ( plan . get base layout ( ) . get spec ( ) @$ plan . get transformed layout ( ) ) ) ; m catalog . complete transform table ( noop journal context . instance @$ db name @$ table name @$ transform_definition . get definition ( ) @$ transformed layouts ) ; table .,generating transform plan
reduce ipc client connection retry <PLACE_HOLDER> and interval time,configuration client conf = new configuration ( false ) ; client conf . set int ( common configuration keys . ipc_client_connect_max_retries_key @$ __num__ ) ; client conf . set int ( common configuration keys . ipc_client_connect_retry_interval_key @$ __num__ ) ;,connection retry max
ok now we are sure to have both a valid post and show gutenberg flag @$ let 's start the editing session <PLACE_HOLDER>,create post editor analytics session tracker ( m show gutenberg editor @$ m edit post repository . get post ( ) @$ m site @$ m is new post ) ;,'s start tracker
a function parameter can not be replaced with a direct inlined value if it is referred to by an inner function . the inner function can out live the call we are replacing @$ so inner function must capture a unique <PLACE_HOLDER> . this approach does not work within loop bodies so those are forbidden elsewhere .,if ( n . is function ( ) ) { in inner function = true ; },function capture name
case 2 : on api 23 @$ we double scheduler workers because <PLACE_HOLDER> scheduler prefers batching . so is the work is periodic @$ we only need to execute it once per interval . also potential bugs in the platform may cause a <PLACE_HOLDER> to run more than once .,if ( m work spec . is periodic ( ) || m work spec . is backed off ( ) ) { long now = system . current time millis ( ) ; boolean is first run = m work spec . period start time == __num__ ; if ( ! is first run && now < m work spec . calculate next run time ( ) ) { logger . get ( ) . debug ( tag @$ string . format ( __str__ + __str__ @$ m work spec . worker class name ) ) ; resolve ( true ) ; return ; } },bugs cause work
this region does not overlap the <PLACE_HOLDER> @$ so skip it .,if ( ! range . overlaps ( byte key range . of ( last end key @$ response end key ) ) ) { last offset = response offset ; last end key = response end key ; continue ; },region overlap key
reuse test generic writable inner classes to test <PLACE_HOLDER> that also implement configurable .,foo generic writable generic = new foo generic writable ( ) ; generic . set conf ( conf ) ; baz baz = new baz ( ) ; generic . set ( baz ) ; baz result = serialization test util . test serialization ( conf @$ baz ) ; assert equals ( baz @$ result ) ; assert not null ( result . get conf ( ) ) ;,reuse test methods
after emitting each pane @$ it will continue accumulating the elements so that each approximation includes <PLACE_HOLDER> of the previous data in addition to the newly arrived data .,p collection < table row > speculative results = flow info . apply ( __str__ @$ window . < kv < string @$ integer > > into ( fixed windows . of ( duration . standard minutes ( window duration ) ) ) . triggering ( repeatedly . forever ( after processing time . past first element in pane ( ) . plus delay of ( one_minute ) ) ) . accumulating fired panes ( ) . with allowed lateness ( one_day ) ) . apply ( new total flow ( __str__ ) ) ;,approximation includes most
there is an existing user who only has unsupported <PLACE_HOLDER> in methods,if ( task . is successful ( ) && last signed in providers . is empty ( ) && ! methods . is empty ( ) ) { return tasks . for exception ( new firebase ui exception ( error codes . developer_error ) ) ; },who has signed
agree on the consensus if this node does not see the <PLACE_HOLDER> at all or if this node sees the <PLACE_HOLDER> however the timeout is out,if ( found client == null || is timeout exceeded ( found client @$ watchdog client retry timeout ) ) { client disconnected consensus = true ; },node sees client
first copy the children as the call to copy.add will modify the <PLACE_HOLDER> we 're iterating on,enumeration < ? > enum from = node . children ( ) ; list < j meter tree node > tmp = new array list < > ( ) ; while ( enum from . has more elements ( ) ) { j meter tree node child = ( j meter tree node ) enum from . next element ( ) ; tmp . add ( child ) ; } for ( j meter tree node j meter tree node : tmp ) { copy . add ( j meter tree node ) ; } tree model . insert node into ( copy @$ target @$ index ++ ) ; nodes for removal . add ( node ) ; paths to select [ path position ++ ] =,call modify order
now @$ both the close action and a grab for an write page <PLACE_HOLDER> is waiting for our first thread . when we release that <PLACE_HOLDER> @$ we should see that either close completes and our second thread @$ the one b<PLACE_HOLDER>ed on the write <PLACE_HOLDER> @$ gets an exception @$ or we should see that the second thread gets the <PLACE_HOLDER> @$ and,unlock latch . count down ( ) ;,thread gets lock
now do get <PLACE_HOLDER> on created mapfile .,map file . reader reader = new map file . reader ( qualified dir name @$ conf ) ; try { assert equals ( null @$ reader . mid key ( ) ) ; } finally { reader . close ( ) ; },now get closest
update processor config : note that we want to run a generation update even if the processor config has no <PLACE_HOLDER> ; we still need to handle <PLACE_HOLDER> in the exported tables,update processor config ( connectors ) ; if ( ! requires new export generation ) { if ( m_generation . get ( ) != null ) { m_generation . get ( ) . update generation id ( catalog context . m_gen id ) ; } export log . info ( __str__ ) ; return ; },config has exchanges
as it turns out @$ starting a new media player on the completion of a previous player ends up slightly overlapping the two <PLACE_HOLDER> @$ so slightly delaying the start of the next player gives a better user experience,if ( m next player != null ) { system clock . sleep ( __num__ ) ; m next player . start ( ) ; },player ends ones
make cdi work in containers with implicit archive scanning <PLACE_HOLDER>,write optional ( output folder @$ new supporting file ( __str__ @$ __str__ @$ __str__ ) ) ;,containers scanning mode
fallback if file not a valid <PLACE_HOLDER> .,changes . put ( file @$ file ) ;,fallback file tex
the file should contain just 1 <PLACE_HOLDER> :,assert equals ( __num__ @$ straus . get len ( ) ) ; har file system . close ( ) ; local fs . delete ( tmp path @$ true ) ;,file contain char
check if the sequence at the current position matches locale <PLACE_HOLDER> .,for ( int i = __num__ ; i < __num__ ; i ++ ) { int digit str len = locale digits [ i ] . length ( ) ; if ( str . region matches ( start @$ locale digits [ i ] @$ __num__ @$ digit str len ) ) { dec val [ __num__ ] = i ; return digit str len ; } },sequence matches digits
inflater likes a <PLACE_HOLDER> of slack,long size = get entry size ( jzentry ) + __num__ ;,inflater likes depth
user should not see anything so give unsatisfiable <PLACE_HOLDER>,return process payload builder . process definitions ( ) . with process definition key ( __str__ + uuid . randomuuid ( ) . to string ( ) ) . build ( ) ;,anything give condition
check for region destroyed <PLACE_HOLDER> from server .,client1 . invoke ( new cache serializable runnable ( __str__ ) { @ override public void run2 ( ) throws cache exception { region local region = get cache ( ) . get region ( __str__ + regions [ __num__ ] ) ; if ( local region != null ) { wait . pause ( __num__ * __num__ ) ; cq query [ ] cqs = get cache ( ) . get query service ( ) . get cqs ( ) ; if ( cqs != null && cqs . length > __num__ ) { assert true ( cqs [ __num__ ] . is closed ( ) ) ; } assert null ( __str__ + __str__ @$ get cache ( ) . get region ( __str__ + regions,check destroyed event
some legacy mode devices do not support <PLACE_HOLDER> off,if ( is hardware level limited or better ( ) ) { check true for key ( key @$ __str__ @$ modes list . contains ( camera metadata . control_af_mode_off ) ) ; },devices support control
no cluster in cfg : get the default <PLACE_HOLDER>,if ( ! clusters . contains field ( i cluster name ) ) cfg = clusters . field ( all_wildcard ) ; else cfg = clusters . field ( i cluster name ) ;,cluster get field
for job props @$ deletion in local folder means <PLACE_HOLDER> from ancestor folder and reschedule .,load new common config and handle new job ( path @$ job scheduler . action . reschedule ) ; return ;,deletion means copy
spellcheck dictionary contains <PLACE_HOLDER> and general accentuation,assert equals ( __num__ @$ rule . match ( lang tool . get analyzed sentence ( __str__ ) ) . length ) ;,dictionary contains subword
see if the alias has a lateral <PLACE_HOLDER> . if so @$ chain the lateral <PLACE_HOLDER> operator on,list < ast node > lateral views = alias to lateral views . get ( alias ) ; if ( lateral views != null ) { operator op = e . get value ( ) ; for ( ast node lateral view tree : alias to lateral views . get ( alias ) ) { op = gen lateral view plan ( qb @$ op @$ lateral view tree ) ; } e . set value ( op ) ; },alias has view
remove any provided <PLACE_HOLDER> that do not exist in this segment .,for ( string column name : immutable list . copy of ( column names ) ) { if ( index . get column holder ( column name ) == null ) { column names . remove ( column name ) ; } },any provided columns
propagate tracing headers @$ so remote service can use current <PLACE_HOLDER> as its parent,map < string @$ list < string > > tracing headers = tracing headers ( tracer @$ current span ) ; map < string @$ list < string > > outbound headers = tracer provider . map ( provider -> provider . update outbound headers ( current span @$ tracer @$ parent span . or else ( null ) @$ tracing headers @$ inbound headers ) ) . or else ( tracing headers ) ;,service use span
since <PLACE_HOLDER> are always running on a single host keep the resolution timeout low as otherwise people running with strange network configurations will see very slow <PLACE_HOLDER>,conf . put ( property key . network_host_resolution_timeout_ms @$ __str__ ) ;,people see tests
hint arrow has no <PLACE_HOLDER> @$ and always returns the current <PLACE_HOLDER>,if ( is in agility arena ( ) ) { world point new ticket position = client . get hint arrow point ( ) ; world point old tick position = last arena ticket position ; last arena ticket position = new ticket position ; if ( old tick position != null && new ticket position != null && ( old tick position . getx ( ) != new ticket position . getx ( ) || old tick position . gety ( ) != new ticket position . gety ( ) ) ) { log . debug ( __str__ @$ old tick position @$ new ticket position ) ; if ( config . notify agility arena ( ) ) { notifier . notify ( __str__ ) ; },arrow has position
okay @$ we have the <PLACE_HOLDER> . now have the agent do the restore .,stage . close ( ) ; m backup data = parcel file descriptor . open ( m backup data name @$ parcel file descriptor . mode_read_only ) ; m new state = parcel file descriptor . open ( m new state name @$ parcel file descriptor . mode_read_write | parcel file descriptor . mode_create | parcel file descriptor . mode_truncate ) ;,now have restore
same dead node <PLACE_HOLDER> of the time .,int max try = config . get node repetitions ( ) + __num__ ; if ( max try < __num__ ) { max try = __num__ ; } return new ketama iterator ( k @$ max try @$ get ketama nodes ( ) @$ hash alg ) ;,dead node time
make a copy of the graph to avoid concurrency problems . graph manipulations are not thread safe @$ and another thread can concurrently inline this <PLACE_HOLDER> .,final structured graph graph = ( structured graph ) method . compilation info . get graph ( ) . copy ( debug ) ; try ( debug context . scope s = debug . scope ( __str__ @$ graph @$ method @$ this ) ) { try { try ( indent in = debug . log and indent ( __str__ @$ method ) ) { boolean inlined = false ; for ( invoke invoke : graph . get invokes ( ) ) { if ( invoke instanceof invoke node ) { throw vm error . should not reach here ( __str__ + invoke . call target ( ) . target method ( ) . format ( __str__ ) + __str__ + ( graph . method ( ) ==,thread inline method
first heartbeat which schedules first <PLACE_HOLDER> .,localizer heartbeat response response = spy service . heartbeat ( stat ) ; assert equals ( __str__ @$ localizer action . live @$ response . get localizer action ( ) ) ;,which schedules resource
reset the flow scope 's syntactic scope to the function block @$ rather than the function node <PLACE_HOLDER> . this allows pulling out local vars from the function by name to verify their types .,if ( cfg root . is function ( ) ) { return scope = rtn state . get in ( ) . with syntactic scope ( scope creator . create scope ( cfg root . get last child ( ) ) ) ; } else { return scope = rtn state . get in ( ) ; },function node itself
the write <PLACE_HOLDER> to use in the states .,write options write options = null ; linked hash map < string @$ rocksdb keyed state backend . rocks db kv state info > kv state information = new linked hash map < > ( ) ; rocksdb db = null ; abstract rocksdb restore operation restore operation = null ; rocks db ttl compact filters manager ttl compact filters manager = new rocks db ttl compact filters manager ( enable ttl compaction filter @$ ttl time provider ) ; resource guard rocksdb resource guard = new resource guard ( ) ; snapshot strategy < k > snapshot strategy ; priority queue set factory priority queue factory ; rocksdb serialized composite key builder < k > shared rocks key builder ;,the write options
instance klass array is sorted by name . do binary <PLACE_HOLDER>,int low = __num__ ; int high = tmp klasses . length - __num__ ; int mid = - __num__ ; while ( low <= high ) { mid = ( low + high ) > > __num__ ; instance klass mid val = tmp klasses [ mid ] ; int cmp = mid val . get name ( ) . as string ( ) . compare to ( class name ) ; if ( cmp < __num__ ) { low = mid + __num__ ; } else if ( cmp > __num__ ) { high = mid - __num__ ; } else { return tmp klasses [ mid ] ; } },klass do search
currently @$ this will always be null because the manifest does n't have any useful <PLACE_HOLDER>,if ( jersey version == null ) jersey version = __str__ ;,manifest have information
stop commit <PLACE_HOLDER> .,return true ;,stop commit job
style rules should always have the lowest <PLACE_HOLDER> .,return - __num__ ;,rules have priority
note : for plugins that depend on other plugin artifacts the plugin realm contains more than one plugin <PLACE_HOLDER> . however @$ only the first <PLACE_HOLDER> is of interest .,if ( ! first descriptor ) { return ; } first descriptor = false ; if ( ! plugin artifact . get group id ( ) . equals ( plugin descriptor . get group id ( ) ) ) { errors . add ( __str__ + plugin descriptor . get group id ( ) ) ; } if ( ! plugin artifact . get artifact id ( ) . equals ( plugin descriptor . get artifact id ( ) ) ) { errors . add ( __str__ + plugin descriptor . get artifact id ( ) ) ; } if ( ! plugin artifact . get base version ( ) . equals ( plugin descriptor . get version ( ) ) ) { errors . add ( __str__,realm contains descriptor
this is a direct call to a method that the static analysis did not see as invoked . this can happen when the receiver is always null . in most cases @$ the method profile also has a length of 0 and the below code to kill the invoke would trigger . but not all methods have <PLACE_HOLDER> @$ for example methods with manually,if ( call target . invoke kind ( ) . is direct ( ) && ! ( ( hosted method ) call target . target method ( ) ) . get wrapped ( ) . is simply implementation invoked ( ) ) { unreachable invoke ( graph @$ invoke @$ call target ) ; continue ; },methods have exceptions
synchronously end the animation @$ jumping to the end state . animator set has synchronous listener <PLACE_HOLDER> on all supported ap is .,if ( ! animated ) { current animation . end ( ) ; },set has listeners
null implies a 'thunk ' <PLACE_HOLDER>,return instruction != null ;,null implies instruction
make sure link properties represents the latest private dns status . this does not need to be done before update dnses because the link properties are not the source of the private dns <PLACE_HOLDER> . update dnses will fetch the private dns <PLACE_HOLDER> from dns manager .,m dns manager . update private dns status ( net id @$ new lp ) ; if ( is default network ( network agent ) ) { handle apply default proxy ( new lp . get http proxy ( ) ) ; } else { update proxy ( new lp @$ old lp ) ; } update wake on lan ( new lp ) ;,dnses fetch state
did not find a child to receive the event . assign the pointer to the least recently added <PLACE_HOLDER> .,if ( new touch target == null && m first touch target != null ) { new touch target = m first touch target ; while ( new touch target . next != null ) { new touch target = new touch target . next ; } new touch target . pointer id bits |= id bits to assign ; },pointer added child
now do a <PLACE_HOLDER> @$ but cancel it in the middle,vm0 . invoke ( ( ) -> { count down latch rebalancing cancelled = new count down latch ( __num__ ) ; count down latch rebalancing finished = new count down latch ( __num__ ) ; internal resource manager manager = get cache ( ) . get internal resource manager ( ) ; internal resource manager . set resource observer ( new resource observer adapter ( ) { @ override public void rebalancing or recovery started ( region region ) { try { rebalancing cancelled . await ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; } } @ override public void rebalancing or recovery finished ( region region ) { rebalancing finished . count,now do rebalancing
this function releases one sync <PLACE_HOLDER> only .,return __num__ ;,function releases lock
existing write <PLACE_HOLDER> against the caller 's val<PLACE_HOLDER> list .,if ( part == null ) { return null ; } if ( mt == null ) { throw new invalid object exception ( __str__ ) ; },existing write item
note that we are always delivering a small <PLACE_HOLDER> to the transport here which may incur transport framing overhead as it may be sent separately to the contents of the grpc frame . the final <PLACE_HOLDER> may not be completely written because we do not flush the last buffer . do not report the last <PLACE_HOLDER> as sent .,sink . deliver frame ( writeable header @$ false @$ false @$ messages buffered - __num__ ) ; messages buffered = __num__ ;,note report header
end enabled <PLACE_HOLDER>,break ;,end enabled check
counting source.unbounded has very good splitting <PLACE_HOLDER>,assert that ( initial inputs @$ has size ( num splits ) ) ; int read per split = __num__ ; int total size = num splits * read per split ; set < long > expected outputs = contiguous set . create ( range . closed open ( __num__ @$ ( long ) total size ) @$ discrete domain . longs ( ) ) ; collection < long > read items = new array list < > ( total size ) ; for ( committed bundle < ? > initial input : initial inputs ) { committed bundle < unbounded source shard < long @$ ? > > shard bundle = ( committed bundle < unbounded source shard < long @$ ? > > ) initial input,source.unbounded has input
this method always throws activation <PLACE_HOLDER>,user service . activate user ( __str__ ) ;,method throws exception
verify that the vertices are all in the same slot sharing <PLACE_HOLDER>,slot sharing group group1 ; slot sharing group group2 ;,vertices sharing group
cache the label field @$ so that if it contains inlined <PLACE_HOLDER> then it 's free . otherwise cache the dynamic <PLACE_HOLDER> in another data structure and point into it .,long label field = node cursor . get label field ( ) ; boolean has inlined labels = ! node labels field . field points to dynamic record of labels ( node cursor . get label field ( ) ) ; if ( labels == null ) { has inlined labels = true ; label field = no_labels_field . long value ( ) ; },cache cache labels
if its completed no further <PLACE_HOLDER>,if ( stage steps collection completed ) { return ; },its completed steps
call resource monitor <PLACE_HOLDER> so that it does all validations .,new health monitor ( deployment . get systemsettings ( ) @$ new dummy snmp trap sender ( ) ) ;,resource monitor instance
0 xc 1 a 5 f <PLACE_HOLDER> e @$ 0 x 40358874 @$ 0 x 64 d 4 ef 0 d @$ 0 xc 0089309,masm . subsd ( xmm5 @$ xmm2 ) ; masm . andl ( rdx @$ __num__ ) ; masm . shrl ( rdx @$ __num__ ) ; masm . movdqu ( xmm0 @$ new amd64 address ( r11 @$ rdx @$ amd64 address . scale . times1 @$ - __num__ ) ) ; masm . movdqu ( xmm4 @$ record external address ( crb @$ coeff16 ) ) ;,5 f fb
client will always see 'initiating ' <PLACE_HOLDER> first .,if ( request . get options ( ) . get run async ( ) ) { return new backup status ( backup id @$ backup state . initiating ) ; },client see state
check if its instance of <PLACE_HOLDER> @$ in that the case throw <PLACE_HOLDER> exists exception .,if ( ind instanceof index ) { if ( remotely originated ) { return ( index ) ind ; } throw new index name conflict exception ( string . format ( __str__ @$ index name ) ) ; } future task < index > old index future task = ( future task < index > ) ind ; index index = null ; boolean interrupted = false ; try { if ( old index future task == null ) { index future task . run ( ) ; index = index future task . get ( ) ; if ( index != null ) { this . indexes . put ( index task @$ index ) ; partitioned index pr index = ( partitioned index ) index ;,case throw index
stream 1 receives another <PLACE_HOLDER>,buffer = create message frame ( fake message ) ; frame handler ( ) . data ( false @$ __num__ @$ buffer @$ message frame length ) ; verify ( frame writer @$ timeout ( time_out_ms ) ) . window update ( eq ( __num__ ) @$ eq ( ( long ) __num__ * message frame length ) ) ;,stream receives message
the anonymous click listener since that changes the model state @$ instead our anonymous click listener should use the hash <PLACE_HOLDER> of the user 's click listener,model click listener model click listener = new model click listener ( ) ; view click listener view click listener = new view click listener ( ) ; test controller controller = new test controller ( ) ; adapter data observer observer mock = mock ( adapter data observer . class ) ; controller . get adapter ( ) . register adapter data observer ( observer mock ) ; model with click listener_ model = new model with click listener_ ( ) ; controller . set model ( model ) ; controller . request model build ( ) ; verify ( observer mock ) . on item range inserted ( eq ( __num__ ) @$ eq ( __num__ ) ) ; model = new model with click listener_,listener use code
transaction log <PLACE_HOLDER>,log provider . raw message matcher ( ) . assert contains ( __str__ ) ; log provider . raw message matcher ( ) . assert contains ( __str__ ) ; management service . shutdown ( ) ;,transaction log provider
do an update @$ which will pick up the large <PLACE_HOLDER>,region . put ( __num__ @$ __num__ ) ;,which pick bucket
for tabs above the focused tab move <PLACE_HOLDER> up to 0 .,if ( i < focus index ) { add animation ( set @$ tab @$ scroll_offset @$ tab . get scroll offset ( ) @$ tab . get scroll offset ( ) - m height - spacing @$ tab_focused_animation_duration @$ __num__ ) ; } else if ( i > focus index ) { float covering tab position = layout tab . gety ( ) ; float distance to border = math utils . clamp ( m height - covering tab position @$ __num__ @$ m height ) ; float delay = tab_focused_max_delay * distance to border / m height ; add animation ( set @$ tab @$ y_in_stack_offset @$ tab . gety in stack offset ( ) @$ tab . gety in stack offset ( ) + m,tabs move position
verify that server 1 's event queue has the default <PLACE_HOLDER>,server1 . invoke ( ( ) -> { internal cache cache = cluster startup rule . get cache ( ) ; async event queue queue = cache . get async event queue ( __str__ ) ; assert that ( queue . get batch size ( ) ) . is equal to ( gateway sender . default_batch_size ) ; assert that ( queue . get batch time interval ( ) ) . is equal to ( default_batch_time_interval ) ; assert that ( queue . get maximum queue memory ( ) ) . is equal to ( gateway sender . default_maximum_queue_memory ) ; } ) ; gfsh . execute and assert that ( __str__ + altered_batch_size + __str__ + altered_batch_time_interval + __str__ + altered_maximum_queue_memory ) . status is success (,queue has value
add <PLACE_HOLDER>@$<PLACE_HOLDER> should returns <PLACE_HOLDER>@$<PLACE_HOLDER>,try { inc ( usage @$ suffix @$ resource . new instance ( __num__ @$ __num__ ) @$ label ) ; check ( __num__ @$ __num__ @$ get ( usage @$ suffix @$ label ) ) ; } catch ( no such method exception e ) { },1 returns 1
creation lock <PLACE_HOLDER>,post destroy action ( ) ;,creation lock action
each interpreter group has one <PLACE_HOLDER>,for ( interpreter group interpreter group : interpreter setting . get all interpreter groups ( ) ) { assert equals ( __num__ @$ interpreter group . get session num ( ) ) ; },group has session
do the carry out <PLACE_HOLDER>,instructions . add ( reil helpers . create and ( base offset ++ @$ byte size @$ __str__ @$ byte size @$ is zero condition seven @$ byte size @$ shifter carry out tmp1 ) ) ; instructions . add ( reil helpers . create bsh ( base offset ++ @$ d word size @$ register node value1 @$ d word size @$ minus thirty one set @$ byte size @$ tmp var5 ) ) ; instructions . add ( reil helpers . create and ( base offset ++ @$ byte size @$ tmp var5 @$ byte size @$ is zero condition four @$ byte size @$ shifter carry out tmp2 ) ) ; instructions . add ( reil helpers . create sub ( base offset ++ @$,the carry type
rehash the table if the <PLACE_HOLDER> of entries would exceed the <PLACE_HOLDER> of buckets .,if ( f num >= f table size ) { rehash ( ) ; bucket = hash % f table size ; } else if ( collision count >= max_hash_collisions && key instanceof string ) { rebalance ( ) ; bucket = hash ( key ) % f table size ; },number exceed number
after terminate suppressed <PLACE_HOLDER> which itself suppressed original err,assert equals ( __num__ @$ after terminate . get suppressed ( ) . length ) ; assert equals ( error @$ after terminate . get suppressed ( ) [ __num__ ] ) ; assert equals ( __num__ @$ error . get suppressed ( ) . length ) ; assert equals ( err @$ error . get suppressed ( ) [ __num__ ] ) ;,itself suppressed error
most likely client browser closed <PLACE_HOLDER>,if ( t instanceof socket exception ) { get logger ( ) . info ( __str__ + __str__ ) ; return ; },browser closed connection
add modules files as inputs . instead they rely on input discovery to recognize the needed ones . however @$ orphan <PLACE_HOLDER> runs before input discovery and thus module files would be discarded as orphans . this is strictly better than marking all transitive modules as inputs @$ which would also effectively disable orphan <PLACE_HOLDER> for .pcm files .,if ( output file . is file type ( cpp file types . cpp_module ) ) { return immutable set . of ( output file ) ; } return super . get mandatory outputs ( ) ;,which disable sampling
let system take <PLACE_HOLDER> for date change events .,final work source work source = null ;,system take effect
add all spanning <PLACE_HOLDER> that need to be inserted after this one .,add spanning cells ( ) ;,all spanning cells
a list which will hold the <PLACE_HOLDER> for the column families once the db is opened,list < column family handle > columns = new array list < > ( ) ; m db = rocksdb . open ( m db opts @$ m db path @$ cf descriptors @$ columns ) ; m checkpoint = checkpoint . create ( m db ) ; for ( int i = __num__ ; i < columns . size ( ) - __num__ ; i ++ ) { m column handles . get ( i ) . set ( columns . get ( i + __num__ ) ) ; },which hold keys
for whatever reason this activity is being launched into a new task ... yet the caller has requested a result <PLACE_HOLDER> . well @$ that is pretty messed up @$ so instead immediately send <PLACE_HOLDER> a cancel and let the new task continue launched as normal without a dependency on its originator .,if ( source stack != null && ( m launch flags & flag_activity_new_task ) != __num__ ) { slog . w ( tag @$ __str__ ) ; source stack . send activity result locked ( - __num__ @$ m start activity . result to @$ m start activity . result who @$ m start activity . request code @$ result_canceled @$ null ) ; m start activity . result to = null ; },caller requested lock
null pkg indicates user <PLACE_HOLDER>,cancel all notifications by list locked ( m notification list @$ calling uid @$ calling pid @$ pkg @$ true @$ channel id @$ flag checker @$ false @$ user id @$ false @$ reason @$ listener name @$ true ) ;,pkg indicates switch
the file name should contain an extension <PLACE_HOLDER> .,if ( file name . last index of ( extension_separator ) == - __num__ ) { return false ; },name contain separator
pod does not have any <PLACE_HOLDER> that references pvc,if ( volume to che volume name . is empty ( ) ) { continue ; },pod have region
someone asked build <PLACE_HOLDER> @$ let stop the build before trying to run another build step,if ( executor != null && executor . is interrupted ( ) ) { throw new interrupted exception ( ) ; },someone build interruption
we check that access control exceptions contain absolute <PLACE_HOLDER> .,path parent = p . get parent ( ) ; assert true ( parent . is uri path absolute ( ) ) ; assert true ( e . get message ( ) . contains ( parent . to string ( ) ) ) ; return false ;,exceptions contain paths
client & server have the same <PLACE_HOLDER>,if ( server methods == null ) { return true ; },client have method
because users do n't have a backup of media @$ it 's safer to import new data and rely on them running a media db <PLACE_HOLDER> to get rid of any unwanted media . in the future we might also want to duplicate this step import media,hash map < string @$ string > name to num = new hash map < > ( ) ; hash map < string @$ string > num to name = new hash map < > ( ) ; file media map file = new file ( dir . get absolute path ( ) @$ __str__ ) ; if ( media map file . exists ( ) ) { json reader jr = new json reader ( new file reader ( media map file ) ) ; jr . begin object ( ) ; string name ; string num ; while ( jr . has next ( ) ) { num = jr . next name ( ) ; name = jr . next string ( ) ; name,them running initialization
new settings for set install location <PLACE_HOLDER> no longer initiated here .,if ( upgrade version == __num__ ) { upgrade version = __num__ ; },settings install action
given : thread which awaited <PLACE_HOLDER> .,segment aware aware = new segment aware ( __num__ @$ false ) ; aware . check can read archive or reserve work segment ( __num__ ) ; try { aware . release work segment ( __num__ ) ; } catch ( assertion error e ) { return ; } fail ( __str__ ) ;,which awaited segment
coalesce can handle this more <PLACE_HOLDER>,list < string > nick names = entity manager . create query ( __str__ + __str__ @$ string . class ) . get result list ( ) ;,coalesce handle case
tree related <PLACE_HOLDER> .,menu = new j menu ( __str__ ) ; menu bar . add ( menu ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new add action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new insert action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action listener ( new reload action ( ) ) ; menu item = menu . add ( new j menu item ( __str__ ) ) ; menu item . add action,tree related items
normal operation blocks the site <PLACE_HOLDER> on the sitetasker queue .,while ( m_should continue ) { m_txn state = null ; site tasker task = m_scheduler . take ( ) ; task . run ( get site procedure connection ( ) ) ; },operation blocks thread
this thread is being killed @$ do not print <PLACE_HOLDER>,if ( t instanceof thread death ) { } else { system . err . println ( __str__ + thread . current thread ( ) . get name ( ) + __str__ + t . get class ( ) . get canonical name ( ) ) ; try { t . print stack trace ( ) ; } catch ( throwable ignored ) { } },thread print history
all <PLACE_HOLDER> input columns are repeating . generate <PLACE_HOLDER> once . lookup once . since the <PLACE_HOLDER> is repeated @$ we must use entry 0 regardless of selected in use .,if ( all key input columns repeating ) { key vector serialize write . set output ( current key output ) ; key vector serialize write . serialize write ( batch @$ __num__ ) ; join util . join result join result ; if ( key vector serialize write . get has any nulls ( ) ) { join result = join util . join result . nomatch ; } else { byte [ ] key bytes = current key output . get data ( ) ; int key length = current key output . get length ( ) ; join result = hash multi set . contains ( key bytes @$ __num__ @$ key length @$ hash multi set results [ __num__ ] ) ; } if,input generate key
no good way to resolve ambiguous <PLACE_HOLDER> at transition @$ but following code work in most case .,tz . get offset ( local millis @$ true @$ offsets ) ; if ( tztype == time type . standard && offsets [ __num__ ] != __num__ || tztype == time type . daylight && offsets [ __num__ ] == __num__ ) { tz . get offset ( local millis - ( __num__ * __num__ * __num__ * __num__ ) @$ true @$ offsets ) ; },way resolve offsets
set a small time unit as cookie max age so that the server sends a <PLACE_HOLDER>,hive conf . set time var ( conf vars . hive_server2_thrift_http_cookie_max_age @$ __num__ @$ time unit . seconds ) ; hive conf . set bool var ( conf vars . hive_support_concurrency @$ false ) ; minihs2 = mini hive kdc . get minihs2 with kerb ( mini hive kdc @$ hive conf ) ; minihs2 . start ( new hash map < string @$ string > ( ) ) ;,server sends 401
if vertex low <PLACE_HOLDER> is same as visited <PLACE_HOLDER> then this is start vertex for strongly connected component . keep popping vertices out of stack still you find current vertex . they are all part of one strongly connected component .,if ( visited time . get ( vertex ) == low time . get ( vertex ) ) { set < vertex < integer > > strongly connected componenet = new hash set < > ( ) ; vertex v ; do { v = stack . poll first ( ) ; on stack . remove ( v ) ; strongly connected componenet . add ( v ) ; } while ( ! vertex . equals ( v ) ) ; result . add ( strongly connected componenet ) ; },then visited time
if the evaluate yields true then pass all <PLACE_HOLDER> else pass 0 <PLACE_HOLDER>,if ( pred instanceof expr node generic func desc ) { expr node generic func desc gen func = ( expr node generic func desc ) pred ; for ( expr node desc leaf : gen func . get children ( ) ) { if ( leaf instanceof expr node generic func desc ) { long new num rows = __num__ ; for ( expr node desc child : gen func . get children ( ) ) { new num rows = evaluate child expr ( stats @$ child @$ asp ctx @$ needed cols @$ op @$ num rows ) ; } return num rows - new num rows ; } else if ( leaf instanceof expr node constant desc ) { expr node constant desc encd,true pass rows
no need to call create all <PLACE_HOLDER> @$ since future drawables will change layout direction when they are prepared .,final int count = m num children ; final drawable [ ] drawables = m drawables ; for ( int i = __num__ ; i < count ; i ++ ) { if ( drawables [ i ] != null ) { boolean child changed = false ; if ( android . os . build . version . sdk_int >= android . os . build . version_codes . m ) { child changed = drawables [ i ] . set layout direction ( layout direction ) ; } if ( i == current index ) { changed = child changed ; } } } m layout direction = layout direction ; return changed ;,need create children
let 's write an invalid <PLACE_HOLDER>,handshake ( client @$ sock -> { buffer buff = buffer . buffer ( ) ; buff . append byte ( ( byte ) ( __num__ ) ) . append byte ( ( byte ) __num__ ) ; sock . write ( buff ) ; } ) ;,'s write frame
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; path output = paths . get ( __str__ ) ; immutable set < source path > inputs before = immutable set . of ( ) ; dep file build rule rule = new dep file build rule ( target @$ filesystem @$ params ) { @ add to rule key private final source path path = path source path . of ( filesystem @$ input file ) ; @ override public immutable list < step > get build steps ( build context context @$ buildable context buildable context ) { return immutable list . of ( new write file step,which writes file
some variables do n't have <PLACE_HOLDER> @$ but use the name of their datatype,if ( datatype != null ) { return datatype . get name ( ) ; } string signature = get signature ( true ) ; string fixed = symbol utilities . replace invalid chars ( signature @$ true ) ; return fixed ;,variables have names
recalculate wait <PLACE_HOLDER> .,woke = ( woke == - __num__ ) ? system . current time millis ( ) : woke ; wait time = this . period - ( woke - start time ) ;,recalculate wait time
and compare the <PLACE_HOLDER> ...,try { assert source record match ( actual record @$ expected record @$ ignorable fields :: contains @$ comparators by field name @$ comparators by schema name ) ; } catch ( assertion error e ) { result . error ( ) ; string msg = __str__ + schema util . as string ( actual record . key ( ) ) + __str__ + e . get message ( ) ; testing . debug ( msg ) ; throw new mismatch record exception ( e @$ msg @$ actual record history @$ expected record history ) ; },and compare fields
our internal version does not have this <PLACE_HOLDER>,if ( ! analysis mock . is this bazel ( ) ) { return ; },version have field
no class found <PLACE_HOLDER> to the system property passed in from bazel,if ( suite == null ) { if ( args . length == __num__ && suite class name != null ) { system . err . printf ( __str__ @$ suite class name ) ; return __num__ ; } },class found fallback
sp plans which have an index which can provide the window <PLACE_HOLDER> ordering do n't create an order by node .,if ( ! ( child instanceof order by plan node ) ) { return plan ; } order by plan node onode = ( order by plan node ) child ; child = onode . get child ( __num__ ) ;,which provide column
make sure the developer is following the <PLACE_HOLDER> .,if ( ( node . get text ( ) == null ) && ( node . get content description ( ) == null ) ) { throw new runtime exception ( __str__ + __str__ ) ; } node . get bounds in parent ( temp parent rect ) ; if ( temp parent rect . equals ( invalid_parent_bounds ) ) { throw new runtime exception ( __str__ + __str__ ) ; } final int actions = node . get actions ( ) ; if ( ( actions & accessibility node info . action_accessibility_focus ) != __num__ ) { throw new runtime exception ( __str__ + __str__ ) ; } if ( ( actions & accessibility node info . action_clear_accessibility_focus ) != __num__ ) { throw new runtime exception,developer following rules
most subclasses will need <PLACE_HOLDER> :,sampler sampler = jmctx . get current sampler ( ) ; bindings . put ( __str__ @$ sampler ) ;,subclasses need these
process named query i <PLACE_HOLDER>,list < string > named query ids = list named queries result . get named query ids ( ) ;,process named ds
the plugin <PLACE_HOLDER> has been changed while a configuration network call is going on @$ we need to dispatch another configure plugin action since we do n't allow multiple configure actions to happen at the same time this might happen either because user changed the <PLACE_HOLDER> or a remove plugin action has started,if ( is plugin state changed since last configuration dispatch ( ) ) { dispatch configure plugin action ( false ) ; } else if ( m is removing plugin && ! m plugin . is active ( ) ) { dispatch remove plugin action ( ) ; m is active = m plugin . is active ( ) ; m switch active . set checked ( m is active ) ; },user changed state
test once using the current correct hash function @$ expect no mispartitioned <PLACE_HOLDER>,client response cr = client . call procedure ( __str__ @$ ( object ) null ) ; volt table hashinator matches = cr . get results ( ) [ __num__ ] ; hashinator matches . advance row ( ) ; while ( hashinator matches . advance row ( ) ) { assert equals ( __num__ @$ hashinator matches . get long ( __str__ ) ) ; } volt table validate result = cr . get results ( ) [ __num__ ] ;,test expect rows
if the current component contains an <PLACE_HOLDER> @$ run that one,if ( component . has ( __str__ ) ) { json object action = component . getjson object ( __str__ ) ; ( ( jason view activity ) context ) . call ( action . to string ( ) @$ new json object ( ) . to string ( ) @$ __str__ @$ v . get context ( ) ) ; } else { view cursor = v ; while ( cursor . get parent ( ) != null ) { json object item = ( json object ) ( ( ( view ) cursor . get parent ( ) ) . get tag ( ) ) ; if ( item != null && ( item . has ( __str__ ) || item . has ( __str__ ),component contains activity
let the local destroy or processing of transfer do the <PLACE_HOLDER>,return ;,destroy do work
the new area tiles contains a <PLACE_HOLDER>,if ( ( collision data flags [ x ] [ checky ] & y wall flags east ) != __num__ ) { return false ; },tiles contains wall
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; source path input = path source path . of ( filesystem @$ filesystem . get root path ( ) . get file system ( ) . get path ( __str__ ) ) ; filesystem . touch ( path resolver . get relative path ( input ) ) ; path output = build target paths . get gen path ( filesystem @$ target @$ __str__ ) ; dep file build rule rule = new dep file build rule ( target @$ filesystem @$ params ) { @ add to rule key private final source path path = input ; @ override,which writes file
lets take a look at the operator memory <PLACE_HOLDER> .,dispatcher disp = null ; final set < map join operator > map joins = new linked hash set < map join operator > ( ) ; linked hash map < rule @$ node processor > rules = new linked hash map < rule @$ node processor > ( ) ; rules . put ( new rule reg exp ( __str__ @$ map join operator . get operator name ( ) + __str__ ) @$ new node processor ( ) { @ override public object process ( node nd @$ stack < node > stack @$ node processor ctx proc ctx @$ object ... node outputs ) { map joins . add ( ( map join operator ) nd ) ; return null ; } } ) ;,lets take state
if the reparent <PLACE_HOLDER> token contains previous display 's last focus <PLACE_HOLDER> @$ means it will end up to gain <PLACE_HOLDER> focus on the target display @$ so it should not be notified that it lost focus from the previous display .,if ( token . has child ( prev dc . m last focus ) ) { prev dc . m last focus = null ; },token contains child
run the counter job @$ outputs to a single reduce <PLACE_HOLDER> and file,log . info ( __str__ ) ; try { boolean success = counter . wait for completion ( true ) ; if ( ! success ) { string message = __str__ + counter . get status ( ) . get state ( ) + __str__ + counter . get status ( ) . get failure info ( ) ; log . error ( message ) ; throw new runtime exception ( message ) ; } } catch ( io exception | interrupted exception | class not found exception e ) { log . error ( __str__ @$ e ) ; throw e ; } log . info ( __str__ ) ;,job reduce task
if no restrictions were saved @$ dpm method should return an empty <PLACE_HOLDER> as per java doc .,return bundle != null ? new bundle ( bundle ) : new bundle ( ) ;,method return branch
get the start index ; the assertions below will fail if the assignment logic does not meet correct <PLACE_HOLDER>,int num consumers = kafka topic partition assigner . assign ( mock get all partitions for topics return . get ( __num__ ) @$ num subtasks ) ; for ( int subtask index = __num__ ; subtask index < mock get all partitions for topics return . size ( ) ; subtask index ++ ) { test partition discoverer partition discoverer = new test partition discoverer ( topics descriptor @$ subtask index @$ mock get all partitions for topics return . size ( ) @$ test partition discoverer . create mock get all topics sequence from fixed return ( collections . singleton list ( test_topic ) ) @$ test partition discoverer . create mock get all partitions from topics sequence from fixed return ( mock get all partitions,logic meet contracts
change the mob compaction merge <PLACE_HOLDER>,conf . set long ( mob constants . mob_compaction_mergeable_threshold @$ merge size ) ; common policy test logic ( __str__ @$ mob compact partition policy . weekly @$ false @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ true ) ;,compaction merge size
otherwise we saw a transaction spend our <PLACE_HOLDER> @$ but we did n't try and spend them ourselves yet . the outputs are already marked as spent by the connect call above @$ so check if there are any more for us to use . move if not .,if ( result == transaction input . connection result . success ) { transaction connected = check not null ( input . get connected transaction ( ) ) ; log . info ( __str__ @$ input . get outpoint ( ) @$ tx . get tx id ( ) ) ; maybe move pool ( connected @$ __str__ ) ; if ( output . is mine or watched ( this ) ) { check state ( my unspents . remove ( output ) ) ; } },transaction spend coins
end if found better end for left <PLACE_HOLDER> end for left state do right restricted <PLACE_HOLDER>,for ( int right state = __num__ ; right state < num states ; right state ++ ) { int narrowl = narrowl extent_end [ right state ] ; if ( narrowl <= start ) { continue ; } binary rule [ ] right rules = bg . split rules withrc ( right state ) ; for ( binary rule rule : right rules ) { int left child = rule . left child ; int narrowr = narrowr extent_start [ left child ] ; if ( narrowr > narrowl ) { continue ; } int min2 = widel extent_end [ right state ] ; int min = ( narrowr > min2 ? narrowr : min2 ) ; int max1 = wider extent_start [ left child ] ;,end do rules
check that obj literal have only <PLACE_HOLDER> that are present in the message text,for ( string ph name : ph names ) { if ( ! used placeholders . contains ( ph name ) ) { throw new malformed exception ( __str__ + ph name @$ node ) ; } },literal have marks
current node already performs <PLACE_HOLDER> .,return ;,node performs cleanup
if the proxy directly proxy the <PLACE_HOLDER> or extends it @$ return true,if ( iface . is assignable from ( get class ( ) ) ) { return true ; } else if ( iface . is assignable from ( delegate . get class ( ) ) ) { return true ; } else if ( wrapper . class . is assignable from ( delegate . get class ( ) ) ) { return ( ( wrapper ) unwrapp6 spy proxy ( ) ) . is wrapper for ( iface ) ; },proxy proxy interface
instance info in response from peer will trigger local registry <PLACE_HOLDER>,create peer eureka node ( ) . heartbeat ( instance info . get app name ( ) @$ instance info . get id ( ) @$ instance info @$ null @$ false ) ; expect request type ( request type . batch ) ;,info trigger heartbeat
key gets <PLACE_HOLDER> here,byte [ ] bytes = sortk1 . to byte array ( ) ; do assert ( bytes . length == __num__ && bytes [ __num__ ] == __num__ && bytes [ __num__ ] == __num__ && bytes [ __num__ ] == __num__ @$ __str__ ) ;,key gets priority
initialize map local <PLACE_HOLDER>,local work = mrwork . get map red local work ( ) ; exec context . set local work ( local work ) ; mapred context . init ( true @$ new job conf ( jc ) ) ; mo . pass exec context ( exec context ) ; mo . initialize local work ( jc ) ; mo . initialize map operator ( jc ) ; if ( local work == null ) { return ; },initialize map work
check whether surface flinger spontaneously changed <PLACE_HOLDER> out from under us . schedule traversals to ensure that the correct state is reapplied if necessary .,if ( m active mode id != __num__ && m active mode id != active record . m mode . get mode id ( ) ) { m active mode invalid = true ; send traversal request locked ( ) ; } boolean records changed = records . size ( ) != m supported modes . size ( ) || modes added ;,flinger changed properties
if this path has a root <PLACE_HOLDER> the given path 's root must match,if ( ! this . root . equals ignore case ( other . root ) ) { return false ; },path has property
arbitrary and not worth documenting @$ as activity manager will kill this <PLACE_HOLDER> shortly anyway .,return __num__ ;,manager kill process
force a 100 continue <PLACE_HOLDER> .,jetty request . get http channel ( ) . send response ( http generator . continue_100_info @$ null @$ false ) ;,100 continue response
create a web hdfs file system <PLACE_HOLDER>,uri uri = new uri ( __str__ + string . value of ( port ) ) ; configuration conf = new configuration ( ) ; web hdfs file system webfs = ( web hdfs file system ) file system . get ( uri @$ conf ) ; content summary summary = webfs . get content summary ( new path ( __str__ ) ) ; verify content summary ( sym link summary for dir contains fromdfs @$ summary ) ;,hdfs file instance
the class in which create <PLACE_HOLDER> is implemented,if ( connector bundle . is connected component connector ( type ) ) { j class type create widget class = connector bundle . find inherited method ( type @$ __str__ ) . get enclosing type ( ) ; j method get widget = connector bundle . find inherited method ( type @$ __str__ ) ; j class type widget type = get widget . get return type ( ) . is class ( ) ; if ( create widget class . get qualified source name ( ) . equals ( abstract component connector . class . get canonical name ( ) ) ) { if ( get widget . get enclosing type ( ) . get qualified source name ( ) . equals ( abstract component connector,which create widget
if the user set a thread stack <PLACE_HOLDER> at thread creation @$ then use that .,if ( stack size != __num__ ) { chosen stack size = stack size ; } else { final int default thread stack size = ( int ) x options . get xss ( ) . get value ( ) ; if ( default thread stack size != __num__ ) { chosen stack size = default thread stack size ; } },thread stack size
for dependencies that are missing we canonicalize and remap the target so we do n't suggest private build <PLACE_HOLDER> .,set < jar owner > canonicalized missing = missing targets . stream ( ) . filter ( owner -> owner . label ( ) . is present ( ) ) . sorted ( comparator . comparing ( ( jar owner owner ) -> owner . label ( ) . get ( ) ) ) . map ( owner -> owner . with label ( owner . label ( ) . map ( label -> canonicalize target ( label ) ) ) ) . collect ( to immutable set ( ) ) ;,private build results
dw suggested buffer size specifies how large a buffer should be used to read this stream . typically @$ this contains a value corresponding to the largest chunk present in the stream . using the correct buffer size makes playback more efficient . use <PLACE_HOLDER> if you do not know the correct buffer size .,d . write int ( tr . quality ) ;,present use 0
call matrix changed <PLACE_HOLDER> if needed,if ( m matrix change listener != null ) { rectf display rect = get display rect ( matrix ) ; if ( display rect != null ) { m matrix change listener . on matrix changed ( display rect ) ; } },matrix changed listener
the second line has a ' <PLACE_HOLDER> ' @$ so it needs more ascent and descent .,if ( m enabled ) { assert equals ( - __num__ * em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; } else { assert equals ( - em @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * em @$ layout . get line descent ( __num__ ) ) ; },line has b
current time should trigger <PLACE_HOLDER> of older active file,rotate . maybe rotate ( current time ) ;,time trigger rotation
mock the second split read stream <PLACE_HOLDER> .,when ( fake storage client . split read stream ( split read stream request . new builder ( ) . set original stream ( streams . get ( __num__ ) ) . set fraction ( __num__ ) . build ( ) ) ) . then return ( split read stream response . new builder ( ) . set primary stream ( streams . get ( __num__ ) ) . set remainder stream ( stream . new builder ( ) . set name ( __str__ ) ) . build ( ) ) ;,split read call
create the module provider ; this class provides a tile <PLACE_HOLDER> that actually loads the tile from the map file .,module provider = new maps forge tile module provider ( simple register receiver @$ from files @$ tile writer ) ;,class provides writer
instant apps need <PLACE_HOLDER> to create foreground services .,if ( r . app info . is instant app ( ) ) { final int mode = m am . m app ops service . check operation ( app ops manager . op_instant_app_start_foreground @$ r . app info . uid @$ r . app info . package name ) ; switch ( mode ) { case app ops manager . mode_allowed : break ; case app ops manager . mode_ignored : slog . w ( tag @$ __str__ + r . app info . package name + __str__ + __str__ ) ; return ; case app ops manager . mode_errored : throw new security exception ( __str__ + r . app info . package name + __str__ ) ; default : m am . enforce permission (,apps need ability
clear the underlying finished <PLACE_HOLDER> .,clear finished bits ( entry . get value ( ) ) ;,the finished bits
set create wal <PLACE_HOLDER> to true and use default values for other options .,util . start mini cluster ( start mini cluster option . builder ( ) . createwal dir ( true ) . build ( ) ) ;,set create dir
fake ftp server has timestamp <PLACE_HOLDER> in minutes . specify milliseconds <PLACE_HOLDER> so that test does not need to wait for minutes .,runner . set property ( list file . target_system_timestamp_precision @$ list file . precision_millis ) ; runner . assert valid ( ) ;,server has precision
super call writes the connection <PLACE_HOLDER> we need to flush to send it this is called only on the client,ctx . flush ( ) ;,call writes context
heartbeat has no <PLACE_HOLDER>,if ( message type != protocol constants . msgtype_heartbeat_request && message type != protocol constants . msgtype_heartbeat_response ) { codec codec = codec factory . get codec ( rpc message . get codec ( ) ) ; body bytes = codec . encode ( rpc message . get body ( ) ) ; compressor compressor = compressor factory . get compressor ( rpc message . get compressor ( ) ) ; body bytes = compressor . compress ( body bytes ) ; full length += body bytes . length ; },heartbeat has support
if the user specified a custom view for the tab indicators @$ then do not draw the bottom <PLACE_HOLDER> .,if ( ! m draw bottom strips ) { return ; },then draw strips
gets set once @$ so doc and descendants have first <PLACE_HOLDER>,assert equals ( __str__ @$ doc . base uri ( ) ) ;,doc have author
concurrent hash map does not need <PLACE_HOLDER> . here,for ( abstract thread group thread group : groups ) { thread group . wait threads stopped ( ) ; },map need synch
we accept property with targets type to be compatible with old jdks see <PLACE_HOLDER>,if ( targets getter . is executed ( ) && ! targets getter . is disposed ( ) && ( targets getter . get actual type ( ) == x atom . xa_atom || targets getter . get actual type ( ) == x data transferer . targets_atom . get atom ( ) ) && targets getter . get actual format ( ) == __num__ ) { int count = targets getter . get number of items ( ) ; if ( count > __num__ ) { long atoms = targets getter . get data ( ) ; formats = new long [ count ] ; for ( int index = __num__ ; index < count ; index ++ ) { formats [ index ] = native .,jdks see 4168833
if the first argument is const then just set the <PLACE_HOLDER> and continue,if ( col name == null ) { is const = true ; prev const = ( ( expr node constant desc ) leaf ) . get value ( ) ; continue ; },argument set value
chrome has been killed @$ reconstruct a download <PLACE_HOLDER> .,if ( m download info == null ) { m download info = new download info . builder ( ) . set file name ( title ) . set description ( c . get string ( c . get column index ( download manager . column_description ) ) ) . set mime type ( c . get string ( c . get column index ( download manager . column_media_type ) ) ) . set content length ( long . parse long ( c . get string ( c . get column index ( download manager . column_total_size_bytes ) ) ) ) . build ( ) ; },chrome reconstruct info
last parameter to copy from referenced method @$ exclude final var <PLACE_HOLDER>,int last = local context . needs var args conversion ( ) ? impl size - __num__ : impl size ;,parameter exclude args
reset the stream @$ but set a lower limit . writing beyond the limit should throw an <PLACE_HOLDER>,stream . reset ( size - __num__ ) ; assert true ( __str__ @$ ( stream . get limit ( ) == size - __num__ ) ) ; caught exception = false ; try { stream . write ( input @$ __num__ @$ size ) ; } catch ( exception e ) { caught exception = true ; } assert true ( __str__ @$ caught exception ) ;,limit throw exception
other errors need a full <PLACE_HOLDER> .,log . error ( __str__ @$ service != null ? ( service . to string ( ) + __str__ + service . get service state ( ) ) : __str__ @$ thrown ) ; exit exception = convert to exit exception ( thrown ) ;,errors need logger
selection triggers <PLACE_HOLDER>,change selection to navigate ( table @$ __num__ @$ __num__ ) ; run swing ( ( ) -> get providers ( ) [ __num__ ] . close component ( ) ) ; assert equals ( addr ( __str__ ) @$ cb . get current address ( ) ) ;,selection triggers completion
wait until the <PLACE_HOLDER> used by res 1 is returned to the pool @$ so that the next request reuses the <PLACE_HOLDER> .,while ( ! connection returned to pool ) { condition . await ( ) ; } lock . unlock ( ) ;,request reuses connection
mimic a task failure ; setting up the task for cleanup simulates the abort protocol to be played . without checks in the framework @$ this will fail as the <PLACE_HOLDER>ter will cause a <PLACE_HOLDER> to happen for the cleanup task .,task . set task cleanup task ( ) ; my umbilical umbilical = new my umbilical ( ) ; task . run ( job @$ umbilical ) ; assert true ( __str__ @$ umbilical . task done ) ;,committer cause same
if only implicit defaults exist @$ have a null default <PLACE_HOLDER> default primitives . this makes it so if there is a nullable object and a primitive in a group @$ the default value will be to null out the object .,if ( default attribute == null || has explicit default ( attribute ) || attribute . has set nullability ( ) ) { default attribute = attribute ; },primitives have default
at this point @$ the configuration 's size needs to be taken into account as not all configurations have all <PLACE_HOLDER> .,int screen layout = __num__ ; int ui mode = __num__ ; int smallest screen width dp = __num__ ; int screen width dp = __num__ ; int screen height dp = __num__ ; byte [ ] locale script = new byte [ __num__ ] ; byte [ ] locale variant = new byte [ __num__ ] ; byte screen layout2 = __num__ ; byte screen config pad1 = __num__ ; short screen config pad2 = __num__ ; if ( size >= screen_config_min_size ) { screen layout = unsigned bytes . to int ( buffer . get ( ) ) ; ui mode = unsigned bytes . to int ( buffer . get ( ) ) ; smallest screen width dp = buffer . get short ( ),configurations have properties
view enters low padding <PLACE_HOLDER> :,if ( view min < padding min ) { first view = view ; if ( m focus scroll strategy == base grid view . focus_scroll_page ) { while ( prepend one column visible items ( ) ) { circular int array positions = m grid . get item positions in rows ( m grid . get first visible index ( ) @$ pos ) [ row ] ; first view = find view by position ( positions . get ( __num__ ) ) ; if ( view max - get view min ( first view ) > client size ) { if ( positions . size ( ) > __num__ ) { first view = find view by position ( positions . get ( __num__ ) ),view enters range
check that resource exhaustion triggers an <PLACE_HOLDER>,try { m ip sec service . reserve net id ( ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { },exhaustion triggers exception
completing the human task a should mark the <PLACE_HOLDER> as completable,cmmn task service . complete ( taska . get id ( ) ) ; plan item instance stage1 plan item instance = cmmn runtime service . create plan item instance query ( ) . plan item instance name ( __str__ ) . single result ( ) ; assert that ( stage1 plan item instance . is completable ( ) ) . is true ( ) ; assert that ( cmmn runtime service . create plan item instance query ( ) . stage instance id ( stage1 plan item instance . get id ( ) ) . list ( ) ) . is not empty ( ) ;,a mark stage
if this is a parent pr @$ create the bucket @$ possibly going over redundancy . we need to do this so that we can create the child region in this member . this member may have the latest <PLACE_HOLDER> for the child region .,if ( has persistent child region ( ) ) { result = partitioned region . get data store ( ) . grab bucket ( bid @$ get distribution manager ( ) . get distribution manager id ( ) @$ true @$ true @$ false @$ null @$ true ) ; } else { if ( this . partitioned region . is shadowpr ( ) && this . partitioned region . get colocated with ( ) != null ) { partitioned region colocated region = colocation helper . get colocated region ( this . partitioned region ) ; if ( this . partitioned region . get data policy ( ) . with persistence ( ) && ! objects . require non null ( colocated region ) . get data,member have snapshot
currently there is no way to stop the meta store service . it will be stopped when the test jvm <PLACE_HOLDER> . this is how other tests are also using meta store server .,hive server2 . stop ( ) ; set started ( false ) ; try { if ( llap cluster != null ) { llap cluster . stop ( ) ; } if ( mr != null ) { mr . shutdown ( ) ; mr = null ; } if ( dfs != null ) { dfs . shutdown ( ) ; dfs = null ; } } catch ( io exception e ) { },test jvm exits
some will have <PLACE_HOLDER> more than everything else .,int num high = ( int ) ( total - ( math . floor ( mean ) * count ) ) ; int num low = ( int ) ( count - num high ) ; min = ( num high * ( math . ceil ( mean ) - mean ) ) + ( num low * ( mean - math . floor ( mean ) ) ) ;,some have 0
make sure our set text call did not trigger the <PLACE_HOLDER> to be created,matching window = text field . get active matching window ( ) ; assert null ( __str__ @$ matching window ) ;,call trigger window
build up the state index . the bg & ug both expect a set <PLACE_HOLDER> of states .,build state index ( ) ; build grammars ( ) ;,bg expect number
check the looking up <PLACE_HOLDER> having only one memory segment,check argument ( key . get segments ( ) . length == __num__ ) ; final int hash code1 = key . hash code ( ) ; int new pos = hash code1 & num buckets mask ;,the looking table
if the layer does n't have a drawable or unresolved theme <PLACE_HOLDER> for a drawable @$ attempt to parse one from the child element . if multiple child elements exist @$ we 'll only use the first one .,if ( layer . m drawable == null && ( layer . m theme attrs == null ) ) { while ( ( type = parser . next ( ) ) == xml pull parser . text ) { } if ( type != xml pull parser . start_tag ) { throw new xml pull parser exception ( parser . get position description ( ) + __str__ + __str__ ) ; } layer . m drawable = drawable . create from xml inner for density ( r @$ parser @$ attrs @$ m layer state . m src density override @$ theme ) ; layer . m drawable . set callback ( this ) ; state . m children changing configurations |= layer . m drawable . get,layer have attribute
to be . note however that in the context of this function @$ the block can claim to be as difficult as it wants to be ... . if somebody was able to take control of our network connection and fork us onto a different chain @$ they could send us valid blocks with ridiculously easy difficulty and this function would accept <PLACE_HOLDER> .,big integer target = get difficulty target as integer ( ) ; big integer h = get hash ( ) . to big integer ( ) ; if ( h . compare to ( target ) > __num__ ) { if ( throw exception ) throw new verification exception ( __str__ + get hash as string ( ) + __str__ + target . to string ( __num__ ) ) ; else return false ; },function accept them
if this is a dead notification @$ then ask the <PLACE_HOLDER> if it 's really failed,if ( status event . get state ( ) == z wave node state . dead ) { z controller . request is failed node ( node . get node id ( ) ) ; },then ask node
previous diff should add <PLACE_HOLDER> to the view,invoke later ( prev diff ) ; view set . add range ( addr ( __str__ ) @$ addr ( __str__ ) ) ; assert equals ( addr ( __str__ ) @$ cb . get current address ( ) ) ; assert equals ( addr ( __str__ ) @$ get diff address ( ) ) ; assert equals ( cb . get current selection ( ) @$ new program selection ( addr ( __str__ ) @$ addr ( __str__ ) ) ) ; assert equals ( view set @$ cb . get view ( ) ) ;,diff add 
wer expect a successful <PLACE_HOLDER>,c . connect blocking ( ) ; c . close blocking ( ) ; http headers . put ( __str__ @$ __str__ ) ; c = new example client ( new uri ( __str__ ) @$ http headers ) ;,wer expect success
bind include <PLACE_HOLDER>,c = new method closure ( this @$ __str__ ) ; super . set variable ( __str__ @$ c ) ;,bind include variable
var has constant value @$ return a <PLACE_HOLDER> .,if ( var . get constant value ( ) != null ) { return tree util . new literal ( var . get constant value ( ) @$ type util ) ; },var return value
generating cpu quota before the query finishes . this assertion verifies cpu <PLACE_HOLDER> during quota generation .,root . generate cpu quota ( __num__ ) ; stream . of ( root @$ child ) . for each ( group -> assert within cpu limit ( group @$ __num__ ) ) ;,assertion verifies usage
enable table @$ use retain <PLACE_HOLDER> to assign regions .,admin . enable table ( table name ) . join ( ) ; list < h region location > regions2 = async meta table accessor . get tableh region locations ( meta table @$ table name ) . get ( ) ;,use retain assignment
loaded key must equal stored <PLACE_HOLDER>,byte [ ] bytesa2 = store . key util ( ) . identity key to bytes ( keya2 ) ; assert true ( __str__ @$ arrays . equals ( bytesa1 @$ bytesa2 ) ) ;,key equal data
iteration of inodes needs exclusive <PLACE_HOLDER>,begin write ( ) ;,iteration needs readlock
the service group had an unknown <PLACE_HOLDER> !,if ( candidate services == null ) { _log . error ( __str__ + colo cluster variant ) ; return exception_exit_code ; },group had cluster
if put if absent fails @$ opportunistically use its return <PLACE_HOLDER>,for ( ; ; ) { v new value = remapping function . apply ( key @$ old value ) ; if ( new value != null ) { if ( old value != null ) { if ( replace ( key @$ old value @$ new value ) ) return new value ; } else if ( ( old value = put if absent ( key @$ new value ) ) == null ) return new value ; else continue have old value ; } else if ( old value == null || remove ( key @$ old value ) ) { return null ; } continue retry ; },opportunistically use value
same stream @$ merge <PLACE_HOLDER> .,if ( stream . equals ( other start . stream ) ) { final map < partition id type @$ sequence offset type > new map = new hash map < > ( partition sequence number map ) ; new map . put all ( other start . partition sequence number map ) ; final set < partition id type > new exclusive partitions = new hash set < > ( ) ; partition sequence number map . for each ( ( partition id @$ sequence offset ) -> { if ( exclusive partitions . contains ( partition id ) && ! other start . partition sequence number map . contains key ( partition id ) ) { new exclusive partitions . add ( partition id ) ;,stream merge means
only the legacy token have <PLACE_HOLDER> on that capability,generate new token ( wc @$ __str__ @$ __str__ ) ; check combination with config and method for legacy token creation ( config @$ wc @$ user ) ;,token have permission
a serializable function can only return an object type @$ so if the do <PLACE_HOLDER> parameter is a primitive type @$ then box it for the return . the return type will be unboxed before being forwarded to the do <PLACE_HOLDER> parameter .,if ( output type . get raw type ( ) . is primitive ( ) ) { output type = type descriptor . of ( primitives . wrap ( output type . get raw type ( ) ) ) ; },the do output
if the length value is wrong or not all data made it to disk this read will not complete correctly . there could be overflow @$ underflow etc . so use a try finally block to indicate that all partitions are now corrupt . the enclosing exception handlers will do the right thing <PLACE_HOLDER> to propagating the error and closing the file .,boolean completed read = false ; int checksum start position = __num__ ; int row count = __num__ ; try { c . b ( ) . clear ( ) ; if ( is compressed ( ) ) { c . b ( ) . limit ( next chunk length + m_table header . capacity ( ) + __num__ ) ; } else { c . b ( ) . limit ( ( next chunk length - __num__ ) + m_table header . capacity ( ) ) ; } m_table header . position ( __num__ ) ; c . b ( ) . put ( m_table header ) ; c . b ( ) . position ( c . b ( ) . position ( ) + __num__,handlers do wrt
client endpoint reads eof and shutdown <PLACE_HOLDER> as result,assert equals ( - __num__ @$ client . get input stream ( ) . read ( ) ) ; client . shutdown input ( ) ;,endpoint reads input
seek to the end just before the last page of stream to get the <PLACE_HOLDER> .,long last page search position = end position - ogg page header . max_page_size ; if ( last page search position > position before seek to end ) { return last page search position ; },page get duration
open tab may throw an <PLACE_HOLDER> when the tab is not found .,outer . open tab ( __str__ ) ;,tab throw exception
add parameters from the configuration in the job trace the reason why the job configuration parameters @$ as seen in the jobconf file @$ are added first because the specialized <PLACE_HOLDER> obtained from rumen should override the job conf <PLACE_HOLDER> .,for ( map . entry < object @$ object > entry : job . get job properties ( ) . get value ( ) . entry set ( ) ) { job conf . set ( entry . get key ( ) . to string ( ) @$ entry . get value ( ) . to string ( ) ) ; },values override values
scroll down and verify that the old elements do n't have the <PLACE_HOLDER> any more,get grid element ( ) . get row ( __num__ ) ; assert false ( has css class ( row2 @$ __str__ ) ) ; assert false ( has css class ( cell4_2 @$ __str__ ) ) ;,elements have stylename
start definitions root <PLACE_HOLDER>,xtw . write start element ( element_definitions ) ; xtw . set default namespace ( cmmn_namespace ) ; xtw . write default namespace ( cmmn_namespace ) ; xtw . write namespace ( xsi_prefix @$ xsi_namespace ) ; xtw . write namespace ( flowable_extensions_prefix @$ flowable_extensions_namespace ) ; xtw . write namespace ( cmmndi_prefix @$ cmmndi_namespace ) ; xtw . write namespace ( omgdc_prefix @$ omgdc_namespace ) ; xtw . write namespace ( omgdi_prefix @$ omgdi_namespace ) ; for ( string prefix : model . get namespaces ( ) . key set ( ) ) { if ( ! default namespaces . contains ( prefix ) && string utils . is not empty ( prefix ) ) xtw . write namespace ( prefix @$ model . get namespaces (,definitions root element
in addition to changing the containing <PLACE_HOLDER> @$ inlining function declarations also changes the function name <PLACE_HOLDER> from the containing <PLACE_HOLDER> to the inner <PLACE_HOLDER> .,if ( is function declaration ) { compiler . report change to change scope ( value ) ; compiler . report change to enclosing scope ( value . get parent ( ) ) ; },declarations changes function
do n't create future with sequenced future manager . otherwise session would receive discontinued sequence number @$ and it would make future work item 'keeping call sequence when session execute <PLACE_HOLDER> ' impossible .,return session result . create future with result ( result_error_permission_denied ) ;,session execute commands
target <PLACE_HOLDER> may not match connected <PLACE_HOLDER>,dc . send ( bb @$ p . get socket address ( ) ) ;,address match address
l l e s t j <PLACE_HOLDER> 0000 m i,byte [ ] expected = { ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__,s t \
we do n't know <PLACE_HOLDER> the last varbit is @$ so we just hit the end @$ then set it for future iterations,log . debug ( __str__ @$ i ) ; num varbits = i ; break ;,varbit is what
running the 'reset expired ' logic should have no <PLACE_HOLDER> @$ the lock time is not yet passed,expired jobs = management service . execute command ( new find expired jobs cmd ( expired jobs pages size @$ job service configuration . get job entity manager ( ) ) ) ; assert equals ( __num__ @$ expired jobs . size ( ) ) ; assert job details ( true ) ;,logic have effect
id @$ input <PLACE_HOLDER> @$ transliterated <PLACE_HOLDER>,string data [ ] = { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ;,id transliterated string
avoid some images do not have a <PLACE_HOLDER> .,if ( size > __num__ ) { point . x = __num__ ; point . y = __num__ ; },images have point
unbuffered . will not play naughty <PLACE_HOLDER> with the file position .,data input stream metadata = new data input stream ( stream ) ;,unbuffered play statements
store the checksum for thw write <PLACE_HOLDER> so that on recovery we know if we have a consistent write <PLACE_HOLDER> on disk .,recovery file . write long ( checksum . get value ( ) ) ;,checksum write lock
the processor does n't taint the consumer <PLACE_HOLDER> which has already finished,span processor span = take span ( consumer spans ) ; assert that ( processor span . id ( ) ) . is not equal to ( consumer span . id ( ) ) ;,processor taint span
the parsed string has a relative <PLACE_HOLDER>,if ( ! relative contributors . is empty ( ) ) { array memory relative = array memory . create hashed ( relative_contributor_names . length ) ; array . ref of index ( __str__ ) . assign ( relative ) ; arrays . stream ( relative_contributor_names ) . for each ( name -> { long value = relative contributors . get or default ( name @$ __num__ ) ; relative . ref of index ( name ) . assign ( value ) ; } ) ; arrays . stream ( relative_bool_contributor_names ) . filter ( relative contributors :: contains key ) . for each ( name -> relative . ref of index ( name ) . assign ( memory . true ) ) ; } return array ;,string has permission
problem when shutdown is called on the cloud eureka client where the application info manager bean is requested but wont be allowed because we are shutting down . to avoid this we use the <PLACE_HOLDER> directly .,application info manager app manager ; if ( aop utils . is aop proxy ( manager ) ) { app manager = proxy utils . get target object ( manager ) ; } else { app manager = manager ; } cloud eureka client cloud eureka client = new cloud eureka client ( app manager @$ config @$ this . optional args @$ this . context ) ; cloud eureka client . register health check ( health check handler ) ; return cloud eureka client ;,problem use object
if prehash make same <PLACE_HOLDER> with sha 1 to check expected code .,if ( pre hash && ! proc name . equals ( __str__ ) ) { params . put ( __str__ @$ get hashed password forhttp var ( password @$ client auth scheme . hash_sha1 ) ) ; call proc overjson raw ( params @$ http port @$ expected code @$ session id ) ; },prehash make call
if this is a property on an object literal @$ always expect an <PLACE_HOLDER> somewhere,if ( name . get parent ( ) . is object literal ( ) ) { return true ; },somewhere expect error
read default write object <PLACE_HOLDER>,boolean called default write object = read boolean ( ) ; read object state . begin unmarshal custom value ( this @$ called default write object @$ ( current class desc . read object method != null ) ) ; if ( current class desc . has read object ( ) ) set state ( in_read_object_remote_not_custom_marshaled ) ;,default write indicator
try to connect client 2 with no credentials verify that the creation of region throws security <PLACE_HOLDER>,if ( gen . class code ( ) . equals ( credential generator . class code . ssl ) ) { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ noforce_authreq_exception ) ) ; client2 . invoke ( ( ) -> do puts ( __num__ @$ other_exception ) ) ; } else { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ authreq_exception ) ) ; },creation throws exception
if the user revoked the api <PLACE_HOLDER> @$ we can delete it,if ( revoked . is legacy ( ) ) { p . api token = null ; },user revoked token
volatile image we immediately punt in subclasses . if this poses a problem we 'll need a more sophisticated detection <PLACE_HOLDER> @$ or api .,this ( buffer_strategy_specified_off ) ;,image need mechanism
clear memory which corresponds to all code blocks no <PLACE_HOLDER> to remove context since this will happen when instruction is added,if ( overwrite ) { for ( address range range : instruction set . get address set ( ) ) { clear code units ( range . get min address ( ) @$ range . get max address ( ) @$ false @$ task monitor adapter . dummy_monitor ) ; } } else { check instruction set ( instruction set @$ skip delay slots ) ; },memory blocks need
collect all metadata @$ newer <PLACE_HOLDER> override older <PLACE_HOLDER>,if ( is newer ( res @$ val ) ) { meta = merge meta ( val . get meta data ( ) @$ meta ) ; res . set ( val ) ; } else { meta = merge meta ( meta @$ val . get meta data ( ) ) ; },values override values
after setting the playback speed @$ reset m is rewinding <PLACE_HOLDER> .,m is rewinding = false ;,m rewinding flag
check if all implementation methods already have a common <PLACE_HOLDER> assigned . each implementation method can have multiple <PLACE_HOLDER>s because of interfaces . we compute the intersection of the <PLACE_HOLDER> sets for all implementation methods .,set < integer > result slots = vtables slots . get ( method . implementations [ __num__ ] ) ; for ( hosted method impl : method . implementations ) { set < integer > impl slots = vtables slots . get ( impl ) ; if ( impl slots == null ) { result slots = null ; break ; } result slots . retain all ( impl slots ) ; } if ( result slots != null && ! result slots . is empty ( ) ) { int result slot = integer . max_value ; for ( int slot : result slots ) { result slot = math . min ( result slot @$ slot ) ; } return result slot ; },methods have slot
selection and tap can move <PLACE_HOLDER> from this tap position .,final float eventx = event . getx ( ) ; final float eventy = event . gety ( ) ; final boolean is mouse = event . is from source ( input device . source_mouse ) ; switch ( event . get action masked ( ) ) { case motion event . action_down : if ( extracted text mode will be started ( ) ) { hide ( ) ; } else { m min touch offset = m max touch offset = m text view . get offset for position ( eventx @$ eventy ) ; if ( m gesture stayed in tap region ) { if ( m tap state == tap_state_double_tap || m tap state == tap_state_triple_click ) { final float deltax = eventx -,selection move focus
make the consumer return a single <PLACE_HOLDER> for each partition,when ( consumer mock . poll ( any long ( ) ) ) . then return ( new consumer records < > ( collections . singleton map ( partition that will be revoked @$ spout with mocked consumer setup helper . create records ( partition that will be revoked @$ __num__ @$ __num__ ) ) ) ) . then return ( new consumer records < > ( collections . singleton map ( assigned partition @$ spout with mocked consumer setup helper . create records ( assigned partition @$ __num__ @$ __num__ ) ) ) ) . then return ( new consumer records < > ( collections . empty map ( ) ) ) ;,consumer return record
this conditional is for test acl submit <PLACE_HOLDER> where app is rejected and no app is added .,if ( scheduler . get scheduler applications ( ) . contains key ( id . get application id ( ) ) ) { scheduler . add application attempt ( id @$ false @$ false ) ; } list < resource request > ask = new array list < > ( requests ) ; rm app rm app = mock ( rm app . class ) ; rm app attempt rm app attempt = mock ( rm app attempt . class ) ; when ( rm app . get current app attempt ( ) ) . then return ( rm app attempt ) ; when ( rm app attempt . getrm app attempt metrics ( ) ) . then return ( new rm app attempt metrics ( id @$,acl submit application
test that it works when the <PLACE_HOLDER> timeout kills the outstanding <PLACE_HOLDER> ...,test shutdown request outstanding ( __num__ @$ __num__ @$ remote invocation exception . class @$ exception . class ) ;,timeout kills request
closing region with colocated regions will throw an <PLACE_HOLDER> and the region will not be closed .,accessor . invoke ( ( ) -> pr colocationd unit test . close region with colocated regions ( customer partitioned region name @$ false ) ) ;,region throw exception
this is to test that the background thread created by the adapter is retaining the stream <PLACE_HOLDER> properly . if it were not @$ the program would crash after this method exits and its outer autorelease pool drains .,assert not null ( stream ) ;,thread retaining reference
middle word gets integer <PLACE_HOLDER> ; lower longword is cleared .,if ( abs round power < two_x_longword_decimal_digits ) { final int adjusted abs power = abs round power - longword_decimal_digits ; final long round factor = power of ten table [ adjusted abs power ] ; result0 = __num__ ; result1 = ( ( fast1 / round factor ) * round factor ) ; result2 = fast2 ; } else { final int adjusted abs power = abs round power - two_x_longword_decimal_digits ; final long round factor = power of ten table [ adjusted abs power ] ; result0 = __num__ ; result1 = __num__ ; result2 = ( ( fast2 / round factor ) * round factor ) ; },word gets rounding
the user asked for stats to be collected . some stats like number of rows require a <PLACE_HOLDER> of the data however @$ some other stats @$ like number of files @$ do not require a complete <PLACE_HOLDER> update the stats which do not require a complete <PLACE_HOLDER> .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },which require scan
get the point which are near the cell covering the <PLACE_HOLDER> and outside of the <PLACE_HOLDER>,m_points for polygons . add ( geography point value . normalize lng lat ( center longitude + radius in degrees @$ center latitude + radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude - radius in degrees @$ center latitude + radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude - radius in degrees @$ center latitude - radius in degrees ) ) ; m_points for polygons . add ( geography point value . normalize lng lat ( center longitude + radius in degrees @$ center latitude - radius in degrees ) ) ;,which covering point
value that has been set is new or modified bring the binary in sync so that the deserialization gives the correct <PLACE_HOLDER>,if ( offset == modified_indicator_offset ) { update binary represenation ( ) ; offset = this . offsets [ field num ] ; },deserialization gives result
suppress all application not found <PLACE_HOLDER> for now .,continue ;,suppress found exceptions
if the qualifier expression contains <PLACE_HOLDER> @$ give up attribution of method reference,if ( expr type . is erroneous ( ) ) { result = that . type = expr type ; return ; },expression contains errors
total dirs includes root <PLACE_HOLDER>,assert equals ( dir count + __num__ @$ total dirs ) ; file status max file = collections . max ( written files . values ( ) @$ new comparator < file status > ( ) { @ override public int compare ( file status first @$ file status second ) { return first . get len ( ) < second . get len ( ) ? - __num__ : ( ( first . get len ( ) == second . get len ( ) ) ? __num__ : __num__ ) ; } } ) ; p = pattern . compile ( __str__ ) ; matcher = p . matcher ( output . to string ( __str__ ) ) ; assert true ( matcher . find ( ),dirs includes directory
sometimes safari does not render <PLACE_HOLDER> correctly when attaching . setting the visibility to hidden and a bit later restoring will make everything just fine .,if ( browser info . get ( ) . is safari ( ) ) { get element ( ) . get style ( ) . set visibility ( style . visibility . hidden ) ; scheduler . get ( ) . schedule finally ( ( ) -> { get element ( ) . get style ( ) . set visibility ( style . visibility . visible ) ; } ) ; },safari render checkbox
add <PLACE_HOLDER> & null objects add <PLACE_HOLDER> & null objects,bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection type ( ) . get element type ( ) ) ) ; bag1 . add ( wrap ( new integer ( __num__ ) @$ bag1 . get collection,integer add null
only unzip zip <PLACE_HOLDER> .,reader = new filtered data entry reader ( new data entry name filter ( new extension matcher ( __str__ ) ) @$ zip reader @$ reader ) ;,unzip zip entries
methods with 1 implementations do not need a <PLACE_HOLDER> because invokes can be done as direct calls without the need for a <PLACE_HOLDER> . methods with 0 implementations are unreachable .,if ( method . wrapped . is invoked ( ) || method . wrapped . is implementation invoked ( ) ) { if ( method . implementations . length > __num__ ) { int slot = find slot ( method @$ vtables map @$ used slots map @$ vtables slots ) ; method . vtable index = slot ; assign implementations ( method . get declaring class ( ) @$ method @$ slot @$ vtables map ) ; } },methods need slot
phreak must also remove the <PLACE_HOLDER> from the rule network evaluator,if ( match . get rule agenda item ( ) != null ) { if ( left tuple . get memory ( ) != null ) { left tuple . get memory ( ) . remove ( left tuple ) ; } },phreak remove task
if the request capacity does not require <PLACE_HOLDER> @$ just update the length of the memory .,if ( ! chunk . unpooled ) { if ( new capacity > length ) { if ( new capacity <= max length ) { length = new capacity ; return this ; } } else if ( new capacity > max length > > > __num__ && ( max length > __num__ || new capacity > max length - __num__ ) ) { length = new capacity ; trim indices to capacity ( new capacity ) ; return this ; } },capacity require cleanup
change the mob compaction merge <PLACE_HOLDER>,conf . set long ( mob constants . mob_compaction_mergeable_threshold @$ merge size ) ; common policy test logic ( __str__ @$ mob compact partition policy . monthly @$ false @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } @$ true ) ;,compaction merge size
note : another sanitize because camelize can create an invalid <PLACE_HOLDER>,return sanitize kotlin specific names ( modified ) ;,camelize create phrase
current logic overflows at 30 @$ so set <PLACE_HOLDER> to max,if ( count >= __num__ ) { return m max sleep ms ; } else { int sleep ms = m base sleep time ms * ( thread local random . current ( ) . next int ( __num__ << count @$ __num__ << ( count + __num__ ) ) ) ; return math . min ( abs ( sleep ms @$ m max sleep ms ) @$ m max sleep ms ) ; },logic overflows default
store it because need elements of it to print <PLACE_HOLDER> in output,stored header = line ;,elements print contents
should only clear this if the last schema had identity <PLACE_HOLDER> .,if ( ! f may match field map . is empty ( ) ) { f may match field map . clear ( ) ; },schema had fields
ensure the invariant that either all main accounts have a <PLACE_HOLDER> set @$ or none .,if ( account one password set != string utils . is not empty ( account two password ) || account one password set != string utils . is not empty ( account three password ) ) { throw new illegal argument exception ( ) ; },accounts have password
now the real test ! set up <PLACE_HOLDER> to be able to pick up correct tokens .,security util . set security info providers ( new custom security info ( ) ) ; token < client toam token identifier > token = converter utils . convert from yarn ( original client toam token @$ am . address ) ;,! set clients
host must have real keyboard <PLACE_HOLDER> .,if ( ! m host . is focused ( ) && ! m host . request focus ( ) ) { return false ; },host have focus
since this was sorted in reverse order @$ <PLACE_HOLDER> ' 0 ' contains the highest ai @$ and <PLACE_HOLDER> ' 1 ' tells us the <PLACE_HOLDER> of the corresponding sentence in its array .,ay eye ray [ sentence number ] = all anonymity indices . get ( sentence number ) [ quality rank ] [ __num__ ] ;,ai tells index
verify that new job requests have no <PLACE_HOLDER> .,job runnable = submit concurrent jobs ( __num__ @$ config @$ false @$ false @$ submit job helper . get delayed resonse answer ( __num__ @$ __num__ ) @$ kill job helper . get delayed resonse answer ( __num__ @$ status bean ) @$ __str__ ) ; assert true ( job runnable . exception == null ) ;,requests have issues
aggregates with <PLACE_HOLDER> aggregate functions cost a bit <PLACE_HOLDER>,float multiplier = __num__ + ( float ) agg calls . size ( ) * __num__ ; for ( aggregate call agg call : agg calls ) { if ( agg call . get aggregation ( ) . get name ( ) . equals ( __str__ ) ) { multiplier += __num__ ; } },aggregates cost some
make sure <PLACE_HOLDER> cache is filled so that initial allocations wo n't sl<PLACE_HOLDER>e high <PLACE_HOLDER> unnecessarily .,try ( indexed id generator freelist = new indexed id generator ( page cache @$ directory . file ( __str__ ) @$ immediate ( ) @$ id type . node @$ false @$ ( ) -> __num__ @$ long . max_value @$ false ) ) { freelist . maintenance ( ) ; race race = new race ( ) ; work sync < indexed id generator @$ ids > work sync = new work sync < > ( freelist ) ; for ( int t = __num__ ; t < threads ; t ++ ) { int thread = t ; race . add contestant ( throwing ( ( ) -> { int cursor = __num__ ; for ( int i = __num__ ; i < transactions_per_thread ;,allocations slide he
the system bars are visible . make any desired <PLACE_HOLDER> to your ui @$ such as showing the action bar or other navigational controls .,mainui . set immersive mode ( false ) ; set immersive timer ( ) ; if ( my debug . log ) log . d ( tag @$ __str__ ) ;,system make adjustments
create a second instance @$ which will pick up the same <PLACE_HOLDER>,try ( s3a file system second delegate = news3a instance ( uri @$ conf ) ) { assert bound todt ( second delegate @$ token kind ) ; if ( encryption test enabled ( ) ) { assert not null ( __str__ @$ second delegate . get server side encryption algorithm ( ) ) ; assert equals ( __str__ @$ fs . get server side encryption algorithm ( ) @$ second delegate . get server side encryption algorithm ( ) ) ; } contract test utils . assert deleted ( second delegate @$ test path @$ true ) ; assert not null ( __str__ @$ second delegate . get delegation token ( __str__ ) ) ; },which pick token
read field offset <PLACE_HOLDER>,access_flags_offset = db . lookup int constant ( __str__ ) . int value ( ) ; name_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; signature_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; initval_index_offset = db . lookup int constant ( __str__ ) . int value ( ) ; low_offset = db . lookup int constant ( __str__ ) . int value ( ) ; high_offset = db . lookup int constant ( __str__ ) . int value ( ) ; field_slots = db . lookup int constant ( __str__ ) . int value ( ) ; fieldinfo_tag_size = db . lookup int constant ( __str__ ) . short value ( ) ;,field offset slots
important : first add factory <PLACE_HOLDER> ; then constructors @$ so latter can override former !,_add factory creators ( ctxt @$ bean desc @$ vchecker @$ intr @$ creators @$ creator defs ) ;,first add constructors
instead @$ the calling code is responsible for the synchronization . see <PLACE_HOLDER> for details .,return get components_ no client code ( ) ;,synchronization see 6784816
the address should have <PLACE_HOLDER>,deque < property > result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; model node handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; string file name = null ;,address have logging
dont do <PLACE_HOLDER>,;,dont do anything
now we know the bytes contained <PLACE_HOLDER> we need to rewrite :,class reader file reader = new class reader ( bytes ) ; rewrite class adaptor class adaptor = new rewrite class adaptor ( type registry ) ; try { file reader . accept ( class adaptor @$ class reader . skip_frames ) ; } catch ( dont rewrite exception drex ) { return bytes ; },bytes contained class
we found the id . deleting the item via the content provider will also remove the <PLACE_HOLDER>,if ( c . move to first ( ) ) { long id = c . get long ( c . get column index or throw ( media store . images . media . _id ) ) ; uri delete uri = content uris . with appended id ( media store . images . media . external_content_uri @$ id ) ; content resolver . delete ( delete uri @$ null @$ null ) ; } else { },id remove file
must use raw local because the checksummer does n't honor <PLACE_HOLDER> .,file system fs = file system . get local ( conf ) . get raw ( ) ; object inspector inspector ; synchronized ( test orc file . class ) { inspector = object inspector factory . get reflection object inspector ( my row . class @$ object inspector factory . object inspector options . java ) ; } properties tbl props = new properties ( ) ; tbl props . set property ( __str__ @$ __str__ ) ; tbl props . set property ( __str__ @$ __str__ ) ; hive conf . set int var ( conf @$ hive conf . conf vars . hive_orc_base_delta_ratio @$ __num__ ) ; acid output format . options options = new acid output format . options ( conf ) . filesystem,checksummer honor flushes
when entity expansion limit is set by the application @$ we need to check for the entity expansion limit set by the parser @$ if number of entity expansions exceeds the entity expansion limit @$ parser will throw fatal <PLACE_HOLDER> . note that this represents the nesting level of open entities .,f entity expansion count ++ ; if ( f limit analyzer != null ) { f limit analyzer . add value ( entity expansion index @$ name @$ __num__ ) ; } if ( f security manager != null && f security manager . is over limit ( entity expansion index @$ f limit analyzer ) ) { f security manager . debug print ( f limit analyzer ) ; f error reporter . report error ( xml message formatter . xml_domain @$ __str__ @$ new object [ ] { f security manager . get limit value by index ( entity expansion index ) } @$ xml error reporter . severity_fatal_error ) ; f entity expansion count = __num__ ; },parser throw exception
the main thread will send a <PLACE_HOLDER>,work unit wu = e . get current work unit ( ) ; if ( wu . is main work ( ) ) { if ( problems == null ) { future . set ( executable ) ; e . get owner ( ) . task completed ( e @$ task @$ duration ) ; } else { future . set ( problems ) ; e . get owner ( ) . task completed with problems ( e @$ task @$ duration @$ problems ) ; } },thread send notification
if event has this <PLACE_HOLDER> to filter on,if ( event dimension transformed . contains ( filter dimension transformed ) || filter dimension transformed . contains ( event dimension transformed ) ) { set < string > event dimension values set = new hash set < > ( event dimension values transformed ) ; event dimension values set . retain all ( filtered dimension values transformed ) ; if ( ! event dimension values set . is empty ( ) ) { filtered events . add ( event ) ; event added = true ; break ; } },event has dimension
client sends an eds <PLACE_HOLDER> containing all clusters being watched to management server .,verify ( request observer ) . on next ( arg that ( new discovery request matcher ( __str__ @$ immutable list . of ( __str__ @$ __str__ ) @$ xds client impl . ads_type_url_eds @$ __str__ ) ) ) ;,client sends request
the automation service can suppress other <PLACE_HOLDER> .,final user state user state = get user state locked ( resolved user id ) ; if ( m ui automation manager . suppressing accessibility services locked ( ) ) { return collections . empty list ( ) ; } final list < accessibility service connection > services = user state . m bound services ; final int service count = services . size ( ) ; final list < accessibility service info > result = new array list < > ( service count ) ; for ( int i = __num__ ; i < service count ; ++ i ) { final accessibility service connection service = services . get ( i ) ; if ( ( service . m feedback type & feedback type ) !=,service suppress accessors
remove any that are constants @$ or expressions where all params exactly match a <PLACE_HOLDER> by expression :,if ( ! unmatched selects . is empty ( ) ) { throw new ksql exception ( __str__ + unmatched selects ) ; } final set view < column reference exp > unmatched selects agg = sets . difference ( aggregate analysis . get aggregate select fields ( ) @$ group by exprs ) ; if ( ! unmatched selects agg . is empty ( ) ) { throw new ksql exception ( __str__ + __str__ + unmatched selects agg ) ; } final set < column reference exp > having columns = aggregate analysis . get non aggregate having fields ( ) ; final set < column reference exp > having only = sets . difference ( having columns @$ group by exprs ) ; if (,params match group
the keys of probe and build <PLACE_HOLDER> are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer join works well in this case .,final int probe vals per key = __num__ ;,keys build sides
there should be at least two iterations of this loop because reset thread loopers calls <PLACE_HOLDER> ' on background loopers once @$ which also resets the scheduler .,for ( int i = __num__ ; i < __num__ ; i ++ ) { assert that ( shadow of ( background looper ) . get scheduler ( ) . size ( ) ) . is equal to ( __num__ ) ; assert that ( shadow of ( background looper ) . get scheduler ( ) . get current time ( ) ) . is equal to ( __num__ ) ; handler . post ( empty ) ; handler . post delayed ( empty @$ __num__ ) ; shadow of ( background looper ) . run to end of tasks ( ) ; assert that ( shadow of ( background looper ) . get scheduler ( ) . get current time ( ) ) . is equal to,loopers calls '
second input parameter but 3 rd <PLACE_HOLDER> .,return get column param string ( __num__ @$ arg1 column ) + __str__ + get column param string ( __num__ @$ arg3 column ) ;,parameter rd column
constructor is not visible and config seems to be null at this point @$ hence we can not use the build <PLACE_HOLDER> here,constructor < transfer manager builder > tmb constructor = transfer manager builder . class . get declared constructor ( config . class @$ transfer manager . class ) ; tmb constructor . set accessible ( true ) ; return tmb constructor . new instance ( config @$ transfer manager ) . with feature ( read after write consistent . class ) . as default ( ) ;,the build class
only my parameter signature changed so auto merge my <PLACE_HOLDER> .,get merge my ( ) . replace function parameters ( entry @$ monitor ) ;,auto merge signatures
add a new feed all new feeds will have the most recent <PLACE_HOLDER> marked as unplayed,feed item most recent = new feed . get most recent item ( ) ; if ( most recent != null ) { most recent . set new ( ) ; } new feeds list . add ( new feed ) ; result feeds [ feed idx ] = new feed ; log . d ( tag @$ __str__ + new feed . get title ( ) + __str__ ) ; collections . sort ( new feed . get items ( ) @$ new feed item pubdate comparator ( ) ) ; if ( new feed . get page nr ( ) == saved feed . get page nr ( ) ) { if ( saved feed . compare with other ( new feed ) ) { log,feeds marked item
note : parser not working properly here @$ use url note : raw live format could crash <PLACE_HOLDER>,m listener . on dash url ( uri . parse ( m dashmpd url . to string ( ) ) ) ;,note crash hotspot
n.b . missing format width exception is the only kind of illegal format exception whose constructor can take and display arbitrary error <PLACE_HOLDER> @$ hence its use below .,int length = pattern . length ( ) ; int arg length = arguments . size ( ) ; int i = __num__ ;,constructor take messages
bypass pe <PLACE_HOLDER> @$ but permit p es to complete ... valid docs wo n't care .,try { for ( ; ; ) { int c = getc ( ) ; if ( c == __str__ ) { c = getc ( ) ; if ( c != __str__ ) { if ( save comment text ) str tmp . append ( __str__ ) ; ungetc ( ) ; continue ; } next char ( __str__ @$ __str__ @$ null ) ; break one comment ; } if ( save comment text ) str tmp . append ( ( char ) c ) ; } } catch ( end of input exception e ) { if ( in . is internal ( ) ) { error ( __str__ @$ null ) ; } fatal ( __str__ ) ; },bypass pe tests
if the peer has an initializing <PLACE_HOLDER> @$ they are also not online .,if ( initializingid != null ) { remove newer persistentid ( offline members @$ initializingid ) ; } handle partially destroyed region ( offline members @$ members to wait for @$ persistentid @$ initializingid @$ disk storeid ) ;,peer has id
the handler created a <PLACE_HOLDER> of the <PLACE_HOLDER> and is now done with it .,dup2 . release ( ) ;,handler created slice
cancel currently executing tasks wait a <PLACE_HOLDER> for tasks to respond to being cancelled,if ( ! executor . await termination ( __num__ @$ time unit . seconds ) ) { log . warn ( __str__ ) ; },tasks wait while
the slow loading resource on that page takes 6 <PLACE_HOLDER> to return @$ but with 'none ' page loading strategy 'refresh ' operation should not wait .,assert that ( duration ) . as ( __str__ ) . is less than ( __num__ ) ;,resource takes seconds
without limiting to be inside the parent <PLACE_HOLDER> @$ the out screen size should keep relative to the input <PLACE_HOLDER> .,final activity record . compat display insets compat intsets = new activity record . compat display insets ( display content ) ; task . compute config resource overrides ( in out config @$ parent config @$ compat intsets ) ; assert equals ( ( short side - status bar height ) * density_default / parent config . density dpi @$ in out config . screen height dp ) ; assert equals ( long side * density_default / parent config . density dpi @$ in out config . screen width dp ) ; assert equals ( configuration . orientation_landscape @$ in out config . orientation ) ;,size keep configuration
note : the python version uses <PLACE_HOLDER> which return an empty list when indexed beyond what the list contains . since we ca n't slice out an empty sublist in java @$ we must check if we 've reached the end and clear the fnames list manually .,if ( cnt == fnames . size ( ) ) { fnames . clear ( ) ; } else { fnames = fnames . sub list ( cnt @$ fnames . size ( ) ) ; } m con . publish progress ( string . format ( anki droid app . get app resources ( ) . get string ( r . string . sync_media_downloaded_count ) @$ m download count ) ) ;,version uses java
now get the operator class which drives the <PLACE_HOLDER>,final class < ? extends driver < s @$ ot > > driver class = this . config . get driver ( ) ; this . driver = instantiation util . instantiate ( driver class @$ driver . class ) ; string head name = get environment ( ) . get task info ( ) . get task name ( ) . split ( __str__ ) [ __num__ ] . trim ( ) ; this . metrics = get environment ( ) . get metric group ( ) . get or add operator ( head name . starts with ( __str__ ) ? head name . substring ( __num__ ) : head name ) ; this . metrics . getio metric group ( ) . reuse input metrics,which drives schema
jersey does n't close the output stream when there is no entity as such the publisher needs to be closed from here ... it is assumed it 's possible to close the publisher @$ the output stream @$ multiple <PLACE_HOLDER>,try { publisher . close ( ) ; } catch ( io exception e ) { throw new illegal state exception ( __str__ @$ e ) ; },jersey close times
raw types take this <PLACE_HOLDER> as well,if ( type instanceof class ) { return for class ( ( class < ? > ) type ) ; } else if ( type instanceof parameterized type ) { final parameterized type parameterized type = ( parameterized type ) type ; final type [ ] type arguments = parameterized type . get actual type arguments ( ) ; final java type definition [ ] generic bounds = new java type definition [ type arguments . length ] ; for ( int i = __num__ ; i < type arguments . length ; i ++ ) { generic bounds [ i ] = resolve type definition ( type arguments [ i ] @$ method @$ method type args ) ; } return for class ( ( class <,types take class
create the reveal animations that will run when the retreat passes <PLACE_HOLDER>,for ( int i = __num__ ; i < steps ; i ++ ) { reveal animations [ i ] = new pending reveal animator ( was - i @$ new leftward start predicate ( dot centerx [ was - i ] ) ) ; dots to hide [ i ] = was - i ; } add update listener ( new animator update listener ( ) { @ override public void on animation update ( value animator value animator ) { retreating joinx2 = ( float ) value animator . get animated value ( ) ; if ( build . version . sdk_int >= build . version_codes . jelly_bean ) { post invalidate on animation ( ) ; } else { post invalidate ( ) ; },retreat passes them
check the base znodes <PLACE_HOLDER> first . only do the recursion if base znode 's perms are not correct .,try { list < acl > actual acls = recoverable zoo keeper . get acl ( znode paths . basez node @$ new stat ( ) ) ; if ( ! is base znode acl setup ( actual acls ) ) { log . info ( __str__ ) ; set znode acls recursive ( znode paths . basez node ) ; } } catch ( keeper exception . no node exception nne ) { return ; } catch ( interrupted exception ie ) { interrupted exception no throw ( ie @$ false ) ; } catch ( io exception | keeper exception e ) { log . warn ( __str__ @$ e ) ; },base znodes acl
first record contains header <PLACE_HOLDER>,if ( use headers ) { for ( int i = __num__ ; i < record . size ( ) && i < serieses . length ; i ++ ) { serieses [ i ] . set name ( record . get ( i ) ) ; } record = it . next ( ) ; },record contains names
compute the missing segments if there are at least two segments and the table has time <PLACE_HOLDER>,int num missing segments = __num__ ; int num segments = offline segmentzk metadata list . size ( ) ; segments validation and retention config validation config = table config . get validation config ( ) ; if ( segment interval utils . eligible for missing segment check ( num segments @$ validation config ) ) { list < interval > segment intervals = new array list < > ( num segments ) ; int num segments with invalid intervals = __num__ ; for ( offline segmentzk metadata offline segmentzk metadata : offline segmentzk metadata list ) { interval time interval = offline segmentzk metadata . get time interval ( ) ; if ( time interval != null && time utils . is valid time interval ( time,table has limit
do abc tag <PLACE_HOLDER> does not have name,continue ;,abc tag plugin
should we change the current <PLACE_HOLDER> ? do we have a <PLACE_HOLDER> set ?,if ( code set == __num__ ) { switch ( new code set ) { case code_code_a : pattern index = code_start_a ; break ; case code_code_b : pattern index = code_start_b ; break ; default : pattern index = code_start_c ; break ; } } else { pattern index = new code set ; },code have code
if none of the arguments have a <PLACE_HOLDER> specified @$ we just send all the values as the entity body,if ( get send parameter values as post body ( ) ) { has entity body = true ; arguments arguments = get arguments ( ) ; string builder entity body content = new string builder ( arguments . get argument count ( ) * __num__ ) ; for ( j meter property j meter property : arguments ) { http argument arg = ( http argument ) j meter property . get object value ( ) ; if ( charset != null ) { entity body content . append ( arg . get encoded value ( charset ) ) ; } else { entity body content . append ( arg . get encoded value ( ) ) ; } } string entity request entity = new string,none have name
last name should have no <PLACE_HOLDER>,check tooltip ( $ ( text field element . class ) . get ( __num__ ) @$ null ) ;,name have tooltip
p 12 key<PLACE_HOLDER> currently does not support separate <PLACE_HOLDER> and entry passwords,if ( p12keystore . equals ignore case ( ks . get type ( ) ) ) { throw une2 ; } else { pkey = get key passwd ( alias @$ null @$ null ) ; pp = new password protection ( pkey ) ; entry = ks . get entry ( alias @$ pp ) ; },keystore support store
if actual dimensions do n't match the declared <PLACE_HOLDER> @$ reset everything .,if ( this . s width > __num__ && this . s height > __num__ && ( this . s width != s width || this . s height != s height ) ) { reset ( false ) ; if ( bitmap != null ) { if ( ! bitmap is cached ) { bitmap . recycle ( ) ; } bitmap = null ; if ( on image event listener != null && bitmap is cached ) { on image event listener . on preview released ( ) ; } bitmap is preview = false ; bitmap is cached = false ; } } this . decoder = decoder ; this . s width = s width ; this . s height = s height ;,dimensions match size
verify escaped partition names do n't return <PLACE_HOLDER>,exception thrown = false ; try { string bad part name = __str__ ; client . get partition ( db name @$ tbl name @$ bad part name ) ; } catch ( no such object exception e ) { exception thrown = true ; } assert true ( __str__ @$ exception thrown ) ; path part path = new path ( part . get sd ( ) . get location ( ) ) ; assert true ( fs . exists ( part path ) ) ; client . drop partition ( db name @$ tbl name @$ part . get values ( ) @$ true ) ; assert false ( fs . exists ( part path ) ) ;,names return exceptions
pairs now contains a uniquified list of the sorted inputs @$ with counts for how often that item appeared . now sort by how frequently they occur @$ and pick the most frequent . if the first place is tied between two @$ do n't pick <PLACE_HOLDER> .,collections . sort ( pairs ) ; final pair first pair = pairs . get ( __num__ ) ; if ( pairs . size ( ) == __num__ ) return first pair . item ; final pair second pair = pairs . get ( __num__ ) ; if ( first pair . count > second pair . count ) return first pair . item ; check state ( first pair . count == second pair . count ) ; return __num__ ;,list pick them
dw <PLACE_HOLDER> specifies the <PLACE_HOLDER> of the data chunk @$ in bytes .,idx1 chunk . finish ( ) ;,size specifies size
do bind implements all four <PLACE_HOLDER> of binding,naming context data store impl = ( naming context data store ) this ; do bind ( impl @$ n @$ nc @$ false @$ binding type . ncontext ) ;,bind implements flavors
a task normally slowly accrues scheduled <PLACE_HOLDER> in a level and then moves to the next @$ but if the split had a particularly long quanta @$ accrue <PLACE_HOLDER> to each level as if it had run in that level up to the level limit .,for ( int current level = old level ; current level < new level ; current level ++ ) { long time accrued to level = math . min ( seconds . to nanos ( level_threshold_seconds [ current level + __num__ ] - level_threshold_seconds [ current level ] ) @$ remaining level contribution ) ; add level time ( current level @$ time accrued to level ) ; remaining level contribution -= time accrued to level ; remaining task time -= time accrued to level ; } add level time ( new level @$ remaining level contribution ) ; long new level min priority = get level min priority ( new level @$ scheduled nanos ) ; return new priority ( new level @$ new level min priority,task accrues tasks
x attr exists @$ and replace it using create| replace <PLACE_HOLDER> .,fs . setx attr ( path @$ name1 @$ value1 @$ enum set . of ( x attr set flag . create ) ) ; fs . setx attr ( path @$ name1 @$ new value1 @$ enum set . of ( x attr set flag . create @$ x attr set flag . replace ) ) ; xattrs = fs . getx attrs ( path ) ; assert . assert equals ( xattrs . size ( ) @$ __num__ ) ; assert . assert array equals ( new value1 @$ xattrs . get ( name1 ) ) ; fs . removex attr ( path @$ name1 ) ;,create| replace flag
its important that we not store the stack <PLACE_HOLDER> in the weak order queue as the stack also is used in the weak hash map as key . so just store the enclosed atomic integer which should allow to have the stack <PLACE_HOLDER> g ced .,head = new head ( stack . available shared capacity ) ; head . link = tail ; interval = stack . interval ; handle recycle count = interval ;,which allow size
both sides contain only one <PLACE_HOLDER>,join function . join ( firstv1 @$ firstv2 @$ collector ) ;,sides contain element
creating the file makes the link <PLACE_HOLDER>,create and write file ( file abs ) ; wrapper . get file status ( link ) ;,file makes writable
circular reveal library uses absolute coordinates setup <PLACE_HOLDER>,support animator anim = io . codetail . animation . view animation utils . create circular reveal ( view @$ centerx @$ centery @$ start radius @$ end radius ) ; anim . set duration ( ( int ) duration ) ; anim . set interpolator ( interpolator ) ;,library uses animation
convert text <PLACE_HOLDER> to table <PLACE_HOLDER> . we assume the text <PLACE_HOLDER> must be the sql output . this assumption is correct for now . ideally livy should return table <PLACE_HOLDER> . we may do it in the future release of livy .,for ( interpreter result message message : result . message ( ) ) { if ( message . get type ( ) == interpreter result . type . text ) { list < string > rows = parsesql output ( message . get data ( ) ) ; result2 . add ( interpreter result . type . table @$ string utils . join ( rows @$ __str__ ) ) ; if ( rows . size ( ) >= ( max result + __num__ ) ) { result2 . add ( result messages . get exceeds limit rows message ( max result @$ zeppelin_livy_spark_sql_max_result ) ) ; } } else { result2 . add ( message . get type ( ) @$ message . get data ( ) ),livy return result
now to test the case when rm already gave <PLACE_HOLDER> @$ and nm suddenly realizes that the container is running .,log . info ( __str__ + __str__ ) ; container statuses . clear ( ) ; container status list . clear ( ) ; container status list . add ( builder utils . new container status ( conts . get ( __num__ ) . get id ( ) @$ container state . running @$ __str__ @$ __num__ @$ conts . get ( __num__ ) . get resource ( ) ) ) ; container statuses . put ( app . get application id ( ) @$ container status list ) ; resp = nm1 . node heartbeat ( container statuses @$ true ) ;,rm gave no
make sure the object outlives the <PLACE_HOLDER>,if ( ( flags & ( call_type_callback | call_type_global_value | call_type_struct_member ) ) > __num__ ) { o . retain ( ) ; },object outlives gc
if the last table does not contain any <PLACE_HOLDER> we still need to mark the last processed event as the last one,if ( snapshot context . last table ) { snapshot context . offset . mark last snapshot record ( ) ; },table contain records
ensure that the notifier updated the <PLACE_HOLDER>,assert equals ( v1 @$ r . get entry ( k3 ) . get value ( ) ) ;,notifier updated value
this call is checking the private field <PLACE_HOLDER> in the reloaded method has been changed to use the <PLACE_HOLDER>ors into the type that can <PLACE_HOLDER> the field from outside,method = st . get clazz ( ) . get method ( __str__ ) ; string = ( string ) method . invoke ( object ) ; assert equals ( __str__ @$ string ) ;,call checking reference
tab 2 has a custom <PLACE_HOLDER> @$ but no text or icon,tab = tab layout . get tab at ( __num__ ) ; assert null ( tab . get text ( ) ) ; assert null ( tab . get icon ( ) ) ; assert not null ( tab . get custom view ( ) ) ; assert equals ( r . id . my_custom_tab @$ tab . get custom view ( ) . get id ( ) ) ;,tab has view
have server write <PLACE_HOLDER> .,try ( blockhead connection server conn = server conn fut . get ( timeouts . connect @$ timeouts . connect_unit ) ) { byte [ ] payload = new byte [ buffer size / __num__ ] ; arrays . fill ( payload @$ ( byte ) __str__ ) ; byte buffer server frame = buffer util . allocate ( buffer size ) ; buffer util . flip to fill ( server frame ) ; server frame . put ( ( byte ) ( __num__ | __num__ ) ) ; server frame . put ( ( byte ) __num__ ) ; server frame . put ( ( byte ) ( payload . length > > __num__ ) ) ; server frame . put ( ( byte ) ( payload,server write data
no need to sync because noone has <PLACE_HOLDER> to new info yet,new info . policy entries . add ( pe ) ;,noone has access
while statement always has a <PLACE_HOLDER> of at least 1,bool comp while ++ ; entry stack . peek ( ) . bump decision points ( bool comp while ) ; super . visit ( node @$ data ) ; logger . exiting ( class_name @$ __str__ ) ; return data ;,statement has complexity
normal passes use syntactic scope creator @$ so that 's <PLACE_HOLDER> we use here .,redeclaration handler redeclaration handler = ( scope s @$ string name @$ node n @$ compiler input input ) -> { } ; syntactic scope creator scope creator = new syntactic scope creator ( compiler @$ redeclaration handler ) ; return scope creator . create scope ( root @$ null ) ;,passes use what
first callback fetches <PLACE_HOLDER> based on a query,data provider < person @$ ? > data provider = data provider . from callbacks ( query -> { int offset = query . get offset ( ) ; int limit = query . get limit ( ) ; list < person > persons = get person service ( ) . fetch persons ( offset @$ limit ) ; return persons . stream ( ) ; } @$ query -> get person service ( ) . get person count ( ) ) ;,callback fetches persons
these exist just to support the 'old ' lombok.experimental.builder @$ which had these properties . <PLACE_HOLDER> no longer has them .,boolean fluent = to boolean ( annotation . get actual expression ( __str__ ) @$ true ) ; boolean chain = to boolean ( annotation . get actual expression ( __str__ ) @$ true ) ; string builder method name = builder instance . builder method name ( ) ; string build method name = builder instance . build method name ( ) ; string builder class name = builder instance . builder class name ( ) ; string to builder method name = __str__ ; boolean to builder = builder instance . to builder ( ) ; java . util . list < name > type args for to builder = null ; if ( builder method name == null ) builder method name = __str__ ;,which had lombok
if our locale does n't have a <PLACE_HOLDER> for some reason @$ then we do n't really have a reasonable default .,if ( text utils . is empty ( system locale . get language ( ) ) ) { return null ; } final list < keyboard layout > layouts = new array list < > ( ) ; visit all keyboard layouts ( new keyboard layout visitor ( ) { @ override public void visit keyboard layout ( resources resources @$ int keyboard layout res id @$ keyboard layout layout ) { if ( layout . get vendor id ( ) != d . get vendor id ( ) || layout . get product id ( ) != d . get product id ( ) ) { return ; } final locale list locales = layout . get locales ( ) ; final int num locales = locales,locale have language
we want to differentiate between io <PLACE_HOLDER>s thrown by the repository and io <PLACE_HOLDER>s thrown from processor code . as a result @$ as have the flow file access input stream that catches io <PLACE_HOLDER> from the repository and translates into either flow file access <PLACE_HOLDER> or content not found <PLACE_HOLDER> . we keep track of any content not found <PLACE_HOLDER> because if it,final flow file access input stream ffais = new flow file access input stream ( counting in @$ source @$ curr claim ) ; final flow file access output stream ffaos = new flow file access output stream ( counting out @$ source ) ; boolean cnfe thrown = false ; try { writer . process ( create task termination stream ( ffais ) @$ create task termination stream ( ffaos ) ) ; } catch ( final content not found exception cnfe ) { cnfe thrown = true ; throw cnfe ; } finally { written to flow file = counting out . get bytes written ( ) ; this . bytes written += written to flow file ; this . bytes read += counting in .,code catches exception
put the call args back on stack so the method call can find <PLACE_HOLDER>,if ( should backup ) { restore stack ( backup args indices @$ arg types @$ is static call ) ; },call find them
if another thread concurrently removes the only remaining <PLACE_HOLDER> from the entry @$ this put and get will return null @$ since the entry is about to be removed from the map . in that case @$ we obtain a fresh entry from the map and do the put on it .,while ( result == null ) { result = get or create entry ( key ) . put and get ( val ) ; },thread removes value
note that this insertion method is worthwhile since the vast majority mark group keys will have only one <PLACE_HOLDER> .,if ( o == null ) { mark groups . put ( token @$ token ) ; } else if ( o instanceof token entry ) { list < token entry > l = new array list < > ( ) ; l . add ( ( token entry ) o ) ; l . add ( token ) ; mark groups . put ( token @$ l ) ; } else { @ suppress warnings ( __str__ ) list < token entry > l = ( list < token entry > ) o ; l . add ( token ) ; } last hash = __num__ ; for ( int end = math . max ( __num__ @$ i - min + __num__ ) ; i >,keys have entry
schedule monitor only if the job wants auto <PLACE_HOLDER> functionality,try { if ( context . get job detail ( ) . get job data map ( ) . get boolean ( auto_interruptible ) ) { job interrupt monitor plugin monitor plugin = ( job interrupt monitor plugin ) context . get scheduler ( ) . get context ( ) . get ( job_interrupt_monitor_key ) ; long job data delay = default_max_runtime ; if ( context . get job detail ( ) . get job data map ( ) . get ( max_run_time ) != null ) { job data delay = context . get job detail ( ) . get job data map ( ) . get long ( max_run_time ) ; } future = monitor plugin . schedule job interrupt monitor ( context . get job,job wants interrupt
dummy out passed target <PLACE_HOLDER> @$ since we do n't care about target .,if ( arg . equals ( __str__ ) ) { get arg value ( args @$ arg ) ; } else if ( platform_module_system_options . contains ( arg ) ) { add platform module system options ( arg @$ get arg value ( args @$ arg ) ) ; } else if ( arg . starts with ( batch_processing_max_flag ) ) { } else if ( obsolete flags . contains ( arg ) ) { } else if ( arg . starts with ( __str__ ) ) { usage ( __str__ + arg ) ; } else if ( name table . is valid class name ( arg ) && ! has known file suffix ( arg ) ) { entry classes . add ( arg ) ;,dummy passed argument
synchronize so default <PLACE_HOLDER> does n't override other default <PLACE_HOLDER>,synchronized ( session ) { object result = session . get attribute ( name ) ; if ( result == null && default value != null ) { session . set attribute ( name @$ default value ) ; result = default value ; } return result ; },value override value
when hive.metastore.transactional.event.listeners is set @$ a failed event should not create a new <PLACE_HOLDER>,dummy raw store fail event . set event succeed ( false ) ; try { ms client . alter_partition ( default db name @$ tbl name @$ new part @$ null ) ; fail ( __str__ ) ; } catch ( exception ex ) { },event create notification
wait for all non cancelled <PLACE_HOLDER> to be completed,try { boolean acquired = sem . try acquire ( max_waiting_time @$ time unit . milliseconds ) ; assert . assert true ( __str__ @$ acquired ) ; background executor . cancel all ( __str__ @$ true ) ; assert . assert equals ( __str__ @$ list . size ( ) @$ nb_add ) ; for ( int i = __num__ ; i < nb_add ; i ++ ) { assert . assert true ( __str__ @$ i < nb_add ) ; } } catch ( interrupted exception e ) { assert . assert false ( __str__ @$ true ) ; },non cancelled operations
different properties means different <PLACE_HOLDER>,assert false ( objects . equals ( empty @$ finger ) ) ; assert false ( objects . equals ( empty @$ finger brand ) ) ; assert false ( objects . equals ( finger @$ finger brand ) ) ;,properties means results
optionally check the byte after this frame matches sync <PLACE_HOLDER> .,if ( ! try read ( pes buffer @$ adts scratch . data @$ __num__ ) ) { return true ; } adts scratch . set position ( __num__ ) ; int frame size = adts scratch . read bits ( __num__ ) ; if ( frame size <= __num__ ) { return false ; },frame matches header
lower android versions have a reference <PLACE_HOLDER> with 1024 entries only,for ( int i = __num__ ; i < __num__ ; i ++ ) { int [ ] ints = jni test . return int array ( ) ; assert not null ( ints ) ; },versions have cache
assert true here to make test tool take this test <PLACE_HOLDER> into account,assert . assert true ( true ) ;,tool take case
setting <PLACE_HOLDER> interval to 1 hour to prevent bp service actor sends <PLACE_HOLDER> periodically to nn during running test case @$ and bp service actor only sends <PLACE_HOLDER> once after startup,conf . set time duration ( dfs_heartbeat_interval_key @$ __num__ @$ time unit . hours ) ; minidfs cluster cluster = new minidfs cluster . builder ( conf ) . nn topology ( minidfsnn topology . simpleha federated topology ( __num__ ) ) . build ( ) ; cluster . wait active ( ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; metrics record builder rb = get metrics ( dn . get metrics ( ) . name ( ) ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$ rb ) ; assert counter ( __str__ @$ __num__ @$,actor sends heartbeat
already have demand @$ so do n't need to do anything @$ the current demand will complete the <PLACE_HOLDER> .,if ( requested ( ) != __num__ ) { return ; },demand complete stream
defaults to cs <PLACE_HOLDER>,fla version = fla version . cs6 ;,defaults cs 6
if it is a dynamic <PLACE_HOLDER> mapping @$ we can safely assume leaf <PLACE_HOLDER> name does not have ' . ' in it validate if parent <PLACE_HOLDER> is specified @$ then parent <PLACE_HOLDER> exists and an instance of auto create enabled parent <PLACE_HOLDER>,queue mapping entity new mapping = validate and get auto created queue mapping ( queue manager @$ mapping @$ queue path ) ; if ( new mapping != null ) { new mappings . add ( new mapping ) ; } else { new mappings . add ( mapping ) ; },instance create queue
when table could not be fetched from metastore @$ it is not known whether the table was added . deleting the table when aborting commit has the <PLACE_HOLDER> of deleting table not added in this transaction . not deleting the table may leave garbage behind . the former is much more dangerous than the latter . therefore @$ the table is not considered added,if ( ! done ) { throw e ; },commit has affect
base 64 encode the <PLACE_HOLDER> and then convert to a string for the header .,encoded buf = base64 . encode ( buf @$ url_safe ) ; return encoded buf . to string ( utf_8 ) ; release ( buf ) ; release ( encoded buf ) ;,base encode buffer
batch 2 does a <PLACE_HOLDER> based on the read,volt queuesql ( write @$ expect_scalar_match ( __num__ ) @$ current + __num__ @$ pkey ) ; volt executesql ( ) ;,batch does write
disable reorder of columns . allowing this would cause the <PLACE_HOLDER> to become uns<PLACE_HOLDER> ; we rely on knowing that mnemonics are always in the first column @$ and that operand columns are in a particular order .,this . get table header ( ) . set reordering allowed ( false ) ;,reorder cause structure
replication scope allows <PLACE_HOLDER> @$ and does not require empty directories,if ( replication spec . is in replication scope ( ) ) { return ; },scope allows itself
client 1 did not register <PLACE_HOLDER>,await ( ) . until asserted ( ( ) -> { assert that ( client1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ - __num__ ) ; assert that ( client2 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ - __num__ ) ; assert that ( server1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ - __num__ ) ; assert that,client register interest
flush those recovered buffered <PLACE_HOLDER> out .,produce synchronously to partition zero ( input @$ as list ( new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __str__ @$ scaled time ( __num__ ) ) ) ) ; verify output ( output raw @$ new hash set < > ( as list ( new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__ @$ scaled time ( __num__ ) ) @$ new key value timestamp < > ( __str__ @$ __num__,those recovered records
remove <PLACE_HOLDER> should also be clearing the last modified <PLACE_HOLDER>,m exif . remove timestamp ( ) ; assert that ( m exif . get last modified timestamp ( ) ) . is equal to ( exif . invalid_timestamp ) ;,timestamp clearing timestamp
inbuilt assumption that the testdir has only one <PLACE_HOLDER> file .,path di test = new path ( tmppath @$ testdir ) ; if ( ! fs . exists ( di test ) ) { throw new runtime exception ( tmpdir + file . separator + testdir + __str__ ) ; } if ( ! shim loader . get hadoop shims ( ) . is directory ( fs . get file status ( di test ) ) ) { throw new runtime exception ( tmpdir + file . separator + testdir + __str__ ) ; } fs data input stream fi test = fs . open ( ( fs . list status ( di test ) ) [ __num__ ] . get path ( ) ) ; file input stream fi gold = new file input stream ( new,testdir has xml
bind include <PLACE_HOLDER>,c = new method closure ( this @$ __str__ ) ; super . set variable ( __str__ @$ c ) ;,bind include variable
the number of got <PLACE_HOLDER>,int got = cis . read ( result ) ;,number got bytes
configure a virtual destination that forwards messages from topic test queue <PLACE_HOLDER>,remote session . create consumer ( included ) ; message producer included producer = local session . create producer ( included ) ; message test = local session . create text message ( __str__ ) ; final destination statistics destination statistics = local broker . get destination ( included ) . get destination statistics ( ) ;,messages queue name
if a partition has multiple partition keys @$ we make the assumption that make part name with one key will return a <PLACE_HOLDER> of the name made with both all the keys .,string escaped name fragment = warehouse . make part name ( part key to val @$ false ) ; if ( key count == __num__ ) { params . put ( param name @$ escaped name fragment ) ; fltr . append ( __str__ ) . append ( is eq ? __str__ : __str__ ) . append ( param name ) ; } else if ( key pos + __num__ == key count ) { params . put ( param name @$ __str__ + escaped name fragment ) ; fltr . append ( is eq ? __str__ : __str__ ) . append ( __str__ ) . append ( param name ) . append ( __str__ ) ; } else if ( key pos == __num__ ) { params,name return parts
until the scan manager apply a new <PLACE_HOLDER> @$ we 're going to work with a default result log config object .,config = new result log config ( ) ; config . set memory max records ( mem capacity ) ; config . set log file name ( get log file name ( false ) ) ; config . set log file max records ( file capacity ) ;,manager apply configuration
a tree on the left @$ an editor on the right : put <PLACE_HOLDER> in a sash form ... right below the editor we display the error messages ...,sash form = new sash form ( shell @$ swt . horizontal ) ; sash form . set layout ( new fill layout ( ) ) ; form data fd sash form = new form data ( ) ; fd sash form . left = new form attachment ( __num__ @$ __num__ ) ; fd sash form . right = new form attachment ( __num__ @$ __num__ ) ; fd sash form . top = new form attachment ( __num__ @$ __num__ ) ; fd sash form . bottom = new form attachment ( buttons composite @$ - __num__ ) ; sash form . set layout data ( fd sash form ) ; form data fdbc = new form data ( ) ; fdbc . left = new,tree put them
roll back so next write buffer can do its <PLACE_HOLDER> on the next call but at the first write we 're at 0 .,if ( m write index == __num__ && m write buf index != __num__ ) { m write index = m chunk size ; m write buf index -- ; },buffer do work
remove the filter popup if the user has cleared all <PLACE_HOLDER>,if ( showing && length == __num__ ) { dismiss popup ( ) ; m filtered = false ; },user cleared state
the current txn is either in open or aborted <PLACE_HOLDER> . mark the write ids <PLACE_HOLDER> as per the txn <PLACE_HOLDER> .,invalid write id list . add ( write id ) ; if ( valid txn list . is txn aborted ( txn id ) ) { aborted bits . set ( invalid write id list . size ( ) - __num__ ) ; } else { min open write id = math . min ( min open write id @$ write id ) ; },either mark state
unconditionally reduce the amount of memory required for flow control because there is no object allocation costs associated with doing so and the stream will not have any more local flow control <PLACE_HOLDER> to keep track of anymore .,stream . set property ( state key @$ reduced_flow_state ) ;,stream have objects
all types must have the same <PLACE_HOLDER>,for ( logical type type : normalized types ) { if ( type . get type root ( ) != type root ) { return null ; } if ( type . get children ( ) . size ( ) != number of children ) { return null ; } },types have parent
if a run instances request does n't specify a client <PLACE_HOLDER> @$ fill one in @$ otherwise retries could result in unwanted instances being launched in the customer 's account .,if ( original request instanceof run instances request ) { run instances request run instances request = ( run instances request ) original request ; if ( run instances request . get client token ( ) == null ) { request . add parameter ( __str__ @$ uuid . randomuuid ( ) . to string ( ) ) ; } } else if ( original request instanceof modify reserved instances request ) { modify reserved instances request modify reserved instances request = ( modify reserved instances request ) original request ; if ( modify reserved instances request . get client token ( ) == null ) { request . add parameter ( __str__ @$ uuid . randomuuid ( ) . to string ( ) ) ; } },request specify token
invalidate both edits <PLACE_HOLDER> .,invalidate edits dir at index ( __num__ @$ true @$ false ) ; invalidate edits dir at index ( __num__ @$ true @$ false ) ;,invalidate edits journals
it makes some <PLACE_HOLDER> to return true @$ as no filter implies all shall pass the filter @$ but if this returns true @$ then any other filters can be used as the source of the data to filter @$ which does n't make <PLACE_HOLDER> if this is meant to only be used by itself .,return false ;,which make sense
let 's skip these strange <PLACE_HOLDER> @$ as shortening the text might leave us behind with invalid markup etc,if ( context . length ( ) > max_context_length ) { continue ; },'s skip cases
verify that the second job used the <PLACE_HOLDER> that was passed to its builder .,assert that ( override prober . probed ( ) @$ is ( true ) ) ;,job used prober
note : adjust the column size after loading the initial <PLACE_HOLDER> so the table has the column <PLACE_HOLDER> generated,this . get column model ( ) . get column ( __num__ ) . set max width ( __num__ ) ;,table has view
start the enable procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new enable table procedure ( proc exec . get environment ( ) @$ table name ) ) ; int last step = __num__ ;,procedure kill executor
should n't ever throw cancelled since this method uses a dummy <PLACE_HOLDER> .,msg . error ( this @$ __str__ + e . get message ( ) @$ e ) ;,method uses exception
splits must be stable @$ and can not change during consecutive executions for example : kafka should not add <PLACE_HOLDER> if more then one topic is read .,for ( int i = __num__ ; i < num splits ; i ++ ) { result . add ( new microbatch source < > ( splits . get ( i ) @$ max read time @$ __num__ @$ num records [ i ] @$ i @$ source id @$ reader cache interval ) ) ; },kafka add things
as we only support limited concurrent checkpoints @$ after checkpoint triggered more than the <PLACE_HOLDER> @$ the current periodic trigger would been assigned as null .,checkpoint coordinator . trigger checkpoint ( system . current time millis ( ) @$ false ) ; assert false ( checkpoint coordinator . is current periodic trigger available ( ) ) ; assert equals ( max concurrent checkpoints @$ checkpoint coordinator . get number of pending checkpoints ( ) ) ; checkpoint coordinator . abort pending checkpoints ( new checkpoint exception ( checkpoint failure reason . job_failover_region ) ) ;,checkpoint triggered time
dmn engine does n't generate the whole <PLACE_HOLDER> when no entry of the decision table match,if ( result raw != null ) { for ( expression element expression element : elements without class ) { if ( ! ( result raw instanceof map ) ) { throw new scenario exception ( __str__ ) ; } map < string @$ object > result = ( map < string @$ object > ) result raw ; result raw = result . get ( expression element . get step ( ) ) ; } } class < ? > result class = result raw != null ? result raw . get class ( ) : null ; object expected result raw = expected result . get raw value ( ) ; return get result wrapper ( fact mapping . get class name ( ) @$ expected,engine generate case
no state changed @$ check when we should remove the delay flag from the shards the next time . if cluster state changed @$ we can leave the scheduling of the next delay up to the cluster changed <PLACE_HOLDER> this should not be needed @$ but we want to be extra safe here,if ( old state == new state ) { schedule if needed ( current nano time ( ) @$ new state ) ; },state changed which
we use the default backend <PLACE_HOLDER>,if ( build time config . default backend . version . is present ( ) ) { add config ( property collector @$ engine settings . default_backend @$ hibernate search elasticsearch recorder . default_backend ) ; },default backend version
strip the noisy stack trace <PLACE_HOLDER> .,string [ ] exclusions = excluded methods . get ( ) ; for ( int k = __num__ ; k < exclusions . length ; k += __num__ ) { if ( exclusions [ k ] . equals ( element . get class name ( ) ) && exclusions [ k + __num__ ] . equals ( element . get method name ( ) ) ) { continue out ; } } buf . append ( __str__ ) ; buf . append ( element . to string ( ) ) ; buf . append ( newline ) ;,noisy stack lines
ok. now i have my <PLACE_HOLDER> up server & region server services and dodgy wal @$ go ahead with test .,file system fs = file system . get ( conf ) ; path root dir = new path ( dir + get name ( ) ) ; dodgyfs log dodgywal = new dodgyfs log ( fs @$ root dir @$ get name ( ) @$ conf ) ; dodgywal . init ( ) ; path originalwal = dodgywal . get current file name ( ) ;,ok. have mocked
check if there have been any changes if we have n't built the media db yet @$ do so on this sync . see <PLACE_HOLDER> at the top of this class about this difference to the original .,try { if ( m col . get media ( ) . need scan ( ) ) { m con . publish progress ( r . string . sync_media_find ) ; m col . log ( __str__ ) ; m col . get media ( ) . find changes ( ) ; } int last usn = m col . get media ( ) . last usn ( ) ; json object ret = m server . begin ( ) ; int srv usn = ret . get int ( __str__ ) ; if ( ( last usn == srv usn ) && ! ( m col . get media ( ) . have dirty ( ) ) ) { return __str__ ; } m col . log,check see comments
a new call will trigger a new <PLACE_HOLDER>,client call < string @$ integer > call2 = oob1 . new call ( method @$ call options . default ) ; call2 . start ( mock call listener2 @$ headers ) ; client call < string @$ integer > call3 = oob1 . new call ( method @$ call options . default . with wait for ready ( ) ) ; call3 . start ( mock call listener3 @$ headers ) ; verify ( mock transport factory @$ times ( __num__ ) ) . new client transport ( eq ( socket address ) @$ eq ( new client transport options ( ) . set authority ( __str__ ) . set user agent ( user_agent ) ) @$ isa ( channel logger . class ) ) ; transport,call trigger timeout
while this is more general than <PLACE_HOLDER> stapler can invoke on these types @$ the above is the only criterion for stapler to attempt dispatch . therefore prohibit this as a regular getter .,if ( parameter types . length > __num__ && parameter types [ __num__ ] == string . class ) { return false ; },stapler invoke which
use a short 100 ms sleep since this could be done inline with a rs startup even if we fail @$ other region servers can take <PLACE_HOLDER> of it,this . executor = new thread pool executor ( nb workers @$ nb workers @$ __num__ @$ time unit . milliseconds @$ new linked blocking queue < > ( ) ) ; thread factory builder tfb = new thread factory builder ( ) ; tfb . set name format ( __str__ ) ; tfb . set daemon ( true ) ; this . executor . set thread factory ( tfb . build ( ) ) ; this . latest paths = new hash map < > ( ) ; this . replication for bulk load data enabled = conf . get boolean ( h constants . replication_bulkload_enable_key @$ h constants . replication_bulkload_enable_default ) ; this . sleep for retries = this . conf . get long ( __str__,servers take care
test that a click on an item invokes the registered <PLACE_HOLDER>,int index to click = expected count - __num__ ; on data ( all of ( is ( instance of ( string . class ) ) @$ is ( expected content [ index to click ] ) ) ) . in root ( is dialog ( ) ) . perform ( click ( ) ) ; verify ( on click listener @$ times ( __num__ ) ) . on click ( m alert dialog @$ index to click ) ;,click invokes listener
close created <PLACE_HOLDER> .,list < completable future < void > > connection close futures = new array list < > ( m connections . size ( ) ) ; for ( connection connection : m connections ) { connection close futures . add ( connection . close ( ) ) ; } m connections . clear ( ) ; return completable future . all of ( connection close futures . to array ( new completable future [ __num__ ] ) ) ;,close created connections
first char is note <PLACE_HOLDER> next : are rest <PLACE_HOLDER> @$ pitch @$ note <PLACE_HOLDER>,int channel = __num__ ; int velocity = __num__ ; int track = __num__ ; final int resolution = __num__ ; final int resolution delta = resolution / __num__ ; int index = __num__ ;,char note 3
end of string @$ process last <PLACE_HOLDER> .,run end = end ;,end process output
check job config for overrides @$ otherwise use the default server <PLACE_HOLDER> .,long job val = job conf . get long ( var . varname @$ - __num__ ) ; return ( job val != - __num__ ) ? job val : hive conf . get long var ( daemon conf @$ var ) ;,job use value
when application explicitly truncates <PLACE_HOLDER> ; timestamp based purge is only used to cleanup log <PLACE_HOLDER> that have been marked for truncation,if ( ( l . is truncated ( ) || ! conf . get explicit truncation by application ( ) ) && ! l . is in progress ( ) && ( l . get completion time ( ) < min timestamp to keep ) ) { purge list . add ( l ) ; } else { break ; },application truncates files
jjt add child <PLACE_HOLDER> it 's child node list according to known indexes . going backwards makes sure the first time it gets the right size avoiding copies .,for ( int i = parameter types . length - __num__ ; i >= __num__ ; i -- ) { ast formal parameter formal parameter = new ast formal parameter ( java parser tree constants . jjtformalparameter ) ; formal parameters . jjt add child ( formal parameter @$ i ) ; formal parameter . jjt set parent ( formal parameters ) ; ast variable declarator id variable declarator id = new ast variable declarator id ( java parser tree constants . jjtvariabledeclaratorid ) ; variable declarator id . set image ( __str__ + i ) ; formal parameter . jjt add child ( variable declarator id @$ __num__ ) ; variable declarator id . jjt set parent ( formal parameter ) ; ast type type = new,jjt add objects
note : we do n't break because other branches may have useful state <PLACE_HOLDER>,if ( m ) { match = true ; },branches have information
create a text file output <PLACE_HOLDER>,string test file output name = name ; text file output meta text file output meta = new text file output meta ( ) ; string text file input pid = registry . get plugin id ( step plugin type . class @$ text file output meta ) ; step meta text file output step = new step meta ( text file input pid @$ test file output name @$ text file output meta ) ;,text file step
r dot java <PLACE_HOLDER>,android resource android resource = new android resource ( build target . with appended flavors ( aar_android_resource_flavor ) @$ project filesystem @$ android resource params @$ graph builder @$ immutable sorted set . < build rule > natural order ( ) . add ( assemble assets directories ) . add ( assemble resource directories ) . add all ( original build rule params . get declared deps ( ) . get ( ) ) . build ( ) @$ assemble resource directories . get source path to output ( ) @$ immutable sorted map . of ( ) @$ null @$ assemble assets directories . get source path to output ( ) @$ immutable sorted map . of ( ) @$ manifest . get source path to output,r dot package
app hit the rate <PLACE_HOLDER> .,if ( ! is under timing session count quota && stats . session count in window < stats . session count limit ) { in quota time elapsed = math . max ( in quota time elapsed @$ stats . session rate limit expiration time elapsed ) ; },app hit limit
management server sends back a cds <PLACE_HOLDER> without cluster for the requested resource .,list < any > clusters = immutable list . of ( any . pack ( build cluster ( __str__ @$ null @$ false ) ) @$ any . pack ( build cluster ( __str__ @$ null @$ false ) ) ) ; discovery response response = build discovery response ( __str__ @$ clusters @$ xds client impl . ads_type_url_cds @$ __str__ ) ; response observer . on next ( response ) ;,server sends response
sadly @$ motion event does n't implement <PLACE_HOLDER> @$ so we compare references .,assert true ( expected == m last context event ) ;,event implement context
assert that the correct dir does not contain the <PLACE_HOLDER> .,assert true ( pairs correct . stream ( ) . none match ( p -> p . get path ( ) . equals ( cwd correct ) ) ) ;,dir contain violation
if a shape has 2 or less <PLACE_HOLDER> it can not be reduced,if ( tolerance <= __num__ || n < __num__ ) { return shape ; } boolean [ ] marked = new boolean [ n ] ;,shape has values
next tag does n't correctly handle dt <PLACE_HOLDER>,if ( event == xml stream constants . start_document ) { while ( ! fast infoset stream reader . is start element ( ) ) event = fast infoset stream reader . next ( ) ; },tag handle element
table should not contain any <PLACE_HOLDER> at initialization,assert null ( table . get column footer ( __str__ ) ) ; assert null ( table . get column footer ( __str__ ) ) ; assert null ( table . get column footer ( __str__ ) ) ;,table contain columns
note on quoting : it would be wrong here @$ since argv will be passed to runtime.exec @$ which should not parse <PLACE_HOLDER> or split on whitespace .,argv . add ( __str__ + name + __str__ + props . get property ( name ) ) ;,which parse spaces
mandatory server hello done <PLACE_HOLDER>,upcoming states . add ( hs_server_hello_done ) ;,hello done signal
the main wf and the sub wf should be in running <PLACE_HOLDER>,workflow = workflow execution service . get execution status ( workflow id @$ true ) ; assert not null ( workflow ) ; assert equals ( running @$ workflow . get status ( ) ) ; assert equals ( __num__ @$ workflow . get tasks ( ) . size ( ) ) ; assert equals ( correlation id @$ workflow . get correlation id ( ) ) ; assert equals ( __str__ @$ workflow . get input ( ) . get ( __str__ ) ) ; assert equals ( __str__ @$ workflow . get input ( ) . get ( __str__ ) ) ; sub workflow = workflow execution service . get execution status ( sub workflow id @$ true ) ; assert not null ( sub workflow,wf running state
any application must override <PLACE_HOLDER> of the package declaration methods .,package declaration ( off @$ nm . id ) ;,application override one
update scripts using the now complete goog module namespaces global state and unspool the script <PLACE_HOLDER> that were queued up by all the recording .,for ( node c : script nodes ) { push script ( script descriptions . remove first ( ) ) ; if ( ! c . is from externs ( ) || node util . is from type summary ( c ) ) { node traversal . traverse ( compiler @$ c @$ new script updater ( ) ) ; } pop script ( ) ; } declare synthetic externs ( ) ;,scripts namespaces changes
bitwise or combines the sign bits so any negative value fails the <PLACE_HOLDER> .,if ( ( index | size | bytes . length - index - size ) < __num__ ) { throw new array index out of bounds exception ( string . format ( __str__ @$ bytes . length @$ index @$ size ) ) ; } int offset = index ; final int limit = offset + size ;,value fails check
the following flags reduce startup <PLACE_HOLDER> and are acceptable only for dev purposes,args . add ( __str__ ) ; if ( ! is preventnoverify ( ) ) { args . add ( __str__ ) ; },flags reduce time
simple check to make sure things lo<PLACE_HOLDER> <PLACE_HOLDER> ...,for ( table catalog_tbl : catalog_db . get tables ( ) ) { string builder sb = new string builder ( ) ; catalog schema tools . to schema ( sb @$ catalog_tbl @$ null @$ false @$ null @$ null ) ; string sql = sb . to string ( ) ; assert true ( sql . starts with ( __str__ + catalog_tbl . get type name ( ) ) ) ; for ( column catalog_col : catalog_tbl . get columns ( ) ) { assert true ( sql . index of ( catalog_col . get type name ( ) ) != - __num__ ) ; } for ( constraint catalog_const : catalog_tbl . get constraints ( ) ) { constraint type const_type = constraint type .,things look ok
wild card ipv 6 single broadcast ipv 6 <PLACE_HOLDER> : fe 80 : xx : xx ... loopback ipv 6 <PLACE_HOLDER> site local ipv 6 <PLACE_HOLDER> : fec 0 : xx : xx ...,if ( inet addr . is any local address ( ) || inet addr . is link local address ( ) || inet addr . is loopback address ( ) || inet addr . is site local address ( ) ) { return true ; },broadcast ipv address
and calculate the difference . expandable will have <PLACE_HOLDER> .,if ( child instanceof i sectionable && has header ( child ) ) { i header header = get header of ( child ) ; if ( ! ( header instanceof i expandable ) ) { return get global position of ( child ) - get global position of ( header ) - __num__ ; } } return get siblings of ( child ) . index of ( child ) ;,expandable have priority
we deal with xml <PLACE_HOLDER> get xml <PLACE_HOLDER>,if ( meta . getxml source file ( ) ) { file object xmlfile validator = kettlevfs . get file object ( xml fieldvalue ) ; if ( xmlfile validator == null || ! xmlfile validator . exists ( ) ) { log error ( base messages . get string ( pkg @$ __str__ @$ xml fieldvalue ) ) ; throw new kettle step exception ( base messages . get string ( pkg @$ __str__ @$ xml fieldvalue ) ) ; } sourcexml = new stream source ( xmlfile validator . get content ( ) . get input stream ( ) ) ; },file get source
since the overlay panel manager can show <PLACE_HOLDER> without request panel show being called @$ the flag for the panel being shown should be set to true here .,m panel shown = true ; super . peek panel ( reason ) ;,manager show widgets
create child creates the entire component <PLACE_HOLDER>,component root = element == null ? null : design context . read design ( element ) ;,child creates root
caller must call strip html <PLACE_HOLDER> on passed val,string [ ] split = val . split ( __str__ @$ __num__ ) ; if ( split . length != __num__ ) { return null ; } string mid = split [ __num__ ] ; val = split [ __num__ ] ; string csum = long . to string ( utils . field checksum ( val ) ) ; list < long > nids = new array list < > ( ) ; cursor cur = null ; try { cur = m col . get db ( ) . get database ( ) . query ( __str__ @$ new string [ ] { mid @$ csum } ) ; long nid = cur . get long ( __num__ ) ; string flds = cur . get string,caller call text
do n't let cp initialization errors kill the <PLACE_HOLDER>,if ( this . cp host != null ) { try { this . cp host . post start master ( ) ; } catch ( io exception ioe ) { log . error ( __str__ @$ ioe ) ; } },errors kill master
the system apk may have been updated with an older version of the one on the data partition @$ but which granted a new system <PLACE_HOLDER> that it did n't have before . in this case we do want to allow the app to now get the new <PLACE_HOLDER> if the ancestral apk is privileged to get it .,if ( disabled ps != null && disabled pkg != null && is package requesting permission ( disabled pkg @$ perm ) && ( ( privileged permission && disabled ps . is privileged ( ) ) || ( oem permission && disabled ps . is oem ( ) && can grant oem permission ( disabled ps @$ perm ) ) ) ) { allowed = true ; },which granted permission
we catch any kind of problem here . some linux distros have <PLACE_HOLDER> accessing the clipboard .,return new data flavor [ __num__ ] ;,distros have issues
construct the build <PLACE_HOLDER> to build the binary jar .,immutable set < java library > transitive classpath deps = java library classpath provider . get classpath deps ( params . get build deps ( ) ) ; immutable set < source path > transitive classpaths = java library classpath provider . get classpaths from libraries ( transitive classpath deps ) ; java binary java binary = new java binary ( binary build target @$ project filesystem @$ params . copy appending extra deps ( transitive classpath deps ) @$ java options . apply ( build target . get target configuration ( ) ) . get java runtime launcher ( graph builder @$ build target . get target configuration ( ) ) @$ args . get main class ( ) . or else ( null ) @$ args,the build rule
mpi is tracking <PLACE_HOLDER> per partition hsid . we need to make sure we write ours into the message getting sent to the mpi,resp . set executor site id ( m_mailbox . geths id ( ) ) ; m_mailbox . send ( counter . m_destination id @$ resp ) ; voltdb . crash global voltdb ( __str__ @$ true @$ null ) ;,mpi tracking props
no parameter @$ just send <PLACE_HOLDER>,if ( params == null || params . size ( ) == __num__ ) return o common const . empty_byte_array ;,parameter send 0
the long press on the original item should not have triggered a long press on the item touch helper . this check is a stand in for a better way to see if the item touch helper 's internal long press callback method was called . m callback.m has drag <PLACE_HOLDER> will only be 0 if the item touch helper 's long press call,assert equals ( __num__ @$ m callback . m has drag flag . size ( ) ) ;,callback.m has flag
by some reason many hive errors have this sql <PLACE_HOLDER>,if ( sql state . sql_08s01 . get code ( ) . equals ( sql state ) ) { return error type . normal ; },errors have state
we 're letting the window listener take <PLACE_HOLDER> of this,this . set default close operation ( j frame . do_nothing_on_close ) ;,listener take care
the first entry provides turn <PLACE_HOLDER> for car and foot only,assert equals ( __num__ @$ cartc enc . get decimal ( false @$ tc flags ) @$ __num__ ) ; assert equals ( __num__ @$ trucktc enc . get decimal ( false @$ tc flags ) @$ __num__ ) ; assert true ( double . is infinite ( biketc enc . get decimal ( false @$ tc flags ) ) ) ; tc storage . read flags ( tc flags @$ gh utility . get edge ( graph @$ __num__ @$ __num__ ) . get edge ( ) @$ __num__ @$ gh utility . get edge ( graph @$ __num__ @$ __num__ ) . get edge ( ) ) ; assert equals ( __num__ @$ cartc enc . get decimal ( false @$ tc flags ) @$ __num__,entry provides flags
the user asked for <PLACE_HOLDER> to be collected . some <PLACE_HOLDER> like number of rows require a scan of the data however @$ some other <PLACE_HOLDER> @$ like number of files @$ do not require a complete scan update the <PLACE_HOLDER> which do not require a complete scan .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },scan update stats
empty point raises <PLACE_HOLDER>,assert invalid multi point ( __str__ @$ __str__ ) ; assert invalid multi point ( __str__ @$ __str__ @$ __str__ ) ;,point raises exception
a new occupant has joined the <PLACE_HOLDER>,if ( ! is user status modification ) { for ( participant status listener listener : participant status listeners ) { listener . joined ( from ) ; } },occupant joined room
now do the 1 big <PLACE_HOLDER> .,file . seek ( wb . offset ) ; if ( max stat > __num__ ) { if ( stat idx < max stat ) { stats [ stat idx ++ ] = sequence . get length ( ) ; } else { long all = __num__ ; for ( ; stat idx > __num__ ; ) { all += stats [ -- stat idx ] ; } system . err . println ( __str__ + all / max stat ) ; } } file . write ( sequence . get data ( ) @$ sequence . get offset ( ) @$ sequence . get length ( ) ) ; replication target replication target = journal . get replication target ( ) ; if ( replication target !=,now do write
ze time expect the <PLACE_HOLDER> in 100 increment,int steps = activity user . get steps goal ( ) / __num__ ;,time expect step
check am 2 get the nm <PLACE_HOLDER> from am 1 .,assert . assert equals ( expectednm tokens . size ( ) @$ register response . getnm tokens from previous attempts ( ) . size ( ) ) ; for ( int i = __num__ ; i < expectednm tokens . size ( ) ; i ++ ) { assert . assert true ( expectednm tokens . get ( i ) . equals ( register response . getnm tokens from previous attempts ( ) . get ( i ) ) ) ; },2 get tokens
consumer ca n't see the odd producer <PLACE_HOLDER>,if ( e == null ) { if ( index != lv producer index ( ) ) { e = spin wait for element ( buffer @$ offset ) ; } else { return null ; } },consumer see index
each test must set the schema access <PLACE_HOLDER> and schema @$ and enable the writer cs,runner . set property ( jolt transform record . record_writer @$ __str__ ) ;,test set scope
load should also clear <PLACE_HOLDER>,binder . read bean ( person ) ; assert null ( name field . get component error ( ) ) ;,load clear these
once the repl load is successful @$ the this config should be unset or else @$ the subsequent repl load will also drop those <PLACE_HOLDER> which will cause data loss .,load with clause = collections . empty list ( ) ;,load drop tables
we currently do not validate the elements on the ancestors @$ assuming they 've already been validated . this also means some checks such as unique ids might not be check all <PLACE_HOLDER> .,for ( annotation handler annotation handler : environment . get handlers ( ) ) { if ( ! annotation handler . is enabled ( ) ) { continue ; } string validator simple name = annotation handler . get class ( ) . get simple name ( ) ; string annotation name = annotation handler . get target ( ) ; set < ? extends element > annotated elements = extracted model . get root annotated elements ( annotation name ) ; set < element > validated annotated elements = new linked hash set < > ( ) ; validating holder . put root annotated elements ( annotation name @$ validated annotated elements ) ; if ( ! annotated elements . is empty ( ) ) { logger,checks check others
skip children which already have their <PLACE_HOLDER> assigned,if ( current child . get type ( ) == null ) { if ( current child . jjt get last token ( ) . to string ( ) . equals ( __str__ ) ) { if ( previous child != null ) { current child . set type definition ( previous child . get type definition ( ) ) ; } else { ast class or interface declaration type declaration = current child . get first parent of type ( ast class or interface declaration . class ) ; if ( type declaration != null ) { current child . set type definition ( type declaration . get type definition ( ) ) ; } } } else if ( current child . jjt get last token,which have types
this branch coordinates fragment task or completed transaction task @$ holds the <PLACE_HOLDER> until all the sites on the node receive the task . task with newer sp handle will,if ( task . need coordination ( ) && m_scoreboard enabled ) { coordinated task queue offer ( task ) ; } else { task queue offer ( task ) ; },task holds lock
extract the resource context that contains projection information for root object <PLACE_HOLDER> @$ metadata and paging .,final resource context resource context = routing result . get context ( ) ;,information object factory
update the keys in vm 0 until the entry <PLACE_HOLDER> rolls over . this means that if we did a conflict check @$ vm 0 's key will have a lower entry <PLACE_HOLDER> than vm 1 @$ which would cause us to prefer vm 1 's value,vm0 . invoke ( ( ) -> { internal region region = ( internal region ) get cache ( ) . get region ( region name ) ; region . put ( __num__ @$ __str__ ) ; region entry region entry = region . get region entry ( __num__ ) ; version tag tag = region entry . get version stamp ( ) . as version tag ( ) ; tag . set entry version ( tag . get entry version ( ) - __num__ ) ; region entry . get version stamp ( ) . set versions ( tag ) ; } ) ;,key have id
produced properties match relevant <PLACE_HOLDER>,global properties gp = new global properties ( ) ; gp . set hash partitioned ( new field list ( __num__ ) ) ; local properties lp = local properties . for grouping ( new field list ( __num__ @$ __num__ ) ) ; requested global properties rgp = new requested global properties ( ) ; rgp . set hash partitioned ( new field list ( __num__ ) ) ; requested local properties rlp = new requested local properties ( ) ; rlp . set grouped fields ( new field list ( __num__ ) ) ; to join1 . set required global props ( rgp ) ; to join1 . set required local props ( rlp ) ; to join1 . set ship strategy ( ship strategy type,properties match input
register used <PLACE_HOLDER> and fields before canonicalization can optimize them .,register used elements ( ) ; canonicalizer phase . create ( ) . apply ( graph @$ bb . get providers ( ) ) ;,register used items
perform a topological sort which returns the state descriptor <PLACE_HOLDER> in their priority .,list < graph vertex < class < ? extends state descriptor > > > sorted successors = topological sort ( preference graph . values ( ) ) ;,which returns predecessors
all symbols that came from goog.module are collected separately because they will have to be processed first . see <PLACE_HOLDER> below .,list < symbol > types = new array list < > ( ) ; list < symbol > goog module export types = new array list < > ( ) ; list < symbol > module types = new array list < > ( ) ;,symbols see comments
provide compatibility with legacy applications which may pass boolean <PLACE_HOLDER> in bind args .,if ( arg instanceof boolean ) { native bind long ( m connection ptr @$ statement ptr @$ i + __num__ @$ ( ( boolean ) arg ) . boolean value ( ) ? __num__ : __num__ ) ; } else { native bind string ( m connection ptr @$ statement ptr @$ i + __num__ @$ arg . to string ( ) ) ; },which pass values
pdx @$ alias @$ nested <PLACE_HOLDER>,return new object [ ] { new object [ ] { __str__ @$ true @$ true @$ true } @$ new object [ ] { __str__ @$ true @$ true @$ false } @$ new object [ ] { __str__ @$ true @$ false @$ true } @$ new object [ ] { __str__ @$ true @$ false @$ false } @$ new object [ ] { __str__ @$ false @$ true @$ true } @$ new object [ ] { __str__ @$ false @$ true @$ false } @$ new object [ ] { __str__ @$ false @$ false @$ true } @$ new object [ ] { __str__ @$ false @$ false @$ false } @$ new object [ ] { __str__ @$ true @$ true,pdx nested queries
call the g rpc echo service <PLACE_HOLDER>,echo . echo response response = echo service grpc . new blocking stub ( channel ) . echo ( echo . echo request . new builder ( ) . set message ( __str__ ) . build ( ) ) ; assert that ( response . get message ( ) @$ is ( __str__ ) ) ; ( ( managed channel ) channel ) . shutdown ( ) . await termination ( __num__ @$ time unit . seconds ) ;,rpc echo api
transfer the location <PLACE_HOLDER> from the incoming event to the event factory so that the event we ask it to generate for us has the same location <PLACE_HOLDER>,xml event factory . set location ( start element . get location ( ) ) ; return xml event factory . create start element ( new q name ( mapping xsd support . instance . hbm xsd ( ) . get namespace uri ( ) @$ start element . get name ( ) . get local part ( ) ) @$ start element . get attributes ( ) @$ target namespaces . iterator ( ) ) ;,event has info
second dfs client does renew <PLACE_HOLDER>,final dfs client mock client2 = create mock client ( ) ; mockito . do return ( true ) . when ( mock client2 ) . renew lease ( ) ; assert same ( renewer @$ lease renewer . get instance ( fake_authority @$ fake_ugi_a @$ mock client2 ) ) ; renewer . put ( mock client2 ) ;,client renew lease
note : m result handler uses main <PLACE_HOLDER> @$ so this must not be blocked .,m result handler = new handler ( context . get main looper ( ) ) ; m next seq number = __num__ ; m pending commands = new array map < > ( ) ; m requested command seq numbers = new array set < > ( ) ; boolean connect requested ; if ( token . get type ( ) == type_session ) { m service connection = null ; connect requested = request connect to session ( connection hints ) ; } else { m service connection = new session service connection ( connection hints ) ; connect requested = request connect to service ( ) ; } if ( ! connect requested ) { close ( ) ; },handler uses loops
orc file does n't have <PLACE_HOLDER> by linkedin 's convention .,return compare iterators ( keys1 @$ keys2 @$ ( key1 @$ key2 ) -> { if ( ! remove extension ( key1 ) . equals ( key2 ) ) { log . error ( string . format ( __str__ @$ key1 @$ key2 ) ) ; return false ; } orc row iterator it1 = expected . get ( key1 ) ; orc row iterator it2 = observed . get ( key2 ) ; if ( ! it1 . get type info ( ) . equals ( it2 . get type info ( ) ) ) { log . error ( string . format ( __str__ @$ key1 @$ key2 ) ) ; return false ; } boolean result = true ; while ( it1 . has next,file have extension
someone else won the <PLACE_HOLDER> and created the file,throw x ;,someone won race
ui 1 has not yet been added in ui.init where logging takes <PLACE_HOLDER>,assert equals ( __str__ @$ get log row ( __num__ ) ) ; string url = get testurl ( getui class ( ) ) . replace ( __str__ @$ __str__ ) ; driver . get ( url ) ;,logging takes place
<PLACE_HOLDER> dot <PLACE_HOLDER>,for ( int j = __num__ ; j < dimension ; j ++ ) { gg += g [ j ] * g [ j ] ; dgg += ( xi [ j ] + g [ j ] ) * xi [ j ] ; },g dot g
client 1 has two <PLACE_HOLDER> to secret 1,acldao . enroll client ( jooq context . configuration ( ) @$ client1 . get id ( ) @$ group1 . get id ( ) ) ; acldao . enroll client ( jooq context . configuration ( ) @$ client1 . get id ( ) @$ group2 . get id ( ) ) ; acldao . allow access ( jooq context . configuration ( ) @$ secret1 . get id ( ) @$ group1 . get id ( ) ) ; acldao . allow access ( jooq context . configuration ( ) @$ secret1 . get id ( ) @$ group2 . get id ( ) ) ; set < sanitized secret > secret = acldao . get sanitized secrets for ( client1 ) ; assert that (,client has access
in case this worker has no <PLACE_HOLDER> on this node,if ( left > right ) { log . debug ( __str__ + nid ) ; this . node pos start [ __num__ * nid + __num__ ] = left ; this . node pos end [ __num__ * nid + __num__ ] = right ; log . debug ( string . format ( __str__ @$ __num__ * nid + __num__ @$ left @$ right ) ) ; this . node pos start [ __num__ * nid + __num__ ] = left ; this . node pos end [ __num__ * nid + __num__ ] = right ; log . debug ( string . format ( __str__ @$ __num__ * nid + __num__ @$ left @$ right ) ) ; return ; },worker has work
first time store has one <PLACE_HOLDER> file @$ next id will be 2,mockito . do answer ( ans ) . when ( m store ) . roll writer ( __num__ ) ;,store has record
since nm 's and rm 's token sequence no is different @$ response should contain system <PLACE_HOLDER> for apps,assert equals ( __num__ @$ response1 . get token sequence no ( ) ) ; assert equals ( __num__ @$ response1 . get system credentials for apps ( ) . size ( ) ) ; resource tracker service . close ( ) ;,response contain credentials
this will fail for all users currently has <PLACE_HOLDER> verified,if ( check valid string ( signal username ) && signal username . equals ( preferences . get verified signal username ( ) ) ) { find preference ( preference manager . verify_signal ) . set summary ( r . string . verification_dialog_summary_verified ) ; } else { find preference ( preference manager . verify_signal ) . set summary ( r . string . verification_dialog_summary ) ; },users has signal
user 1 creates the <PLACE_HOLDER>,try { muc = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc . create ( __str__ ) ; form form = new form ( form . type_submit ) ; form field field = new form field ( __str__ ) ; field . set type ( __str__ ) ; form . add field ( field ) ; form . set answer ( __str__ @$ arrays . as list ( __str__ ) ) ; muc . send configuration form ( form ) ; } catch ( exception e ) { e . print stack trace ( ) ; fail ( e . get message ( ) ) ; },user creates roomnick
kill zoo keeper 0 @$ removing monitor <PLACE_HOLDER> .,try { fail site ( __num__ ) ; } catch ( exception e ) { fail ( ) ; } while ( monitor for manager4 . m_members . size ( ) != __num__ && ! monitor for manager4 . has identical membership ( monitor for manager2 ) ) { thread . yield ( ) ; },removing monitor itself
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( test index scan count suite . class ) ;,suite using tests
any application must override <PLACE_HOLDER> of the define field methods .,identifier arg ids [ ] = null ; identifier exp ids [ ] = null ; if ( args != null ) { arg ids = new identifier [ args . length ] ; for ( int i = __num__ ; i < args . length ; i ++ ) { arg ids [ i ] = args [ i ] . id ; } } if ( exp != null ) { exp ids = new identifier [ exp . length ] ; for ( int i = __num__ ; i < exp . length ; i ++ ) { exp ids [ i ] = exp [ i ] . id ; } } define field ( where @$ doc @$ mod @$ t @$ nm,application override one
at normal playback speed @$ we stop buffering when the buffer reaches the <PLACE_HOLDER> .,assert that ( load control . should continue loading ( min_buffer_us @$ speed ) ) . is false ( ) ;,buffer reaches end
bypass get transaction <PLACE_HOLDER> .,set transaction type ( cardio2e transaction types . get ) ; super . set object type ( cardio2e object types . zones_bypass ) ; set object number ( object number ) ;,bypass get type
add column families build a <PLACE_HOLDER> of keys,list < column family descriptor builder > family builders = new array list < > ( ) ; sorted map < byte [ ] @$ integer > map = new tree map < > ( bytes . bytes_comparator ) ; visit bulkh files ( fs @$ hfof dir @$ new bulkh file visitor < column family descriptor builder > ( ) { @ override public column family descriptor builder bulk family ( byte [ ] family name ) { column family descriptor builder builder = column family descriptor builder . new builder ( family name ) ; family builders . add ( builder ) ; return builder ; } @ override public void bulkh file ( column family descriptor builder builder @$ file status hfile status ) throws,families build list
check for offline members @$ if the region has <PLACE_HOLDER> even if we intend to replace offline data @$ we still need to make sure the bucket is n't completely offline,if ( ! replace offline data || redundancy == - __num__ ) { bucket persistence advisor persist advisor = get persistence advisor ( ) ; if ( persist advisor != null ) { if ( ! persist advisor . was hosting ( ) && advisor . get had primary ( ) ) { final persistent membership view membership view = persist advisor . get membership view ( ) ; if ( membership view == null ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . partitioned region . getpr id ( ) @$ partitioned region . bucket_id_separator @$ bid ) ; } return false ; } set < persistent memberid > offline members = membership view .,region has redundancy
per javadoc log exceptions but still go online . note : this does not include errors @$ which indicate a fatal <PLACE_HOLDER>,logger . log ( warning @$ string . format ( __str__ @$ cl . get class ( ) ) @$ e ) ;,which indicate problem
get the app log aggregation impl <PLACE_HOLDER> to crash,local dirs handler service mocked dir svc = mock ( local dirs handler service . class ) ; log aggregation service log aggregation service = new log aggregation service ( dispatcher @$ this . context @$ del srvc @$ mocked dir svc ) ; log aggregation service . init ( this . conf ) ; log aggregation service . start ( ) ; application id application1 = builder utils . new application id ( __num__ @$ __num__ ) ; log aggregation service . handle ( new log handler app started event ( application1 @$ this . user @$ null @$ this . acls ) ) ; log aggregation service . handle ( new log handler app finished event ( application1 ) ) ; dispatcher . await ( ),app log thread
invoke the method to have the subclass write <PLACE_HOLDER>,try { write entities ( tl conf @$ manager @$ context ) ; } finally { manager . close ( ) ; },subclass write itself
depth first search from root directory for all application log <PLACE_HOLDER>,remote iterator < file status > iter = list ( dirpath ) ; while ( iter . has next ( ) ) { file status stat = iter . next ( ) ; path child path = stat . get path ( ) ; if ( stat . is directory ( ) ) { application id app id = parse application id ( child path . get name ( ) ) ; if ( app id != null ) { app log dir present . set true ( ) ; if ( should clean app log dir ( child path @$ now @$ fs @$ retain millis ) ) { delete dir ( child path ) ; } } else { clean app log dir ( child path,search log files
double precision only has 52 <PLACE_HOLDER> of mantissa,if ( level > __num__ ) return point crossings for line ( px @$ py @$ x0 @$ y0 @$ x1 @$ y1 ) ; double xmid = ( xc0 + xc1 ) / __num__ ; double ymid = ( yc0 + yc1 ) / __num__ ; xc0 = ( x0 + xc0 ) / __num__ ; yc0 = ( y0 + yc0 ) / __num__ ; xc1 = ( xc1 + x1 ) / __num__ ; yc1 = ( yc1 + y1 ) / __num__ ; double xc0m = ( xc0 + xmid ) / __num__ ; double yc0m = ( yc0 + ymid ) / __num__ ; double xmc1 = ( xmid + xc1 ) / __num__ ; double ymc1 = ( ymid + yc1 ),precision has bits
godin : note that other channels use method <PLACE_HOLDER> in order to do the same thing,tmp builder . set length ( __num__ ) ; return true ;,channels use index
test case 8 : 2 components @$ 2 instances for each comp 2 already <PLACE_HOLDER>ed . comp 1 has a new instance <PLACE_HOLDER> @$ we should terminate the service,comp = create component ( service scheduler @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component1 instances = comp . get all component instances ( ) ; container status . set exit status ( - __num__ ) ; component comp2 = create component ( component instance . get component ( ) . get scheduler ( ) @$ org . apache . hadoop . yarn . service . api . records . component . restart policy enum . never @$ __num__ @$ __num__ ) ; collection < component instance > component2 instances = comp2 . get all component instances ( ) ; map <,components has finish
check this tree supports this <PLACE_HOLDER> by having properly populated core label 's,list < tree > leaves = this . get leaves ( ) ; if ( ! ( leaves . get ( __num__ ) . label ( ) instanceof core label ) ) { throw new illegal argument exception ( __str__ ) ; } else if ( ( ( core label ) leaves . get ( __num__ ) . label ( ) ) . word ( ) == null ) { throw new illegal argument exception ( __str__ ) ; } else if ( ( ( core label ) leaves . get ( __num__ ) . label ( ) ) . after ( ) == null ) { throw new illegal argument exception ( __str__ ) ; } list < core label > core labels = this . get,tree supports concept
inflater likes a <PLACE_HOLDER> of slack,long size = get entry size ( jzentry ) + __num__ ;,inflater likes depth
check whether the buffer contains the <PLACE_HOLDER> to be read,if ( cur < bar ) { pos ++ ; return buff [ cur ] ; },buffer contains portion
has <PLACE_HOLDER> collection that we get back is immutable so we keep track of which <PLACE_HOLDER>s need to be deleted after they 've been folded into the janus graph step and then remove them from their step using has <PLACE_HOLDER>.remove has <PLACE_HOLDER>,final list < has container > removable has containers = new array list < > ( ) ; final set < string > step labels = current step . get labels ( ) ; has container holder . get has containers ( ) . for each ( has container -> { if ( graph step . process has container ids ( graph step @$ has container ) ) { step labels . for each ( janusgraph step :: add label ) ; removable has containers . add ( has container ) ; } } ) ;,collection has container
do n't calculate cost if the server does n't have enough <PLACE_HOLDER> or is loading the segment,if ( proposal segment size > server . get available size ( ) || server . is loading segment ( proposal segment ) ) { return double . positive_infinity ; },server have space
do we need to pad the file so the stripe does n't straddle a block <PLACE_HOLDER> ?,long start = raw writer . get bytes written ( ) ; final long current stripe size = index size + data size + footer . get serialized size ( ) ; final long available = block size - ( start % block size ) ; final long overflow = current stripe size - adjusted stripe size ; final float avail ratio = ( float ) available / ( float ) default stripe size ; if ( avail ratio > __num__ && avail ratio < __num__ && avail ratio > padding tolerance ) { float correction = overflow > __num__ ? ( float ) overflow / ( float ) adjusted stripe size : __num__ ; correction = correction > padding tolerance ? padding tolerance : correction ; adjusted,stripe straddle boundary
no alias to stage.. no local <PLACE_HOLDER>,if ( localwork . get alias to fetch work ( ) . is empty ( ) ) { new local work . set has staged alias ( false ) ; curr task . set backup task ( local task . get backup task ( ) ) ; curr task . set backup children tasks ( local task . get backup children tasks ( ) ) ; return ; },alias stage.. work
unless an accept <PLACE_HOLDER> parameter is explicitly set to false @$ respond that this server accepts <PLACE_HOLDER>,if ( ! boolean . to string ( false ) . equals ignore case ( request . get parameter ( __str__ ) ) ) { response . set header ( __str__ @$ __str__ ) ; },server accepts responses
we need to check if the class has a custom timer <PLACE_HOLDER>,return class timed == null || class timed . name ( ) . is empty ( ) ? metric registry . name ( clazz @$ method name ) : metric registry . name ( class timed . name ( ) @$ method name ) ;,class has name
element consumed <PLACE_HOLDER> if policy element found,return false ;,element consumed check
we need to run the subprocedure even if we have no relevant regions . the coordinator expects <PLACE_HOLDER> in the procedure and without sending message the snapshot attempt will hang and fail .,log . debug ( __str__ + snapshot . get name ( ) + __str__ + snapshot . get table ( ) + __str__ + snapshot . get type ( ) ) ; foreign exception dispatcher exn dispatcher = new foreign exception dispatcher ( snapshot . get name ( ) ) ; configuration conf = rss . get configuration ( ) ; long timeout millis = conf . get long ( snapshot_timeout_millis_key @$ snapshot_timeout_millis_default ) ; long wake millis = conf . get long ( snapshot_request_wake_millis_key @$ snapshot_request_wake_millis_default ) ; switch ( snapshot . get type ( ) ) { case flush : snapshot subprocedure pool task manager = new snapshot subprocedure pool ( rss . get server name ( ) . to string ( ) @$ conf @$,coordinator expects participation
this method will only return full resource <PLACE_HOLDER> when activating one @$ to give the caller the result atomically with the activation .,wm full resource plan full plan after alter = getms ( ) . alter resource plan ( request . get resource plan name ( ) @$ request . get ns ( ) @$ request . get resource plan ( ) @$ request . is is enable and activate ( ) @$ request . is is force deactivate ( ) @$ request . is is replace ( ) ) ; if ( full plan after alter != null ) { response . set full resource plan ( full plan after alter ) ; } return response ;,method return plan
if the filter does not need bound <PLACE_HOLDER> @$ execute query using full engine,if ( ! needs bound value ( filter expression ) ) { materialized result result = runner . execute ( __str__ + filter ) ; assert equals ( result . get types ( ) . size ( ) @$ __num__ ) ; boolean query result ; if ( result . get materialized rows ( ) . is empty ( ) ) { query result = false ; } else { assert equals ( result . get materialized rows ( ) . size ( ) @$ __num__ ) ; query result = ( boolean ) iterables . get only element ( result . get materialized rows ( ) ) . get field ( __num__ ) ; } results . add ( query result ) ; } return results ;,filter need values
is ok when considering two zero duration inervals this is the simplest case @$ as the two intervals either have a <PLACE_HOLDER> or not if not @$ then they are equal and abut,interval test0808 = new interval ( __num__ @$ __num__ ) ; interval test1010 = new interval ( __num__ @$ __num__ ) ;,intervals have value
this intentionally does disconnect without <PLACE_HOLDER>ing the vaadin session to avoid dead<PLACE_HOLDER>s where the server uses a <PLACE_HOLDER> for the websocket connection,new thread ( ( ) -> { set push connection ( null ) ; } ) . start ( ) ;,server uses lock
if the decision point list contains <PLACE_HOLDER> of the parent rows @$ update it to include the new row as well,if ( ( decision point list . contains ( new integer ( old row num ) ) || decision point list . contains ( new integer ( new row num ) ) ) && ! decision point list . contains ( new integer ( combined row num ) ) ) { decision point list . add element ( new integer ( combined row num ) ) ; },list contains any
we know that the parameters have the same <PLACE_HOLDER> @$ since this is what the descriptor 's hash code & equality are based on . the only thing that may be different is the description . and since the proposed parameter does not have a description @$ we want to use whatever is currently set .,return old parameter == null ? descriptor : old parameter . get descriptor ( ) ;,parameter have description
when tag compression is been used in this file @$ tag compression context will have a not null <PLACE_HOLDER> passed .,if ( tag compression context != null ) { tag compression context . uncompress tags ( source @$ dest @$ tags length ) ; } else { byte buffer utils . copy from stream to buffer ( dest @$ source @$ tags length ) ; },context passed value
we need to pass the meta from the pod in the deployment as that is what matches machine <PLACE_HOLDER>,final object meta template meta = to create . get spec ( ) . get template ( ) . get metadata ( ) ; store starting machine ( created pod @$ template meta @$ machine configs @$ server resolver ) ;,what matches config
first describe what is taking a long <PLACE_HOLDER> .,switch ( m_fragment context ) { default : case ro_batch : case rw_batch : sb . append ( __str__ + m_current procedure name ) ; break ; case catalog_update : sb . append ( __str__ ) ; break ; case catalog_load : sb . append ( __str__ ) ; break ; },what taking time
attention : only update the exist <PLACE_HOLDER> for sorted storage method,double [ ] values ; if ( vector storage utils . use int key ( vector ) ) { values = get int double vector ( ) . get storage ( ) . get values ( ) ; } else { values = get long double vector ( ) . get storage ( ) . get values ( ) ; } for ( int i = __num__ ; i < values . length ; i ++ ) { values [ i ] = func . update ( ) ; },attention exist values
map types attribute as key enum no <PLACE_HOLDER> as value,ret . add ( get attribute def ( attr name prefix @$ atlas base type def . get map type name ( attribute type @$ enum_def_with_no_default . get name ( ) ) ) ) ;,types attribute default
get transform record ; if no transform is found @$ will throw illegal argument <PLACE_HOLDER>,transform record info = user record . m transform records . get resource or throw ( resource id ) ;,record throw exception
the status from error response overwrites the one in update response . however @$ results and errors are not expected to have overlapping key . see batch update response <PLACE_HOLDER> .,if ( error data != null ) { update data . put ( __str__ @$ error data . get ( __str__ ) ) ; update data . put ( __str__ @$ error data ) ; },batch update tag
remember that these elements got <PLACE_HOLDER>,for ( element element : instance . non zeroes ( ) ) { int j = element . index ( ) ; update steps . set quick ( j @$ get step ( ) ) ; update counts . increment quick ( j @$ __num__ ) ; } next step ( ) ;,elements got updated
remove the linkstamp objects from inputs so that create linkstamp compile <PLACE_HOLDER> does n't cause a circular dependency .,immutable set < artifact > expanded linker artifacts no linkstamps = sets . difference ( expanded linker artifacts @$ linkstamp object artifacts ) . immutable copy ( ) ; cc toolchain variables variables ; try { immutable list . builder < string > user link flags = immutable list . < string > builder ( ) . add all ( linkopts ) . add all ( cpp configuration . get linkopts ( ) ) ; if ( is lto indexing && cpp configuration . use standalone lto indexing command lines ( ) ) { user link flags . add all ( cpp configuration . get lto index options ( ) ) ; } variables = link build variables . setup variables ( get link type ( ) .,linkstamp compile rule
file tree file name iterator returns all paths in the directory tree which file name is build file name <PLACE_HOLDER> out build file name to get a path pointing to package root folder all paths are relative,package roots = immutable sorted set . copy of ( iterators . transform ( file tree file name iterator . of ( file tree @$ build file name ) @$ path -> more paths . get parent or empty ( path ) ) ) ; break ; default : throw new illegal state exception ( ) ;,which file strip
the window manager will give us a valid window <PLACE_HOLDER>,if ( m show dialog for submenu ) { m sub menu helper = new menu dialog helper ( sub menu ) ; m sub menu helper . show ( null ) ; return true ; },manager give selector
the following should take over a <PLACE_HOLDER>,secure random rand = new secure random ( ) ; rand . next bytes ( new byte [ __num__ ] ) ;,following take thread
note : a valid authorization response contains either a <PLACE_HOLDER> ' or 'error ' parameter .,http servlet response response = mock ( http servlet response . class ) ; filter chain filter chain = mock ( filter chain . class ) ; this . filter . do filter ( request @$ response @$ filter chain ) ; verify ( filter chain ) . do filter ( any ( http servlet request . class ) @$ any ( http servlet response . class ) ) ;,response contains '
the caller specified a data <PLACE_HOLDER>,for ( string datanode uuid : data node uuids ) { boolean found = false ; for ( int index = __num__ ; index < info . length ; index ++ ) { if ( info [ index ] . get datanode uuid ( ) . equals ( datanode uuid ) ) { data node names . add ( info [ index ] . get xfer addr ( ) ) ; datanode infos . add ( name node adapter . get datanode ( cluster . get namesystem ( nn index ) @$ info [ index ] ) ) ; found = true ; break ; } } if ( ! found ) { throw new io exception ( __str__ + datanode uuid ) ; } },caller specified node
note that this might miss collisions @$ use internal tick callback to check for collision on every tick . see internal tick <PLACE_HOLDER> on how to implement it .,contacts . clear ( ) ;,note see test
known previous uid @$ so we know <PLACE_HOLDER> package set to check,for ( string pkg : package names ) { hash set < string > set = m backup participants . get ( old uid ) ; if ( set != null && set . contains ( pkg ) ) { remove package from set locked ( set @$ pkg ) ; if ( set . is empty ( ) ) { if ( more_debug ) slog . v ( tag @$ __str__ ) ; m backup participants . remove ( old uid ) ; } } },package set which
make sure dn 's jmx sees the failed <PLACE_HOLDER>,final string [ ] expected failed volumes = { dn1 vol1 . get absolute path ( ) } ; data node test utils . trigger heartbeat ( dn ) ; fs dataset spi < ? > fsd = dn . getfs dataset ( ) ; assert equals ( expected failed volumes . length @$ fsd . get num failed volumes ( ) ) ; assert array equals ( expected failed volumes @$ convert to absolute paths ( fsd . get failed storage locations ( ) ) ) ;,jmx sees volume
this filter allows all <PLACE_HOLDER> to stay in the view,set filter text ( __str__ ) ; tree path first visible path = scroll to ( __num__ ) ; set filter text ( __str__ ) ; tree path new first visible path = get last visible path ( ) ; assert close enough ( first visible path @$ new first visible path ) ;,filter allows nodes
if all the others require <PLACE_HOLDER> @$ this is likely a bug,assert equals ( __str__ @$ native meta . get modify column statement ( __str__ @$ new value meta string ( __str__ @$ __num__ @$ __num__ ) @$ __str__ @$ false @$ __str__ @$ false ) ) ;,others require parameter
if any provider has been disabled @$ clear all last <PLACE_HOLDER>s for all providers . this is to be on the safe side in case a provider has <PLACE_HOLDER> derived from this disabled provider .,if ( ! m useable ) { m last location . clear ( ) ; m last location coarse interval . clear ( ) ; },provider has location
client 1 add <PLACE_HOLDER> and put all,async invocation < void > register interest and put all in client1 = client1 . invoke async ( ( ) -> { region < string @$ ticker data > region = get cache ( ) . get region ( region name ) ; region . get attributes mutator ( ) . add cache listener ( new counting cache listener < > ( ) ) ; region . register interest ( __str__ ) ; do put all ( region @$ title @$ one_hundred ) ; } ) ;,client add listener
same as above @$ except this is a bit of stress testing . attach 5 database <PLACE_HOLDER> and make sure they are all removed .,int n = __num__ ; array list < string > attached db files = new array list < string > ( n ) ; for ( int i = __num__ ; i < n ; i ++ ) { attached db files . add ( m database . get path ( ) + i ) ; } db obj = sq lite database . open or create database ( m database . get path ( ) @$ null ) ; db obj . execsql ( __str__ ) ; for ( int i = __num__ ; i < n ; i ++ ) { db obj . execsql ( __str__ + attached db files . get ( i ) + __str__ + i ) ; } assert true (,same attach files
release only tickets that have been flushed . fail the <PLACE_HOLDER> .,iterator < flush ticket > ticket iterator = m ticket set . iterator ( ) ; while ( ticket iterator . has next ( ) ) { flush ticket ticket = ticket iterator . next ( ) ; ticket iterator . remove ( ) ; if ( ticket . get target counter ( ) <= m flush counter . get ( ) ) { ticket . set completed ( ) ; } else { ticket . set error ( exc ) ; } },release fail test
since the 200 was committed @$ the 500 did not get the <PLACE_HOLDER> to be written,assert that ( __str__ @$ response . get status ( ) @$ is ( __num__ ) ) ; assert that ( __str__ @$ response . get content ( ) @$ is ( __str__ ) ) ; assuming that ( http version == http version . http_1_1 @$ ( ) -> assert that ( response @$ contains header value ( __str__ @$ __str__ ) ) ) ;,500 get chance
only append if target does not contain the <PLACE_HOLDER>,if ( ! target . contains key ( key ) ) { target . put ( key @$ entry . get value ( ) . get value ( profiles ) ) ; },target contain key
at the moment @$ web assembly only supports one memory <PLACE_HOLDER> @$ thus the only valid memory index is 0 .,assert . assert int equal ( mem index @$ __num__ @$ __str__ ) ; long data offset = __num__ ; byte instruction ; do { instruction = read1 ( ) ; switch ( instruction ) { case instructions . i32_const : data offset = read signed int32 ( ) ; break ; case instructions . global_get : read global index ( ) ; throw new wasm exception ( __str__ ) ; case instructions . end : break ; default : assert . fail ( string . format ( __str__ @$ instruction ) ) ; } } while ( instruction != instructions . end ) ;,assembly supports instruction
some adjustments to not crash the <PLACE_HOLDER> with empty data,if ( m max elements == __num__ ) { m max elements = __num__ ; } if ( m mcount == __num__ ) { m mcount = __num__ ; } if ( m first element == m last element ) { m first element = __num__ ; m last element = __num__ ; } if ( m max cards == __num__ ) m max cards = __num__ ; meta info . setm dynamic axis ( true ) ; meta info . setm has colored cumulative ( true ) ; meta info . setm type ( type ) ; meta info . setm title ( r . string . stats_forecast ) ; meta info . setm backwards ( true ) ; meta info . setm value labels ( m,adjustments crash chartbuilding
replica set <PLACE_HOLDER> with no address ...,host and port = server address . default host ( ) ;,replica set cast
if any of the declared <PLACE_HOLDER>s match the package <PLACE_HOLDER> @$ it 's valid .,for ( signature signature : provider . signatures ) { if ( signature . equals ( package info . signatures [ __num__ ] ) ) return true ; } return false ;,any match signature
negative transition @$ which makes a duplicated local time <PLACE_HOLDER>,if ( ( ( duplicated time opt & std_dst_mask ) == local_std && dst to std ) || ( ( duplicated time opt & std_dst_mask ) == local_dst && std to dst ) ) { transition += offset after ; } else if ( ( ( duplicated time opt & std_dst_mask ) == local_std && std to dst ) || ( ( duplicated time opt & std_dst_mask ) == local_dst && dst to std ) ) { transition += offset before ; } else if ( ( duplicated time opt & former_latter_mask ) == local_former ) { transition += offset before ; } else { transition += offset after ; },which makes range
join keys have difference <PLACE_HOLDER> ?,if ( k1 . size ( ) != k2 . size ( ) ) { return k1 . size ( ) - k2 . size ( ) ; } if ( comparators . length == __num__ ) { return __num__ ; },keys have sizes
last chance @$ look in the old hive config value . still avoiding <PLACE_HOLDER> .,if ( reporters to start == null ) { reporters to start = conf . get ( metastore conf . conf vars . hive_metrics_reporter . get hive name ( ) ) ; if ( reporters to start == null ) { reporters to start = metastore conf . get var ( conf @$ metastore conf . conf vars . metrics_reporters ) ; } },chance avoiding them
iterator should have an <PLACE_HOLDER> when dir is not empty,it = fs . list status iterator ( d ) ; assert true ( it . has next ( ) ) ; it . next ( ) ; assert false ( it . has next ( ) ) ; for ( int i = __num__ ; i < list limit * __num__ ; i ++ ) { p = new path ( d @$ __str__ + i ) ; assert . assert true ( fs . create new file ( p ) ) ; },iterator have item
this will be picked up by janitor to figure out what really happened and correct the state if needed note that the default case includes the null <PLACE_HOLDER>,case undefined : default : return transaction status . unknown ;,case includes value
move over a bit so the oval does n't overwrite the <PLACE_HOLDER>,int label padding = __num__ ; location . x -= label padding * __num__ ; location . y -= label padding * __num__ ;,oval overwrite label
make this loop forever in case a hang in the first client prevents the listener from ever starting . this loop should make the test <PLACE_HOLDER> and thus we 'll get a thread dump,while ( true ) { try { s = new socket ( __str__ @$ node1 port ) ; break ; } catch ( connect exception ce ) { thread . sleep ( __num__ ) ; } } input stream in = s . get input stream ( ) ; final long end = system . current time millis ( ) + __num__ ; while ( system . current time millis ( ) < end ) { log ( __str__ ) ; thread . sleep ( __num__ ) ; try { if ( in . read ( ) < __num__ ) { break ; } } catch ( io exception ioe ) { break ; } } if ( system . current time millis ( ) > end ),loop make timeout
all set ; now set up the <PLACE_HOLDER> and launch the agent,set up pipes ( ) ; m agent = m backup manager service . bind to agent synchronous ( m target app @$ full backup . key_value_data_token . equals ( info . domain ) ? application thread constants . backup_mode_incremental : application thread constants . backup_mode_restore_full ) ; m agent package = pkg ;,all set pipes
all types must have the same <PLACE_HOLDER> of children,if ( type . get children ( ) . size ( ) != number of children ) { return null ; },types have number
n.b . do n't set regexp <PLACE_HOLDER> to empty to keep regexp,this . regexp data field . set text ( __str__ ) ;,n.b set field
check file content and bucket <PLACE_HOLDER> .,map < file @$ string > contents = test utils . get file content by path ( out dir ) ; for ( map . entry < file @$ string > file contents : contents . entry set ( ) ) { integer bucket id = integer . parse int ( file contents . get key ( ) . get parent file ( ) . get name ( ) ) ; assert . assert true ( bucket id >= __num__ && bucket id <= __num__ ) ; assert . assert equals ( string . format ( __str__ @$ bucket id @$ bucket id ) @$ file contents . get value ( ) ) ; },check file ids
another listener should not have received any <PLACE_HOLDER>,assert that ( another queue . is empty ( ) @$ is ( true ) ) ; kv state registry . unregister kv state ( job id @$ job vertex id @$ key group range @$ registration name @$ kv stateid ) ; assert that ( state deregistration notifications . poll ( ) @$ equal to ( job id ) ) ;,listener received notifications
same type @$ request code and intent action implies <PLACE_HOLDER> .,assert that ( pending intent . get activity ( context @$ __num__ @$ new intent ( __str__ ) @$ flag_no_create ) ) . is same instance as ( pending intent ) ;,code implies inequality
priority 4 : find all supported java <PLACE_HOLDER> @$ and use the newest .,list < file > java home dirs = java finder . find supported java home from installations ( java config @$ java filter ) ; if ( ! java home dirs . is empty ( ) ) { java home dir = java home dirs . iterator ( ) . next ( ) ; if ( save ) { java config . save java home ( java home dir ) ; } system . out . println ( java home dir ) ; return exit_success ; } return exit_failure ;,all supported home
get the annotation associated witht the local attr <PLACE_HOLDER>,if ( attr decl . get attribute node ( schema symbols . att_ref ) == null ) { attr use . f annotations = attribute . get annotations ( ) ; } else { xs object list annotations ; if ( annotation != null ) { annotations = new xs object list impl ( ) ; ( ( xs object list impl ) annotations ) . addxs object ( annotation ) ; } else { annotations = xs object list impl . empty_list ; } attr use . f annotations = annotations ; },annotation associated node
buffers are only discarded after they are acked . discarding them here would cause the <PLACE_HOLDER> to generate too much work for the receiver .,m_future . set ( true ) ;,them cause queue
use a new redirect each time as different clients could be requesting the <PLACE_HOLDER> with different host names,final redirect handler redirect handler = new redirect handler ( location ) ; redirect handler . handle request ( exchange ) ;,time requesting url
initialize the default edit <PLACE_HOLDER> .,get view ( ) . add view mode ( m_edit mode ) ; m_edit mode listener = new internal edit mode listener < node type @$ edge type > ( m_graph listeners ) ; m_edit mode . add listener ( m_edit mode listener ) ;,default edit mode
a volt db extension to support indexed <PLACE_HOLDER> and partial indexes,expression predicate = ( expression ) arguments [ __num__ ] ; @ suppress warnings ( __str__ ) java . util . list < expression > index exprs = ( java . util . list < expression > ) arguments [ __num__ ] ; boolean assume unique = ( ( boolean ) arguments [ __num__ ] ) . boolean value ( ) ; if ( index exprs != null ) { table works . add expr index ( index columns @$ index exprs . to array ( new expression [ index exprs . size ( ) ] ) @$ name @$ unique @$ migrating @$ predicate ) . set assume unique ( assume unique ) ; break ; } org . hsqldb_voltpatches . index . index added index =,extension support expressions
check access in case the principal has no authorization <PLACE_HOLDER>,try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_general ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { } try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_auditing ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { } try { acl authorization strategy2 . security check ( acl2 @$ acl authorization strategy . change_ownership ) ; fail ( __str__ ) ; } catch ( not found exception expected ) { },principal has entries
caller did not specify any root <PLACE_HOLDER> @$ so just use our own .,if ( throwable == null ) { return new cache closed exception ( reason @$ disconnect cause ) ; },caller specify cause
delegate to the old behavior @$ allowing <PLACE_HOLDER> to override .,map to map from id ( data @$ obj ) ;,delegate allowing users
compensate for the dropped <PLACE_HOLDER>,s . request ( __num__ ) ; g . on next ( t ) ;,the dropped request
this function also starts new <PLACE_HOLDER>,scan entity reference ( f content buffer ) ;,function starts entries
snapshot directory @$ as some file system implementations do not modify the parent directory 's mod <PLACE_HOLDER> when there are new sub items @$ for example @$ s 3 .,file status [ ] snapshot dirs = fs utils . list status ( fs @$ snapshot dir @$ p -> ! p . get name ( ) . equals ( snapshot description utils . snapshot_tmp_dir_name ) ) ;,implementations modify count
ensure that committed watermarks match exactly the input <PLACE_HOLDER> because we shutdown in an orderly manner .,assert . assert true ( one record extractor . validate watermarks ( true @$ external watermark storage ) ) ;,watermarks match rows
make sure the client processes the <PLACE_HOLDER> correctly,test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( __str__ + client conn . get data events buffer ( ) . last written scn ( ) ) ; return client conn . get data events buffer ( ) . last written scn ( ) == __num__ ; } } @$ __str__ + consumer . get sequences ( ) @$ __num__ @$ log ) ; test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( __str__ + consumer . get event num ( ) ) ; return stats . get total stats ( ) . get num data,client processes response
insert fake <PLACE_HOLDER> with the correct modification time . the call should return the fake <PLACE_HOLDER>,format = new dummy file input format ( ) ; format . set file path ( temp dir ) ; format . configure ( new configuration ( ) ) ; file base statistics out dated fake stats = new file base statistics ( math . min ( math . min ( mod time1 @$ mod time2 ) @$ mod time3 ) - __num__ @$ fake_size @$ base statistics . avg_record_bytes_unknown ) ; base statistics re gathered = format . get statistics ( out dated fake stats ) ; assert . assert equals ( __str__ @$ total @$ re gathered . get total input size ( ) ) ;,call return stats
animated image drawable calls post <PLACE_HOLDER> and release only if m post <PLACE_HOLDER>or exists .,if ( decoder . m animated ) { image decoder post process ptr = decoder . m post processor == null ? null : decoder ; decoder . check state ( true ) ; drawable d = new animated image drawable ( decoder . m native ptr @$ post process ptr @$ decoder . m desired width @$ decoder . m desired height @$ decoder . get color space ptr ( ) @$ decoder . check for extended ( ) @$ src density @$ src . compute dst density ( ) @$ decoder . m crop rect @$ decoder . m input stream @$ decoder . m asset fd ) ; decoder . m input stream = null ; decoder . m asset fd = null ; return,drawable calls processor
the data is locked in the <PLACE_HOLDER> @$ or we 're ignoring the <PLACE_HOLDER> . bypass the <PLACE_HOLDER> and read from upstream .,if ( next span == null ) { next data source = upstream data source ; next data spec = new data spec ( uri @$ http method @$ null @$ read position @$ read position @$ bytes remaining @$ key @$ flags ) ; } else if ( next span . is cached ) { uri file uri = uri . from file ( next span . file ) ; long file position = read position - next span . position ; long length = next span . length - file position ; if ( bytes remaining != c . length_unset ) { length = math . min ( length @$ bytes remaining ) ; } next data spec = new data spec ( file uri @$,cache bypass timeout
eap 7 does not have the clone <PLACE_HOLDER>,profile clone eap7x ( ) ;,eap have operation
check the next check respected <PLACE_HOLDER> @$ because there 's no point of sending a request immediately only to get a 'connection refused ' error .,final stopwatch stopwatch = stopwatch . create started ( ) ; health check request logs . take ( ) ; assert that ( stopwatch . elapsed ( time unit . milliseconds ) ) . is greater than ( retry_interval . to millis ( ) * __num__ / __num__ ) ;,check respected timeout
use a custom merge <PLACE_HOLDER> for put values .,super ( ( old item @$ new item ) -> new git hub repository ( old item ) . overwrite ( new item ) @$ git hub repository :: get id @$ repository -> repository != null ? repository : git hub repository . none ( ) @$ git hub repository :: none ) ;,custom merge operation
set <PLACE_HOLDER> to holder first because asynchronous fork : :put record can pull the <PLACE_HOLDER> when it detects fork state.failed status .,fork throwable holder holder = task . get fork throwable holder ( this . broker ) ; holder . set throwable ( this . get index ( ) @$ t ) ; this . fork state . set ( fork state . failed ) ; this . logger . error ( string . format ( __str__ @$ this . index @$ this . task id @$ holder ) @$ t ) ;,record pull exception
this test can be very liberal . too liberal will just do some <PLACE_HOLDER> refreshes . too conservative will display obsolete info .,if ( upper . index of ( __str__ ) > - __num__ || upper . index of ( __str__ ) > - __num__ || upper . index of ( __str__ ) > - __num__ ) { direct refresh tree ( ) ; },liberal do children
if the proxy host requires authentication then add the host <PLACE_HOLDER> to the <PLACE_HOLDER> provider,final auth configuration auth = proxy . get auth ( ) ; if ( auth != null ) { if ( credentials provider == null ) { credentials provider = new basic credentials provider ( ) ; } auth scope auth scope = new auth scope ( http host @$ auth . get realm ( ) @$ auth . get auth scheme ( ) ) ; credentials credentials = configure credentials ( auth ) ; credentials provider . set credentials ( auth scope @$ credentials ) ; },authentication add credentials
if another engine has set the async history session <PLACE_HOLDER> already @$ there 's no need to do it again .,if ( ! session factories . contains key ( async history session . class ) ) { async history session factory async history session factory = new async history session factory ( ) ; if ( async history listener == null ) { init default async history listener ( ) ; } async history session factory . set async history listener ( async history listener ) ; session factories . put ( async history session . class @$ async history session factory ) ; } ( ( async history session factory ) session factories . get ( async history session . class ) ) . register job data types ( cmmn async history constants . ordered_types ) ;,engine set factory
the super implementation does not handle the following <PLACE_HOLDER>,params . set endpoint identification algorithm ( identification protocol ) ; params . set algorithm constraints ( algorithm constraints ) ; if ( sni matchers . is empty ( ) && ! no sni matcher ) { params . setsni matchers ( null ) ; } else { params . setsni matchers ( sni matchers ) ; },implementation handle parameters
the system ui checks the <PLACE_HOLDER> of app ops and updates the location icon accordingly .,intent intent = new intent ( location manager . high_power_request_change_action ) ; m context . send broadcast as user ( intent @$ user handle . all ) ;,ui checks number
its not unique @$ add a suitable <PLACE_HOLDER> ...,string stub = name . substring ( __num__ @$ name . length ( ) - ext . length ( ) - __num__ ) ; int index = __num__ ; do { index ++ ; name = stub + __str__ + index + __str__ + ext ; } while ( this . get script impl ( name ) != null ) ; return name ;,its add index
this call generally deletes any files at locations that are declared <PLACE_HOLDER> of the action @$ although some actions perform additional work @$ while others intentionally keep previous <PLACE_HOLDER> in place .,try ( silent closeable d = profiler . profile ( profiler task . info @$ __str__ ) ) { action . prepare ( action execution context . get exec root ( ) ) ; } catch ( io exception e ) { throw to action execution exception ( __str__ @$ e @$ action @$ null ) ; },others keep parts
ensure wrapper view child has a <PLACE_HOLDER>,wrapper view wrapper view child = ( wrapper view ) child ; if ( ! wrapper view child . has header ( ) ) { continue ; },child has header
each volume has 2 <PLACE_HOLDER>,int initial block count = num volumes * __num__ ; create file ( test file @$ initial block count ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; final fs dataset spi < ? extends fs volume spi > data = dn . data ; dn . data = mockito . spy ( data ) ; final int new volume count = __num__ ; list < thread > add volume delayed threads = collections . synchronized list ( new array list < > ( ) ) ; atomic boolean add volume error = new atomic boolean ( false ) ; atomic boolean list storage error = new atomic boolean ( false ) ; count down latch add volume,volume has blocks
first let 's remove all <PLACE_HOLDER> which do n't belong in the parents,array list < expandable notification row > to remove = new array list < > ( ) ; for ( int i = __num__ ; i < m list container . get container child count ( ) ; i ++ ) { view view = m list container . get container child at ( i ) ; if ( ! ( view instanceof expandable notification row ) ) { continue ; } expandable notification row parent = ( expandable notification row ) view ; list < expandable notification row > children = parent . get notification children ( ) ; list < expandable notification row > ordered children = m tmp child order map . get ( parent ) ; if ( children != null ) { to,'s remove children
everything has a <PLACE_HOLDER> even arrays and null,size += __num__ ;,everything has number
the list of remote <PLACE_HOLDER> got updated @$ so update the cached list <PLACE_HOLDER> which includes both local and network <PLACE_HOLDER>,if ( ! arrays . equals ( prev remote printers @$ current remote printers ) ) { refresh services ( ) ; prev remote printers = current remote printers ; },which includes 
call the get <PLACE_HOLDER> to recursively fetch the resource,if ( obj != null ) { obj = obj . get ( a key @$ aliases visited @$ requested ) ; },the get method
make sure that in the working dir abc.jar has the new <PLACE_HOLDER>,assert that ( file utils . read file to string ( jar1 . to file ( ) @$ __str__ ) ) . is equal to ( __str__ ) ; verify ( config region @$ times ( __num__ ) ) . put ( eq ( __str__ ) @$ argument captor . capture ( ) @$ eq ( __str__ ) ) ; configuration = argument captor . get value ( ) ; assert that ( configuration . get jar names ( ) ) . contains exactly in any order ( __str__ @$ __str__ ) ;,abc.jar has configuration
if both match @$ take <PLACE_HOLDER>,if ( got positive && got negative ) { if ( positive suffix . length ( ) > negative suffix . length ( ) ) { got negative = false ; } else if ( positive suffix . length ( ) < negative suffix . length ( ) ) { got positive = false ; } },both match precedence
1 because a send causes the <PLACE_HOLDER> to be recreated again which sends a new demand advisory,assert remote advisory count ( advisory consumer @$ __num__ ) ; assert advisory broker counts ( __num__ @$ __num__ @$ __num__ ) ;,send causes consumer
identifier <PLACE_HOLDER> with keyword in another release . identifier <PLACE_HOLDER> with keyword in letter @$ but not in case .,if ( ( token . equals ( token . identifier ) || token . equals ( token . macro identifier ) ) && ! token . is escaped ( ) ) { if ( token . collides with keyword ( ) ) parse exception . warning ( scanner @$ util . get message ( __str__ @$ token . name ) ) ; },identifier identifier starts
unregister should notify dropwizard <PLACE_HOLDER>,registry . remove ( metric name . name ( __str__ ) . build ( ) ) ; mockito . verify ( listener ) . on gauge removed ( mockito . eq ( __str__ ) ) ; assert equals ( __num__ @$ registry . get gauges ( ) . size ( ) ) ; assert equals ( __num__ @$ registry . get metric registry ( ) . get gauges ( ) . size ( ) ) ;,unregister notify listener
test that both spawns have the local <PLACE_HOLDER> attached as a execution info,assert that ( spawn . get execution info ( ) ) . contains key ( __str__ ) ; action execution context context = invocation . get argument ( __num__ ) ; file out err out err = context . get file out err ( ) ; called . add ( out err ) ; if ( spawn . get output files ( ) . size ( ) != __num__ ) { try ( output stream stream = out err . get output stream ( ) ) { stream . write ( __str__ . get bytes ( utf_8 ) ) ; stream . write ( ( test log helper . header_delimiter + __str__ ) . get bytes ( utf_8 ) ) ; stream . write ( __str__ . get,spawns have context
we are dragging directly over a card @$ make sure that we also catch the gesture even if nobody else wants the touch <PLACE_HOLDER> .,if ( m callback . get child at position ( ev ) != null ) { on intercept touch event ( ev ) ; return true ; } else { cancel long press ( ) ; return false ; },nobody wants events
intercept the record @$ which can be potentially modified ; this method does not throw <PLACE_HOLDER>,producer record < k @$ v > intercepted record = this . interceptors . on send ( record ) ; return do send ( intercepted record @$ callback ) ;,method throw exception
aaa entity only gets <PLACE_HOLDER> when a persistent property is accessed,assert equals ( ( short ) __num__ @$ aaa entity . get field ina mapped superclass ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert true ( hibernate . is initialized ( aaa entity ) ) ; assert equals ( true @$ aaa entity . get field ina entity ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert equals ( __str__ @$ aaa entity . get field inaa entity ( ) ) ; assert equals ( __num__ @$ stats . get prepare statement count ( ) ) ; assert equals ( __num__ @$ aaa entity . get field inaaa entity ( ) ) ; assert equals,entity gets initialized
could n't find an external in latest that we think matches this <PLACE_HOLDER> in my . so just add this <PLACE_HOLDER> and give it a conflict name if necessary .,if ( latest external location == null ) { try { external location result external location = add external ( my external location @$ monitor ) ; external location [ ] external locations = new external location [ ] { result external location @$ latest external location @$ my external location @$ null } ; adjustid maps for add ( external locations @$ result external location @$ my ) ; } catch ( duplicate name exception e ) { msg . error ( this @$ __str__ + my external location . get symbol ( ) . get name ( true ) + __str__ + e . get message ( ) ) ; } catch ( invalid input exception e ) { msg . error ( this @$ __str__,external add location
copy stored explicit <PLACE_HOLDER> to the beginning of the array .,system . arraycopy ( state @$ __num__ @$ new state @$ __num__ @$ n ) ; state = new state ;,copy stored state
create three link tree objects . one will change the dependencies @$ and one just changes destination <PLACE_HOLDER> to make sure that 's taken into account,python symlink tree first sym link tree build rule = new python symlink tree ( __str__ @$ build target @$ project filesystem @$ output path @$ immutable map . of ( paths . get ( __str__ ) @$ path source path . of ( project filesystem @$ more paths . relativize ( tmp dir . get root ( ) @$ file1 ) ) ) @$ first merge directories @$ graph builder ) ; python symlink tree second sym link tree build rule = new python symlink tree ( __str__ @$ build target @$ project filesystem @$ output path @$ immutable map . of ( paths . get ( __str__ ) @$ path source path . of ( project filesystem @$ more paths . relativize ( tmp dir .,one changes subdirs
the order of the next two <PLACE_HOLDER>s is important @$ as a change listener of the transport combo sets the proxy port <PLACE_HOLDER> to its default,connection panel . set selected transport ( preferred transport ) ; connection panel . set proxy port ( proxy port ) ; security panel . load account ( sip acc reg . get security registration ( ) ) ; presence panel . reinit ( ) ; presence panel . set presence enabled ( enable presence ) ; presence panel . set force peer to peer mode ( forcep2p ) ; presence panel . set poll period ( polling period ) ; presence panel . set subscription expiration ( subscription period ) ; if ( ! enable presence ) { presence panel . set presence options enabled ( enable presence ) ; } connection panel . set keep alive method ( keep alive method ) ; connection panel .,listener sets component
and that local files always takes <PLACE_HOLDER> over remote files .,action lookup key action lookup key = new action lookup key ( ) { @ override public sky function name function name ( ) { return sky function name . for_testing ; } } ; sky key action key1 = action lookup data . create ( action lookup key @$ __num__ ) ; sky key action key2 = action lookup data . create ( action lookup key @$ __num__ ) ; artifact out1 = create derived artifact ( __str__ ) ; artifact out2 = create derived artifact ( __str__ ) ; map < sky key @$ sky value > metadata to inject = new hash map < > ( ) ; metadata to inject . put ( action key1 @$ action value with remote artifact ( out1 @$,files takes precedence
check for overflow @$ if overflow is detected set the current <PLACE_HOLDER> to the max <PLACE_HOLDER> .,if ( current interval millis >= max interval millis / multiplier ) { current interval millis = max interval millis ; } else { current interval millis *= multiplier ; },check set interval
if element has <PLACE_HOLDER> @$ then serialize them @$ otherwise serialize en empty tag .,if ( elem . has child nodes ( ) ) { state = enter element state ( null @$ null @$ tag name @$ preserve space ) ; child = elem . get first child ( ) ; while ( child != null ) { serialize node ( child ) ; child = child . get next sibling ( ) ; } end elementio ( tag name ) ; } else { if ( ! is document state ( ) ) { state . after element = true ; state . empty = false ; } },element has children
1 is excluded via property @$ 3 already has <PLACE_HOLDER> @$ so we only expect two updates .,drain work queue ( su @$ nonstat_part_count - __num__ ) ; for ( int i = __num__ ; i < nonstat_part_count ; ++ i ) { verify stats up to date ( __str__ @$ __str__ + i @$ lists . new array list ( __str__ ) @$ ms client @$ i != excluded_part ) ; } verify stats up to date ( __str__ @$ __str__ + excluded_part @$ lists . new array list ( __str__ ) @$ ms client @$ false ) ; ms client . close ( ) ;,3 has one
no standard add key <PLACE_HOLDER> as we want to catch ctrl globally no matter of focus,keyboard focus manager . get current keyboard focus manager ( ) . add key event post processor ( new key event post processor ( ) { @ override public boolean post process key event ( key event e ) { if ( e . getid ( ) == key event . key_pressed ) { la . key pressed ( e ) ; } if ( e . getid ( ) == key event . key_released ) { la . key released ( e ) ; } if ( e . getid ( ) == key event . key_typed ) { la . key typed ( e ) ; } return false ; } } ) ;,standard add event
the host has partition <PLACE_HOLDER> @$ go ahead to move them,if ( m_cartographer . get master count ( ihid ) > __num__ ) { migrate partition leader message message = new migrate partition leader message ( ihid @$ integer . min_value ) ; for ( integer integer : live hids ) { m_mailbox . send ( core utils . geths id from host and site ( integer @$ host messenger . client_interface_site_id ) @$ message ) ; } message = new migrate partition leader message ( ihid @$ integer . min_value ) ; message . set start task ( ) ; message . set stop node service ( ) ; m_mailbox . send ( core utils . geths id from host and site ( ihid @$ host messenger . client_interface_site_id ) @$ message ) ; },host partition leadership
chrome <PLACE_HOLDER> does not necessarily match the desired <PLACE_HOLDER> because of auto updates ...,if ( browser util . is chrome ( get desired capabilities ( ) ) ) { browser identifier = get expected user agent string ( get desired capabilities ( ) ) + __str__ ; } else if ( browser util . is firefox ( get desired capabilities ( ) ) ) { browser identifier = get expected user agent string ( get desired capabilities ( ) ) + __str__ ; } else { browser identifier = get expected user agent string ( desired capabilities ) + desired capabilities . get version ( ) ; },version match version
we assume here that programs do n't change the <PLACE_HOLDER> of the keyword undefined to something other than the <PLACE_HOLDER> undefined .,if ( __str__ . equals ( name ) || __str__ . equals ( name ) ) { return ternary value . false ; } else if ( __str__ . equals ( name ) ) { return ternary value . true ; } else { return ternary value . unknown ; },programs change value
indicate that the active request has performed the <PLACE_HOLDER>,active request . update performed ( ) ; return generate ok response ( response entity ) . build ( ) ;,request performed update
if this project has correlated <PLACE_HOLDER> @$ create value generator and produce the correlated variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel @$ frame ) ; },project correlated reference
if not websocket @$ then just return the <PLACE_HOLDER>,return msg ;,then return message
add view start add view <PLACE_HOLDER>,skeleton models . add ( new skeleton model builder ( ) . set start view ( btn3 ) . set end view ( btn4 ) . build ( ) ) ;,start add 2
now we look if user 2 received an <PLACE_HOLDER>,assert equals ( __str__ @$ __num__ @$ op set2 collector . collected events . size ( ) ) ;,user received event
short circuit the no time bound <PLACE_HOLDER> .,if ( timeout < __num__ ) { try { processor . process ( now @$ region @$ mutations @$ wal edit ) ; } catch ( io exception e ) { string row = processor . get rows to lock ( ) . is empty ( ) ? __str__ : __str__ + bytes . to string binary ( processor . get rows to lock ( ) . iterator ( ) . next ( ) ) + __str__ ; log . warn ( __str__ + processor . get class ( ) . get name ( ) + __str__ + row @$ e ) ; throw e ; } return ; },circuit bound processor
web view inside browser does n't want initial <PLACE_HOLDER> to be set .,settings . set need initial focus ( false ) ;,view want focus
the kerberos based <PLACE_HOLDER> .,token token = logged in user . do as ( new privileged exception action < token > ( ) { @ override public token run ( ) throws io exception { get delegation token request request = records . new record ( get delegation token request . class ) ; request . set renewer ( renewer string ) ; return hs service . get delegation token ( request ) . get delegation token ( ) ; } } ) ; return token ;,kerberos based keytab
if no service has the capability to introspect screen @$ we do not register callback in the window manager for window changes @$ so we have to ask the window manager what the focused window is to update the active one . the active window also determined <PLACE_HOLDER> from which windows are delivered .,synchronized ( m lock ) { if ( m windows for accessibility callback == null ) { m focused window id = get focused window id ( ) ; if ( window id == m focused window id ) { m active window id = window id ; } } },window determined which
all trackers should have the node available with data <PLACE_HOLDER>,assert not null ( local tracker . get data ( false ) ) ; assert not null ( local tracker . block until available ( ) ) ; assert true ( bytes . equals ( local tracker . get data ( false ) @$ data one ) ) ; assert not null ( second tracker . get data ( false ) ) ; assert not null ( second tracker . block until available ( ) ) ; assert true ( bytes . equals ( second tracker . get data ( false ) @$ data one ) ) ; assert true ( thread . has data ) ; assert true ( bytes . equals ( thread . tracker . get data ( false ) @$ data one ) ),trackers have one
basic key chain can mix output script <PLACE_HOLDER> .,if ( ( result = basic . find key from pub hash ( pub key hash ) ) != null ) return result ; if ( chains != null ) { for ( deterministic key chain chain : chains ) { if ( script type != null && script type != chain . get output script type ( ) ) continue ; if ( ( result = chain . find key from pub hash ( pub key hash ) ) != null ) return result ; } },chain mix type
one write <PLACE_HOLDER> for session 1,request req1 = create write request ( __num__ @$ __num__ ) ; process request with wait ( req1 ) ;,one write request
check if ping command does not encounter any <PLACE_HOLDER>,if ( process . exit value ( ) == __num__ ) { shell_result = parse ( process ) ; } else { shell_result = su_busyboox_ping ( params [ __num__ ] . address ) ; },command encounter errors
3 sentries @$ each completion should trigger the <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { case instance case instance = cmmn runtime service . create case instance builder ( ) . case definition key ( __str__ ) . start ( ) ; list < plan item instance > plan item instances = cmmn runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state ( plan item instance state . active ) . order by name ( ) . asc ( ) . list ( ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; cmmn runtime service . trigger plan item instance ( plan,sentries trigger start
need to convert value to number this happens because json treats <PLACE_HOLDER> as an integer even if the field is supposed to be a long,if ( value instanceof number ) { number number value = ( number ) value ; class < ? > clazz = field . get type ( ) ; if ( clazz == integer . class || clazz == int . class ) { return number value . int value ( ) ; } else if ( clazz == long . class || clazz == long . class ) { return number value . long value ( ) ; } else if ( clazz == double . class || clazz == double . class ) { return number value . double value ( ) ; } else if ( clazz == float . class || clazz == float . class ) { return number value . float value,json treats them
when we get this far @$ we 're exiting @$ so no need to reset the <PLACE_HOLDER> .,system . set property ( __str__ @$ factory class ) ; transformer factory transformer factory = transformer factory . new instance ( ) ; transformer trans = transformer factory . new transformer ( ) ; trans . set output property ( output keys . omit_xml_declaration @$ __str__ ) ; trans . set output property ( output keys . indent @$ __str__ ) ; trans . set output property ( __str__ @$ __str__ ) ;,need reset property
we have a bucket so assign the frist left <PLACE_HOLDER> and return,if ( list != null ) { tuple = list . get first ( ) ; return tuple ; },bucket assign tuple
get object can return <PLACE_HOLDER> if constraints were specified but not met,if ( s3 object == null ) return null ; output stream output stream = null ; try { output stream = new buffered output stream ( new file output stream ( destination file ) ) ; byte [ ] buffer = new byte [ __num__ * __num__ ] ; int bytes read ; while ( ( bytes read = s3 object . get object content ( ) . read ( buffer ) ) > - __num__ ) { output stream . write ( buffer @$ __num__ @$ bytes read ) ; } } catch ( io exception e ) { throw new sdk client exception ( __str__ + e . get message ( ) @$ e ) ; } finally { close quietly ( output stream @$,object return null
configure how content assist <PLACE_HOLDER> will appear .,assistant . enable auto activation ( store . get boolean ( sql preference constants . enable_auto_activation ) ) ; assistant . set auto activation delay ( store . get int ( sql preference constants . auto_activation_delay ) ) ; assistant . set proposal popup orientation ( i content assistant . proposal_overlay ) ; assistant . set sorter ( new sql completion sorter ( ) ) ; assistant . set information control creator ( get information control creator ( source viewer ) ) ;,content assist text
the caller expects <PLACE_HOLDER> @$ we have to use an intermediary .,return new id enumeration ( collections . enumeration ( availablei ds ) ) ;,caller expects one
test server can see <PLACE_HOLDER> written by client,server . invoke ( client server host name verification distributed test :: do server region test ) ;,server see data
if not active or if the flow catalog is not up yet then ca n't process config <PLACE_HOLDER>,if ( ! is active || ! this . flow catalog . is running ( ) ) { log . warn ( __str__ @$ this . is active ) ; return false ; } return true ;,then process files
a volt db extension to make the third parameter <PLACE_HOLDER>,parse list alt = new short [ ] { tokens . openbracket @$ tokens . question @$ tokens . comma @$ tokens . question @$ tokens . x_option @$ __num__ @$ tokens . comma @$ tokens . question @$ tokens . closebracket } ;,extension make optional
and adding an old one will not trigger any <PLACE_HOLDER>,cache . check and store ( now @$ a1 ) ;,one trigger check
the <PLACE_HOLDER> in the linear axis will not have <PLACE_HOLDER> after the decimal point .,vertical axis . set label format ( __str__ ) ; linear axis horizontal axis = new linear axis ( ) ; horizontal axis . set label format ( __str__ ) ; scatter point series . set vertical axis ( vertical axis ) ; scatter point series . set horizontal axis ( horizontal axis ) ;,values have values
pipeline extends pipeline <PLACE_HOLDER>,add pipeline base extends ( __str__ @$ pipeline transform . class ) ;,pipeline extends transform
check if the array is really too big . this is an optimistic check because the heap probably has other <PLACE_HOLDER> in it @$ so the next collection will throw an out of memory error if this object is allocated and survives .,if ( size . above or equal ( heap policy . get large array threshold ( ) ) ) { if ( size . above or equal ( heap policy . get maximum heap size ( ) ) ) { throw array allocation too large ; } unaligned heap chunk . unaligned header u chunk = heap chunk provider . get ( ) . produce unaligned chunk ( size ) ; result = allocate large array ( hub @$ length @$ size @$ u chunk @$ tlab @$ remembered set ) ; } else { aligned header new chunk = prepare new allocation chunk ( tlab ) ; result = allocate small array ( hub @$ length @$ size @$ tlab @$ remembered set @$ new chunk ),heap has records
to avoid duplication only first subtask keeps track of next transactional id hint . otherwise all of the subtasks would write exactly same <PLACE_HOLDER> .,if ( get runtime context ( ) . get index of this subtask ( ) == __num__ && semantic == semantic . exactly_once ) { check state ( next transactional id hint != null @$ __str__ ) ; long next free transactional id = next transactional id hint . next free transactional id ; if ( get runtime context ( ) . get number of parallel subtasks ( ) > next transactional id hint . last parallelism ) { next free transactional id += get runtime context ( ) . get number of parallel subtasks ( ) * kafka producers pool size ; } next transactional id hint state . add ( new next transactional id hint ( get runtime context ( ) . get number of parallel,track write information
check that net flow into a vertex equals <PLACE_HOLDER> @$ except at source and sink,if ( math . abs ( value + excess ( g @$ s ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ s ) ) ; system . err . println ( __str__ + value ) ; return false ; } if ( math . abs ( value - excess ( g @$ t ) ) > floating_point_epsilon ) { system . err . println ( __str__ + excess ( g @$ t ) ) ; system . err . println ( __str__ + value ) ; return false ; } for ( int v = __num__ ; v < g . v ( ) ; v ++ ) { if ( v == s || v == t,flow equals impact
remove all elements . should set array <PLACE_HOLDER> to null,partitions . clear ( ) ; table entity . set attribute ( __str__ @$ partitions ) ; init ( ) ; response = entity store . create or update ( new atlas entity stream ( entities info ) @$ false ) ; updated table = response . get first updated entity by type name ( table_type ) ; validate entity ( entities info @$ get entity from store ( updated table ) ) ;,elements set attribute
okay @$ it 's showtime . let 's add the <PLACE_HOLDER> through live ddl,try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; } try { admin client . update application catalog ( null @$ new file ( path to deployment ) ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; },'s add package
test no balance to send <PLACE_HOLDER> . query balance before send <PLACE_HOLDER> .,long deplay account before balance = public methed . query account ( delay account3 address @$ blocking stub full ) . get balance ( ) ; long recevier account before balance = public methed . query account ( receiver account4 address @$ blocking stub full ) . get balance ( ) ; logger . info ( __str__ + deplay account before balance ) ; logger . info ( __str__ + recevier account before balance ) ; assert . assert false ( public methed . sendcoin delayed ( receiver account4 address @$ send coin amount @$ delay second @$ delay account3 address @$ delay account3 key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ;,balance send coin
they may have changed the document and the variables via instrumentation so update the <PLACE_HOLDER> to it,execution input = execution input . transform ( builder -> builder . variables ( parse result . get variables ( ) ) ) ; execution input ref . set ( execution input ) ; log not safe . debug ( __str__ @$ query ) ; final list < validation error > errors = validate ( execution input @$ document @$ graphql schema @$ instrumentation state ) ; if ( ! errors . is empty ( ) ) { log not safe . warn ( __str__ @$ query ) ; return new preparsed document entry ( errors ) ; } return new preparsed document entry ( document ) ;,document update input
check request size will throw io <PLACE_HOLDER> if request is rejected,if ( is large request ( length ) ) { check request size when message received ( length ) ; si . set large request size ( length ) ; },size throw exception
at this point @$ suggestion <PLACE_HOLDER> matches the new selection <PLACE_HOLDER>,if ( update prompt and selection if match found ) { if ( ! suggestion key . equals ( selected option key ) || suggestion . get replacement string ( ) . equals ( tb . get text ( ) ) || force update text ) { set text ( suggestion . get replacement string ( ) ) ; selected option key = suggestion key ; } },key matches key
script output contains ambiguous node <PLACE_HOLDER>,string script content = __str__ + __str__ + __str__ ; write node attribute script file ( script content @$ true ) ; node attributes provider . init ( get conf for node attribute script ( ) ) ; node attributes provider . start ( ) ;,output contains attributes
this case represents only the polymer <PLACE_HOLDER> that are inside a function which is an arg to goog.load module,check state ( is function arg in goog load module ( enclosing node ) ) ; node enclosing script = node util . get enclosing script ( enclosing node ) ; node insertion point = get node for insertion ( enclosing script ) ; insertion point . add child to front ( declaration code ) ; compiler . report change to change scope ( insertion point ) ;,case represents calls
if timestamps are equal then match images did not correctly match up the image <PLACE_HOLDER> and image proxy,preconditions . check argument ( ! min image info timestamp . equals ( min image proxy timestamp ) ) ; if ( min image info timestamp > min image proxy timestamp ) { for ( int i = m pending images . size ( ) - __num__ ; i >= __num__ ; i -- ) { if ( m pending images . key at ( i ) < min image info timestamp ) { image proxy image proxy = m pending images . value at ( i ) ; image proxy . close ( ) ; m pending images . remove at ( i ) ; } } } else { for ( int i = m pending image infos . size ( ) - __num__ ; i,images match info
clear annotation database finder restore hierarchy <PLACE_HOLDER>,guice . set annotation database package names ( null ) ; guice . set hierarchy traversal filter factory ( new hierarchy traversal filter factory ( ) ) ;,finder restore indexes
start the prepared thread so that it is writing znodes while the follower is restarting . on the first restart @$ the follow should use txnlog to catchup . for subsequent restart @$ the follower should use a <PLACE_HOLDER> to catchup .,if ( i == __num__ ) { mytestfoo thread . start ( ) ; log . info ( __str__ @$ index ) ; qu . restart ( index ) ; thread . sleep ( __num__ ) ; log . info ( __str__ @$ index ) ; qu . shutdown ( index ) ; thread . sleep ( __num__ ) ; log . info ( __str__ @$ index ) ; qu . restart ( index ) ; log . info ( __str__ @$ index ) ; },follower use diff
validate report <PLACE_HOLDER> .,file report file = new file ( location ) ; assert . assert true ( __str__ + location @$ report file . exists ( ) ) ; validate jdr report contents ( report file ) ;,validate report file
do n't publish directly @$ ensure that the user explicitly reviews and approves <PLACE_HOLDER> first .,idempotent executor . execute and swallowio exceptions ( title @$ title @$ ( ) -> get or create blogger service ( auth data ) . posts ( ) . insert ( blog id @$ post ) . set is draft ( true ) . execute ( ) . get id ( ) ) ;,user reviews post
load balancer receives all <PLACE_HOLDER> of callbacks,transport info1 . listener . transport ready ( ) ; verify ( state listener1 @$ times ( __num__ ) ) . on subchannel state ( state info captor . capture ( ) ) ; assert same ( connecting @$ state info captor . get all values ( ) . get ( __num__ ) . get state ( ) ) ; assert same ( ready @$ state info captor . get all values ( ) . get ( __num__ ) . get state ( ) ) ; verify ( state listener2 ) . on subchannel state ( state info captor . capture ( ) ) ; assert same ( connecting @$ state info captor . get value ( ) . get state ( ) ) ; resolver . listener,balancer receives kinds
namespace declarations parameter has no <PLACE_HOLDER> if namespaces is false .,if ( ! f namespace declarations && f namespace aware ) { int len = attributes . get length ( ) ; for ( int i = len - __num__ ; i >= __num__ ; -- i ) { if ( xml symbols . prefix_xmlns == attributes . get prefix ( i ) || xml symbols . prefix_xmlns == attributes . getq name ( i ) ) { attributes . remove attribute at ( i ) ; } } } super . start element ( element @$ attributes @$ augs ) ;,declarations has effect
the candidate name did n't return any <PLACE_HOLDER> so the name is unique,if ( null == item ) { return true ; } else if ( item . get name ( ) . equals ( current job name ) ) { return true ; } else { return false ; },name return item
the super implementation does not handle the following <PLACE_HOLDER>,identification protocol = params . get endpoint identification algorithm ( ) ; algorithm constraints = params . get algorithm constraints ( ) ; prefer local cipher suites = params . get use cipher suites order ( ) ; list < sni server name > sni names = params . get server names ( ) ; if ( sni names != null ) { no sni extension = sni names . is empty ( ) ; server names = sni names ; } collection < sni matcher > matchers = params . getsni matchers ( ) ; if ( matchers != null ) { no sni matcher = matchers . is empty ( ) ; sni matchers = matchers ; } if ( ( handshaker != null ) &&,implementation handle parameters
the key'algo ' contains the <PLACE_HOLDER> @$ 'algorithm ' is the long version,final string algo full name = readkv ( __str__ ) ;,key'algo contains key
we do n't validate the string but let the parser do the <PLACE_HOLDER> for us it throws a validation exception,validate optional ( key @$ is optional @$ v -> { logical type t = logical type parser . parse ( v ) ; if ( t . get type root ( ) == logical type root . unresolved ) { throw new validation exception ( __str__ + v + __str__ ) ; } } ) ;,parser do work
set background @$ if your root layout does n't have <PLACE_HOLDER>,final drawable window background = get window ( ) . get decor view ( ) . get background ( ) ; top blur view . setup with ( root ) . set frame clear drawable ( window background ) . set blur algorithm ( new support render script blur ( this ) ) . set blur radius ( radius ) . set has fixed transformation matrix ( true ) ; bottom blur view . setup with ( root ) . set frame clear drawable ( window background ) . set blur algorithm ( new support render script blur ( this ) ) . set blur radius ( radius ) . set has fixed transformation matrix ( true ) ; int initial progress = ( int ) ( radius,layout have one
component has a <PLACE_HOLDER>,if ( constraints . ascent >= __num__ ) { int baseline = constraints . ascent ; constraints . descent = h - constraints . ascent + constraints . insets . bottom ; constraints . ascent += constraints . insets . top ; constraints . baseline resize behavior = c . get baseline resize behavior ( ) ; constraints . center padding = __num__ ; if ( constraints . baseline resize behavior == component . baseline resize behavior . center_offset ) { int next baseline = c . get baseline ( w @$ h + __num__ ) ; constraints . center offset = baseline - h / __num__ ; if ( h % __num__ == __num__ ) { if ( baseline != next baseline ) { constraints . center,component has adjustment
raptor connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support comment
why do we have a separate field for the singular form ? assoc <PLACE_HOLDER> and assoc key .,if ( finder schema . has assoc key ( ) ) { string part key = finder schema . get assoc key ( ) ; compound key key = ( compound key ) generate key ( ) ; finder . assoc key ( part key @$ key . get part ( part key ) ) ; },key assoc part
not a list @$ but not a primtive @$ return the nested group <PLACE_HOLDER>,return g . get group ( field index @$ __num__ ) ;,list return object
version 2 layout draws outer <PLACE_HOLDER> bigger than inner,if ( controller . get version ( ) == time picker dialog . version . version_1 ) { m text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_outer ) ) ; m inner text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_inner ) ) ; } else { m text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_outer_v2 ) ) ; m inner text size multiplier = float . parse float ( res . get string ( r . string . mdtp_text_size_multiplier_inner_v2 ) ) ; } m inner text grid heights = new float [ __num__ ] ;,version draws margin
size of metadata has increased @$ the most complex <PLACE_HOLDER> @$ more atoms affected,int additional space required for metadata = new ilst size - old ilst size ;,size increased code
o auth encodes some <PLACE_HOLDER> differently :,return url decoder . decode ( url @$ __str__ ) . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) ;,auth encodes characters
the service that detects lost standby master <PLACE_HOLDER>,get executor service ( ) . submit ( new heartbeat thread ( heartbeat context . master_lost_master_detection @$ new lost master detection heartbeat executor ( ) @$ ( int ) server configuration . get ms ( property key . master_standby_heartbeat_interval ) @$ server configuration . global ( ) @$ m master context . get user state ( ) ) ) ; get executor service ( ) . submit ( new heartbeat thread ( heartbeat context . master_log_config_report_scheduling @$ new log config report heartbeat executor ( ) @$ ( int ) server configuration . get ms ( property key . master_log_config_report_heartbeat_interval ) @$ server configuration . global ( ) @$ m master context . get user state ( ) ) ) ; if ( server configuration . get boolean (,detects lost heartbeat
management server sends back another rds <PLACE_HOLDER> containing the route configuration for the requested resource .,route configs = immutable list . of ( any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) @$ build virtual host ( immutable list . of ( __str__ @$ __str__ ) @$ __str__ ) ) ) ) ) ; response = build discovery response ( __str__ @$ route configs @$ xds client impl . ads_type_url_rds @$ __str__ ) ; response observer . on next ( response ) ;,server sends response
this is dangerous : new instance can throw checked <PLACE_HOLDER> . this is dangerous : config is mutable .,if ( ! runnable before . is interface ( ) ) { runnable before . new instance ( ) . run ( get configuration ( ) ) ; },instance throw exceptions
invalidate throws entry not found exception then repopulate <PLACE_HOLDER>,try ( ignored exception ignored1 = add ignored exception ( entry not found exception . class . get name ( ) ) ; ignored exception ignored2 = add ignored exception ( reply exception . class . get name ( ) ) ) { for ( int i = invalidate range_2 start ; i <= invalidate range_2 end ; i ++ ) { string key = integer . to string ( i ) ; string value = integer . to string ( i ) ; assert that thrown by ( ( ) -> region . invalidate ( key ) ) . is instance of ( entry not found exception . class ) ; region . create ( key @$ value ) ; } },exception repopulate list
properties javadoc discourages <PLACE_HOLDER> of put so we <PLACE_HOLDER> set property,if ( name instanceof string && value instanceof string ) { props . set property ( ( string ) name @$ ( string ) value ) ; },javadoc discourages use
always use the smallest radius to make sure the rounded corners will completely cover the <PLACE_HOLDER> .,return math . min ( top radius @$ bottom radius ) ;,corners cover outcome
the two methods follow different code paths to determine <PLACE_HOLDER> the build target for the result should be and we want to test both of them .,try ( fixture fixture = create synchronous execution fixture ( __str__ ) ) { cell cell = fixture . get cell ( ) ; path root build file path = cell . get filesystem ( ) . resolve ( __str__ ) ; path a build file path = cell . get filesystem ( ) . resolve ( __str__ ) ; fixture . get target node parse pipeline ( ) . get all requested target nodes ( cell @$ root build file path @$ optional . empty ( ) ) ; optional < build file manifest > root raw nodes = fixture . get raw node parse pipeline cache ( ) . lookup computed node ( cell @$ root build file path @$ event bus ) ; fixture .,the build what
unlike list popup window @$ popup menu does n't have an api to check whether it is showing . use a custom <PLACE_HOLDER> to check the visibility of the drop down list view instead .,on view ( with class name ( matchers . is ( drop_down_class_name ) ) ) . in root ( is platform popup ( ) ) . check ( matches ( is displayed ( ) ) ) ;,api use matcher
expected exception as the function is shutting down the target <PLACE_HOLDER> and the result collector will get member departed exception,return __str__ ;,function shutting thread
transition to fatal error since we have unresolved <PLACE_HOLDER> .,sender . run once ( ) ;,transition have dependencies
asserts object <PLACE_HOLDER> on the object graph .,assert true ( copya . get object ( ) == copyc . get object ( ) ) ; assert true ( copya . get objects ( ) . get ( __num__ ) == copyc . get objects ( ) . get ( __num__ ) ) ; assert true ( copya == copyc . get objects ( ) . get ( __num__ ) ) ; assert true ( copyc == copya . get objects ( ) . get ( __num__ ) ) ;,asserts object collisions
we do a remote call to have an io exception if the connection is broken . see the <PLACE_HOLDER> 4939578,return connection . get connection id ( ) ;,4939578 see bug
the list of relational values should contain 2 or more values : the first represents the <PLACE_HOLDER> the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ jaxb any mapping . get name ( ) ) @$ origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source type source = new hibernate type source impl ( jaxb any mapping . get meta type ( ) ) ; private final relational value source relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > value mappings = new hash map < string @$ string > ( ) ; { for ( jaxb hbm any value,first represents discriminator
creation of an isa will force a <PLACE_HOLDER> .,inet socket address initial isa = new inet socket address ( hostname @$ port ) ; if ( initial isa . get address ( ) == null ) { throw new illegal argument exception ( __str__ + initial isa ) ; } final list < blocking service and interface > sai = new array list < > ( __num__ ) ;,creation force resolve
the keys of probe and build sides are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer join <PLACE_HOLDER> well in this case .,final int probe_vals_per_key = __num__ ;,side join matchs
javac does not allow final or static in interface methods order annotation <PLACE_HOLDER> hence no need to check that this is not a method or annotation field,while ( modifier != null ) { final int type = modifier . get type ( ) ; if ( type == token types . literal_public || type == token types . literal_static && ast . get type ( ) != token types . method_def || type == token types . abstract && ast . get type ( ) != token types . class_def || type == token types . final && ast . get type ( ) != token types . class_def ) { log ( modifier @$ msg_key @$ modifier . get text ( ) ) ; break ; } modifier = modifier . get next sibling ( ) ; },javac allow types
set capacity will change <PLACE_HOLDER> of underlying byte array,buf . set capacity ( start size ) ; assert equals ( start size @$ buf . capacity ( ) ) ; check buffer ( buf @$ empty ) ;,capacity change length
find out @$ which output number the <PLACE_HOLDER> to the parent,int num ; for ( num = __num__ ; num < this . outgoing connections . size ( ) ; num ++ ) { if ( this . outgoing connections . get ( num ) == to parent ) { break ; } } if ( num >= this . outgoing connections . size ( ) ) { throw new compiler exception ( __str__ + __str__ ) ; },which output connection
the spy throws an io <PLACE_HOLDER> when writing to the second directory,do answer ( new faulty save image ( false ) ) . when ( spy image ) . savefs image ( any ( ) @$ any ( ) @$ any ( ) ) ; should fail = false ; break ; case save_all_fsimages :,spy throws exception
fetch the cr ls via ldap . ldap cert store has its own caching <PLACE_HOLDER> @$ see the class description for more info . safe cast since xsel is an x 509 certificate selector .,try { return ( collection < x509crl > ) ldap cert store . getcr ls ( xsel ) ; } catch ( cert store exception cse ) { throw new pkix . cert store type exception ( __str__ @$ cse ) ; },store has mechanism
buck modules do not support <PLACE_HOLDER>,return ( __ @$ ___ ) -> true ;,modules support java
server sent <PLACE_HOLDER>,assert equals ( ( long ) expected settings . get max header list size ( ) @$ new settings . max header list size ( ) ) ; assert equals ( ( integer ) expected settings . get max frame size ( ) @$ new settings . max frame size ( ) ) ; assert equals ( ( integer ) expected settings . get initial window size ( ) @$ new settings . initial window size ( ) ) ; assert equals ( ( long ) expected settings . get max concurrent streams ( ) @$ new settings . max concurrent streams ( ) ) ; assert equals ( null @$ new settings . header table size ( ) ) ; complete ( ) ; break ; default,server sent headers
in the second rebalance the new member gets its assignment and this member has no <PLACE_HOLDER> or revocations,expect rebalance ( __num__ @$ collections . empty list ( ) @$ collections . empty list ( ) ) ; member . poll ( easy mock . any int ( ) ) ; power mock . expect last call ( ) ; power mock . replay all ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ double . positive_infinity ) ; herder . tick ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; herder . tick ( ) ; time . sleep ( __num__ ) ; assert statistics ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; power mock . verify all ( ),assignment has relocations
we use a runnable to make sure this work . because if the list view is handling <PLACE_HOLDER> @$ this might not work .,m view files . post ( new runnable ( ) { @ override public void run ( ) { if ( should be selected idx >= __num__ && should be selected idx < m file adapter . get count ( ) ) { m view files . set selection ( should be selected idx ) ; } else if ( ! m file adapter . is empty ( ) ) m view files . set selection ( __num__ ) ; } } ) ;,view handling failures
the first two tasks have the same <PLACE_HOLDER>,final task task = noop task . create ( math . min ( __num__ @$ ( i - __num__ ) * __num__ ) ) ;,tasks have weight
we should show the promo view only when the panel has reached the exact expanded <PLACE_HOLDER> .,if ( percentage == __num__ ) { show promo view ( ) ; } else { hide promo view ( ) ; },panel reached size
no fallback should take more than 2 <PLACE_HOLDER> .,if ( new style2 != null ) { if ( fallback cache [ new style2 . ordinal ( ) ] != null ) { throw new illegal state exception ( __str__ ) ; } },fallback take entries
strip auth <PLACE_HOLDER> from result .,result . remove ( account manager . key_authtoken ) ; if ( log . is loggable ( tag @$ log . verbose ) ) { log . v ( tag @$ get class ( ) . get simple name ( ) + __str__ + response ) ; },strip auth token
only the first request ever generated should contain an install <PLACE_HOLDER> .,if ( succeeded && sending install request ) { m send install event = false ; register new request ( current timestamp ) ; generate and post request ( current timestamp @$ sessionid ) ; },request contain event
lazily initialized <PLACE_HOLDER> of the notification .,status bar notification sbn = r . sbn ; status bar notification old sbn = ( old != null ) ? old . sbn : null ; trim cache trim cache = new trim cache ( sbn ) ; for ( final managed service info info : get services ( ) ) { boolean sbn visible = is visible to listener ( sbn @$ info ) ; boolean old sbn visible = old sbn != null ? is visible to listener ( old sbn @$ info ) : false ; if ( ! old sbn visible && ! sbn visible ) { continue ; } if ( r . is hidden ( ) && info . target sdk version < build . version_codes . p ) { continue,lazily initialized version
configure the atomic <PLACE_HOLDER> directories key so every folder will have atomic <PLACE_HOLDER> applied .,conf . set ( azure native file system store . key_atomic_rename_directories @$ __str__ ) ; azure blob storage test account test account = azure blob storage test account . create ( conf ) ; assume not null ( test account ) ; test statistics with account ( test account ) ;,folder have rename
otherwise we can get into an infinite loop because the object type hash code method calls this <PLACE_HOLDER> .,return objects . hash code ( properties . key set ( ) ) ;,method calls method
filter projects only to groups which match project 's <PLACE_HOLDER>,set < group > copy = group . matching ( project @$ groups ) ;,which match pattern
check mapping a different element returns the expected <PLACE_HOLDER> .,bounded window input window2 = new interval window ( instant . now ( ) @$ duration . standard minutes ( __num__ ) ) ; assert equals ( global window . instance @$ window mapping fn . get side input window ( input window2 ) ) ; assert equals ( input window2 @$ ( ( kv ) test sdk harness . get input values ( ) . get ( __num__ ) . get value ( ) ) . get value ( ) ) ;,check returns result
update <PLACE_HOLDER> visibility only triggers when a scroll is completed . so a user might click the <PLACE_HOLDER> when the animation is still ongoing potentially pushing the target position outside of the bounds of the day picker view,if ( position >= __num__ && position < day picker view . get count ( ) ) { day picker view . smooth scroll to position ( position ) ; update button visibility ( position ) ; },user click button
since value may exceed integer <PLACE_HOLDER> @$ use stock parser which checks for this .,if ( length >= __num__ ) { value = integer . parse int ( text . sub sequence ( position @$ position += length ) . to string ( ) ) ; } else { int i = position ; if ( negative ) { i ++ ; } try { value = text . char at ( i ++ ) - __str__ ; } catch ( string index out of bounds exception e ) { return ~ position ; } position += length ; while ( i < position ) { value = ( ( value << __num__ ) + ( value << __num__ ) ) + text . char at ( i ++ ) - __str__ ; } if ( negative ) { value = -,value exceed limits
if system time has gone backwards increase <PLACE_HOLDER> by 1 ms to maintain uniqueness,last time = ( now < last time ) ? last time + __num__ : now ; last count = short . min_value ; done = true ;,time gone count
query the data using the <PLACE_HOLDER>,query data ( ) ;,data using api
test show create <PLACE_HOLDER>,string expected sql = format sql text ( format ( __str__ @$ get session ( ) . get catalog ( ) . get ( ) @$ get session ( ) . get schema ( ) . get ( ) @$ __str__ @$ query ) ) . trim ( ) ; actual = compute actual ( __str__ ) ; assert equals ( get only element ( actual . get only column as set ( ) ) @$ expected sql ) ; actual = compute actual ( format ( __str__ @$ get session ( ) . get catalog ( ) . get ( ) @$ get session ( ) . get schema ( ) . get ( ) ) ) ; assert equals ( get only element ( actual .,show create table
pattern order is used downstream so that we can know what order the text is in instead of encoding it in the string @$ which would require more <PLACE_HOLDER> later to remove it pre feature selection .,map < string @$ integer > pattern order = new hash map < > ( ) ; int order = __num__ ; if ( has option ( from_option [ __num__ ] ) ) { patterns . add ( mail processor . from_prefix ) ; pattern order . put ( mail options . from @$ order ++ ) ; } if ( has option ( to_option [ __num__ ] ) ) { patterns . add ( mail processor . to_prefix ) ; pattern order . put ( mail options . to @$ order ++ ) ; } if ( has option ( references_option [ __num__ ] ) ) { patterns . add ( mail processor . refs_prefix ) ; pattern order . put ( mail options . refs @$,which require processing
total jobs number is threads num <PLACE_HOLDER>,assert equals ( __str__ @$ threads num * jobs per task @$ stolen . get ( ) + none stolen . get ( ) ) ; assert false ( __str__ @$ stolen . get ( ) == __num__ ) ; for ( ignite g : g . all grids ( ) ) assert true ( __str__ @$ nodes . contains ( g . name ( ) ) ) ; assert true ( __str__ + stolen + __str__ + none stolen + __str__ @$ math . abs ( stolen . get ( ) - __num__ * none stolen . get ( ) ) <= __num__ ) ;,jobs threads 3
request a list of upload ur ls for the dropped <PLACE_HOLDER>,if ( ! file params . is empty ( ) ) { get rpc proxy ( file drop target rpc . class ) . drop ( file params ) ; } event . prevent default ( ) ; event . stop propagation ( ) ;,the dropped file
wrong ur ls missing <PLACE_HOLDER> @$ however accepted by digest url and multi protocol url constructors,test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; test urls . add ( __str__ ) ; digesturl java url = new digesturl ( java url str ) ; string java hash result = ascii . string ( java url . hash ( ) ) ;,ur ls attributes
in theory @$ the subclasses of stream source may implement the bounded one input <PLACE_HOLDER> @$ so we still need the following call to end the input,if ( ! is canceled or stopped ( ) ) { synchronized ( locking object ) { operator chain . end head operator input ( __num__ ) ; } },subclasses implement operator
log any changes to what is currently driving the brightness <PLACE_HOLDER> .,if ( ! m brightness reason temp . equals ( m brightness reason ) || brightness adjustment flags != __num__ ) { slog . v ( tag @$ __str__ + brightness + __str__ + m brightness reason temp . to string ( brightness adjustment flags ) + __str__ + m brightness reason + __str__ ) ; m brightness reason . set ( m brightness reason temp ) ; },what driving adjustment
presto only uses get reflection object inspector here @$ in a test method . therefore @$ we choose to work around this issue by synchronizing this method . before synchronizing this method @$ test in this class fails approximately <PLACE_HOLDER> out of <PLACE_HOLDER>0 runs on travis .,return get reflection object inspector ( type @$ object inspector options . java ) ;,test fails time
if the thread context classloader is set @$ then try its <PLACE_HOLDER> to find a matching context,class loader tccl = thread . current thread ( ) . get context class loader ( ) ; loader = tccl ; if ( loader != null ) { if ( log . is debug enabled ( ) ) log . debug ( __str__ ) ; synchronized ( __context map ) { while ( ctx == null && loader != null ) { ctx = get context for class loader ( loader ) ; if ( ctx == null && loader != null ) loader = loader . get parent ( ) ; } if ( ctx == null ) { ctx = new naming context ( obj @$ tccl @$ env @$ name @$ name ctx ) ; __context map . put ( tccl @$ ctx ),then try entry
each time the version <PLACE_HOLDER> as we walk the list @$ that counts as a successful operation,long version = - __num__ ; for ( int i = __num__ ; i < number of operations ; i ++ ) { if ( operations . get ( i ) . version ( ) >= version ) { count ++ ; version = operations . get ( i ) . version ( ) ; } },each time changes
each transform also gets its own <PLACE_HOLDER>,list < string > expected groups = arrays . as list ( connector config . common_group @$ connector config . transforms_group @$ connector config . error_group @$ __str__ @$ __str__ ) ; assert equals ( expected groups @$ result . groups ( ) ) ; assert equals ( __num__ @$ result . error count ( ) ) ;,transform gets group
create some destroyed <PLACE_HOLDER> so the gc service is populated,serializable callable create = new serializable callable ( __str__ ) { @ override public object call ( ) { region factory f = get cache ( ) . create region factory ( get region attributes ( ) ) ; cc region = ( local region ) f . create ( name ) ; return cc region . get distribution manager ( ) . get distribution manager id ( ) ; } } ;,some destroyed entries
now based on that floating point percentage compute the real scroller <PLACE_HOLDER> .,final int visible amt = f scroll bar . get visible amount ( ) ; final int max = f scroll bar . get maximum ( ) ; final int min = f scroll bar . get minimum ( ) ; final int extent = max - min ;,now compute size
check if the line has at least one <PLACE_HOLDER> or digit,for ( string line : lines ) { if ( character_digits_pattern . matcher ( line ) . matches ( ) ) { stderr . append ( line ) ; } },line has character
dep 1 should have only dep 1 abi <PLACE_HOLDER>,dex produced from java library java dep1 dex rule = ( dex produced from java library ) graph builder . get rule ( build target factory . new instance ( __str__ ) ) ; assert not null ( java dep1 dex rule ) ; assert that ( java dep1 dex rule . get desugar deps ( ) @$ has size ( __num__ ) ) ; assert that ( java dep1 dex rule . get desugar deps ( ) @$ has item ( java dep2 abi . get source path to output ( ) ) ) ; assert that ( java dep1 dex rule . get build deps ( ) @$ all of ( has item ( java dep2 abi ) @$ not ( has item ( java dep1,dep have dependency
make the client use client cache <PLACE_HOLDER> so it will have a default pool,this . durable clientvm . invoke ( ( ) -> cache server test util . create client cache ( get client pool ( get server host name ( ) @$ server1 port @$ true ) @$ region name @$ get client distributed system properties ( durable client id ) ) ) ;,client use server
error while invoking found <PLACE_HOLDER> @$ use empty <PLACE_HOLDER>,result = ( i persistable ) the class . new instance ( ) ; result . deserialize ( in ) ;,invoking found class
finishing task 2 should complete the <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( plan item instances . get ( __num__ ) . get id ( ) ) ; plan item instances = cmmn runtime service . create plan item instance query ( ) . case instance id ( case instance . get id ( ) ) . plan item instance state ( plan item instance state . active ) . order by name ( ) . asc ( ) . list ( ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; expected names = new string [ ] { __str__ } ; for ( int i = __num__ ; i < plan item instances . size ( ) ; i ++ ) { assert equals,task complete stage
validate inode tree <PLACE_HOLDER> match given <PLACE_HOLDER> .,for ( mutable inode < ? > node : journaled ) { assert true ( tree entries . contains ( node . to journal entry ( ) ) ) ; },entries match entries
fastpath : do not construct a new <PLACE_HOLDER> if the src is a <PLACE_HOLDER> and is already normalized .,if ( src instanceof string ) { int span length = span quick check yes ( src ) ; if ( span length == src . length ( ) ) { return ( string ) src ; } string builder sb = new string builder ( src . length ( ) ) . append ( src @$ __num__ @$ span length ) ; return normalize second and append ( sb @$ src . sub sequence ( span length @$ src . length ( ) ) ) . to string ( ) ; },fastpath construct string
second combine should replace <PLACE_HOLDER> ; should read existing data @$ and write final data to disk .,current time += second_in_millis ; reader . reset ( ) ; rotate . combine active ( reader @$ writer ( __str__ ) @$ current time ) ; reader . assert read ( __str__ ) ; assert read all ( rotate @$ __str__ ) ;,combine replace old
do bind implements all four <PLACE_HOLDER> of binding,naming context data store impl = ( naming context data store ) this ; do bind ( impl @$ n @$ nc @$ true @$ binding type . ncontext ) ;,bind implements flavors
one <PLACE_HOLDER> reader closes @$ the other still receives the <PLACE_HOLDER>,output consumer binding . dispose ( ) ; consumed out . reset ( ) ; consumed out2 . reset ( ) ; context . eval ( source ) ; engine output = engine output + full output ; assert equals ( engine output @$ to unix string ( engine out ) ) ; assert equals ( __num__ @$ consumed out . size ( ) ) ; assert true ( consumed out2 . size ( ) > __num__ ) ; from out reader2 = new buffered reader ( new input stream reader ( new byte array input stream ( consumed out2 . to byte array ( ) ) ) ) ; assert equals ( full lines @$ read lines list ( from out reader2 ) ) ;,other receives input
if right becomes root parent means rotation happened at lower level . so just return right so that nodes at upper level can set their <PLACE_HOLDER> correctly,if ( right == root . parent ) { return right ; },nodes set child
if child is not updated yet call <PLACE_HOLDER>,if ( child != null && child . get visibility ( ) != view . gone ) { if ( child . get measured height ( ) == __num__ || child . get measured width ( ) == __num__ ) child . measure ( measure spec . make measure spec ( width @$ measure spec . at_most ) @$ measure spec . make measure spec ( height @$ measure spec . at_most ) ) ; layout params lp = ( layout params ) child . get layout params ( ) ; final int child width = child . get measured width ( ) ; final int child height = child . get measured height ( ) ; if ( child top + child height + lp . top,child updated measure
for now the call bellow will return type <PLACE_HOLDER>,type = compiler . resolve type ref ( model @$ bkm @$ bkm @$ null ) ;,bellow return unknown
table row specified <PLACE_HOLDER> @$ match anything character class .,if ( table el . f char class == __num__ ) { break ; },row specified 0
very simple format that shows millisecond <PLACE_HOLDER>,try { return long . to string ( d . get time ( ) ) ; } catch ( exception ignore ) { return __str__ ; },shows millisecond timestamp
ensure the work handler has processed the <PLACE_HOLDER> inside the cache listener of wifi tracker,wait for handlers to process currently enqueued messages ( tracker ) ;,handler processed messages
0 x 100231 d : p 1 and p 2 have same note <PLACE_HOLDER> .,program builder1 . create bookmark ( __str__ @$ bookmark type . note @$ __str__ @$ __str__ ) ; program builder2 . create bookmark ( __str__ @$ bookmark type . note @$ __str__ @$ __str__ ) ;,1 have bookmarks
by default @$ parse validates the <PLACE_HOLDER> @$ which is what we want .,schema schema = new schema . parser ( ) . parse ( metadata . get schema string ( ) ) ; assert equals ( __num__ @$ schema . get fields ( ) . size ( ) ) ;,parse validates schema
a media period may report a discontinuity at the current playback position to ensure the renderers are flushed . only report the discontinuity <PLACE_HOLDER> if the position changed .,if ( discontinuity position us != playback info . position us ) { playback info = copy with new position ( playback info . period id @$ discontinuity position us @$ playback info . content position us ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( playing period holder != queue . get reading period ( ) ) ;,period report when
camera device should call configure <PLACE_HOLDER> and have it finish before constructing us,if ( configure success ) { m state callback . on configured ( this ) ; if ( debug ) log . v ( tag @$ m id string + __str__ ) ; m configure success = true ; } else { m state callback . on configure failed ( this ) ; m closed = true ; log . e ( tag @$ m id string + __str__ ) ; m configure success = false ; },device call success
only the stream which was just added will change <PLACE_HOLDER> . so we only need an array of size 1 .,list < parent changed event > events = new array list < parent changed event > ( __num__ ) ; connection state . take child ( new parent @$ false @$ events ) ; notify parent changed ( events ) ;,stream change events
does the list already contain that <PLACE_HOLDER> ?,return ! cached device list . contains ( id ) ;,list contain device
optimization : the special case where we 're a <PLACE_HOLDER> of size 1 @$ and the other factor is receiving the <PLACE_HOLDER> @$ and of size 2,if ( domain . size ( ) == __num__ && ( result domain . size ( ) == other domain . size ( ) ) && result domain . size ( ) == __num__ ) { return other . multiply ( this ) ; } else { int [ ] mapping = new int [ result . neighbor indices . length ] ; int [ ] other mapping = new int [ result . neighbor indices . length ] ; for ( int i = __num__ ; i < result . neighbor indices . length ; i ++ ) { mapping [ i ] = domain . index of ( result . neighbor indices [ i ] ) ; other mapping [ i ] = other domain .,factor receiving dom
should use provided <PLACE_HOLDER> @$ with original bit array size,bit array array3 = new bit array ( __num__ ) ; array3 = matrix . get row ( __num__ @$ array3 ) ; assert equals ( __num__ @$ array3 . get size ( ) ) ; for ( int x = __num__ ; x < __num__ ; x ++ ) { boolean on = ( x & __num__ ) == __num__ ; assert equals ( on @$ array . get ( x ) ) ; assert equals ( on @$ array2 . get ( x ) ) ; assert equals ( on @$ array3 . get ( x ) ) ; },use provided object
all symbols must have declaration <PLACE_HOLDER> .,check not null ( decl node ) ; return declare symbol ( sym . get name ( ) @$ get type ( sym ) @$ is type inferred ( sym ) @$ scope @$ decl node @$ sym . getjs doc info ( ) ) ;,symbols have nodes
assert that the log contains the correct <PLACE_HOLDER> .,html page log = j . create web client ( ) . get page ( proj @$ __str__ ) ; string logastext = log . as text ( ) ; assert true ( logastext . contains ( __str__ + abstract project . workspace offline reason . all_suitable_nodes_are_offline . name ( ) + __str__ ) ) ;,log contains information
since the link is relative and the holder is not even being documented @$ this must be an inherited link . redirect it . the current class either overrides the referenced <PLACE_HOLDER> or inherits it automatically .,if ( see . text ( ) . trim ( ) . starts with ( __str__ ) && ! ( containing . is public ( ) || util . is linkable ( containing @$ configuration ) ) ) { if ( this instanceof class writer impl ) { containing = ( ( class writer impl ) this ) . get class doc ( ) ; } else if ( ! containing . is public ( ) ) { configuration . get doclet specific msg ( ) . warning ( see . position ( ) @$ __str__ @$ tag name @$ containing . qualified name ( ) ) ; } else { configuration . get doclet specific msg ( ) . warning ( see . position ( ) @$,class overrides class
if no values in this block were set @$ we can just set its <PLACE_HOLDER> to be the same as some other block with no values set @$ assuming we 've seen one yet .,if ( ! block touched [ plane ] [ i ] && i untouched != - __num__ ) { indices [ plane ] [ i ] = i untouched ; } else { int j block start = limit compacted * blockcount ; if ( i > limit compacted ) { system . arraycopy ( values [ plane ] @$ i block start @$ values [ plane ] @$ j block start @$ blockcount ) ; } if ( ! block touched [ plane ] [ i ] ) { i untouched = ( short ) j block start ; } indices [ plane ] [ i ] = ( short ) j block start ; limit compacted ++ ; },block set index
after tx state proxy committed @$ get <PLACE_HOLDER> will get the <PLACE_HOLDER> for the oldtx but caller should not perform ops on this tx state proxy,assert true ( tx mgr . get lock ( tx @$ txid ) ) ;,lock get lock
if this list does not contain our <PLACE_HOLDER> name @$ then its not referencing our <PLACE_HOLDER> this is a cheezy test ... but it errs on the side of less false hits .,if ( meth . get reference names ( ) . is empty ( ) && ! meth . is super ( ) ) { list < string > pack class = meth . get qualifier names ( ) ; if ( ! pack class . is empty ( ) ) { for ( string name : pack class ) { if ( name . equals ( class name ) ) { found = true ; break ; } } } else { found = true ; } },its referencing class
the user allowed the <PLACE_HOLDER> to toggle touch exploration .,m enable touch exploration dialog = new alert dialog . builder ( m context ) . set icon attribute ( android . r . attr . alert dialog icon ) . set positive button ( android . r . string . ok @$ new on click listener ( ) { @ override public void on click ( dialog interface dialog @$ int which ) { user state . m touch exploration granted services . add ( service . m component name ) ; persist component names to setting locked ( settings . secure . touch_exploration_granted_accessibility_services @$ user state . m touch exploration granted services @$ user state . m user id ) ; user state . m is touch exploration enabled = true ; final long identity =,user allowed user
local job tracker allows the only <PLACE_HOLDER> per wave @$ but text input format replaces it with the calculated value based on input split size option .,if ( __str__ . equals ( cfg . get ( __str__ @$ __str__ ) ) ) { file input format . set min input split size ( job cfg @$ __num__ * __num__ * __num__ ) ; file input format . set max input split size ( job cfg @$ long . max_value ) ; },tracker allows parameter
if maps @$ recursively compare the key and value <PLACE_HOLDER>,if ( c1 . equals ( category . map ) ) { map object inspector mapoi1 = ( map object inspector ) o1 ; map object inspector mapoi2 = ( map object inspector ) o2 ; object inspector child key1 = mapoi1 . get map key object inspector ( ) ; object inspector child key2 = mapoi2 . get map key object inspector ( ) ; if ( compare types ( child key1 @$ child key2 ) ) { object inspector child val1 = mapoi1 . get map value object inspector ( ) ; object inspector child val2 = mapoi2 . get map value object inspector ( ) ; if ( compare types ( child val1 @$ child val2 ) ) { return true ; } } return,maps compare fields
json does n't support null <PLACE_HOLDER> @$ so just write the null key value,if ( input == null ) { map . put ( null key value @$ null ) ; } else { map . put ( input @$ input ) ; },json support keys
do not animate the clock when waking up from a pulse . the height callback will take <PLACE_HOLDER> of pushing the clock to the right position .,if ( ! m pulsing && ! m dozing ) { m animate next position update = false ; } m notification stack scroller . set pulsing ( pulsing @$ animate pulse ) ; m keyguard status view . set pulsing ( pulsing ) ;,callback take care
list of need fetch next contains all tables that we have joined <PLACE_HOLDER> in their candidate storage @$ and we need to clear candidate storage and promote their next group storage to candidate storage and fetch <PLACE_HOLDER> until we reach a new group .,if ( ( list of need fetch next . size ( ) > __num__ ) && clear ) { for ( byte b : list of need fetch next ) { try { fetch next group ( b ) ; } catch ( exception e ) { throw new hive exception ( e ) ; } } },list fetch data
we inserted a keyframe let 's shift the <PLACE_HOLDER> .,j ++ ;,'s shift shortcut
test that generated policy file has included both <PLACE_HOLDER>,assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( class loader perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ; assert . assert true ( files . read all lines ( paths . get ( generated policy ) ) . contains ( socket perm string . to string ( ) . split ( __str__ ) [ __num__ ] ) ) ;,file included components
metered change should update <PLACE_HOLDER>,m cell network agent . add capability ( network capabilities . net_capability_not_metered ) ; wait for idle ( ) ; verify ( m stats service @$ at least once ( ) ) . force update ifaces ( eq ( only cell ) @$ any ( network state [ ] . class ) @$ eq ( mobile_ifname ) @$ eq ( new vpn info [ __num__ ] ) ) ; reset ( m stats service ) ; m cell network agent . remove capability ( network capabilities . net_capability_not_metered ) ; wait for idle ( ) ; verify ( m stats service @$ at least once ( ) ) . force update ifaces ( eq ( only cell ) @$ any ( network state [ ] . class ),change update ifaces
show a unified format which contains <PLACE_HOLDER> @$ host and port .,string src service name = src fs . get canonical service name ( ) ; string des service name = des fs . get canonical service name ( ) ; if ( src service name == null || des service name == null ) { return false ; } if ( src service name . equals ( des service name ) ) { return true ; } if ( src service name . starts with ( __str__ ) && des service name . starts with ( __str__ ) ) { collection < string > internal name services = conf . get trimmed string collection ( __str__ ) ; if ( ! internal name services . is empty ( ) ) { if ( internal name services . contains,which contains hostname
make cookie secure when using <PLACE_HOLDER>,if ( use secure ) { root context . get session handler ( ) . get session cookie config ( ) . set secure ( use secure ) ; },cookie using elytron
components commonly have conditional <PLACE_HOLDER>s assigned . using the click <PLACE_HOLDER> matcher we can assert whether or not a given component has a <PLACE_HOLDER> attached to them . noinspection unchecked,assert that ( c @$ component ) . extracting sub component at ( __num__ ) . has ( sub component with ( c @$ test footer component . matcher ( c ) . click handler ( is null . < event handler < click event > > null value ( null ) ) . build ( ) ) ) ;,component has event
wsdl does n't have any schema <PLACE_HOLDER>,return new source [ __num__ ] ;,wsdl have methods
memory map does duplicate <PLACE_HOLDER>,return - __num__ ;,map duplicate entries
jar files have only generated <PLACE_HOLDER> @$ not the source toc file,return null ;,files generated code
skip bundles that have n't reached resolved <PLACE_HOLDER> ; skip fragments .,if ( b . get state ( ) <= bundle . resolved || b . get headers ( ) . get ( constants . fragment_host ) != null ) continue ; try { cls = b . load class ( name ) ; break ; } catch ( class not found exception ignored ) { },bundles resolved states
check if instrument uses general midi 2 default <PLACE_HOLDER> .,int bank = get patch ( ) . get bank ( ) ; if ( bank > > __num__ == __num__ || bank > > __num__ == __num__ ) { boolean [ ] ch = new boolean [ __num__ ] ; for ( int i = __num__ ; i < ch . length ; i ++ ) ch [ i ] = true ; return ch ; } boolean [ ] ch = new boolean [ __num__ ] ; for ( int i = __num__ ; i < ch . length ; i ++ ) ch [ i ] = true ; ch [ __num__ ] = false ; return ch ;,instrument uses algorithm
return new byte buffer every <PLACE_HOLDER> to prevent potential side effects,if ( clazz . equals ( byte buffer . class ) ) { byte buffer buff = byte buffer . allocate ( __num__ ) ; rand . next bytes ( buff . array ( ) ) ; return buff ; } else if ( type . equals ( placement constraint . class ) ) { placement constraint . abstract constraint s constraint expr = target in ( node @$ allocation tag ( __str__ ) ) ; ret = placement constraints . build ( s constraint expr ) ; },byte buffer time
completing all the tasks ends the case <PLACE_HOLDER>,for ( task t : cmmn task service . create task query ( ) . case instance id ( case instance . get id ( ) ) . list ( ) ) { cmmn task service . complete ( t . get id ( ) ) ; } assert case instance ended ( case instance ) ;,tasks ends instance
check new server registers <PLACE_HOLDER> for reconnected client .,try ( ignite new srv = start grid ( server count ( ) + __num__ ) ) { await partition map exchange ( ) ; lsnr . latch = new count down latch ( __num__ ) ; ignite cache < object @$ object > new srv cache = new srv . cache ( default_cache_name ) ; for ( integer key : primary keys ( new srv cache @$ __num__ ) ) new srv cache . put ( key @$ key ) ; assert true ( lsnr . latch . await ( __num__ @$ milliseconds ) ) ; } cur . close ( ) ;,server registers partition
check if list has some <PLACE_HOLDER> that can be rebalanced,while ( iterator . has next ( ) ) { if ( iterator . next ( ) . ds member list . size ( ) > __num__ ) { flag to continue with rebalance = true ; break ; } } if ( ! flag to continue with rebalance ) { rebalance result . set status message ( cli strings . rebalance__msg__no_rebalancing_regions_on_ds ) ; rebalance result . set success ( true ) ; return rebalance result ; } list < rebalance region result > rebalance region results = new array list < > ( ) ; for ( memberpr info memberpr : list member region ) { try { if ( memberpr . ds member list . size ( ) > __num__ ) { for ( int i,list has members
protected mode should have deleted the <PLACE_HOLDER>,assert . assert equals ( children . size ( ) @$ __num__ @$ children . to string ( ) ) ;,mode deleted child
prefer no default domain @$ as wild fly does not register an mbean <PLACE_HOLDER> with a default domain,if ( null != m bean server . get default domain ( ) ) { for ( int i = __num__ ; i < m bean server list . size ( ) ; i ++ ) { m bean server anm bean server = m bean server list . get ( i ) ; if ( null == anm bean server . get default domain ( ) ) { m bean server = anm bean server ; break ; } } },fly register server
executors provide no <PLACE_HOLDER> @$ so make our own .,int queued = queue size . get and increment ( ) ; if ( max queue length > __num__ && queued >= max queue length ) { queue size . decrement and get ( ) ; return false ; } executor . execute ( new fifo call runner ( task ) { @ override public void run ( ) { task . set status ( rpc server . get status ( ) ) ; task . run ( ) ; queue size . decrement and get ( ) ; } } ) ; return true ;,executors provide queue
does the program class already have a new <PLACE_HOLDER> ?,string new class name = new class name ( program class ) ; if ( new class name != null ) { class names to avoid . add ( mixed case class name ( new class name ) ) ; if ( repackage classes == null || ! allow access modification ) { string class name = program class . get name ( ) ; map package name ( class name @$ new class name @$ repackage classes == null && flatten package hierarchy == null ) ; } },class have name
alice can see upstream @$ so <PLACE_HOLDER> gets built @$ but the upstream build can not see <PLACE_HOLDER> :,auth . grant ( item . read ) . on items ( upstream ) . to ( __str__ @$ __str__ ) ; map < string @$ authentication > qia config = new hash map < string @$ authentication > ( ) ; qia config . put ( upstream name @$ user . get ( __str__ ) . impersonate ( ) ) ; qia config . put ( downstream name @$ user . get ( __str__ ) . impersonate ( ) ) ; queue item authenticator configuration . get ( ) . get authenticators ( ) . replace ( new mock queue item authenticator ( qia config ) ) ; b = r . build and assert success ( upstream ) ; r . assert log not contains (,upstream see downstream
output <PLACE_HOLDER> is activity manager service dump activities <PLACE_HOLDER>,if ( __str__ . equals ( cmd ) || __str__ . equals ( cmd ) ) { m atm internal . write activities to proto ( proto ) ; } else if ( __str__ . equals ( cmd ) || __str__ . equals ( cmd ) ) { synchronized ( this ) { write broadcasts to proto locked ( proto ) ; } } else if ( __str__ . equals ( cmd ) ) { string [ ] new args ; string name ; if ( opti >= args . length ) { name = null ; new args = empty_string_array ; } else { name = args [ opti ] ; opti ++ ; new args = new string [ args . length - opti ] ;,service dump proto
imply <PLACE_HOLDER> if owner has <PLACE_HOLDER> set .,implicit |= sym . owner . flags_field & strictfp ; break ; default : throw new assertion error ( ) ;,owner has strictfp
if our constituents have post exported linker flags @$ our dependents should use <PLACE_HOLDER> .,for ( native linkable linkable : constituents . get linkables ( ) ) { args builder . add all ( linkable . get exported post linker flags ( graph builder ) ) ; } return native linkable input . of ( args builder . build ( ) @$ immutable list . of ( ) @$ immutable list . of ( ) ) ;,dependents use them
update the screen to see changes take <PLACE_HOLDER>,if ( m map view != null ) { this . invalidate compass ( ) ; },changes take effect
response account features op package <PLACE_HOLDER>,m ams . has features ( m mock account manager response @$ account manager service test fixtures . account_success @$ null @$ __str__ ) ;,features op name
nm 5 has no <PLACE_HOLDER> allocated containers .,for ( container moo container : allocated containers ) { assert . assert false ( moo container . get node id ( ) . equals ( nm5 . get node id ( ) ) ) ; },nm has any
recompute system ui <PLACE_HOLDER> .,if ( ( m window attributes changes flag & window manager . layout params . translucent_flags_changed ) != __num__ ) { m attach info . m recompute global attributes = true ; },system ui attributes
tccl ca n't see the <PLACE_HOLDER>,is context loader = false ;,tccl see context
first @$ reduce to account asset balance . else ca n't complete this test <PLACE_HOLDER> .,account capsule to account = db manager . get account store ( ) . get ( byte array . from hex string ( to_address ) ) ; long id = db manager . get dynamic properties store ( ) . get token id num ( ) ; to account . reduce asset amountv2 ( byte string . copy from utf8 ( string . value of ( id ) ) . to byte array ( ) @$ total_supply - __num__ @$ db manager . get dynamic properties store ( ) @$ db manager . get asset issue store ( ) ) ; db manager . get account store ( ) . put ( to account . get address ( ) . to byte array ( ) @$ to account,first complete case
for local queries returning pdx <PLACE_HOLDER> wrap the resultset with results collection pdx deserializer wrapper which deserializes these pdx <PLACE_HOLDER> .,if ( needspdx deserialization wrapper ( false ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ needs copy on read wrapper ) ; } else if ( ! is remote query ( ) && this . cache . get copy on read ( ) && result instanceof select results ) { if ( needs copy on read wrapper ) { result = new results collection copy on read wrapper ( ( select results ) result ) ; } },which deserializes records
call set <PLACE_HOLDER> with empty bounds to remove the <PLACE_HOLDER> .,set cutout ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ;,call set set
assert that the current value in store reflects all <PLACE_HOLDER> being processed,assert that ( new active store . get ( key ) @$ is ( equal to ( total num messages - __num__ ) ) ) ;,value reflects messages
all fail return <PLACE_HOLDER>,return tc . get random data node ( ) ;,all fail null
verify that scan encounters correct <PLACE_HOLDER>,scan scan = new scan ( ) ; try { result scanner scanner = ht . get scanner ( scan ) ; result res = null ; do { res = scanner . next ( ) ; } while ( res != null ) ; } catch ( table not enabled exception e ) { ok = true ; } assert true ( ok ) ; admin . enable table ( table ) ; assert true ( __str__ @$ test_util . geth base cluster ( ) . get master ( ) . get table state manager ( ) . is table state ( ht . get name ( ) @$ table state . state . enabled ) ) ; assert equals ( table state . state . enabled,scan encounters exception
the editing stopped event is how the client can use the enter <PLACE_HOLDER> to close the widget,assert equals ( __str__ + __str__ @$ listener . stopped count @$ __num__ ) ;,client use method
master holds the <PLACE_HOLDER> @$ so restart the master .,if ( server . equals ( cluster status . get master name ( ) ) ) { restart master ( server @$ sleep time ) ; } else { restart rs ( server @$ sleep time ) ; },master holds lock
then the produced thread dump should contain that expected <PLACE_HOLDER> at least,assert true ( file contains ( threaddump file @$ __str__ @$ dumpable process . class . get name ( ) ) ) ;,dump contain name
if string labeled <PLACE_HOLDER> @$ create aux vec,map < string @$ integer > map = m . get row label bindings ( ) ; if ( map != null ) { labels = make empty str vec ( frame . any vec ( ) ) ; vec . writer writer = labels . open ( ) ; map < integer @$ string > rmap = reverse map ( map ) ; for ( int r = __num__ ; r < m . row size ( ) ; r ++ ) { writer . set ( r @$ rmap . get ( r ) ) ; } writer . close ( closer ) ; },string labeled field
be sure to let our parent perform any <PLACE_HOLDER> needed,j label renderer = ( j label ) super . get table cell renderer component ( data ) ; object value = data . get value ( ) ; j table table = data . get table ( ) ; boolean is selected = data . is selected ( ) ; set text ( __str__ ) ; set horizontal alignment ( center ) ; vt match match = ( vt match ) value ; vt association association = match . get association ( ) ; vt association status association status = association . get status ( ) ; if ( ! is selected ) { renderer . set background ( match table renderer . get background color ( association @$ table @$ renderer . get background ( ),parent perform initialization
the value specified in xml takes <PLACE_HOLDER> over the environment variable !,if ( config . get object timeout minutes ( ) > __num__ ) { object timeout = config . get object timeout minutes ( ) ; } else if ( ! utils . is empty ( system timeout ) ) { object timeout = const . to int ( system timeout @$ __num__ ) ; } else { object timeout = __num__ * __num__ ; },value takes precedence
pulling this common case out of ` is timer enabled slow ` gives c 1 a better <PLACE_HOLDER> to inline this method .,if ( ! metrics enabled ) { return false ; },slow gives chance
default config should not change the <PLACE_HOLDER>,identity transformer identity transformer = get transformer with default identity config ( config ) ; identity transformer . transform acl entries for set request ( acl entries ) ; check acl entries list ( acl entries to be transformed @$ acl entries ) ; reset identity config ( config ) ;,config change identities
one clone <PLACE_HOLDER>,list < scanner report . duplication > duplication groups = result . duplications for ( input file ) ; assert that ( duplication groups ) . has size ( __num__ ) ; scanner report . duplication clone group = duplication groups . get ( __num__ ) ; assert that ( clone group . get origin position ( ) . get start line ( ) ) . is equal to ( __num__ ) ; assert that ( clone group . get origin position ( ) . get end line ( ) ) . is equal to ( __num__ ) ; assert that ( clone group . get duplicate list ( ) ) . has size ( __num__ ) ; assert that ( clone group . get duplicate ( __num__,one clone group
the filter should filter out all <PLACE_HOLDER> @$ but we still expect to see every row .,filter filter = new row filter ( compare operator . equal @$ new binary comparator ( bytes . to bytes ( __str__ ) ) ) ; scan = new scan ( base scan ) ; scan . set filter ( filter ) ; test metric ( scan @$ server side scan metrics . count_of_rows_scanned_key_metric_name @$ rows . length ) ;,filter filter rows
lease will not be granted if the time taken so far plus lease time exceeds the max <PLACE_HOLDER> .,while ( segment completion mgr . _seconds + lease time sec <= start time + segment completion manager . get max commit time for all segments seconds ( ) ) { params = new request . params ( ) . with instance id ( s2 ) . with offset ( s2 offset ) . with segment name ( segment name str ) . with extra time sec ( lease time sec ) ; response = segment completion mgr . extend build time ( params ) ; assert . assert equals ( response . get status ( ) @$ controller response status . processed ) ; assert . assert true ( ( fsm map . contains key ( segment name str ) ) ) ; segment completion mgr .,time exceeds completion
test read last buffer <PLACE_HOLDER> across input data <PLACE_HOLDER>s .,test array . set position ( __num__ ) ; result [ __num__ ] = __num__ ; test array . read bits ( result @$ __num__ @$ __num__ ) ; assert that ( result [ __num__ ] ) . is equal to ( ( byte ) __num__ ) ;,test read bit
checks the query buffer for cte queries or simple select statements returns the <PLACE_HOLDER> where the injection should start .,final int offset = get statement index ( sb ) ; if ( ! limit helper . has first row ( selection ) ) { add top expression ( sb @$ offset ) ; } else { final string select clause = fill alias in select clause ( sb @$ offset ) ; if ( shallow index of pattern ( sb @$ order_by_pattern @$ offset ) > __num__ ) { add top expression ( sb @$ offset ) ; } enclose with outer query ( sb @$ offset ) ; sb . insert ( offset @$ ! iscte ? __str__ : __str__ ) ; sb . append ( __str__ ) . append ( select clause ) . append ( __str__ ) ; sb . append ( __str__ ),checks returns index
to test that the function correctly puts <PLACE_HOLDER> in the resulting template .,string key uri = __str__ ; key template template = aead key templates . create kms aead key template ( key uri ) ; assert equals ( new kms aead key manager ( ) . get key type ( ) @$ template . get type url ( ) ) ; assert equals ( output prefix type . tink @$ template . get output prefix type ( ) ) ; kms aead key format format = kms aead key format . parse from ( template . get value ( ) @$ extension registry lite . get empty registry ( ) ) ; assert equals ( key uri @$ format . get key uri ( ) ) ;,function puts them
either node attribute constraint or source allocation <PLACE_HOLDER> alone,if ( splitted . length == __num__ ) { node constraint parser np = new node constraint parser ( spec str ) ; optional < abstract constraint > constraint optional = optional . of nullable ( np . try parse ( ) ) ; if ( constraint optional . is present ( ) ) { st = source tags . empty source tags ( ) ; constraint = constraint optional . get ( ) . build ( ) ; } else { st = source tags . parse from ( spec str ) ; constraint = null ; } } else { throw new placement constraint parse exception ( __str__ + spec str ) ; },node attribute usage
since we ca n't see the request object on the remote side @$ we ca n't <PLACE_HOLDER> whether the remote side actually performed an authorization <PLACE_HOLDER> here @$ so always set this to true for the proxy servlet . if the remote node failed to perform an authorization <PLACE_HOLDER> @$ pre response authorization <PLACE_HOLDER> filter will log that on the remote node .,client request . set attribute ( auth config . druid_authorization_checked @$ true ) ;,side performed check
revoke table read <PLACE_HOLDER> to test grant revoke .,try { revoke from table using access control client ( test_util @$ system user connection @$ test grant revoke . get short name ( ) @$ test_table @$ null @$ null @$ permission . action . read ) ; } catch ( throwable e ) { log . error ( __str__ @$ e ) ; },table read permission
provide an empty key store since trust manager impl does n't support null key <PLACE_HOLDER> . trust manager impl will use cert store to lookup certificates .,key store store = key store . get instance ( key store . get default type ( ) ) ; store . load ( null ) ; m delegate = new trust manager impl ( store @$ null @$ cert store ) ;,impl support chains
make the user agent tester change its <PLACE_HOLDER> and make sure we are notified,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = supported status set2 . get ( jabber status enum . free_for_chat ) ;,tester change states
now make sure declarative cache ca n't create the same <PLACE_HOLDER>,try { cache creation cache = new cache creation ( ) ; cache . create pool factory ( ) . add locator ( alias2 @$ __num__ ) . create ( __str__ ) ; ignored exception expected exception = add ignored exception ( string . format ( __str__ @$ __str__ ) ) ; try { test xml ( cache ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { } finally { expected exception . remove ( ) ; } } finally { pool manager . close ( ) ; },cache create ones
excepted error will be something like : javax.ejb.ejb exception : wflyjpa 0030 : found extended persistence <PLACE_HOLDER> in sfsb invocation call stack but that can not be used because the transaction already has a transactional <PLACE_HOLDER> associated with it ...,try { top level bean . reference two distinct extended persistence contexts in sametx_fail ( ) ; } catch ( ejb exception caught ) { error = caught ; } assert not null ( __str__ + __str__ @$ error ) ;,transaction has context
these should only affect the given <PLACE_HOLDER>,session s = open session ( ) ; transaction t = s . begin transaction ( ) ; int count = s . create query ( __str__ ) . set string ( __str__ @$ __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; count = s . create query ( __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; t . commit ( ) ; s . close ( ) ; data . cleanup ( ) ;,these affect ugi
some point the proxy client class should centralize the connection aliveness <PLACE_HOLDER> and no longer rely on the custom tabs to ping the connections .,try { getm bean server connection ( ) . get default domain ( ) ; } catch ( io exception ex ) { vm panel . get proxy client ( ) . mark as dead ( ) ; },class centralize analysis
only static endpoints @$ return an optimized endpoint <PLACE_HOLDER> .,return new static endpoint group ( static endpoints ) ;,endpoints return group
wait 2 election <PLACE_HOLDER>outs so that this master and other masters have <PLACE_HOLDER> to realize they are not leader .,common utils . sleep ms ( __num__ * m conf . get election timeout ms ( ) ) ; if ( state machine . get last applied sequence number ( ) != last appliedsn || state machine . get last primary start sequence number ( ) != gain primacysn ) { continue ; },master have time
find all terminal nodes having no incoming <PLACE_HOLDER> .,set < object > all dependents = new hash set < > ( ) ; for ( dependency dep : edges ) { if ( ( dep . depends on instanceof local thread ) ) { if ( dep . depender instanceof message key ) { all dependents . add ( dep . depends on ) ; } } else { all dependents . add ( dep . depends on ) ; } } list < dependency graph > result = new linked list < > ( ) ; for ( object depender : depends on obj ) { if ( ! all dependents . contains ( depender ) ) { result . add ( get sub graph ( depender ) ) ; } } return result ;,nodes having dependents
the result has not all words in it . find another sentence that represents the missing other words and find recursively more <PLACE_HOLDER>,if ( this . remaining terms . size ( ) < query terms . size ( ) ) { max length = max length - this . snippet string . length ( ) ; if ( max length < __num__ ) max length = __num__ ; try { tsr = new snippet extractor ( order . values ( ) @$ this . remaining terms @$ max length ) ; } catch ( final unsupported operation exception e ) { throw e ; } final string next snippet = tsr . snippet string ; if ( next snippet == null ) return ; this . snippet string = this . snippet string + ( __str__ + next snippet ) ; this . remaining terms = tsr . remaining terms,words find sentences
this must be done to assure the correct vfs file system drivers will process the <PLACE_HOLDER>,string scheme = extract scheme ( full parameter name ) ; try { delegating file system options builder delegatefs options builder = new delegating file system options builder ( kettlevfs . get instance ( ) . get file system manager ( ) ) ; if ( scheme != null ) { delegatefs options builder . set config string ( opts @$ scheme @$ name @$ value ) ; } else { log . log minimal ( __str__ + vfs url ) ; } } catch ( file system exception e ) { if ( ( e . get code ( ) != null ) && ( e . get code ( ) . equals ignore case ( __str__ ) ) ) { log . log minimal ( __str__,drivers process html
view enters high padding <PLACE_HOLDER> :,if ( view max > client size + padding min ) { if ( m focus scroll strategy == base grid view . focus_scroll_page ) { first view = view ; do { circular int array positions = m grid . get item positions in rows ( pos @$ m grid . get last visible index ( ) ) [ row ] ; last view = find view by position ( positions . get ( positions . size ( ) - __num__ ) ) ; if ( get view max ( last view ) - view min > client size ) { last view = null ; break ; } } while ( append one column visible items ( ) ) ; if ( last view != null,view enters range
if the text wrapper does n't have a start <PLACE_HOLDER> and the new character is a starting one,if ( squigglies . get ( i ) . start index == - __num__ && ! is closing squiggly ( squiggly ) ) { if ( squiggly of interest == - __num__ ) { squiggly of interest = i ; } else { if ( squigglies . get ( i ) . end index < squigglies . get ( squiggly of interest ) . end index ) { squiggly of interest = i ; } } },wrapper have index
if the original test turned off sanity <PLACE_HOLDER> @$ make sure our synthesized code passes it .,if ( ! validity check && ! compiler . has errors ( ) ) { new var check ( compiler @$ true ) . process ( externs @$ root ) ; },test turned checks
exception should cause a <PLACE_HOLDER>,system . err . println ( ) ;,exception cause problem
this test may not trigger eviction each <PLACE_HOLDER> @$ repeat it 20 <PLACE_HOLDER>s .,for ( int i = __num__ ; i < __num__ ; i ++ ) { delete during eviction ( i ) ; common utils . sleep ms ( __num__ * heartbeat_interval_ms ) ; },test trigger time
since on <PLACE_HOLDER> unselected is triggered before on <PLACE_HOLDER> selected when transferring to another <PLACE_HOLDER> @$ pending update if m is selecting <PLACE_HOLDER> is true to prevent dialog from being dismissed in the process of selecting <PLACE_HOLDER> .,if ( m route for volume updating by user != null || m is selecting route || m is animating volume slider layout ) { return true ; },m selecting item
the value of call count can exceed 1 only if the callback thread survives the <PLACE_HOLDER> thrown by the first callback .,assert true ( call count . get ( ) > __num__ ) ; if ( watcher != null ) { watcher . stop ( ) ; watcher . wait for state ( file change watcher . state . stopped ) ; },thread survives exception
a media period may report a <PLACE_HOLDER> at the current playback position to ensure the renderers are flushed . only report the <PLACE_HOLDER> externally if the position changed .,if ( period position us != playback info . position us ) { playback info = playback info . copy with new position ( playback info . period id @$ period position us @$ playback info . content position us @$ get total buffered duration us ( ) ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( ) ; period position us = playing period holder . to period time ( renderer position us ) ; maybe trigger pending messages ( playback info . position us @$ period position us ) ; playback info . position us = period position us ;,period report discontinuity
the following check handles <PLACE_HOLDER> of the cutover year before the cutover itself happens .,if ( is gregorian != ( jd >= cutover julian day ) ) { invert gregorian = true ; jd = super . handle compute julian day ( best field ) ; } return jd ;,check handles date
host name should be valid @$ but most probably not existing if its not enough @$ then should probably run 'list ' <PLACE_HOLDER> first to be sure ...,final string not existent fake host name = __str__ ; string credentials not found msg = null ; try { run credential program ( not existent fake host name @$ credential helper name ) ; log . warn ( __str__ @$ credential helper name ) ; } catch ( exception e ) { if ( e instanceof invalid result exception ) { credentials not found msg = extract credential provider error message ( ( invalid result exception ) e ) ; } if ( is blank ( credentials not found msg ) ) { log . warn ( __str__ @$ credential helper name @$ e . get message ( ) ) ; } else { log . debug ( __str__ @$ credentials not found msg ) ; },then run program
register an ejb channel open <PLACE_HOLDER>,open listener channel open listener = remoteejb service . get open listener ( ) ; try { registration = endpoint . register service ( ejb_channel_name @$ channel open listener @$ this . channel creation options ) ; } catch ( service registration exception e ) { throw new start exception ( e ) ; },ejb channel listener
decide whether to perform a get or put <PLACE_HOLDER>,while ( benchmark complete . get ( ) == false ) { if ( rand . next double ( ) < config . getputratio ) { try { client response response = client . call procedure ( __str__ @$ processor . generate random key for retrieval ( ) ) ; final volt table pair data = response . get results ( ) [ __num__ ] ; if ( pair data . get row count ( ) == __num__ ) missed gets . increment and get ( ) ; else { final payload processor . pair pair = processor . retrieve from store ( pair data . fetch row ( __num__ ) . get string ( __num__ ) @$ pair data . fetch row ( __num__ ) . get,a get operation
sets handshaker <PLACE_HOLDER> .,if ( handshaker options . get rpc protocol versions ( ) != null ) { start client req . set rpc versions ( handshaker options . get rpc protocol versions ( ) ) ; } if ( handshaker options instanceof alts client options ) { alts client options client options = ( alts client options ) handshaker options ; if ( ! strings . is null or empty ( client options . get target name ( ) ) ) { start client req . set target name ( client options . get target name ( ) ) ; } for ( string service account : client options . get target service accounts ( ) ) { start client req . add target identities builder ( ) . set,sets handshaker options
time may be negative which means time milli <PLACE_HOLDER> before 00:00:00 this maybe a bug in calcite avatica,while ( time < __num__ ) { time += millis_per_day ; } int h = time / __num__ ; int time2 = time % __num__ ; int m = time2 / __num__ ; int time3 = time2 % __num__ ; int s = time3 / __num__ ; int ms = time3 % __num__ ; int2 ( buf @$ h ) ; buf . append ( __str__ ) ; int2 ( buf @$ m ) ; buf . append ( __str__ ) ; int2 ( buf @$ s ) ; if ( precision > __num__ ) { buf . append ( __str__ ) ; while ( precision > __num__ ) { buf . append ( ( char ) ( __str__ + ( ms / __num__ ) ) ) ;,which means seconds
if not @$ let 's see <PLACE_HOLDER> factory method to use :,if ( type . is enum type ( ) ) { return factory . create enum deserializer ( ctxt @$ type @$ bean desc ) ; } if ( type . is container type ( ) ) { if ( type . is array type ( ) ) { return factory . create array deserializer ( ctxt @$ ( array type ) type @$ bean desc ) ; } if ( type . is map like type ( ) ) { json format . value format = bean desc . find expected format ( type . get raw class ( ) ) ; if ( format . get shape ( ) != json format . shape . pojo ) { map like type mlt = ( map like,method use which
dynamic ser de always writes out bytes <PLACE_HOLDER>,bytes writable bw = ( bytes writable ) r ; out stream . write ( bw . get ( ) @$ __num__ @$ bw . get size ( ) ) ; out stream . write ( final row separator ) ;,ser writes writable
excess processors will notice that they are not needed right now @$ and will park until they are . the most important thing here is that future units will have a lower <PLACE_HOLDER> of processor as expected max .,return lock ;,units have number
schema and relation id pools are tapped @$ schema id pool twice because the renew is triggered . each id acquisition requires 1 <PLACE_HOLDER> and 2 reads,verify store metrics ( get config ( ) . get ( ids_store_name ) @$ system_metrics @$ immutable map . of ( m_mutate @$ __num__ @$ m_get_slice @$ __num__ ) ) ;,acquisition requires getter
round screen @$ check boxed @$ do n't use <PLACE_HOLDER> on boxed,if ( m is round ) { if ( ( lp . boxed edges & layout params . box_left ) == __num__ ) { margin left = lp . left margin ; } if ( ( lp . boxed edges & layout params . box_right ) == __num__ ) { margin right = lp . right margin ; } if ( ( lp . boxed edges & layout params . box_top ) == __num__ ) { margin top = lp . top margin ; } if ( ( lp . boxed edges & layout params . box_bottom ) == __num__ ) { margin bottom = lp . bottom margin ; } } else { margin left = lp . left margin ; margin top = lp . top,screen use margin
unexpected in bazel logic @$ but skyframe makes no <PLACE_HOLDER> that this package and configuration are actually present .,if ( env . values missing ( ) ) { return null ; },unexpected makes guarantees
we update m last frame always rather than in the conditional with the last inset variables @$ because m frame size changed only tracks the <PLACE_HOLDER> and height changing .,m window frames . m last frame . set ( m window frames . m frame ) ; if ( did frame insets change || win animator . m surface resized || config changed || drag resizing changed || m report orientation changed ) { if ( debug_resize || debug_orientation ) { slog . v ( tag_wm @$ __str__ + this + __str__ + __str__ + m window frames . get insets changed info ( ) + __str__ + win animator . m surface resized + __str__ + config changed + __str__ + drag resizing changed + __str__ + m report orientation changed ) ; } if ( m app token != null && m app died ) { m app token . remove dead windows ( ),size tracks size
the token was valid @$ but the impersonation failed . this token is clearly not his real password @$ so there 's no point in continuing the request processing . report this <PLACE_HOLDER> and abort .,logger . log ( warning @$ __str__ + username + __str__ @$ x ) ; throw new servlet exception ( x ) ; throw new servlet exception ( x ) ;,processing report error
set bucket encryption has no output <PLACE_HOLDER>,return new set bucket encryption result ( ) ;,encryption has shape
marks both theme configs as changed so main activity restarts <PLACE_HOLDER> on return,find preference ( __str__ ) . set on preference change listener ( new preference . on preference change listener ( ) { @ override public boolean on preference change ( preference preference @$ object new value ) { config . mark changed ( get activity ( ) @$ __str__ ) ; config . mark changed ( get activity ( ) @$ __str__ ) ; get activity ( ) . recreate ( ) ; return true ; } } ) ;,activity restarts itself
new streams will use the last <PLACE_HOLDER>,delayed stream wfr5 = ( delayed stream ) delayed transport . new stream ( method @$ headers @$ wait for ready call options ) ; assert null ( wfr5 . get real stream ( ) ) ; in order . verify ( picker ) . pick subchannel ( new pick subchannel args impl ( method @$ headers @$ wait for ready call options ) ) ; in order . verify no more interactions ( ) ; assert equals ( __num__ @$ delayed transport . get pending streams count ( ) ) ;,streams use one
if records have been read @$ check the has next <PLACE_HOLDER> @$ if not then get the next file to process,if ( this . current file != null && this . current file itr != null ) { this . has next = this . current file itr . has next ( ) ; if ( ! this . has next ) { get next file to read ( ) ; } } else { get next file to read ( ) ; },the has field
the below loop creates dump <PLACE_HOLDER> for each table . it reads through the list of write notification events @$ groups the entries per table and creates the lists of files to be replicated . the event <PLACE_HOLDER> in the dump path will have sub<PLACE_HOLDER> for each table . this folder will have metadata for the table and the list of files to be,for ( int idx = __num__ ; idx < num entry ; idx ++ ) { ql md table = new org . apache . hadoop . hive . ql . metadata . table ( event message . get table obj ( idx ) ) ; if ( ql md table prev == null ) { ql md table prev = ql md table ; } if ( ! ql md table prev . get complete name ( ) . equals ( ql md table . get complete name ( ) ) ) { create dump file for table ( within context @$ ql md table prev @$ ql ptns @$ files tobe added ) ; ql ptns = new array list < > ( ) ; files,loop creates files
first client that connects completes this <PLACE_HOLDER> .,if ( ! connected client . set ( multiplexer ) ) { additional multiplexers . offer ( multiplexer ) ; } try { return connected client . get ( ) . get inbound observer ( ) ; } catch ( interrupted exception | execution exception e ) { throw new runtime exception ( e ) ; },client completes future
balancer should unassign the <PLACE_HOLDER>,assert true ( __str__ @$ admin . balance ( ) ) ; test_util . wait until no regions in transition ( ) ; admin . assign ( region . get encoded name as bytes ( ) ) ; test_util . wait until no regions in transition ( __num__ ) ; currentfn = fnm . get favored nodes ( region ) ; assert not null ( currentfn ) ; assert equals ( __str__ @$ favored node assignment helper . favored_nodes_num @$ currentfn . size ( ) ) ; assert true ( __str__ @$ admin . balance ( ) ) ; test_util . wait until no regions in transition ( __num__ ) ; check favored node assignments ( table name @$ fnm @$ region states ) ;,balancer unassign region
the second request should fail with unprocessed request exception which has a <PLACE_HOLDER> of go away received exception .,assert that thrown by ( future2 :: join ) . is instance of ( completion exception . class ) . has cause instance of ( unprocessed request exception . class ) . has root cause instance of ( go away received exception . class ) ;,which has cause
resetting the session <PLACE_HOLDER> should not reset the poll <PLACE_HOLDER>,time . sleep ( max poll interval ms + __num__ ) ; heartbeat . reset session timeout ( ) ; assert true ( heartbeat . poll timeout expired ( time . milliseconds ( ) ) ) ;,timeout reset timeout
register the result in the session to ensure retries receive the same <PLACE_HOLDER> for the command .,session . register result ( sequence @$ result ) ;,retries receive response
the max and min distance is the total height of the recycler view minus the height of the last child . this ensures that each scroll will never scroll more than a single <PLACE_HOLDER> on the recycler view . that is @$ the max scroll will make the last child the first child and vice versa when scrolling the opposite way .,if ( percentage visible > __num__ ) { max distance -= layout manager . get decorated measured height ( last child ) ; },scroll scroll page
those modes never skip <PLACE_HOLDER> @$ there is no need to track unskipped <PLACE_HOLDER> .,if ( build mode == build type . deep || build mode == build type . populate_from_remote_cache ) { return optional . empty ( ) ; },modes skip builds
right paren encountered matched <PLACE_HOLDER> of expression node @$ or end of expression matched with a left paren node .,if ( n . f precedence != p ) { error ( rbbi rule builder . u_brk_mismatched_paren ) ; },paren encountered start
the above method will perform the quit as long as the user does not cancel the <PLACE_HOLDER>,td . display user save ( tool dialog . quit ) ;,user cancel request
pipeline should have ssl <PLACE_HOLDER> and server tls <PLACE_HOLDER>,iterator < map . entry < string @$ channel handler > > iterator = pipeline . iterator ( ) ; assert that ( iterator . next ( ) . get value ( ) ) . is instance of ( ssl handler . class ) ;,pipeline have handler
if the patch <PLACE_HOLDER> does n't equal port <PLACE_HOLDER> @$ return error here in case of mismatch between audio ports and audio patches .,if ( patch generation [ __num__ ] != port generation [ __num__ ] ) { return error ; } for ( int i = __num__ ; i < new patches . size ( ) ; i ++ ) { for ( int j = __num__ ; j < new patches . get ( i ) . sources ( ) . length ; j ++ ) { audio port config port cfg = update port config ( new patches . get ( i ) . sources ( ) [ j ] @$ new ports ) ; new patches . get ( i ) . sources ( ) [ j ] = port cfg ; } for ( int j = __num__ ; j < new patches . get (,generation equal generation
chunk contains <PLACE_HOLDER> from glmmme run,glm weights fun [ ] glmfun rand = null ; long chk start row idx = chunks [ __num__ ] . start ( ) ;,chunk contains infos
bits consumed range : 1 to 8 @$ where 8 indicates last <PLACE_HOLDER> fully consumed,bits consumed = __num__ - last bitfield dt . get bit offset ( ) ;,8 indicates bit
we rebuild the the external <PLACE_HOLDER> using the has of the parent <PLACE_HOLDER> @$ the shared timestamp and the sequence number,string time stamp = string utils . substring after ( internal meeting id @$ __str__ ) ; string external hash = digest utils . sha1 hex ( ( parent meeting id + __str__ + time stamp + __str__ + params . get ( api params . sequence ) ) ) ; external meeting id = external hash + __str__ + time stamp ;,the has issue
m fake dragging is true when a fake drag is interrupted by an a 11 y command set it to false so end fake drag wo n't fling the recycler <PLACE_HOLDER>,m fake dragging = false ; boolean has new target = m target != target ; m target = target ; dispatch state changed ( scroll_state_settling ) ; if ( has new target ) { dispatch selected ( target ) ; },drag fling view
the while loop will create a rotating <PLACE_HOLDER>,int current index = i ; int proper doc id ; while ( actual doc id != ( proper doc id = sorted doc ids [ current index ] ) ) { int current doc id = current index + start doc id offset ; _data buffer . copy to ( proper doc id * _doc size long @$ _data buffer @$ current doc id * _doc size long @$ _doc size long ) ; sorted doc ids [ current index ] = current doc id ; current index = proper doc id - start doc id offset ; },loop create order
this condition matches the head <PLACE_HOLDER> but has some inputs those are created by notification .,return inputs == null || ( ( collection ) inputs ) . is empty ( ) || ( ( collection < map < string @$ object > > ) inputs ) . stream ( ) . any match ( m -> ! __str__ . equals ( m . get ( __str__ ) ) ) ;,condition matches key
set max queue <PLACE_HOLDER> to 3 so that 2 calls from the test wo n't trigger back off because the queue is full .,rpc . builder builder = new server builder ( conf ) . set queue size per handler ( queue size per handler ) . set num handlers ( __num__ ) . set verbose ( true ) ; return setup test server ( builder ) ;,max queue size
we enqueued a pending frame @$ let 's try <PLACE_HOLDER> else next .,break ;,let try something
atomically run this function on a clone of the bits from the existing key and install the result as the new value . this function may run multiple <PLACE_HOLDER> if there are collisions .,atomic q = new atomic2 ( ) ; q . invoke ( key ) ;,function run times
always has a left <PLACE_HOLDER> .,node left node = nodes [ left index ] ; float left value = left node . value ;,always has node
fail rarely ... but allow failure as this one has no <PLACE_HOLDER>,if ( math . random ( ) > __num__ ) { throw new runtime exception ( __str__ ) ; },one has effect
all elements of each list must have equal <PLACE_HOLDER> so we find the <PLACE_HOLDER> using the first element .,strict bit vector res = new strict bit vector ( alist . get ( __num__ ) . get size ( ) + blist . get ( __num__ ) . get size ( ) ) ; list < strict bit vector > products = int stream . range ( __num__ @$ alist . size ( ) ) . parallel ( ) . map to obj ( i -> multiply without reduction ( alist . get ( i ) @$ blist . get ( i ) ) ) . collect ( collectors . to list ( ) ) ; products . stream ( ) . reduce ( res @$ ( a @$ b ) -> { a . xor ( b ) ; return a ; } ) ; return,elements have length
mssql bukl insert can only use local <PLACE_HOLDER> @$ so that 's what we limit ourselves to .,if ( ! ( file object instanceof local file ) ) { throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ vfs filename ) ) ; },insert use files
ensure that rtr is doing <PLACE_HOLDER> and still making progress,tasks [ __num__ ] = test tasks . unending ( __str__ ) ; results [ __num__ ] = remote task runner . run ( tasks [ __num__ ] ) ; wait for one worker to have unacked tasks ( ) ; if ( rtr test utils . task announced ( __str__ @$ tasks [ __num__ ] . get id ( ) ) ) { rtr test utils . mock worker running task ( __str__ @$ tasks [ __num__ ] ) ; rtr test utils . mock worker complete successful task ( __str__ @$ tasks [ __num__ ] ) ; } else { rtr test utils . mock worker running task ( __str__ @$ tasks [ __num__ ] ) ; rtr test utils . mock worker complete successful task,rtr doing something
this prevents the case where a program represents the entire address <PLACE_HOLDER> in which everything looks like a pointer,if ( program is entire memory space ( program ) ) { return null ; },program represents table
mark all classes with package visible members . mark all exception catches of methods . count all method <PLACE_HOLDER> . mark super <PLACE_HOLDER> and other access of methods .,stack size computer stack size computer = new stack size computer ( ) ; program class pool . classes accept ( new multi class visitor ( new class visitor [ ] { new package visible member containing class marker ( ) @$ new all constant visitor ( new package visible member invoking class marker ( ) ) @$ new all method visitor ( new optimization info member filter ( new all attribute visitor ( new multi attribute visitor ( new attribute visitor [ ] { stack size computer @$ new catch exception marker ( ) @$ new all instruction visitor ( new multi instruction visitor ( new instruction visitor [ ] { new super invocation marker ( ) @$ new dynamic invocation marker ( ) @$ new backward,mark mark methods
explicitly setting service rpc for datanode . this because dfs util.get nn service rpc addresses for cluster looks up client facing port and service port at the same time @$ and if no setting for service rpc @$ it would return client port @$ in this case @$ it will be the auxiliary port for data node . which is not <PLACE_HOLDER> auxiliary is,cluster conf . set ( dfs_namenode_service_rpc_address_key @$ __str__ ) ; cluster conf . set ( common configuration keys . hadoop_security_sasl_props_resolver_class @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set ( __str__ @$ __str__ ) ; cluster conf . set boolean ( dfs_namenode_send_qop_enabled @$ true ) ;,auxiliary for what
domain attribute must contain at least one embedded <PLACE_HOLDER> @$ or the value must be equal to .local .,int dot index = cookie domain . index of ( __str__ @$ __num__ ) ; if ( ( ( dot index < __num__ ) || ( dot index == cookie domain . length ( ) - __num__ ) ) && ( ! cookie domain . equals ( __str__ ) ) ) { throw new malformed cookie exception ( __str__ + cookie . get domain ( ) + __str__ + __str__ ) ; },attribute contain dot
start the truncate procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new truncate table procedure ( proc exec . get environment ( ) @$ table name @$ preserve splits ) ) ; test recovery and double execution ( util @$ proc id @$ step ) ; procedure testing utility . set kill and toggle before store update ( proc exec @$ false ) ; util . wait until all regions assigned ( table name ) ;,procedure kill executor
no container to cleanup . cleanup app level <PLACE_HOLDER> .,if ( app . containers . is empty ( ) ) { app . handle app finish with containers cleanedup ( ) ; return application state . application_resources_cleaningup ; },container app resources
try all delimiters and choose the delimiter which yields the shortest <PLACE_HOLDER> .,int min frame length = integer . max_value ; channel buffer min delim = null ; for ( channel buffer delim : delimiters ) { int frame length = index of ( buffer @$ delim ) ; if ( frame length >= __num__ && frame length < min frame length ) { min frame length = frame length ; min delim = delim ; } } if ( min delim != null ) { int min delim length = min delim . capacity ( ) ; channel buffer frame ; if ( discarding too long frame ) { discarding too long frame = false ; buffer . skip bytes ( min frame length + min delim length ) ; int too long frame length = this . too,which yields frame
path is n't supported @$ mimic resolver <PLACE_HOLDER> .,if ( remote locations . is empty ( ) ) { return null ; },path mimic behavior
let system take <PLACE_HOLDER> for time tick events .,final work source work source = null ;,system take ownership
so that future open gl revisions wo n't break jme <PLACE_HOLDER> fall through intentional,case __num__ : caps . add ( caps . glsl450 ) ; case __num__ : caps . add ( caps . glsl440 ) ; case __num__ : caps . add ( caps . glsl430 ) ; case __num__ : caps . add ( caps . glsl420 ) ; case __num__ : caps . add ( caps . glsl410 ) ; case __num__ : caps . add ( caps . glsl400 ) ; case __num__ : caps . add ( caps . glsl330 ) ; case __num__ : caps . add ( caps . glsl150 ) ; case __num__ : caps . add ( caps . glsl140 ) ; case __num__ : caps . add ( caps . glsl130 ) ; case __num__ : caps . add ( caps,revisions break stores
note : the original implementation had both <PLACE_HOLDER> calling the 3 arg method . this is preserved but perhaps it should pass the args array instead of null .,for ( int i = __num__ ; i < ifcs . length ; i ++ ) { method = internal find method ( ifcs [ i ] @$ method name @$ arg count @$ null ) ; if ( method != null ) { break ; } },implementation had methods
both the collapsed and full qs panels get this <PLACE_HOLDER> @$ this check determines which one should handle showing the detail .,if ( should show detail ( ) ) { qs panel . this . show detail ( show @$ r ) ; } if ( m detail record == r ) { fire toggle state changed ( state ) ; } r . scan state = state ; if ( m detail record == r ) { fire scan state changed ( r . scan state ) ; } if ( announcement != null ) { m handler . obtain message ( h . announce_for_accessibility @$ announcement ) . send to target ( ) ; },collapsed get notification
font shapes define no fillstyles per se @$ but do reference fillstyle index 1 @$ which represents the font <PLACE_HOLDER> . we just report null in this case .,begin fill ( null ) ;,which represents color
we are interested in this fetch only if the beginning offset matches the current consumed <PLACE_HOLDER>,if ( error == errors . none ) { subscription state . fetch position position = subscriptions . position ( tp ) ; if ( position == null || position . offset != fetch offset ) { log . debug ( __str__ + __str__ @$ tp @$ fetch offset @$ position ) ; return null ; } log . trace ( __str__ @$ partition . records . size in bytes ( ) @$ tp @$ position ) ; iterator < ? extends record batch > batches = partition . records . batches ( ) . iterator ( ) ; completed fetch = next completed fetch ; if ( ! batches . has next ( ) && partition . records . size in bytes ( ) > __num__ ),beginning matches position
the first round of scan uses the m iterator left from last eviction . then scan the <PLACE_HOLDER> from a new iterator for at most two round : first round to mark candidate.m is accessed as false @$ second round to remove the candidate from the <PLACE_HOLDER> .,int round to scan = __num__ ; while ( num to evict > __num__ && round to scan > __num__ ) { if ( ! m iterator . has next ( ) ) { m iterator = m pool . entry set ( ) . iterator ( ) ; round to scan -- ; } map . entry < k @$ resource > candidate map entry = m iterator . next ( ) ; resource candidate = candidate map entry . get value ( ) ; if ( candidate . m is accessed ) { candidate . m is accessed = false ; } else { if ( candidate . m ref count . compare and set ( __num__ @$ integer . min_value ) ) { m iterator,iterator scan pool
negative cols have <PLACE_HOLDER> of cols added,if ( cols [ __num__ ] < __num__ ) for ( int i = __num__ ; i < cols . length ; i ++ ) cols [ i ] += fr . num cols ( ) ;,cols have number
intensity steps hr distance <PLACE_HOLDER>,return sample ;,steps hr calories
add properties from builder configuration @$ after the injected build <PLACE_HOLDER> .,final variable resolver < string > resolver = new union < > ( new by map < > ( env ) @$ vr ) ; args . add key value pairs from property string ( __str__ @$ this . properties @$ resolver @$ sensitive vars ) ; if ( uses private repository ( ) ) args . add ( __str__ + build . get workspace ( ) . child ( __str__ ) ) ; args . add tokenized ( normalized target ) ; wrap up arguments ( args @$ normalized target @$ build @$ launcher @$ listener ) ; build env vars ( env @$ mi ) ; if ( ! launcher . is unix ( ) ) { args = args . to windows command ( ),injected build rules
we need some way to tell <PLACE_HOLDER> the real author is . right now i 'm just going to arbitrarily decide that this is via the document title @$ as i ca n't really think of an easy way to do it otherwise ; no matter what we decide to use @$ the test set will need to be prepped before hand regardless .,for ( string test doc : results . key set ( ) ) { document curr test doc = null ; for ( document d : test docs ) { if ( d . get title ( ) . equals ( test doc ) ) { curr test doc = d ; break ; } } string selected author = __str__ ; double max = __num__ ; for ( string potential author : results . get ( curr test doc . get title ( ) ) . key set ( ) ) { if ( results . get ( curr test doc . get title ( ) ) . get ( potential author ) . double value ( ) > max ) { max = results . get,author is what
null params indicate external subject <PLACE_HOLDER> . no <PLACE_HOLDER> context will be attached .,return do subject login ( subject @$ null ) ;,params indicate user
v 0 has the smallest <PLACE_HOLDER> @$ stricter checking is done later,if ( record size < legacy record . record_overhead_v0 ) throw new corrupt record exception ( string . format ( __str__ @$ record size @$ legacy record . record_overhead_v0 ) ) ; if ( record size > max message size ) throw new corrupt record exception ( string . format ( __str__ @$ record size @$ max message size ) ) ; if ( remaining < header_size_up_to_magic ) return null ; byte magic = buffer . get ( buffer . position ( ) + magic_offset ) ; if ( magic < __num__ || magic > record batch . current_magic_value ) throw new corrupt record exception ( __str__ + magic ) ; return record size + log_overhead ;,v has overhead
use a condition that folds after floating guards and before guard lowering . after disabling pea and read elimination @$ the following condition does the <PLACE_HOLDER> .,if ( static value == static value ) { value = a . x ; result = foo ( a ) ; },condition does work
clone the call to prevent a <PLACE_HOLDER> with another thread stomping on the response while being sent . the original call is effectively discarded since the wait count wo n't hit zero,call = new rpc call ( this ) ; setup response ( call @$ status @$ rpc error code proto . error_rpc_server @$ null @$ t . get class ( ) . get name ( ) @$ string utils . stringify exception ( t ) ) ; setup response ( call @$ call . response params . return status @$ call . response params . detailed err @$ call . rv @$ call . response params . error class @$ call . response params . error ) ;,call prevent recursion
close server 2 and pause so server has a <PLACE_HOLDER> to close,close server ( server2 ) ; wait . pause ( __num__ * __num__ ) ; wait for cqs disconnected ( client @$ __str__ @$ __num__ ) ;,server has chance
hand reset the big table <PLACE_HOLDER> .,for ( int column : big table retain column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } for ( int column : non outer small table key column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } if ( outer small table key column map != null ) { for ( int column : outer small table key column map ) { column vector col vector = overflow batch . cols [ column ] ; col vector . reset ( ) ; } },hand reset columns
the view root <PLACE_HOLDER>,vertical layout view layout = new vertical layout ( fields ) ; view layout . set size full ( ) ; view layout . set component alignment ( fields @$ alignment . middle_center ) ; view layout . set style name ( reindeer . layout_blue ) ; set composition root ( view layout ) ;,view root layout
send two task start and two task fail <PLACE_HOLDER> for tasks 0 and 1,mockito . when ( reader . get next event ( ) ) . then answer ( new answer < history event > ( ) { public history event answer ( invocation on mock invocation ) throws io exception { int event id = num events read . get and increment ( ) ; taskid tid = tids [ event id & __num__ ] ; if ( event id < __num__ ) { return new task started event ( tid @$ __num__ @$ task type @$ __str__ ) ; } if ( event id < __num__ ) { task failed event tfe = new task failed event ( tid @$ __num__ @$ task type @$ __str__ @$ __str__ @$ null @$ new counters ( ) ) ; tfe .,start fail events
constrain the maximum state version this backup agent can handle in case a newer or corrupt backup <PLACE_HOLDER> existed,if ( state version > state_version ) { state version = state_version ; },agent handle session
only privileged apps and updated privileged apps can add child <PLACE_HOLDER> .,if ( pkg . child packages != null && ! pkg . child packages . is empty ( ) ) { if ( ( scan flags & scan_as_privileged ) == __num__ ) { throw new package manager exception ( __str__ + __str__ + pkg . package name ) ; } final int child count = pkg . child packages . size ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { package parser . package child pkg = pkg . child packages . get ( i ) ; if ( m settings . has other disabled system pkg with childl pr ( pkg . package name @$ child pkg . package name ) ) { throw new package manager,apps add packages
need to extend this test to validate that different users can create <PLACE_HOLDER> of exact same name . so far only create followed by build is tested . need to test create followed by create .,client . action destroy ( same app name ) ;,users create files
was this condition here because it can be set by set camera ? set camera will not trigger events for camera listeners ? or other cause ? actually we can say if we are starting fling here if reason is set @$ was it an original condition <PLACE_HOLDER> here ?,if ( m camera change tracker . is empty ( ) ) { m camera change tracker . set reason ( reason ) ; log . d ( __str__ @$ __str__ + reason + __str__ + m camera change tracker . is user interaction ( ) + __str__ + m camera change tracker . is animated ( ) ) ; handle map changed event ( event types . region_will_change ) ; } else { log . d ( __str__ @$ __str__ ) ; },reason set anyways
check that ranks provide a valid topological <PLACE_HOLDER>,for ( int v = __num__ ; v < g . v ( ) ; v ++ ) { for ( int w : g . adj ( v ) ) { if ( rank ( v ) > rank ( w ) ) { system . err . printf ( __str__ @$ v @$ w @$ v @$ rank ( v ) @$ w @$ rank ( w ) ) ; return false ; } } },ranks provide order
set up for the schema factory and then set a global one which overrides the <PLACE_HOLDER> .,conf . set ( committer_factory_class @$ __str__ ) ; create committer factory ( simple committer factory . class @$ http_path @$ conf ) ;,which overrides others
wait a bit to make sure compiler had a <PLACE_HOLDER> to do it 's stuff :,try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { } start time = system . nano time ( ) ; record loop with expected interval ( histogram @$ timing loop count @$ expected interval ) ; end time = system . nano time ( ) ; delta usec = ( end time - start time ) / __num__ ; rate = __num__ * timing loop count / delta usec ; system . out . println ( label + __str__ ) ; system . out . println ( label + timing loop count + __str__ + delta usec + __str__ + rate + __str__ ) ; rate = __num__ * histogram . get total count ( ) / delta usec ;,compiler had cache
private method call leaving the invokespecial alone will cause a verify <PLACE_HOLDER>,if ( owner . equals ( classname ) ) { string descriptor = utils . insert extra parameter ( owner @$ desc ) ; super . visit method insn ( invokestatic @$ utils . get executor name ( classname @$ suffix ) @$ name @$ descriptor @$ false ) ; return ; } else { type descriptor supertype descriptor = get type ( owner ) ; method member target = supertype descriptor . get by name and descriptor ( name + desc ) ; if ( target != null && target . is protected ( ) ) { super . visit method insn ( invokespecial @$ classname @$ name + method suffix super dispatcher @$ desc @$ false ) ; } else { super . visit method insn,call cause error
use linked hash <PLACE_HOLDER> for predictable iteration order .,this . method to node = new linked hash map < > ( ) ;,use linked map
we are not interested in defining a new enter <PLACE_HOLDER> . instead we change default <PLACE_HOLDER> duration,get window ( ) . get enter transition ( ) . set duration ( get resources ( ) . get integer ( r . integer . anim_duration_long ) ) ;,new enter animation
this is all jealously ; should be refactored to ask tibm if it wants to be filtered for rejoin and eliminate this horrible introspection . this implementation mimics the original live rejoin <PLACE_HOLDER> for execution site ... multi part ad hoc does not need to be checked because its an alias and runs procedure as planned .,if ( ! msg . is sys proc task ( ) ) { return true ; },implementation mimics logic
sleep to ensure session registry impl will update <PLACE_HOLDER>,thread . sleep ( __num__ ) ;,session update session
computation : unconfigured build <PLACE_HOLDER> to raw <PLACE_HOLDER> node computation,default unconfigured target node factory raw target node factory = new default unconfigured target node factory ( params . get known rule types provider ( ) @$ new built target verifier ( ) @$ cell . get cell path resolver ( ) @$ new selector list factory ( new selector factory ( params . get unconfigured build target factory ( ) ) ) ) ; build target to unconfigured target node computation build target to unconfigured target node computation = build target to unconfigured target node computation . of ( raw target node factory @$ cell ) ;,unconfigured build target
looks like we 're going to need the buffers ... we know the new string will be shorter . using the old length may overshoot a <PLACE_HOLDER> @$ but it will save us from resizing the buffer .,if ( decoded == null ) { decoded = new string buffer ( old length ) ; out = new byte array output stream ( __num__ ) ; } else { out . reset ( ) ; },looks overshoot break
during this time @$ one of the threads should initialize the partition <PLACE_HOLDER> @$ and then all of the threads should get the new ring .,assert equals ( count done ( results ) @$ results . size ( ) ) ; executor . shutdown now ( ) ;,one initialize pool
workers have some <PLACE_HOLDER> to complete their in progress tasks,long until = system . current time millis ( ) + shutdown timeout in ms ; log . debug ( __str__ @$ shutdown timeout in ms ) ; while ( system . current time millis ( ) < until && ce worker controller . has at least one processing worker ( ) ) { thread . sleep ( __num__ ) ; },workers have time
missing response gets <PLACE_HOLDER> around,for ( int row = __num__ ; row < y [ __num__ ] . _len ; ++ row ) { if ( y [ __num__ ] . isna ( row ) ) { if ( ( start + row ) % nfolds == test fold ) y [ __num__ ] . set ( row @$ test fold ) ; } else { if ( y [ __num__ ] . at8 ( row ) == ( classes == null ? class label : classes [ class label ] ) ) { if ( test fold == get fold id ( start + row @$ seeds [ class label ] ) ) y [ __num__ ] . set ( row @$ test fold ) ; } } },response gets backwards
remove all declared <PLACE_HOLDER> and find out which modules were used while at it .,cc compilation context . headers and modules headers and modules = cc compilation context . compute declared headers and used modules ( use pic @$ undeclared headers @$ header info ) ; used modules = immutable list . copy of ( headers and modules . modules ) ; undeclared headers . remove all ( headers and modules . headers ) ;,all declared headers
weird <PLACE_HOLDER> : does not find <PLACE_HOLDER> @$ because it scans for consistent case only,assert false ( r . contains ignore case ( __str__ ) ) ;,not find exception
it will be checked inside the put <PLACE_HOLDER> that poor <PLACE_HOLDER> does not overwrite rich <PLACE_HOLDER>,segment . fulltext ( ) . put metadata ( entry ) ;,metadata overwrite metadata
... then fetch the <PLACE_HOLDER> for the specified uv set name ...,list < vector2f > uvs for name = uvs for material . get ( uv set name ) ; if ( uvs for name == null ) { uvs for name = new array list < vector2f > ( ) ; uvs for material . put ( uv set name @$ uvs for name ) ; },then fetch vs
blk can see <PLACE_HOLDER> back,epsilon ( blk end @$ loop ) ;,blk see the
set stopped if no more write <PLACE_HOLDER> tp meta tables since last time we went around the loop . any open meta regions will be closed on our way out .,if ( all user regions offline ) { if ( old request count == get write request count ( ) ) { stop ( __str__ ) ; break ; } old request count = get write request count ( ) ; } else { close user regions ( this . abort requested ) ; },tables write requests
note : these lines center the <PLACE_HOLDER> dialog in the current window .,point p = main frame . get location on screen ( ) ; dimension d1 = main frame . get size ( ) ; dimension d2 = dialog . get size ( ) ; dialog . set location ( p . x + ( d1 . width - d2 . width ) / __num__ @$ p . y + ( d1 . height - d2 . height ) / __num__ ) ; dialog . set visible ( true ) ;,lines center icon
push view and context related <PLACE_HOLDER> from android to flutter .,send user settings to flutter ( ) ; send locales to flutter ( get resources ( ) . get configuration ( ) ) ; send viewport metrics to flutter ( ) ;,view related events
check if the range is preceding the <PLACE_HOLDER> or after it .,boolean is before = range . second . equals ( quote begin token index - __num__ ) ;,range preceding call
revisit : the following should also update id <PLACE_HOLDER>,attr . set node value ( attr value ) ; index = f element . set xerces attribute node ( attr ) ; f augmentations . insert element at ( new augmentations impl ( ) @$ index ) ; attr . set specified ( false ) ;,the update node
update video track count @$ audio & subtitle track <PLACE_HOLDER> .,m video track count = __num__ ; m audio tracks = new array list < > ( ) ; m subtitle tracks = new array list < > ( ) ; m selected audio track index = __num__ ;,audio track list
checks for entries that failed to import . get failures provides up to <PLACE_HOLDER> of the first failed entries for the job @$ if any exist .,list < string > failed endpoints = get import job result . get import job response ( ) . get failures ( ) ; if ( failed endpoints != null ) { system . out . println ( __str__ ) ; for ( string failed endpoint : failed endpoints ) { system . out . println ( failed endpoint ) ; } },failures provides 100
assign playback info immediately such that all getters return the right <PLACE_HOLDER> .,playback info previous playback info = this . playback info ; this . playback info = playback info ; boolean is playing = is playing ( ) ; notify listeners ( new playback info update ( playback info @$ previous playback info @$ listeners @$ track selector @$ position discontinuity @$ position discontinuity reason @$ timeline change reason @$ seek processed @$ play when ready @$ previous is playing != is playing ) ) ;,getters return values
durable client registers durable <PLACE_HOLDER> on server,register interest ( durable clientvm @$ region name @$ true @$ interest result policy . keys_values ) ;,client registers cq
initialize drag <PLACE_HOLDER> .,previous touch point px . set ( e . getx ( ) @$ e . gety ( ) ) ; return true ;,initialize drag background
lazy definition of schema : do not write empty <PLACE_HOLDER>,final boolean solrlazy = get config bool ( switchboard constants . federated_service_solr_indexing_lazy @$ true ) ;,definition write tables
make sure the user now has defaultproc <PLACE_HOLDER>,try { user client . call procedure ( __str__ @$ __num__ @$ __num__ ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; fail ( __str__ ) ; } threw = false ; try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { assert true ( pce . get message ( ) . contains ( __str__ ) ) ; threw = true ; } assert true ( __str__ @$ threw ) ; threw = false ; try { admin client . call procedure ( __str__ @$ __str__ ) ; } catch ( proc call exception pce ) { assert true ( pce . get message ( ),user has permissions
now @$ both the close action and a grab for an write page <PLACE_HOLDER> is waiting for our first thread . when we release that <PLACE_HOLDER> @$ we should see that either close completes and our second thread @$ the one b<PLACE_HOLDER>ed on the write <PLACE_HOLDER> @$ gets an exception @$ or we should see that the second thread gets the <PLACE_HOLDER> @$ and,unlock latch . count down ( ) ;,thread gets lock
we need two certificates @$ one for the client and one for the server . the client must know the server 's public <PLACE_HOLDER> to make a curve connection .,z cert client cert = new z cert ( ) ; z cert server cert = new z cert ( ) ;,client know key
check sm <PLACE_HOLDER> and member access before cracking .,try { check access ( ref kind @$ defc @$ member ) ; check security manager ( defc @$ member ) ; } catch ( illegal access exception ex ) { throw new illegal argument exception ( ex ) ; } if ( allowed modes != trusted && member . is caller sensitive ( ) ) { class < ? > caller class = target . internal caller class ( ) ; if ( ! has private access ( ) || caller class != lookup class ( ) ) throw new illegal argument exception ( __str__ + caller class ) ; },check sm owner
now the block has 10 internal <PLACE_HOLDER> .,assert equals ( __num__ @$ new dn locs . length ) ;,block has locations
now let 's try to do an upsert where the outcome relies <PLACE_HOLDER> on doing an update @$ doing an insert and also triggering a delete .,client . call procedure ( __str__ @$ __num__ @$ __num__ @$ __num__ ) ; client . call procedure ( __str__ @$ __num__ @$ __num__ @$ __num__ ) ;,outcome relies yes
each follower may have just sent a leader check @$ which receives no <PLACE_HOLDER>,cluster . stabilise ( math . max ( default millis ( leader_check_timeout_setting ) + default millis ( leader_check_interval_setting ) + default_delay_variability + default_election_delay @$ default millis ( follower_check_timeout_setting ) + default millis ( follower_check_interval_setting ) + default_delay_variability ) + default_cluster_state_update_delay + default_cluster_state_update_delay ) ;,which receives response
configure preexechooks with disallow transform hook to disallow transform <PLACE_HOLDER>,if ( session ctx . get client type ( ) == hive authz session context . client_type . hiveserver2 && hive conf . get bool var ( hive conf . conf vars . hive_authorization_enabled ) ) { string hooks = hive conf . get var ( hive conf . conf vars . preexechooks ) . trim ( ) ; if ( hooks . is empty ( ) ) { hooks = disallow transform hook . class . get name ( ) ; } else { hooks = hooks + __str__ + disallow transform hook . class . get name ( ) ; } log . debug ( __str__ + hooks ) ; hive conf . set var ( hive conf . conf vars . preexechooks @$ hooks ),preexechooks transform information
this object holds its <PLACE_HOLDER> . we want to write a send channel that the other side can use to retrieve that <PLACE_HOLDER> .,if ( m have data ) { if ( m send channel == null ) { m send channel = new send channel ( this ) ; } out . write strong binder ( m send channel ) ; } else { out . write strong binder ( m receive channel ) ; },object holds data
check that slave satisfies min <PLACE_HOLDER> .,if ( cont . get resource ( ) . get virtual cores ( ) < props . cpus per node ( ) || cont . get resource ( ) . get memory ( ) < props . total memory per node ( ) ) { log . log ( level . fine @$ __str__ @$ new object [ ] { cont . get node id ( ) . get host ( ) @$ cont . get resource ( ) . get virtual cores ( ) @$ cont . get resource ( ) . get memory ( ) } ) ; return false ; } return true ;,slave satisfies requirements
verify login user got a new <PLACE_HOLDER> .,login user . relogin from keytab ( ) ; kerberos ticket new login ticket = check ticket and keytab ( login user @$ principal1 @$ true ) ; assert . assert not equals ( login ticket . get auth time ( ) @$ new login ticket . get auth time ( ) ) ;,user got ticket
doesnt matter <PLACE_HOLDER> node.id and voldemort.home values are for this test,props . set property ( __str__ @$ __str__ ) ; props . set property ( __str__ @$ __str__ ) ; voldemort config = new voldemort config ( props ) ; if ( this . prefix partition id ) { voldemort config . set rocksdb prefix keys with partition id ( true ) ; } this . rocks db config = new rocks db storage configuration ( voldemort config ) ; this . rocks db store = ( rocks db storage engine ) rocks db config . get store ( test utils . make store definition ( __str__ ) @$ test utils . make single node routing strategy ( ) ) ; random = new random ( ) ;,values test which
instantiate new headless analyzer and parse <PLACE_HOLDER> .,headless analyzer analyzer = headless analyzer . get loggable instance ( log file @$ script log file @$ true ) ; headless options options = analyzer . get options ( ) ; parse options ( options @$ args @$ option start index @$ ghidraurl @$ files to import ) ;,headless analyzer options
check if any parameter is referenced . if so @$ user must have read <PLACE_HOLDER> on the parameter context,for ( final string proposed property value : proposed properties . values ( ) ) { parameter token list token list = parameter parser . parse tokens ( proposed property value ) ; if ( ! token list . to reference list ( ) . is empty ( ) ) { references parameter = true ; break ; } },user read permission
in case we have a pending <PLACE_HOLDER> @$ <PLACE_HOLDER> now . see m pending <PLACE_HOLDER> for more information .,synchronized ( this ) { key = m pending repin . get or default ( uid @$ - __num__ ) ; if ( key == - __num__ ) { return ; } m pending repin . remove ( uid ) ; },repin see repaint
test trace signature uniqueness touch the trace <PLACE_HOLDER>,fs . delete ( test trace file @$ false ) ;,uniqueness touch file
fair scheduler does n't support this <PLACE_HOLDER> @$ set capacity scheduler as the scheduler for this <PLACE_HOLDER> .,conf . set ( yarn configuration . rm_scheduler @$ capacity scheduler . class . get name ( ) ) ; m clock = mock ( clock . class ) ; mcs = mock ( capacity scheduler . class ) ; when ( mcs . get resource calculator ( ) ) . then return ( rc ) ; lm = mock ( rm node labels manager . class ) ; try { when ( lm . is exclusive node label ( any string ( ) ) ) . then return ( true ) ; } catch ( io exception e ) { },scheduler support instance
offset the start so keys do n't overlap and cause constraint <PLACE_HOLDER>,m_rangemin = m_rows * topicnum ;,keys overlap violations
close the fd via the parent stream 's close <PLACE_HOLDER> . the parent will reinvoke our close <PLACE_HOLDER> @$ which is defined in the superclass abstract interruptible channel @$ but the is open logic in that <PLACE_HOLDER> will prevent this <PLACE_HOLDER> from being reinvoked .,if ( parent != null ) { ( ( java . io . closeable ) parent ) . close ( ) ; } else { nd . close ( fd ) ; },method prevent method
if there was more than one @$ see <PLACE_HOLDER> one can take us forward the most words,if ( candidates > __num__ ) { if ( f iter . get index ( ) < range end ) { found best : do { int words matched = __num__ ; if ( words [ ( words found + __num__ ) % thai_lookahead ] . candidates ( f iter @$ f dictionary @$ range end ) > __num__ ) { if ( words matched < __num__ ) { words [ words found % thai_lookahead ] . mark current ( ) ; words matched = __num__ ; } if ( f iter . get index ( ) >= range end ) { break found best ; } do { if ( words [ ( words found + __num__ ) % thai_lookahead ] . candidates ( f iter @$,one take which
let 's define the schema of the data that we want to import the <PLACE_HOLDER> in which columns are defined here should match the <PLACE_HOLDER> in which they appear in the input data,schema input data schema = new schema . builder ( ) . add column string ( __str__ ) . add column categorical ( __str__ @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ) . add column string ( __str__ ) . build ( ) ;,order match order
only one shard should have all imported <PLACE_HOLDER> @$ since we have the same routing for both <PLACE_HOLDER>,response = execute ( __str__ ) ; assert that ( response . rows ( ) [ __num__ ] [ __num__ ] @$ is ( __num__ ) ) ;,shard have data
units can not be new because wdtk represents <PLACE_HOLDER> as strings already,return null ;,wdtk represents them
cassandra connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support comment
striped executor uses a custom <PLACE_HOLDER> .,if ( striped exec svc != null ) { monitor striped pool ( striped exec svc ) ; },executor uses impl
server provided the first <PLACE_HOLDER>,if ( sasl auth type . has challenge ( ) ) { challenge token = sasl auth type . get challenge ( ) . to byte array ( ) ; sasl auth type = sasl auth . new builder ( sasl auth type ) . clear challenge ( ) . build ( ) ; } else if ( sasl client . has initial response ( ) ) { challenge token = new byte [ __num__ ] ; },server provided response
<PLACE_HOLDER> str not str <PLACE_HOLDER> since shows up in intelli j next to <PLACE_HOLDER>,vec [ ] vecs = vecs ( ) ; string s [ ] = new string [ vecs . length ] ; for ( int i = __num__ ; i < vecs . length ; ++ i ) s [ i ] = vecs [ i ] . get_type_str ( ) ; return s ;,types str type
remove the register shutdown hook which disconnects the <PLACE_HOLDER> from the distributed system upon jvm shutdown,remove shutdown hook ( ) ; logger . info ( __str__ ) ; logging session . shutdown ( ) ;,which disconnects component
now we know that both implementation yielded same <PLACE_HOLDER>,if ( got status == - __num__ ) { if ( ! got . to string ( ) . equals ( exp . to string ( ) ) ) { errln ( __str__ + refidna name + __str__ + uidna name + __str__ + exp + __str__ + got + __str__ + prettify ( src ) + __str__ + options ) ; } } else { logln ( __str__ + refidna name + __str__ + uidna name + __str__ + prettify ( src ) + __str__ + options ) ; },implementation yielded result
nm 1 do 50 <PLACE_HOLDER>,donm heartbeat ( rm1 @$ nm1 . get node id ( ) @$ __num__ ) ; check num of containers in an app on given node ( __num__ @$ nm1 . get node id ( ) @$ cs . get application attempt ( am2 . get application attempt id ( ) ) ) ; report nm1 = rm1 . get resource scheduler ( ) . get node report ( nm1 . get node id ( ) ) ; assert . assert equals ( __num__ * gb @$ report nm1 . get used resource ( ) . get memory size ( ) ) ; assert . assert equals ( __num__ * gb @$ report nm1 . get available resource ( ) . get memory size ( ) ) ;,nm do heartbeats
need to set nodelay or else batches larger than mtu can trigger 40 ms nagling <PLACE_HOLDER> .,conf copy . set boolean ( common configuration keys public . ipc_client_tcpnodelay_key @$ true ) ; rpc . set protocol engine ( conf copy @$ q journal protocolpb . class @$ protobuf rpc engine . class ) ; return security util . do as login user ( new privileged exception action < q journal protocol > ( ) { @ override public q journal protocol run ( ) throws io exception { rpc . set protocol engine ( conf copy @$ q journal protocolpb . class @$ protobuf rpc engine . class ) ; q journal protocolpb pbproxy = rpc . get proxy ( q journal protocolpb . class @$ rpc . get protocol version ( q journal protocolpb . class ) @$ addr @$ conf copy ),need trigger errors
google analytics uses no grouping <PLACE_HOLDER>,conversion meta . set grouping symbol ( null ) ;,analytics uses symbol
server did not perform the <PLACE_HOLDER> @$ so do n't leave an invalid entry here,if ( owner . get concurrency checks enabled ( ) && event . no version received from server ( ) ) { if ( is debug enabled ) { logger . debug ( __str__ @$ event ) ; } return false ; },server perform invalidation
check that null pd array throws <PLACE_HOLDER>,try { new access control context ( null ) ; throw new exception ( __str__ ) ; } catch ( exception e ) { if ( ! ( e instanceof null pointer exception ) ) { throw new exception ( __str__ ) ; } },array throws exception
the user has a preferred <PLACE_HOLDER> set and it just became available @$ thus setting it as active,if ( ( configured handler != null ) && configured handler . equals ( handler . get class ( ) . get name ( ) ) ) { set active popup message handler ( handler ) ; },user has handler
test fact handle <PLACE_HOLDER>,assert true ( fact handle == connected fact handle ) ; assert true ( ! ( fact handle == disconnected fact handle ) ) ;,fact handle type
the container fills its <PLACE_HOLDER> so we can use it orientation if it has one specified ; otherwise we prefer to use the orientation of its topmost child that has one specified and fall back on this container 's unset or unspecified value as a candidate if none of the children have a better candidate for the orientation .,if ( m orientation != screen_orientation_unset && m orientation != screen_orientation_unspecified ) { return m orientation ; } for ( int i = m children . size ( ) - __num__ ; i >= __num__ ; -- i ) { final window container wc = m children . get ( i ) ; final int orientation = wc . get orientation ( candidate == screen_orientation_behind ? screen_orientation_behind : screen_orientation_unset ) ; if ( orientation == screen_orientation_behind ) { candidate = orientation ; continue ; } if ( orientation == screen_orientation_unset ) { continue ; } if ( wc . fills parent ( ) || orientation != screen_orientation_unspecified ) { return orientation ; } },container fills parent
the mediacodec process has changed @$ clean up the old <PLACE_HOLDER> and client before we boost the new process @$ so that the state is left clean if things go wrong .,m boosted pid = - __num__ ; if ( m client != null ) { try { m client . unlink to death ( m death recipient @$ __num__ ) ; } catch ( exception e ) { } finally { m client = null ; } } try { client . link to death ( m death recipient @$ __num__ ) ; log . i ( tag @$ __str__ + pid + __str__ + process . thread_group_top_app ) ; process . set process group ( pid @$ process . thread_group_top_app ) ; m boosted pid = pid ; m client = client ; return package manager . permission_granted ; } catch ( exception e ) { log . e ( tag @$ __str__ + e ) ;,state clean process
loose signatures allow a return <PLACE_HOLDER> of object ...,return impl method . return type . equals ( __str__ ) || ( impl method . return type . equals ( __str__ ) && sdk method . return type . ends with ( __str__ ) ) ;,signatures allow type
simple counter does n't receive the <PLACE_HOLDER> after simple is removed from the composite,assert that ( simple counter . count ( ) ) . is equal to ( __num__ ) ; composite . add ( simple ) ; composite counter . increment ( ) ;,counter receive event
does offset exceed buffer <PLACE_HOLDER> or negative ?,if ( offset > data . limit ( ) || offset < __num__ ) { throw new assertion error ( ) ; },offset exceed limit
history guru constructor needs environment <PLACE_HOLDER> so no locking is done here .,history guru hist guru = history guru . get instance ( ) ;,constructor needs state
the context could hold a <PLACE_HOLDER> to a wgl surface data @$ which in turn has a <PLACE_HOLDER> back to this wgl graphics config @$ so in order for this instance to be disposed we need to break the connection,ogl render queue rq = ogl render queue . get instance ( ) ; rq . lock ( ) ; try { ogl context . invalidate current context ( ) ; } finally { rq . unlock ( ) ; },context hold reference
someone else wants to subscribe to our presence . ask <PLACE_HOLDER> for approval,swing utilities . invoke later ( ( ) -> { try { subscription request ( presence . get from ( ) . as bare jid ( ) ) ; } catch ( smack exception . not connected exception | interrupted exception e ) { log . warning ( __str__ + presence . get from ( ) @$ e ) ; } } ) ; break ; case unsubscribe :,someone ask everyone
do null byte buffer test <PLACE_HOLDER>,if ( ! null byte buffer test ( the mac ) ) { system . out . println ( __str__ ) ; return false ; } return true ;,byte buffer case
only play this sound if the feet hit the <PLACE_HOLDER>,if ( event . get character relative position ( ) . y == __num__ && character sounds . last sound time + min_time < time . get game time in ms ( ) ) { boolean old block is liquid = event . get old block ( ) . is liquid ( ) ; boolean new block is liquid = event . get new block ( ) . is liquid ( ) ; static sound sound = null ; if ( ! old block is liquid && new block is liquid ) { sound = random . next item ( character sounds . enter water sounds ) ; } else if ( old block is liquid && ! new block is liquid ) { sound = random .,feet hit screen
use the derived mean total <PLACE_HOLDER> as a side input,p collection < kv < string @$ integer > > filtered = sum scores . apply ( __str__ @$ par do . of ( new do fn < kv < string @$ integer > @$ kv < string @$ integer > > ( ) { private final counter num spammer users = metrics . counter ( __str__ @$ __str__ ) ; @ process element public void process element ( process context c ) { integer score = c . element ( ) . get value ( ) ; double gmc = c . side input ( global mean score ) ; if ( score > ( gmc * score_weight ) ) { log . info ( __str__ + c . element ( ) . get key ( ),derived mean score
begin pulse . note that it 's very important that the pulse finished callback be invoked when we 're done so that the caller can drop the pulse <PLACE_HOLDER> .,m pulse callback = callback ; m pulse reason = reason ;,caller drop record
check if listener @$ defined by classname @$ received all <PLACE_HOLDER>,list < activiti event > events = static test activiti event listener . get events received ( ) ; assert false ( events . is empty ( ) ) ; boolean insert found = false ; boolean delete found = false ; for ( activiti event e : events ) { if ( activiti event type . entity_created == e . get type ( ) ) { insert found = true ; } else if ( activiti event type . entity_deleted == e . get type ( ) ) { delete found = true ; } } assert true ( insert found ) ; assert true ( delete found ) ;,listener received events
exception should cause a <PLACE_HOLDER>,system . err . println ( ) ;,exception cause problem
which otherwise can cause <PLACE_HOLDER> with adding multiple entries to an ordered linked hash map,return new server web exchange matcher ( ) { @ override public mono < match result > matches ( server web exchange exchange ) { return server web exchange matcher . match result . match ( ) ; } } ;,which cause issues
one observer is the use <PLACE_HOLDER> group . the other observer removes the use <PLACE_HOLDER> upon the lifecycle 's destruction .,assert that ( m lifecycle . get observer count ( ) ) . is equal to ( __num__ ) ;,observer removes case
start playback and wait until playback reaches second <PLACE_HOLDER> .,action schedule action schedule = new action schedule . builder ( __str__ ) . pause ( ) . seek ( __num__ ) . wait for seek processed ( ) . seek ( __num__ ) . seek ( __num__ ) . wait for playback state ( player . state_ready ) . seek ( __num__ ) . play until start of window ( __num__ ) . seek ( __num__ ) . seek ( __num__ ) . play ( ) . build ( ) ;,playback reaches point
it makes some sense to return true @$ as no <PLACE_HOLDER> implies all shall pass the <PLACE_HOLDER> @$ but if this returns true @$ then any other <PLACE_HOLDER>s can be used as the source of the data to <PLACE_HOLDER> @$ which does n't make sense if this is meant to only be used by itself .,return false ;,all pass check
unknown method @$ let the delegate return a usual <PLACE_HOLDER> .,if ( method == null ) { return delegate ( ) . serve ( ctx @$ req ) ; },delegate return result
the owning rule 's divisor controls the <PLACE_HOLDER> of this substitution : rather than keeping a backpointer to the rule @$ we keep a copy of the divisor,this . divisor = rule . get divisor ( ) ; if ( divisor == __num__ ) { throw new illegal state exception ( __str__ + divisor + __str__ + description . substring ( __num__ @$ pos ) + __str__ + description . substring ( pos ) ) ; },divisor controls result
if mode is add <PLACE_HOLDER> check if the repository name does not exist in the repository list <PLACE_HOLDER> close this dialog if mode is edit <PLACE_HOLDER> check if the repository name is the same as before if not check if the new name does not exist in the repository . otherwise return true to this method @$ which will mean that repository already exist,if ( meta . get name ( ) != null ) { if ( mode == mode . add ) { if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ; } else { display repository already exist message ( meta . get name ( ) ) ; } } else { if ( master repository name . equals ( meta . get name ( ) ) ) { repository meta = meta ; hide ( ) ; } else if ( master repositories meta . search repository ( meta . get name ( ) ) == null ) { repository meta = meta ; hide ( ) ;,repository exist then
this is the first thread which looks up the <PLACE_HOLDER> this host or the cache entry for this host has been expired so this thread should do the lookup .,try { for ( name service name service : name services ) { try { addresses = name service . lookup all host addr ( host ) ; success = true ; break ; } catch ( unknown host exception uhe ) { if ( host . equals ignore case ( __str__ ) ) { inet address [ ] local = new inet address [ ] { impl . loopback address ( ) } ; addresses = local ; success = true ; break ; } else { addresses = unknown_array ; success = false ; ex = uhe ; } } } if ( req addr != null && addresses . length > __num__ && ! addresses [ __num__ ] . equals ( req addr ) ),which looks addresses
j label contains html <PLACE_HOLDER>,if ( at != null ) { return at . get before index ( part @$ index ) ; },label contains text
assert that only elements in the specified bucket shows up @$ and each element shows up 3 <PLACE_HOLDER> .,int bucket count = __num__ ; set < long > expected ids = long stream . range ( __num__ @$ row count ) . filter ( x -> bucket ids . contains ( to int exact ( x % bucket count ) ) ) . boxed ( ) . collect ( to immutable set ( ) ) ;,elements shows times
we can inherit parent key serde if user do not provide specific <PLACE_HOLDER>,return new k table impl < k @$ change < vr > @$ vr > ( k tablek table join node . node name ( ) @$ k tablek table join node . key serde ( ) @$ k tablek table join node . value serde ( ) @$ all source nodes @$ k tablek table join node . queryable store name ( ) @$ k tablek table join node . join merger ( ) @$ k tablek table join node @$ builder ) ;,user provide values
it is strange that an operation set presence does not have a presence <PLACE_HOLDER> so it may be safer to not mess with it .,forget presence status ( pps ) ; presence . remove provider presence status listener ( presence status listener ) ;,presence have status
confirm that peer with state a will reject replication <PLACE_HOLDER> .,verify replication request rejection ( util1 @$ true ) ; verify replication request rejection ( util2 @$ false ) ; util1 . get admin ( ) . disable replication peer ( peer_id ) ; write ( util1 @$ __num__ @$ __num__ ) ; thread . sleep ( __num__ ) ;,peer reject request
top left bottom left bottom <PLACE_HOLDER> top <PLACE_HOLDER>,top card . set card vertices ( new float [ ] { __num__ @$ bitmap . get height ( ) @$ __num__ @$ __num__ @$ bitmap . get height ( ) / __num__ @$ __num__ @$ bitmap . get width ( ) @$ bitmap . get height ( ) / __num__ @$ __num__ @$ bitmap . get width ( ) @$ bitmap . get height ( ) @$ __num__ } ) ;,bottom left right
the current system does not meet our <PLACE_HOLDER> to disconnect and call recursively to get a new system .,if ( need new system ) { get log writer ( ) . info ( __str__ ) ; disconnect fromds ( ) ; get system ( props ) ; },system meet requirement
to prevent concurrent remove see <PLACE_HOLDER> 41646,synchronized ( this . overflow map ) { overflow oplog oplog = get child ( ( int ) oplog id ) ; if ( oplog != null ) { oplog . remove ( dr @$ entry ) ; } },remove see bug
test pascal 255 select the fourth <PLACE_HOLDER>,select rows ( table @$ addr ( __num__ ) ) ; selected row = table . get selected row ( ) ; perform action ( make string action @$ model ) ;,255 select row
throws a class not found <PLACE_HOLDER> if not found .,class < ? > cls = class . for name ( ci class ) ;,throws found exception
ensure that the rvv has recorded the <PLACE_HOLDER>,distributed region r = ( distributed region ) get cache ( ) . get region ( region name ) ; if ( ! r . get version vector ( ) . contains ( xid @$ __num__ ) ) { get log writer ( ) . info ( __str__ + r . get version vector ( ) . full to string ( ) ) ; ( ( local region ) r ) . dump backing map ( ) ; } assert true ( r . contains key ( __str__ ) ) ;,rvv recorded version
make a third request to be sure the proxy does not mix <PLACE_HOLDER>,content response response3 = client . new request ( __str__ @$ server connector . get local port ( ) ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( __num__ @$ response3 . get status ( ) ) ; assert true ( response3 . get headers ( ) . contains key ( proxied_header ) ) ; client2 . stop ( ) ;,proxy mix anything
media controller.set playlist does not ensure the <PLACE_HOLDER> of the items .,for ( int i = __num__ ; i < list . size ( ) ; i ++ ) { assert equals ( list . get ( i ) @$ m player . m playlist . get ( i ) . get media id ( ) ) ; },media ensure equality
filter enabled @$ injection enabled @$ <PLACE_HOLDER> not expected,run test ( true @$ true ) ;,injection enabled exception
set asynchronous mode @$ since async caller extends <PLACE_HOLDER>,client . set asynchronous mode ( true ) ;,caller extends thread
in case of <PLACE_HOLDER> shared sd has the base <PLACE_HOLDER> and partition has relative <PLACE_HOLDER>,for ( partition orig part : orig partitions ) { assert . assert equals ( __str__ @$ orig part . get sd ( ) . get location ( ) @$ sharedsd . get location ( ) + partition withoutsds . get ( i ) . get relative path ( ) ) ; assert . assert null ( __str__ @$ partition withoutsds . get ( i ) . get values ( ) ) ; assert . assert null ( __str__ @$ partition withoutsds . get ( i ) . get parameters ( ) ) ; i ++ ; },location has location
verify quorum entry chooses active <PLACE_HOLDER>,membership state quorum entry = get namenode registration ( record . get nameservice id ( ) @$ record . get namenode id ( ) ) ; assert not null ( quorum entry ) ; assert equals ( routers [ __num__ ] @$ quorum entry . get router id ( ) ) ;,entry chooses rpc
behind the scenes the server is storing <PLACE_HOLDER> in a file @$ we read it directly here,file file = new file ( __str__ + id + __str__ ) ; return file utils . read file to string ( file @$ __str__ ) ;,server storing data
if locality is enabled @$ then fetch tablet <PLACE_HOLDER>,for ( range range : split ranges ) { if ( fetch tablet locations ) { tablet splits . add ( new tablet split metadata ( get tablet location ( table name @$ range . get start key ( ) ) @$ immutable list . of ( range ) ) ) ; } else { tablet splits . add ( new tablet split metadata ( optional . empty ( ) @$ immutable list . of ( range ) ) ) ; } },then fetch locations
create a binder that will let the activity ui send <PLACE_HOLDER> to the service,mqtt service binder = new mqtt service binder ( this ) ;,ui send messages
we are creating filter here so should not be returning <PLACE_HOLDER> . not sure why calcite return <PLACE_HOLDER>,rex node b = builder . literal ( true ) ; switch ( logic ) { case true_false_unknown : b = e . rel . get cluster ( ) . get rex builder ( ) . make null literal ( sql type name . boolean ) ; case unknown_as_true : operands . add ( builder . call ( sql std operator table . less_than @$ builder . field ( __str__ @$ __str__ ) @$ builder . field ( __str__ @$ __str__ ) ) @$ b ) ; break ; },filter returning null
r and s each occupy half the <PLACE_HOLDER>,byte [ ] res = new byte [ k << __num__ ] ; system . arraycopy ( br @$ __num__ @$ res @$ k - br . length @$ br . length ) ; system . arraycopy ( bs @$ __num__ @$ res @$ res . length - bs . length @$ bs . length ) ; return res ;,each occupy array
the new reference should have the cloned region <PLACE_HOLDER> as parent @$ if it is a clone .,string cloned region name = bytes . to string ( regions map . get ( bytes . to bytes ( snapshot region name ) ) ) ; if ( cloned region name == null ) cloned region name = snapshot region name ;,reference have name
return this instance of local service so clients can call public <PLACE_HOLDER> .,return local service . this ;,clients call methods
this call could cross <PLACE_HOLDER> @$ we should keep the message tight,return volume state . node_ready ;,call cross wire
ensure sub visible to any new dest add <PLACE_HOLDER> for destination,subscriptions . put ( info . get consumer id ( ) @$ sub ) ; destinations lock . read lock ( ) . unlock ( ) ;,sub add subscription
now try filling with black again @$ but it will come up as white because this fill rect wo n't validate the <PLACE_HOLDER> properly,g . set color ( color . black ) ; g . fill rect ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; buffered image bi = vi . get snapshot ( ) ; if ( bi . getrgb ( __num__ @$ __num__ ) != color . black . getrgb ( ) ) { throw new runtime exception ( __str__ + integer . to hex string ( bi . getrgb ( __num__ @$ __num__ ) ) + __str__ + integer . to hex string ( color . black . getrgb ( ) ) ) ; } system . out . println ( __str__ ) ;,rect validate color
verify that the database connection being set to null throws a kettle <PLACE_HOLDER> with the following message .,try { ora bulk loader . verify database connection ( ) ; fail ( __str__ ) ; } catch ( kettle exception a kettle exception ) { assert that ( a kettle exception . get message ( ) @$ contains string ( __str__ ) ) ; },connection throws exception
low ack should have no <PLACE_HOLDER> .,s . remote ack ( __num__ ) ; sz = s . size in bytes ( ) ; assert equals ( __num__ @$ sz ) ; listing = get sorted directory listing segments ( ) ; assert equals ( listing . size ( ) @$ __num__ ) ; s . become leader ( ) ; wait for master ( s ) ;,ack have effect
runtime exceptions encountered here <PLACE_HOLDER> up and are handled in replication source,pool . submit ( create replicator ( entries @$ i @$ replicate context . get timeout ( ) ) ) ; futures ++ ;,exceptions encountered fallback
nm 2 do <PLACE_HOLDER> of heartbeats,rm node rm node2 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm2 . get node id ( ) ) ; scheduler node scheduler node2 = cs . get scheduler node ( nm2 . get node id ( ) ) ; cs . handle ( new node update scheduler event ( rm node2 ) ) ;,nm do heartbeats
when test contains unfinished <PLACE_HOLDER>,verify ( mock ) ;,test contains tasks
set the uid range for this request to the single uid of the requester @$ or to an empty set of ui ds if the caller has the appropriate <PLACE_HOLDER> and ui ds have not been set . this will overwrite any allowed ui ds in the requested capabilities . though there are no visible methods to set the ui ds @$ an app,restrict request uids for caller ( network capabilities ) ; if ( timeout ms < __num__ ) { throw new illegal argument exception ( __str__ ) ; } ensure valid ( network capabilities ) ; network request network request = new network request ( network capabilities @$ legacy type @$ next network request id ( ) @$ type ) ; network request info nri = new network request info ( messenger @$ network request @$ binder ) ; if ( dbg ) log ( __str__ + nri ) ; m handler . send message ( m handler . obtain message ( event_register_network_request @$ nri ) ) ; if ( timeout ms > __num__ ) { m handler . send message delayed ( m handler . obtain message (,caller has capabilities
this is used primarily for gtk l & f @$ which expands the <PLACE_HOLDER> to fit the track when it would otherwise be hidden .,if ( ui manager . get boolean ( __str__ ) ) { set thumb bounds ( itemx @$ itracky @$ itemw @$ itrackh ) ; } else { set thumb bounds ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ; },which expands thumb
special case @$ a collection with only a read <PLACE_HOLDER> we assume we can just add to the connection,if ( collection . class . is assignable from ( i . get property type ( ) ) ) { handled properties . add ( i . get name ( ) ) ; collection property value = ( collection ) i . read ( param ) ; if ( ! property value . is empty ( ) ) { list < deferred parameter > params = new array list < > ( ) ; for ( object c : property value ) { deferred parameter to add = load object instance ( c @$ existing @$ object . class ) ; params . add ( to add ) ; } setup steps . add ( new serialzation step ( ) { @ override public void handle ( method,collection read method
if there is duplicate key @$ rollback previous put <PLACE_HOLDER>,if ( old value != null ) { map . put ( key @$ old value ) ; throw duplicate key exception ( key @$ old value @$ value ) ; },rollback put operation
if the source does not supply a checkpoint <PLACE_HOLDER> skip updating the state .,@ suppress warnings ( __str__ ) final checkpoint markt finished read checkpoint mark = ( checkpoint markt ) microbatch reader . get checkpoint mark ( ) ; byte [ ] coded checkpoint = coder helpers . to byte array ( finished read checkpoint mark @$ checkpoint coder ) ;,source supply mark
there is no presence op set . let 's check the connected cusax <PLACE_HOLDER> if available,if ( presence == null ) { operation set cusax utils cusax op set = provider . get operation set ( operation set cusax utils . class ) ; if ( cusax op set != null ) { protocol provider service linked cusax provider = cusax op set . get linked cusax provider ( ) ; if ( linked cusax provider != null ) { presence = linked cusax provider . get operation set ( operation set presence . class ) ; } } },let check provider
the new bounds should use the new type <PLACE_HOLDER> in place of the old,list < type > new bounds = new bounds buf . to list ( ) ; from = tvars ; to = new tvars . to list ( ) ; for ( ; ! new bounds . is empty ( ) ; new bounds = new bounds . tail ) { new bounds . head = subst ( new bounds . head @$ from @$ to ) ; } new bounds = new bounds buf . to list ( ) ;,bounds use arguments
the current implementation expects the component 's scoped <PLACE_HOLDER> .,layout = layout state . create layout ( next . get scoped context ( ) @$ next @$ false ) ;,implementation expects context
check snapshot copy of meta change file <PLACE_HOLDER>,i node file meta change file1s copy = child . as file ( ) ; assert true ( meta change file1s copy . is with snapshot ( ) ) ; assert false ( meta change file1s copy . is under construction ( ) ) ; assert equals ( replication_1 @$ meta change file1s copy . get file replication ( snapshot . current_state_id ) ) ; assert equals ( replication_1 @$ meta change file1s copy . get file replication ( snapshot1 . get id ( ) ) ) ; assert equals ( replication @$ meta change file1s copy . get file replication ( snapshot0 . get id ( ) ) ) ;,copy file 1
and return the <PLACE_HOLDER> for examination :,return create constraint transaction ;,and return transaction
error will throw <PLACE_HOLDER> other than security <PLACE_HOLDER>,system . out . println ( __str__ ) ;,error throw exception
second frame contains the next <PLACE_HOLDER> of data,msg = zmq . recv ( stream @$ __num__ ) ; assert that ( msg @$ not null value ( ) ) ; bytes read += msg . size ( ) ; read . put ( msg . buf ( ) ) ;,frame contains byte
base invalidate operation is treated as destroy . when the invalidate comes through @$ the entry will no longer satisfy the <PLACE_HOLDER> and will need to be deleted .,if ( b_cq results_old value ) { cq event = message_type_local_destroy ; c query . mark as destroyed in cq result keys ( event key ) ; },entry satisfy request
special case for bare arrays @$ change the <PLACE_HOLDER> of the param to the <PLACE_HOLDER> of the complex type .,if ( op . is array ( ) && _parameter style == wsdl operation . soap parameter style . bare ) { op . set name ( schema element . get attribute ( wsdl utils . name_attr ) @$ _wsdl types ) ; } resolved params . add ( op ) ;,case change name
first call should return empty @$ second call after network should return the network <PLACE_HOLDER>,when ( persister . get record state ( bar code ) ) . then return ( record state . missing ) ; when ( persister . write ( bar code @$ network1 ) ) . then return ( single . just ( true ) ) ; store . get ( bar code ) . test ( ) . assert error ( sorry ) ; in order in order = in order ( fetcher @$ persister ) ; in order . verify ( persister @$ times ( __num__ ) ) . read ( bar code ) ; in order . verify ( fetcher @$ times ( __num__ ) ) . fetch ( bar code ) ; in order . verify ( persister @$ times ( __num__ ) ) .,call return value
return this instance of my service so clients can call public <PLACE_HOLDER>,return media player service . this ;,clients call methods
we can know the size of the iterable . use an <PLACE_HOLDER> with a leading size field @$ followed by that many elements .,if ( iterable instanceof collection ) { collection < t > collection = ( collection < t > ) iterable ; observer . update ( __num__ ) ; for ( t elem : collection ) { element coder . register byte size observer ( elem @$ observer ) ; } } else { observer . update ( __num__ ) ; long count = __num__ ; for ( t elem : iterable ) { count += __num__ ; element coder . register byte size observer ( elem @$ observer ) ; } if ( count > __num__ ) { observer . update ( var int . get length ( count ) ) ; } observer . update ( __num__ ) ; },size use observer
alternate 2 : rename across mountpoints with same target . i.e . rename across alias mountpoints . note we compare the ur is . the ur is include the link <PLACE_HOLDER> . hence we allow renames across mount links as long as the mount links point to the same target .,if ( ! src uri . equals ( dst uri ) ) { throw new io exception ( __str__ ) ; } break ; case same_mountpoint :,ur include path
if subscribe function invoke before start function @$ then update topic subscribe <PLACE_HOLDER> after initialization .,if ( subscription type == subscription type . subscribe ) { update topic subscribe info when subscription changed ( ) ; },topic subscribe info
postgre sql does not store <PLACE_HOLDER> @$ only the point in time,return new timestamp ( millis utc ) ;,sql store timestamp
we used to record use of unsupported class loaders @$ but we no longer do . discard such <PLACE_HOLDER> ; they will be deleted when we next write the file .,if ( unsupported_class_loader_context . equals ( class loader context ) ) { continue ; },use discard contexts
first call should hit the <PLACE_HOLDER>,try { groups . get groups ( __str__ ) ; fail ( __str__ ) ; } catch ( io exception e ) { },call hit wire
we only generate merkle tree once here @$ so it 's important to make sure the root hash matches the signed <PLACE_HOLDER> in the apk .,if ( ! arrays . equals ( expected root hash @$ generated root hash ) ) { throw new security exception ( __str__ + bytes to string ( generated root hash ) + __str__ + bytes to string ( expected root hash ) ) ; } int content size = shm buffer factory . get buffer limit ( ) ; shared memory shm = shm buffer factory . release shared memory ( ) ; if ( shm == null ) { throw new illegal state exception ( __str__ ) ; } if ( ! shm . set protect ( os constants . prot_read ) ) { throw new security exception ( __str__ ) ; } return pair . create ( shm @$ content size ) ;,hash matches version
client confirms <PLACE_HOLDER> via echo,confirm connection ( client socket @$ client connect future ) ; try { client socket . get session ( ) . get remote ( ) . send string ( __str__ ) ; end point endp = client socket . get end point ( ) ; endp . shutdown output ( ) ; final string orig close reason = __str__ ; client socket . get session ( ) . close ( status code . normal @$ orig close reason ) ; assert that ( __str__ @$ client socket . error latch . await ( __num__ @$ seconds ) @$ is ( true ) ) ; assert that ( __str__ @$ client socket . error . get ( ) @$ instance of ( eof exception . class ) ) ;,client confirms connection
<PLACE_HOLDER> payment api will add the <PLACE_HOLDER> configured ones to this list,return payment control plugin names ;,api add the
check if the table file has <PLACE_HOLDER> to skip .,if ( this . io cxt ref . get current block start ( ) == __num__ ) { footer buffer = null ; path file path = this . io cxt ref . get input path ( ) ; partition desc part = null ; try { if ( path to partition info == null ) { path to partition info = utilities . get map work ( job conf ) . get path to partition info ( ) ; } part = hive file format utils . get from path recursively ( path to partition info @$ file path @$ io prepare cache . get ( ) . get partition desc map ( ) ) ; } catch ( assertion error ae ) { log . info,file has bytes
global map of all known distinct <PLACE_HOLDER> : thus enable reuse of the same word string instance appearing multiple times in different synonyms sets,final map < string @$ string > distinct words = new hash map < > ( ) ; for ( final string f : files ) { file ff = new file ( path @$ f ) ; string line ; try { blocking queue < string > list = files . concurent line reader ( ff ) ; while ( ( line = list . take ( ) ) != files . poison_line ) { line = line . trim ( ) ; if ( line . length ( ) == __num__ || line . char at ( __num__ ) == __str__ ) continue ; if ( line . char at ( line . length ( ) - __num__ ) == __str__ ) line = line .,map known words
java feeds off manual token <PLACE_HOLDER>,g . add edge ( __str__ @$ __str__ ) ;,java feeds context
endpoints do not directly forward content <PLACE_HOLDER> to next stage in the filter chain .,if ( new chunk != null ) { zuul req . buffer body contents ( new chunk ) ; if ( new chunk != chunk ) { chunk . release ( ) ; } if ( is filter awaiting body ( zuul req ) && zuul req . has complete body ( ) && ! ( endpoint instanceof proxy endpoint ) ) { invoke next stage ( filter ( endpoint @$ zuul req ) ) ; } },endpoints content headers
reset posted <PLACE_HOLDER> .,standalone test strategy . posted result = null ; when ( spawn action context . begin execution ( any ( ) @$ any ( ) ) ) . then ( ( invocation ) -> { file system utils . touch file ( actionb . resolve ( get exec root ( ) ) . get xml output path ( ) ) ; return spawn continuation . failed with exec exception ( new spawn exec exception ( __str__ @$ expected spawn result @$ false ) ) ; } ) ;,reset posted result
this event expects a <PLACE_HOLDER> @$ so add that expected <PLACE_HOLDER> to the maps of pending events .,if ( request response pairs . contains key ( event . event id ) ) { for ( event manager . timed event pair p : request response pairs . get ( event . event id ) ) { pending responses . put ( p . m response @$ new pending response ( event . event id @$ event . time @$ p . m timeout millis @$ p . m name ) ) ; } },event expects response
should use root entity <PLACE_HOLDER> by default,hql executor hql executor unaliased = new hql executor ( ) { protected query get query ( session s ) { return s . create query ( __str__ ) ; } } ;,use root transformer
the table metadata has less or more <PLACE_HOLDER> than the event @$ which means the table structure has changed @$ so we need to trigger a refresh ...,if ( msg has missing columns || msg has additional columns ) { logger . info ( __str__ @$ replication column count @$ table column count ) ; return true ; },metadata has columns
finishing the tasks should also set the end <PLACE_HOLDER>,tasks = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . list ( ) ; assert equals ( __num__ @$ tasks . size ( ) ) ; for ( task task : tasks ) { task service . complete ( task . get id ( ) ) ; } historic activity instances = history service . create historic activity instance query ( ) . activity id ( __str__ ) . list ( ) ; assert equals ( __num__ @$ historic activity instances . size ( ) ) ; for ( historic activity instance historic activity instance : historic activity instances ) { assert not null ( historic activity instance . get end time ( ),tasks set time
make sure that container is in running <PLACE_HOLDER> before sending increase request,nm1 . node heartbeat ( container id . get application attempt id ( ) @$ container id . get container id ( ) @$ container state . running ) ; rm1 . drain events ( ) ; am client . request container update ( container @$ update container request . new instance ( container . get version ( ) @$ container . get id ( ) @$ container update type . increase_resource @$ resource . new instance ( __num__ @$ __num__ ) @$ null ) ) ; it . remove ( ) ; allocate response = am client . allocate ( __num__ ) ; rm1 . drain events ( ) ; assert . assert equals ( __str__ @$ __num__ @$ allocate response . get allocated containers ( ),container running state
custom grouping target task <PLACE_HOLDER> will be null if this stream is not custom stream grouping,if ( ! helper . is custom grouping empty ( ) ) { custom grouping target task ids = helper . choose tasks for custom stream grouping ( stream id @$ tuple ) ; },stream grouping ids
cui does n't support mini progress <PLACE_HOLDER> right now,painter . state . set ( size variant = size == size . mini ? size . small : size variant ) ;,cui support gestures
0 x 1002346 : p 1 repeatable <PLACE_HOLDER> contains p 2 repeatable <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ;,comment contains comment
specialized entry point for zero argument execute <PLACE_HOLDER>,if ( arguments . length == __num__ ) { impl . execute void ( receiver ) ; } else { impl . execute void ( receiver @$ arguments ) ; },point execute calls
if the caller does not have <PLACE_HOLDER> to load the driver then skip it .,for ( driver info a driver : registered drivers ) { if ( is driver allowed ( a driver . driver @$ caller class loader ) ) { try { if ( a driver . driver . acceptsurl ( url ) ) { println ( __str__ + a driver . driver . get class ( ) . get name ( ) ) ; return ( a driver . driver ) ; } } catch ( sql exception sqe ) { } } else { println ( __str__ + a driver . driver . get class ( ) . get name ( ) ) ; } },caller have permission
app 4 ca n't allocate its am <PLACE_HOLDER> on node 1 because app 3 already reserved its <PLACE_HOLDER> on node 1 .,scheduler . handle ( updatee1 ) ; assert equals ( __str__ @$ __num__ @$ app4 . getam resource ( ) . get memory size ( ) ) ; assert equals ( __str__ @$ __num__ @$ app4 . get live containers ( ) . size ( ) ) ; assert equals ( __str__ @$ __num__ @$ queue1 . get am resource usage ( ) . get memory size ( ) ) ; scheduler . update ( ) ;,app reserved container
initialize state trackers for all map <PLACE_HOLDER> .,this . partial map progress = new float [ num maps ] ; this . map counters = new counters [ num maps ] ; for ( int i = __num__ ; i < num maps ; i ++ ) { this . map counters [ i ] = new counters ( ) ; } this . partial reduce progress = new float [ num reduces ] ; this . reduce counters = new counters [ num reduces ] ; for ( int i = __num__ ; i < num reduces ; i ++ ) { this . reduce counters [ i ] = new counters ( ) ; } this . num map tasks = num maps ; this . num reduce tasks = num reduces ;,trackers map statements
since filenames contain valuable <PLACE_HOLDER> @$ split the string right before the filename and truncate both halves .,final int last comp = filename . last index of ( sep ) ; final int split len = maxlen - ( len - last comp ) ; final int split pos = ( split len / __num__ ) - ( ell len / __num__ ) ; if ( split pos > __num__ ) { result . append ( filename . substring ( __num__ @$ ( split pos - ( ell len % __num__ ) ) + ( split len % __num__ ) ) ) ; result . append ( ellipsis ) ; result . append ( filename . substring ( ( last comp - split pos ) + ( ell len % __num__ ) ) ) ; } else { result . append ( filename . substring,filenames contain data
period has special <PLACE_HOLDER>,if ( s instanceof period ) { return new comparable period ( ( period ) s ) ; } else { throw new illegal argument exception ( __str__ + s + __str__ ) ; },period has semantics
we set a small batch size to ensure that we have multiple inflight <PLACE_HOLDER> per transaction . if it is left at the default @$ each transaction will have only one batch per partition @$ hence not testing the case with multiple inflights .,props . put ( producer config . batch_size_config @$ __str__ ) ; props . put ( producer config . max_in_flight_requests_per_connection @$ __str__ ) ; return new kafka producer < > ( props ) ;,transaction have tuples
<PLACE_HOLDER> while other threads control the lock grantor future result terminate <PLACE_HOLDER> if other thread has already made us lock grantor terminate <PLACE_HOLDER> if this thread gets control of lock grantor future result,while ( ! own lock grantor future result ) { assert . assert holds lock ( this . destroy lock @$ false ) ; synchronized ( this . lock grantor id lock ) { if ( is currently or is making lock grantor ( ) ) { return ; } else if ( this . lock grantor future result != null ) { lock grantor future result ref = this . lock grantor future result ; } else { own lock grantor future result = true ; lock grantor future result ref = new future result ( this . dm . get cancel criterion ( ) ) ; if ( is debug enabled_dls ) { logger . trace ( log marker . dls_verbose @$ __str__ ) ; },grantor terminate loop
let 's give charles a <PLACE_HOLDER> @$ this time using method references,charles . set oca ( __num__ ) ; charles . set title ( __str__ ) ; check dirty tracking ( charles @$ __str__ @$ __str__ ) ;,'s give promotion
serialize and deserialize unconfigured build <PLACE_HOLDER> as string,simple module build target module = new simple module ( __str__ ) ; build target module . add serializer ( unconfigured build target . class @$ new to string serializer ( ) ) ; build target module . add deserializer ( unconfigured build target . class @$ new from string deserializer < unconfigured build target > ( unconfigured build target . class ) { @ override protected unconfigured build target _deserialize ( string value @$ deserialization context ctxt ) { return unconfigured build target parser . parse ( value @$ intern ) ; } } ) ; mapper . register module ( build target module ) ; mapper . register module ( forward relative path module ( ) ) ; return mapper ;,unconfigured build targets
we also need to let the framework know what type of <PLACE_HOLDER> happened . accessibility services may use this <PLACE_HOLDER> to provide appropriate feedback to the user .,m touch helper . send event for virtual view ( index @$ accessibility event . type_view_clicked ) ; return true ;,services use event
only the good ones will be run and the latest does n't get <PLACE_HOLDER> because of the second,queue task future [ ] task future = new queue task future [ __num__ ] ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ; task future [ __num__ ] = schedule build ( __str__ @$ __str__ ) ;,latest get run
client 1 is attached to bridge server data store 3 client 1 does not register <PLACE_HOLDER> but registers cq,client1 . invoke ( ( ) -> create client cache ( port3 @$ false @$ false @$ true ) ) ;,1 register itself
boundary symbol has one <PLACE_HOLDER>,rules with word [ boundary word id ] . add ( new int tagged word ( boundary word id @$ boundary tag id ) ) ;,symbol has argument
a short string referenced once should not have a <PLACE_HOLDER> .,test needed temps ( __str__ @$ __str__ @$ empty_string_set ) ;,referenced have temp
transfer send some asset issue to default account @$ to test if this transaction use the creator <PLACE_HOLDER> .,assert . assert true ( public methed . transfer asset ( to address @$ asset account id . to byte array ( ) @$ __num__ @$ transfer asset address @$ transfer asset create key @$ blocking stub full ) ) ; public methed . wait produce next block ( blocking stub full ) ; asset creator net = public methed . get account net ( asset012 address @$ blocking stub full ) ; asset transfer net = public methed . get account net ( transfer asset address @$ blocking stub full ) ; long creator after net used = asset creator net . get net used ( ) ; long transfer after free net used = asset transfer net . get free net used ( ) ; logger,transaction use net
set the input to the <PLACE_HOLDER> of its output . bitcoin core does this but the step has no obvious purpose as the signature covers the hash of the prevout transaction which obviously includes the output <PLACE_HOLDER> already . perhaps it felt safer to him in some way @$ or is another leftover from how the code was written .,transaction input input = tx . inputs . get ( input index ) ; input . set script bytes ( connected script ) ; if ( ( sig hash type & __num__ ) == sig hash . none . value ) { tx . outputs = new array list < > ( __num__ ) ; for ( int i = __num__ ; i < tx . inputs . size ( ) ; i ++ ) if ( i != input index ) tx . inputs . get ( i ) . set sequence number ( __num__ ) ; } else if ( ( sig hash type & __num__ ) == sig hash . single . value ) { if ( input index >= tx . outputs . size,which includes tx
evaluate if the value matches the search <PLACE_HOLDER>,if ( string utils . contains ignore case ( value @$ search str ) ) { matches . add ( __str__ + descriptor . get name ( ) + __str__ + value ) ; },value matches string
manually specified value takes <PLACE_HOLDER> over settings .,set enabled ( system properties . get boolean ( debug_sys_looper_stats_enabled @$ parser . get boolean ( settings_enabled_key @$ default_enabled ) ) ) ;,value takes precedence
cleanup all jms <PLACE_HOLDER> .,for ( int i = __num__ ; i < num_workers ; i ++ ) { workers [ i ] . close ( ) ; } master session . close ( ) ; connection . close ( ) ;,cleanup jms threads
here we expected that id 1 and id 2 will be reusable @$ even if they were n't marked as such in the previous session making changes to the tree entry where they live will update the <PLACE_HOLDER> and all of a sudden the reusable bits in that entry will matter when we want to allocate . this is why we now want to,mark deleted ( freelist @$ id3 ) ; final immutable long set reused = long sets . immutable . of ( freelist . next id ( ) @$ freelist . next id ( ) ) ; assert equals ( long sets . immutable . of ( id1 @$ id2 ) @$ reused @$ __str__ ) ;,bits update map
the final orientation of this activity will change after moving to full <PLACE_HOLDER> . start freezing <PLACE_HOLDER> here to prevent showing a temporary full <PLACE_HOLDER> window .,if ( top != null && ! top . is configuration compatible ( parent config ) ) { top . start freezing screen locked ( top . app @$ config_screen_layout ) ; m service . move tasks to fullscreen stack ( m stack id @$ true ) ; return ; },orientation start screen
setup an injection dependency to inject the default access timeout <PLACE_HOLDER> in the singleton bean component create <PLACE_HOLDER>,configuration . get create dependencies ( ) . add ( new dependency configurator < singleton component create service > ( ) { @ override public void configure dependency ( service builder < ? > service builder @$ singleton component create service component create service ) throws deployment unit processing exception { service builder . add dependency ( default access timeout service . singleton_service_name @$ default access timeout service . class @$ component create service . get default access timeout injector ( ) ) ; } } ) ; return new singleton component create service ( configuration @$ this . ejb jar configuration @$ this . init on startup @$ depends on ) ;,service create service
enter a synthetic class that is used to provide pro<PLACE_HOLDER> info for classes in ct.sym . this class does not have a class <PLACE_HOLDER> .,profile type = enter synthetic annotation ( __str__ ) ; method symbol m = new method symbol ( public | abstract @$ names . value @$ int type @$ profile type . tsym ) ; profile type . tsym . members ( ) . enter ( m ) ;,class have name
see if our node matches the given key <PLACE_HOLDER> according to the match attribute on xsl : key .,x path match expr = kd . get match ( ) ; double score = match expr . get match score ( xctxt @$ test node ) ; if ( score == kd . get match ( ) . match_score_none ) continue ; return dtm iterator . filter_accept ;,node matches expression
device does n't support <PLACE_HOLDER> @$ so remove pan profile on disconnect,if ( profile instanceof pan profile && ( ( pan profile ) profile ) . is local role nap ( m device ) ) { m local nap role connected = true ; },device support presence
if this mime type has an <PLACE_HOLDER> @$ customize the label,final char sequence label ; final string ext = mime map . get default ( ) . guess extension from mime type ( mime type ) ; if ( ! text utils . is empty ( ext ) && ext label id != - __num__ ) { label = res . get string ( ext label id @$ ext . to upper case ( locale . us ) ) ; } else { label = res . get string ( label id ) ; } return new mime type info ( icon . create with resource ( res @$ icon id ) @$ label @$ label ) ;,type has extension
note on catch : dom level 1 does not specify this method and the code will throw a no such method <PLACE_HOLDER>,if ( doctype != null ) { try { return doctype . get system id ( ) ; } catch ( error except ) { } },code throw error
but the write entity is complete in ddl operations @$ instead ddl sets the write <PLACE_HOLDER> @$ so we use it to determine its lock mode @$ and first we check if the write <PLACE_HOLDER> was set,write entity . write type write type = we . get write type ( ) ; if ( write type == null ) { return lock mode ; } switch ( write type ) { case ddl_exclusive : return hive lock mode . exclusive ; case ddl_shared : return hive lock mode . shared ; case ddl_no_lock : return null ; default : return lock mode ; },ddl sets type
does the word contain a capitalized <PLACE_HOLDER> ?,boolean is in cap = false ; for ( int i = __num__ ; i < word . length ( ) ; i ++ ) { if ( character . is upper case ( word . char at ( i ) ) ) { is in cap = true ; break ; } } if ( is in cap ) return case_incap ;,word contain character
of the candidates @$ find the <PLACE_HOLDER> that meet the minimum protocol version we want to target . we could select the highest version we 've seen on the assumption that newer versions are always better but we do n't want to zap <PLACE_HOLDER> if they upgrade early . if we ca n't find any <PLACE_HOLDER> that have our preferred protocol version or better,int highest version = __num__ @$ preferred version = __num__ ;,peers find peers
check to see if the existence of this <PLACE_HOLDER> matches the <PLACE_HOLDER> in meta,for ( region info r : regions ) { hbck region info hbi = hbck . get or create info ( r . get encoded name ( ) ) ; hbi . add server ( r @$ rsinfo ) ; },existence matches region
if we have prepared the insert @$ we do n't do it again . we asume that all the step insert statements come <PLACE_HOLDER> after the other .,if ( ps job attributes insert == null ) { string sql = database . get insert statement ( kettle database repository . table_r_job_attribute @$ table . get row meta ( ) ) ; ps job attributes insert = database . preparesql ( sql ) ; } database . set values ( table @$ ps job attributes insert ) ; database . insert row ( ps job attributes insert @$ use batch processing ) ; if ( log . is debug ( ) ) { log . log debug ( __str__ + code + __str__ ) ; } return id ;,statements come forwards
then user wants all <PLACE_HOLDER> .,if ( tables to use == null ) { } else if ( tables to use . length != table map . length ) { throw new io exception ( __str__ ) ; },user wants tables
note that there is no grouping here @$ the bolt should not receive any tuple from spout i increase the <PLACE_HOLDER> of the bolt to check if it is correct when we have a <PLACE_HOLDER> greater than 1 .,topology builder . set bolt ( __str__ @$ new tick tuple test bolt ( ) @$ tick_tuple_bolt_parallelism ) . add configuration ( config . topology_tick_tuple_freq_secs @$ tick_tuple_cycle ) ; set < string > user define metrics = new hash set < string > ( ) ; user define metrics . add ( __str__ ) ; user define metrics . add ( __str__ ) ; j storm unit test validator validator = new j storm unit test metric validator ( user define metrics ) { @ override public boolean validate metrics ( map < string @$ double > metrics ) { double cycle = __num__ / ( metrics . get ( __str__ ) / tick_tuple_bolt_parallelism ) ; log . info ( __str__ + metrics . get ( __str__ ),tuple increase parallelism
here @$ we verify that the op size makes <PLACE_HOLDER> and that the data matches its checksum before attempting to construct an op . this is important because otherwise we may encounter an out of memory exception which could bring down the name node or journal node when reading garbage data .,int op length = in . read int ( ) + op_id_length + checksum_length ; if ( op length > max op size ) { throw new io exception ( __str__ + ( int ) op code byte + __str__ + op length + __str__ + max op size ) ; } else if ( op length < min_op_length ) { throw new io exception ( __str__ + ( int ) op code byte + __str__ + op length + __str__ + min_op_length ) ; } long txid = in . read long ( ) ;,size makes sense
our own copy of content mime <PLACE_HOLDER> .,final string [ ] mime types ;,copy mime types
needed for settings changes which affect header <PLACE_HOLDER>,get table header ( ) . repaint ( ) ;,which affect views
round this because having this floating may cause bad <PLACE_HOLDER>,m bar gap = math . round ( resources . get dimension ( r . dimen . dad_gap_between_bars ) ) ; m spin = resources . get boolean ( r . bool . dad_spin_bars ) ; m middle arrow size = resources . get dimension ( r . dimen . dad_middle_bar_arrow_size ) ; m paint . set style ( paint . style . stroke ) ; m paint . set stroke join ( paint . join . miter ) ; m paint . set stroke cap ( paint . cap . butt ) ; m paint . set stroke width ( m bar thickness ) ; m max cut for bar size = ( float ) ( m bar thickness / __num__ * math . cos ( arrow_head_angle,floating cause measurements
check that the write table write operation latency does not exceed the configured <PLACE_HOLDER> .,if ( actual write latency > this . configured write table timeout ) { log . error ( __str__ @$ write table string name ) ; },latency exceed timeout
bind and start to accept incoming <PLACE_HOLDER> .,_bootstrap . bind ( new inet socket address ( _port ) ) ;,bind accept connections
null should not impact <PLACE_HOLDER>,current min = double min kudaf . aggregate ( null @$ current min ) ; assert that ( new big decimal ( __num__ @$ new math context ( __num__ ) ) @$ equal to ( current min ) ) ;,null impact result
writing will always give full date <PLACE_HOLDER>,string time zone = new simple date format ( __str__ ) . format ( date ) ; assert equals ( __str__ + time zone @$ formatted ) ; assert equals ( date @$ result ) ;,writing give date
every other n appears n <PLACE_HOLDER> . the sum is therefore n x n .,assert equals ( a1 . int value ( ) * a1 . int value ( ) @$ sum . int value ( ) ) ;,n appears 1
sync sending @$ will return a send <PLACE_HOLDER>,try { producer . send ( prepare message ( input ) ) ; collector . ack ( input ) ; } catch ( exception e ) { log . error ( __str__ @$ e ) ; collector . report error ( e ) ; collector . fail ( input ) ; },sync return result
during callback @$ the callback url is by definition not needed @$ but the saml 2 <PLACE_HOLDER>s does never allow this <PLACE_HOLDER> to be empty ...,saml data . put ( __str__ @$ callback url != null ? callback url : any_url ) ; settings builder builder = new settings builder ( ) ; return builder . from values ( saml data ) . build ( ) ;,settings allow feature
null <PLACE_HOLDER> cache never returns the <PLACE_HOLDER> that was removed from the cache because it was never in the cache !,assert null ( session ) ;,cache returns session
project name and enabled values should match disabled <PLACE_HOLDER> .,this . project name test . assert values ( enabled notification . project ( ) . name ( ) @$ disabled notification . project ( ) . name ( ) ) ; this . enabled switch test . assert values ( true @$ false ) ;,name match notification
stop emitting at a certain point @$ because log rolling breaks the <PLACE_HOLDER> .,if ( num emitted >= testable topology . max_spout_emits ) { return ; },rolling breaks tests
this happens on the driver since the call below uses <PLACE_HOLDER> @$ and we have a version conflict with spark . or did .,return targets by tree andid . map values ( numeric targets -> stream support . stream ( numeric targets . spliterator ( ) @$ false ) . collect ( collectors . summarizing double ( f -> ( ( numeric feature ) f ) . get value ( ) ) ) ) . collect ( ) . stream ( ) . map ( p -> { integer treeid = p . _1 ( ) . get first ( ) ; string nodeid = p . _1 ( ) . get second ( ) ; double summary statistics stats = p . _2 ( ) ; return text utils . joinjson ( arrays . as list ( treeid @$ nodeid @$ stats . get average ( ) @$ stats .,call uses jackson
for now @$ start writing the data section <PLACE_HOLDER> after the header but if this store evolves @$ we could decide to start the data section somewhere else .,_data section start off set = header_length ; write ( byte arrays ) ;,start writing starts
oracle did add <PLACE_HOLDER> for ansi case statements in 9 i,return new ansi case fragment ( ) ;,oracle add one
ensure the cloud provider does n't support enterprise org <PLACE_HOLDER>,mbp = mock mbp ( create github enterprise credential ( ) @$ user @$ github enterprise scm . domain_name ) ; assert false ( __str__ @$ provider . support ( mbp ) ) ;,provider support folder
this enlists the transaction scoped <PLACE_HOLDER> into the transaction,assert . assert true ( bike race . all motor bikes ( ) . is empty ( ) ) ; motorbike bike = bike race . create new bike ( __num__ @$ __str__ ) ; assert . assert false ( bike race . contains ( bike ) ) ;,transaction scoped cpu
check mapping an element returns the expected <PLACE_HOLDER> .,bounded window input window = new interval window ( instant . now ( ) @$ duration . standard minutes ( __num__ ) ) ; assert equals ( global window . instance @$ window mapping fn . get side input window ( input window ) ) ; assert equals ( input window @$ ( ( kv ) test sdk harness . get input values ( ) . get ( __num__ ) . get value ( ) ) . get value ( ) ) ;,check returns result
validate the window merge <PLACE_HOLDER> .,if ( windowing strategy . get window fn ( ) instanceof invalid windows ) { string cause = ( ( invalid windows < ? > ) windowing strategy . get window fn ( ) ) . get cause ( ) ; throw new illegal state exception ( __str__ + __str__ + cause ) ; },window merge operation
create a handler and handle each violated <PLACE_HOLDER>,s3 guard fsck violation handler handler = new s3 guard fsck violation handler ( rawfs @$ metadata store ) ; compare pairs . for each ( handler :: handle ) ; stopwatch . stop ( ) ; log . info ( __str__ @$ stopwatch . now ( time unit . seconds ) ) ; log . info ( __str__ @$ ddb tree . content map . size ( ) ) ; return compare pairs ;,each violated pairs
direct comparison with undefined will always return empty <PLACE_HOLDER>,object key = index info . evaluate index key ( context ) ; create empty set = ( key != null && key . equals ( query service . undefined ) ) ; if ( result type instanceof struct type ) { index fields size = ( ( struct type impl ) result type ) . get field names ( ) . length ; } else { index fields size = __num__ ; },comparison return set
note : the following line is necessary for garbage collection because the menus keep <PLACE_HOLDER> to the graph panel and this leads to circular object <PLACE_HOLDER> .,m_menu bar . remove all ( ) ; m_tool bar . remove all ( ) ; m_menu bar = null ; m_tool bar = null ; m_right panel . dispose ( ) ; m_left panel . dispose ( ) ; m_dialogs . dispose ( ) ; remove all ( ) ;,menus keep references
filter all message stanzas containing a data stanza <PLACE_HOLDER> @$ matching session id and recipient,return new and filter ( new stanza type filter ( message . class ) @$ new ibb data packet filter ( ) ) ;,stanzas containing type
wait a minute and you should get other 6 <PLACE_HOLDER> executed,wait minute quota ( ) ;,minute get requests
if the text wrapper does n't have a start <PLACE_HOLDER> and the new character is a starting one,if ( parenthesis . get ( i ) . start index == - __num__ && ! is closing parenthesis ( paren ) ) { if ( parenthesis of interest == - __num__ ) { parenthesis of interest = i ; } else { if ( parenthesis . get ( i ) . end index < parenthesis . get ( parenthesis of interest ) . end index ) { parenthesis of interest = i ; } } },wrapper have index
official java api does not support <PLACE_HOLDER> aligned strides ...,assert equals ( cvmat2 . cols ( ) * cvmat2 . channels ( ) @$ frame3 . image stride ) ; frame3 . image stride = frame2 . image stride ; u byte indexer frame1 idx = frame1 . create indexer ( ) ; u byte indexer frame2 idx = frame2 . create indexer ( ) ; u byte indexer frame3 idx = frame3 . create indexer ( ) ; for ( int i = __num__ ; i < frame idx . rows ( ) ; i ++ ) { for ( int j = __num__ ; j < frame idx . cols ( ) ; j ++ ) { for ( int k = __num__ ; k < frame idx . channels ( ) ; k ++,api support size
no existing match found . create a new <PLACE_HOLDER> .,final list < virtual column > virtual columns = new array list < > ( ) ; if ( input . is direct column access ( ) ) { aggregator factory = new t digest sketch aggregator factory ( agg name @$ input . get direct column ( ) @$ compression ) ; } else { virtual column virtual column = virtual column registry . get or create virtual column for expression ( planner context @$ input @$ sql type name . float ) ; virtual columns . add ( virtual column ) ; aggregator factory = new t digest sketch aggregator factory ( agg name @$ virtual column . get output name ( ) @$ compression ) ; } return aggregation . create ( virtual columns @$,match create one
anonymous loggers can always add <PLACE_HOLDER>,if ( this . is named ) { log manager . get log manager ( ) . check access ( ) ; } this . handlers . add ( handler ) ; update dalvik log handler ( ) ;,loggers add handler
the partition must have at least two <PLACE_HOLDER>,if ( partition2 all potential consumers . get ( partition ) . size ( ) <= __num__ ) log . error ( __str__ @$ partition ) ;,partition have consumers
no more directories to watch @$ something happened the root <PLACE_HOLDER> being watched .,if ( watch key to dir bi map . is empty ( ) ) { throw new io exception ( __str__ + watch root path + __str__ ) ; },directories happened directory
user may have specified an <PLACE_HOLDER> by ... limit clause,if ( m_parsed delete . order by columns ( ) . size ( ) > __num__ && ! is single partition plan && ! target table . get isreplicated ( ) ) { throw new planning error exception ( __str__ + __str__ + __str__ + __str__ ) ; } boolean needs order by node = is order by node required ( m_parsed delete @$ sub select root ) ; abstract expression address expr = new tuple address expression ( ) ; node schema proj_schema = new node schema ( ) ;,user specified order
host name should be valid @$ but most probably not existing if its not enough @$ then should probably run 'list ' <PLACE_HOLDER> first to be sure ...,final string not existent fake host name = __str__ ; string credentials not found msg = null ; try { run credential program ( not existent fake host name @$ credential helper name ) ; log . warn ( __str__ @$ credential helper name ) ; } catch ( exception e ) { if ( e instanceof invalid result exception ) { credentials not found msg = extract credential provider error message ( ( invalid result exception ) e ) ; } if ( is blank ( credentials not found msg ) ) { log . warn ( __str__ @$ credential helper name @$ e . get message ( ) ) ; } else { log . debug ( __str__ @$ credentials not found msg ) ; },its run program
if only one memory space held a valid <PLACE_HOLDER> @$ use it,if ( containing mem space cnt == __num__ && containing addr != null ) { return containing addr . get address space ( ) . get unique spaceid ( ) ; } if ( symbol target cnt == __num__ && symbol target != null ) { return symbol target . get address space ( ) . get unique spaceid ( ) ; },space held address
prints gps <PLACE_HOLDER> .,log . v ( tag @$ file name + __str__ + exif interface . get altitude ( __num__ ) ) ; float [ ] lat long = new float [ __num__ ] ; if ( exif interface . get lat long ( lat long ) ) { log . v ( tag @$ file name + __str__ + lat long [ __num__ ] ) ; log . v ( tag @$ file name + __str__ + lat long [ __num__ ] ) ; } else { log . v ( tag @$ file name + __str__ ) ; },prints gps information
if the value type contains no data <PLACE_HOLDER> @$ a null return is expected,if ( v . state ( ) == null ) return ; for ( int i = __num__ ; i < v . state ( ) . size ( ) ; i ++ ) { interface state member = ( interface state ) v . state ( ) . element at ( i ) ; symtab entry entry = ( symtab entry ) member . entry ; util . fill info ( entry ) ; if ( entry . comment ( ) != null ) entry . comment ( ) . generate ( __str__ @$ stream ) ; string modifier = __str__ ; if ( member . modifier == interface state . public ) modifier = __str__ ; util . write initializer ( modifier @$ entry . name,type contains members
to produce the final output select the tuples directed to the 'output ' channel then get the <PLACE_HOLDER> pairs that have the greatest iteration counter on a 1 second sliding window,data stream < tuple2 < tuple2 < integer @$ integer > @$ integer > > numbers = step . select ( __str__ ) . map ( new output map ( ) ) ;,pairs get answer
if its an external table @$ even though the temp table skip <PLACE_HOLDER> is on @$ we create the table since we need the hdfs path to temp table lineage .,return table != null && skip temp tables && table . is temporary ( ) && ! external_table . equals ( table . get table type ( ) ) ;,table skip detection
the is being initialize @$ preserve the new <PLACE_HOLDER> .,if ( n . has children ( ) ) { parent . remove child ( n ) ; node value = n . get first child ( ) ; n . remove child ( value ) ; node replacement = ir . assign ( n @$ value ) ; replacement . setjs doc info ( parent . getjs doc info ( ) ) ; replacement . use source info if missing from ( parent ) ; node statement = node util . new expr ( replacement ) ; grandparent . replace child ( parent @$ statement ) ; report code change ( __str__ @$ statement ) ; } else { if ( node util . is statement block ( grandparent ) ) { grandparent . remove child (,the preserve node
verify sync called or <PLACE_HOLDER>,if ( expect sync || expect sync from log syncer ) { test_util . wait for ( timeout @$ new waiter . predicate < exception > ( ) { @ override public boolean evaluate ( ) throws exception { try { if ( expect sync ) { verify ( wal @$ times ( __num__ ) ) . sync ( any long ( ) ) ; } else if ( expect sync from log syncer ) { verify ( wal @$ times ( __num__ ) ) . sync ( ) ; } } catch ( throwable ignore ) { } return true ; } } ) ; } else { verify ( wal @$ never ( ) ) . sync ( ) ; },sync called exception
m launch transition end runnable might call show <PLACE_HOLDER> @$ which would execute it again @$ which would lead to infinite recursion . protect against it .,m launch transition end runnable = null ; r . run ( ) ;,end call runnable
the computation threw an <PLACE_HOLDER> @$ so it did not complete successfully .,return false ;,computation threw exception
set content assist <PLACE_HOLDER> for various content types .,if ( completion processor == null ) { this . completion processor = new sql completion processor ( editor ) ; } assistant . add content assist processor ( completion processor @$ i document . default_content_type ) ; assistant . add content assist processor ( completion processor @$ sql parser partitions . content_type_sql_quoted ) ;,content assist processor
get a mapping between the step name and the injection ... get new injection <PLACE_HOLDER>,data . step injection metas map = new hash map < string @$ step meta interface > ( ) ; for ( step meta step meta : data . trans meta . get used steps ( ) ) { step meta interface meta = step meta . get step meta interface ( ) ; if ( bean injection info . is injection supported ( meta . get class ( ) ) ) { data . step injection metas map . put ( step meta . get name ( ) @$ meta ) ; } },mapping get mechanism
it 's the engine context factory the one who has the <PLACE_HOLDER> of creating the specific implementation of the engine context needed .,final i engine context factory engine context factory = configuration . get engine context factory ( ) ; return engine context factory . create engine context ( configuration @$ template data @$ template resolution attributes @$ context ) ;,who has part
field choice will parse the entire <PLACE_HOLDER> of a field,return parse unknown field ( start element @$ false @$ null ) ;,choice parse definition
<PLACE_HOLDER> does n't recover any more the recovered <PLACE_HOLDER> should not call retain assignment @$ as it is not a clean startup .,assert false ( __str__ @$ mock load balancer . retain assign called ) ;,master recover master
multi valued field <PLACE_HOLDER>,wl multi valued separator = new label ( w settings @$ swt . right ) ; wl multi valued separator . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl multi valued separator ) ; fdl multi valued separator = new form data ( ) ; fdl multi valued separator . left = new form attachment ( __num__ @$ __num__ ) ; fdl multi valued separator . top = new form attachment ( w operation @$ margin ) ; fdl multi valued separator . right = new form attachment ( middle @$ - margin ) ; wl multi valued separator . set layout data ( fdl multi valued separator ) ; w multi valued separator =,multi valued separator
make sure the client recognizes the underlying <PLACE_HOLDER> otherwise @$ throw a do not retry io <PLACE_HOLDER> .,if ( version info util . has minimum version ( connection header . get version info ( ) @$ request too big exception . major_version @$ request too big exception . minor_version ) ) { req too big . set response ( null @$ null @$ simple rpc server . request_too_big_exception @$ msg ) ; } else { req too big . set response ( null @$ null @$ new do not retryio exception ( ) @$ msg ) ; },a retry exception
push twice will fail and temp dir <PLACE_HOLDER>,file out dir = new file ( string utils . format ( __str__ @$ config . get storage directory ( ) @$ segment path ) ) ; out dir . set read only ( ) ; try { pusher . push ( segment dir @$ segments [ i ] @$ false ) ; } catch ( io exception e ) { assert . fail ( __str__ ) ; },push fail cleaned
simple insert of simple keys @$ with no reprobing on insert until the table gets full exactly . then do a <PLACE_HOLDER> ' on the totally full table .,non blocking hash map < integer @$ object > map = new non blocking hash map < > ( __num__ ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { map . put ( i @$ new object ( ) ) ; } map . get ( __num__ ) ;,insert do '
localized string file <PLACE_HOLDER>,args . filtered resources provider = new resources filter ( aapt target . with flavors ( internal flavor . of ( __str__ ) ) @$ filesystem @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ immutable sorted set . of ( resource1 @$ resource2 ) @$ graph builder @$ immutable list . of ( resource1 . get res ( ) @$ resource2 . get res ( ) ) @$ immutable set . of ( ) @$ immutable set . of ( ) @$ null @$ resources filter . resource compression mode . disabled @$ filter resources steps . resource filter . empty_filter @$ optional . empty ( ) ) ;,string file name
if buffer <PLACE_HOLDER> is after queue <PLACE_HOLDER> we use queue <PLACE_HOLDER> . we need to handle overflow so can not use math.min,if ( p buffer limit - p queue limit > __num__ ) { p buffer limit = p queue limit ; },limit use limit
zap : added the type <PLACE_HOLDER> .,vector < object > v = get list ( uri @$ key ) ; if ( v == null || v . size ( ) == __num__ ) { return null ; } return v . get ( __num__ ) ;,zap added argument
report all volumes as unmounted until we 've recorded that user 0 has <PLACE_HOLDER> . there are no guarantees that callers will see a consistent view of the volume before that point,final boolean system user unlocked = is system unlocked ( user handle . user_system ) ; final boolean user key unlocked ; final boolean storage permission ; final long token = binder . clear calling identity ( ) ; try { user key unlocked = is user key unlocked ( user id ) ; storage permission = m storage manager internal . has external storage ( uid @$ package name ) ; } finally { binder . restore calling identity ( token ) ; } boolean found primary = false ; final array list < storage volume > res = new array list < > ( ) ; synchronized ( m lock ) { for ( int i = __num__ ; i < m volumes . size (,user has access
setup the configurator to inject the pool config in the message driven component create <PLACE_HOLDER>,final message driven component description mdb component description = ( message driven component description ) mdb component configuration . get component description ( ) ; mdb component configuration . get create dependencies ( ) . add ( new pool injecting configurator ( mdb component description ) ) ;,component create service
the following code is for mapjoin initialize all the dummy <PLACE_HOLDER>,log . info ( __str__ ) ; list < operator < ? extends operator desc > > dummy ops = local work . get dummy parent op ( ) ; for ( operator < ? extends operator desc > dummy op : dummy ops ) { dummy op . set exec context ( exec context ) ; dummy op . initialize ( jc @$ null ) ; },mapjoin initialize ops
local variable types must be resolved from the point of view of the annotated class . so do the <PLACE_HOLDER> early @$ because users of the local variable table only have access to the original .,if ( annotated . get local variable table ( ) != null ) { local [ ] orig locals = annotated . get local variable table ( ) . get locals ( ) ; local [ ] new locals = new local [ orig locals . length ] ; resolved java type accessing class = annotated . get declaring class ( ) ; for ( int i = __num__ ; i < new locals . length ; i ++ ) { local orig local = orig locals [ i ] ; new locals [ i ] = new local ( orig local . get name ( ) @$ orig local . get type ( ) . resolve ( accessing class ) @$ orig local . get startbci (,types do lookup
base the data ref on the request parameter but ensure the <PLACE_HOLDER> is based off the incoming request ... this is necessary for scenario 's where the ni fi instance is behind a proxy running a different <PLACE_HOLDER>,ref uri builder . scheme ( request . get scheme ( ) ) ;,instance running scheme
read a property file and convert <PLACE_HOLDER> into configuration lines,file input stream in stream = null ; try { final file f = new file ( args [ __num__ ] ) ; final properties p = new properties ( ) ; in stream = new file input stream ( f ) ; p . load ( in stream ) ; final string builder sb = new string builder ( ) ; sb . append ( __str__ ) ; for ( int i = __num__ ; i <= __num__ ; i ++ ) { sb . append ( __str__ ) ; final string s = p . get property ( __str__ + i ) ; final string [ ] l = common pattern . comma . split ( s ) ; for ( final string element : l,property file them
the source character was not nfd . skip this case ; the first step in obtaining a skeleton is to nfd the <PLACE_HOLDER> @$ so the mapping in this line of confusables.txt will never be applied .,if ( ! normalizer . is normalized ( from ) ) { continue ; },character nfd mapping
it 's an array field type @$ lets <PLACE_HOLDER> the inner type,inner field type . check compatibility ( ( ( array field type ) other ) . inner field type @$ conflicts @$ strict ) ;,type lets check
if this rule does n't define local <PLACE_HOLDER> @$ no resource processing was done @$ so it does n't produce data binding output .,if ( ! android resources . defines android resources ( rule context . attributes ( ) ) ) { return immutable list . of ( ) ; },rule define resources
get this <PLACE_HOLDER> <PLACE_HOLDER> get this <PLACE_HOLDER> <PLACE_HOLDER>,byte [ ] [ ] cols = new byte [ ] [ ] { bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) @$ bytes . to bytes ( __str__ ) } ;,buffer get buffer
use this setting to improve performance if you know that changes in content do not change the layout <PLACE_HOLDER> of the recycler view,m recycler view . set has fixed size ( true ) ; recycler view . layout manager layout manager = new linear layout manager ( get activity ( ) ) ; m recycler view . set layout manager ( layout manager ) ; m recycler view . set adapter ( adapter ) ; m recycler view . add item decoration ( new spaces item decoration ( quick return utils . dp2px ( get activity ( ) @$ __num__ ) ) ) ; array list < view > header views = new array list < > ( ) ; header views . add ( get action bar view ( ) ) ; array list < view > footer views = new array list < > ( ) ; m,changes change size
the server initially reduces the connection flow control <PLACE_HOLDER> to 0 .,run in channel ( server connected channel @$ new http2 runnable ( ) { @ override public void run ( ) throws http2 exception { http2 server . encoder ( ) . write settings ( server ctx ( ) @$ new http2 settings ( ) . copy from ( http2 server . decoder ( ) . local settings ( ) ) . initial window size ( __num__ ) @$ server new promise ( ) ) ; http2 server . flush ( server ctx ( ) ) ; } } ) ; assert true ( server settings ack latch1 . await ( default_await_timeout_seconds @$ seconds ) ) ;,server reduces window
the removed admin might have disabled <PLACE_HOLDER> @$ so update user restrictions .,if ( removed admin ) { push user restrictions ( user handle ) ; },admin disabled camera
even after the security realm deleted the <PLACE_HOLDER> @$ they can still connect @$ until session invalidation,assert user connected ( wc @$ alice ) ; request renew seed for user ( alice ) ; assert user not connected ( wc @$ alice ) ; assert user connected ( wc @$ __str__ ) ; try { wc . login ( alice ) ; fail ( __str__ ) ; } catch ( failing http status code exception e ) { assert equals ( __num__ @$ e . get status code ( ) ) ; },realm deleted realm
the button takes <PLACE_HOLDER> over the summary text and the chevron .,int edit button state = get edit button state ( ) ; if ( edit button state == edit_button_gone ) { m edit button view . set visibility ( gone ) ; m chevron view . set visibility ( m display mode == display_mode_expandable ? visible : gone ) ; boolean show summary = m is summary allowed && ! text utils . is empty ( m summary left text view . get text ( ) ) ; m summary layout . set visibility ( show summary ? visible : gone ) ; } else { boolean is button allowed = m display mode == display_mode_expandable || m display mode == display_mode_normal ; m summary layout . set visibility ( gone ) ; m chevron view . set,button takes precedence
there is a possibility that another thread may have defined the same <PLACE_HOLDER> in the meantime,try { the class = cl . load class ( stub class name ) ; } catch ( class not found exception e1 ) { ejb logger . root_logger . dynamic stub creation failed ( stub class name @$ ex ) ; throw ex ; },thread defined stub
filter and reverse sort the parsed <PLACE_HOLDER> .,tree set < string > parsed set = new tree set < string > ( string . case_insensitive_order ) ; parsed set . add ( text ) ; parsed set . add ( final text ) ; if ( variants != null ) { for ( int i = variants . length ; -- i >= __num__ ; ) { parsed set . add ( variants [ i ] ) ; } } array list < string > parsed list = new array list < string > ( parsed set ) ; collections . reverse ( parsed list ) ; i parsed forms = parsed list . to array ( new string [ parsed list . size ( ) ] ) ;,filter sort form
iterate through all source paths to make sure we are generating a complete <PLACE_HOLDER> of source folders for the source paths .,set < string > src folders = new hash set < > ( ) ; loop through source path : for ( source path java src path : java srcs ) { if ( rule finder . get rule ( java src path ) . is present ( ) ) { continue ; } path java src relative path = rule finder . get source path resolver ( ) . get relative path ( java src path ) ; for ( string src folder : src folders ) { if ( java src relative path . starts with ( src folder ) ) { continue loop through source path ; } } immutable sorted set < string > paths from root = default java package finder . get,paths generating list
now the tricky one . 'before <PLACE_HOLDER> ' le<PLACE_HOLDER>ds to 'c<PLACE_HOLDER>ll <PLACE_HOLDER>ctivity <PLACE_HOLDER> ' @$ which c<PLACE_HOLDER>lls subprocess 02 which termin<PLACE_HOLDER>tes,process instance = runtime service . start process instance by key ( __str__ ) ; tasks = assert task names ( process instance @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; task task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ; assert historic process,a leads a
if current seek <PLACE_HOLDER> is already set @$ update the next seek <PLACE_HOLDER> .,m next seek position = new position ;,current seek position
note : use a message object to facilite script message log <PLACE_HOLDER>,msg . info ( ghidra script . class @$ new script message ( decorated message ) ) ; if ( is running headless ( ) ) { return ; } plugin tool tool = state . get tool ( ) ; if ( tool == null ) { return ; } console service console = tool . get service ( console service . class ) ; if ( console == null ) { return ; } try { console . add message ( get script name ( ) @$ message ) ; } catch ( exception e ) { msg . error ( this @$ __str__ + message @$ e ) ; },message log stuff
let the data store know about the real <PLACE_HOLDER> at this point so that other v ms which discover the real <PLACE_HOLDER> via a profile exchange can send messages to the data store and safely use the <PLACE_HOLDER> .,if ( buk reg != null ) { observer . before assign bucket ( this . partitioned region @$ possibly free bucket id ) ; assign bucket region ( buk reg . get id ( ) @$ buk reg ) ; buk . set hosting ( true ) ; buk reg . invoke partition listener after bucket created ( ) ; } else { if ( buk . get partitioned region ( ) . get colocated with ( ) == null ) { buk . get bucket advisor ( ) . set shadow bucket destroyed ( true ) ; clear all temp queue for shadowpr ( buk . get bucket id ( ) ) ; } },which discover bucket
if the pom has no <PLACE_HOLDER> @$ we cached a missing artifact @$ only return the cached data if no update forced,if ( cached != null && ( ! request . is force update ( ) || has file ( cached . get pom artifact ( ) ) ) ) { return cached ; },pom has edit
this one uses a primitive <PLACE_HOLDER> as much as possible,if ( prop . is unboxable ( ) ) return frf . get required unboxed ( ) ; else return frf . get single ( ) ;,one uses type
test that it works when the shutdown kills the outstanding <PLACE_HOLDER> ...,test shutdown request outstanding ( __num__ @$ __num__ @$ remote invocation exception . class @$ timeout exception . class ) ;,shutdown kills request
the spans that do not cross the network boundary should have the same <PLACE_HOLDER> .,assert that ( client bar span . id ( ) ) . is equal to ( service bar span . id ( ) ) ; assert that ( client qux span . id ( ) ) . is equal to ( service qux span . id ( ) ) ;,spans have id
old implementations might call <PLACE_HOLDER> for initialization @$ so ensure it is initialized here as well . initialization has no side effects if already done .,if ( is enabled ( ) ) { initialize ( ) ; clear events list ( ) ; },implementations call method
to do not introduce the <PLACE_HOLDER> of the separator @$ let us point to the last symbol of it .,if ( ! escaped && space_byte != after next or space && tab_byte != after next or space && linefeed_byte == current ) { return i + __num__ ; },to introduce end
after resume second client join @$ transaction should succesfully await new <PLACE_HOLDER> and commit .,tx fut . get ( ) ;,transaction await connection
we do n't know how the results have changed after being filtered . must set <PLACE_HOLDER> according to contents of results now .,if ( scanner context . get keep progress ( ) ) { scanner context . set progress ( initial batch progress @$ initial size progress @$ initial heap size progress ) ; } else { scanner context . clear progress ( ) ; } scanner context . increment batch progress ( results . size ( ) ) ; for ( cell cell : results ) { scanner context . increment size progress ( private cell util . estimated serialized size of ( cell ) @$ cell . heap size ( ) ) ; },now set progress
return null so that server will return 550 <PLACE_HOLDER> not found .,return null ;,server return response
increment the generation first @$ so observers always see the new <PLACE_HOLDER>,final int key = make key ( settings_type_secure @$ user id ) ; m generation registry . increment generation ( key ) ; final uri uri = get notification uri for ( key @$ secure . location_mode ) ; m handler . obtain message ( my handler . msg_notify_uri_changed @$ user id @$ __num__ @$ uri ) . send to target ( ) ;,observers see value
check if the dataset version contains the correct <PLACE_HOLDER>,assert . assert true ( dataset version . get partition ( ) . get complete name ( ) . equals ignore case ( partition_name ) ) ;,version contains partition
if these targets were treated as distinct targets @$ the rule will have duplicate <PLACE_HOLDER> .,assume that ( platform . detect ( ) @$ is ( not ( windows ) ) ) ; pair < project workspace @$ project workspace > cells = prepare ( __str__ @$ __str__ ) ; project workspace primary = cells . get first ( ) ; project workspace secondary = cells . get second ( ) ; register cell ( primary @$ __str__ @$ primary ) ; register cell ( secondary @$ __str__ @$ primary ) ; path output = primary . build and return output ( __str__ ) ; assert equals ( __str__ @$ __num__ @$ primary . run command ( output . to string ( ) ) . get exit code ( ) ) ;,rule have locations
the private <PLACE_HOLDER> section contains both the public <PLACE_HOLDER> and the private <PLACE_HOLDER>,string key type = key buffer . read string ( ) ;,section contains key
make sure users with huge dict do n't blow up the <PLACE_HOLDER>,if ( dict . size ( ) <= __num__ ) { user dict cache . put ( user id @$ dict ) ; } else { logger . info ( __str__ + dict . size ( ) + __str__ + user id + __str__ ) ; },users blow cache
no values provided . read all <PLACE_HOLDER> .,if ( no from ) { m sector range . set text ( get string ( r . string . text_sector_range_all ) ) ; if ( save as default . is checked ( ) ) { save mapping range ( __str__ @$ __str__ ) ; } return ; },values read sections
not a small battery device @$ so plugged in status should not affect forced app <PLACE_HOLDER>,m is small battery device = false ; final app state tracker testable instance = new instance ( ) ; call start ( instance ) ; assert false ( instance . is force all apps standby enabled ( ) ) ; m power save mode = true ; m power save observer . accept ( get power save state ( ) ) ; assert true ( instance . is force all apps standby enabled ( ) ) ;,device affect state
we need a real server and client in this test @$ because netty 's embedded channel is not failing the channel <PLACE_HOLDER> of failed writes .,netty server and client server and client = init server and client ( protocol @$ create config ( ) ) ; channel ch = connect ( server and client ) ; network client handler handler = get client handler ( ch ) ;,channel failing reading
if it is a node try to parse with the node parser to find out whether we should may use the generated create and get uncached <PLACE_HOLDER> .,if ( node code generator . is specialized node ( parameter . get type ( ) ) ) { node parser parser = node parser . create default parser ( ) ; parser . node only = true ; type element element = element utils . cast type element ( parameter . get type ( ) ) ; if ( ! node only ) { node data parsed node = parser . parse ( element ) ; if ( parsed node != null ) { list < code executable element > executables = node factory factory . create factory methods ( parsed node @$ element filter . constructors in ( element . get enclosed elements ( ) ) ) ; type element type = element utils . cast,generated create doc
if the scope has no <PLACE_HOLDER> try to remove it,if ( ! ( ( basic scope ) broadcast scope ) . has event listeners ( ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ ) ; } scope . remove child scope ( broadcast scope ) ; } log . debug ( __str__ @$ name ) ;,scope has listeners
insert rows on different streams but since they all go thru one export track <PLACE_HOLDER> thru the check stream,for ( int i = __num__ ; i < loop_count ; i ++ ) { for ( int j = __num__ ; j < m_stream count ; j ++ ) { string stream name = string . format ( stream_template @$ j ) ; for ( int k = row count ; k < row count + row_batch ; k ++ ) { data [ __num__ ] = k ; m_verifier . add row ( m_client @$ check stream @$ k @$ data ) ; m_client . call procedure ( __str__ @$ __str__ + stream name + __str__ + k + __str__ ) ; } row count += row_batch ; } },export track all
for now we just propagate statistics for source symbols . handling semi join output symbols requires <PLACE_HOLDER> for correlation for boolean columns .,return optional . of ( source stats ) ;,symbols requires support
this method always throws activation <PLACE_HOLDER>,user service . activate user ( __str__ ) ;,method throws exception
rather than risk a mysterious class cast <PLACE_HOLDER> during unit tests @$ throw an explanatory <PLACE_HOLDER> .,throw new unsupported operation exception ( __str__ + __str__ + __str__ ) ;,class cast exception
and add a new <PLACE_HOLDER> to that dir :,path uf = ro path . resolve ( system utils . file name ( __str__ ) ) ; files . write ( uf @$ arrays . as list ( __str__ @$ __str__ ) ) ; files . set posix file permissions ( uf @$ posix file permissions . from string ( __str__ ) ) ; files . set posix file permissions ( ro path @$ posix file permissions . from string ( __str__ ) ) ;,and add file
super translation should contain the generic private method <PLACE_HOLDER> .,assert translation ( super translation @$ __str__ ) ;,translation contain signature
json serializer will always close the <PLACE_HOLDER>,try { json writer . write value ( os @$ yielder ) ; os . flush ( ) ; os . close ( ) ; } catch ( exception ex ) { e = ex ; log . no stack trace ( ) . error ( ex @$ __str__ ) ; throw new runtime exception ( ex ) ; } finally { thread . current thread ( ) . set name ( curr thread name ) ; query lifecycle . emit logs and metrics ( e @$ req . get remote addr ( ) @$ os . get count ( ) ) ; if ( e == null ) { successful query count . increment and get ( ) ; } else { failed query count . increment,serializer close stream
we perform the analogous test on the get <PLACE_HOLDER> .,subclass . set field ( type @$ subclass @$ boolean field @$ illegal argument exception class @$ value ) ;,test get methods
update queues @$ all queue can access this <PLACE_HOLDER>,for ( queue q : queue collections . values ( ) ) { resources . subtract from ( q . resource @$ oldnm . resource ) ; },queue access node
been returned from the discovery agent . up to that point the reconnect delay is used which has a default <PLACE_HOLDER> of 10,long initial reconnect delay = __num__ ; long startt = system . current time millis ( ) ; string group id = __str__ + startt ; try { string url str = __str__ + group id + __str__ + initial reconnect delay ; activemq connection factory factory = new activemq connection factory ( url str ) ; log . info ( __str__ ) ; connection connection = factory . create connection ( ) ; connection . set clientid ( __str__ ) ; fail ( __str__ ) ; } catch ( jms exception expected ) { assert true ( __str__ + expected . get cause ( ) @$ expected . get cause ( ) instanceof java . io . io exception ) ; long duration = system . current,which has value
the list has exactly the same <PLACE_HOLDER> of elements as the list of element matchers :,for ( int i = __num__ ; i < pattern token matchers . size ( ) ; i ++ ) { p tokens matched . add ( boolean . false ) ; } int i = __num__ ; int min occur correction = get min occurrence correction ( ) ; while ( i < limit + min occur correction && ! ( rule . is sent start ( ) && i > __num__ ) ) { int skip shift total = __num__ ; boolean all elements match = false ; unified tokens = null ; int matching tokens = __num__ ; int first match token = - __num__ ; int last match token = - __num__ ; int first marker match token = - __num__ ; int last,list has number
step 1 : ensure that user has write permissions to the process group . if not @$ then immediately fail . step 2 : retrieve <PLACE_HOLDER> from <PLACE_HOLDER> registry,if ( version control info != null && request process group entity . get versioned flow snapshot ( ) == null ) { final versioned flow snapshot flow snapshot = get flow from registry ( version control info ) ; service facade . discover compatible bundles ( flow snapshot . get flow contents ( ) ) ; service facade . resolve inherited controller services ( flow snapshot @$ group id @$ ni fi user utils . get ni fi user ( ) ) ; request process group entity . set versioned flow snapshot ( flow snapshot ) ; },not retrieve flow
2 partitions commit <PLACE_HOLDER>,for ( int partition id = __num__ ; partition id < __num__ ; partition id ++ ) { string segment name = new llc segment name ( raw_table_name @$ partition id @$ __num__ @$ current_time_ms ) . get segment name ( ) ; committing segment descriptor committing segment descriptor = new committing segment descriptor ( segment name @$ partition_offset + num_docs @$ __num__ ) ; committing segment descriptor . set segment metadata ( mock segment metadata ( ) ) ; segment manager . commit segment metadata ( realtime_table_name @$ committing segment descriptor ) ; } test set up new partitions ( segment manager @$ false ) ;,partitions commit segment
aws silences all the <PLACE_HOLDER> but illegal argument exception @$ throw runtime exception .,rate limited log ( level . error @$ e @$ __str__ @$ m_config . get resourceid ( ) ) ; m_worker . shutdown ( ) ;,aws silences output
the face timeout message is not very actionable @$ let 's ask the <PLACE_HOLDER> to manually retry .,if ( msg id == face manager . face_error_timeout ) { show swipe up to unlock ( ) ; } else if ( m status bar keyguard view manager . is bouncer showing ( ) ) { m status bar keyguard view manager . show bouncer message ( err string @$ m initial text color state ) ; } else if ( update monitor . is screen on ( ) ) { show transient indication ( err string ) ; hide transient indication delayed ( hide_delay_ms ) ; } else { m message to show on screen on = err string ; },let ask server
setup the build params we 'll pass to description when generating the build <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; cxx source rule factory cxx source rule factorypdc = cxx source rule factory helper . of ( filesystem . get root path ( ) @$ target @$ cxx platform @$ pic type . pdc ) ; cxx library builder cxx library builder = new cxx library builder ( target ) . set exported headers ( immutable sorted map . of ( gen header name @$ default build target source path . of ( gen header target ) ) ) . set srcs ( immutable sorted set . of ( source with flags . of ( fake source path . of ( source name ) ) @$ source with flags . of ( default build,the build rules
first verify that our function does not already contain the <PLACE_HOLDER> we 're going to add .,collection < ? extends function tag > tags = get all tags ( ) ; assert true ( ! is tag name in list ( tag name1 @$ tags ) ) ; assert true ( ! is tag name in list ( tag name2 @$ tags ) ) ; assert true ( ! is tag name in list ( tag name3 @$ tags ) ) ;,function contain tags
let the ejb jar metadata tell us <PLACE_HOLDER> the version is,return ejb jar meta data . is version greater than or equal ( ejb jar version . ejb_3_2 ) ;,version is what
this worker has this <PLACE_HOLDER> @$ so it is no longer lost .,m lost blocks . remove ( block id ) ;,worker has block
check that new dynamic config includes the updated client port . check that server changed server <PLACE_HOLDER> erased client port from static config . check that other servers still have client port in static config .,for ( int i = __num__ ; i < server_count ; i ++ ) { reconfig test . test server has config ( zk [ i ] @$ new servers @$ null ) ; properties static cfg = read properties from file ( mt [ i ] . conf file ) ; if ( i == changed server id ) { assert false ( static cfg . contains key ( __str__ ) ) ; } else { assert true ( static cfg . contains key ( __str__ ) ) ; } } for ( int i = __num__ ; i < server_count ; i ++ ) { mt [ i ] . shutdown ( ) ; zk [ i ] . close ( ) ; zk admin [,server changed id
this <PLACE_HOLDER> wraps the results <PLACE_HOLDER> and guarantees the scrolling behaviour,j panel wrapper = new j panel ( new border layout ( ) ) ; wrapper . set background ( color scheme . dark_gray_color ) ; wrapper . add ( search items panel @$ border layout . north ) ;,panel wraps panel
optimization : default serializer just writes <PLACE_HOLDER> @$ so we can avoid a call :,if ( is default serializer ( ser ) ) { if ( unwrap single == _unwrap single ) { return this ; } return _with resolved ( property @$ unwrap single ) ; },serializer writes string
add padding only if the list has items in it @$ that <PLACE_HOLDER> we do n't show the popup if it is not needed,if ( list content > __num__ ) other heights += padding ; return list content + other heights ;,list has way
we do not dump acid table related <PLACE_HOLDER> when taking a bootstrap dump of acid tables as part of an incremental dump . so we should n't be dumping any changes to acid table as part of the commit . at the same time we need to dump the commit transaction event so that replication can end a transaction opened when replaying open transaction,if ( within context . hive conf . get bool var ( hive conf . conf vars . repl_bootstrap_acid_tables ) ) { log . debug ( __str__ + __str__ ) ; replicating acid events = false ; } else if ( ! repl utils . include acid table in dump ( within context . hive conf ) ) { log . debug ( __str__ + __str__ ) ; replicating acid events = false ; },bootstrap dump events
call create <PLACE_HOLDER> log and move the resulting <PLACE_HOLDER> to the name dir .,create edits log . main ( new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ test_dir . get absolute path ( ) } ) ; path edits wildcard = new path ( test_dir . get absolute path ( ) @$ __str__ ) ; file context local fc = file context . get localfs file context ( ) ; for ( file status edits : local fc . util ( ) . glob status ( edits wildcard ) ) { path src = edits . get path ( ) ; path dst = new path ( new file ( name dir @$ __str__ ) . get absolute path ( ) @$ src . get name ( ) ) ; local fc . rename,edits log edits
short codes can not have <PLACE_HOLDER>,if ( separator position < separator_position ) { return false ; },codes have separators
if a metric is counter @$ the <PLACE_HOLDER> sent via rpc should be the incremental <PLACE_HOLDER> ; i.e . the amount the <PLACE_HOLDER> has changed since the last rpc . the master should equivalently increment its <PLACE_HOLDER> based on the received metric rather than replacing it .,if ( ! metric set . add ( metric ) ) { metric old metric = metric set . get first by field ( full_name_index @$ metric . get full metric name ( ) ) ; if ( metric . get metric type ( ) == metric type . counter ) { if ( metric . get value ( ) != __num__ ) { old metric . add value ( metric . get value ( ) ) ; } } else { old metric . set value ( metric . get value ( ) ) ; } },master increment stats
if system audio control feature is enabled @$ turn on system audio <PLACE_HOLDER> when new avr is detected . otherwise @$ turn off system audio <PLACE_HOLDER> .,boolean target system audio mode = tv ( ) . is system audio control feature enabled ( ) ; if ( current system audio mode != target system audio mode ) { add and start action ( new system audio action from tv ( tv ( ) @$ m avr address @$ target system audio mode @$ null ) ) ; } else { tv ( ) . set system audio mode ( target system audio mode ) ; },mode turn mode
delivery timed out @$ and the timeout handling already took <PLACE_HOLDER> of updating our tracking here @$ so we need n't do anything further .,if ( debug_listener_callback ) { slog . i ( tag @$ __str__ + who ) ; },handling took care
decide whether to perform a get or put <PLACE_HOLDER>,boolean response ; if ( rand . next double ( ) < config . getputratio ) { mp rand = rand . next double ( ) ; if ( mp rand < config . multisingleratio ) { if ( total connections . get ( ) > __num__ && config . poolsize > __num__ ) { slow = true ; debug = true ; } else debug = false ; response = client . call procedure ( new get callback ( mp rand ) @$ __str__ @$ processor . generate random key for retrieval ( ) ) ; } else { response = client . call procedure ( new get callback ( mp rand ) @$ __str__ @$ processor . generate random key for retrieval ( ) ) ;,a get operation
delete interpret odex for android o @$ directory change . fortunately @$ we do n't need to support android o interpret <PLACE_HOLDER> any more,log . i ( tag @$ __str__ ) ; share patch file util . delete dir ( patch version directory + __str__ + share constants . interpret_dex_optimize_path ) ;,o interpret sudo
test missing proxy class class not found <PLACE_HOLDER>,oin = new proxy blind input stream ( new byte array input stream ( bout . to byte array ( ) ) ) ; try { oin . read object ( ) ; throw new error ( ) ; } catch ( class not found exception ex ) { },class found exception
one millisecond <PLACE_HOLDER>,timeout millis . set ( persists3 . _bucket cache @$ __num__ ) ;,one millisecond timeout
maps must take 2 type <PLACE_HOLDER> @$ not just one,try { tf . construct parametric type ( map . class @$ strc ) ; } catch ( illegal argument exception e ) { verify exception ( e @$ __str__ ) ; },maps take parameters
this version does not support fractional <PLACE_HOLDER>,int nano of second = __num__ ;,version support digits
do not allow to reset password when current user has a managed <PLACE_HOLDER>,if ( ! is managed profile ( user handle ) ) { for ( user info user info : m user manager . get profiles ( user handle ) ) { if ( user info . is managed profile ( ) ) { if ( ! pren ) { throw new illegal state exception ( __str__ ) ; } else { slog . e ( log_tag @$ __str__ ) ; return false ; } } } },user has profile
note : the lookup is enforcing security across users by making sure the caller can only access <PLACE_HOLDER> it hosts or provides .,widget widget = lookup widget locked ( app widget id @$ binder . get calling uid ( ) @$ calling package ) ; if ( widget != null ) { update app widget instance locked ( widget @$ views @$ partially ) ; },caller access widgets
dump the last row which has less than 16 <PLACE_HOLDER> .,if ( remainder != __num__ ) { final int row start index = ( full rows << __num__ ) + start index ; append hex dump row prefix ( dump @$ full rows @$ row start index ) ; final int row end index = row start index + remainder ; for ( int j = row start index ; j < row end index ; j ++ ) { dump . append ( byte2hex [ get unsigned byte ( buffer @$ j ) ] ) ; } dump . append ( hex_padding [ remainder ] ) ; dump . append ( __str__ ) ; for ( int j = row start index ; j < row end index ; j ++ ) { dump . append (,which has bytes
the first read should allocate one shared memory <PLACE_HOLDER> and slot .,dfs test util . read file buffer ( fs @$ test_path1 ) ;,read allocate buffer
any app can add new static shared <PLACE_HOLDER>,if ( scan result . static shared library info != null ) { return collections . singleton list ( scan result . static shared library info ) ; } final boolean has dynamic libraries = ( pkg . application info . flags & application info . flag_system ) != __num__ && scan result . dynamic shared library infos != null ; if ( ! has dynamic libraries ) { return null ; } final boolean is updated system app = pkg . is updated system app ( ) ;,app add libraries
need to dig out actual destination map object and use map property <PLACE_HOLDER> to set the value on that target object ... .,if ( get dest field map get method ( ) != null || mapping utils . is supported map ( determine actual property type ( get dest field name ( ) @$ is dest field indexed ( ) @$ get dest field index ( ) @$ dest obj @$ true ) ) ) { prepare target object result result = prepare target object ( dest obj ) ; target object = result . target object ; prop descriptor = result . prop descriptor ; } else { prop descriptor = super . get dest property descriptor ( dest obj . get class ( ) ) ; },need object descriptor
perform <PLACE_HOLDER> substitution @$ this is useful when the library is using a <PLACE_HOLDER> in a key element @$ we however do not need to record these substitutions so feed it with a fake merging report .,merging report . builder builder = new merging report . builder ( merging report builder . get logger ( ) ) ; builder . get action recorder ( ) . record default node action ( library document . get root node ( ) ) ; perform place holder substitution ( manifest info @$ library document @$ builder ) ; if ( builder . has errors ( ) ) { builder . build ( ) . log ( m logger ) ; },library using default
this interface @$ a default implementation is supplied which uses the old <PLACE_HOLDER> . all new implementations must override this interface and should not use the other add response time <PLACE_HOLDER> .,int queue time ms = ( int ) details . get ( processing details . timing . queue @$ time unit . milliseconds ) ; int processing time ms = ( int ) details . get ( processing details . timing . processing @$ time unit . milliseconds ) ; add response time ( call name @$ schedulable . get priority level ( ) @$ queue time ms @$ processing time ms ) ;,which uses format
does unaltered <PLACE_HOLDER> still match ref <PLACE_HOLDER> ?,bos . reset ( ) ; bs ref . serialize ( b ref @$ bos ) ; ser deser ( bs @$ b1 @$ bos . to byte array ( ) @$ null @$ null ) ;,block match block
complete task a which should start another <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name and state ( plan item instances @$ __str__ @$ active ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert equals ( __num__ @$ plan item instances . size ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active @$ waiting_for_repetition ) ; assert plan item instance state ( plan item instances @$ __str__ @$ available ) ; assert plan item instance state ( plan item instances @$ __str__ @$ available ) ;,which start stage
shutdown the client <PLACE_HOLDER> which will trigger the channel pool manager <PLACE_HOLDER> sharing connecion shutdown,future callback < none > shutdown callback = new future callback < > ( ) ; client factory . shutdown ( shutdown callback ) ; shutdown callback . get ( __num__ @$ time unit . seconds ) ;,which trigger factory
no previous session or invalid @$ accept this <PLACE_HOLDER>,if ( session == null || ! is valid ( session ) ) { requested session id = id ; session = s ; } else { if ( s != null && is valid ( s ) ) throw new bad message exception ( __str__ + requested session id + __str__ + id ) ; },session accept request
but if we are deserializing an exception too many received versions use a <PLACE_HOLDER> anyway .,long delta = next version - previous version ; if ( use tree sets for testing || ( delta > rvv_max_bitset_span && initial exception count * __num__ < delta ) ) { return new rvv exceptiont ( previous version @$ next version ) ; } return new rvv exceptionb ( previous version @$ next version ) ;,versions use three
set up duplicate counter . expect exactly the responses corresponding to needs repair . these may @$ or may not @$ include the local <PLACE_HOLDER> .,list < long > expectedhs ids = new array list < long > ( needs repair ) ; duplicate counter counter = new duplicate counter ( host messenger . valhalla @$ message . get txn id ( ) @$ expectedhs ids @$ message @$ m_mailbox . geths id ( ) ) ; final duplicate counter key dc key = new duplicate counter key ( message . get txn id ( ) @$ message . get sp handle ( ) ) ; update or add duplicate counter ( dc key @$ counter ) ; m_unique id generator . update most recently generated unique id ( message . get unique id ( ) ) ;,these include site
internal state updated @$ now notify the view <PLACE_HOLDER> .,m view . dispatch moved to display ( m display @$ config ) ;,updated notify view
check for loops in the chain . if there are repeated <PLACE_HOLDER> @$ the set of <PLACE_HOLDER> in the chain will contain fewer <PLACE_HOLDER> than the chain,set < certificate > set = new hash set < > ( arrays . as list ( cert chain ) ) ; return set . size ( ) == cert chain . length ;,certs contain certs
stores all found style <PLACE_HOLDER> from the container,if ( container . get style map ( ) != null ) { super . assign style map ( container . get style map ( ) @$ get styles renderer ( ) ) ; },stores found map
this check needs node <PLACE_HOLDER> to work correctly . if we 're not in ide mode @$ we do n't have that information @$ so just skip the check .,if ( length == __num__ ) { return ; },check needs attempts
refresh store files post <PLACE_HOLDER> @$ this should not open already compacted files,hr1 . refresh store files ( true ) ; int num regions before split = admin . get regions ( test_table ) . size ( ) ;,files post compaction
app engine does n't support thread <PLACE_HOLDER> @$ so do n't even try,if ( is app engine ( ) ) { return executor ; },engine support renaming
the query delegate does n't support <PLACE_HOLDER> .,query delegate . set order by ( sorters ) ;,delegate support order
move out one step @$ if the current instance has an outer <PLACE_HOLDER>,member definition outer member = thisc . find outer member ( ) ; if ( outer member == null ) { thise = null ; continue ; },instance has member
change the reference to new topology . so everyone who captured old <PLACE_HOLDER> will see a consistent snapshot .,nodes = updated top ;,who captured version
if we have still have disabling services then we may have a stuck service so throw an <PLACE_HOLDER>,if ( service states . get disabling ( ) > __num__ ) { if ( should print ( properties ) ) { print services still disabling ( flow client @$ pg id ) ; } throw new command exception ( __str__ ) ; },service throw exception
we do not yet know <PLACE_HOLDER> these numbers are @$ so giving them meaningless names for now .,md signed encoded number a @$ b @$ c @$ name num ; switch ( code ) { case __str__ : { code = dmang . get and increment ( ) ; switch ( code ) { case __str__ : name num = new md signed encoded number ( dmang ) ; name num . parse ( ) ; name = name num . to string ( ) ; break ; case __str__ : { dmang . push modifier context ( ) ; md object object = new md objectcpp ( dmang ) ; object . parse ( ) ; dmang . pop context ( ) ; string builder builder = new string builder ( ) ; object . insert ( builder ) ; dmang . insert string,numbers are what
check that the name matches the found <PLACE_HOLDER>,for ( object name found : found names ) { if ( name . apply ( found ) ) { result . add ( found ) ; } },name matches predicate
if the last marker instruction encountered was an outlineable chunk <PLACE_HOLDER> @$ it means that the current instruction marks the <PLACE_HOLDER> of a chunk that contained child chunks . those children might need to be examined below in case they are better candidates for outlining than the current chunk .,if ( ! open chunk at curr level ) { nested sub chunks = curr level chunks ; curr level chunks = ( array list ) sub chunk stack . pop ( ) ; },instruction marks end
the limit and offset drop the first and last <PLACE_HOLDER> @$ leaving 2 <PLACE_HOLDER> of 1 and 1 group of 2 .,validate table of longs ( client @$ __str__ + tb + __str__ @$ new long [ ] [ ] { { __num__ @$ __num__ } @$ { __num__ @$ __num__ } } ) ;,limit drop groups
attempting to create transform a second time with the same sp is should throw an <PLACE_HOLDER> ...,try { m ip sec service . create transform ( ip sec config @$ new binder ( ) @$ __str__ ) ; fail ( __str__ ) ; } catch ( illegal state exception expected ) { },time throw exception
not enough space to hold the whole <PLACE_HOLDER> @$ we will try again later .,if ( offset == limits . iov_max || ! iov array . add ( buf @$ index @$ len ) ) { return false ; },space hold buffer
no exception means <PLACE_HOLDER> passed,return true ;,exception means allocation
if limit specified and is smaller than reasonable large batch <PLACE_HOLDER> then we force batch <PLACE_HOLDER> to be the same as limit @$ but negative @$ this force cursor to close right after result is sent,if ( limit <= large_batch_size ) { cursor . batch size ( - limit ) ; },size force size
set view 0 default <PLACE_HOLDER> .,bgfx_set_view_rect ( __num__ @$ __num__ @$ __num__ @$ width @$ height ) ;,set view rect
ensure every key has a value and substitute <PLACE_HOLDER> for values,if ( ivs . size ( ) > __num__ ) { int template key count = __num__ ; for ( interpolation variable variable : ivs . key set ( ) ) { string value = row data provider . get template key value ( variable . get var name ( ) ) ; if ( ! __str__ . equals ( value ) ) { template key count ++ ; } } if ( action instanceof free form line ) { add action = template key count == ivs . size ( ) ; } else if ( template key count > __num__ ) { add action = true ; } },key has characters
finally @$ the consumer operation provides formatted log <PLACE_HOLDER>,log . info ( string . format ( __str__ @$ kw . get key ( ) . get key ( ) @$ kw . get value ( ) ) ) ;,operation provides information
get unsafe access impl.unsafe get int <PLACE_HOLDER>,registration r = new registration ( invocation plugins @$ my unsafe access . class ) ; truffle graph builder plugins . register unsafe load store plugins ( r @$ false @$ null @$ java kind . int ) ; super . register invocation plugins ( invocation plugins ) ;,impl.unsafe get method
if the user specified a custom <PLACE_HOLDER> for the tab indicators @$ then do not draw the bottom strips .,if ( ! m draw bottom strips ) { return ; },user specified value
check the rowcount value @$ if it is equal to 0 it means the call did not delete the <PLACE_HOLDER> from federation state store,if ( cstmt . get int ( __num__ ) == __num__ ) { string err msg = __str__ + request . get application id ( ) + __str__ ; federation state store utils . log and throw store exception ( log @$ err msg ) ; },call delete application
if it 's a virtual service then receive the <PLACE_HOLDER> from parent service,if ( object . get virtual ( ) == __num__ ) { rec data = get data from service ( object . get parent ( ) ) ; } else { rec data = get data from service ( service ) ; },service receive data
copy is necessary since the instance info builder just uses the original <PLACE_HOLDER> @$ and we do n't want to corrupt the global eureka copy of the object which may be used by other clients in our system,instance info copy = new instance info ( ii ) ; if ( is secure ) { ii = new instance info . builder ( copy ) . set secure port ( override port ) . build ( ) ; } else { ii = new instance info . builder ( copy ) . set port ( override port ) . build ( ) ; },builder uses one
should not throw <PLACE_HOLDER>,handler . on throwable ( e ) ;,not throw exception
update scores should not update any <PLACE_HOLDER> since they are both unregistered,m network score service . update scores ( new scored network [ ] { scored_network } ) ;,scores update counts
do n't use delayed clipboard rendering for the transferable 's data . if we did that @$ we would c<PLACE_HOLDER> transferable.get transfer data on the toolkit thread @$ which is a security hole . get <PLACE_HOLDER> of the target formats into which the transferable can be translated . then @$ for each format @$ translate the data and post it to the clipboard .,data transferer data transferer = data transferer . get instance ( ) ; long [ ] format array = data transferer . get formats for transferable as array ( contents @$ flavor map ) ; declare types ( format array @$ this ) ; map < long @$ data flavor > format map = data transferer . get formats for transferable ( contents @$ flavor map ) ; for ( map . entry < long @$ data flavor > entry : format map . entry set ( ) ) { long format = entry . get key ( ) ; data flavor flavor = entry . get value ( ) ; try { byte [ ] bytes = data transferer . get instance ( ) . translate transferable,formats get all
reachable only if the file naming is incorrect by current standard . thus we log an <PLACE_HOLDER> instead of recording failure to uma .,if ( tries >= max_tries_allowed || tries < __num__ ) { log . e ( tag @$ __str__ + minidump file name + __str__ ) ; return ; },reachable log error
set the scrollbars <PLACE_HOLDER> . if the thumb has reached the end of the scrollbar @$ then just set the <PLACE_HOLDER> to its maximum . otherwise compute the <PLACE_HOLDER> as accurately as possible .,if ( thumb pos == thumb max ) { if ( scrollbar . get orientation ( ) == j scroll bar . vertical || scrollbar . get component orientation ( ) . is left to right ( ) ) { scrollbar . set value ( model . get maximum ( ) - model . get extent ( ) ) ; } else { scrollbar . set value ( model . get minimum ( ) ) ; } } else { float value max = model . get maximum ( ) - model . get extent ( ) ; float value range = value max - model . get minimum ( ) ; float thumb value = thumb pos - thumb min ; float thumb range = thumb max,set compute position
menu currently always enabled @$ but dont do <PLACE_HOLDER> unless a message is being held,if ( breakpoint management interface . is hold message ( null ) ) { breakpoint management interface . cont ( ) ; },dont do anything
and the outer struct has <PLACE_HOLDER> that are defined to be within the footprint of the embedded struct 's trailing padding .,debug info entry intdie = add int ( cu ) ;,struct has entries
set output collector for any reduce sink <PLACE_HOLDER> in the pipeline .,list < operator < ? > > children = new array list < > ( ) ; children . add ( reducer ) ; children . add all ( dummy ops ) ; create output map ( ) ; operator utils . set children collector ( children @$ out map ) ; check abort condition ( ) ; reducer . set reporter ( reporter ) ; mapred context . get ( ) . set reporter ( reporter ) ;,collector reduce operators
let 's park this <PLACE_HOLDER> and wait for a primary !,synchronized ( this ) { stop watch timer = new stop watch ( true ) ; long warn time = get distribution manager ( ) . get config ( ) . get ack wait threshold ( ) * __num__ ; boolean logged warning = false ; try { for ( ; ; ) { get advisee ( ) . get cancel criterion ( ) . check cancel in progress ( null ) ; final internal cache cache = get bucket ( ) . get cache ( ) ; if ( cache != null && cache . is cache at shutdown all ( ) ) { throw cache . get cache closed exception ( __str__ ) ; } if ( get bucket redundancy ( ) == - __num__ ),'s park thread
we expect that both nn 1 and nn 2 will have some <PLACE_HOLDER> of deletions queued up for the d ns .,banner ( __str__ ) ; block manager test util . compute invalidation work ( nn1 . get namesystem ( ) . get block manager ( ) ) ; banner ( __str__ ) ; block manager test util . compute invalidation work ( nn2 . get namesystem ( ) . get block manager ( ) ) ;,nn have kinds
user 1 sends an empty room configuration <PLACE_HOLDER> which indicates that we want an instant room,form form = new form ( form . type_submit ) ; form field field = new form field ( __str__ ) ; field . set type ( __str__ ) ; form . add field ( field ) ; form . set answer ( __str__ @$ arrays . as list ( __str__ ) ) ; muc . send configuration form ( form ) ;,user sends form
set up the 2 d array used to hold the names . the first column contains the olson <PLACE_HOLDER> .,string [ ] [ ] result = new string [ available time zone ids . length ] [ __num__ ] ; for ( int i = __num__ ; i < available time zone ids . length ; ++ i ) { result [ i ] [ __num__ ] = available time zone ids [ i ] ; } long native start = system . current time millis ( ) ; fill zone strings ( locale . to string ( ) @$ result ) ; long native end = system . current time millis ( ) ; intern strings ( result ) ;,column contains ids
update vertex positions for ' z samples ' parallel <PLACE_HOLDER> .,float z rate = ( radius * __num__ ) / ( float ) ( z samples ) ; float z height = - radius + ( z rate / __num__ ) ; float rb = __num__ / z samples ; float b = rb / __num__ ; for ( int k = __num__ ; k < z samples ; k ++ ) { angle = __num__ ; float scale = __num__ * fast math . sqrt ( b - b * b ) ; for ( int i = __num__ ; i < samples ; i ++ ) { float x = radius * fast math . cos ( angle ) ; float y = radius * fast math . sin ( angle ) ; pb . put,' z points
this situation is possible only if a callee throws an <PLACE_HOLDER> which type extends throwable directly,if ( cause instanceof command action execution exception ) { command action execution exception command action execution exception = ( command action execution exception ) cause ; cause = command action execution exception . get cause ( ) ; },callee throws exception
one file may have several <PLACE_HOLDER>,final list < b object > l = patho . get list ( ) ;,file have entries
if the app did everything <PLACE_HOLDER> @$ return without logging .,if ( app op mode == app ops manager . mode_allowed ) { return location permission result . allowed ; } else { log . i ( tag @$ query . calling package + __str__ + location type for log + __str__ + __str__ ) ; return app ops mode to permission result ( app op mode ) ; },app did ok
j rockit throws this <PLACE_HOLDER> instead of returning null as the javadocs say it should . see bug 36348,this . set collection usage unsupported ( mp ) ;,rockit throws exception
confirm that the result represents a v<PLACE_HOLDER>eo . otherwise @$ the item will not contain a v<PLACE_HOLDER>eo <PLACE_HOLDER> .,if ( r id . get kind ( ) . equals ( __str__ ) ) { thumbnail thumbnail = single video . get snippet ( ) . get thumbnails ( ) . get default ( ) ; system . out . println ( __str__ + r id . get video id ( ) ) ; system . out . println ( __str__ + single video . get snippet ( ) . get title ( ) ) ; system . out . println ( __str__ + thumbnail . get url ( ) ) ; system . out . println ( __str__ ) ; },item contain id
timeout . we do n't need to call ` terminate ` here @$ because wait for automatically terminates the <PLACE_HOLDER> in case of a timeout .,return wait result . timeout ; default :,wait terminates process
if the autocomplete whitelist does n't contain the <PLACE_HOLDER> @$ skip storing its value,continue ;,whitelist contain key
if we have an image src and the target is reinitializing <PLACE_HOLDER> the get initial image so that it will wait until the target region is fully initialized before responding to the get image request . otherwise @$ the source may respond with no data because it is still initializing @$ e.g . loading a snapshot .,cache observer holder . get instance ( ) . after markinggii started ( ) ;,target reinitializing look
genrule uses legacy <PLACE_HOLDER>,genrule output path = build target paths . get gen path ( filesystem @$ build target factory . new instance ( __str__ ) @$ __str__ ) . resolve ( __str__ ) . to string ( ) ;,genrule uses paths
we wo n't race another upgrade attempt because only one thread will get the <PLACE_HOLDER> from the map,integer timeout = local sessions with timeouts . remove ( session id ) ; if ( timeout != null ) { log . info ( __str__ @$ long . to hex string ( session id ) ) ; track session ( session id @$ timeout ) ; upgrading sessions . put ( session id @$ timeout ) ; local session tracker . remove session ( session id ) ; return timeout ; },thread get session
all in the same call stack @$ the upgrade codec should receive the <PLACE_HOLDER> @$ written the upgrade response @$ and upgraded the pipeline .,assert true ( write upgrade message ) ; assert false ( write flushed ) ; assert null ( ctx . pipeline ( ) . get ( http server codec . class ) ) ; assert not null ( ctx . pipeline ( ) . get ( __str__ ) ) ; in read call = false ;,codec receive message
if an exception here occurs then there is the danger that urls which had been in the crawler are overwritten a second <PLACE_HOLDER> to prevent that @$ we reject urls in these events,concurrent log . log exception ( e ) ; return __str__ + e . get message ( ) ;,urls overwritten time
copy dex apply <PLACE_HOLDER> directly on dex,steps . add ( copy step . for file ( filesystem @$ input primary dex path @$ output primary dex path ) ) ; steps . add ( new default shell step ( filesystem . get root path ( ) @$ immutable list . of ( reorder tool . to string ( ) @$ reorder data file . to string ( ) @$ output primary dex path . to string ( ) ) ) ) ;,dex apply magic
get all cookies that domain matches the <PLACE_HOLDER>,for ( map . entry < uri @$ list < http cookie > > entry : map . entry set ( ) ) { if ( uri . equals ( entry . get key ( ) ) ) { continue ; } list < http cookie > entry cookies = entry . get value ( ) ; for ( iterator < http cookie > i = entry cookies . iterator ( ) ; i . has next ( ) ; ) { http cookie cookie = i . next ( ) ; if ( ! http cookie . domain matches ( cookie . get domain ( ) @$ uri . get host ( ) ) ) { continue ; } if ( cookie . has expired ( ),domain matches uri
update last applied tx <PLACE_HOLDER> even in case of error @$ since some ops may have been successfully applied before the error .,last applied tx id = loader . get last applied tx id ( ) ;,update applied id
the second observer should only get the newest <PLACE_HOLDER> and any later <PLACE_HOLDER>s .,live data . observe ( m lifecycle owner @$ new observer < string > ( ) { @ override public void on changed ( @ nullable string s ) { output2 . add ( s ) ; } } ) ; live data . remove observer ( m observer ) ; processor . on next ( __str__ ) ; assert that ( m live data output @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ; assert that ( output2 @$ is ( arrays . as list ( __str__ @$ __str__ ) ) ) ;,observer get value
group set position includes all the <PLACE_HOLDER>,final immutable bit set group set = immutable bit set . of ( group set positions ) ; list < aggregate call > aggregate calls = lists . new array list ( ) ; rel data type agg fn ret type = type converter . convert ( type info factory . long type info @$ cluster . get type factory ( ) ) ; aggregate call aggregate call = hive calcite util . create single arg agg call ( __str__ @$ cluster @$ type info factory . long type info @$ input . get row type ( ) . get field list ( ) . size ( ) @$ agg fn ret type ) ; aggregate calls . add ( aggregate call ) ; return new hive aggregate,position includes positions
verify the signature .. shows the response was generated by someone who knew the associated private <PLACE_HOLDER>,signature sig = signature . get instance ( sig alg . get object id ( ) . get id ( ) @$ __str__ ) ; sig . init verify ( pubkey ) ; sig . update ( content . get bytes ( ) ) ; return sig . verify ( sig bits ) ;,who knew key
indent:11 exp:12 warn indent:8 <PLACE_HOLDER>,return __str__ ;,exp:12 warn exp
if this <PLACE_HOLDER> is finished @$ notify the client of this @$ so the client will destroy this <PLACE_HOLDER>,if ( pages . is empty ( ) && no more pages ) { return empty results ( task instance id @$ current sequence id . get ( ) @$ true ) ; },client destroy page
skip block if block error has no instruction <PLACE_HOLDER>,continue ;,error has target
special case because xp has no <PLACE_HOLDER> for menus,if ( part == part . menu ) { if ( flat menus ) { return new xp fill border ( ui manager . get color ( __str__ ) @$ __num__ ) ; } else { return null ; } },xp has support
execute process does n't wait for finishing to drain error stream if it 's configure not to redirect stream . this causes test failure when draining the error stream did n't finish fast enough before the thread of this test case method checks the warn msg <PLACE_HOLDER> . so @$ this loop wait for a while until the log msg <PLACE_HOLDER> becomes expected number,final int expected warning messages = __num__ ; final int max retry = __num__ ; for ( int i = __num__ ; i < max retry && ( runner . get logger ( ) . get warn messages ( ) . size ( ) < expected warning messages ) ; i ++ ) { try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { } } final list < log message > warn messages = runner . get logger ( ) . get warn messages ( ) ; assert equals ( __str__ + __str__ @$ expected warning messages @$ warn messages . size ( ) ) ; final list < mock flow file > succeeded = runner . get flow files,thread checks count
3 rd parameter has no <PLACE_HOLDER> for ownerless,test ( false @$ true @$ false ) ;,parameter has affect
base condition 2 : check if we are already at the target . if so @$ return an empty <PLACE_HOLDER> .,if ( ( src node . equals ( dest node ) ) && dest dataset descriptor . contains ( src dataset descriptor ) ) { return new array list < > ( ) ; } linked list < flow edge context > edge queue = new linked list < > ( ) ; edge queue . add all ( get next edges ( src node @$ src dataset descriptor @$ dest dataset descriptor ) ) ; for ( flow edge context flow edge context : edge queue ) { this . path map . put ( flow edge context @$ flow edge context ) ; },condition return list
updates the port when the user changes the security <PLACE_HOLDER> . this allows us to show a reasonable default which the user can change .,m security type view . set on item selected listener ( new adapter view . on item selected listener ( ) { @ override public void on item selected ( adapter view < ? > parent @$ view view @$ int position @$ long id ) { if ( m current security type view position != position ) { update port from security type ( ) ; validate fields ( ) ; } } @ override public void on nothing selected ( adapter view < ? > parent ) { } } ) ;,user changes type
background uses total duration ms locked @$ while total uses total <PLACE_HOLDER> locked,dump timer ( proto @$ uid proto . sync . total @$ timer @$ raw realtime us @$ which ) ; dump timer ( proto @$ uid proto . sync . background @$ bg timer @$ raw realtime us @$ which ) ; proto . end ( sy token ) ;,background uses time
send event notifications saying that all our <PLACE_HOLDER> are offline . the protocol does not implement top level <PLACE_HOLDER> nor subgroups for top level groups so a simple nested loop would be enough .,if ( new status . equals ( offline status ) ) { iterator < contact group > groups iter = get server stored contact list root ( ) . subgroups ( ) ; while ( groups iter . has next ( ) ) { contact group group = groups iter . next ( ) ; iterator < contact > contacts iter = group . contacts ( ) ; while ( contacts iter . has next ( ) ) { contact jabber impl contact = ( contact jabber impl ) contacts iter . next ( ) ; update contact status ( contact @$ offline status ) ; } } iterator < contact > contacts iter = get server stored contact list root ( ) . contacts ( ) ;,protocol implement buddies
the kings hand should not pass any <PLACE_HOLDER> before he received one,verify zero interactions ( observer ) ;,hand pass interceptor
if the java library does n't generate any <PLACE_HOLDER> @$ it does n't contribute a gwt module,if ( java library . get source path to output ( ) == null ) { return rule deps ; } build rule gwt module = graph builder . compute if absent ( java library . get build target ( ) . assert unflavored ( ) . with flavors ( java library . gwt_module_flavor ) @$ gwt module target -> { immutable sorted set < source path > files for gwt module = immutable sorted set . < source path > natural order ( ) . add all ( java library . get sources ( ) ) . add all ( java library . get resources ( ) ) . build ( ) ; immutable sorted set < build rule > deps = immutable sorted set . copy,library generate code
<PLACE_HOLDER> of the skip region is verified and <PLACE_HOLDER> is not skipping the verified <PLACE_HOLDER>,if ( verified position > get file pointer ( ) ) { input . seek ( verified position ) ; skip bytes ( pos - verified position ) ; } else { skip bytes ( pos - get file pointer ( ) ) ; },portion skipping part
get the super<PLACE_HOLDER> type def . ot the <PLACE_HOLDER> the <PLACE_HOLDER> scope belongs to,try { java type definition super class = get super class type definition ( ( ( class scope ) scope ) . get class declaration ( ) . get node ( ) @$ null ) ; java type definition found type def = get field type ( super class @$ image @$ accessing class ) ; if ( found type def != null ) { return found type def ; } } catch ( class cast exception ignored ) { },def ot class
metadata files have no <PLACE_HOLDER>,assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( entropy_marker ) ) ) ; assert that ( handle . get metadata handle ( ) . get file path ( ) . to string ( ) @$ not ( contains string ( resolved_marker ) ) ) ;,files have entropy
start new transaction does n't recover cache <PLACE_HOLDER> on failed channel .,try ( client transaction tx1 = client . transactions ( ) . tx start ( ) ) { fail ( ) ; } catch ( client exception expected ) { },transaction recover operations
if that is first collection we split single composite key on several keys @$ each of those composite keys contain single <PLACE_HOLDER> from collection,if ( ! contains collection ) for ( int i = __num__ ; i < collection size ; i ++ ) { final o composite key composite key = new o composite key ( first key . get keys ( ) ) ; composite keys . add ( composite key ) ; } else throw new o index exception ( __str__ ) ;,each contain element
simple equals can cause troubles here because of how equals works <PLACE_HOLDER> . between lists and sets .,return collection utils . is equal collection ( state objects @$ that . state objects ) ;,equals works themselves
the finally clause will send an <PLACE_HOLDER> .,remove decoder ( imgd ) ; if ( thread . current thread ( ) . is interrupted ( ) || ! thread . current thread ( ) . is alive ( ) ) { error all consumers ( imgd . queue @$ true ) ; } else { error all consumers ( imgd . queue @$ false ) ; },clause send error
assumption : the client has already retrieved <PLACE_HOLDER> to the given address,if ( addr . equals ( code unit address ) ) { continue ; },client retrieved access
media player 2 uses <PLACE_HOLDER> instead of application mime types .,if ( mime types . application_cea608 . equals ( mime type ) ) { media format . set string ( media format . key_mime @$ mimetype_text_cea_608 ) ; } else if ( mime types . application_cea708 . equals ( mime type ) ) { media format . set string ( media format . key_mime @$ mimetype_text_cea_708 ) ; },media uses strings
now let 's visit the <PLACE_HOLDER> of the class,for ( property node pn : get properties ( ) ) { visitor . visit property ( pn ) ; } for ( field node fn : get fields ( ) ) { visitor . visit field ( fn ) ; } for ( constructor node cn : get declared constructors ( ) ) { visitor . visit constructor ( cn ) ; } visit methods ( visitor ) ;,'s visit properties
when data file contains more <PLACE_HOLDER> than header file,int extra columns = __num__ ; run import ( __str__ @$ bad . get absolute path ( ) @$ __str__ @$ integer . to string ( node ids . size ( ) * extra columns ) @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ string . value of ( config . array delimiter ( ) ) @$ __str__ + node header ( config ) . get absolute path ( ) + __str__ + node data ( false @$ config @$ node ids @$ true @$ charset . default charset ( ) @$ extra columns ) . get absolute path ( ) @$ __str__ @$ relationship header ( config ) . get absolute path ( ) + __str__ + relationship data ( false @$ config @$ node ids,file contains columns
fail fast in case this structure has no associated <PLACE_HOLDER> .,if ( this . template boundaries processors . length == __num__ ) { this . next . handle template end ( itemplate end ) ; return ; },structure has processors
if the local is just being created @$ mark the store as the <PLACE_HOLDER> of its live range . note that it might have been created by initialize variables already @$ which would have set the <PLACE_HOLDER> of the live range already .,if ( create local ) { _local . set start ( store inst ) ; } string signature = _type . to signature ( ) ;,which set start
only recreate the context if the sampler made a <PLACE_HOLDER>,if ( sampled == null && ( sampled = sampler . try sample ( request ) ) != null ) { extracted = extracted . sampled ( sampled . boolean value ( ) ) ; } return extracted . context ( ) != null ? tracer . join span ( extracted . context ( ) ) : tracer . next span ( extracted ) ;,sampler made decision
generate the <PLACE_HOLDER>s use <PLACE_HOLDER> 1 for data node,string rack1 = get rack ( __num__ @$ level ) ;,racks use rack
<PLACE_HOLDER> has n't changed ; construct old binding using <PLACE_HOLDER> from new binding,binding old bd = new binding ( new bd . get name ( ) @$ null @$ new bd . is relative ( ) ) ; naming event e = new naming event ( event src @$ naming event . object_changed @$ new bd @$ old bd @$ new long ( changeid ) ) ; support . queue event ( e @$ naming listeners ) ;,construct using object
need shut down background <PLACE_HOLDER> gracefully @$ driver.close will inform background <PLACE_HOLDER> a cancel request is sent .,if ( should run async ( ) && state != operation state . canceled && state != operation state . timedout ) { future < ? > background handle = get background handle ( ) ; if ( background handle != null ) { boolean success = background handle . cancel ( true ) ; string query id = query state . get query id ( ) ; if ( success ) { log . info ( __str__ + query id ) ; } else if ( state == operation state . canceled ) { log . info ( __str__ + query id ) ; } } } if ( driver != null ) { driver . close ( ) ; driver . destroy ( ) ; },driver.close inform handle
the decimal column vector set method will quickly copy the deserialized decimal writable <PLACE_HOLDER> .,( ( decimal column vector ) col vector ) . set ( batch index @$ deserialize read . current hive decimal writable ) ;,method copy fields
since a is in the using list @$ lr.a and rr.a are the same . this is not ambiguous . the two aliases reference the same <PLACE_HOLDER> .,query = __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . seqscan ) ; query = __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . projection @$ plan node type . orderby @$ plan node type . nestloop @$ plan node type . seqscan @$ plan node type . seqscan ) ; query = __str__ + __str__ ; compile to top down tree ( query @$ __num__ @$ plan node type . send @$ plan node type . projection @$ plan node type . orderby @$ plan node type . seqscan ) ; query = __str__ ; compile to top down tree,aliases reference schema
non existing offset will be seeking <PLACE_HOLDER> offset,new input split = new kafka input split ( tp . topic ( ) @$ tp . partition ( ) @$ existing input split . get end offset ( ) @$ existing input split . get end offset ( ) @$ existing input split . get path ( ) ) ;,non seeking end
check the v 8 has no co location <PLACE_HOLDER>,execution vertex [ ] v8s = eg . get job vertex ( v8 . getid ( ) ) . get task vertices ( ) ; for ( int i = __num__ ; i < v8 . get parallelism ( ) ; i ++ ) { assert null ( v8s [ i ] . get location constraint ( ) ) ; } e . print stack trace ( ) ; fail ( e . get message ( ) ) ;,8 has constraint
wait for monitoring interval time and verify server is still in running <PLACE_HOLDER>,resume and wait ( monitoring_interval + __num__ ) ; assert equals ( operation mode . running @$ voltdb . instance ( ) . get mode ( ) ) ;,server running mode
change offset to 1 @$ preview should show <PLACE_HOLDER> at offset 1,set offset field value ( __num__ ) ; assert equals ( __str__ @$ preview text field . get text ( ) ) ;,preview show string
if requested id contains invalid <PLACE_HOLDER> @$ then sso can not exist and would otherwise cause sso lookup to fail,try { base64 . get url decoder ( ) . decode ( id ) ; } catch ( illegal argument exception e ) { return null ; } batcher < batch > batcher = this . manager . get batcher ( ) ;,id contains characters
now make all calls to the fake group mapper throw <PLACE_HOLDER>,fake group mapping . set throw exception ( true ) ;,calls throw exceptions
prevents warning when script imports <PLACE_HOLDER> that will get compiled,options . add ( __str__ ) ;,script imports code
a volt db extension to support the assume unique <PLACE_HOLDER>,index = index . set assume unique ( c . assume unique ) ;,extension support attribute
event bus goes here . this method is only c<PLACE_HOLDER>lled if one of the binding providers provide <PLACE_HOLDER> binding for the given 'item n<PLACE_HOLDER>me ' .,if ( command instanceof on off type ) { final on off type switch command = ( on off type ) command ; final open sprinkler binding provider binding provider = find first matching binding provider ( item name @$ command ) ; final int station = binding provider . get station number ( item name ) ; if ( station < __num__ || station >= number of stations ) { logger . warn ( __str__ + station + __str__ + __num__ + __str__ + number of stations + __str__ ) ; return ; } switch ( switch command ) { case on : open sprinkler . open station ( station ) ; break ; case off : open sprinkler . close station ( station ) ; break,one provide a
synchronize on scanner read <PLACE_HOLDER>s so that nobody calculates get smallest read <PLACE_HOLDER> @$ before scanner read <PLACE_HOLDER>s is updated .,isolation level isolation level = scan . get isolation level ( ) ; long mvcc read point = package private field accessor . get mvcc read point ( scan ) ; synchronized ( scanner read points ) { if ( mvcc read point > __num__ ) { this . read pt = mvcc read point ; } else if ( nonce == h constants . no_nonce || rs services == null || rs services . get nonce manager ( ) == null ) { this . read pt = get read point ( isolation level ) ; } else { this . read pt = rs services . get nonce manager ( ) . get mvcc from operation context ( nonce group @$ nonce ) ; } scanner,synchronize read point
we expect only 1 request @$ but we ask for 2 <PLACE_HOLDER> here so that if a misbehaving client sends more than 1 <PLACE_HOLDER> @$ server call will catch it . note that disabling auto inbound flow control has no effect on unary calls .,call . request ( __num__ ) ; return new unary server call listener ( response observer @$ call ) ;,client sends request
the maximum <PLACE_HOLDER> exceeds the default max header <PLACE_HOLDER> of 10 1024,return arrays . as list ( new object [ ] [ ] { { test type . frame_max_greater_than_header_max @$ __num__ * __num__ } @$ { test type . frame_max_less_than_header_max @$ __num__ * __num__ } @$ { test type . frame_max_less_than_action_max @$ __num__ } } ) ;,size exceeds size
create threads and make them run workload <PLACE_HOLDER> .,try { workload = new workload [ num threads ] ; for ( int i = __num__ ; i < num threads ; i ++ ) { workload [ i ] = new workload ( append test util . next long ( ) @$ fs @$ i @$ number of files @$ replication @$ __num__ ) ; workload [ i ] . start ( ) ; } mod thread = new modify ( conf @$ cluster ) ; mod thread . start ( ) ; for ( int i = __num__ ; i < num threads ; i ++ ) { try { system . out . println ( __str__ + i + __str__ ) ; workload [ i ] . join ( ) ; if ( i,them run concurrently
just read it @$ the checked input stream will update the <PLACE_HOLDER> on it 's own,try ( checked input stream stream = new checked input stream ( new buffered input stream ( files . new input stream ( source file . to path ( ) ) ) @$ new adler32 ( ) ) ) { io utils . skip fully ( stream @$ source file . length ( ) ) ; return stream . get checksum ( ) . get value ( ) ; } catch ( final io exception ignored ) { },stream update checksum
if still null then the implementation does n't offer a presence operation <PLACE_HOLDER> which is unacceptable for gibberish .,if ( op set pers presence2 == null ) throw new null pointer exception ( __str__ + __str__ + __str__ ) ; contact group root group1 = op set pers presence1 . get server stored contact list root ( ) ;,implementation offer set
3 function executions took <PLACE_HOLDER>,function service stats function service stats = ids . get function stats manager ( ) . get function service stats ( ) ; wait no functions running ( function service stats ) ; no of execution calls_ aggregate += __num__ ; no of executions completed_ aggregate += __num__ ; assert equals ( no of execution calls_ aggregate @$ function service stats . get function execution calls ( ) ) ; assert equals ( no of executions completed_ aggregate @$ function service stats . get function executions completed ( ) ) ; function stats function stats = function stats manager . get function stats ( test function . test_function2 @$ ids ) ;,executions took place
if media file <PLACE_HOLDER> failed @$ let 's stop here and prompt the user,if ( m is media error ) { return upload post task result . error ; } if ( m post . get category id list ( ) . size ( ) > __num__ ) { m has category = true ; },media file upload
: a blank line matches no <PLACE_HOLDER> @$ so it can serve as a separator for readability .,if ( line . length ( ) == __num__ ) continue ;,line matches newline
if this method did not throw remote <PLACE_HOLDER> as required @$ generate the error but continue @$ so that multiple such errors can be reported .,if ( ! has remote exception ) { env . error ( __num__ @$ __str__ @$ interface def . get name ( ) @$ member . to string ( ) ) ; errors = true ; continue next member ; },method throw exceptions
checks whether the dashboard configuration contains <PLACE_HOLDER> expected fields,get dashboard configuration ( base url ) ; hs . stop ( ) ;,configuration contains all
empty <PLACE_HOLDER> empty binary <PLACE_HOLDER>,assert that ( bytes ) . contains sequence ( type_list @$ __num__ @$ __num__ @$ type_struct @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) . contains sequence ( type_list @$ __num__ @$ __num__ @$ type_struct @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ;,annotations empty annotations
bean may have acquired new weak <PLACE_HOLDER>,target = result . get node ( ) ; assert . assert equals ( count ++ @$ result . get value ( ) . int value ( ) ) ;,bean acquired affinity
player reads all <PLACE_HOLDER> in arbitrary directory structure and creates a map task for each file . we use ' ; ' as separator because wal file names contains ' @$ ',string dirs = string utils . join ( dir paths @$ __str__ ) ; string jobname = __str__ + backup id ; path bulk output path = get bulk output dir ( ) ; conf . set ( wal player . bulk_output_conf_key @$ bulk output path . to string ( ) ) ; conf . set ( wal player . input_files_separator_key @$ __str__ ) ; conf . set boolean ( wal player . multi_tables_support @$ true ) ; conf . set ( job_name_conf_key @$ jobname ) ; string [ ] player args = { dirs @$ string utils . join ( table list @$ __str__ ) } ; try { player . set conf ( conf ) ; int result = player . run ( player args ),player reads files
param type will stay <PLACE_HOLDER> if there is no parameter,if ( param type == null ) { param type = setter . get parameter type ( __num__ ) ; },type stay same
copy all rocks db sst <PLACE_HOLDER> to local disk,string sst file list = rocks db path + __str__ + sst_file_list ; file file = new file ( sst file list ) ; list < string > sst files = file utils . read lines ( file ) ; log . debug ( __str__ @$ sst files ) ; for ( string sst file : sst files ) { dfs . copy to local ( remote db backup dir + __str__ + sst file @$ rocks db path ) ; } file utils . delete quietly ( file ) ;,rocks db files
for compatibility @$ first do the lookup and then verify the provider . this makes the difference between a nsae and a security exception if the provider does not support the <PLACE_HOLDER> .,if ( provider checked == false ) { exception ve = jce security . get verification result ( provider ) ; if ( ve != null ) { string msg = __str__ + provider . get name ( ) ; throw new security exception ( msg @$ ve ) ; } provider checked = true ; },provider support attribute
start the java spark <PLACE_HOLDER>,try ( print writer report writer = checker utils . init report file ( ) ) { spark conf conf = new spark conf ( ) . set app name ( spark integration checker . class . get name ( ) ) ; java spark context sc = new java spark context ( conf ) ; checker . print config info ( conf @$ report writer ) ; status result status = checker . run ( sc @$ report writer @$ alluxio conf ) ; checker . print result info ( result status @$ report writer ) ; report writer . flush ( ) ; system . exit ( result status . equals ( status . success ) ? __num__ : __num__ ) ; },java spark context
by design @$ the buffer must always have enough <PLACE_HOLDER> for one packet,if ( build config . debug && len > buffer . remaining ( ) ) { log . e ( tag @$ len + __str__ + buffer . remaining ( ) ) ; log . e ( tag @$ buffer . to string ( ) ) ; throw new assertion error ( __str__ ) ; } buffer . put ( b @$ off @$ len ) ; buffer . flip ( ) ; sink ( ) ; buffer . compact ( ) ;,buffer have space
the target did n't specify the <PLACE_HOLDER> for the reply .,handle drag reply ( action @$ x root @$ y root ) ;,target specify coordinates
the cache should not contain any <PLACE_HOLDER> now .,assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ; assert . assert false ( m ufs absent path cache . is absent ( new alluxiouri ( __str__ ) ) ) ;,cache contain entries
recurring plans will always have an active <PLACE_HOLDER>,if ( plan . get cycle rule ( ) . is recurring ( ) ) { return plan ; } else { final range < zoned date time > cycle = plan . cycle iterator ( ) . next ( ) ; if ( cycle . contains ( zoned date time . now ( m clock ) ) ) { return plan ; } },plans have cycle
and that local files always takes <PLACE_HOLDER> over remote files .,action lookup key action lookup key = new action lookup key ( ) { @ override public sky function name function name ( ) { return sky function name . for_testing ; } } ; sky key action key = action lookup data . create ( action lookup key @$ __num__ ) ; special artifact tree artifact = create tree artifact ( __str__ ) ; tree artifact . get path ( ) . create directory and parents ( ) ; map < path fragment @$ remote file artifact value > tree artifact metadata = new hash map < > ( ) ; tree artifact metadata . put ( path fragment . create ( __str__ ) @$ create remote file artifact value ( __str__ ) ) ; tree artifact,files takes precedence
need to deal with the case where the needed permission has more than one <PLACE_HOLDER> and the collection has individual permissions that sum up to the needed .,for ( int i = __num__ ; i < len ; i ++ ) { service permission x = ( service permission ) perms . get ( i ) ; if ( ( ( needed & x . get mask ( ) ) != __num__ ) && x . implies ignore mask ( np ) ) { effective |= x . get mask ( ) ; if ( ( effective & desired ) == desired ) return true ; needed = ( desired ^ effective ) ; } },permission has value
normal case : one size has both width and height <PLACE_HOLDER>,dimension size1 = new dimension ( __num__ @$ __num__ ) ; dimension size2 = new dimension ( __num__ @$ __num__ ) ; assert . assert equals ( __num__ @$ icon entry . get distance ( size1 @$ size2 ) @$ __num__ ) ;,size has width
ooohml provides the <PLACE_HOLDER> for disconnecting and closing cache on out of off heap memory exception,out of off heap memory listener ooohml = new disconnecting out of off heap memory listener ( ( internal distributed system ) system ) ; return basic create off heap storage ( sf @$ off heap memory size @$ ooohml ) ;,ooohml provides method
let 's do sanity <PLACE_HOLDER> ; easier to spot buggy handlers,if ( instantiator == null ) { ctxt . report bad type definition ( bean desc @$ __str__ @$ insts . get class ( ) . get name ( ) ) ; },'s do checks
make sure kill the son before kill the <PLACE_HOLDER> .,for ( int i = pid list . size ( ) - __num__ ; i >= __num__ ; i -- ) { string ppid = pid list . get ( i ) ; if ( ! is pid running ( ppid ) ) { continue ; } if ( force ) { exe whole cmd ( __str__ + ppid ) ; } else { exe whole cmd ( __str__ + ppid ) ; } },son kill executor
this statement has a return <PLACE_HOLDER> on the stack .,if ( save != null ) { class definition def = ctx . field . get class definition ( ) ; if ( ! have non local finally ) { local member lf = ctx . get local field ( id finally return value ) ; num = new integer ( lf . number ) ; asm . add ( where @$ opc_istore + save . get type code offset ( ) @$ num ) ; } else { switch ( ctx . field . get type ( ) . get return type ( ) . get type code ( ) ) { case tc_void : break ; case tc_double : case tc_long : asm . add ( where @$ opc_pop2 ) ; break ; default : asm,statement has type
kerberos if delegation token is passed from the client side @$ do not set the <PLACE_HOLDER>,if ( matcher . group ( __num__ ) . equals ignore case ( __str__ ) && ! ( conn params . get session vars ( ) . contains key ( jdbc connection params . auth_type ) && conn params . get session vars ( ) . get ( jdbc connection params . auth_type ) . equals ignore case ( jdbc connection params . auth_token ) ) && ! ( conn params . get session vars ( ) . contains key ( jdbc connection params . auth_principal ) ) ) { conn params . get session vars ( ) . put ( jdbc connection params . auth_principal @$ matcher . group ( __num__ ) ) ; },kerberos set cookie
android just does the right <PLACE_HOLDER> .,charset cs = charset . for name ( __str__ ) ; charset encoder e = cs . new encoder ( ) ; e . on malformed input ( cea ) ; e . on unmappable character ( cea ) ; byte buffer bb = byte buffer . allocate ( __num__ ) ; coder result cr = e . encode ( char buffer . wrap ( new char [ ] { __str__ } ) @$ bb @$ false ) ; assert equals ( coder result . underflow @$ cr ) ; assert equals ( __num__ @$ bb . position ( ) ) ; cr = e . encode ( char buffer . wrap ( new char [ ] { __str__ } ) @$ bb @$ false ) ; assert,android does thing
note end of write . this does not change the <PLACE_HOLDER> of the remote fs .,write operation helper . write successful ( bytes ) ;,end change state
removing this line will prevent the <PLACE_HOLDER>,max log summand = math . max ( max log summand @$ log summand ) ;,line prevent removal
create e service which should process the submitted <PLACE_HOLDER> .,final runnable srv runnable = new service executor ( msg queue ) ;,which process messages
validate our name matches an existing <PLACE_HOLDER>,file topic = get help topic ( ) ; file image file = get image file ( topic @$ name ) ; finished ( topic @$ image file . get name ( ) ) ;,name matches file
if we cant parse the listen address we have no information on how to proceed with the migration the config will handle the <PLACE_HOLDER> later,default values . put ( advertised address @$ new advertised . to string ( ) ) ;,config handle mapping
since the api request specified a unique channel section <PLACE_HOLDER> @$ the api response should return exactly one channel section . if the response does not contain a channel section @$ then the specified channel section <PLACE_HOLDER> was not found .,list < channel section > channel section list = channel section list response . get items ( ) ; if ( channel section list . is empty ( ) ) { system . out . println ( __str__ + channel section id ) ; return ; } channel section channel section = channel section list . get ( __num__ ) ;,request specified id
if the user had a <PLACE_HOLDER> highlighted and we 're updating the list @$ we want to keep the <PLACE_HOLDER> highlighted if it 's in the updated list,string prev selected element = __str__ ; if ( main . elements to add pane . get selected value ( ) != null ) prev selected element = main . elements to add pane . get selected value ( ) ; if ( main . elements to remove table . get selected row ( ) != - __num__ ) prev selected element = ( string ) main . elements to remove table . get model ( ) . get value at ( main . elements to remove table . get selected row ( ) @$ __num__ ) ; if ( main . elements to remove table . get row count ( ) > __num__ ) main . elements to remove table . remove all elements ( ) ;,user had button
if an user used <PLACE_HOLDER> for projects he perhaps just used the key value for project without a name but the code expects a name for the project . therefore we fill the name according to the project key which is the same .,for ( entry < string @$ project > entry : cfg . get projects ( ) . entry set ( ) ) { if ( entry . get value ( ) . get name ( ) == null ) { entry . get value ( ) . set name ( entry . get key ( ) ) ; } },user used defaults
recursively search given subpackages . if any packages are found @$ add <PLACE_HOLDER> to the list .,if ( ! doc classes ) { map < string @$ list < java file object > > package files = search sub packages ( sub packages @$ names @$ excluded packages ) ; for ( list < string > packs = names . to list ( ) ; packs . non empty ( ) ; packs = packs . tail ) { string package name = packs . head ; parse package classes ( package name @$ package files . get ( package name ) @$ pack trees @$ excluded packages ) ; } if ( messager . nerrors ( ) != __num__ ) return null ; docenv . notice ( __str__ ) ; javadoc enter . main ( class trees . to list ( ) .,search add them
rs and datanode may have different <PLACE_HOLDER> in local machine test,if ( ! top hosts . contains ( server . get server name ( ) . get hostname ( ) ) ) { continue ; } for ( int j = __num__ ; j < server num ; j ++ ) { server name server name = cluster . get region server ( j ) . get server name ( ) ; assert true ( servers . contains ( server name ) ) ; },rs have hosts
we put 2 photos in album 2 ; delete <PLACE_HOLDER>,assert . assert equals ( _entry res . purge ( long . value of ( __num__ ) @$ null ) @$ __num__ ) ;,photos delete them
try to move focus to the next visible stack with a running activity if this stack is not covering the entire <PLACE_HOLDER> or is on a secondary display with no home stack .,if ( next focused stack != null ) { return m root activity container . resume focused stacks top activities ( next focused stack @$ prev @$ null ) ; },stack covering screen
below we create a ldap server which will accept a client <PLACE_HOLDER> @$ authenticate it successfully ; but it will never reply to the following query <PLACE_HOLDER> . client of this ldap server is expected to get a read timeout .,final thread ldap server = new thread ( new runnable ( ) { @ override public void run ( ) { try { try ( socket client sock = server sock . accept ( ) ) { io utils . skip fully ( client sock . get input stream ( ) @$ __num__ ) ; client sock . get output stream ( ) . write ( authenticate_success_msg ) ; fin latch . await ( ) ; } } catch ( exception e ) { e . print stack trace ( ) ; } } } ) ; ldap server . start ( ) ; final ldap groups mapping mapping = new ldap groups mapping ( ) ; string ldap url = __str__ + server sock . get local,which accept socket
volt db supports <PLACE_HOLDER> with only one parameter,preconditions . check state ( aggr expr indexes . size ( ) < __num__ ) ; abstract expression aggr expr = null ; if ( ! aggr expr indexes . is empty ( ) ) { rel data type field field = fields . get ( aggr expr indexes . get ( __num__ ) ) ; aggr expr = rex converter . convert data type field ( field ) ; } else if ( expression type . aggregate_count == aggr type ) { aggr type = expression type . aggregate_count_star ; } assert ( aggr field idx < aggr row type . get field count ( ) ) ; apn . add aggregate ( aggr type @$ aggr call . is distinct ( ) @$ aggr field idx,db supports operator
the service may not exist on first app <PLACE_HOLDER> . choose sane defaults,if ( m aimsicd service == null ) { toggle attack detection menu item . set checked ( false ) ; toggle cell tracking menu item . set checked ( false ) ; } else { toggle attack detection menu item . set checked ( m aimsicd service . is monitoring cell ( ) ) ; toggle cell tracking menu item . set checked ( m aimsicd service . is tracking cell ( ) ) ; } return true ;,service exist target
management server sends back an eds <PLACE_HOLDER> containing cluster load assignments for some cluster not requested .,list < any > cluster load assignments = immutable list . of ( any . pack ( build cluster load assignment ( __str__ @$ immutable list . of ( build locality lb endpoints ( __str__ @$ __str__ @$ __str__ @$ immutable list . of ( build lb endpoint ( __str__ @$ __num__ @$ health status . healthy @$ __num__ ) ) @$ __num__ @$ __num__ ) ) @$ immutable list . < policy . drop overload > of ( ) ) ) ) ; discovery response response = build discovery response ( __str__ @$ cluster load assignments @$ xds client impl . ads_type_url_eds @$ __str__ ) ; response observer . on next ( response ) ;,server sends response
running a combiner over the result . the combiner 's accumulator would be the state we use below . however @$ combiners can not emit intermediate <PLACE_HOLDER> @$ thus we need to wait for the pending reduce fn api .,person existing person = person state . read ( ) ; if ( existing person != null ) { for ( auction new auction : c . element ( ) . get value ( ) . get all ( nexmark query util . auction_tag ) ) { new auction counter . inc ( ) ; new old output counter . inc ( ) ; c . output ( kv . of ( new auction @$ existing person ) ) ; } return ; },combiners emit data
child left tuple is now null @$ so the next check will attempt <PLACE_HOLDER> for new bucket,child left tuple = next child ;,check attempt matches
request a <PLACE_HOLDER> for am 2 @$ will reserve a <PLACE_HOLDER> on nm 1,am2 . allocate ( __str__ @$ __num__ @$ __num__ @$ new array list < > ( ) ) ; cs . handle ( new node update scheduler event ( rm . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ) ) ; response = r . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . path ( __str__ ) . accept ( media type . application_json ) . get ( client response . class ) ; assert equals ( media type . application_json_type + __str__ + jetty utils . utf_8 @$ response . get type ( ) . to string ( ) ) ; json = response . get entity,container reserve container
triggering the task should start the child case <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( plan item instance . get id ( ) ) ; assert equals ( __num__ @$ cmmn runtime service . create case instance query ( ) . count ( ) ) ; assert equals ( __str__ @$ cmmn rule . get cmmn history service ( ) . create historic variable instance query ( ) . case instance id ( case instance . get id ( ) ) . variable name ( __str__ ) . single result ( ) . get value ( ) ) ; remove all deployments ( ) ;,task start instance
else only the latest could have changed param info & we have those <PLACE_HOLDER> .,if ( latest changed param sig ) { if ( my changed param sig || my changed param info || my changed return ) { save function detail conflict ( functions @$ func_signature ) ; } } else if ( my changed param sig ) { if ( latest changed param info || latest changed return ) { save function detail conflict ( functions @$ func_signature ) ; } else { get merge my ( ) . replace function parameters ( entry @$ monitor ) ; } },only have changes
ok @$ pop jar open & make md <PLACE_HOLDER>,input stream is = null ; try { is = new file input stream ( jarpath ) ; message digest md5 = message digest . get instance ( __str__ ) ; byte [ ] buf = new byte [ __num__ ] ; int pos ; while ( ( pos = is . read ( buf ) ) > __num__ ) md5 . update ( buf @$ __num__ @$ pos ) ; return md5 . digest ( ) ; } catch ( io exception | no such algorithm exception e ) { log . err ( e ) ; } finally { try { if ( is != null ) is . close ( ) ; } catch ( io exception ignore ) { } },& make 5
you may not call the build <PLACE_HOLDER> in a subbuilder .,throw new doclet abort exception ( __str__ ) ;,the build method
searches started from a folder list activity will provide an <PLACE_HOLDER> @$ but no folder,if ( app data . get string ( extra_search_folder ) != null ) { search . add allowed folder ( app data . get string ( extra_search_folder ) ) ; },searches provide entry
otherwise @$ throw file already exists <PLACE_HOLDER> @$ which means the file owner is dead,removable = true ;,file exists exception
the touch was not on the end button so the touch highlight should cover <PLACE_HOLDER> except the end button .,return m overlay panel . get content view width px ( ) - m end button width - get divider line width ( ) ;,highlight cover everything
all <PLACE_HOLDER> input columns are repeating . generate <PLACE_HOLDER> once . lookup once . since the <PLACE_HOLDER> is repeated @$ we must use entry 0 regardless of selected in use .,if ( all key input columns repeating ) { join util . join result join result ; if ( ! join col vector . no nulls && join col vector . is null [ __num__ ] ) { join result = join util . join result . nomatch ; } else { byte [ ] key bytes = vector [ __num__ ] ; int key start = start [ __num__ ] ; int key length = length [ __num__ ] ; join result = hash multi set . contains ( key bytes @$ key start @$ key length @$ hash multi set results [ __num__ ] ) ; } if ( log . is debug enabled ( ) ) { log . debug ( class_name + __str__ +,input generate key
the child case instance has the plan item instance <PLACE_HOLDER> as callback <PLACE_HOLDER> stored . when the child case instance is finished @$ the plan item of the parent case needs to be triggered .,if ( case instance state . terminated . equals ( callback data . get new state ( ) ) || case instance state . completed . equals ( callback data . get new state ( ) ) ) { command context command context = command context util . get command context ( ) ; plan item instance entity plan item instance entity = command context util . get plan item instance entity manager ( command context ) . find by id ( callback data . get callback id ( ) ) ; if ( plan item instance entity != null ) { command context util . get agenda ( command context ) . plan trigger plan item instance operation ( plan item instance entity ) ; } },instance has id
second consider the first <PLACE_HOLDER> packet . get the value of the piggyback associated with the erased location,if ( is direct ) { int idx to write = __num__ ; do decode by piggy back ( inputs [ __num__ ] @$ tmp outputs [ __num__ ] [ idx to write ] @$ piggy back @$ erased location to fix ) ; } else { byte buffer buffer ; byte [ ] [ ] [ ] new inputs = new byte [ get sub packet size ( ) ] [ inputs [ __num__ ] . length ] [ ] ; int [ ] [ ] input offsets = new int [ get sub packet size ( ) ] [ inputs [ __num__ ] . length ] ; byte [ ] [ ] [ ] new outputs = new byte [ get sub packet size ( ),second consider subprocess
noinspection object <PLACE_HOLDER> in loop,bucket = new osb tree bucketv1 < > ( cache entry ) ; if ( item index == osb tree bucketv1 . max_page_size_bytes + __num__ ) { item index = bucket . size ( ) - __num__ ; },noinspection object allocation
make sure we do n't accidently overwrite this transformation so we 'll remove the file<PLACE_HOLDER> and object id modify the <PLACE_HOLDER> so the users sees it 's a result,if ( step meta . get step meta interface ( ) instanceof meta inject meta ) { trans meta . set filename ( null ) ; trans meta . set object id ( null ) ; string append name = __str__ + base messages . get string ( pkg @$ __str__ ) + __str__ ; if ( ! trans meta . get name ( ) . ends with ( append name ) ) { trans meta . set name ( trans meta . get name ( ) + append name ) ; } },id modify name
as last resort ask the <PLACE_HOLDER> @$ cache it and return it,internal vertex retrieve vertex = retriever . get ( vertex id ) ; cache . put ( vertex id @$ retrieve vertex ) ; return retrieve vertex ;,resort ask retry
register before completion callback the before causes this thread to wait until the reaper thread aborts our <PLACE_HOLDER>,final session implementor session = entity manager . unwrap ( session implementor . class ) ; session . get action queue ( ) . register process ( new before callback completion handler ( ) ) ; testing jta platform impl . transaction manager ( ) . commit ( ) ;,thread aborts queue
make sure platform can handle <PLACE_HOLDER>,if ( replaced value . contains ( __str__ ) ) { try { file tmp = file . create temp file ( __str__ @$ __str__ ) ; tmp . delete ( ) ; } catch ( io exception e ) { throw new invalid builds dir ( new builds dir value + __str__ ) ; } },platform handle removal
window has been removed or hidden ; no draw will now happen @$ so stop <PLACE_HOLDER> .,if ( win . m removed || ! win . m has surface || ! win . is visible by policy ( ) ) { if ( debug_screen_on ) slog . w ( tag_wm @$ __str__ + win ) ; m waiting for drawn . remove ( win ) ; } else if ( win . has drawn lw ( ) ) { if ( debug_screen_on ) slog . d ( tag_wm @$ __str__ + win ) ; m waiting for drawn . remove ( win ) ; },draw happen piping
convert multimap to a map @$ because every key should have only one <PLACE_HOLDER> .,immutable map . builder < string @$ path > builder = immutable map . builder ( ) ; for ( map . entry < string @$ path > entry : multimap . entries ( ) ) { builder . put ( entry ) ; } return builder . build ( ) ;,key have value
disable the native input method so that the other input method could get the input <PLACE_HOLDER> .,disable input method ( ) ; if ( need resetxic ) { resetxic ( ) ; need resetxic client . clear ( ) ; need resetxic = false ; },method get data
a locally generated change should always have a later <PLACE_HOLDER> than one received from a wan gateway @$ so fake a <PLACE_HOLDER> if necessary,if ( time <= stamp . get version time stamp ( ) && dsid != tag . get distributed system id ( ) ) { time = stamp . get version time stamp ( ) + __num__ ; } tag . set version time stamp ( time ) ; tag . set distributed system id ( dsid ) ;,change have time
audit sql <PLACE_HOLDER> for example wo n't extend entity sql <PLACE_HOLDER>,if ( ! ( type instanceof java . lang . reflect . parameterized type ) ) { return null ; },dao extend driver
when reader contains multiple <PLACE_HOLDER> @$ set it to indicate we are in batch mode .,boolean is batch = false ;,reader contains rows
undocumented feature alert ! microsoft compilers will sometimes put field sig <PLACE_HOLDER> in this table @$ too @$ despite that not fitting the iso standard they wrote themselves .,if ( cli sig field . is field sig ( sig blob ) ) { sig = new cli sig field ( sig blob ) ; } else { sig = new cli sig stand alone method ( sig blob ) ; },compilers put bytes
must ensure that all names do not <PLACE_HOLDER> and that variable types are resolved to this program so that they have the proper sizes,list < variable > cloned params = new array list < > ( ) ; for ( int i = __num__ ; i < new params . size ( ) ; i ++ ) { variable p = new params . get ( i ) ; if ( ! use custom storage && ( p instanceof auto parameter impl ) ) { continue ; } if ( p . is unique variable ( ) ) { throw new illegal argument exception ( __str__ ) ; } check for parameter name conflict ( p @$ new params @$ non param names ) ; cloned params . add ( get resolved variable ( p @$ false @$ ! use custom storage ) ) ; } new params = cloned params,names do conflict
api version 8 has package info @$ 10 has loaded <PLACE_HOLDER> . 9 @$ i do n't know .,class < ? > loaded apk class ; try { loaded apk class = class . for name ( __str__ ) ; } catch ( class not found exception e ) { loaded apk class = class . for name ( __str__ ) ; } res dir = find field ( loaded apk class @$ __str__ ) ; packages filed = find field ( activity thread @$ __str__ ) ; if ( build . version . sdk_int < __num__ ) { resource packages filed = find field ( activity thread @$ __str__ ) ; },10 loaded apk
json payload did not match expected <PLACE_HOLDER>,try ( response response = client . new call ( json request ) . execute ( ) ) { assert equals ( http response status . bad_request . code ( ) @$ response . code ( ) ) ; },payload match format
verify that the subprocess had no system.out or system.err <PLACE_HOLDER> .,string out string = out . to string ( ) ; string err string = err . to string ( ) ; system . err . println ( __str__ ) ; system . err . print ( out ) ; system . err . println ( __str__ ) ; system . err . print ( err ) ; system . err . println ( __str__ ) ; if ( out string . length ( ) > __num__ || err string . length ( ) > __num__ ) { throw new error ( __str__ ) ; } system . err . println ( __str__ ) ;,subprocess had chars
has to be a concurrent hash map because tests might access this <PLACE_HOLDER> concurrently via contains job,job leader services = new concurrent hash map < > ( __num__ ) ; state = job leader service . state . created ; owner address = null ; rpc service = null ; high availability services = null ; job leader listener = null ;,tests access list
the file had bad characters in the name that the filesystem does n't like . fall <PLACE_HOLDER> with null value @$ caller will get original file back as result and they can deal with io exception errors when they try to use it .,string case sensitive name = case sensitive file . get name ( ) ; return ( canonical name != null ) && canonical name . equals ignore case ( case sensitive name ) && ! canonical name . equals ( case sensitive name ) ? null : case sensitive file ;,file fall pass
the method should still be present @$ just not public @$ let 's try declared <PLACE_HOLDER>,actual method = actual test instance . get class ( ) . get declared method ( get method ( ) . get name ( ) @$ convert totccl ( get method ( ) . get parameter types ( ) ) ) ; actual method . set accessible ( true ) ;,try declared name
test parallelism in detached mode @$ should use parallelism <PLACE_HOLDER>,string [ ] parameters = { __str__ @$ __str__ @$ get test jar path ( ) } ; verify cli frontend ( get cli ( configuration ) @$ parameters @$ __num__ @$ true ) ;,parallelism use options
need to close before setting the flag since the close function itself may trigger rebalance <PLACE_HOLDER> that needs the consumer to be open still,if ( ! closed ) { close ( timeout . to millis ( ) @$ false ) ; },itself trigger action
for backward compatibility with old <PLACE_HOLDER> service impl which do n't extend region <PLACE_HOLDER> .,if ( coprocessor service . class . is assignable from ( impl class ) ) { coprocessor service cs ; cs = impl class . as subclass ( coprocessor service . class ) . get declared constructor ( ) . new instance ( ) ; return new coprocessor service backward compatiblity . region coprocessor service ( cs ) ; } else { log . error ( __str__ @$ impl class . get name ( ) @$ coprocessor host . region_coprocessor_conf_key ) ; return null ; },which extend coprocessor
pdx @$ alias @$ nested <PLACE_HOLDER>,return new object [ ] { new object [ ] { __str__ @$ true } @$ new object [ ] { __str__ @$ false } @$ new object [ ] { __str__ @$ true } @$ new object [ ] { __str__ @$ false } } ;,pdx nested queries
key which indicates the <PLACE_HOLDER> of a record,string record key = null ;,which indicates start
sometime @$ the dataset contains duplicate <PLACE_HOLDER> . if the distances are same @$ we sort by the sample index .,return d == __num__ ? index - o . index : d ;,dataset contains samples
transient status bar is not allowed if status bar is on lockscreen or status bar is expecting the navigation <PLACE_HOLDER> from the user .,if ( ( m force status bar from keyguard || status bar forces showing navigation ) && m status bar controller . is transient showing ( ) ) { m status bar controller . update visibility lw ( false @$ m last system ui flags @$ m last system ui flags ) ; },bar expecting intent
to avoid changing the output path of binaries built without a flavor @$ we 'll default to no flavor @$ which implicitly builds the default <PLACE_HOLDER> .,return immutable sorted set . of ( ) ;,which builds library
apply the changes the apply will drop the undefined at 0 <PLACE_HOLDER> @$ and param size will become 0 x 5 .,invoke ( apply action ) ; assert equals ( __str__ @$ model . get status ( ) ) ; stack = function . get stack frame ( ) ; assert equals ( __num__ @$ stack . get frame size ( ) ) ; assert equals ( __num__ @$ stack . get parameter size ( ) ) ; assert equals ( __num__ @$ stack model . get frame size ( ) ) ; assert equals ( __num__ @$ stack model . get parameter size ( ) ) ; close editor ( ) ;,apply drop flag
should not work : <PLACE_HOLDER> on custom type,group ds . max by ( __num__ ) ;,not work groups
trigger source info refresh for lazy source and check that the timeline now contains all <PLACE_HOLDER> for all windows .,test runner . run on playback thread ( ( ) -> lazy sources [ __num__ ] . set new source info ( create fake timeline ( __num__ ) @$ null ) ) ; timeline = test runner . assert timeline change blocking ( ) ; timeline asserts . assert period counts ( timeline @$ __num__ @$ __num__ ) ; timeline asserts . assert window tags ( timeline @$ __num__ @$ __num__ ) ; timeline asserts . assert window is dynamic ( timeline @$ false @$ false ) ; test runner . assert prepare and release all periods ( ) ; test runner . assert completed manifest loads ( __num__ @$ __num__ ) ; assert completed all media period loads ( timeline ) ;,timeline contains history
<PLACE_HOLDER>re must be 2 keys 42 @$ 43 registered for <PLACE_HOLDER> watermark callback all <PLACE_HOLDER> seen elements must be in <PLACE_HOLDER> priority queues but no nfa yet .,assert equals ( __num__ @$ harness . num event time timers ( ) ) ; assert equals ( __num__ @$ operator . getpq size ( __num__ ) ) ; assert equals ( __num__ @$ operator . getpq size ( __num__ ) ) ; assert true ( ! operator . has non empty shared buffer ( __num__ ) ) ; assert true ( ! operator . has non empty shared buffer ( __num__ ) ) ; harness . process watermark ( new watermark ( __num__ ) ) ; verify watermark ( harness . get output ( ) . poll ( ) @$ long . min_value ) ; verify watermark ( harness . get output ( ) . poll ( ) @$ __num__ ) ;,keys callback time
the area of the html content is absolute inside of the entire <PLACE_HOLDER> . however @$ the user is viewing the <PLACE_HOLDER> inside of a scroll pane . so @$ we want the offset of the element within the viewer @$ not the absolute position .,area . y -= view position . y ;,user viewing screen
basic internal frame ui creates an action with the same name @$ we override it as motif internal frame title pane has a title pane <PLACE_HOLDER> that shadows the title pane <PLACE_HOLDER> in basic internal frame ui @$ making supers action throw an npe for us .,if ( map != null ) { map . put ( __str__ @$ new abstract action ( ) { public void action performed ( action event e ) { title pane . show system menu ( ) ; } public boolean is enabled ( ) { return is key binding active ( ) ; } } ) ; },pane has section
set the choice to 0 x 01 if the sender should switch the 0 and 1 <PLACE_HOLDER>,if ( choice bit ^ choices . get bit ( offset @$ false ) == true ) { switch bit [ __num__ ] = __num__ ; } network . send ( resources . get other id ( ) @$ switch bit ) ;,sender switch state
if the bounds are currently frozen @$ it means that the layout size that the app sees and the bounds we clip this <PLACE_HOLDER> to might be different . in order to avoid holes @$ we simulate that we are still resizing so the app fills the hole with the resizing background .,return ( get display content ( ) . m divider controller locked . is resizing ( ) || m app token != null && ! m app token . m frozen bounds . is empty ( ) ) && ! task . in freeform windowing mode ( ) && ! is gone for layout lw ( ) ;,size clip window
the number of unenqueued bytes that the decoder thread keeps <PLACE_HOLDER> of .,int unqueued bytes = stream . get unqueued buffer bytes ( ) ;,thread keeps track
1 means asc @$ could really use <PLACE_HOLDER> here in the thrift if,sort order = collections . singleton list ( __num__ ) ;,means use order
is no inheritance on constructors @$ so reloaded superclasses can affect method <PLACE_HOLDER> in the same way .,class < ? > clazz = c . get declaring class ( ) ; reloadable type rtype = get reloadable type if has been reloaded ( clazz ) ; if ( rtype == null ) { c = as accessible constructor ( c @$ true ) ; return c . new instance ( params ) ; } else { boolean ctor changed = rtype . get live version ( ) . has constructor changed ( utils . to constructor descriptor ( c . get parameter types ( ) ) ) ; if ( ! ctor changed ) { c = as accessible constructor ( c @$ true ) ; return c . new instance ( params ) ; } as accessible constructor ( c @$ false ) ;,superclasses affect signature
null default support <PLACE_HOLDER> .,assert null ( dpm . get long support message ( admin1 ) ) ; assert null ( dpm . get short support message ( admin1 ) ) ; m context . binder . calling uid = dpm mock context . system_uid ; assert null ( dpm . get short support message for user ( admin1 @$ dpm mock context . caller_user_handle ) ) ; assert null ( dpm . get long support message for user ( admin1 @$ dpm mock context . caller_user_handle ) ) ; m mock context . binder . calling uid = dpm mock context . caller_uid ;,default support message
rpc lit can not inline repeated <PLACE_HOLDER> in wrapper,if ( parent . get binding ( ) . is rpc lit ( ) || wrapper == null ) return null ;,lit inline parameters
the application can customize its <PLACE_HOLDER> dispatching mechanism .,reactor = new nio reactor ( dispatcher ) ;,application customize coin
json does not deliver a <PLACE_HOLDER> of all columns for replica identity default,object [ ] values = new object [ columns without toasted . size ( ) < schema columns . size ( ) ? schema columns . size ( ) : columns without toasted . size ( ) ] ; final set < string > undelivered toastable columns = new hash set < > ( schema . get toastable columns for table id ( table . id ( ) ) ) ; for ( replication message . column column : columns ) { final string column name = strings . unquote identifier part ( column . get name ( ) ) ; undelivered toastable columns . remove ( column name ) ; int position = get position ( column name @$ table @$ values ) ; if ( position,json deliver list
druid itself does n't explictly handle options <PLACE_HOLDER> @$ no resource handler will authorize such <PLACE_HOLDER> . so this filter catches all options <PLACE_HOLDER> and authorizes them .,if ( http method . options . equals ( http req . get method ( ) ) ) { if ( http req . get attribute ( auth config . druid_authentication_result ) == null ) { if ( allow unauthenticated http options ) { http req . set attribute ( auth config . druid_authentication_result @$ new authentication result ( auth config . allow_all_name @$ auth config . allow_all_name @$ null @$ null ) ) ; } else { ( ( http servlet response ) response ) . send error ( http servlet response . sc_unauthorized ) ; } } http req . set attribute ( auth config . druid_authorization_checked @$ true ) ; },filter catches requests
small retry number can speed up the failed <PLACE_HOLDER> .,util . get configuration ( ) . set int ( h constants . hbase_client_retries_number @$ __num__ ) ; util . start mini cluster ( ) ;,number speed client
deletes realm so key 2 <PLACE_HOLDER> . this should work as a realm should n't be cached if initialization failed .,assert true ( realm . delete realm ( configa ) ) ; realm = realm . get instance ( configb ) ; realm . close ( ) ;,deletes realm deletes
ar link commands can also generate huge command <PLACE_HOLDER> .,if ( link target type . linker or archiver ( ) == linker or archiver . archiver ) { list < string > param file args = new array list < > ( ) ; list < string > commandline args = new array list < > ( ) ; extract arguments for static link param file ( args @$ commandline args @$ param file args ) ; return pair . of ( commandline args @$ param file args ) ; } else { list < string > param file args = new array list < > ( ) ; list < string > commandline args = new array list < > ( ) ; extract arguments for dynamic link param file ( args @$ commandline args @$,commands generate args
reconnect internet after testing network health triggered <PLACE_HOLDER>,get device ( ) . execute shell command ( __str__ ) ; get device ( ) . execute shell command ( __str__ ) ;,health triggered broadcast
this test is specifically checking loose property check <PLACE_HOLDER> .,disable strict missing property checks ( ) ; test types ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,test checking behavior
java does not recognize <PLACE_HOLDER> or \v @$ apparently .,switch ( b ) { case __num__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __num__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; case __str__ : builder . append ( __str__ ) ; break ; default : if,java recognize '
if liveness hits <PLACE_HOLDER> @$ queue is considered disconnected,liveness = heartbeat_liveness ; heartbeat at = system . current time millis ( ) + heartbeat ;,liveness hits zero
config log 4 <PLACE_HOLDER> with customized files check whether hive conf initialize log 4 <PLACE_HOLDER> correctly,config log ( hive log4j property @$ hive exec log4j property ) ;,config log j
target float value hard rouding <PLACE_HOLDER>,string [ ] [ ] rounding test cases = { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__ } @$ { __str__ @$ __str__,value rouding shard
reset the schedule and reload the latest card off the top of the stack if required . the card could have been rescheduled @$ the deck could have changed @$ or a change of note type could have <PLACE_HOLDER> to the card being deleted,if ( data != null && data . has extra ( __str__ ) ) { get col ( ) . get sched ( ) . reset ( ) ; deck task . launch deck task ( deck task . task_type_answer_card @$ m answer card handler @$ new deck task . task data ( null @$ __num__ ) ) ; } if ( request code == edit_current_card ) { if ( result code == result_ok ) { timber . i ( __str__ ) ; deck task . launch deck task ( deck task . task_type_update_fact @$ m update card handler @$ new deck task . task data ( m current card @$ true ) ) ; } else if ( result code == result_canceled && ! ( data !=,change have success
increment job execution time . this counter gets <PLACE_HOLDER> once this job will be accounted for in metrics .,long exec time = worker . get execute time ( ) ; finished jobs time . add ( exec time ) ; total execution time metric . add ( exec time ) ; max finished jobs time . set if greater ( exec time ) ; if ( job always activate ) { if ( metrics update freq > - __num__ ) update job metrics ( ) ; if ( ! remove from active ( worker ) ) cancelled jobs . remove ( worker . get job id ( ) @$ worker ) ; held jobs . remove ( worker . get job id ( ) ) ; } else { if ( ! rw lock . try read lock ( ) ) { if ( log .,counter gets updated
specified <PLACE_HOLDER> does not match the existing <PLACE_HOLDER> for the pipeline . returns channel back to the pool and throws exception to the caller .,if ( ! scheme . equals ignore case ( _scheme ) ) { ( ( request with callback ) msg ) . handle ( ) . release ( ) ; throw new illegal state exception ( string . format ( __str__ @$ _scheme @$ scheme @$ ctx . channel ( ) . remote address ( ) ) ) ; },scheme match scheme
the probe covers <PLACE_HOLDER>,probe = join probe factory . create join probe ( page ) ; for ( int join position = __num__ ; probe . advance next position ( ) ; join position ++ ) { lookup join page builder . append row ( probe @$ lookup source @$ join position ) ; } output = lookup join page builder . build ( probe ) ; assert equals ( output . get channel count ( ) @$ __num__ ) ; assert false ( output . get block ( __num__ ) instanceof dictionary block ) ; assert equals ( output . get position count ( ) @$ entries ) ; for ( int i = __num__ ; i < entries ; i ++ ) { assert equals ( output . get,probe covers everything
in general a filter can not be pushed below a windowing calculation . applying the filter before the aggregation function changes the <PLACE_HOLDER> of the windowing invocation . when the filter is on the partition by expression of the over clause it can be pushed down . for now we do n't support this .,if ( rex over . contains over ( project . get projects ( ) @$ null ) ) { return ; },function changes order
only input 2 side has <PLACE_HOLDER>,if ( input col vector1 . no nulls && ! input col vector2 . no nulls ) { if ( ( input col vector1 . is repeating ) && ( input col vector2 . is repeating ) ) { outv . is repeating = true ; output vector [ __num__ ] = vector1 [ __num__ ] | vector2 [ __num__ ] ; output is null [ __num__ ] = ( vector1 [ __num__ ] == __num__ ) && input col vector2 . is null [ __num__ ] ; } else if ( input col vector1 . is repeating && ! input col vector2 . is repeating ) { if ( batch . selected in use ) { for ( int j = __num__ ; j != n ;,side has nulls
test fs 2 contains fs <PLACE_HOLDER> .,fs1 . clear ( ) ; fs2 . clear ( ) ; fs result . clear ( ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs2 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs1 . add range ( __num__ @$ __num__ ) ; fs result = new field selection ( fs1 ) ; fs1 . intersect ( fs2 ) ; assert equals ( fs result @$ fs1 ) ;,fs contains 1
type procname client handle extension <PLACE_HOLDER>,int size = __num__ + __num__ + get proc name bytes ( ) . length + __num__ + __num__ + batch extension size + all partition extension size + partition destination size ;,client handle parameters
return the fragment noinspection simplifiable conditional <PLACE_HOLDER>,return new sentence fragment ( fragment tree @$ fragment . has assumed truth ( ) ? fragment . get assumed truth ( ) : true @$ false ) . change score ( fragment . has score ( ) ? fragment . get score ( ) : __num__ ) ;,noinspection simplifiable conditions
database will result in a log.wtf call that will crash this process on eng builds . to allow the <PLACE_HOLDER> to run through to completion skip this <PLACE_HOLDER> on eng builds .,if ( build . is_eng ) { return ; } account account = new account ( __str__ @$ __str__ ) ; long id = m accounts db . insert ce account ( account @$ __str__ ) ; assert equals ( __str__ @$ - __num__ @$ id ) ;,builds skip process
tm ca n't know this application <PLACE_HOLDER> @$ so it needs cancel .,uba . cancel ( ) ;,tm know state
if parent provides a <PLACE_HOLDER> @$ do n't measure unlimited .,m layout state . m infinite = resolve is infinite ( ) ; m layout state . m extra = get extra layout space ( state ) ; m layout state . m layout direction = layout direction ; int scrolling offset ; if ( layout direction == layout state . layout_end ) { m layout state . m extra += m orientation helper . get end padding ( ) ; final view child = get child closest to end ( ) ; m layout state . m item direction = m should reverse layout ? layout state . item_direction_head : layout state . item_direction_tail ; m layout state . m current position = get position ( child ) + m layout state . m item direction ;,parent provides hint
2 nd tx should put <PLACE_HOLDER> back into existence,check result ( new index query ( def store @$ predicate condition . of ( text @$ text . contains @$ __str__ ) ) @$ def doc ) ;,tx put param
if file has reached max <PLACE_HOLDER> defined size . close current file and open a new file .,if ( ! meta . is file name in field ( ) && ( get lines output ( ) > __num__ ) && ( data . split every > __num__ ) && ( ( get lines output ( ) + meta . get footer shift ( ) ) % data . split every ) == __num__ ) { if ( meta . is footer enabled ( ) ) { write header ( ) ; } close file ( filename ) ; data . splitnr ++ ; data . fos = null ; data . out = null ; data . writer = null ; filename = get output file name ( null ) ; is write header = is write header ( filename ) ; init file stream,file reached user
if the tests are run on windows @$ the expected paths need to be adjusted . on platforms that use the unix convention @$ the following does not actually change the test <PLACE_HOLDER> .,for ( int i = __num__ ; i < data . length ; ++ i ) { if ( data [ i ] != null ) { file file = new file ( data [ i ] ) ; if ( data [ i ] . starts with ( __str__ ) ) { file = file . get absolute file ( ) ; } data [ i ] = file . get path ( ) ; } } for ( int i = __num__ ; i < data . length ; i += __num__ ) { build image configuration config = new build image configuration . builder ( ) . context dir ( data [ i ] ) . docker file ( data [ i + __num__ ],following change rules
the process will throw an error <PLACE_HOLDER> @$ which is caught and escalated by a user org.flowable.task.service.task,assert equals ( __str__ @$ __num__ @$ task service . create task query ( ) . task definition key ( __str__ ) . count ( ) ) ;,process throw event
stop the timers because they 're implicitly globally referenced and thus do n't let them retain this <PLACE_HOLDER> .,typing timer . stop ( ) ; typing timer . remove action listener ( this ) ; stopped typing timer . stop ( ) ; stopped typing timer . remove action listener ( this ) ; if ( typing state != operation set typing notifications . state_stopped ) stop typing timer ( ) ; if ( outdated resource timer != null ) { outdated resource timer . cancel ( ) ; outdated resource timer . purge ( ) ; outdated resource timer = null ; } editor pane . remove key listener ( this ) ; menu listeners . clear ( ) ; if ( right button menu != null ) { right button menu . dispose ( ) ; right button menu = null ; } scroll,them retain character
asynchronous release the <PLACE_HOLDER> @$ but still is single thread,if ( _buffer . get cursor ( ) > _consumer . get ( ) ) consume batch when available ( handler ) ;,asynchronous release buffer
req.get remote host returns ip <PLACE_HOLDER> @$ try to resolve hostname to be consistent with raw protocol .,try { final inet address client address = inet address . get by name ( client host name ) ; client host name = client address . get host name ( ) ; } catch ( unknown host exception e ) { logger . info ( __str__ @$ client host name @$ e . get message ( ) ) ; },returns ip address
set sign data and set keep mapping data must before read xml <PLACE_HOLDER> or it will read,read xml config ( config ) ; this . m7zip path = sevenzip path ; this . m zipalign path = zip align path ;,data read config
hive does not support result set meta <PLACE_HOLDER> on prepared statement @$ and hive describe does not support queries @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,describe support data
options expect activity <PLACE_HOLDER>,wait for latch ( latch ) ; verify ( m mock account manager response ) . on result ( m bundle captor . capture ( ) ) ; bundle result = m bundle captor . get value ( ) ;,options expect launch
on error @$ report failure to container and signal abort notify <PLACE_HOLDER> of failed localization,container id c id = context . get container id ( ) ; dispatcher . get event handler ( ) . handle ( new container resource failed event ( c id @$ null @$ exception . get message ( ) ) ) ;,abort notify listener
create threads and make them run <PLACE_HOLDER> concurrently .,thread thread id [ ] = new thread [ num_threads ] ; for ( int i = __num__ ; i < num_threads ; i ++ ) { transactions trans = new transactions ( namesystem @$ num_transactions @$ i * num_transactions ) ; thread id [ i ] = new thread ( trans @$ __str__ + i ) ; thread id [ i ] . start ( ) ; },them run transactions
the wait is necessary to have the poll function complete and propagate the <PLACE_HOLDER> from database one to two over the postgre sql back end .,synchronized ( lock ) { lock . await ( __num__ @$ time unit . milliseconds ) ; } final list < i comment > one three = database one text node . get comments ( ) ; final list < i comment > two three = database two text node . get comments ( ) ; assert equals ( one two size - __num__ @$ one three . size ( ) ) ; assert equals ( two two size - __num__ @$ two three . size ( ) ) ; assert equals ( one three @$ two three ) ;,function complete changes
note that concurrent linked queue actually supports doing this the iterator does not throw a concurrent modification <PLACE_HOLDER>,new references . remove all ( collected new references ) ; processed references . remove all ( collected processed references ) ;,iterator throw exception
for posterity : the moment this user unlocked the easter <PLACE_HOLDER>,try { settings . system . put long ( cr @$ __str__ @$ system . current time millis ( ) ) ; } catch ( runtime exception e ) { log . e ( __str__ @$ __str__ @$ e ) ; },user unlocked button
and associate <PLACE_HOLDER> with players via the special one v one methods . clear the team reference to players @$ which should orphan the <PLACE_HOLDER> . orphaning the team should delete the team .,session s = open session ( ) ; transaction tx = s . begin transaction ( ) ; soccer team team = new soccer team ( ) ; team . set name ( __str__ ) ; player player1 = new player ( ) ; player1 . set name ( __str__ ) ; team . set one vone player ( player1 ) ; player1 . set one vone team ( team ) ; s . persist ( team ) ; soccer team team2 = new soccer team ( ) ; team2 . set name ( __str__ ) ; player player2 = new player ( ) ; player2 . set name ( __str__ ) ; team2 . set one vone player ( player2 ) ; player2 . set one vone,which orphan team
foo 's runtime bind release service should now have a <PLACE_HOLDER> to the new bind,assert true ( foo du binding references . contains ( service name ) ) ;,service have reference
interface jars artifacts have proper owner <PLACE_HOLDER>,return new java compilation artifacts . builder ( ) . add runtime jars ( jars ) . add full compile time jars ( jars ) . add interface jars ( interface jars ) . build ( ) ;,artifacts have hash
make the last field occupy the remaining <PLACE_HOLDER>,last field . set width ( last width ) ;,field occupy width
check that they are referentially equal . since getting a group for a users that does n't exist creates a new string <PLACE_HOLDER> the only way that they should be referentially equal is if the cache worked and made sure we did n't go to hadoop 's script twice .,assert true ( u one . get group names ( ) == u two . get group names ( ) ) ; assert equals ( __num__ @$ ugi one . get group names ( ) . length ) ;,check creates entry
this asserts that all <PLACE_HOLDER> have wait wail the testing thread notifies all . we have to do this to guarantee that the thread pool has 10 live <PLACE_HOLDER> before we check the 'created ' meter .,try { all tasks are counted . await ( ) ; } catch ( interrupted exception e ) { interrupted . increment and get ( ) ; thread . current thread ( ) . interrupt ( ) ; },pool has tasks
not null constraint should reference a single <PLACE_HOLDER>,m constraint muk = new m constraint ( constraint name @$ constraint type @$ __num__ @$ null @$ null @$ enable validate rely @$ parent table @$ null @$ parentcd @$ null @$ null @$ parent integer index @$ constraint value ) ;,constraint reference parent
static shared libs can not declare permission <PLACE_HOLDER>,if ( ! pkg . permission groups . is empty ( ) ) { throw new package manager exception ( __str__ ) ; },libs declare groups
read the block from io engine based on the bucket entry 's offset and length @$ notice : the block will use the ref <PLACE_HOLDER> of bucket entry @$ which means if two h file block mapping to the same bucket entry @$ then all of the three will share the same ref <PLACE_HOLDER> .,if ( bucket entry . equals ( backing map . get ( key ) ) ) { cacheable cached block = io engine . read ( bucket entry ) ; if ( io engine . uses shared memory ( ) ) { cached block . retain ( ) ; } if ( update cache metrics ) { cache stats . hit ( caching @$ key . is primary ( ) @$ key . get block type ( ) ) ; cache stats . io hit ( system . nano time ( ) - start ) ; } bucket entry . access ( access count . increment and get ( ) ) ; if ( this . io error start time > __num__ ) { io error start time,block use cnt
load balancer will normally shutdown all <PLACE_HOLDER>,subchannel1 . shutdown ( ) ; subchannel2 . shutdown ( ) ;,balancer shutdown subchannels
if it is a dynamic <PLACE_HOLDER> mapping @$ we can safely assume leaf <PLACE_HOLDER> name does not have ' . ' in it validate if parent <PLACE_HOLDER> is specified @$ then parent <PLACE_HOLDER> exists and an instance of auto create enabled parent <PLACE_HOLDER>,queue mapping new mapping = validate and get auto created queue mapping ( queue manager @$ mapping @$ queue path ) ; if ( new mapping != null ) { new mappings . add ( new mapping ) ; } else { new mappings . add ( mapping ) ; },instance create queue
since each partition may have <PLACE_HOLDER> collected for different set of columns @$ we request them separately .,if ( get col stats ) { for ( partition part : ret ) { string part name = warehouse . make part name ( table . get partition keys ( ) @$ part . get values ( ) ) ; list < column statistics > part col stats list = getms ( ) . get partition column statistics ( parsed cat name @$ parsed db name @$ tbl name @$ collections . singleton list ( part name ) @$ stats setup const . get columns having stats ( part . get parameters ( ) ) @$ engine ) ; if ( part col stats list != null && ! part col stats list . is empty ( ) ) { column statistics part col stats = part,partition have statistics
start new instance of secondary and verify that a new roll edit log <PLACE_HOLDER> in spite of the fact that we had a partially failed checkpoint previously .,secondary = start secondary name node ( conf ) ; secondary . do checkpoint ( ) ;,roll edit works
ensure the specification contains all <PLACE_HOLDER> .,return specification . services ( ) . stream ( ) . collect ( to immutable map ( service info :: name @$ function . identity ( ) ) ) ;,specification contains services
no context class <PLACE_HOLDER> @$ try the current class <PLACE_HOLDER>,in = ss . get resource as stream ( cl @$ service ) ;,loader try loader
instance is cached by realm @$ so no need to keep strong <PLACE_HOLDER>,return flowable . create ( new flowable on subscribe < realm > ( ) { @ override public void subscribe ( final flowable emitter < realm > emitter ) throws exception { final realm observable realm = realm . get instance ( realm config ) ; final realm change listener < realm > listener = new realm change listener < realm > ( ) { @ override public void on change ( realm realm ) { if ( ! emitter . is cancelled ( ) ) { emitter . on next ( realm ) ; } } } ; observable realm . add change listener ( listener ) ; emitter . set disposable ( disposables . from runnable ( new runnable ( ) { @ override public void,need keep reference
false signifies that a marker message has not already been processed . generate and send <PLACE_HOLDER> .,if ( proxy != null ) { proxy . start or resume message dispatcher ( false ) ; },signifies generate them
the real value will be sent back as long as the field is marked as dirty and diffstate contains <PLACE_HOLDER> the client has,assert equals ( __str__ @$ get diff state string ( rta @$ __str__ ) ) ; assert true ( __str__ @$ is dirty ( rta ) ) ;,client has what
dynamic ser de always writes out bytes <PLACE_HOLDER>,bytes writable bw = ( bytes writable ) r ; out stream . write ( bw . get ( ) @$ __num__ @$ bw . get size ( ) ) ;,ser writes writable
make sure the component are the same if the input <PLACE_HOLDER> has the same real <PLACE_HOLDER> as the one in the task because either one of them could be the alias <PLACE_HOLDER> .,if ( objects . equals ( real activity @$ r . m activity component ) && this . intent != null ) { intent . set component ( this . intent . get component ( ) ) ; } return intent . filter equals ( this . intent ) ;,activity has activity
this tells the target entry to not set the size header field @$ to ensure that no tar <PLACE_HOLDER> accidentally extracts only a portion of the file data . if the <PLACE_HOLDER> ca n't read the correct size from the pif data @$ we want the <PLACE_HOLDER> to report that so the user can get a better tar <PLACE_HOLDER> !,pax sized = true ;,user get check
this is somewhat unsafe @$ but it 's better than outright throwing an <PLACE_HOLDER> here . returning null will just cause an <PLACE_HOLDER> down the pipeline .,return ( jc expression ) in ;,here cause exception
open a new region which uses this <PLACE_HOLDER>,table descriptor htd = table descriptor builder . new builder ( table name . value of ( this . name . get method name ( ) ) ) . set column family ( column family descriptor builder . of ( b ) ) . build ( ) ; region info hri = region info builder . new builder ( htd . get table name ( ) ) . build ( ) ; chunk creator . initialize ( mem storelab impl . chunk_size_default @$ false @$ __num__ @$ __num__ @$ __num__ @$ null ) ; final h region region = test_util . create localh region ( hri @$ htd @$ log ) ; executor service exec = executors . new fixed thread pool ( __num__ ) ;,which uses table
if this node implements the manufacturer specific command <PLACE_HOLDER> @$ we use it to get manufacturer info .,if ( manufacturer specific != null ) { logger . debug ( __str__ @$ node . get node id ( ) ) ; add to queue ( manufacturer specific . get manufacturer specific message ( ) ) ; },node implements command
read repair get operation produces different <PLACE_HOLDER> for same entries loaded via read through feature .,if ( context ( ) . read through ( ) ) { throw new unsupported operation exception ( __str__ ) ; },operation produces values
it 's important to use the page transition from the suggestion or we might end up saving generated ur ls as typed ur ls @$ which would then pollute the subsequent omnibox <PLACE_HOLDER> . there is one special case where the suggestion text was pasted @$ where we want the transition type to be link .,int transition = suggestion match . get type ( ) == omnibox suggestion type . url_what_you_typed && m url bar . is pasted text ( ) ? page transition . link : suggestion match . get transition ( ) ; load url from omnibox match ( suggestion match url @$ transition @$ suggestion match position @$ suggestion match . get type ( ) ) ;,which pollute matches
to avoid rejected execution exception in basic directory model wait a <PLACE_HOLDER>,try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; },exception wait sec
these are all of the events listed in the javadoc for xml event . the spec only really describes <PLACE_HOLDER> of them .,while ( true ) { switch ( event ) { case xml stream constants . start_element : handle start element ( ) ; depth ++ ; break ; case xml stream constants . end_element : depth -- ; handle end element ( ) ; if ( depth == __num__ ) break outer ; break ; case xml stream constants . characters : case xml stream constants . cdata : case xml stream constants . space : handle characters ( ) ; break ; } event = stax stream reader . next ( ) ; },spec describes 11
the subscriber will flow <PLACE_HOLDER> before an entire term buffer . so @$ send until ca n't send no 'more . then start up subscriber to drain .,final int term buffer length = __num__ * __num__ ; final int num messages per term = __num__ ; final int message length = ( term buffer length / num messages per term ) - header_length ; final int max fails = __num__ ; int messages sent = __num__ ; context . publication term buffer length ( term buffer length ) ; launch ( channel ) ; for ( int i = __num__ ; i < num messages per term ; i ++ ) { int offer fails = __num__ ; while ( publication . offer ( buffer @$ __num__ @$ message length ) < __num__ ) { if ( ++ offer fails > max fails ) { break ; } system test . check interrupted status (,subscriber flow messages
check template literal has type <PLACE_HOLDER>,test types ( __str__ @$ lines ( __str__ @$ __str__ @$ __str__ ) ) ;,literal has parameters
check whether next token matches the specified <PLACE_HOLDER>,if ( matcher . matches ( ) ) { cached next index = find start index ; match successful = true ; has next = true ; } recover previous status ( ) ; return has next ;,token matches pattern
if event contains new <PLACE_HOLDER> @$ then it may mean that the delta bytes should not be applied . this is possible if the event originated locally .,if ( this . delta bytes != null && this . new value == null && this . new value bytes == null ) { process delta bytes ( old value for delta ) ; } if ( owner != null ) { owner . generate and set version tag ( this @$ reentry ) ; } else { get region ( ) . generate and set version tag ( this @$ reentry ) ; } generate new value from bytes if needed ( ) ; object v = this . new value ; if ( v == null ) { v = is local invalid ( ) ? token . local_invalid : token . invalid ; } else { get region ( ) . set region invalid,event contains value
upload service.get pending media for post will be populated only when the user exits the <PLACE_HOLDER> but if the user does n't exit the <PLACE_HOLDER> and sends the app to the background @$ a reattachment for the media within this post is needed as soon as the app comes back to foreground @$ so we get the list of progressing media for this post,if ( m editor fragment instanceof aztec editor fragment && m editor media upload listener != null ) { set < media model > uploading media in post = m edit post repository . get pending media for post ( ) ; list < media model > all uploading media in post = new array list < > ( uploading media in post ) ; for ( media model media1 : m edit post repository . get pending or in progress media uploads for post ( ) ) { boolean found = false ; for ( media model media2 : uploading media in post ) { if ( media1 . get id ( ) == media2 . get id ( ) ) { found = true ; break,user exits editor
use annotation declared <PLACE_HOLDER>,container . add endpoint ( server endpoint config . builder . create ( pong socket . class @$ __str__ ) . build ( ) ) ;,annotation declared endpoint
create a bunch of test files with random replication factors . insert <PLACE_HOLDER> into a linked list .,try { for ( int i = __num__ ; i < number of files ; i ++ ) { final int replication = append test util . next int ( num datanodes - __num__ ) + __num__ ; path test file = new path ( __str__ + i + __str__ ) ; fs data output stream stm = append test util . create file ( fs @$ test file @$ replication ) ; stm . close ( ) ; test files . add ( test file ) ; } workload = new workload [ num threads ] ; for ( int i = __num__ ; i < num threads ; i ++ ) { workload [ i ] = new workload ( cluster @$ i @$ append to,bunch insert them
expect an error dialog create 3 <PLACE_HOLDER> of the program,do create versions ( ) ; final g tree node node = get node ( program_a ) ; select node ( node ) ; final docking action if history action = get action ( __str__ ) ; run swing ( ( ) -> history action . action performed ( get domain file action context ( node ) ) ) ; version history dialog dialog = wait for dialog component ( version history dialog . class ) ; docking action if delete action = get delete action ( dialog ) ; g table table = find component ( dialog @$ g table . class ) ; run swing ( ( ) -> table . select row ( __num__ ) ) ; perform action ( delete action @$ false ),dialog create versions
wrapping the cleanup logic in an auto closable automatically suppresses additional <PLACE_HOLDER>,try ( final cleanup yarn application ignored = new cleanup yarn application ( ) ) { test . run ( ) ; },logic suppresses resources
we collapse property definitions after collapsing property references because this step can alter the parse <PLACE_HOLDER> above property references @$ invalidating the node ancestry stored with each reference .,for ( name name : global names ) { collapse declaration of name and descendants ( name @$ name . get base name ( ) @$ escaped ) ; },step alter information
open enough file descriptors that we will crash something if we leak f ds or address <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { uri uri = uri . parse ( __str__ ) ; input stream in = resolver . open input stream ( uri ) ; assert not null ( __str__ + i @$ in ) ; byte [ ] buf = new byte [ memory file provider . test_blob . length ] ; int count = in . read ( buf ) ; assert equals ( buf . length @$ count ) ; assert true ( arrays . equals ( memory file provider . test_blob @$ buf ) ) ; in . close ( ) ; },f ds entries
we do n't need to retry this operation in the case of failure as zk will remove ephemeral <PLACE_HOLDER> and we do n't wan na hang this process when closing if we can not reconnect to zk,if ( ! is closed ( ) && id != null ) { try { zoo keeper operation zopdel = ( ) -> { zookeeper . delete ( id @$ - __num__ ) ; return boolean . true ; } ; zopdel . execute ( ) ; } catch ( interrupted exception e ) { log . warn ( __str__ @$ e ) ; thread . current thread ( ) . interrupt ( ) ; } catch ( keeper exception . no node exception e ) { } catch ( keeper exception e ) { log . warn ( __str__ @$ e ) ; throw new runtime exception ( e . get message ( ) @$ e ) ; } finally { lock listener lock listener = get,zk remove nodes
make sure no partition has more than active <PLACE_HOLDER>,volt table stats = client . call procedure ( __str__ @$ __str__ @$ __num__ ) . get results ( ) [ __num__ ] ; map < string @$ integer > master counters = maps . new hash map ( ) ; while ( stats . advance row ( ) ) { string target = stats . get string ( __str__ ) ; string ttable = stats . get string ( __str__ ) ; string is master = stats . get string ( __str__ ) ; long pid = stats . get long ( __str__ ) ; string key = __str__ + target + __str__ + ttable + __str__ + pid + __str__ + is master ; integer count = master counters . get ( key ) ; if,partition has partitions
some services automatically adds <PLACE_HOLDER> from an addressbook to our roster and this <PLACE_HOLDER> are with subscription none . if such already exist @$ remove it . this is typically our own contact,if ( ! server stored contact list jabber impl . is entry displayable ( item ) ) { if ( contact != null ) { remove contact ( contact ) ; sscl callback . fire contact removed ( this @$ contact ) ; } continue ; } if ( contact != null ) { contact . set resolved ( item ) ; sscl callback . fire contact resolved ( this @$ contact ) ; } else { contact jabber impl new contact = new contact jabber impl ( item @$ sscl callback @$ true @$ true ) ; add contact ( new contact ) ; sscl callback . fire contact added ( this @$ new contact ) ; },services adds contact
minimum scale reached so do n't pan . adjust start <PLACE_HOLDER> so any expand will zoom in .,if ( scale <= min scale ( ) ) { v dist start = v dist end ; scale start = min scale ( ) ; v center start . set ( v center endx @$ v center endy ) ; v translate start . set ( v translate ) ; } else if ( pan enabled ) { float v left start = v center start . x - v translate start . x ; float v top start = v center start . y - v translate start . y ; float v left now = v left start * ( scale / scale start ) ; float v top now = v top start * ( scale / scale start ) ; v translate .,pan adjust position
execute client put <PLACE_HOLDER> from multithread client,assert that ( region . size ( ) ) . is zero ( ) ;,client put request
draw text is always relative to text view 's origin @$ this translation brings this <PLACE_HOLDER> of text back to the top left corner of the viewport,try { recording canvas . translate ( - left @$ - top ) ; layout . draw text ( recording canvas @$ block begin line @$ block end line ) ; m text render nodes [ block index ] . is dirty = false ; } finally { block display list . end recording ( ) ; block display list . set clip to bounds ( false ) ; },translation brings portion
since lambda form caches are based on soft references @$ gc can cause element <PLACE_HOLDER> .,if ( lambda form0 != lambda form1 ) { if ( nogc happened ( ) ) { system . err . println ( __str__ ) ; system . err . println ( lambda form0 ) ; system . err . println ( __str__ ) ; system . err . println ( lambda form1 ) ; throw new assertion error ( __str__ + __str__ ) ; } else { system . err . println ( __str__ ) ; } },gc cause corruption
doesnt matter <PLACE_HOLDER> node.id and voldemort.home values are for this test,props . set property ( __str__ @$ __str__ ) ; props . set property ( __str__ @$ __str__ ) ; voldemort config = new voldemort config ( props ) ; voldemort config . set rocksdb prefix keys with partition id ( this . prefix partition id ) ; this . rocks db config = new rocks db storage configuration ( voldemort config ) ; this . rocks db store = ( rocks db storage engine ) rocks db config . get store ( test utils . make store definition ( __str__ ) @$ test utils . make single node routing strategy ( ) ) ; random = new random ( ) ;,values test which
then evaluation is successful and the nodes have the expected <PLACE_HOLDER> .,assert that evaluation result ( result ) . has entry that ( a key ) . is equal to ( new string value ( __str__ ) ) ; assert that evaluation result ( result ) . has entry that ( b key ) . is equal to ( new string value ( __str__ ) ) ;,nodes have values
options processed by abstract java jaxrs server <PLACE_HOLDER>,additional properties . put ( codegen constants . impl_folder @$ __str__ ) ; additional properties . put ( bean validation features . use_beanvalidation @$ __str__ ) ; additional properties . put ( abstract javajaxrs server codegen . server_port @$ __str__ ) ;,options jaxrs codegen
the amount with which to adjust the user provided content <PLACE_HOLDER> to account for stroke and shape corners .,int content padding offset = ( int ) ( ( include corner padding ? calculate actual corner padding ( ) : __num__ ) - get parent card view calculated corner padding ( ) ) ; material card view . set ancestor content padding ( user content padding . left + content padding offset @$ user content padding . top + content padding offset @$ user content padding . right + content padding offset @$ user content padding . bottom + content padding offset ) ;,amount provided padding
returns object <PLACE_HOLDER> from document loader .,return get document loader ( parent identifier ) . query child documents ( projection @$ parent identifier ) ;,returns object reference
lots of messages may be lost when deserialize queue has n't finished init <PLACE_HOLDER>,while ( ! bstart rec ) { log . info ( __str__ ) ; boolean is finish init = true ; for ( integer task : worker tasks ) { if ( deserialize queues . get ( task ) == null ) { is finish init = false ; j storm utils . sleep ms ( __num__ ) ; break ; } } if ( is finish init ) { bstart rec = is finish init ; } } short type = message . get_type ( ) ; if ( type == task message . normal_message ) { int task = message . task ( ) ; disruptor queue queue = deserialize queues . get ( task ) ; if ( queue == null ) { log .,queue finished processing
if the value contains a decimal or grouping <PLACE_HOLDER> or some sort @$ it 's not an integer,if ( ( c == __str__ && cmm . get conversion meta ( ) . is integer ( ) ) || ( c == __str__ && cmm . get conversion meta ( ) . is integer ( ) ) ) { evaluation results . remove ( cmm ) ; stop = true ; break ; } if ( c == __str__ ) { nr dots ++ ; } if ( c == __str__ ) { nr commas ++ ; } pos ++ ;,value contains digit
fire column resize <PLACE_HOLDER> for all columns but the source of the resize action @$ since an event will fire separately for this .,list < header cell > columns = new array list < header cell > ( available cells . values ( ) ) ; columns . remove ( source ) ; send column width updates ( columns ) ; force realign column headers ( ) ;,column resize events
best effort to create a rich error <PLACE_HOLDER>,message proxy creation error = iterables . get only element ( e . get errors ( ) ) ; message cycle dependencies message = create cycle dependencies message ( locks cycle @$ proxy creation error ) ;,effort create message
wait for first job begin <PLACE_HOLDER> .,job executed latch . await ( ) ; start grid ( __num__ ) ; for ( ignite g : g . all grids ( ) ) info ( __str__ + g . cluster ( ) . local node ( ) . id ( ) + __str__ + g . cluster ( ) . local node ( ) . metrics ( ) + __str__ ) ; future . get ( ) ; assert null ( __str__ @$ fail . get ( ) ) ;,wait begin call
updates cached <PLACE_HOLDER> with the blob information .,path layer file = cache storage files . get layer file ( written layer . layer digest @$ written layer . layer diff id ) ; return cached layer . builder ( ) . set layer digest ( written layer . layer digest ) . set layer diff id ( written layer . layer diff id ) . set layer size ( written layer . layer size ) . set layer blob ( blobs . from ( layer file ) ) . build ( ) ;,updates cached layer
need to close before emiting file to the subscriber @$ because when subscriber receives <PLACE_HOLDER> in the same thread the file may be truncated,sink . close ( ) ; sink = null ; subscriber . on next ( output ) ; subscriber . on completed ( ) ; if ( sink != null ) { try { sink . close ( ) ; } catch ( io exception e ) { subscriber . on error ( e ) ; } },subscriber receives messages
let others send <PLACE_HOLDER> . unless there are miltiple oob send calls @$ there can be only one waiter @$ the responder thread . in any case @$ only one needs to be notified .,synchronized ( this ) { sending = false ; notify ( ) ; },others send ack
force transparence of this layer only . if no buffering we would clear <PLACE_HOLDER> below,if ( buffering ) { comp = g2 . get composite ( ) ; g2 . set composite ( alpha composite . clear ) ; } g2 . set color ( new color ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; g2 . fill rect ( __num__ @$ __num__ @$ bounds . width @$ bounds . height ) ; g2 . set color ( col ) ; if ( comp != null ) { g2 . set composite ( comp ) ; },buffering clear everything
user jack can not recover <PLACE_HOLDER> from journal @$ in which the root is owned by alluxio .,master test utils . create leader file system master from journal ( new test user state ( user @$ server configuration . global ( ) ) ) . close ( ) ;,jack recover master
older versions did not have a resource <PLACE_HOLDER> @$ they stuffed it into the description .,if ( origin type == origin type . resource && resource or null == null ) { resource or null = description ; } return new simple config origin ( description @$ line number != null ? line number : - __num__ @$ end line number != null ? end line number : - __num__ @$ origin type @$ url or null @$ resource or null @$ comments or null ) ;,versions have field
0 progress does n't do <PLACE_HOLDER> .,assert . assert equals ( collections . n copies ( __num__ @$ false ) @$ multithreaded executor . invoke all ( collections . n copies ( __num__ @$ ( ) -> allocation completion tracker . update progress ( allocation tree . child1 @$ __num__ ) ) ) ) ; assert . assert equals ( arrays . as list ( allocation tree . root @$ allocation tree . child1 @$ allocation tree . child1 child @$ allocation tree . child2 ) @$ allocation completion tracker . get unfinished allocations ( ) ) ;,progress do anything
for local queries returning pdx objects wrap the resultset with results collection pdx deserializer <PLACE_HOLDER> which deserializes these pdx objects .,if ( needspdx deserialization wrapper ( true ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ false ) ; },collection pdx wrapper
check:3 compare the key <PLACE_HOLDER>,for ( int i = __num__ ; i < prev count ; i ++ ) { value meta interface pre value = null ; for ( int j = __num__ ; j < rows . length ; j ++ ) { value meta interface v = rows [ j ] . search value meta ( key list . get ( j ) [ i ] ) ; if ( v == null ) { return false ; } if ( j != __num__ && v . get type ( ) != pre value . get type ( ) ) { log error ( __str__ ) ; return false ; } else { pre value = v ; } } },check:3 compare list
map the spring to the feedback bar position so that its hidden off <PLACE_HOLDER> and bounces in on tap .,float bar position = ( float ) spring util . map value from range to range ( value @$ __num__ @$ __num__ @$ m feedback bar . get height ( ) @$ __num__ ) ; m feedback bar . set translationy ( bar position ) ;,its hidden screen
spawn a new thread modify and cust id <PLACE_HOLDER> in another tx so that outer thread fails,class tx thread extends thread { @ override public void run ( ) { cache transaction manager mgr = get gemfire cache ( ) . get tx manager ( ) ; mgr . set distributed ( true ) ; mgr . begin ( ) ; cust id cust id one = new cust id ( __num__ ) ; customer customer one = new customer ( __str__ @$ __str__ ) ; region < cust id @$ customer > cust region = get cache ( ) . get region ( region name ) ; cust region . put ( cust id one @$ customer one ) ; mgr . commit ( ) ; } } tx thread tx thread = new tx thread ( ) ; tx thread . start,thread modify one
the jitter allows the <PLACE_HOLDER> to get the data at different times @$ and avoids thundering herd,if ( ! ( boolean ) topology conf . get ( config . topology_disable_loadaware_messaging ) ) { worker state . refresh load timer . schedule recurring with jitter ( __num__ @$ __num__ @$ __num__ @$ worker . this :: do refresh load ) ; } worker state . refresh connections timer . schedule recurring ( __num__ @$ ( integer ) conf . get ( config . task_refresh_poll_secs ) @$ worker state :: refresh connections ) ; worker state . reset log levels timer . schedule recurring ( __num__ @$ ( integer ) conf . get ( config . worker_log_level_reset_poll_secs ) @$ log config manager :: reset log levels ) ; worker state . refresh active timer . schedule recurring ( __num__ @$ ( integer ) conf . get,jitter allows heartbeats
listener for changes to the destination pattern entry <PLACE_HOLDER> and not a permission entry .,event context . add naming listener ( topic search base @$ __str__ @$ new search controls ( ) @$ this . new cachedldap authorization map namespace change listener ( destination type . topic @$ null ) ) ;,listener entry itself
only a ui automation can set this <PLACE_HOLDER> and now that it is finished we make sure it is reset to its default .,m user is monkey = false ;,automation set flag
first check length of journal to make sure it makes sense to even try . if there is only one journal file with acks in it we do n't need to move it since it wo n't be chained to any later logs . if the logs have n't grown since the last <PLACE_HOLDER> then we need to compact otherwise there seems to still,if ( ++ check point cycles with nogc >= get compact acks after nogc ( ) ) { if ( metadata . ack message file map . size ( ) > __num__ && ( journal log on last compaction check == journal . get current data file id ( ) || is compact acks ignores store growth ( ) ) ) { log . trace ( __str__ ) ; try { scheduler . execute ( new ack compaction runner ( ) ) ; } catch ( exception ex ) { log . warn ( __str__ @$ ex ) ; } } else { log . trace ( __str__ ) ; } check point cycles with nogc = __num__ ; } else { log . trace ( __str__ @$,compaction run compaction
if this dialog has n't got an <PLACE_HOLDER> or has been already closed @$ do n't send notification,if ( hwnd != __num__ ) { window . modal enable ( ( dialog ) target ) ; },dialog got event
count the constant method return <PLACE_HOLDER> .,if ( method propagation returnvalue ) { program class pool . classes accept ( new all method visitor ( new constant member filter ( method propagation returnvalue counter ) ) ) ; },method return values
'next ' contains the resulting <PLACE_HOLDER> after transformation in the current iteration . it should not contain null values .,final linked list < event object > next = new linked list < event object > ( ) ; for ( map . entry < integer @$ vector < transform layer > > entry : op set message transform . transform layers . entry set ( ) ) { for ( transform layer transform layer : entry . get value ( ) ) { next . clear ( ) ; while ( ! current . is empty ( ) ) { final event object event = current . remove ( ) ; switch ( event type ) { case message delivered : message delivered event transformed delivered = transform layer . message delivered ( ( message delivered event ) event ) ; if ( transformed delivered != null,'next contains data
build map of all hash <PLACE_HOLDER> note : full outer join does n't use hash <PLACE_HOLDER>,map < hash computation @$ symbol > all hash symbols = new hash map < > ( ) ; if ( node . get type ( ) == inner || node . get type ( ) == left ) { all hash symbols . put all ( left . get hash symbols ( ) ) ; } if ( node . get type ( ) == inner || node . get type ( ) == right ) { all hash symbols . put all ( right . get hash symbols ( ) ) ; } return build join node with preferred hashes ( node @$ left @$ right @$ all hash symbols @$ parent preference @$ optional . of ( left hash symbol ) @$ optional . of,join use symbols
debug mode overrides all saved <PLACE_HOLDER> so no setup needed,set acra config ( __str__ @$ anki droid app . get shared prefs ( instrumentation registry . get instrumentation ( ) . get target context ( ) ) ) ; assert array equals ( __str__ @$ app . get acra core config builder ( ) . build ( ) . logcat arguments ( ) . to array ( ) @$ new immutable list < > ( debug logcat arguments ) . to array ( ) ) ; verify debugacra preferences ( ) ;,all saved preferences
also @$ equality should work <PLACE_HOLDER>,assert equals ( result @$ double node . value of ( value ) ) ;,equality work ok
show that hql does <PLACE_HOLDER>,do in hibernate ( this :: session factory @$ session -> { query query = session . create query ( __str__ @$ foo . class ) ; list < foo > list = query . get result list ( ) ; assert equals ( __num__ @$ list . size ( ) ) ; } ) ;,hql does work
completing task will trigger completion <PLACE_HOLDER>,task service . complete ( new task . get id ( ) ) ; assert equals ( __num__ @$ task service . create task query ( ) . count ( ) ) ; assert process ended ( proc id ) ;,task trigger info
search backwards from the end of the zip file @$ searching for the eocd signature @$ which designates the <PLACE_HOLDER> of the eocd .,int eocd offset = ( int ) zip size - zip entry . endhdr ; while ( map . get int ( eocd offset ) != zip entry . endsig ) { eocd offset -- ; } long cd entries = short . to unsigned long ( map . get short ( eocd offset + zip entry . endtot ) ) ; if ( ( cd entries & __num__ ) == __num__ ) { int zip64eocd offset = eocd offset ; while ( map . get int ( zip64eocd offset ) != zip64_endsig ) { zip64eocd offset -- ; } cd entries = map . get long ( zip64eocd offset + __num__ ) ; },which designates end
this timezone did n't have any daylight <PLACE_HOLDER> prior to 1917 and this date is sometime in 1901 .,assert false ( tz . in daylight time ( new date ( - __num__ ) ) ) ; assert equals ( - __num__ @$ tz . get offset ( - __num__ ) ) ;,timezone have offset
note : the python version uses slices which return an empty <PLACE_HOLDER> when indexed beyond what the <PLACE_HOLDER> contains . since we ca n't slice out an empty sub<PLACE_HOLDER> in java @$ we must check if we 've reached the end and clear the fnames <PLACE_HOLDER> manually .,if ( cnt == fnames . size ( ) ) { fnames . clear ( ) ; } else { fnames = fnames . sub list ( cnt @$ fnames . size ( ) ) ; } m con . publish progress ( string . format ( anki droid app . get app resources ( ) . get string ( r . string . sync_media_downloaded_count ) @$ m download count ) ) ;,which return list
the sub directory should also have the old <PLACE_HOLDER>,acl entry [ ] child dir expected acl = new acl entry [ ] { acl entry ( access @$ user @$ __str__ @$ all ) @$ acl entry ( access @$ group @$ read_execute ) @$ acl entry ( default @$ user @$ all ) @$ acl entry ( default @$ user @$ __str__ @$ all ) @$ acl entry ( default @$ group @$ read_execute ) @$ acl entry ( default @$ mask @$ all ) @$ acl entry ( default @$ other @$ read_execute ) } ; acl status child dir acl = hdfs . get acl status ( child dir ) ; assert array equals ( child dir expected acl @$ child dir acl . get entries ( ) . to array ( ),directory have acls
the user may have changed <PLACE_HOLDER> .,if ( tab id == m tab model selector . get current tab id ( ) ) { request reader panel show ( state change reason . unknown ) ; },user changed tabs
told server do not hang <PLACE_HOLDER> up if new initializing cache data added in,if ( is initializing cache list ) { headers . add ( __str__ ) ; headers . add ( __str__ ) ; } if ( string utils . is blank ( probe update string ) ) { return collections . empty list ( ) ; } try { long read timeout ms = timeout + ( long ) math . round ( timeout > > __num__ ) ; http result result = agent . http post ( constants . config_controller_path + __str__ @$ headers @$ params @$ agent . get encode ( ) @$ read timeout ms ) ; if ( httpurl connection . http_ok == result . code ) { set health server ( true ) ; return parse update data id response ( result . content,server hang finger
most hebrew labels should select the <PLACE_HOLDER> to last character,if ( locale . get language ( ) . equals ( __str__ ) || locale . get language ( ) . equals ( __str__ ) ) { if ( m day label calendar . get ( calendar . day_of_week ) != calendar . saturday ) { int len = day name . length ( ) ; day label = day name . substring ( len - __num__ @$ len - __num__ ) ; } else { day label = day name . to upper case ( locale ) . substring ( __num__ @$ __num__ ) ; } },labels select lundle
done @$ no one is asking <PLACE_HOLDER> from us,if ( operation < __num__ ) { return ; },one asking anything
legacy level does n't report min frame <PLACE_HOLDER>,if ( m static info . is hardware level legacy ( ) ) { if ( candidate size . get width ( ) <= video sz . get width ( ) && candidate size . get height ( ) <= video sz . get height ( ) ) { video snapshot sz = candidate size ; } } else { long jpeg frame duration = min frame duration map . get ( candidate size ) ; assert true ( __str__ + candidate size @$ jpeg frame duration != null ) ; if ( candidate size . get width ( ) <= video sz . get width ( ) && candidate size . get height ( ) <= video sz . get height ( ) && jpeg frame duration,level report duration
this should never happen with <PLACE_HOLDER> as errors . the plus set should always contain at least the <PLACE_HOLDER> in <PLACE_HOLDER> as errors .,if ( xlint plus . is empty ( ) ) { normalized . add ( __str__ ) ; },set contain gram
truncate did not complete immediately @$ we must wait for the operation to complete and re<PLACE_HOLDER> the <PLACE_HOLDER> .,if ( ! truncated ) { wait until lease is revoked ( file system @$ path ) ; },truncate complete lease
if the list accepts the key events and the key event was a click @$ the text view gets the selected <PLACE_HOLDER> from the drop down as its content,if ( consumed && key event . is confirm key ( key code ) ) { dismiss ( ) ; },view gets item
something else caused the <PLACE_HOLDER> @$ throw it ...,if ( ! ( surface data . is null ( dst data ) || surface data . is null ( src data ) ) ) { throw e ; },something caused exception
creating a size and time <PLACE_HOLDER> does not need a separate triggering <PLACE_HOLDER> set on the appender because this <PLACE_HOLDER> registers the trigger <PLACE_HOLDER>,final size and time based rolling policy < e > size and time based rolling policy = new size and time based rolling policy < > ( ) ; size and time based rolling policy . set max file size ( new file size ( max file size . to bytes ( ) ) ) ; rolling policy = size and time based rolling policy ;,policy need policy
check if the user has this <PLACE_HOLDER> defined in the context,for ( role role : this . user name to user . get ( user . name ) . roles ) { if ( role == null ) continue ; for ( permission permitted : role . permissions ) { if ( permitted . implies ( context ) ) { return true ; } } } return false ;,user has permission
it depends ... for example @$ tasks and stacks are only visible if there children are visible but @$ window state are not visible if there parent are not visible . maybe have the container specify <PLACE_HOLDER> direction to traverse for visibility ?,for ( int i = m children . size ( ) - __num__ ; i >= __num__ ; -- i ) { final window container wc = m children . get ( i ) ; if ( wc . is visible ( ) ) { return true ; } } return false ;,direction traverse which
element has no <PLACE_HOLDER>,if ( elem . get local name ( ) == null ) { if ( fdom error handler != null ) { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { elem . get node name ( ) } ) ; modifydom error ( msg @$ dom error . severity_error @$ null @$ elem ) ; boolean continue process = fdom error handler . handle error ( fdom error ) ; if ( ! continue process ) { throw new runtime exception ( dom message formatter . format message ( dom message formatter . serializer_domain @$ __str__ @$ null ) ) ; } } } else { uri = fns binder . geturi (,element has namespace
setup server certificates and private keys . clients will receive a <PLACE_HOLDER> back to request certificates .,if ( ! client ) { set < string > key types = new hash set < string > ( ) ; for ( string enabled cipher suite : enabled cipher suites ) { if ( enabled cipher suite . equals ( native crypto . tls_empty_renegotiation_info_scsv ) ) { continue ; } string key type = cipher suite . get by name ( enabled cipher suite ) . get server key type ( ) ; if ( key type != null ) { key types . add ( key type ) ; } } for ( string key type : key types ) { try { set certificate ( ssl parameters . get key manager ( ) . choose server alias ( key type @$ null @$ this,clients receive reply
check if any of the filters disallows this <PLACE_HOLDER>,for ( fetch filter f : fetch filters ) { fetch status s = f . check filter ( uriv ) ; if ( s != fetch status . valid ) { log . debug ( __str__ + uriv + __str__ + s ) ; spider . notify listeners founduri ( uri @$ http request header . get @$ s ) ; return ; } },any disallows uri
go through the set of partition <PLACE_HOLDER> @$ and find their representatives in the values these represent the bucketed <PLACE_HOLDER>,list < bucket col > bucket cols = extract bucket cols ( rop @$ output values ) ;,these represent columns
as the generate bucket name function uses a <PLACE_HOLDER> @$ this should pretty much never happen,if ( storage . get ( bucket ) != null ) { fail ( __str__ + bucket + __str__ ) ; },function uses string
if the cookie does n't have a <PLACE_HOLDER> @$ set one . if it does @$ validate it .,if ( cookie . get path ( ) == null ) { cookie . set path ( path to cookie path ( uri . get path ( ) ) ) ; } else if ( ! http cookie . path matches ( cookie @$ uri ) ) { continue ; },cookie have path
renamed failed so lets do some checks rename the <PLACE_HOLDER> back to the original file new file doesnt exist,if ( ! rename result ) { if ( ! new file . exists ( ) ) { logger . warning ( error message . general_write_failed_new_file_doesnt_exist . get msg ( new file . get absolute path ( ) ) ) ; } if ( ! original file backup . rename to ( af . get file ( ) ) ) { logger . warning ( error message . general_write_failed_to_rename_original_backup_to_original . get msg ( original file backup . get absolute path ( ) @$ af . get file ( ) . get name ( ) ) ) ; } logger . warning ( error message . general_write_failed_to_rename_to_original_file . get msg ( af . get file ( ) . get absolute path ( ) @$ new file . get name,checks rename backup
set <PLACE_HOLDER> may wrap the given <PLACE_HOLDER> with spanned string . check the contents by casting to string .,try { tv . get text ( ) ; fail ( ) ; } catch ( illegal argument exception e ) { },text wrap text
if we are using the bean item container or another container which stores the <PLACE_HOLDER> as ids then just return the instances,if ( id instanceof calendar event ) { event = ( calendar event ) id ; } else { basic event basic event = new container calendar event ( index ) ; if ( caption property != null && item . get item property ids ( ) . contains ( caption property ) ) { basic event . set caption ( string . value of ( item . get item property ( caption property ) . get value ( ) ) ) ; } if ( description property != null && item . get item property ids ( ) . contains ( description property ) ) { basic event . set description ( string . value of ( item . get item property ( description property ) .,which stores lists
if we have processed all connected vertices and there are edges remaining @$ graph has multiple connected <PLACE_HOLDER> .,if ( edges . size ( ) > __num__ ) return null ; return sorted ;,graph has edges
if there is a channel @$ then setting the socket <PLACE_HOLDER> should not matter . if there is not a channel @$ it will take effect .,s . set so timeout ( __num__ ) ; if ( with channel ) { assert read timeout ( stm @$ __num__ ) ; } else { assert read timeout ( stm @$ __num__ ) ; } io utils . close stream ( stm ) ; io utils . close socket ( s ) ; ss . close ( ) ;,matter setting timeout
memoized should always return the same <PLACE_HOLDER>,assert same ( class nodes @$ supplier . get ( ) ) ;,memoized return object
vertical layout and a child defines <PLACE_HOLDER>,return ol instanceof vertical layout && has non relative width component ( ol ) ;,layout defines width
verify the async event listener has received the substituted <PLACE_HOLDER>,my async event listener listener = ( my async event listener ) queue . get async event listener ( ) ; final map events map = listener . get events map ( ) ; assert not null ( events map ) ; assert equals ( expected num invocations @$ events map . size ( ) ) ; for ( iterator i = events map . entry set ( ) . iterator ( ) ; i . has next ( ) ; ) { map . entry < integer @$ string > entry = ( map . entry < integer @$ string > ) i . next ( ) ; assert equals ( my gateway event substitution filter . substitution_prefix + entry . get key ( ) @$ entry,listener received values
remove any array models which lack <PLACE_HOLDER> and enums,if ( cm . is array model && ! model property . is enum && ! model property . has validation ) { model schemas to remove . put ( cm . name @$ model schema ) ; },which lack validators
poor man 's dependency <PLACE_HOLDER> through the jersey application scope .,m block master = ( ( alluxio master process ) context . get attribute ( master web server . alluxio_master_servlet_resource_key ) ) . get master ( block master . class ) ;,man dependency injection
because an unresolved address always has a host <PLACE_HOLDER> .,name resolver . resolve ( unresolved address . get host name ( ) ) . add listener ( new future listener < inet address > ( ) { @ override public void operation complete ( future < inet address > future ) throws exception { if ( future . is success ( ) ) { promise . set success ( new inet socket address ( future . get now ( ) @$ unresolved address . get port ( ) ) ) ; } else { promise . set failure ( future . cause ( ) ) ; } } } ) ;,address has name
another bookmark @$ then remove the old <PLACE_HOLDER> .,if ( shortcut != __num__ ) { cr . delete ( content_uri @$ s shortcut selection @$ new string [ ] { string . value of ( ( int ) shortcut ) } ) ; } content values values = new content values ( ) ; if ( title != null ) values . put ( title @$ title ) ; if ( folder != null ) values . put ( folder @$ folder ) ; values . put ( intent @$ intent . to uri ( __num__ ) ) ; if ( shortcut != __num__ ) values . put ( shortcut @$ ( int ) shortcut ) ; values . put ( ordering @$ ordering ) ; return cr . insert ( content_uri @$ values ) ;,bookmark remove one
log changes in upstream network signal <PLACE_HOLDER> @$ if available .,if ( network . equals ( m tethering upstream network ) && new nc . has signal strength ( ) ) { final int new signal = new nc . get signal strength ( ) ; final string prev signal = get signal strength ( prev . network capabilities ) ; m log . logf ( __str__ @$ prev signal @$ new signal ) ; } m network map . put ( network @$ new network state ( null @$ prev . link properties @$ new nc @$ network @$ null @$ null ) ) ;,changes signal type
let the session deliver the <PLACE_HOLDER>,new thread ( ) { @ override public void run ( ) { session . run ( ) ; log . debug ( __str__ ) ; synchronized ( pool ) { try { log . debug ( __str__ ) ; session . commit ( ) ; log . debug ( __str__ ) ; } catch ( jms exception e ) { log . error ( __str__ @$ e ) ; } pool . server session in use = false ; } } } . start ( ) ;,session deliver messages
set transient state to true to simulate the view is running custom view property <PLACE_HOLDER> .,m recycler view . get child at ( __num__ ) . set has transient state ( true ) ;,state running rules
javac can put <PLACE_HOLDER> in whatever order it desires . hopper does it one way and mantis another .,test2 ( method @$ variables @$ __str__ @$ __str__ @$ __str__ ) ; test ( method @$ byname @$ __str__ @$ __str__ ) ; test ( method @$ arguments @$ __str__ @$ __str__ ) ;,javac put methods
to simplify @$ offer messages first offer all the necessary fragment <PLACE_HOLDER> to satisfy deps just be lazy and perturb the buddy response here,plan . generated responses . get ( plan . generated responses . size ( ) - __num__ ) . set status ( fragment response message . unexpected_error @$ new ee exception ( __num__ ) ) ; for ( fragment response message msg : plan . generated responses ) { system . out . println ( __str__ + msg ) ; dut . offer received fragment response ( msg ) ; },messages offer responses
clear interrupt flag if execute called <PLACE_HOLDER> .,interrupted ( ) ;,flag called init
always use uncustomized version for editing . it helps caching and customized lambda forms reuse transform cache <PLACE_HOLDER> to keep a link to uncustomized version .,return new lambda form editor ( lambda form . uncustomize ( ) ) ;,forms transform resolver
endpoint should have three different <PLACE_HOLDER> in the end order of the <PLACE_HOLDER> is not important,endpoint . expected message count ( __num__ ) ; endpoint . assert is satisfied ( ) ;,endpoint have messages
if we received invalid end offset <PLACE_HOLDER> @$ we clear the known offset to refetch the last committed offset from metadata . if any end offset <PLACE_HOLDER> are invalid @$ we treat the entire set as invalid as a safety measure .,boolean end offsets are invalid = false ; for ( entry < partition id type @$ sequence offset type > entry : publishing task end offsets . entry set ( ) ) { partition id type partition = entry . get key ( ) ; sequence offset type sequence = entry . get value ( ) ; if ( sequence . equals ( get end of partition marker ( ) ) ) { log . info ( __str__ @$ task id @$ partition ) ; end offsets are invalid = true ; partition offsets . put ( partition @$ get not set marker ( ) ) ; } } if ( ! end offsets are invalid ) { for ( entry < partition id type @$ sequence offset,committed offset values
s means <PLACE_HOLDER>,if ( c == __str__ ) { result . append ( __str__ ) ; } else { result . append ( c ) ; },s means 0
assert that the input script only contains 3 <PLACE_HOLDER>,assert true ( input script . get chunks ( ) . size ( ) == __num__ ) ;,script contains chunks
component 4 is try block map <PLACE_HOLDER> or displacement .,return eh data type utilities . get component address ( get data type ( ) @$ try_block_map_ordinal @$ get mem buffer ( ) ) ;,component try pointer
otherwise tests of deprecated code generate nuisance warnings . do n't report <PLACE_HOLDER> if the current target is also deprecated @$ or if the current context is evaluating an aspect @$ as the base target would have already printed the <PLACE_HOLDER> warnings .,return ( ! for aspect && prerequisite deprecation != null && ! is same logical package ( this package @$ prerequisite package ) && this deprecation == null ) ;,aspect report these
filter will decide if the requester wants <PLACE_HOLDER> or pull requests,collection all jobs matchin filter = container filter . filter ( pipeline . mbp . get all jobs ( ) ) ;,requester wants beginning
the method should have caught the <PLACE_HOLDER> and reported it to the logger .,final list < log message > log messages = runner . get logger ( ) . get error messages ( ) ; assert false ( log messages . is empty ( ) ) ; assert equals ( __num__ @$ log messages . size ( ) ) ;,method caught exception
load read the encoding <PLACE_HOLDER> from the output stream,return block encoding . read block ( this @$ input ) ;,load read block
check to ensure mesh has <PLACE_HOLDER> and normals before generating,if ( mesh . get buffer ( type . tex coord ) != null && mesh . get buffer ( type . normal ) != null ) { generate ( geom . get mesh ( ) @$ true @$ split mirrored ) ; },mesh has texcoords
use local <PLACE_HOLDER> for embedded spark <PLACE_HOLDER> when spark.master is not found,conf . set if missing ( __str__ @$ __str__ ) ; this . inner interpreter = load spark scala interpreter ( conf ) ; this . inner interpreter . open ( ) ; sc = this . inner interpreter . get spark context ( ) ; jsc = java spark context . from spark context ( sc ) ; spark version = spark version . from version string ( sc . version ( ) ) ; if ( enable supported version check && spark version . is unsupported version ( ) ) { throw new exception ( __str__ + spark version + __str__ + __str__ ) ; } sql context = this . inner interpreter . get sql context ( ) ; spark session = this . inner,mode spark context
output call <PLACE_HOLDER>,if ( instr . get flow type ( ) . is call ( ) ) { address call addr = get call address ( instr ) ; if ( call addr != null ) { s = symbol table . get primary symbol ( call addr ) ; if ( s != null ) { buf . append ( __str__ ) ; buf . append ( s . get name ( ) ) ; buf . append ( __str__ ) ; } } },output call address
this should probably be a java.text.parse exception @$ but illegal argument exception is <PLACE_HOLDER> android 's idn expects .,if ( m - n > ( integer . max_value - delta ) / ( h + __num__ ) ) { throw new illegal argument exception ( __str__ ) ; },idn expects what
there is no support for a native boolean type that accepts values of true @$ false or unknown . using the tinyint type requires <PLACE_HOLDER> of true and false .,get default properties ( ) . set property ( environment . query_substitutions @$ __str__ ) ;,type requires values
t<PLACE_HOLDER>t that submit do<PLACE_HOLDER> n't throw np <PLACE_HOLDER>,executor service . submit ( new test event handler ( mocked server @$ event type . m_server_shutdown @$ lock @$ counter ) ) ;,submit throw c
need to catch this @$ as windows does not support the posix <PLACE_HOLDER> . this is not an error @$ however @$ and should just silently fail .,files . set posix file permissions ( file . to path ( ) @$ perms ) ;,windows support permissions
both listeners should fire and not cause the <PLACE_HOLDER> not to fire .,assert . assert equals ( __num__ @$ io exception on online listener . on online count ) ; assert . assert equals ( __num__ @$ runtime exception on online listener . on online count ) ;,listeners fire other
the following should have the same <PLACE_HOLDER> as the previous @$ since it has the same restrictions applied in reverse order .,s = open session ( ) ; s . get transaction ( ) . begin ( ) ; root criteria = s . create criteria ( order . class @$ __str__ ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ; root criteria . create criteria ( __str__ @$ __str__ @$ join type . left_outer_join ) . add ( restrictions . eq ( __str__ @$ __str__ ) ) ;,following have result
the list of relational <PLACE_HOLDER> should contain 2 or more <PLACE_HOLDER> : the first represents the discriminator the rest represent the fk,if ( relational value sources . size ( ) < __num__ ) { throw new mapping exception ( string . format ( locale . english @$ __str__ @$ plural attribute source . get attribute role ( ) . get full path ( ) ) @$ mapping document . get origin ( ) ) ; } this . discriminator source = new any discriminator source ( ) { private final hibernate type source discriminator type source = new hibernate type source impl ( jaxb many to any mapping . get meta type ( ) ) ; private final relational value source discriminator relational value source = relational value sources . get ( __num__ ) ; private final map < string @$ string > discriminator value mapping = new hash,list contain values
return the first call to a function which takes <PLACE_HOLDER> as an argument,for ( reference ref : collection utils . as iterable ( ref iter ) ) { if ( ! func . get body ( ) . contains ( ref . get from address ( ) ) ) { continue ; } if ( is valid call reference ( ref ) ) { function called func = current program . get function manager ( ) . get function at ( ref . get to address ( ) ) ; parameter [ ] params = called func . get parameters ( ) ; for ( parameter param : params ) { address addr = param . get min address ( ) ; if ( addr != null && addr . equals ( variable addr ) ) { return ref .,which takes itself
binding view parts to view <PLACE_HOLDER>,view holder . price = cell . find view by id ( r . id . title_price ) ; view holder . time = cell . find view by id ( r . id . title_time_label ) ; view holder . date = cell . find view by id ( r . id . title_date_label ) ; view holder . from address = cell . find view by id ( r . id . title_from_address ) ; view holder . to address = cell . find view by id ( r . id . title_to_address ) ; view holder . requests count = cell . find view by id ( r . id . title_requests_count ) ; view holder . pledge price = cell . find view by,parts view holder
serialized output should still contain the v.2 <PLACE_HOLDER>,byte [ ] v1 bytes = v1 adapter . encode ( v1 ) ;,output contain header
use the selected versions field to embed the original build target <PLACE_HOLDER> . we 'll use this to verify the correct node was recovered from the target graph .,target node builder . set selected versions ( immutable map . of ( build target @$ version . of ( __str__ ) ) ) ; for ( target node < ? > dep : deps ) { target node builder . add dep ( dep . get build target ( ) ) ; } return target node builder . build ( ) ;,original build node
store the found <PLACE_HOLDER> back into the intent @$ because now that we have it we never want to do this again . for example @$ if the user navigates back to this point in the history @$ we should always restart the exact same activity .,if ( a info != null ) { intent . set component ( new component name ( a info . application info . package name @$ a info . name ) ) ; if ( ! a info . process name . equals ( __str__ ) ) { if ( ( start flags & ( start_flag_debug | start_flag_native_debugging | start_flag_track_allocation ) ) != __num__ || profiler info != null ) { synchronized ( m service . m global lock ) { final message msg = pooled lambda . obtain message ( activity manager internal :: set debug flags for starting activity @$ m service . m am internal @$ a info @$ start flags @$ profiler info @$ m service . m global lock ) ; m service,the found component
client sends an ack rds <PLACE_HOLDER> .,verify ( request observer ) . on next ( eq ( build discovery request ( node @$ __str__ @$ __str__ @$ xds client impl . ads_type_url_rds @$ __str__ ) ) ) ; verify ( config watcher @$ never ( ) ) . on config changed ( any ( config update . class ) ) ; verify ( config watcher @$ never ( ) ) . on error ( any ( status . class ) ) ;,client sends request
trailers always end the <PLACE_HOLDER> even if not explicitly set .,end of stream = true ; log builder . request trailers ( trailers ) ; log builder . increase request length ( ( http data ) o ) ;,trailers end stream
enum class needed <PLACE_HOLDER> for jackson 's json creator,if ( additional properties . contains key ( serialization_library_jackson ) ) { model . imports . add ( __str__ ) ; model . imports . add ( __str__ ) ; },class needed imports
0 x 1002 cfc : p 1 and p 2 have mem <PLACE_HOLDER> @$ but p 1 has offset .,program builder1 . create offset mem reference ( __str__ @$ __str__ @$ __num__ @$ ref type . read @$ source type . user_defined @$ __num__ ) ; program builder2 . create memory reference ( __str__ @$ __str__ @$ ref type . read @$ source type . user_defined @$ __num__ ) ; program diff = new program diff ( p1 @$ p2 ) ; address set as = new address set ( addr ( __num__ ) @$ addr ( __num__ ) ) ; program diff . set filter ( new program diff filter ( program diff filter . reference_diffs ) ) ; assert equals ( as @$ program diff . get differences ( program diff . get filter ( ) @$ null ) ) ;,0 have refs
this is a user who already configured the <PLACE_HOLDER>,if ( get polling thread count ( ) != threads_default ) { return true ; },who configured thread
it is not guaranteed that the source record will have a <PLACE_HOLDER> associated with it . for @$ example @$ if this method is being called for processing a pending activity launch @$ it is possible that the activity has been removed from the <PLACE_HOLDER> after the launch was enqueued .,final task record source task = m source record . get task record ( ) ; m new task intent = source task != null ? source task . intent : null ;,record have task
the execution attempt id should make no <PLACE_HOLDER> in this case,final function < intermediate result partitionid @$ result partitionid > partitionid mapper = intermediate result partitionid -> new result partitionid ( intermediate result partitionid @$ new execution attemptid ( ) ) ; final result partition availability checker result partition availability checker = new execution graph result partition availability checker ( partitionid mapper @$ partition tracker ) ; for ( intermediate result partitionid irpid : expected availability . key set ( ) ) { assert equals ( expected availability . get ( irpid ) @$ result partition availability checker . is available ( irpid ) ) ; },id make sense
a volt db <PLACE_HOLDER> to avoid using exceptions for flow control .,if ( parse list . length == __num__ ) { return new expression or exception ( function ) ; },volt db extension
if we cant parse the advertised address we act as if it has no port specified if invalid hostname config will report the <PLACE_HOLDER>,if ( ! advertised already has port ) { try { int port = socket_address . parse ( listen value ) . get port ( ) ; if ( port >= __num__ ) { socket address new advertised = new socket address ( advertised value @$ port ) ; log . warn ( __str__ @$ port @$ listen address @$ advertised address ) ; default values . put ( advertised address @$ new advertised . to string ( ) ) ; } } catch ( runtime exception e ) { } },config report error
the server does n't compress the <PLACE_HOLDER> in the first place @$ so the client can read the <PLACE_HOLDER> .,client . send request ( find request @$ request context ) . get response ( ) ;,server compress response
rather than fight it @$ let root have an <PLACE_HOLDER>,nodes . put ( __str__ @$ root ) ; nodes . put without digest ( root zookeeper @$ root ) ; root . add child ( proc child zookeeper ) ; nodes . put ( proc zookeeper @$ proc data node ) ; proc data node . add child ( quota child zookeeper ) ; nodes . put ( quota zookeeper @$ quota data node ) ; add config node ( ) ; node data size . set ( approximate data size ( ) ) ; try { data watches = watch manager factory . create watch manager ( ) ; child watches = watch manager factory . create watch manager ( ) ; } catch ( exception e ) { log . error ( __str__ @$ e,root have effect
no local class for this descriptor @$ skip over the data for this class . like default read object with a null current object . the code will read the <PLACE_HOLDER> but discard them .,object stream field [ ] fields = current class desc . get fields no copy ( ) ; if ( fields . length > __num__ ) { input class fields ( null @$ current class @$ fields @$ sender ) ; },default read values
streams should have no <PLACE_HOLDER> .,path src = get test root path ( f sys @$ __str__ ) ; fs data output stream out = f sys . create ( src ) ; out . write char ( __str__ ) ;,streams have effect
lookup does n't find <PLACE_HOLDER>,when ( security context . get user principal ( ) ) . then return ( simple principal . of ( __str__ ) ) ; when ( clientdao . get client ( __str__ ) ) . then return ( optional . empty ( ) ) ;,lookup find principal
at this point we 've entered the threshold zone so more keys wo n't immediately trigger more <PLACE_HOLDER> .,assert equals ( __num__ @$ listener keys . size ( ) ) ;,keys trigger callbacks
after opening the link remove the current <PLACE_HOLDER> to avoid clicking on the window to gain focus to open the link again,this . current href = __str__ ;,link remove href
these two objects should not equal each <PLACE_HOLDER>,if ( o1 . equals ( o2 ) ) { throw new assertion error ( string . format ( __str__ @$ o1 @$ o2 ) ) ; } if ( o2 . equals ( o1 ) ) { throw new assertion error ( string . format ( __str__ @$ o2 @$ o1 ) ) ; } verify hash code consistency ( o1 @$ o2 ) ;,objects equal other
we do n't have the last output perform a shallow <PLACE_HOLDER>,if ( it . has next ( ) ) { stream record < out > shallow copy = record . copy ( record . get value ( ) ) ; out . collect ( shallow copy ) ; } else { out . collect ( record ) ; break ; },output perform copy
client worker require verified <PLACE_HOLDER> .,msg . verify ( get local node id ( ) ) ;,worker require message
determine which hive <PLACE_HOLDER> we will read,list < hive column handle > read columns = columns . stream ( ) . filter ( column -> column . get column type ( ) == regular ) . collect ( to immutable list ( ) ) ; list < integer > read hive column indexes = read columns . stream ( ) . map ( hive column handle :: get hive column index ) . collect ( to immutable list ( ) ) ;,which hive columns
error on ri ri throw array <PLACE_HOLDER> out of bounds exception,assert equals ( __str__ @$ f . to string ( ) ) ;,error throw index
modifying the ac ls of root directory of the snapshot should refer new acl <PLACE_HOLDER> . and old acl <PLACE_HOLDER> should be referenced by snapshot,hdfs . modify acl entries ( path @$ acl spec ) ; path snapshot path = snapshot test helper . create snapshot ( hdfs @$ path @$ snapshot name ) ; acl feature snapshot acl = fs acl base test . get acl feature ( snapshot path @$ cluster ) ; acl feature = fs acl base test . get acl feature ( path @$ cluster ) ; assert equals ( __str__ @$ __num__ @$ acl feature . get ref count ( ) ) ; list < acl entry > new acl = lists . new array list ( acl entry ( access @$ user @$ __str__ @$ all ) ) ; hdfs . modify acl entries ( path @$ new acl ) ; acl feature = fs,ls refer feature
construct a new model with the intended architecture and print summary note : this architecture is constructed with the primary intent of demonstrating use of the transfer learning api @$ secondary to what might give better <PLACE_HOLDER>,computation graph vgg16 transfer = new transfer learning . graph builder ( vgg16 ) . fine tune configuration ( fine tune conf ) . set feature extractor ( feature extraction layer ) . n out replace ( __str__ @$ __num__ @$ weight init . xavier ) . remove vertex and connections ( __str__ ) . add layer ( __str__ @$ new dense layer . builder ( ) . activation ( activation . tanh ) . n in ( __num__ ) . n out ( __num__ ) . build ( ) @$ __str__ ) . add layer ( __str__ @$ new output layer . builder ( loss functions . loss function . negativeloglikelihood ) . activation ( activation . softmax ) . n in ( __num__ ) . n,secondary give performance
if the proxied object implements the wrapper <PLACE_HOLDER> @$ then return the result of it 's is wrapper for method .,if ( wrapper . class . is assignable from ( delegate . get class ( ) ) ) { return ( ( wrapper ) unwrapp6 spy proxy ( ) ) . is wrapper for ( iface ) ; },object implements interface
what we really want to do here is use a versioned <PLACE_HOLDER> @$ however @$ this will suffice for now .,if ( default_version . equals ( get version ( ) ) ) { return get short name ( ) ; } else { return get version ( ) . to string ( ) ; },here use name
attribute change enables capability <PLACE_HOLDER>,if ( ! registered && should register ) { context . register capability ( capability . resolve ( address ) ) ; } else if ( registered && ! should register ) { context . deregister capability ( capability . resolve ( address ) . get name ( ) ) ; },change enables registration
test that inverting a transformed <PLACE_HOLDER> gives the original <PLACE_HOLDER> .,region placement maintainer . randomized matrix rm = new region placement maintainer . randomized matrix ( rows @$ cols ) ; float [ ] [ ] transformed = rm . transform ( matrix ) ; float [ ] [ ] inverted transformed = rm . invert ( transformed ) ; for ( int i = __num__ ; i < rows ; i ++ ) { for ( int j = __num__ ; j < cols ; j ++ ) { if ( matrix [ i ] [ j ] != inverted transformed [ i ] [ j ] ) { throw new runtime exception ( ) ; } } },test gives matrix
wsaw wsdl binding case will have some <PLACE_HOLDER> set on wbo,return wbo != null ? wbo . get anonymous ( ) : wsdl bound operation . anonymous . optional ;,case have variables
configure the cluster and commit the configuration as we know the successful response indicates <PLACE_HOLDER> .,configure ( configuration ) . commit ( ) ; future . complete ( null ) ;,response indicates success
without the metadata the status and health check ur ls will not be set and the status page and health check url <PLACE_HOLDER>s will not include the context <PLACE_HOLDER> so set them here,if ( string utils . has text ( management context path ) ) { instance . set health check url path ( management context path + instance . get health check url path ( ) ) ; instance . set status page url path ( management context path + instance . get status page url path ( ) ) ; },page include path
shorter value to make <PLACE_HOLDER> faster,viewer . set popup delay ( __num__ ) ;,value make computation
response account <PLACE_HOLDER>,m ams . get account by type and features ( m mock account manager response @$ null @$ account manager service test fixtures . account_features @$ __str__ ) ;,response account type
authentication ok : ' r ' | int 32 len | int 32 <PLACE_HOLDER>,assert that ( response bytes @$ is ( new byte [ ] { __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ) ) ;,| int abcdef
inject our empty <PLACE_HOLDER> into the replication queue @$ and then roll the original <PLACE_HOLDER> @$ which enqueues a new <PLACE_HOLDER> behind our empty <PLACE_HOLDER> . we must roll the <PLACE_HOLDER> here as now we use the <PLACE_HOLDER> to determine if the file being replicated currently is still opened for write @$ so just inject a new <PLACE_HOLDER> to the replication queue does not,for ( int i = __num__ ; i < num rs ; i ++ ) { h region server hrs = util1 . geth base cluster ( ) . get region server ( i ) ; replication replication service = ( replication ) hrs . get replication source service ( ) ; replication service . get replication manager ( ) . pre log roll ( empty wal paths . get ( i ) ) ; replication service . get replication manager ( ) . post log roll ( empty wal paths . get ( i ) ) ; region info region info = util1 . geth base cluster ( ) . get regions ( htable1 . get name ( ) ) . get ( __num__ ) . get,which enqueues file
signatures <PLACE_HOLDER> jca does not expose via signature . we thus have to support this .,set keymaster purpose override ( keymaster defs . km_purpose_sign ) ; return true ;,jca expose which
check failure for partitioned table where where clause can not infer <PLACE_HOLDER>,verify stmt fails ( client @$ __str__ @$ __str__ + __str__ + __str__ + __str__ ) ;,clause infer column
expr index replace missing <PLACE_HOLDER> with replace missing <PLACE_HOLDER>,byte [ ] expected cache key = new byte [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ } ;,index replace key
it is an error if document has dom l 1 <PLACE_HOLDER> .,if ( attr . get local name ( ) == null ) { if ( f namespace validation ) { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { attr . get node name ( ) } ) ; reportdom error ( f error handler @$ f error @$ f locator @$ msg @$ dom error . severity_fatal_error @$ __str__ ) ; } else { string msg = dom message formatter . format message ( dom message formatter . dom_domain @$ __str__ @$ new object [ ] { attr . get node name ( ) } ) ; reportdom error ( f error handler @$ f error @$ f locator @$ msg @$ dom,document dom node
all the stripes are expired @$ so the compactor will not create any <PLACE_HOLDER> . we need to create an empty file to preserve metadata,stripe compactor sc = create compactor ( ) ; list < path > paths = scr . execute ( sc @$ no limit throughput controller . instance @$ null ) ; assert equals ( __num__ @$ paths . size ( ) ) ;,compactor create files
print requested <PLACE_HOLDER> @$ unless verbose was selected,if ( verbose ) { zkdu . print size map ( ) ; } else { zkdu . print size map depth ( depth ) ; } zkdu . close ( ) ;,print requested size
now set the partitioning <PLACE_HOLDER> for the slave step ... for the slave step @$ we only should those partition i ds that are interesting for the current slave ...,step partitioning meta = target step partitioning meta . clone ( ) ; partition schema = step partitioning meta . get partition schema ( ) ; partition schema . set name ( create slave partition schema name ( partition schema . get name ( ) ) ) ; if ( partition schema . is dynamically defined ( ) ) { partition schema . expand partitions dynamically ( cluster schema . find nr slaves ( ) @$ original transformation ) ; },now set schema
when an unrelated notification gets a new app <PLACE_HOLDER>,m controller . update notifications for app op ( app ops manager . op_camera @$ __num__ @$ __str__ @$ true ) ;,notification gets info
default to number class in exception details @$ else use the specified number <PLACE_HOLDER> .,return cast to number ( object @$ number . class ) ;,default use type
o verride the <PLACE_HOLDER> to return our mock protocol,try { mockito . do return ( mock protocol ) . when ( spy ) . get proxy ( mockito . < configuration > any ( ) @$ mockito . any int ( ) ) ; mockito . do return ( mock zkfc protocol ) . when ( spy ) . getzkfc proxy ( mockito . < configuration > any ( ) @$ mockito . any int ( ) ) ; } catch ( io exception e ) { throw new assertion error ( e ) ; },o verride name
bg <PLACE_HOLDER> state future finished @$ let the ui thread complete <PLACE_HOLDER> . countdown happens here if test is successful . otherwise it will happen when the bg thread computes <PLACE_HOLDER> @$ which means ui thread still gets unblocked but assertion will fail .,unlockui thread layout . count down ( ) ;,thread computes state
no need to write anything on the client side @$ it will just confuse the <PLACE_HOLDER> .,client out = byte buffer . wrap ( __str__ . get bytes ( ) ) ;,need confuse string
this is the response for the first page . leave the <PLACE_HOLDER> in the cache so subsequent requests for the first page can return immediately .,if ( state tag . request position == null ) { should remove = false ; } else { should remove = true ; },page leave item
now remove the bytes we just processed . if @$ for some reason @$ the bytes array does not contain at least as many bytes as the instruction @$ then there 's a problem with the input . just print a <PLACE_HOLDER> to the user and exit .,if ( all bytes . size ( ) < instruction . get length ( ) ) { msg panel . set message text ( __str__ @$ color . red ) ; return ; } all bytes . sub list ( __num__ @$ instruction . get length ( ) ) . clear ( ) ;,now print message
not seen yet ; must add an entry @$ return it . for that @$ we need <PLACE_HOLDER>,object id generator < ? > generator = null ; if ( _object id generators == null ) { _object id generators = new array list < object id generator < ? > > ( __num__ ) ; } else { for ( int i = __num__ @$ len = _object id generators . size ( ) ; i < len ; ++ i ) { object id generator < ? > gen = _object id generators . get ( i ) ; if ( gen . can use for ( generator type ) ) { generator = gen ; break ; } } } if ( generator == null ) { generator = generator type . new for serialization ( this ) ; _object id generators .,not need none
reverse the parameter order so that the queue keeps the oldest <PLACE_HOLDER>,return b . last accessed copy < a . last accessed copy ;,queue keeps copy
the host matches a <PLACE_HOLDER> ; use its key,hash alias = alias ; key alias = alias ;,host matches hash
make sure user who do n't have read <PLACE_HOLDER> to file ca n't <PLACE_HOLDER> raw xattr .,try { dfs . getx attr ( path @$ raw1 ) ; fail ( __str__ ) ; } catch ( access control exception ace ) { },who read access
try to verify whether the previous set data <PLACE_HOLDER> or not,if ( is retry ) { try { stat stat = new stat ( ) ; byte [ ] rev data = check zk ( ) . get data ( path @$ false @$ stat ) ; if ( bytes . compare to ( rev data @$ new data ) == __num__ ) { return stat ; } } catch ( keeper exception keeper exception ) { throw keeper exception ; } },previous set changes
<PLACE_HOLDER> any interfering variables with different <PLACE_HOLDER>s and any variables that can be safely coalesced wih the same <PLACE_HOLDER> .,graph coloring < var @$ void > coloring = new greedy graph coloring < > ( interference graph @$ coloring tie breaker ) ; coloring . color ( ) ; colorings . push ( coloring ) ;,color wih color
this fragment causes each execution <PLACE_HOLDER> to create the files that will be written to during the snapshot,byte [ ] hashinator bytes = ( hashinator data != null ? hashinator data . m_ser data : null ) ; long hashinator version = ( hashinator data != null ? hashinator data . m_version : __num__ ) ; return create and execute sys proc plan ( sys proc fragment id . pf_create snapshot targets @$ sys proc fragment id . pf_create snapshot targets results @$ file path @$ file nonce @$ per partition txn ids @$ block @$ format . name ( ) @$ data @$ hashinator bytes @$ hashinator version @$ system . current time millis ( ) @$ path type ) ;,fragment causes parallelism
no output stream . just return the <PLACE_HOLDER> of the serialized trie @$ in bytes .,if ( os == null ) { return length ; },output return length
initialize a complete map reduce <PLACE_HOLDER>,try { expr node desc expr1 = new expr node column desc ( type info factory . string type info @$ f1 @$ __str__ @$ false ) ; expr node desc expr2 = new expr node column desc ( type info factory . string type info @$ f2 @$ __str__ @$ false ) ; expr node desc filter expr = type check proc factory . default expr processor . get func expr node desc ( __str__ @$ expr1 @$ expr2 ) ; filter desc filter ctx = new filter desc ( filter expr @$ false ) ; operator < filter desc > op = operator factory . get ( new compilation op context ( ) @$ filter desc . class ) ; op . set conf ( filter,map reduce operator
.split main body of client we use a lazy pirate strategy in the client . if there 's no reply within our timeout @$ we close the socket and try again . in binary star @$ it 's the client vote that decides <PLACE_HOLDER> server is primary ; the client must therefore try to connect to each server in turn :,if ( poller . pollin ( __num__ ) ) { string reply = client . recv str ( ) ; if ( integer . parse int ( reply ) == sequence ) { system . out . printf ( __str__ @$ reply ) ; expect reply = false ; thread . sleep ( __num__ ) ; } else system . out . printf ( __str__ @$ reply ) ; } else { system . out . printf ( __str__ ) ; poller . unregister ( client ) ; ctx . destroy socket ( client ) ; server nbr = ( server nbr + __num__ ) % __num__ ; thread . sleep ( settle_delay ) ; system . out . printf ( __str__ @$ server [ server nbr ],server primary which
show only what the user required but make sure we show something and the spinners have higher <PLACE_HOLDER>,if ( ! spinners shown && ! calendar view shown ) { set spinners shown ( true ) ; } else { set spinners shown ( spinners shown ) ; set calendar view shown ( calendar view shown ) ; },something have priority
if all maps are assigned @$ then ramp up all reduces <PLACE_HOLDER> of the headroom,if ( scheduled maps == __num__ && num pending reduces > __num__ ) { log . info ( __str__ + __str__ + num pending reduces ) ; schedule all reduces ( ) ; return ; } float completed map percent = __num__ ; if ( total maps != __num__ ) { completed map percent = ( float ) completed maps / total maps ; } else { completed map percent = __num__ ; },ramp reduces part
lock acquired @$ current thread owns this instance <PLACE_HOLDER>,try { switch ( state ) { case ready : return instance ; case injecting : return instance ; case validated : state = injectable reference state . injecting ; break ; case new : throw new illegal state exception ( __str__ ) ; default : throw new illegal state exception ( __str__ + state ) ; } try { members injector . inject and notify ( instance @$ key @$ provision callback @$ source @$ injector . options . stage == stage . tool ) ; } catch ( internal provision exception ipe ) { throw ipe . add source ( source ) ; } state = injectable reference state . ready ; return instance ; } finally { lock . unlock ( ) ; },thread owns pool
implementation could have started <PLACE_HOLDER> when <PLACE_HOLDER> was enabled @$ in which case size can be 0,if ( size < __num__ ) { throw new runtime exception ( __str__ + size ) ; } doit ( ) ;,implementation started sampling
search left <PLACE_HOLDER> for first view,while ( left view offset > left border && left view position >= __num__ ) { if ( m is collapsed ) { left view offset -= ( m settings . get view width px ( ) + math . abs ( overlap distance ) ) ; } else { left view offset -= ( m settings . get view width px ( ) - math . abs ( overlap distance ) ) ; } left view position -- ; },search left point
for root elements skip serializer <PLACE_HOLDER>,if ( ! is nested ) container . offset ++ ; if ( serializer . get current serializer ( ) . is serializing class name by default ( ) || is nested ) { read string ( container ) ; } int len = __num__ ; while ( len != __num__ ) { len = o var int serializer . read as integer ( container ) ; if ( len > __num__ ) { container . offset += len ; int pointer = read integer ( container ) ; pointer -= step size ; o integer serializer . instance . serialize literal ( pointer @$ container . bytes @$ container . offset - o integer serializer . int_size ) ; container . offset ++ ; } else if,elements skip information
rare averaging improves <PLACE_HOLDER> @$ but might reduce model accuracy,parallel wrapper wrapper = new parallel wrapper . builder ( net ) . prefetch buffer ( __num__ ) . workers ( __num__ ) . averaging frequency ( __num__ ) . report score after averaging ( true ) . build ( ) ;,averaging improves performance
there are no regions @$ or no samples available . just scan the entire <PLACE_HOLDER> .,if ( sample row keys . is empty ( ) ) { log . info ( __str__ @$ this ) ; return collections . singleton list ( this ) ; } log . info ( __str__ @$ desired bundle size bytes @$ sample row keys . size ( ) @$ sample row keys . get ( __num__ ) ) ; immutable list . builder < bigtable source > splits = immutable list . builder ( ) ; for ( byte key range range : ranges ) { splits . add all ( split range based on samples ( desired bundle size bytes @$ sample row keys @$ range ) ) ; } return splits . build ( ) ;,samples scan table
assuming side fields are preserving its <PLACE_HOLDER>,for ( string field : origin fields ) { if ( side fields . contains ( field ) ) { ret . add ( side values . get ( side idx ++ ) ) ; } else { ret . add ( state . group . get ( join fields . field index ( field ) ) ) ; } },fields preserving index
internal or empty means internal <PLACE_HOLDER> @$ neither or them means sdcard <PLACE_HOLDER>,if ( ! __str__ . equals ignore case ( lite pal attr . get storage ( ) ) && ! text utils . is empty ( lite pal attr . get storage ( ) ) ) { string db path = environment . get external storage directory ( ) . get path ( ) + __str__ + lite pal attr . get storage ( ) ; db path = db path . replace ( __str__ @$ __str__ ) ; if ( base utility . is class and method exist ( __str__ @$ __str__ ) && context compat . check self permission ( lite pal application . get context ( ) @$ manifest . permission . write_external_storage ) != package manager . permission_granted ) { throw new database generate,means means storage
then the listeners receive separate <PLACE_HOLDER> of the bubble clock plugin .,argument captor < clock plugin > captor1 = argument captor . for class ( clock plugin . class ) ; argument captor < clock plugin > captor2 = argument captor . for class ( clock plugin . class ) ; verify ( m mock listener1 ) . on clock changed ( captor1 . capture ( ) ) ; verify ( m mock listener2 ) . on clock changed ( captor2 . capture ( ) ) ; assert that ( captor1 . get value ( ) ) . is instance of ( bubble_clock_class ) ; assert that ( captor2 . get value ( ) ) . is instance of ( bubble_clock_class ) ; assert that ( captor1 . get value ( ) ) . is not same as (,listeners receive instances
rightmost branch determines the <PLACE_HOLDER> .,return immutable list . of ( __str__ @$ __str__ ) ;,branch determines order
the job should wait more than one <PLACE_HOLDER>,if ( cur priority > executable . get default priority ( ) + __num__ ) { add to job pool ( executable @$ cur priority ) ; } else { left job priorities . put ( executable . get id ( ) @$ cur priority + __num__ ) ; },job wait sec
no node label node <PLACE_HOLDER> @$ so return the strict resource request node <PLACE_HOLDER>,return nodes for reqs . size ( ) ;,node label ids
no input consumed @$ better add an error <PLACE_HOLDER>,if ( _input . index ( ) == i ) { if ( e instanceof input mismatch exception ) { input mismatch exception ime = ( input mismatch exception ) e ; token tok = e . get offending token ( ) ; int expected token type = token . invalid_type ; if ( ! ime . get expected tokens ( ) . is nil ( ) ) { expected token type = ime . get expected tokens ( ) . get min element ( ) ; } token err token = get token factory ( ) . create ( new pair < token source @$ char stream > ( tok . get token source ( ) @$ tok . get token source ( ) . get input,input add channel
when view has no <PLACE_HOLDER>,get elements = new get elements . builder ( ) . input ( new entity seed ( __str__ ) @$ new entity seed ( __str__ ) ) . view ( new view . builder ( ) . entity ( test groups . entity ) . build ( ) ) . build ( ) ; results = graph . execute ( get elements @$ new user ( ) ) ;,view has edges
when one line is larger than the other @$ it contains extra vertical padding . this produces more apparent whitespace above or below the text lines . add a <PLACE_HOLDER> offset to compensate .,if ( line1 height != line2 height ) { vertical offset += ( line2 height - line1 height ) / __num__ ; },vertical add increment
copy process <PLACE_HOLDER> instantiate the process <PLACE_HOLDER> @$ renaming as necessary,final set < process groupdto > groups = new hash set < > ( ) ; if ( snippet contents . get process groups ( ) != null ) { for ( final process groupdto groupdto : snippet contents . get process groups ( ) ) { final process groupdto cp = dto factory . copy ( groupdto @$ false ) ; cp . set id ( generate id ( groupdto . get id ( ) @$ id generation seed @$ is copy ) ) ; cp . set parent group id ( group id ) ; final flow snippetdto contents copy = copy contents for group ( groupdto . get contents ( ) @$ cp . get id ( ) @$ connectable map @$ service id map,groups instantiate groups
no such method should throw the caught <PLACE_HOLDER> . so if we get here @$ there was such a method .,if ( ! modifier . is static ( is method . get modifiers ( ) ) && is method . get annotation ( transient . class ) == null ) { check get and is variants ( container class @$ property name @$ get method @$ is method ) ; },method throw exception
note : the hls spec forbids initialization <PLACE_HOLDER> for packed audio .,if ( init load completed || init data spec == null ) { return ; },spec forbids logic
specified attributes should already have a normalized <PLACE_HOLDER> since those were added by validator,if ( ! attr . get specified ( ) ) { return value ; },attributes have value
verifies write <PLACE_HOLDER> to the source and destination,return with write lock ( service facade @$ request connection entity @$ request revision @$ lookup -> { final authorizable authorizable = lookup . get connection ( id ) . get authorizable ( ) ; authorizable . authorize ( authorizer @$ request action . write @$ ni fi user utils . get ni fi user ( ) ) ; authorizable . get parent authorizable ( ) . authorize ( authorizer @$ request action . write @$ ni fi user utils . get ni fi user ( ) ) ; } @$ ( ) -> service facade . verify delete connection ( id ) @$ ( revision @$ connection entity ) -> { final connection entity entity = service facade . delete connection ( revision @$ connection entity .,verifies write access
calculate ifd group data area <PLACE_HOLDER>s . ifd group data area is assigned to save the entry value which has a bigger <PLACE_HOLDER> than 4 bytes .,for ( int i = __num__ ; i < exif_tags . length ; ++ i ) { int sum = __num__ ; for ( map . entry entry : ( set < map . entry > ) m attributes [ i ] . entry set ( ) ) { final exif attribute exif attribute = ( exif attribute ) entry . get value ( ) ; final int size = exif attribute . size ( ) ; if ( size > __num__ ) { sum += size ; } } ifd data sizes [ i ] += sum ; },which has size
since we start extra <PLACE_HOLDER> @$ there may be extra start and cancel events @$ so we check only the difference between start and cancel and not start and cancel events individually .,assert equals ( name @$ total instances @$ dummy service . started ( name ) - dummy service . cancelled ( name ) ) ; check count ( name @$ g @$ total instances ) ; stop extra nodes ( extra nodes ) ;,events start nodes
we could move this three statements below registration @$ but then we should change the logic of the subscriber : if you register before any value is in @$ you 'll get a null value before in on initialize and subsequently values in on add . therefore moving those need a <PLACE_HOLDER> of the subscriber,zk persistent connection zk persistent connection = zk store test only util . get zk persistent connection ( port ) ; zoo keeper ephemeral store < string > writer store = zk store test only util . get zoo keeper ephemeral store ( port ) ; writer store . put ( test_zk_prop_name @$ __str__ ) ; property event bus . register ( collections . singleton ( test_zk_prop_name ) @$ new property event subscriber < string > ( ) { @ override public void on initialize ( string property name @$ string property value ) { if ( property name != null ) { initialized latch . count down ( ) ; } } @ override public void on add ( string property name @$ string property value ),those need hold
drop the request silently if memory aware thread pool has set the <PLACE_HOLDER> .,if ( read suspended ) { e . get future ( ) . set success ( ) ; return true ; },pool set flag
see if user is opening a new policy <PLACE_HOLDER>,if ( filename == null ) { modified = false ; return ; },user opening file
make sure that the scanner does n't throw an <PLACE_HOLDER> after the connection cache timeout,for ( int i = __num__ ; i < num trials ; i ++ ) { list < t result > results = handler . get scanner rows ( scan id @$ __num__ ) ; assert array equals ( bytes . to bytes ( __str__ + i ) @$ results . get ( __num__ ) . get row ( ) ) ; thread . sleep ( trial pause ) ; },scanner throw exception
drwho is not authorized to access test protocol 1 because it uses the default blocked <PLACE_HOLDER> .,try { service authorization manager . authorize ( drwho @$ test protocol1 . class @$ conf @$ inet address . get by name ( address ) ) ; fail ( ) ; } catch ( authorization exception e ) { },default blocked ime
set hll log 2 <PLACE_HOLDER> .,_hll log2m = segment metadata properties configuration . get int ( segment_hll_log2m @$ hll constants . default_log2m ) ;,hll log m
let the emulator view handle <PLACE_HOLDER> if mouse tracking is active,if ( view . is mouse tracking active ( ) ) return false ;,view handle things
in this case the top activity on the <PLACE_HOLDER> is the same as the one being launched @$ so we take that as a request to bring the <PLACE_HOLDER> to the foreground . if the top activity in the <PLACE_HOLDER> is the root activity @$ deliver this new intent to it if it desires .,if ( m start activity . m activity component . equals ( intent activity . get task record ( ) . real activity ) ) { if ( ( ( m launch flags & flag_activity_single_top ) != __num__ || launch_single_top == m launch mode ) && intent activity . m activity component . equals ( m start activity . m activity component ) ) { if ( intent activity . front of task ) { intent activity . get task record ( ) . set intent ( m start activity ) ; } deliver new intent ( intent activity ) ; } else if ( ! intent activity . get task record ( ) . is same intent filter ( m start activity ) ) { m adding,request bring task
user specified default <PLACE_HOLDER> there wo n't be any <PLACE_HOLDER> to annotate @$ so disable them automatically as a usability feature,if ( default package . length ( ) == __num__ ) { package level annotations = false ; },user specified packages
this tests also object type <PLACE_HOLDER> concurrency,final string drl = __str__ + bean . class . get canonical name ( ) + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; test concurrent insertions ( drl @$ __num__ @$ __num__ @$ true @$ true ) ;,tests object comparison
the time this app will have <PLACE_HOLDER> again .,long in quota time elapsed = stats . in quota time elapsed ; if ( ! is under job count quota && stats . bg job count in window < stats . job count limit ) { in quota time elapsed = math . max ( in quota time elapsed @$ stats . job rate limit expiration time elapsed ) ; },app have quota
unlike pql @$ sql expects the group <PLACE_HOLDER> in select statements .,string group by columns = string utils . join ( _group columns @$ __str__ ) ;,sql expects columns
the current path must have <PLACE_HOLDER> to be better than the best match,if ( ! best match . is empty ( ) && path info . distance ( current path ) >= path info . distance ( best match ) ) return false ;,path have distance
if cr<PLACE_HOLDER> just keep <PLACE_HOLDER>,if ( c == __str__ ) { c = read ( reader ) ; if ( c != __str__ ) { last read = c ; use last read = true ; c = __str__ ; } },crlf keep row
lhs shares and rhs shares must necessarily have the same length @$ because everyone uses the same master resource <PLACE_HOLDER> .,for ( int i = lhs shares . length - __num__ ; i >= __num__ ; i -- ) { if ( lhs shares [ i ] == float . positive_infinity || rhs shares [ i ] == float . positive_infinity ) { continue ; } diff = lhs shares [ i ] - rhs shares [ i ] ; if ( diff != __num__ ) { break ; } } return diff ;,everyone uses count
a single checkout does not trigger a <PLACE_HOLDER>,return ;,checkout trigger reply
let the optional watcher take a <PLACE_HOLDER>,if ( m_watcher != null ) { m_watcher . handle line ( data ) ; },watcher take action
we 'll skip words which are <PLACE_HOLDER> . retrieve tags indicating <PLACE_HOLDER> in this treebank .,set < string > punctuation tags = get punctuation tags ( ) ; if ( trees . size ( ) != gold trees . size ( ) ) { log . error ( __str__ ) ; return null ; } int correct arcs = __num__ ; int correct arcs no punc = __num__ ; int correct heads = __num__ ; int correct heads no punc = __num__ ; int correct trees = __num__ ; int correct trees no punc = __num__ ; int correct root = __num__ ; int sum arcs = __num__ ; int sum arcs no punc = __num__ ; for ( int i = __num__ ; i < trees . size ( ) ; ++ i ) { list < core label > tokens =,tags indicating duplicates
the default entry point starts the user <PLACE_HOLDER> .,start application ( null ) ;,point starts application
these credentials must match <PLACE_HOLDER> in the default password file,string [ ] credentials = new string [ ] { __str__ @$ __str__ } ; cli_env . put ( __str__ @$ credentials ) ; jmxc = jmx connector factory . connect ( url @$ cli_env ) ; m bean server connection mbsc = jmxc . getm bean server connection ( ) ;,credentials match those
there exists one independent iterator in the current condition which is also a part of the intermediate resultset identify the final list which will depend upon the complete expansion flag identify the <PLACE_HOLDER> to be expanded to @$ which will also depend upon complete expansion flag..,if ( no of indexes to use == __num__ ) { list total exp list = new array list ( single usableich . expansion list ) ; if ( complete expansion needed ) { support . assert ( expn itrs to ignore != null @$ __str__ ) ; expn itrs to ignore . add all ( single usableich . final list ) ; int size = final list . size ( ) ; for ( int i = __num__ ; i < size ; ++ i ) { runtime iterator curr itr = ( runtime iterator ) final list . get ( i ) ; if ( ! expn itrs to ignore . contains ( curr itr ) ) { total exp list . add ( curr itr,list identify iterators
these classes should have no line <PLACE_HOLDER> @$ except for one in the implicit constructor .,reference type rt = find reference type ( __str__ ) ; if ( rt == null ) { throw new exception ( __str__ ) ; } method method = find method ( rt @$ __str__ @$ __str__ ) ; if ( method == null ) { throw new exception ( __str__ ) ; } test ( method @$ variables @$ __str__ @$ __str__ ) ; test ( method @$ byname @$ __str__ @$ __str__ ) ; test ( method @$ arguments @$ __str__ @$ __str__ ) ; method = find method ( rt @$ __str__ @$ __str__ ) ; if ( method == null ) { throw new exception ( __str__ ) ; } test ( method @$ variables @$ __str__ @$ __str__ ) ; test ( method,classes have numbers
since all types does n't have a pk we now have two <PLACE_HOLDER> .,assert equals ( __num__ @$ realm . where ( all types . class ) . find all ( ) . size ( ) ) ;,types have entries
if the pattern has an <PLACE_HOLDER> as source it wo n't be relevant for calculation of property reactivity masks,if ( ! ( pattern . get source ( ) instanceof accumulate ) ) { context . set last built pattern ( pattern ) ; },pattern has ulplacement
do n't do anything here . the user did n't change the <PLACE_HOLDER> .,if ( producer edited == false ) { } else { try { tag . set field ( field key . producer @$ producer edit text . get text ( ) . to string ( ) ) ; } catch ( key not found exception e ) { e . print stack trace ( ) ; } catch ( field data invalid exception e ) { e . print stack trace ( ) ; } catch ( no such element exception e ) { e . print stack trace ( ) ; } },user change producer
since this function may not access the underlying inner <PLACE_HOLDER> @$ we need to validate if <PLACE_HOLDER> is open outside as well .,validate store open ( ) ; final key value iterator < windowed < bytes > @$ byte [ ] > underlying iterator = wrapped ( ) . fetch ( from @$ to @$ time from @$ time to ) ; if ( cache == null ) { return underlying iterator ; } final peeking key value iterator < bytes @$ lru cache entry > cache iterator = wrapped ( ) . persistent ( ) ? new cache iterator wrapper ( from @$ to @$ time from @$ time to ) : cache . range ( name @$ cache function . cache key ( key schema . lower range ( from @$ time from ) ) @$ cache function . cache key ( key schema . upper range (,function access store
windows secure container executor will set <PLACE_HOLDER> to allow nm to read the file,line ( __str__ ) ; line with len check ( string . format ( __str__ @$ src . to string ( ) @$ dest . to string ( ) ) ) ;,executor set stdin
data source should start <PLACE_HOLDER> only after command log replay on a recover,source . set ready for polling ( m_start polling ) ; runner . run ( ) ;,source start polling
ensure broker gets a <PLACE_HOLDER> to send on the new connection,time unit . seconds . sleep ( __num__ ) ; log . info ( __str__ + socket ) ; socket . close ( ) ;,broker gets chance
backslash and double quote need double the <PLACE_HOLDER> for both java and haskell,special char replacements . remove ( __str__ ) ; special char replacements . remove ( __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ;,backslash double handling
check that the byte array contains the complete <PLACE_HOLDER>,if ( len > b . length ) { throw new illegal argument exception ( __str__ ) ; } try { synchronized ( send lock ) { send packet0 ( id @$ b ) ; } } catch ( io exception ioe ) { if ( ! is open ( ) ) { throw new closed connection exception ( __str__ ) ; } else { throw ioe ; } },array contains packet
we ca n't do anr checks after we cease to exist ! reset any blocking behavior <PLACE_HOLDER> we might have made .,if ( m closed . compare and set ( false @$ true ) ) { set detect not responding ( __num__ ) ; if ( m stable ) { return m content resolver . release provider ( m content provider ) ; } else { return m content resolver . release unstable provider ( m content provider ) ; } } else { return false ; },any blocking changes
create three task generator threads . each of them will submit different <PLACE_HOLDER> of jobs .,final runnable task runnable1 = new task generator ( msg queue @$ __num__ ) ; final runnable task runnable2 = new task generator ( msg queue @$ __num__ ) ; final runnable task runnable3 = new task generator ( msg queue @$ __num__ ) ;,threads submit types
check again with hashtable just in case another thread created a <PLACE_HOLDER> since we last checked,handler2 = handlers . get ( protocol ) ; if ( handler2 != null ) { return handler2 ; },thread created handler
accessible j label implements accessible <PLACE_HOLDER> if the j label contains html <PLACE_HOLDER>,if ( note label != null ) { return note label . get accessible context ( ) . get accessible text ( ) ; },label contains text
if no entry keep skipping rows until we come to the end @$ or find <PLACE_HOLDER> that is populated,while ( this . entry == null && this . row < this . length ) { this . entry = this . table [ this . row ] ; this . row ++ ; } return this . entry ;,entry keep one
only global stats make <PLACE_HOLDER>,_total stats . register buffer metrics ( r @$ s @$ since @$ free space ) ;,stats make sense
each source lines below should represent 32 <PLACE_HOLDER> @$ until the next comment .,return new byte array input stream ( ( __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ) . get bytes ( ) ) ;,lines represent bytes
does the class have optimization <PLACE_HOLDER> ?,if ( class optimization info . get class optimization info ( library class ) != null ) { class visitor . visit library class ( library class ) ; },class have info
this check is somewhat unnecessary as all partitioned regions should have the same <PLACE_HOLDER> due to the fact that partitioned regions do no support <PLACE_HOLDER> .,if ( profile . scope . is distributed ( ) && other scope . is distributed ( ) ) { if ( profile . scope != other scope ) { result = string . format ( __str__ @$ this . region path @$ profile . scope @$ my id @$ other scope ) ; } },regions do scope
when streaming @$ we only process whole <PLACE_HOLDER> thus some updates are only done on paragraph boundaries,if ( ( reordering options & option_streaming ) != __num__ ) { length = i ; this . control count = control count ; },streaming process paragraph
re<PLACE_HOLDER>ing the input from the previous test does not change the resulting <PLACE_HOLDER>,list < spawn exec > l = test stable sort ( immutable list . of ( f @$ e @$ d @$ c @$ b @$ a ) ) ; assert that ( l ) . contains exactly ( a @$ c @$ d @$ b @$ e @$ f ) . in order ( ) ;,input change state
j progress bar.vertical paint each <PLACE_HOLDER> vertically,for ( int i = y - offset ; i < height + tile width ; i += tile width ) { context . get painter ( ) . paint progress bar foreground ( context @$ g @$ x @$ i @$ width @$ tile width @$ p bar . get orientation ( ) ) ; },bar.vertical paint target
do everything as bytes ; do n't want any <PLACE_HOLDER>,@ suppress warnings ( __str__ ) buffered input stream istr = new buffered input stream ( new file input stream ( args [ __num__ ] ) ) ; byte array output stream ostr = new byte array output stream ( ) ; byte [ ] bytes = new byte [ __num__ ] ; boolean more = true ; while ( more ) { int len = istr . read ( bytes ) ; if ( len < bytes . length ) { more = false ; if ( len > __num__ ) { ostr . write ( bytes @$ __num__ @$ len ) ; } } else { ostr . write ( bytes ) ; } } byte [ ] data = ostr . to byte array (,everything want comments
add the mapper scanner as a bean definition rather than explicitly setting a post processor on the context so initialization follows the same code <PLACE_HOLDER> as reading from an xml config file,generic bean definition definition = new generic bean definition ( ) ; definition . set bean class ( mapper scanner configurer . class ) ; definition . get property values ( ) . add ( __str__ @$ __str__ ) ; application context . register bean definition ( __str__ @$ definition ) ; setup sql session factory ( __str__ ) ;,initialization follows path
this rule has an <PLACE_HOLDER> .,verifier rule rule1 = verifier component mock factory . create rule1 ( ) ; pattern pattern1 = verifier component mock factory . create pattern1 ( ) ; sub pattern pp1 = new sub pattern ( pattern1 @$ __num__ ) ; sub pattern pp2 = new sub pattern ( pattern1 @$ __num__ ) ; incompatibility i1 = new incompatibility ( pp1 @$ pp2 ) ; sub rule rp1 = new sub rule ( rule1 @$ __num__ ) ; rp1 . add ( pp1 ) ; rp1 . add ( pp2 ) ; sub pattern pp3 = new sub pattern ( pattern1 @$ __num__ ) ; sub pattern pp4 = new sub pattern ( pattern1 @$ __num__ ) ; incompatibility i2 = new incompatibility ( pp1 @$ pp2 ) ;,rule has error
pkcs 11 test.main will remove this <PLACE_HOLDER> if needed,providers . set at ( p @$ __num__ ) ; random = new secure random ( ) ; factory = certificate factory . get instance ( __str__ ) ; try { factory . generate certificate ( null ) ; } catch ( certificate exception e ) { },test.main remove provider
tests all expect to create a new ds reset the test object <PLACE_HOLDER> for the next test .,test object . num instance = __num__ ; portfolio pdx . debug = false ;,ds reset numinstance
check that deleting the ip address stops the <PLACE_HOLDER> .,link properties bogus lp = new link properties ( lp ) ; try ( socket keepalive ka = m cm . create socket keepalive ( my net @$ test socket @$ myi pv4 @$ dsti pv4 @$ executor @$ callback ) ) { ka . start ( valid ka interval ) ; callback . expect started ( ) ; bogus lp . remove link address ( new link address ( myi pv4 @$ __num__ ) ) ; bogus lp . add link address ( new link address ( not myi pv4 @$ __num__ ) ) ; m wi fi network agent . send link properties ( bogus lp ) ; callback . expect error ( socket keepalive . error_invalid_ip_address ) ; m wi fi network agent . send,check stops keepalive
unknown jvm <PLACE_HOLDER> @$ assuming native .,log . ignore ( t ) ;,unknown jvm type
just for overriding the findbugs np <PLACE_HOLDER> @$ as the parameter is marked as nullable in the guava predicate .,return iterables . filter ( files @$ new predicate < file status > ( ) { @ override public boolean apply ( file status file ) { if ( file == null ) { return false ; } string wal = file . get path ( ) . get name ( ) ; boolean log in replication queue = wals . contains ( wal ) ; if ( log in replication queue ) { log . debug ( __str__ @$ wal ) ; } return ! log in replication queue && ( file . get modification time ( ) < readzk timestamp ) ; } } ) ;,findbugs np warnings
the act of persisting does n't flush <PLACE_HOLDER> @$ so our id is still 1 .,assert equals ( integer . value of ( __num__ ) @$ author . get id ( ) ) ;,act flush anything
if this is a .com or jetpack blog @$ tapping the title shows the associated <PLACE_HOLDER> in the reader,if ( can request post ) { if ( ! post exists ) { app log . d ( t . comments @$ __str__ ) ; reader post actions . request blog post ( site . get site id ( ) @$ post id @$ new reader actions . on request listener ( ) { @ override public void on success ( ) { if ( ! is added ( ) ) { return ; } if ( ! has title ) { string post title = reader post table . get post title ( site . get site id ( ) @$ post id ) ; if ( ! text utils . is empty ( post title ) ) { set post title ( txt post title,title shows comments
! ! ! would n't preflighting be simpler ? this looks like it is effectively be doing that . it seems that for every true error the code will call <PLACE_HOLDER> @$ which will throw the error @$ which this will catch @$ which this will then rethrow the error . just seems cumbersome .,if ( e . get message ( ) . index of ( __str__ ) >= __num__ ) { warnln ( __str__ ) ; } else { errln ( e . get message ( ) ) ; },code call system
this is admittedly a bit simple @$ stats object converter seems to allow old stats attributes to be kept if the new values do not overwrite <PLACE_HOLDER> .,for ( column statistics obj col stat : new col list ) { old stats . put ( col stat . get col name ( ) . to lower case ( ) @$ col stat ) ; },values overwrite them
user list notifications have a footer if they have 10 or more users in the body the last block will not have a <PLACE_HOLDER> @$ so we can use that to determine if it is the footer,if ( m note . is follow type ( ) || m note . is like type ( ) || m note . is reblog type ( ) ) { return block object . get type ( ) == null ; },block have type
the enum bit set ; classes for specialized enum constants do n't do the <PLACE_HOLDER> .,return ( this . get modifiers ( ) & enum ) != __num__ && this . get superclass ( ) == java . lang . enum . class ;,classes do same
update button deselects bundles @$ revert buttons to defautl <PLACE_HOLDER>,default button state ( ) ;,buttons defautl state
if the user sets a message <PLACE_HOLDER> @$ use it .,final string message id = one way feature . get message id ( ) ; if ( ! is message id added && message id != null ) { headers . add ( new string header ( av . messageid tag @$ message id ) ) ; },user sets id
need also consider the <PLACE_HOLDER> if surface width is smaller than scissor width .,if ( surface width < scissor width ) { surface percentageh *= ( float ) surface width / scissor width ; },need consider case
some services automatically add <PLACE_HOLDER> from their addressbook to the roster and those <PLACE_HOLDER> are with subscription none . if such already exist @$ remove them . this is typically our own contact,if ( ! is entry displayable ( item ) ) { if ( contact != null ) { contact group parent = contact . get parent contact group ( ) ; if ( parent instanceof root contact group jabber impl ) ( ( root contact group jabber impl ) parent ) . remove contact ( contact ) ; else ( ( contact group jabber impl ) parent ) . remove contact ( contact ) ; fire contact removed ( parent @$ contact ) ; } continue ; } if ( contact == null ) { contact = new contact jabber impl ( item @$ this @$ true @$ true ) ; root group . add contact ( contact ) ; fire contact added ( root group @$ contact,services add contact
if parent statistics is null then that branch of the tree is not walked yet . do n't update the <PLACE_HOLDER> until all branches are walked,if ( stats == null && op . get parent operators ( ) != null ) { if ( is all parents contain statistics ( op ) ) { for ( operator < ? extends operator desc > parent : op . get parent operators ( ) ) { statistics parent stats = parent . get statistics ( ) ; if ( stats == null ) { stats = parent stats . clone ( ) ; } else { stats . add basic stats ( parent stats ) ; } stats . update column stats state ( parent stats . get column stats state ( ) ) ; list < col statistics > col stats = stats utils . get col statistics from expr map ( hconf @$,branch update stats
indicates that the cell has a the <PLACE_HOLDER> which was modified in the src replication cluster,tag tag = pair . get second ( ) ; if ( cell visibility == null && tag != null ) { cell visibility = new cell visibility ( tag . get value as string ( tag ) ) ; modified tag found = true ; },cell has tag
call set system ui visibility with flags which will cause window <PLACE_HOLDER> to be dispatched,final int flags = view . system_ui_flag_layout_stable | view . system_ui_flag_layout_fullscreen ; on view ( with id ( r . id . test_content ) ) . perform ( set system ui visibility ( flags ) ) ;,which cause events
shallow equals is a middle ground between using default equals @$ which might miss nested <PLACE_HOLDER> with the same elements @$ and deep equality checking @$ which would be expensive . all three choices are sound @$ since shallow equals and default equals are more conservative than deep equals . using shallow equals means that we may unnecessarily consider some values unequal that are,return objects . equals ( this . value @$ that . value ) && objects . equals ( this . error info @$ that . error info ) && transitive events . shallow equals ( that . transitive events ) && transitive postables . shallow equals ( that . transitive postables ) ;,which miss sets
let authenticator know the <PLACE_HOLDER> of the caller,login options . put int ( account manager . key_caller_uid @$ caller uid ) ; login options . put int ( account manager . key_caller_pid @$ binder . get calling pid ( ) ) ; if ( notify on auth failure ) { login options . put boolean ( account manager . key_notify_on_failure @$ true ) ; } long identity token = clear calling identity ( ) ; try { final byte [ ] caller pkg sig digest = calculate package signature digest ( caller pkg ) ; if ( ! custom tokens && permission granted ) { string auth token = read auth token internal ( accounts @$ account @$ auth token type ) ; if ( auth token != null ) { bundle result = new,authenticator know uid
default to the global scope because externs may have qualified <PLACE_HOLDER> with undeclared roots .,if ( root var == null ) { return this . get parent ( ) . get global scope ( ) . get own slot ( name ) ; } else { return root var . get scope ( ) . get own slot ( name ) ; },externs have names
only new user state has <PLACE_HOLDER> defined ; different,final package user state test user state01 = new package user state ( ) ; test user state01 . disabled components = new array set < > ( ) ; assert that ( test user state01 . equals ( old user state ) @$ is ( false ) ) ;,state has array
clear calling <PLACE_HOLDER> as initialization enforces the system <PLACE_HOLDER> but we can be coming from shell .,if ( get user manager ( ) . is user unlocked ( user id ) ) { long old id = binder . clear calling identity ( ) ; try { start service for user ( user id ) ; } finally { binder . restore calling identity ( old id ) ; } },identity enforces identity
null should not affect <PLACE_HOLDER>,current count = double count kudaf . aggregate ( null @$ current count ) ; assert that ( __num__ @$ equal to ( current count ) ) ;,null affect result
get the first person mount <PLACE_HOLDER> and rotate it away from the camera,character held item component character held item component = local player . get character entity ( ) . get component ( character held item component . class ) ; first person held item mount point component mount point component = local player . get camera entity ( ) . get component ( first person held item mount point component . class ) ; if ( character held item component == null || mount point component == null ) { return ; } location component location component = mount point component . mount point entity . get component ( location component . class ) ; if ( location component == null ) { return ; } long time elapsed since last used = time . get game time in,person mount point
no node for this letter yet . make <PLACE_HOLDER> .,if ( ! found ) { char node = new trie node ( ) ; m trie [ herep ] = node ; m trie [ m trie [ herep ] + trie_c ] = c ; m trie [ m trie [ herep ] + trie_off ] = trie_null ; m trie [ m trie [ herep ] + trie_next ] = trie_null ; m trie [ m trie [ herep ] + trie_child ] = trie_null ; if ( i == slen - __num__ ) { m trie [ m trie [ herep ] + trie_off ] = off ; return ; } herep = m trie [ herep ] + trie_child ; },node make one
order does n't affect <PLACE_HOLDER>,output assembler temp = assembly buffer . create output assembler ( byte order . native order ( ) ) ; write ( temp ) ; return temp . pos ( ) ;,order affect result
package 3 has a <PLACE_HOLDER> working on person instances . as we added person instance in advance @$ <PLACE_HOLDER> should fire now,working memory . fire all rules ( ) ; assert equals ( __str__ @$ __str__ @$ bob . get status ( ) ) ; assert equals ( __num__ @$ list . size ( ) ) ; assert equals ( bob @$ list . get ( __num__ ) ) ; kpkgs = serialization helper . serialize object ( load knowledge packages ( __str__ ) ) ; kbase . add packages ( kpkgs ) ; working memory . fire all rules ( ) ; kbase = serialization helper . serialize object ( kbase ) ; assert equals ( __str__ @$ __str__ @$ bob . get status ( ) ) ; assert equals ( __num__ @$ list . size ( ) ) ; assert equals ( bob @$ list . get,package has rule
if the referred document has a target <PLACE_HOLDER> differing from the caller @$ it 's an error,if ( callertns != curr schema info . f target namespace ) { report schema error ( ns_error_codes [ refer type ] [ second idx ] @$ new object [ ] { callertns @$ curr schema info . f target namespace } @$ schema root ) ; return null ; },document has namespace
check whether first tile covers the <PLACE_HOLDER> of the tile vertically,int first top = traps . get top ( trap list . get int ( __num__ ) ) ; int first bottom = traps . get bottom ( trap list . get int ( __num__ ) ) ; if ( first top > tile starty || first bottom < tile starty ) { return false ; },tile covers bounds
no reducers . just write straight to table . call init table reducer <PLACE_HOLDER> because it sets up the table output format .,job . set mapper class ( importer . class ) ; table map reduce util . init table reducer job ( table name . get name as string ( ) @$ null @$ job ) ; job . set num reduce tasks ( __num__ ) ;,reducers call job
'not found ' can happen if import creates more than one <PLACE_HOLDER>,throw new runtime exception ( __str__ + __str__ + from + __str__ + to + __str__ + req @$ ex ) ;,import creates segment
java 's replace function replace every <PLACE_HOLDER> in origin with replacement @$ while origin value should not be changed is expected in sql .,if ( target . length ( ) == __num__ ) { return origin ; } return origin . replace ( target @$ replacement ) ;,function replace character
if the size of bloom filter is smaller than 1 mb @$ use default max false positive <PLACE_HOLDER>,if ( num bits required <= max num bits ) { return default max false pos probability ; },default max probability
assume approximately half of values will satisfy a <PLACE_HOLDER>,return cost_node_unspecific_predicate ;,half satisfy predicate
sometimes providers do not encode <PLACE_HOLDER> the same way . e.g . bouncy castle may use long form encoding @$ where jdk uses a short encoding with named curves . this checks the encodings and logs them if they differ .,string hex reencoded key = hex . encode ( pub key . get encoded ( ) ) ; if ( ! hex pub key . equals ( hex reencoded key ) ) { system . out . println ( __str__ + hex pub key ) ; system . out . println ( __str__ + hex reencoded key ) ; },providers encode keys
this call will put all the basic <PLACE_HOLDER> into the element,element element = super . toxml ( ) ;,call put stuff
moving the <PLACE_HOLDER> 6 minutes should trigger the <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { current time = new date ( current time . get time ( ) + ( __num__ * __num__ * __num__ ) ) ; set clock to ( current time ) ; job job = cmmn management service . create timer job query ( ) . case instance id ( case instance . get id ( ) ) . single result ( ) ; assert true ( job . get duedate ( ) . get time ( ) - current time . get time ( ) <= ( __num__ * __num__ * __num__ ) ) ; job = cmmn management service . move timer to executable job ( job . get id ( ),timer trigger time
this checks for null data type manager below since bad data type wo n't have <PLACE_HOLDER> .,source archive source archive = data type . get source archive ( ) ; boolean local source = ( source archive == null ) || ( ( data type manager != null ) && system utilities . is equal ( data type manager . get universalid ( ) @$ source archive . get source archiveid ( ) ) ) ; if ( local source ) { source archive = originaldtm . get source archive ( originaldtm . get universalid ( ) ) ; } data type found data type = originaldtm . get data type ( data type . get data type path ( ) ) ; string display name = __str__ ; if ( found data type != null && ( data type manager != null ),checks have one
at boot time @$ we know that the screen is on and the electron beam animation is not playing . we do n't know the screen 's brightness though @$ so prepare to set it to a known state when the state is next applied . although we set the brightness to full on here @$ the display power controller will reset the brightness,m screen state = display . state_on ; m screen brightness = power manager . brightness_on ; schedule screen update ( ) ; m color fade prepared = false ; m color fade level = __num__ ; m color fade ready = true ;,changes have effect
use use a <PLACE_HOLDER> of 1 @$ and then wack the matrix each time we actually use it .,shader = new linear gradient ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ shader . tile mode . clamp ) ; paint . set shader ( shader ) ; paint . set xfermode ( new porter duff xfermode ( porter duff . mode . dst_out ) ) ; this . host = host ;,use use density
check if the current segment contains the given <PLACE_HOLDER> first in order to prevent an unnecessary map lookup .,if ( current segment != null && index > current segment . index ( ) ) { return current segment ; },segment contains index
we will approve the request @$ which will update the <PLACE_HOLDER>,variables = new hash map < > ( ) ; variables . put ( __str__ @$ boolean . true ) ; task task = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) @$ variables ) ;,which update entity
try to find columns in left table which match unique <PLACE_HOLDER> in right table,sql = generate table join by columns ( monitor @$ left table @$ left alias @$ right table @$ right alias ) ; if ( sql != null ) return sql ;,which match aliases
trigger should remove <PLACE_HOLDER> .,assert true ( m accounts db . delete de account ( acc id ) ) ;,trigger remove account
this overwrites whatever setting the <PLACE_HOLDER> configured in the properties,adjust auto commit config ( properties @$ offset commit mode ) ; return new kafka fetcher < > ( source context @$ assigned partitions with initial offsets @$ watermarks periodic @$ watermarks punctuated @$ runtime context . get processing time service ( ) @$ runtime context . get execution config ( ) . get auto watermark interval ( ) @$ runtime context . get user code class loader ( ) @$ runtime context . get task name with subtasks ( ) @$ deserializer @$ properties @$ poll timeout @$ runtime context . get metric group ( ) @$ consumer metric group @$ use metrics ) ;,whatever setting user
if s does n't cause the current <PLACE_HOLDER> to cross the limit @$ buffer it and return . we 'll decide whether or not we have to wrap it later .,if ( next newline == - __num__ && column + s . length ( ) <= column limit ) { buffer . append ( s ) ; column += s . length ( ) ; return ; },s cause column
drop should succeed since no query is using the <PLACE_HOLDER>,final optional < command status > drop table command status2 = statement executor . get status ( drop table command id2 ) ; assert . assert true ( drop table command status2 . is present ( ) ) ; assert that ( drop table command status2 . get ( ) . get status ( ) @$ equal to ( command status . status . success ) ) ;,query using connection
there are too many configuration processors so we do n't know <PLACE_HOLDER> one to run so report the error .,if ( user supplied configuration processor count > __num__ ) { string builder sb = new string builder ( string . format ( __str__ @$ user supplied configuration processor count ) ) ; for ( entry < string @$ configuration processor > entry : configuration processors . entry set ( ) ) { string hint = entry . get key ( ) ; if ( ! hint . equals ( settings xml configuration processor . hint ) ) { configuration processor configuration processor = entry . get value ( ) ; sb . append ( string . format ( __str__ @$ configuration processor . get class ( ) . get name ( ) ) ) ; } } sb . append ( __str__ ) ; throw new exception,one run which
we need to go through a list of appenders and locate the async ones @$ as those could have <PLACE_HOLDER> left to write . since there is no flushing mechanism built into logback @$ we wait for a short period of time before giving up that the appender will be completely flushed .,try { final logger logger = logger context . get logger ( org . slf4j . logger . root_logger_name ) ; final list < appender < i logging event > > appenders = lists . of ( logger . iterator for appenders ( ) ) ; for ( appender < i logging event > appender : appenders ) { if ( appender instanceof async appender base ) { flush appender ( ( async appender base < ? > ) appender ) ; } else if ( appender instanceof async appender base proxy ) { flush appender ( ( ( async appender base proxy < ? > ) appender ) . get appender ( ) ) ; } } } catch ( interrupted exception ignored ) { thread .,those have data
make sure master is fully up before progressing . could take a <PLACE_HOLDER> if regions being reassigned .,while ( ! master . get master ( ) . is initialized ( ) ) { threads . sleep ( __num__ ) ; },progressing take while
cache was probably closed which destroyed this lock service <PLACE_HOLDER> : destroyed lock services release all held locks,cache . get cancel criterion ( ) . check cancel in progress ( null ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . lock service @$ e ) ; },which destroyed note
now do the same <PLACE_HOLDER> @$ but make all the requests for one group first @$ then the second group .,for ( int i = __num__ ; i < __num__ ; i ++ ) { server location l = sn . get server for connection ( __str__ @$ collections . empty_set ) ; assert true ( l1 . equals ( l ) || l2 . equals ( l ) ) ; } expected = new hash map ( ) ; expected . put ( l1 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; expected . put ( l2 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; expected . put ( l3 @$ new server load ( __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; assert equals ( expected @$ sn . get,now do thing
request decommission for data nodes <PLACE_HOLDER> and 2 .,list < datanode info > decom data nodes = take node outof service ( __num__ @$ lists . new array list ( get cluster ( ) . get data nodes ( ) . get ( __num__ ) . get datanode uuid ( ) @$ get cluster ( ) . get data nodes ( ) . get ( __num__ ) . get datanode uuid ( ) ) @$ long . max_value @$ null @$ null @$ admin states . decommissioned ) ; generic test utils . wait for ( new supplier < boolean > ( ) { @ override public boolean get ( ) { try { string err msg = check file ( file sys @$ file @$ repl @$ decom data nodes . get ( __num__ ),decommission nodes 1
scan will cause <PLACE_HOLDER> to be added in block cache,scan all regions forrs ( rs1 ) ; assert equals ( block cache1 . get block count ( ) - initial block count1 @$ htu . get numh files forrs ( rs1 @$ table_name @$ family ) ) ; scan all regions forrs ( rs2 ) ; assert equals ( block cache2 . get block count ( ) - initial block count2 @$ htu . get numh files forrs ( rs2 @$ table_name @$ family ) ) ; cache eviction stats stats = admin . clear block cache ( table_name ) . get ( ) ; assert equals ( stats . get evicted blocks ( ) @$ htu . get numh files forrs ( rs1 @$ table_name @$ family ) + htu . get numh files forrs (,scan cause blocks
expression has n't nested <PLACE_HOLDER>,if ( tail == null ) { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( __str__ @$ offset @$ result ) ; } else { result . add ( new flat field descriptor ( offset @$ field type ) ) ; } } else { if ( field type instanceof composite type ) { ( ( composite type ) field type ) . get flat fields ( tail @$ offset @$ result ) ; } else { throw new invalid field reference exception ( __str__ + tail + __str__ + field type + __str__ ) ; } },expression nested field
make sure both <PLACE_HOLDER>s are targeting the same <PLACE_HOLDER> .,if ( ! item to move . get target package name ( ) . equals ( m items . get ( parent idx ) . get target package name ( ) ) ) { return false ; } m items . remove ( move idx ) ; final int new parent idx = select ( new parent package name @$ user id ) + __num__ ; m items . add ( new parent idx @$ item to move ) ; return move idx != new parent idx ;,packages targeting package
only the stream which was just added will change <PLACE_HOLDER> . so we only need an array of size 1 .,list < parent changed event > events = new array list < parent changed event > ( __num__ ) ; connection state . take child ( state @$ false @$ events ) ; notify parent changed ( events ) ; state only removal queue . remove typed ( state ) ; state . stream = stream ;,stream change events
even the looser regime forbids the toplevel <PLACE_HOLDER> to be list or dict .,if ( x instanceof starlark list || x instanceof dict ) { throw new eval exception ( null @$ string . format ( __str__ @$ eval utils . get data type name ( x ) ) ) ; },regime forbids type
if non empty basic <PLACE_HOLDER>,if ( end >= start ) { for ( int i = start ; i < end ; ++ i ) { code . data [ i ] = opcodes . nop ; } code . data [ end ] = ( byte ) opcodes . athrow ; start frame ( start @$ __num__ @$ __num__ ) ; frame [ frame index ++ ] = frame . object | cw . add type ( __str__ ) ; end frame ( ) ; },non empty block
the encryption <PLACE_HOLDER> factory only returns a <PLACE_HOLDER> if encryption is enabled .,data encryption key encryption key = ! trusted channel resolver . is trusted ( ) ? encryption key factory . new data encryption key ( ) : null ; io stream pair ios = send ( socket . get inet address ( ) @$ underlying out @$ underlying in @$ encryption key @$ access token @$ datanode id @$ null ) ; return ios != null ? ios : new io stream pair ( underlying in @$ underlying out ) ;,factory returns key
this would mean we 're explicitly queued or we have no running nodes but do have a cause of blockage @$ which works out the <PLACE_HOLDER>,if ( ! is running && ( is queued || cause of blockage != null ) ) { return blue run state . queued ; },which works same
this operation does not change the current <PLACE_HOLDER> of the file,if ( n > __num__ ) { decrypt ( position @$ buffer @$ offset @$ n ) ; },operation change offset
if any event in the set has a <PLACE_HOLDER> associated with it @$ they all will @$ so just grab the first one .,if ( set . size ( ) > __num__ ) { event event = set . iterator ( ) . next ( ) ; thread = event thread ( event ) ; } else { thread = null ; },event has thread
search in descending order @$ so that the first found <PLACE_HOLDER> is the result .,for ( ninja scope included scope : sub map . descending map ( ) . values ( ) ) { t included value = included scope . find by name and offset recursively ( integer . max_value @$ name @$ map supplier ) ; if ( included value != null ) { return included value ; } } if ( current scope value != null ) { return current scope value . get second ( ) ; } if ( parent scope != null ) { preconditions . check not null ( include point ) ; return parent scope . find by name and offset recursively ( include point - __num__ @$ name @$ map supplier ) ; },first found value
load read the encoding <PLACE_HOLDER> from the output stream,return block encoding . read block ( this @$ input ) ;,load read block
test 1 : does not include <PLACE_HOLDER>,text view = m grid view . find view holder for adapter position ( __num__ ) . item view . find view by id ( r . id . t1 ) ; rect . set ( __num__ @$ __num__ @$ text view . get width ( ) @$ text view . get height ( ) ) ; m grid view . offset descendant rect to my coords ( text view @$ rect ) ; assert equals ( window align center @$ rect . top @$ delta ) ;,not include padding
do n't import nested classes because some of them have the same <PLACE_HOLDER> .,annotation mirror am = get current annotation ( ) ; annotation value av = helpers . get annotation type mirror value ( am @$ __str__ ) ; annotation value cv = helpers . get annotation type mirror value ( am @$ __str__ ) ; annotation value min sdk val = helpers . get annotation type mirror value ( am @$ __str__ ) ; int min sdk = min sdk val == null ? - __num__ : helpers . get annotation int value ( min sdk val ) ; annotation value max sdk val = helpers . get annotation type mirror value ( am @$ __str__ ) ; int max sdk = max sdk val == null ? - __num__ : helpers . get annotation int value ( max,some have signature
next @$ look for errors from the unsuccessfully evaluated transitive traversal <PLACE_HOLDER> .,iterable < sky key > unsuccessful keys = iterables . filter ( transitive traversal keys @$ predicates . not ( predicates . in ( successful keys ) ) ) ; set < map . entry < sky key @$ exception > > error entries = graph . get missing and exceptions ( unsuccessful keys ) . entry set ( ) ; set < sky key > missing keys = new hash set < > ( ) ; for ( map . entry < sky key @$ exception > entry : error entries ) { if ( entry . get value ( ) == null ) { missing keys . add ( ( sky key ) entry . get key ( ) . argument ( ) ) ; },look evaluated keys
get the next instruction . the loop will perform one extra <PLACE_HOLDER> after it reaches the end of the instruction list @$ with current handle set to null .,do { current handle = instructions . has next ( ) ? ( instruction handle ) instructions . next ( ) : null ; instruction inst = ( current handle != null ) ? current handle . get instruction ( ) : null ; if ( first instruction ) { open chunk at curr level = true ; curr level chunks . add ( current handle ) ; first instruction = false ; } if ( inst instanceof outlineable chunk start ) { if ( open chunk at curr level ) { sub chunk stack . push ( curr level chunks ) ; curr level chunks = new array list ( ) ; } open chunk at curr level = true ; curr level chunks . add (,loop perform wait
note : lobounds should have at least one <PLACE_HOLDER>,type owntype = lobounds . tail . tail == null ? lobounds . head : infer . types . lub ( lobounds ) ; if ( owntype . is primitive ( ) || owntype . has tag ( error ) ) { throw infer . inference exception . set message ( __str__ @$ uv . qtype @$ lobounds ) ; } else { return owntype ; },lobounds have element
check that the api handles the <PLACE_HOLDER>,final string multipart array api = files . get ( __str__ ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ; assert . assert true ( multipart array api . contains ( __str__ ) ) ;,api handles array
testing if an extremely large pattern will fail the <PLACE_HOLDER>,pattern = __str__ ; for ( int count = __num__ ; count < __num__ ; count ++ ) { pattern += temp ; } try { result = new string search ( pattern @$ new string character iterator ( text ) @$ m_en_us_ @$ null ) ; logln ( __str__ + result . get pattern ( ) ) ; } catch ( exception e ) { errln ( __str__ ) ; return ; },pattern fail test
this method does not cache the <PLACE_HOLDER>,return new hash set < > ( get matching beans ( new resolvable ( required type @$ qualifiers ) ) ) ;,method cache results
driver join <PLACE_HOLDER> auxiliary join <PLACE_HOLDER>,double inner join row count = left_rows_count * right_rows_count / left_join_column_ndv * left_join_column_non_nulls * right_join_column_non_nulls * unknown_filter_coefficient ;,auxiliary join clause
the very first notification is likely to come in slower than all the others . because that test is n't targeting the <PLACE_HOLDER> notifications are delivered with @$ we prefer to secure it .,if ( i == __num__ && notif == null ) { system . out . println ( __str__ + time for notification in seconds + __str__ + __str__ ) ; notif = notif list . poll ( time for notification in seconds @$ time unit . seconds ) ; } if ( notif == null ) { error count ++ ; system . out . println ( __str__ + __str__ + time for notification in seconds + __str__ ) ; } else { error count += check notification ( notif @$ ( string ) send notif param [ __num__ ] @$ basic . notification_message @$ obj name ) ; },test targeting which
if a target has more than one source <PLACE_HOLDER> @$ the latter one will take effect .,bind mounts . put ( mount target @$ mount source ) ;,target has path
this probably wo n't actually be uploaded @$ as android will probably kill all <PLACE_HOLDER> & data before it gets sent to the network .,if ( m is native initialized ) { record histogram . record enumerated histogram ( __str__ @$ option_clear_app_data @$ option_max ) ; },android kill application
nobody calls <PLACE_HOLDER>,assert equals ( __num__ @$ calling . size ( ) ) ;,nobody calls super
..and most applications are not currently executing an <PLACE_HOLDER> ... it needs to activate some monitoring flags that are usually off..,return null ;,applications executing intent
if we do n't have an mx record @$ try the <PLACE_HOLDER> itself,if ( ( attr == null ) || ( attr . size ( ) == __num__ ) ) { attrs = ictx . get attributes ( host name @$ new string [ ] { __str__ } ) ; attr = attrs . get ( __str__ ) ; if ( attr == null ) { throw new naming exception ( base messages . get string ( pkg @$ __str__ @$ host name ) ) ; } },itself try instance
toggling check box requires <PLACE_HOLDER>,assert counts ( __num__ @$ __num__ ) ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { e . print stack trace ( ) ; },box requires time
negative test that assert glob does throw an <PLACE_HOLDER> when asserting against the wrong values .,illegal argument exception e = assert throws ( illegal argument exception . class @$ ( ) -> assert glob matches ( immutable list . of ( __str__ @$ __str__ ) @$ immutable list . of ( __str__ @$ __str__ ) @$ immutable list . < string > of ( ) @$ true ) ) ;,test throw exception
fetch the viterbi generated <PLACE_HOLDER>,int [ ] computed = hmm algorithms . viterbi algorithm ( get model ( ) @$ get sequence ( ) @$ false ) ;,viterbi generated sequence
create two files to ensure each storage has a <PLACE_HOLDER>,dfs test util . create file ( cluster . get file system ( ) @$ new path ( __str__ ) @$ __num__ @$ __num__ @$ __num__ @$ ( short ) __num__ @$ __num__ ) ; dfs test util . create file ( cluster . get file system ( ) @$ new path ( __str__ ) @$ __num__ @$ __num__ @$ __num__ @$ ( short ) __num__ @$ __num__ ) ;,storage has length
nosonar expose internal <PLACE_HOLDER>,return decorations ;,nosonar expose decorations
<PLACE_HOLDER> for a non java jndi resource @$ inject factory which does a true jndi <PLACE_HOLDER>,managed reference factory = new managed reference factory ( ) { @ override public managed reference get reference ( ) { try { return new immediate managed reference ( new initial context ( ) . lookup ( lookup name ) ) ; } catch ( naming exception e ) { ee logger . root_logger . tracef ( e @$ __str__ @$ lookup name ) ; return null ; } } } ;,which does lookup
<PLACE_HOLDER> 's equality does n't consider seq so if only seq number changes in resolved override <PLACE_HOLDER> . therefore <PLACE_HOLDER> container does n't change merged override <PLACE_HOLDER> @$ but it 's used to push <PLACE_HOLDER> changes so explicitly update that .,if ( get merged override configuration ( ) . seq != get resolved override configuration ( ) . seq ) { on merged override configuration changed ( ) ; },container change configuration
now asynchronously request a <PLACE_HOLDER> that initial fails .,when ( work executor . request checkpoint ( ) ) . then return ( null ) ; progress updater . request checkpoint ( ) ;,now request checkpoint
if added only one computed flow reference @$ then recovered a good <PLACE_HOLDER> .,return found count . get ( ) == __num__ ;,then recovered one
if the parent has a default <PLACE_HOLDER> @$ copy that default <PLACE_HOLDER> an ded with the umask as the new file 's access <PLACE_HOLDER> . if it is a metadata load operation @$ do not consider the umask .,default access control list d acl = current inode directory . get defaultacl ( ) ; short mode = context . is metadata load ( ) ? mode . create full access ( ) . to short ( ) : new file . get mode ( ) ; if ( ! d acl . is empty ( ) ) { access control list acl = d acl . generate child fileacl ( mode ) ; new file . set internal acl ( acl ) ; } if ( file context . is cacheable ( ) ) { new file . set cacheable ( true ) ; } if ( file context . get write type ( ) == write type . async_through ) { new file . set,parent has acl
zip entry does n't override <PLACE_HOLDER>,assert equals ( entry . get name ( ) @$ entries . next element ( ) . get name ( ) ) ;,entry override name
dispatch message received <PLACE_HOLDER>,if ( context . get process engine configuration ( ) . get event dispatcher ( ) . is enabled ( ) ) { delegate execution execution = process instance . get executions ( ) . get ( __num__ ) ; activiti event dispatcher event dispatcher = context . get process engine configuration ( ) . get event dispatcher ( ) ; event dispatcher . dispatch event ( activiti event builder . create message received event ( execution @$ message name @$ null @$ variables ) ) ; },message received event
conditions to reset the loop <PLACE_HOLDER>,if ( break loop || end of arguments ( ) || loop count >= get end index ( ) ) { reset break loop ( ) ; reset loop count ( ) ; return null ; },conditions reset count
do not reset the partial flag for local tracker @$ as here the local tracker only know the <PLACE_HOLDER> which are modified in this file .,procedure map . merge ( local procedure map ) ;,tracker know problems
node has both left and right <PLACE_HOLDER> @$ find the minimum of the two .,t left child value = _values . get ( left child index ) ; t right child value = _values . get ( right child index ) ; if ( compare ( left child value @$ right child value ) <= __num__ ) { min index = left child index ; } else { min index = right child index ; },node has children
temp array holds those <PLACE_HOLDER> we know we want to keep,if ( len != __num__ ) { int newlen = __num__ ; object [ ] temp = new object [ len ] ; for ( int i = __num__ ; i < len ; ++ i ) { object element = elements [ i ] ; if ( c . contains ( element ) ) temp [ newlen ++ ] = element ; } if ( newlen != len ) { set array ( arrays . copy of ( temp @$ newlen ) ) ; return true ; } },array holds elements
the leader executes the following <PLACE_HOLDER> @$ which essentially shuts down the peer if it is not the last round .,if ( v . get id ( ) == i ) { log . info ( __str__ @$ i ) ; if ( lc < this . total rounds ) { log . info ( __str__ @$ i ) ; fast leader election election = ( fast leader election ) peer . get election alg ( ) ; election . shutdown ( ) ; assert equals ( - __num__ @$ election . get vote ( ) . get id ( ) ) ; log . info ( __str__ @$ i ) ; break ; } },leader executes task
the second update should n't cause a item <PLACE_HOLDER>,model = new model with click listener_ ( ) ; model . click listener ( model click listener ) ; controller . set model ( model ) ; lifecycle helper . build models and bind ( controller ) ; model = new model with click listener_ ( ) ; model . click listener ( view click listener ) ; controller . set model ( model ) ; lifecycle helper . build models and bind ( controller ) ; verify ( observer mock @$ times ( __num__ ) ) . on item range changed ( eq ( __num__ ) @$ eq ( __num__ ) @$ any ( ) ) ; verify no more interactions ( observer mock ) ;,update cause change
assert that the table created still has no hcat <PLACE_HOLDER>,table table2 = client . get table ( __str__ @$ __str__ ) ; assert . assert true ( table2 . get sd ( ) . get input format ( ) . equals ( h cat constants . hive_rcfile_if_class ) ) ; driver . run ( __str__ ) ;,table has data
by jmf . if the directory does not exist or it does not contain a jmf.properties file . or if the jmf.properties file has 0 <PLACE_HOLDER> then this is the first time we 're running and should continue to with jmf init,string home dir = system . get property ( __str__ ) ; file jmf dir = new file ( home dir @$ __str__ ) ; string classpath = system . get property ( __str__ ) ; classpath += system . get property ( __str__ ) + jmf dir . get absolute path ( ) ; system . set property ( __str__ @$ classpath ) ; if ( ! jmf dir . exists ( ) ) jmf dir . mkdir ( ) ; file jmf properties = new file ( jmf dir @$ __str__ ) ; if ( ! jmf properties . exists ( ) ) { try { jmf properties . create new file ( ) ; } catch ( io exception ex ) { system . out,file has length
as part of the process of launching @$ activity thread also performs a <PLACE_HOLDER> .,if ( and resume && ready to resume ( ) ) { stack . minimal resume activity locked ( r ) ; } else { if ( debug_states ) slog . v ( tag_states @$ __str__ + r + __str__ ) ; r . set state ( paused @$ __str__ ) ; },thread performs requirement
else : the command itself @$ such as a shutdown task @$ might have cancelled all the scheduled <PLACE_HOLDER> already .,future = fake clock . get scheduled executor service ( ) . schedule ( wrap @$ delay @$ time unit ) ;,itself cancelled executors
if the child has an <PLACE_HOLDER> and a behavior @$ let it save some state ...,if ( child id != no_id && b != null ) { parcelable state = b . on save instance state ( this @$ child ) ; if ( state != null ) { behavior states . append ( child id @$ state ) ; } },child has id
if another thread has recomputed the <PLACE_HOLDER> @$ no need to recompute again .,synchronized ( this ) { if ( m should update . get ( ) ) { m version = compute ( ) ; m should update . set ( false ) ; } },thread recomputed data
insert into its base table get the generated id update <PLACE_HOLDER>,try { return run task ( new query task < long > ( ) { @ override public long handle ( connection connection ) throws exception { pojo info pojo info = pojo info map . get ( pojo . get class ( ) ) ; abstract json entity generic json entity = new generic json entity ( ) ; generic json entity . set create time ( new timestamp ( system . current time millis ( ) ) ) ; generic json entity . set update time ( new timestamp ( system . current time millis ( ) ) ) ; generic json entity . set version ( __num__ ) ; generic json entity . set bean class ( pojo . get class ( ) . get,id update time
frame can hold the entire display . use exact <PLACE_HOLDER> .,if ( frame width >= display width && frame height >= display height ) { return new recording info ( display width @$ display height @$ camera frame rate @$ display density ) ; },frame hold length
note : the legacy jni code used to do a query right after a load success to synchronize the service cache . instead store the <PLACE_HOLDER> that was requested to load to update the cache later without doing a query .,if ( result == context hub transaction . result_success ) { m nano app state manager . add nano app instance ( context hub id @$ nano app binary . get nano app id ( ) @$ nano app binary . get nano app version ( ) ) ; },note store data
because there is no need to expand the visual range @$ no row or spacer contents get <PLACE_HOLDER> . all rows @$ spacers @$ and scroll position simply need to be shifted down accordingly and the spacer indexes need updating .,spacer container . update spacer indexes for row and after ( index @$ old top row logical index + visual row order . size ( ) @$ number of rows ) ;,contents get updated
the first and second streams in a reprocessable combination have the same <PLACE_HOLDER> and format . the first is the input and the second is the output used for generating the subsequent input buffers .,if ( is reprocessable ) { array list < size > input size = new array list < size > ( ) ; int format ; if ( comb template . m reprocess type == reprocess type . private ) { input size . add ( max private input size ) ; format = image format . private ; } else { input size . add ( maxyuv input size ) ; format = image format . yuv_420_888 ; } streams info . add ( new mandatory stream information ( input size @$ format @$ true ) ) ; streams info . add ( new mandatory stream information ( input size @$ format ) ) ; },streams have size
make sure null set of anchors throws null pointer <PLACE_HOLDER>,try { pkix builder parameters p = new pkix builder parameters ( ( set ) null @$ null ) ; throw new exception ( __str__ ) ; } catch ( null pointer exception npe ) { },set throws exception
load default font <PLACE_HOLDER> .,default font size = config service . get string ( __str__ ) ;,default font size
prime the connection to allow client and server <PLACE_HOLDER> to be exchanged .,content response response = client . new request ( __str__ @$ connector . get local port ( ) ) . path ( __str__ ) . timeout ( __num__ @$ time unit . seconds ) . send ( ) ; assert equals ( http status . ok_200 @$ response . get status ( ) ) ; org . eclipse . jetty . client . api . request request = client . new request ( __str__ @$ connector . get local port ( ) ) . method ( http method . head ) . path ( __str__ ) ; request . send ( result -> { if ( result . is failed ( ) ) latch . count down ( ) ; } ) ; assert true ( stream latch,connection allow messages
if all of the options come from the same package @$ show the application 's label and icon instead of the generic resolver 's . some calls like intent.resolve activity info query the resolve info from here and then throw away the resolve info itself @$ meaning that the caller loses the resolve package name . therefore the activity info.label res above provides a,final string intent package = intent . get package ( ) ; if ( ! text utils . is empty ( intent package ) && all have package ( query @$ intent package ) ) { final application info appi = query . get ( __num__ ) . activity info . application info ; ri . resolve package name = intent package ; if ( user needs badging ( user id ) ) { ri . no resource id = true ; } else { ri . icon = appi . icon ; } ri . icon resource id = appi . icon ; ri . label res = appi . label res ; } ri . activity info . application info = new application info ( ri,res provides intent
now do a <PLACE_HOLDER> that ensures stuff works when we go over block boundary @$ especially that we return good length on file .,final byte [ ] value = new byte [ __num__ * __num__ ] ;,now do check
this color <PLACE_HOLDER> list does not declare an activated <PLACE_HOLDER> so this should not yield a change,boolean changed = result . on state changed ( new int [ ] { android . r . attr . state_activated } ) ; assert false ( changed ) ; assert equals ( result . get color ( ) @$ csl . get default color ( ) ) ;,list declare state
if the method has <PLACE_HOLDER> @$ skip it,try { if ( method . is empty ( ) || method . get parameter types ( ) . length != __num__ ) { continue ; } } catch ( not found exception e ) { continue ; },method has parameters
uri parsing has <PLACE_HOLDER> .,try { uri oracle rac = uri . create ( __str__ + __str__ + __str__ + __str__ ) ; logger . debug ( oracle rac . to string ( ) ) ; logger . debug ( oracle rac . get scheme ( ) ) ; assert . fail ( ) ; } catch ( exception e ) { },parsing has limitation
new collection has one less <PLACE_HOLDER>,int new row location = get celly ( __num__ ) ; assert close to ( __str__ @$ new row location @$ row location ) ;,collection has row
start a container that listens on a poke port @$ and once poked runs a web <PLACE_HOLDER>,final job job = job . new builder ( ) . set name ( test job name ) . set version ( test job version ) . set image ( nginx ) . set command ( as list ( __str__ @$ __str__ @$ __str__ ) ) . add port ( __str__ @$ port mapping . of ( __num__ ) ) . add port ( __str__ @$ port mapping . of ( __num__ ) ) . add registration ( service endpoint . of ( __str__ @$ __str__ ) @$ service ports . of ( __str__ ) ) . set health check ( health check ) . build ( ) ; assert container registers after poke ( client @$ job ) ;,poked runs service
could n't find a named 'event bus ' event bus object . try to find the first typed <PLACE_HOLDER> we can :,for ( object v : objects . values ( ) ) { if ( v instanceof event bus ) { return ( event bus ) v ; } } return null ;,bus find one
<PLACE_HOLDER> for cert store is collection type @$ using collection with crl create <PLACE_HOLDER>,collection cert store parameters params = new collection cert store parameters ( crl collection ) ;,collection create parameters
nn should have received a new <PLACE_HOLDER>,long nn image after = nn . getfs image ( ) . get storage ( ) . get most recent checkpoint tx id ( ) ; assert true ( __str__ + nn image before + __str__ + nn image after @$ nn image after > nn image before ) ;,nn received checkpoint
if we could n't find the region because the cache is closed @$ throw a cache closed <PLACE_HOLDER>,if ( rgn == null ) { if ( cache . is closed ( ) ) { throw new cache closed exception ( ) ; } throw new region not found exception ( string . format ( __str__ @$ this . region path ) ) ; },cache closed exception
<PLACE_HOLDER> could imagine an empty <PLACE_HOLDER> otherwise,if ( length < __num__ ) { throw new illegal argument exception ( ) ; },one imagine one
root states should only have <PLACE_HOLDER>,for ( string root : start symbols ) { if ( new state split counts . get count ( root ) > __num__ ) { new state split counts . set count ( root @$ __num__ ) ; } } if ( new state split counts . get count ( lexicon . boundary_tag ) > __num__ ) { new state split counts . set count ( lexicon . boundary_tag @$ __num__ ) ; } state split counts = new state split counts ;,states have one
if reference identifies a <PLACE_HOLDER> @$ use exclusively,if ( f != null ) { factory = get object factory from reference ( ref @$ f ) ; if ( factory != null ) { return factory . get object instance ( ref @$ name @$ name ctx @$ environment ) ; } return ref info ; } else { answer = processurl addrs ( ref @$ name @$ name ctx @$ environment ) ; if ( answer != null ) { return answer ; } },reference identifies factory
ttl tags specify <PLACE_HOLDER> in milliseconds,region . put ( new put ( row ) . add ( new key value ( row @$ fam1 @$ q3 @$ now + __num__ - __num__ @$ h constants . empty_byte_array @$ new array backed tag [ ] { new array backed tag ( tag type . ttl_tag_type @$ bytes . to bytes ( __num__ ) ) } ) ) ) ;,tags specify ts
cut the connection @$ so the server will create close <PLACE_HOLDER> as part of expiring the <PLACE_HOLDER> .,client . dont reconnect ( ) ; client . disconnect ( ) ; watcher . reset ( ) ;,server create connection
recurring alarms may have passed several alarm <PLACE_HOLDER> while the alarm was kept pending . send the appropriate trigger count .,if ( alarm . repeat interval > __num__ ) { alarm . count += ( nowelapsed - alarm . expected when elapsed ) / alarm . repeat interval ; final long delta = alarm . count * alarm . repeat interval ; final long next elapsed = alarm . expected when elapsed + delta ; set impl locked ( alarm . type @$ alarm . when + delta @$ next elapsed @$ alarm . window length @$ max trigger time ( nowelapsed @$ next elapsed @$ alarm . repeat interval ) @$ alarm . repeat interval @$ alarm . operation @$ null @$ null @$ alarm . flags @$ true @$ alarm . work source @$ alarm . alarm clock @$ alarm . uid @$ alarm . package,alarms passed intervals
count down every time a thread finishes <PLACE_HOLDER> @$ so that we can wait for <PLACE_HOLDER> to complete,final count down latch work finished latch = new count down latch ( num samples ) ;,thread finishes work
assertion that accreditations collection table got cleaned up if they did n't @$ the delete should have caused a constraint <PLACE_HOLDER> @$ but just to be sure ...,session s = open session ( ) ; s . begin transaction ( ) ; s . do work ( new work ( ) { @ override public void execute ( connection connection ) throws sql exception { final statement statement = connection . create statement ( ) ; final result set result set = statement . execute query ( __str__ ) ; assert true ( result set . next ( ) ) ; final int count = result set . get int ( __num__ ) ; assert equals ( __num__ @$ count ) ; } } ) ; s . get transaction ( ) . commit ( ) ; s . close ( ) ;,delete caused violation
the above transaction now contain the changes we want to expose to the index updater as updates . this will happen when we commit the transaction . the transaction now also holds the schema read <PLACE_HOLDER> @$ so we ca n't begin creating our constraint just yet . we first have to un<PLACE_HOLDER> the schema @$ and then b<PLACE_HOLDER> just before we send off,lock lock blocking data change transaction = get lock service ( ) . acquire node lock ( block data change transaction on lock on id @$ lock type . write_lock ) ;,thread take lock
a <PLACE_HOLDER> resource ca n't use <PLACE_HOLDER> system api or we get an infinite loop since <PLACE_HOLDER> system uses configuration api . use java.io.<PLACE_HOLDER> instead .,if ( resource instanceof path ) { file file = new file ( ( ( path ) resource ) . to uri ( ) . get path ( ) ) . get absolute file ( ) ; if ( file . exists ( ) ) { if ( ! quiet ) { log . debug ( __str__ + file ) ; } reader = ( xml stream reader2 ) parse ( new buffered input stream ( files . new input stream ( file . to path ( ) ) ) @$ ( ( path ) resource ) . to string ( ) @$ is restricted ) ; } } else if ( resource instanceof input stream ) { reader = ( xml stream reader2 ) parse ( (,resource use file
some ipp printers like lexc 710 do not have <PLACE_HOLDER> of supported media but cups can get the media from ppd @$ so we still report as supported category .,if ( is cups printer ) { if ( ! cat list . contains ( media . class ) ) { cat list . add ( media . class ) ; } cat list . add ( media printable area . class ) ; cat list . add ( destination . class ) ; if ( ! print service lookup provider . is linux ( ) ) { cat list . add ( sheet collate . class ) ; } },printers have list
render the world using the debug <PLACE_HOLDER>,renderer . render ( world @$ camera . combined ) ; float render time = ( time utils . nano time ( ) - start time ) / __num__ ; batch . begin ( ) ; font . draw ( batch @$ __str__ + gdx . graphics . get frames per second ( ) + __str__ + update time + __str__ + render time @$ __num__ @$ __num__ ) ; batch . end ( ) ;,world using progress
many tools @$ just keep the largest <PLACE_HOLDER>,analysis shared thread pool size = math . max ( analysis shared thread pool size @$ current tool size ) ;,tools keep one
get the currency pattern for this locale . we have to fish it out of the resource bundle directly @$ since decimal format.to pattern will return the localized <PLACE_HOLDER> @$ not \00 a 4,string [ ] num patterns = ( string [ ] ) rb . get object ( __str__ ) ; string pattern = num patterns [ __num__ ] ; if ( pattern . index of ( __str__ ) == - __num__ ) { errln ( __str__ + locales [ i ] + __str__ + pattern ) ; },pattern return symbol
this is important since some of the x content builders write <PLACE_HOLDER> on close . in order to write the footer we need to prevent closing the actual index input .,try ( output stream index output output stream = new index output output stream ( index output ) { @ override public void close ( ) { } } ) { bytes . write to ( index output output stream ) ; },some write bytes
one shortcut when contracting node <PLACE_HOLDER>,add shortcut ( nodea @$ nodeb @$ e3toa . get edge ( ) @$ e3tob . get edge ( ) @$ e3toa . get edge ( ) @$ e3tob . get edge ( ) @$ __num__ ) ; set level equal to node id for all nodes ( ) ;,contracting node 3
first comment <PLACE_HOLDER> consists of exactly one character . so only <PLACE_HOLDER> break or <PLACE_HOLDER> feed can be left . delete <PLACE_HOLDER> .,if ( m_editable object . get length ( ) == __num__ ) { if ( y > __num__ ) { m_carety = y - __num__ ; final zy line content prev line content = get line content ( m_carety ) ; m_caretx = prev line content . get text ( ) . length ( ) ; } changed text = get multiline comment ( y @$ changed text ) ; } else { m_caretx = get caret end posx ( ) ; m_carety = get caret mouse releasedy ( ) ; return ; },line delete text
if both threads finished already @$ assert that both threads throw <PLACE_HOLDER>,assert that ( ai1 . get exception ( ) instanceof unsupported operation exception ) . is true ( ) ; assert that ( ai2 . get exception ( ) instanceof unsupported operation exception ) . is true ( ) ;,threads throw exception
node might 've not have received the license cluster <PLACE_HOLDER>,if ( current license == null ) { return true ; },might received document
arbitrary size matches bitmap <PLACE_HOLDER> .,byte [ ] storage = new byte [ __num__ * __num__ ] ; image decoder decoder = null ; try { decoder = n create ( is @$ storage @$ source ) ; } finally { if ( decoder == null ) { if ( close input stream ) { io utils . close quietly ( is ) ; } } else { decoder . m input stream = is ; decoder . m owns input stream = close input stream ; decoder . m temp storage = storage ; } } return decoder ;,size matches size
fetcher has copied the input <PLACE_HOLDER> when object reuse is enabled,fetcher . flat map ( in @$ get fetcher collector ( ) ) ; if ( is left outer join && ! collector . is collected ( ) ) { out row . replace ( in @$ null row ) ; out row . set header ( in . get header ( ) ) ; out . collect ( out row ) ; },fetcher copied rows
how may patterns specify this <PLACE_HOLDER>,numfixed = get num fixed ( sbit @$ __num__ @$ context ) ;,patterns specify value
verify false constant replaces the <PLACE_HOLDER> with the else expression .,assert translation ( translation @$ __str__ ) ;,verify replaces conditional
count unprivileged <PLACE_HOLDER> for the same uid across networks .,int unprivileged count same uid = __num__ ; for ( final hash map < integer @$ keepalive info > ka for network : m keepalives . values ( ) ) { for ( final keepalive info ki : ka for network . values ( ) ) { if ( ki . m uid == m uid ) { unprivileged count same uid ++ ; } } } if ( unprivileged count same uid > m allowed unprivileged slots for uid ) { return error_insufficient_resources ; } return success ;,count unprivileged slots
without a copy @$ but gives this instance its own <PLACE_HOLDER> and limit .,this . content = content . as read only buffer ( ) ;,but gives buffer
the client has departed . remove this last <PLACE_HOLDER> and unregister it .,boolean needs unregister = false ; synchronized ( chm lock ) { if ( chm registered ) { needs unregister = true ; chm registered = false ; } } if ( unregister client ) { clean client auths ( ) ; },departed remove connection
we know root has 2 <PLACE_HOLDER> because we create 2 memory blocks in the set up .,f1 = ( program fragment ) groups [ __num__ ] ; f2 = ( program fragment ) groups [ __num__ ] ;,root has groups
this thread has escaped the blocked state <PLACE_HOLDER> the lock and a short wait before continue,try { lock . wait ( __num__ ) ; } catch ( interrupted exception e ) { interrupted . increment and get ( ) ; },thread escaped check
<PLACE_HOLDER> check should match default <PLACE_HOLDER> of that url,test implies ( thisurl @$ thaturl @$ true ) ; system . out . println ( __str__ ) ;,check match settings
yolo pick the first <PLACE_HOLDER> !,return this ;,yolo pick slot
if the genrule rewrites the source map @$ we have to create the parent dir @$ and record the build <PLACE_HOLDER>,if ( rewrite sourcemap ) { source path source path to source map = get source path to source map ( ) ; buildable context . record artifact ( source path resolver adapter . get relative path ( source path to source map ) ) ; builder . add ( mkdir step . of ( build cell relative path . from cell relative path ( context . get build cell root path ( ) @$ get project filesystem ( ) @$ source path resolver adapter . get relative path ( source path to source map ) . get parent ( ) ) ) ) ; },dir build artifact
arizona does not observe <PLACE_HOLDER> @$ so even phoenix and denver have the same raw offest @$ they have different rules .,a = time zone . get time zone ( __str__ ) ; b = time zone . get time zone ( __str__ ) ; assert equals ( a . get raw offset ( ) @$ b . get raw offset ( ) ) ; assert false ( a . has same rules ( b ) ) ;,arizona observe millis
only process if curi contains <PLACE_HOLDER> of fetch attempt,return uri . contains data key ( a_fetch_began_time ) && uri . get recorder ( ) != null && uri . get recorder ( ) . get response content length ( ) >= get lower bound ( ) && uri . get recorder ( ) . get response content length ( ) <= get upper bound ( ) ;,curi contains evidence
for read lock @$ if the caller has locked the same <PLACE_HOLDER> previously @$ it will not try to acquire the same read lock . it simply returns the previous <PLACE_HOLDER> lock .,if ( read lock ) { row lock impl prev row lock impl = ( row lock impl ) prev row lock ; if ( ( prev row lock impl != null ) && ( prev row lock impl . get lock ( ) == row lock context . read write lock . read lock ( ) ) ) { success = true ; return prev row lock ; } result = row lock context . new read lock ( ) ; } else { result = row lock context . new write lock ( ) ; },caller locked row
consume native enter <PLACE_HOLDER> if we generate one,if ( id == mouse event . mouse_entered ) { e . consume ( ) ; },native enter event
add in reverse order so we can tell that sorting actually changed <PLACE_HOLDER>,points . add ( second ) ; points . add ( first ) ; collections . sort ( points ) ; assert with message ( string . format ( __str__ + __str__ @$ points . index of ( second ) @$ points . index of ( first ) ) ) . that ( points . index of ( second ) ) . is greater than ( points . index of ( first ) ) ;,sorting changed size
add snowplow <PLACE_HOLDER> to context @$ contributors can use this <PLACE_HOLDER> to emit other event,emitter = telemetry emitter . builder ( ) . build ( ) ; root context . put ( emitter . class @$ emitter ) ;,contributors use emits
check that the api handles the single <PLACE_HOLDER>,final string multipart single api = files . get ( __str__ ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ; assert . assert true ( multipart single api . contains ( __str__ ) ) ;,api handles file
if so use sentences to get so <PLACE_HOLDER> from sentences,if ( sentences != null && options . coreference context size > __num__ ) { list < core label > tokens = sentences . get ( mention . sent num - __num__ ) . get ( core annotations . tokens annotation . class ) ; int context start = math . max ( mention . start index - __num__ - __num__ @$ __num__ ) ; int context end = math . min ( mention . end index - __num__ + __num__ @$ tokens . size ( ) ) ; string left context = string utils . join words ( tokens @$ __str__ @$ context start @$ mention . start index - __num__ ) ; string right context = string utils . join words ( tokens @$ __str__ @$,sentences get attribute
we only cache get and head <PLACE_HOLDER>,if ( ! exchange . get request method ( ) . equals ( get ) && ! exchange . get request method ( ) . equals ( head ) ) { return false ; } if ( entry == null ) { this . response cachable = mark cacheable ; return false ; },cache get methods
the file filter will tell us which files <PLACE_HOLDER> and which do n't .,try { final file filter filter = config . build file filter ( ) ; while ( ! list . is empty ( ) && state == running ) { final file current = list . poll ( ) ; task . scanned ++ ; if ( current . is file ( ) ) { task . matching ++ ; act on ( current ) ; } if ( current . is directory ( ) ) { final file [ ] content = current . list files ( filter ) ; if ( content == null ) continue ; list . add all ( __num__ @$ arrays . as list ( content ) ) ; } } if ( list . is empty ( ) ) { task,which files runs
if the option accepts <PLACE_HOLDER> and a non blank argname,if ( option . accept value ( ) && ( option . get arg name ( ) == null || option . get arg name ( ) . length ( ) != __num__ ) ) { buff . append ( is null or empty ( option . get short name ( ) ) ? get long option separator ( ) : __str__ ) ; buff . append ( __str__ ) . append ( option . get arg name ( ) != null ? option . get arg name ( ) : get arg name ( ) ) . append ( __str__ ) ; },option accepts value
intent filter verification is only for filters that specify a host . so do n't return <PLACE_HOLDER> that handle all web uris .,if ( ri target user . handle all web datauri ) { continue ; } string package name = ri target user . activity info . package name ; package setting ps = m settings . m packages . get ( package name ) ; if ( ps == null ) { continue ; } long verification state = get domain verification statusl pr ( ps @$ parent user id ) ; int status = ( int ) ( verification state > > __num__ ) ; if ( result == null ) { result = new cross profile domain info ( ) ; result . resolve info = create forwarding resolve info unchecked ( new intent filter ( ) @$ source user id @$ parent user id ),verification return apps
wraps sticky or default . then has next and next just call the <PLACE_HOLDER> .,if ( is addr disposition retry ) { return true ; } boolean result ; if ( primary to contact info != null ) { result = primary to contact info . has next ( primary contact info @$ previous contact info @$ list of contact infos ) ; } else { result = effective targetior iterator . has next ( ) ; } return result ;,sticky call method
we now should only have zero since the disk store had no <PLACE_HOLDER> remaining in it .,file count = __num__ ; for ( file disk dir : disk dirs ) { file [ ] files = disk dir . list files ( ) ; file count += files . length ; } assert that ( file count ) . is equal to ( __num__ ) ;,store had files
custom deserializer always produces <PLACE_HOLDER> like this :,assert equals ( __str__ @$ bean . a ) ; assert equals ( __str__ @$ bean . b ) ;,deserializer produces types
since task resource has been closed @$ broker should return a new <PLACE_HOLDER> of the object,test factory . shared resource task resource2 = task broker . get shared resource at scope ( new test factory < gobblin scope types > ( ) @$ new test resource key ( __str__ ) @$ gobblin scope types . task ) ; assert . assert not equals ( task resource @$ task resource2 ) ; top broker . close ( ) ; assert . assert true ( job resource . is closed ( ) ) ; assert . assert true ( task resource . is closed ( ) ) ;,broker return instance
create a walker which walks the <PLACE_HOLDER> in a dfs manner while maintaining the operator stack . the dispatcher generates the plan from the operator <PLACE_HOLDER>,set multimap < integer @$ node processor > ast node to processor = hash multimap . create ( ) ; ast node to processor . put ( hive parser . tok_null @$ tf . get null expr processor ( ) ) ; ast node to processor . put ( hive parser . number @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . integral literal @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . number literal @$ tf . get num expr processor ( ) ) ; ast node to processor . put ( hive parser . identifier @$ tf . get str expr processor,which walks tree
class extends or implements <PLACE_HOLDER>,case class_extends : print ( __str__ ) ; print ( pos . type_index ) ; break ;,class extends clause
use previously defined job entry <PLACE_HOLDER> !,if ( je . get nr ( ) > __num__ ) { je . set entry ( prev . get entry ( ) ) ; prev = find job entry ( je . get name ( ) @$ je . get nr ( ) @$ true ) ; if ( prev != null ) { int idx = index of job entry ( prev ) ; remove job entry ( idx ) ; } },use defined position
since behavior files might contain language <PLACE_HOLDER> that are n't present in the class file @$ we might need to update the feature set .,if ( cls . features != null ) { node script node = node util . get enclosing script ( parent ) ; feature set old features = ( feature set ) script node . get prop ( node . feature_set ) ; feature set new features = old features . union ( cls . features ) ; if ( ! new features . equals ( old features ) ) { script node . put prop ( node . feature_set @$ new features ) ; compiler . report change to change scope ( script node ) ; } } if ( node util . is name declaration ( expr root ) ) { node assign expr = var to assign ( expr root ) ; parent . replace,files contain features
we do n't use do unchecked here as there is a chance the below method can run <PLACE_HOLDER> supplied code,if ( wild fly security manager . is checking ( ) ) { access controller . do privileged ( ( privileged action < object > ) ( ) -> { session destroyed impl ( se ) ; return null ; } ) ; } else { session destroyed impl ( se ) ; },method run home
these tests do n't handle <PLACE_HOLDER> from local parallel,session . session builder session builder = test session builder ( ) . set catalog ( catalog_id ) . set schema ( __str__ ) . set system property ( __str__ @$ __str__ ) ;,tests handle exchanges
noinspection object <PLACE_HOLDER> in loop,if ( insertion index >= key bucket . size ( ) ) { entry = key bucket . get entry ( insertion index - __num__ @$ key serializer @$ value serializer ) ; } else { entry = key bucket . get entry ( insertion index @$ key serializer @$ value serializer ) ; },noinspection object allocation
log <PLACE_HOLDER> can not be loaded by the classloader which loaded the custom <PLACE_HOLDER> implementation . the custom implementation is not viable until this is corrected . ensure that the jcl jar and the custom class are available from the same classloader . running with diagnostics on should give information about the classloaders used to load the custom <PLACE_HOLDER> .,log diagnostic ( __str__ + __str__ ) ;,which loaded warnings
the tree accepts a jmx <PLACE_HOLDER>,data flavor [ ] flavors = support . get data flavors ( ) ; for ( data flavor flavor : flavors ) { if ( flavor . is flavor java file list type ( ) ) { return true ; } },tree accepts file
to take appropriate action when the activity looses <PLACE_HOLDER>,super . on pause ( ) ; mgl surface view . on pause ( ) ;,activity looses focus
one for event id set message <PLACE_HOLDER>,message . set message type ( message type . register_instantiators ) ; for ( int i = __num__ ; i < instantiators length ; i = i + __num__ ) { message . add bytes part ( this . serialized instantiators [ i ] ) ; message . add bytes part ( this . serialized instantiators [ i + __num__ ] ) ; message . add bytes part ( this . serialized instantiators [ i + __num__ ] ) ; } message . set transaction id ( __num__ ) ; message . add obj part ( this . get event id ( ) ) ; return message ;,one set type
this time @$ we have seven display <PLACE_HOLDER> rad dude 's surfer gal 'replaces ' later 's surfer gal,map dids = get display names ( service ) ; iterator iter = dids . entry set ( ) . iterator ( ) ; int count = __num__ ; while ( iter . has next ( ) ) { ++ count ; entry e = ( entry ) iter . next ( ) ; logln ( __str__ + e . get key ( ) + __str__ + e . get value ( ) ) ; },time have names
store any attached <PLACE_HOLDER> in drive in a new folder .,if ( as object . first image ( ) != null ) { drive drive interface = get or create drive service ( auth data ) ; string folder id = idempotent executor . execute or throw exception ( __str__ @$ __str__ @$ ( ) -> create album folder ( drive interface ) ) ; for ( link value image : as object . image ( ) ) { try { string new img src = idempotent executor . execute and swallowio exceptions ( image . to string ( ) @$ __str__ @$ ( ) -> upload image ( ( as object ) image @$ drive interface @$ folder id ) ) ; content += __str__ + new img src + __str__ ; } catch ( runtime exception,any attached images
some statistics for bitmap : 1 million distinct keys takes about 2 <PLACE_HOLDER> storage 5 million takes 10 <PLACE_HOLDER> 10 million takes 12 <PLACE_HOLDER>,return __num__ * __num__ * __num__ ;,statistics takes mb
do n't let the region attributes creation <PLACE_HOLDER> to the user,attributes factory factory = new attributes factory ( creation ) ; region attributes attrs = factory . create region attributes ( ) ; cache . set region attributes ( id @$ attrs ) ;,region attributes escape
websphere servlet which implements websocket <PLACE_HOLDER> @$ dynamically added,if ( servlet class name . equals ( __str__ ) ) { return false ; },which implements service
cup cursor <PLACE_HOLDER> .,enter string ( __str__ + ( i + __num__ ) + __str__ + ( j + __num__ ) + __str__ ) ;,cup cursor position
verify that the append returned <PLACE_HOLDER> due to timeotu,assert . assert equals ( satus @$ status . backoff ) ;,append returned failure
less common case : there are no valid omnibox suggestions . this can happen if the user tapped the url <PLACE_HOLDER> to dismiss the suggestions @$ then pressed enter . this can also happen if the user presses enter before any suggestions have been received from the autocomplete controller .,suggestion match = m autocomplete . classify ( url text ) ; suggestion match position = __num__ ;,user tapped text
since null could be a valid <PLACE_HOLDER> @$ explicitly check whether map contains the <PLACE_HOLDER>,try { if ( ! serialized to original . contains key ( raw entry . get key ( ) ) ) { log . error ( __str__ + __str__ @$ raw entry . get key ( ) ) ; continue ; } map < string @$ t > orig key = serialized to original . get ( raw entry . get key ( ) ) ; schema and value deserialized schema and value = value converter . to connect data ( namespace @$ raw entry . get value ( ) != null ? raw entry . get value ( ) . array ( ) : null ) ; object deserialized value = deserialized schema and value . value ( ) ; offset utils . validate format ( deserialized,map contains key
ok @$ this action contains a <PLACE_HOLDER> to the file containing the definition . such actions do n't have to have component alias defined .,if ( action . get component ( ) == null && ( action . get reference ( ) != null || action . get reference content ( ) != null ) ) { continue ; },action contains path
extend is in left @$ top @$ right @$ bottom ; which is very uncommon @$ photon uses the <PLACE_HOLDER>,if ( extent . length == __num__ ) { this . extent = new b box ( extent [ __num__ ] @$ extent [ __num__ ] @$ extent [ __num__ ] @$ extent [ __num__ ] ) ; } else { throw new runtime exception ( __str__ + extent . length ) ; },photon uses subpixels
this is the second <PLACE_HOLDER> on row 3 @$ drag the first <PLACE_HOLDER> past the middle of this <PLACE_HOLDER> which causes this <PLACE_HOLDER> to move to the left edge,int startx = bf . get startx ( ) ; int width = bf . get width ( ) ; int middle first fieldx = startx / __num__ ; int past middle second fieldx = startx + width / __num__ + __num__ ; int row = __num__ ; drag field ( row @$ middle first fieldx @$ row @$ past middle second fieldx ) ; wait for swing ( ) ; bf = cb . get current field ( ) ; assert equals ( __num__ @$ bf . get startx ( ) ) ;,which causes field
meta key key listener does n't track <PLACE_HOLDER> key state . need to check the key event instead .,boolean is ctrl active = ( ( event . get meta state ( ) & key event . meta_ctrl_on ) != __num__ ) ; boolean is shift active = ( get meta state ( content @$ meta_shift_on @$ event ) == __num__ ) ; boolean is alt active = ( get meta state ( content @$ meta_alt_on @$ event ) == __num__ ) ; if ( is ctrl active ) { if ( is alt active || is shift active ) { return false ; } return delete until word boundary ( view @$ content @$ is forward delete ) ; },listener track the
chinese date format has pattern <PLACE_HOLDER>etter ' <PLACE_HOLDER> ' for <PLACE_HOLDER>eap month marker in addition to regu<PLACE_HOLDER>ar date format,cal . clear ( ) ; cal . set ( __num__ @$ calendar . june @$ __num__ ) ;,format has h
if this server has not yet applied entries up to the <PLACE_HOLDER> 's session id @$ forward the query to the leader . this ensures that a follower does not tell the <PLACE_HOLDER> its session does n't exist if the follower has n't had a chance to see the session 's registration entry .,if ( raft . get last applied ( ) < request . session ( ) ) { return completable future . completed future ( log response ( query response . builder ( ) . with status ( raft response . status . error ) . with error ( raft error . type . unknown_session @$ __str__ ) . build ( ) ) ) ; },follower tell client
this method automatically registers the <PLACE_HOLDER> with the given registry .,@ suppress warnings ( __str__ ) closingfs data input stream pis = closingfs data input stream . wrap safe ( test stream @$ ( safety net closeable registry ) registry @$ debug ) ;,method registers stream
redis async commands extends redis <PLACE_HOLDER> async commands,if ( redis cluster async commands . class . is assignable from ( clazz ) ) { key += __str__ ; } else if ( redis cluster reactive commands . class . is assignable from ( clazz ) ) { key += __str__ ; } else { throw new illegal argument exception ( clazz . get name ( ) ) ; },commands extends cluster
flush the <PLACE_HOLDER> @$ this is necessary for f str buffered <PLACE_HOLDER> .,_printer . flush ( ) ;,f str output
check row uses <PLACE_HOLDER> to check we are writing in order .,if ( ! check row ( cell ) ) { if ( start offset < __num__ ) { start offset = out . size ( ) ; } rows offsetbaos . write int ( out . size ( ) - start offset ) ; } last cell = cell ; return encoder . write ( cell ) ;,row uses comparator
nightly testing showed some intermittent <PLACE_HOLDER> . check here to get diagnostic information if some strange behavior occurs .,check thread count ( expected count @$ current @$ __num__ ) ;,testing showed errors
publish topic name must not contain any <PLACE_HOLDER>,for ( char c : topic_wildcards ) { if ( topic name . index of ( c ) >= __num__ ) { return false ; } } return true ;,name contain wildcards
step light idle state locked does n't handle the active <PLACE_HOLDER> @$ so the state should stay as active .,verify light state conditions ( light_state_active ) ;,state handle state
we do n't need to add the source file <PLACE_HOLDER> as it is a mandatory input .,if ( artifact != null ) { if ( ! artifact . equals ( source file ) ) { inputs . add ( artifact ) ; } continue ; },source file itself
if global contains <PLACE_HOLDER> @$ individual modules can only contain additional <PLACE_HOLDER> .,if ( ! root config . exclude . is empty ( ) && ! entry . get value ( ) . include . is empty ( ) ) { throw new illegal state exception ( string . format ( __str__ @$ entry . get key ( ) ) ) ; },modules contain exports
find installed font <PLACE_HOLDER> always returns an available font <PLACE_HOLDER>,return font tag . find installed font name ( get font name ( ) ) ;,name returns name
test non zero <PLACE_HOLDER> .,iterator < element > it = vector . non zeroes ( ) . iterator ( ) ; int i = __num__ ; while ( it . has next ( ) ) { it . next ( ) ; ++ i ; },non zero iterator
negative exit values are modulo <PLACE_HOLDER> :,for ( int exit : new int [ ] { - __num__ @$ - __num__ @$ - __num__ } ) { int expected = __num__ + exit ; string [ ] args = { __str__ @$ __str__ @$ __str__ + exit } ; bad exit status exception e = assert throws ( __str__ + expected @$ bad exit status exception . class @$ ( ) -> new command ( args ) . execute ( ) ) ; assert that ( e ) . has message that ( ) . is equal to ( __str__ + expected ) ; check command elements ( e @$ __str__ @$ __str__ @$ __str__ + exit ) ; termination status status = e . get result ( ) . get termination status (,values modulo hours
1 st root <PLACE_HOLDER>,if ( parent == null ) { long next = ( ( long ) phase << phase_shift ) | adj ; if ( state . compare and set ( this @$ s @$ next ) ) break ; } else { main lock . lock ( ) ; try { if ( state == s ) { parent . do register ( __num__ ) ; do { phase = ( int ) ( root . state > > > phase_shift ) ; } while ( ! state . compare and set ( this @$ state @$ ( ( long ) phase << phase_shift ) | adj ) ) ; break ; } } finally { main lock . unlock ( ) ; } },st root registration
next try the current class loader which loaded <PLACE_HOLDER> 6 s<PLACE_HOLDER>y,if ( result == null ) { result = p6 util . class . get class loader ( ) . get resource ( filename ) ; },which loaded file
add a new routes that will handle endpoints form wire tap route <PLACE_HOLDER> .,camel context . add routes ( new route builder ( ) { @ override public void configure ( ) throws exception { from ( __str__ ) . log ( __str__ ) ; from ( __str__ ) . log ( __str__ ) ; } } ) ;,wire tap api
view pager does not give its child views any callbacks when it moves content onto the screen @$ so we need to attach a listener to give us the <PLACE_HOLDER> that we require .,view parent view parent = litho view . get parent ( ) ; while ( view parent != null ) { if ( view parent instanceof view pager ) { final view pager view pager = ( view pager ) view parent ; final incremental mount helper . view pager listener view pager listener = new view pager listener ( m component tree @$ view pager ) ; try { view pager . add on page change listener ( view pager listener ) ; } catch ( concurrent modification exception e ) { view compat . post on animation ( view pager @$ new runnable ( ) { @ override public void run ( ) { view pager . add on page change listener ( view pager listener,pager give insets
shrink data buffer id <PLACE_HOLDER>,int [ ] new data buffer id table = new int [ new index count ] ; system . arraycopy ( data buffer id table @$ __num__ @$ new data buffer id table @$ __num__ @$ new data buffer id table . length ) ; data buffer id table = new data buffer id table ;,data buffer table
check to see if the cipher even supports the <PLACE_HOLDER> before trying to instantiate it .,try { if ( ! match attribute ( service @$ attribute_modes @$ tokenized transformation [ __num__ ] ) || ! match attribute ( service @$ attribute_paddings @$ tokenized transformation [ __num__ ] ) ) { return null ; } cipher spi and provider sap = new cipher spi and provider ( ( cipher spi ) service . new instance ( null ) @$ service . get provider ( ) ) ; if ( sap . cipher spi == null || sap . provider == null ) { return null ; } cipher spi spi = sap . cipher spi ; if ( ( ( type == need to set . mode ) || ( type == need to set . both ) ) && ( tokenized transformation [,cipher supports attributes
the rest api returns <PLACE_HOLDER> in decreasing order @$ but we want them in increasing order,collections . sort ( src versions ) ; for ( final integer src version : src versions ) { final versioned flow snapshot src flow snapshot = src client . get flow snapshot client ( ) . get ( src flow id @$ src version ) ; src flow snapshot . set flow ( null ) ; src flow snapshot . set bucket ( null ) ; final versioned flow snapshot metadata dest metadata = new versioned flow snapshot metadata ( ) ; dest metadata . set bucket identifier ( dest flow . get bucket identifier ( ) ) ; dest metadata . set flow identifier ( dest flow id ) ; dest metadata . set version ( src version ) ; dest metadata . set comments (,api returns versions
this is a delayed chat room <PLACE_HOLDER> @$ a history <PLACE_HOLDER> for the room coming from server . lets check have we already shown this <PLACE_HOLDER> and if this is the case skip it otherwise save it as last seen delayed <PLACE_HOLDER>,if ( last seen delayed message == null ) { string timestamp = configuration utils . get chat room property ( provider @$ get identifier ( ) @$ last_seen_delayed_message_prop ) ; try { last seen delayed message = new date ( long . parse long ( timestamp ) ) ; } catch ( throwable t ) { } },check shown message
a 'direct ' read actually has three phases . the first drains any remaining bytes from the slow read buffer . after this the read is guaranteed to be on a checksum chunk boundary . if there are still bytes to read @$ the fast direct path is used for as many remaining bytes as possible @$ up to a multiple of the checksum,if ( verify checksum ) { if ( slow read buff . has remaining ( ) ) { int from slow read buff = math . min ( buf . remaining ( ) @$ slow read buff . remaining ( ) ) ; write slice ( slow read buff @$ buf @$ from slow read buff ) ; n read += from slow read buff ; } if ( buf . remaining ( ) >= bytes per checksum && offset from chunk boundary == __num__ ) { int len = buf . remaining ( ) - ( buf . remaining ( ) % bytes per checksum ) ; len = math . min ( len @$ slow read buff . capacity ( ) ) ; int oldlimit =,which involves length
weighted multi work unit weighted queue @$ the job will add new work <PLACE_HOLDER> to the queue along with a weight for each work unit . the queue will take care of balancing the work <PLACE_HOLDER> amongst a set number of multi work <PLACE_HOLDER>,multi work unit weighted queue multi work unit weighted queue = new multi work unit weighted queue ( this . max work units per job ) ;,queue add units
check to see if the resulting graph contains the expected <PLACE_HOLDER>,structured graph replacement = get replacements ( ) . get substitution ( real method @$ - __num__ @$ false @$ null @$ graph . get options ( ) ) ; if ( replacement == null ) { assert in graph ( graph @$ expected node ) ; } option values options ; boolean need check node = true ; if ( java version util . java_spec <= __num__ ) { need check node = false ; } else { list < string > vm args = graal services . get input arguments ( ) ; assume . assume true ( vm args != null ) ; for ( string vm arg : vm args ) { if ( vm arg . equals ( disable_compactstrings_flag ) ) { need,graph contains node
not a scheme start <PLACE_HOLDER> .,return - __num__ ;,scheme start boundary
very special case to ensure that an indexed property descriptor does n't contain less <PLACE_HOLDER> than the enclosed property descriptor . if it does @$ then recreate as a property descriptor . see 4168833,if ( pd instanceof indexed property descriptor ) { ipd = ( indexed property descriptor ) pd ; if ( ipd . get indexed read method ( ) == null && ipd . get indexed write method ( ) == null ) { pd = new property descriptor ( ipd ) ; } },descriptor contain elements
we deliberately do not create the call node during parsing @$ because the call target is only created after the code entry is parsed . the code entry might not be yet parsed when we encounter this call . furthermore @$ if the call target is imported from another module @$ then that other module might not have been parsed yet . therefore @$,call nodes . add ( new wasm call stub node ( function ) ) ; break ;,wasm function implementation
playback will claim active <PLACE_HOLDER> . otherwise audio system will .,if ( device type == hdmi device info . device_playback ) { hdmi cec local device playback playback = playback ( ) ; playback . set is active source ( true ) ; playback . wake up if active source ( ) ; playback . may send active source ( source ) ; set active source ( playback . m address @$ physical address ) ; } if ( device type == hdmi device info . device_audio_system ) { hdmi cec local device audio system audio system = audio system ( ) ; if ( playback ( ) != null ) { audio system . set is active source ( false ) ; } else { audio system . set is active source ( true ) ; audio,playback claim source
this should n't ever be possible as annotation processor should generate empty <PLACE_HOLDER>,throw new runtime exception ( e ) ;,processor generate classes
this checks for the build file <PLACE_HOLDER> in the correct precedence order .,for ( root package path entry : pkg locator . get path entries ( ) ) { for ( build file name build file name : build files by priority ) { package lookup value result = get package lookup value ( env @$ package path entry @$ package key @$ build file name ) ; if ( result == null ) { return null ; } if ( result != package lookup value . no_build_file_value ) { return result ; } } },checks build names
when entity expansion <PLACE_HOLDER> is set by the application @$ we need to check for the entity expansion <PLACE_HOLDER> set by the parser @$ if number of entity expansions exceeds the entity expansion <PLACE_HOLDER> @$ parser will throw fatal error . note that this represents the nesting level of open entities .,f entity expansion count ++ ; if ( f limit analyzer != null ) { f limit analyzer . add value ( entity expansion index @$ name @$ __num__ ) ; } if ( f security manager != null && f security manager . is over limit ( entity expansion index @$ f limit analyzer ) ) { f security manager . debug print ( f limit analyzer ) ; f error reporter . report error ( xml message formatter . xml_domain @$ __str__ @$ new object [ ] { f security manager . get limit value by index ( entity expansion index ) } @$ xml error reporter . severity_fatal_error ) ; f entity expansion count = __num__ ; },number exceeds limit
first check to see if this plain old field map has <PLACE_HOLDER> to the actual type .,if ( result == null ) { if ( field map . get dest hint container ( ) != null ) { class < ? > dest hint type = field map . get dest hint type ( src field value . get class ( ) ) ; if ( dest hint type != null ) { dest field type = dest hint type ; } } string map id = field map . get map id ( ) ; class < ? > target class ; if ( field map . get dest hint container ( ) != null && field map . get dest hint container ( ) . get hint ( ) != null ) { target class = field map . get dest hint,map has access
indent:8 exp:12 warn indent:8 <PLACE_HOLDER>,try { } catch ( throwable t ) { system . identity hash code ( __str__ ) ; } finally { },exp:12 warn exp
first c<PLACE_HOLDER> contains <PLACE_HOLDER> @$ better use activate ? if you have providers,logger . debug ( __str__ @$ arrays . to string ( provider . get item names ( ) . to array ( ) ) ) ; for ( string item name : provider . get item names ( ) ) { bind gpio pin ( ( mcp3424 binding provider ) provider @$ item name ) ; },call contains all
the type flow of a field load is the type flow of the field itself . it accumulates all <PLACE_HOLDER> ever stored to the field .,if ( n instanceof load field node ) { load field node node = ( load field node ) n ; if ( is object ( node ) ) { register flow ( node @$ results . lookup field ( node . field ( ) ) ) ; } } else if ( n instanceof store field node ) { store field node node = ( store field node ) n ; if ( is object ( node . value ( ) ) ) { type flow field flow = results . lookup field ( node . field ( ) ) ; lookup flow ( node . value ( ) ) . add use ( field flow ) ; } } else if ( n instanceof return node,flow accumulates references
run streaming k means <PLACE_HOLDER> in parallel by spawning 1 thread per input path to process .,executor service pool = executors . new cached thread pool ( ) ; list < future < iterable < centroid > > > intermediate centroid futures = new array list < > ( ) ; for ( file status status : hadoop util . list status ( file system . get ( conf ) @$ input @$ path filters . logscrc filter ( ) ) ) { intermediate centroid futures . add ( pool . submit ( new streamingk means thread ( status . get path ( ) @$ conf ) ) ) ; } log . info ( __str__ ) ;,k means job
dispose the input <PLACE_HOLDER> after removing the window so the window manager does n't interpret the input <PLACE_HOLDER> being closed as an abnormal termination .,if ( m input channel != null ) { m input channel . dispose ( ) ; m input channel = null ; } m display manager . unregister display listener ( m display listener ) ; unschedule traversals ( ) ;,manager interpret channel
m vpns lock <PLACE_HOLDER> to be hold here to ensure that the active vpn can not be changed between these two calls .,synchronized ( m vpns ) { old blocked = is uid networking with vpn blocked ( nri . m uid @$ uid rules @$ old metered @$ old restrict background ) ; new blocked = is uid networking with vpn blocked ( nri . m uid @$ uid rules @$ new metered @$ new restrict background ) ; } if ( old blocked != new blocked ) { call callback for request ( nri @$ nai @$ connectivity manager . callback_blk_changed @$ encode bool ( new blocked ) ) ; },vpns lock state
the outbound buffer contains only one <PLACE_HOLDER> or it contains a file region .,object msg = in . current ( ) ; if ( msg instanceof byte buf ) { return write bytes ( in @$ ( byte buf ) msg ) ; } else if ( msg instanceof default file region ) { return write default file region ( in @$ ( default file region ) msg ) ; } else if ( msg instanceof file region ) { return write file region ( in @$ ( file region ) msg ) ; } else { throw new error ( ) ; },buffer contains message
sometimes most recent focus <PLACE_HOLDER> may be null @$ but focus <PLACE_HOLDER> is not e.g . we reset most recent focus <PLACE_HOLDER> if user removes focus <PLACE_HOLDER>,if ( focus owner == null ) { focus owner = keyboard focus manager . get current keyboard focus manager ( ) . get focus owner ( ) ; if ( focus owner != null && focus owner . get containing window ( ) != window ) { focus owner = null ; } },user removes owner
if the caller does not have <PLACE_HOLDER> to load the driver then skip it .,for ( driver info a driver : registered drivers ) { if ( is driver allowed ( a driver . driver @$ caller class loader ) ) { result . add element ( a driver . driver ) ; } else { println ( __str__ + a driver . get class ( ) . get name ( ) ) ; } },caller have permission
some exceptions rudely do n't allow cause <PLACE_HOLDER>,return new throwable ;,rudely allow exceptions
the other thread will now try to close the connection map but it will block because this thread has locked <PLACE_HOLDER> of the connections,await ( ) . until ( ( ) -> connection map . closing ) ; try { connection manager . borrow connection ( __num__ ) ; fail ( __str__ ) ; } catch ( cache closed exception e ) { },thread locked all
let the os choose the start <PLACE_HOLDER> of the region in memory,mmap ptr = os . mmap ( __num__ @$ mmap region size @$ os constants . prot_read @$ os constants . map_shared | os constants . map_populate @$ m fd @$ mmap file position ) ;,os choose size
test method java.util.zip.checked output <PLACE_HOLDER>,try { file output stream out file = new file output stream ( file . create temp file ( __str__ @$ __str__ ) ) ; checked output stream chk out = new checked output stream ( out file @$ new crc32 ( ) ) ; assert equals ( __str__ @$ __num__ @$ chk out . get checksum ( ) . get value ( ) ) ; out file . close ( ) ; } catch ( io exception e ) { fail ( __str__ ) ; } catch ( security exception e ) { fail ( __str__ ) ; },method java.util.zip.checked stream
let the remote peer know that we support rtcp xr in general and vo ip metrics report <PLACE_HOLDER> in particular .,string rtcpxr = md . get attribute ( rtcp extended report . sdp_attribute ) ; if ( rtcpxr == null ) { md . set attribute ( rtcp extended report . sdp_attribute @$ rtcp extended report . voip metrics report block . sdp_parameter ) ; } int ptime setting = sip activator . get configuration service ( ) . get int ( __str__ + __str__ @$ __num__ ) ;,xr report block
we do n't use preconditions.check argument because it requires boxing field id @$ which affects inner loop <PLACE_HOLDER>,if ( ! types [ field id ] . get java type ( ) . equals ( type ) ) { throw new illegal argument exception ( format ( __str__ @$ type @$ types [ field id ] @$ field id ) ) ; },which affects type
verify that both the shutdown command and the application finished <PLACE_HOLDER> .,output . should have exit value ( __num__ ) ; process thread . join and throw ( ) ; process thread . get output ( ) . should have exit value ( __num__ ) ;,command finished execution
when traversing dependencies for <PLACE_HOLDER> @$ we get <PLACE_HOLDER> attached to libraries that are statically linked @$ and <PLACE_HOLDER> attached to the initial bundle . this heuristic is based on the idea that the bundle holding the compiled code of a library also holds the <PLACE_HOLDER> .,case copying_include_shared_resources :,bundle holds resources
note : the lookup is enforcing security across users by making sure the caller can only access <PLACE_HOLDER> it hosts or provides .,widget widget = lookup widget locked ( app widget id @$ binder . get calling uid ( ) @$ calling package ) ; if ( widget == null ) { throw new illegal argument exception ( __str__ ) ; },caller access widgets
no luck with this constructor @$ let 's try another <PLACE_HOLDER>,throw new illegal state exception ( __str__ ) ;,'s try one
fake client as client.default follows <PLACE_HOLDER> .,test interface api = feign . builder ( ) . client ( ( request @$ options ) -> response ) . target ( test interface . class @$ __str__ + server . get port ( ) ) ; assert equals ( api . response ( ) . headers ( ) . get ( __str__ ) @$ collections . singleton list ( __str__ ) ) ;,client follows protocol
first round @$ only calculate those have a pa <PLACE_HOLDER>,for ( int i = __num__ ; i < e types . length ; i ++ ) { pa data . salt and params snp = pa data . get salt and params ( e types [ i ] @$ pa list ) ; if ( snp != null ) { if ( e types [ i ] != encrypted data . etype_arcfour_hmac && snp . salt != null ) { salt = snp . salt ; } result [ i ] = encryption key . acquire secret key ( cname @$ password @$ e types [ i ] @$ snp ) ; } },round have list
make sure the response has at least 1 <PLACE_HOLDER> with a valid dependency id,if ( response . get table count ( ) == __num__ ) { response . add dependency ( new dependency pair . buffer dependency pair ( m_fragment msg . get output dep id ( __num__ ) @$ m_raw dummy result @$ __num__ @$ m_raw dummy result . length ) ) ; },response has result
page views . do this once we have the right padding <PLACE_HOLDER> from above .,for ( int i = __num__ ; i < count ; i ++ ) { final view child = get child at ( i ) ; if ( child . get visibility ( ) == gone ) { continue ; } final layout params lp = ( layout params ) child . get layout params ( ) ; if ( lp . is decor ) { continue ; } final item info ii = info for child ( child ) ; if ( ii == null ) { continue ; } if ( lp . needs measure ) { lp . needs measure = false ; final int width spec = measure spec . make measure spec ( ( int ) ( child width * lp . width,views have offsets
key size | expected <PLACE_HOLDER>,should put and get key size ( __num__ @$ __num__ ) ; should put and get key size ( key_one_byte_max @$ __num__ ) ; should put and get key size ( key_two_byte_min @$ __num__ ) ; should put and get key size ( key_two_byte_max @$ __num__ ) ; should put and get key size ( key_two_byte_no_offload_max @$ __num__ ) ;,| expected length
predicate may contain any <PLACE_HOLDER> .,for ( string col name : serde . column names ) { analyzer . allow column name ( col name ) ; } list < index search condition > search conditions = new linked list < index search condition > ( ) ; expr node desc residual = analyzer . analyze predicate ( predicate @$ search conditions ) ; decomposed predicate decomposed = new decomposed predicate ( ) ; decomposed . pushed predicate = analyzer . translate search conditions ( search conditions ) ; decomposed . residual predicate = ( expr node generic func desc ) residual ; return decomposed ;,predicate contain column
auto expand only support when each cell occupy one <PLACE_HOLDER> .,if ( ! laying out in primary direction && remaining span == __num__ && ( count == consumed span count ) && range style . m is auto expand ) { if ( layout in vertical ) { range style . m size per span = ( m total size - ( count - __num__ ) * range style . mh gap ) / count ; } else { range style . m size per span = ( m total size - ( count - __num__ ) * range style . mv gap ) / count ; } },cell occupy span
restart and gii @$ it should be full gii because number of unfinished operations exceeds <PLACE_HOLDER>,change unfinished operation limit ( r @$ __num__ ) ; create distributed region ( r ) ;,number exceeds limit
empty constructor should produce a new <PLACE_HOLDER> of adapter delegates manager,list delegation adapter < list < object > > adapter = new list delegation adapter < list < object > > ( ) { @ override public int get item count ( ) { assert . assert not null ( this . delegates manager ) ; return __num__ ; } } ;,constructor produce instance
case 9 : true if an attribute named integer att name of type integer has the <PLACE_HOLDER> integer <PLACE_HOLDER> we cover javax.management.binary rel query exp with a rel op equal to eq and javax.management.numeric <PLACE_HOLDER> exp,queries . add ( query . eq ( query . attr ( integer att name ) @$ query . value ( integer value ) ) ) ;,name has value
we expect this to throw an io exception since we 're faking socket connect <PLACE_HOLDER> every time,learner . connect to leader ( new multiple addresses ( addr ) @$ __str__ ) ;,time connect errors
sets payload to empty as frame contains no <PLACE_HOLDER>,frame builder . payload ( new byte [ ] { } ) ; curr bytes . reset ( ) ; curr state = beats state . complete ; window size = frame builder . data size ; break ;,frame contains data
hider and hidden needs to be field @$ method or type and fields hide fields @$ types hide types @$ methods hide methods ie a field does n't hide a methods <PLACE_HOLDER>,element kind hider kind = hider . get kind ( ) ; element kind hidden kind = hidden . get kind ( ) ; if ( hider kind . is field ( ) && ! hidden kind . is field ( ) ) { return false ; } else if ( hider kind . is class ( ) && ! ( hidden kind . is class ( ) || hidden kind . is interface ( ) ) ) { return false ; } else if ( hider kind . is interface ( ) && ! ( hidden kind . is class ( ) || hidden kind . is interface ( ) ) ) { return false ; } else if ( hider kind == element kind . method,field hide etc
load apdex satisfied <PLACE_HOLDER>,final long apdex satisfied threshold = get required property ( props @$ report_generator_key_apdex_satisfied_threshold @$ report_generator_key_apdex_satisfied_threshold_default @$ long . class ) ; configuration . set apdex satisfied threshold ( apdex satisfied threshold ) ;,apdex satisfied threshold
5 minutes in milliseconds create a <PLACE_HOLDER> from the arguments,config config = new config ( ) ; config . parse ( async benchmark . class . get name ( ) @$ args ) ; system . out . print ( horizontal_rule ) ; log . info ( __str__ ) ; log . info ( horizontal_rule ) ; log . info ( config . get config dump string ( ) ) ; if ( config . latencyreport ) { log . info ( __str__ ) ; },minutes create configuration
... but the three requests that follow requests include an authorization <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { request = server . take request ( ) ; assert equals ( __str__ @$ request . get request line ( ) ) ; assert contains ( request . get headers ( ) @$ __str__ + simple authenticator . base_64_credentials ) ; },requests include header
using factory get an <PLACE_HOLDER> of document builder,try { document builder db = dbf . new document builder ( ) ; document doc = db . parse ( is ) ; element doc ele = doc . get document element ( ) ; node list nl = doc ele . get elements by tag name ( __str__ ) ; if ( nl != null && nl . get length ( ) > __num__ ) { element el = ( element ) nl . item ( __num__ ) ; string reason = get text value from attribute ( el @$ __str__ ) ; throw new logout notification exception ( __str__ + reason ) ; } nl = doc ele . get elements by tag name ( __str__ ) ; if ( nl != null && nl,factory get instance
user trigger <PLACE_HOLDER> to tr<PLACE_HOLDER>nsfer token to b,string param = __str__ + base58 . encode58 check ( transfer token contract address ) + __str__ + asset account id . to string utf8 ( ) + __str__ ; final string trigger txid = public methed . trigger contract ( transfer token contract address @$ __str__ @$ param @$ false @$ __num__ @$ __num__ @$ __str__ @$ __num__ @$ user001 address @$ user001 key @$ blocking stub full ) ; public methed . wait produce next block ( blocking stub full ) ; account infoafter = public methed . query account ( user001 address @$ blocking stub full ) ; account resource message resource infoafter = public methed . get account resource ( user001 address @$ blocking stub full ) ; long after balance = infoafter .,user trigger a
test that all subtasks are taken into the account for the summary . the correctness of the actual results is checked in the test of the min max avg <PLACE_HOLDER> .,task state stats . task state stats summary summary = task stats . get summary stats ( ) ; assert equals ( subtasks . length @$ summary . get state size stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get ack timestamp stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get sync checkpoint duration stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get async checkpoint duration stats ( ) . get count ( ) ) ; assert equals ( subtasks . length @$ summary . get alignment buffered stats ( ) . get count (,test max value
when the route has empty producible types @$ the result has the highest <PLACE_HOLDER> .,assert that ( get result . has highest score ( ) ) . is true ( ) ; assert that ( get result . negotiated response media type ( ) ) . is null ( ) ;,route has score
if user edit <PLACE_HOLDER> or port @$ we have to find the row with the previous value,if ( is edit mode ) { stun server = get stun server ( previous server @$ previous port ) ; } else { stun server = get stun server ( address @$ port ) ; },user edit host
determine container and cache names using legacy <PLACE_HOLDER>,string replication config cache name = ( config != null ) ? config . get cache name ( ) : null ; service name replication config service name = service name factory . parse service name ( ( replication config cache name != null ) ? replication config cache name : __str__ ) ; service name base replication config service name = service name . jboss . append ( __str__ ) ; if ( ! base replication config service name . is parent of ( replication config service name ) ) { replication config service name = base replication config service name . append ( replication config service name ) ; } string container name = ( ( replication config service name . length ( ) > __num__,names using behavior
even though we 've added a <PLACE_HOLDER> @$ the original list should have an empty row added to make it the same size,assert equals ( field list . size ( ) @$ other field list . size ( ) ) ; for ( int i = __num__ ; i < field list . size ( ) ; i ++ ) { data type line data type line = ( data type line ) field list . get ( i ) ; data type line other data type line = ( data type line ) other field list . get ( i ) ; if ( i == ( insert indexb + __num__ ) ) { assert true ( data type line instanceof empty data type line ) ; assert not null ( other data type line . get name color ( ) ) ; assert not null ( other data,size added field
get the lease renewer now so we can verify it later without calling get lease renewer @$ which will automatically add the <PLACE_HOLDER> into it .,final lease renewer lease renewer = dfs . get client ( ) . get lease renewer ( ) ; stream . write ( __str__ . get bytes ( ) ) ; try { stream . hflush ( ) ; fail ( __str__ ) ; } catch ( ds quota exceeded exception expected ) { },which add crash
set the default max thread <PLACE_HOLDER> to 100 to limit the <PLACE_HOLDER> of concurrent requests so that rest server does n't oom easily . jetty set the default max thread <PLACE_HOLDER> to 250 @$ if we do n't set it . our default min thread <PLACE_HOLDER> 2 is the same as that used by jetty .,int max threads = servlet . get configuration ( ) . get int ( rest_thread_pool_threads_max @$ __num__ ) ; int min threads = servlet . get configuration ( ) . get int ( rest_thread_pool_threads_min @$ __num__ ) ;,jetty set number
map the seek <PLACE_HOLDER> to a <PLACE_HOLDER> in the corresponding timeline .,pair < object @$ long > period position ; try { period position = seek timeline . get period position ( window @$ period @$ seek position . window index @$ seek position . window position us ) ; } catch ( index out of bounds exception e ) { return null ; },the seek position
the order in which the partition by keys are listed should not radically change the plan <PLACE_HOLDER> .,windowed query = __str__ ; validate windowed function plan ( windowed query @$ __num__ @$ __num__ @$ __num__ @$ expression type . aggregate_windowed_rank ) ;,order change structure
ensure that per rfc 6265 @$ cookie.value follows syntax <PLACE_HOLDER>,syntax . require validrfc6265 cookie value ( _value ) ;,cookie.value follows declaration
see which urls the pws did n't give us a <PLACE_HOLDER> for .,set < string > missed = new hash set < > ( broadcast urls ) ; missed . remove all ( found urls ) ; for ( string url : missed ) { pws result callback . on pws result absent ( url ) ; } record response ( ) ; pws result callback . on pws result error ( broadcast urls @$ response code @$ e ) ;,pws give look
yay ! let 's extract the port <PLACE_HOLDER>,string s = m . group ( __num__ ) ; port = integer . parse int ( s ) ; inet address add = server . get inet address ( ) ; if ( add != null ) { dest = new inet socket address ( add @$ port ) ; } else { dest = inet socket address . create unresolved ( server addr . get host name ( ) @$ port ) ; },'s extract name
if cipher is null @$ get random <PLACE_HOLDER> failed @$ which means encryption is meaningless . therefore @$ do not save anything . this will cause users to lose incognito state in certain cases . that is annoying @$ but is better than failing to provide the guarantee of incognito mode .,return ;,failed get bytes
calls on monitors from child should reach both <PLACE_HOLDER>,my monitor child monitor = child . new monitor ( my monitor . class ) ; child monitor . a void ( ) ; mockito . verify ( parent listener @$ mockito . times ( __num__ ) ) . a void ( ) ; mockito . verify ( child listener ) . a void ( ) ;,calls reach listeners
let 's create <PLACE_HOLDER>,data . sftpclient . create folder ( spool directory ) ; if ( is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ spool directory ) ) ; },'s create folder
the wire protocol requires an <PLACE_HOLDER> of keys .,char sequence [ ] sequence = { key to release } ; executor . execute ( driver command . send_keys_to_active_element @$ immutable map . of ( __str__ @$ sequence ) ) ;,protocol requires array
be aware that probe index builder will not clear its <PLACE_HOLDER>,probe index builder . clear ( ) ; build page builder . reset ( ) ; estimated probe block bytes = __num__ ; is sequential probe indices = true ;,builder clear data
process entries in vx works symbol <PLACE_HOLDER>,address vx sym tbl = vx num sym entries addr . subtract ( vx num sym entries * sym_entry_size ) ; for ( int i = __num__ ; i < vx num sym entries ; i ++ ) { if ( monitor . is cancelled ( ) ) { return ; } println ( __str__ + i ) ; address sym entry = vx sym tbl . add ( i * sym_entry_size ) ; address sym name addr = to addr ( mem . get int ( sym entry . add ( sym_name_off ) ) ) ; address sym loc addr = to addr ( mem . get int ( sym entry . add ( sym_loc_off ) ) ) ; byte sym type = mem . get byte,entries works table
count variational numbers . every next one either has the same <PLACE_HOLDER> or previous one plus probability of loss .,for ( int i = __num__ ; i < losses . length ; i ++ ) if ( i == __num__ ) loss probs [ i ] = get loss probability ( losses @$ __num__ ) ; else if ( losses [ i ] != losses [ i - __num__ ] ) loss probs [ i ] = get loss probability ( losses @$ i ) + loss probs [ i - __num__ ] ; else loss probs [ i ] = loss probs [ i - __num__ ] ;,one has value
close req 1 and make sure req 2 does not affect num active <PLACE_HOLDER> .,actual res1 . close ( ) ; await ( ) . until asserted ( ( ) -> assert that ( client . num active requests ( ) ) . is zero ( ) ) ;,req affect requests
okay @$ ` former method ' and ` method ' both have the same <PLACE_HOLDER> . see if they are compatible .,all methods . replace ( method ) ;,method have signature
use regular expressions to filter the query result retrieve the attributes needed retrieve the mbeans object <PLACE_HOLDER> specified by the query,return new m beans attribute query filter ( jmx connection @$ attributes @$ new m beans object name query filter ( jmx connection ) ) ;,result retrieve names
the following trick leverages the <PLACE_HOLDER> of a record writer via the job thus supporting arbitrary output formats .,job job = job . get instance ( context . get configuration ( ) ) ; job . set output format class ( get named output format class ( context @$ name output ) ) ; job . set output key class ( get named output key class ( context @$ name output ) ) ; job . set output value class ( get named output value class ( context @$ name output ) ) ; task context = new task attempt context impl ( job . get configuration ( ) @$ context . get task attemptid ( ) @$ new wrapped status reporter ( context ) ) ; task contexts . put ( name output @$ task context ) ; return task context ;,trick leverages instantiation
took header does not make <PLACE_HOLDER> as we stream,return response . ok ( out ) . build ( ) ;,header make sense
a long does not match empty <PLACE_HOLDER>,assert filter matches skip vectorize ( edf ( __str__ ) @$ immutable list . of ( ) ) ;,long match string
if this was the last listener that was registered with us then no long need to have a delegator registered with the call peer <PLACE_HOLDER> handlers . we therefore remove it so that audio level calculations would be ceased .,if ( ( local user audio level listeners == null ) || local user audio level listeners . is empty ( ) ) { iterator < t > call peer iter = get call peers ( ) ; while ( call peer iter . has next ( ) ) { call peer iter . next ( ) . get media handler ( ) . set local user audio level listener ( null ) ; } },handlers peer volume
if write id list is not null @$ that means <PLACE_HOLDER> are requested within a txn context . so set <PLACE_HOLDER> compliant to false @$ if are txn <PLACE_HOLDER> supported is false or the write id which has updated the <PLACE_HOLDER> in not compatible with write id list . this is done within table lock as the number of partitions may be more than,list < column statistics > column statistics = shared cache . get partition col stats list from cache ( cat name @$ db name @$ tbl name @$ part names @$ col names @$ write id list @$ are txn stats supported ) ; if ( column statistics == null ) { return raw store . get partition column statistics ( cat name @$ db name @$ tbl name @$ part names @$ col names @$ engine @$ write id list ) ; } return column statistics ;,which updated stats
ticket set to anonymous for anonymous user . simplify <PLACE_HOLDER> .,subject current user = org . apache . shiro . security utils . get subject ( ) ; if ( current user . is authenticated ( ) ) { current user . logout ( ) ; } log . debug ( __str__ + current user ) ; if ( ! current user . is authenticated ( ) ) { username password token token = new username password token ( user name @$ password ) ; response = proceed to login ( current user @$ token ) ; } if ( response == null ) { response = new json response ( response . status . forbidden @$ __str__ @$ __str__ ) ; } log . warn ( response . to string ( ) ) ; return response .,ticket simplify testing
each instance should have 18 <PLACE_HOLDER> assigned,num segments assigned per instance = segment assignment utils . get num segments assigned per instance ( new assignment @$ new instances ) ; expected num segments assigned per instance = new int [ new num instances ] ; new num segments per instance = num segments * num_replicas / new num instances ; arrays . fill ( expected num segments assigned per instance @$ new num segments per instance ) ; assert equals ( num segments assigned per instance @$ expected num segments assigned per instance ) ;,instance have segments
repository objects take priority so let 's overwrite <PLACE_HOLDER> ...,read databases ( trans meta @$ true ) ; read partition schemas ( trans meta @$ true ) ; read slaves ( trans meta @$ true ) ; read clusters ( trans meta @$ true ) ; return trans meta . get shared objects ( ) ;,'s overwrite them
key a was added first and key b has id <PLACE_HOLDER>,assert equals ( __num__ @$ key set utils . get key set ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get pub key ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get key set ref count ( m ksms @$ __num__ ) ) ; assert equals ( __num__ @$ key set utils . get pub key ref count ( m ksms @$ __num__ ) ) ; assert equals ( keyb @$ key set utils . get pub key ( m ksms @$ __num__ ) ) ; mapping = ks mapping . get ( __num__ ) ; assert equals ( __num__ @$ mapping . size ( ),key id 1
a newly created one from the source should have a different <PLACE_HOLDER> .,recoverable js ast ast2 = new recoverable js ast ( real ast @$ true ) ; check compile ( real ast @$ make defensive copy ( ast2 ) @$ __str__ ) ;,one have type
verify that the class does not also define a <PLACE_HOLDER> with the same stem name with 'is ',try { final method get method = container class . get declared method ( __str__ + stem name ) ; if ( ! modifier . is static ( get method . get modifiers ( ) ) && get method . get annotation ( transient . class ) == null ) { check get and is variants ( container class @$ property name @$ get method @$ is method ) ; } } catch ( no such method exception ignore ) { },class define method
children only represent configuration <PLACE_HOLDER> of the parent @$ and are not independent entities,return false ;,children represent parts
make the icon change between media icon and switch field icon depending on whether editing note <PLACE_HOLDER>,if ( edit model mode && allow field remapping ( ) ) { media button . set background resource ( icons [ __num__ ] ) ; set remap button listener ( media button @$ i ) ; } else if ( edit model mode && ! allow field remapping ( ) ) { media button . set background resource ( __num__ ) ; } else { media button . set background resource ( icons [ __num__ ] ) ; setmm button listener ( media button @$ i ) ; },editing note muted
and now pretend that the remote contact has granted us <PLACE_HOLDER>,return new authorization response ( authorization response . accept @$ __str__ ) ;,contact granted authorization
note : this is crazy . epoch <PLACE_HOLDER> up @$ but time format down we maintain this behavior for backward compatibility .,long start units = ( long ) math . ceil ( start . get millis ( ) ) / __num__ ; long end units = ( long ) math . ceil ( end exclusive . get millis ( ) ) / __num__ ; if ( objects . equals ( start units @$ end units ) ) { return string . format ( __str__ @$ get to unix time clause ( time format @$ time field @$ source name ) @$ start units ) ; } return string . format ( __str__ @$ get to unix time clause ( time format @$ time field @$ source name ) @$ start units @$ end units ) ;,note epoch rounds
test that the module map content contains the individual <PLACE_HOLDER> inside the header tree artifact .,assert that ( module map content ) . contains ( __str__ ) ; assert that ( umbrella header content ) . contains ( headers . get exec path string ( ) + __str__ ) ; assert that ( umbrella header content ) . contains ( headers . get exec path string ( ) + __str__ ) ;,content contains headers
emit a copy files <PLACE_HOLDER> for each destination .,for ( copy file phase destination spec destination spec : rule by destination spec . key set ( ) ) { iterable < target node < ? > > target nodes = rule by destination spec . get ( destination spec ) ; phases . add ( get single copy files build phase ( destination spec @$ target nodes ) ) ; } return phases . build ( ) ;,copy files phase
now <PLACE_HOLDER> that using different translates with lbm is ok this <PLACE_HOLDER> does n't prove a lot since showing a leak really requires a basher <PLACE_HOLDER> that can run for a long time .,for ( int x = __num__ ; x < __num__ ; x ++ ) { for ( int y = __num__ ; y < __num__ ; y ++ ) { attributed character iterator aci = van gogh . get iterator ( ) ; affine transform tx = affine transform . get translate instance ( x @$ y ) ; font render context frc = new font render context ( tx @$ false @$ false ) ; line break measurer lbm = new line break measurer ( aci @$ frc ) ; lbm . set position ( aci . get begin index ( ) ) ; while ( lbm . get position ( ) < aci . get end index ( ) ) { lbm . next layout (,test requires test
now if we found a region load get the <PLACE_HOLDER> of cost that was requested .,if ( region load list != null ) { cost = ( long ) ( cost + get region load cost ( region load list ) ) ; },load get value
if no <PLACE_HOLDER> in the holder @$ or if the holder has different <PLACE_HOLDER> loaded @$ then load the configuration and set the new <PLACE_HOLDER> in the holder,if ( resources == null || ! config resources . equals ( resources . get config resources ( ) ) ) { get logger ( ) . debug ( __str__ ) ; final configuration config = new extended configuration ( get logger ( ) ) ; config . set class loader ( thread . current thread ( ) . get context class loader ( ) ) ; resources = new validation resources ( config resources @$ get configuration from resources ( config @$ config resources ) ) ; validation resource holder . set ( resources ) ; } final configuration conf = resources . get configuration ( ) ; results . add all ( kerberos properties . validate principal and keytab ( this . get class ( ),holder has resources
needed because this class overrides pull next <PLACE_HOLDER> @$ move down .,move down ( ) ;,overrides pull time
file . we 're going to check the consistency of the resource file while building this mapping @$ and throw errors if the file does not meet our <PLACE_HOLDER> .,map < string @$ collection < string > > lines = new hash map < > ( ) ; final check charset mapping mapping = new check charset mapping ( ) ; for ( string key : props . string property names ( ) ) { collection < string > values = get values ( props . get property ( key ) ) ; lines . put ( key @$ values ) ; mapping . add mapping ( key @$ values ) ; },file meet requirements
big query requires <PLACE_HOLDER> so we have to divide here,row . put ( __str__ @$ now in millis / __num__ ) ; client . insert row ( row @$ schema @$ table name ) ;,query requires integer
after terminate suppressed <PLACE_HOLDER> which itself suppressed original err,assert equals ( __num__ @$ after terminate . get suppressed ( ) . length ) ; assert equals ( error @$ after terminate . get suppressed ( ) [ __num__ ] ) ; assert equals ( __num__ @$ error . get suppressed ( ) . length ) ; assert equals ( err @$ error . get suppressed ( ) [ __num__ ] ) ;,itself suppressed error
copy the <PLACE_HOLDER> of the transformation to the step ... do n't share . each copy of the step has its own <PLACE_HOLDER> .,step . initialize variables from ( this ) ; step . set using thread priority managment ( trans meta . is using thread priority managment ( ) ) ;,copy has variables
create a table with a primary key named 'name ' @$ which holds a <PLACE_HOLDER>,try { create table ( ) ; describe table ( ) ; put item ( new item ( __str__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) ; put item ( new item ( __str__ @$ __num__ @$ __str__ @$ __str__ @$ __str__ ) ) ; get item ( __str__ ) ; get item ( __str__ ) ; map < string @$ condition > scan filter = new hash map < string @$ condition > ( ) ; condition condition = new condition ( ) . with comparison operator ( comparison operator . gt . to string ( ) ) . with attribute value list ( new attribute value ( ) . withn ( __str__ ) ) ; scan filter . put ( __str__ @$ condition,which holds string
<PLACE_HOLDER> 1 should always beat <PLACE_HOLDER> 2 @$ all other things being equal .,assert equals ( expected phone1 scored suggestion @$ m time zone detector strategy . find best phone suggestion for tests ( ) ) ;,phone beat phone
when process is creating a lot of connections this can take some time so increase the <PLACE_HOLDER>,if ( holder == null ) { holder = new connection holder ( this @$ the serverid @$ options ) ; connection holder prev holder = connections . put if absent ( the serverid @$ holder ) ; if ( prev holder != null ) { holder = prev holder ; } else { holder . connect ( ) ; } },time increase timeout
this call triggers j <PLACE_HOLDER> to load our spy,new spy go to helper ( ) ; tool . get options ( __str__ ) . set enum ( __str__ @$ navigation options . external navigation enum . navigate to external program ) ;,call triggers mockit
snapshot has no <PLACE_HOLDER> after flush,int number of mem scanners after flush = input cells after snapshot . is empty ( ) ? __num__ : __num__ ; boolean more ; int cell count = __num__ ; do { list < cell > cells = new array list < > ( ) ; more = s . next ( cells ) ; cell count += cells . size ( ) ; assert equals ( more ? number of mem scanners after flush : __num__ @$ count mem store scanner ( s ) ) ; } while ( more ) ; assert equals ( __str__ + input cells before snapshot . size ( ) + __str__ + input cells after snapshot . size ( ) @$ input cells before snapshot . size ( ) +,snapshot has scanners
merge similar styles . doing so will improve <PLACE_HOLDER> .,last style = styles . get ( styles . size ( ) - __num__ ) ; if ( last style . similar to ( style ) && ( last style . start + last style . length == style . start ) ) { last style . length += style . length ; } else { styles . add ( style ) ; },styles improve performance
if it is not a global @$ it might be accessing a local of the outer <PLACE_HOLDER> . if that 's the case the functions between the variable 's declaring <PLACE_HOLDER> and the variable reference <PLACE_HOLDER> can not be moved .,if ( var . get scope ( ) != t . get scope ( ) ) { for ( name context context : symbol stack ) { if ( context . scope == var . get scope ( ) ) { break ; } context . name . read closure variables = true ; } },functions declaring scope
if at least one has some <PLACE_HOLDER> @$ mergee has some too page buffer client statuses may be long @$ so we do n't want to combine the lists,return new exchange client status ( ( buffered bytes + other . buffered bytes ) / __num__ @$ math . max ( max buffered bytes @$ other . max buffered bytes ) @$ merge avgs ( average bytes per request @$ successful requests count @$ other . average bytes per request @$ other . successful requests count ) @$ successful requests count + other . successful requests count @$ buffered pages + other . buffered pages @$ no more locations && other . no more locations @$ immutable list . of ( ) ) ;,mergee has locations
the server side will write <PLACE_HOLDER> @$ and in order to proceed with the initial tls handshake we need to start reading before waiting for the callback .,byte [ ] buffer = new byte [ __num__ ] ; int len = client . get input stream ( ) . read ( buffer ) ; assert equals ( __str__ @$ new string ( buffer @$ __num__ @$ len @$ standard charsets . utf_8 ) ) ; assert null ( _write callback . get ( __num__ @$ time unit . seconds ) ) ;,side write something
if instruction has fall <PLACE_HOLDER>,address fall thru = null ; if ( instr . has fallthrough ( ) ) { if ( check non returning ( program @$ flow type @$ instr ) ) { target = get next target ( body @$ untried target list ) ; repeat instruction byte tracker . reset ( ) ; continue ; } new target = instr . get fall through ( ) ; fall thru = new target ; } else { address next addr = max addr . next ( ) ; if ( target list . contains ( next addr ) ) { new target = next addr ; } else if ( flow type . is jump ( ) ) { address flows [ ] = instr . get flows (,instruction has thru
native library merge code <PLACE_HOLDER>,android binary graph enhancer graph enhancer = new android binary graph enhancer ( toolchain provider @$ context . get cell path resolver ( ) @$ build target @$ context . get project filesystem ( ) @$ android platform target @$ params @$ graph builder @$ args . get aapt mode ( ) @$ immutable list . of ( ) @$ resource compression mode . disabled @$ filter resources steps . resource filter . empty_filter @$ enum set . none of ( r type . class ) @$ optional . empty ( ) @$ optional . empty ( ) @$ immutable set . of ( ) @$ null @$ args . get manifest ( ) @$ args . get manifest skeleton ( ) @$ optional . empty ( ),library merge generator
0 x 100248 f : p 1 and p 2 have same function <PLACE_HOLDER> .,program builder1 . create function comment ( __str__ @$ __str__ ) ; program builder2 . create function comment ( __str__ @$ __str__ ) ; check no comment difference ( ) ;,1 have comment
without the name @$ the suppression node increments the topology <PLACE_HOLDER>,assert that ( anonymous node topology @$ is ( anonymous_intermediate_topology ) ) ;,node increments index
do n't need to clone these @$ since the trigger context does n't allow <PLACE_HOLDER>,for ( map . entry < w @$ value state < bit set > > entry : state . access in each merging window ( finished_bits_tag ) . entry set ( ) ) { builder . put ( entry . get key ( ) @$ read finished bits ( entry . get value ( ) ) ) ; clear finished bits ( entry . get value ( ) ) ; },context allow them
add the tail string which contains no <PLACE_HOLDER> and return the result .,sbuf . append ( message pattern . substring ( i @$ message pattern . length ( ) ) ) ; return sbuf . to string ( ) ;,which contains variables
' : ' is an indicator <PLACE_HOLDER> extensible rules,if ( filter [ i ] == __str__ && ftype == ldap_filter_ext ) { if ( filter [ i - __num__ ] == __str__ ) { throw new invalid search filter exception ( __str__ ) ; } extensible start = i ; break ; },' indicator of
replace spaces because importer ca n't read attributes <PLACE_HOLDER> in quotes,writer . append ( __str__ ) ; edge iterable edge iterable = graph . get edges ( ) ; for ( edge edge : edge iterable ) { print edge data ( edge @$ edge . get source ( ) @$ edge . get target ( ) @$ graph ) ; if ( ! edge . is directed ( ) && ! edge . is self loop ( ) ) { print edge data ( edge @$ edge . get target ( ) @$ edge . get source ( ) @$ graph ) ; } progress . progress ( progress ticket ) ; if ( cancel ) { edge iterable . do break ( ) ; return ; } },importer read them
pmd can not resolve array length <PLACE_HOLDER> @$ but only the,if ( expressions . size ( ) == number constants . integer_size_or_length_2 ) { ast primary expression left = expressions . get ( __num__ ) ; ast primary expression right = expressions . get ( __num__ ) ; boolean both array length = is array length ( left ) && is array length ( right ) ; boolean both wrapper type = node utils . is wrapper type ( left ) && node utils . is wrapper type ( right ) ; if ( ! both array length && both wrapper type ) { add violation with message ( data @$ node @$ __str__ ) ; } },pmd resolve predicates
the destination will contain a matching <PLACE_HOLDER> which does not contain a null value which we do n't want to overwrite .,child src child1 = new child ( __num__ @$ __str__ @$ null ) ; parent with child list src = new parent with child list ( src child1 @$ new child ( __num__ @$ __str__ @$ __str__ ) ) ; parent with child list dest = new parent with child list ( new child ( __num__ @$ __str__ @$ __str__ ) @$ new child ( __num__ @$ __str__ @$ __str__ ) ) ;,destination contain child
if true @$ returns corrupt <PLACE_HOLDER> instead of correct protobufs .,boolean poisoned = false ; if ( worker options . poison after > __num__ && work unit counter > worker options . poison after ) { poisoned = true ; } if ( poisoned && worker options . hard poison ) { system . err . println ( __str__ ) ; system . exit ( __num__ ) ; } else { int exit code = __num__ ; try { options parser parser = parser helper ( request . get arguments list ( ) ) ; example work multiplexer options options = parser . get options ( example work multiplexer options . class ) ; if ( options . write counter ) { counter output = work unit counter ++ ; } results . add ( executor service .,returns corrupt results
if no application can handle the <PLACE_HOLDER> @$ assume that the web view can handle it .,return overriding url loading ;,application handle url
let 's use simple baseline value @$ arbitrary date in gmt @$ using the standard <PLACE_HOLDER>,string input str = __str__ ; date input date = mapper . read value ( __str__ + input str + __str__ @$ java . util . date . class ) ; calendar c = calendar . get instance ( time zone . get time zone ( __str__ ) ) ; c . set time ( input date ) ; assert equals ( __num__ @$ c . get ( calendar . year ) ) ; assert equals ( calendar . december @$ c . get ( calendar . month ) ) ; assert equals ( __num__ @$ c . get ( calendar . day_of_month ) ) ;,value using test
v \u 00 f 2 \u 00 cd z \u 00 b 8 \u 0002 \u <PLACE_HOLDER> \u 0004 j \u <PLACE_HOLDER>,byte [ ] template = { ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ ( byte ) __num__ @$ __num__ @$ ( byte ) __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__,j \u 0000
check if current wait <PLACE_HOLDER> exceeds the max wait <PLACE_HOLDER>,long current wait time ; site tasker next task = m_tasks . peek ( ) ; if ( next task == null ) { current wait time = __num__ ; } else { current wait time = current time - next task . get queue offer time ( ) ; },max wait time
both should have the same <PLACE_HOLDER> of servers .,assert equals ( first balancer . get no of servers ( ) @$ second balancer . get no of servers ( ) ) ;,both have number
this constructor delays dns <PLACE_HOLDER> to detect changes,channel builder = netty channel builder . for address ( inet server address . get host name ( ) @$ inet server address . get port ( ) ) ; channel builder = netty channel builder . for address ( address ) ;,delays dns lookup
perfect match @$ just shove the replacement <PLACE_HOLDER> in .,if ( text1 . equals ( text2 ) ) { text = text . substring ( __num__ @$ start_loc ) + diff_text2 ( a patch . diffs ) + text . substring ( start_loc + text1 . length ( ) ) ; } else { linked list < diff > diffs = diff_main ( text1 @$ text2 @$ false ) ; if ( text1 . length ( ) > this . match_ max bits && diff_levenshtein ( diffs ) / ( float ) text1 . length ( ) > this . patch_ delete threshold ) { results [ x ] = false ; } else { diff_cleanup semantic lossless ( diffs ) ; int index1 = __num__ ; for ( diff a diff : a patch . diffs,match shove text
if the post does n't have location <PLACE_HOLDER> @$ show the picker directly,if ( ! get edit post repository ( ) . has location ( ) ) { show location picker ( ) ; return ; },post have info
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( testjdbc connection fail . class ) ;,suite using tests
check that the starting <PLACE_HOLDER> occurs before the started <PLACE_HOLDER>,assert true ( listener . starting time <= listener . started time @$ __str__ ) ;,the started time
as tomcat can not specify the provider <PLACE_HOLDER> in the configuration . it 'll go into this path,if ( secret provider == null ) { string config prefix = filter config . get init parameter ( config_prefix ) ; config prefix = ( config prefix != null ) ? config prefix + __str__ : __str__ ; try { secret provider = authentication filter . construct secret provider ( filter config . get servlet context ( ) @$ super . get configuration ( config prefix @$ filter config ) @$ false ) ; this . is initialized by tomcat = true ; } catch ( exception ex ) { throw new servlet exception ( ex ) ; } },tomcat specify object
together @$ these two take <PLACE_HOLDER> that multi exceptions thrown from route resource come out as json or gpx @$ depending on the media type,environment . jersey ( ) . register ( new multi exception mapper ( ) ) ; environment . jersey ( ) . register ( new multi exceptiongpx message body writer ( ) ) ; environment . jersey ( ) . register ( new illegal argument exception mapper ( ) ) ; environment . jersey ( ) . register ( new gh point converter provider ( ) ) ; final graph hopper managed graph hopper managed = new graph hopper managed ( configuration . get graph hopper configuration ( ) @$ environment . get object mapper ( ) ) ; environment . lifecycle ( ) . manage ( graph hopper managed ) ; environment . jersey ( ) . register ( new abstract binder ( ) { @ override,two take meaning
what is current sequenceid ? we read the current sequenceid from the current file . after we read it @$ another thread could come in and compete with us writing out next version of file . the below retries should help in this case some but its hard to do <PLACE_HOLDER> in face of concurrent schema edits .,int current sequence id = current descriptor file == null ? __num__ : get table info sequence id ( current descriptor file . get path ( ) ) ; int new sequence id = current sequence id ;,some do something
last element gets <PLACE_HOLDER> : what 's the name ?,data . item element = meta . get input position ( ) [ meta . get input position ( ) . length - __num__ ] ; data . item count = xml handler . count nodes ( data . section @$ data . item element ) ; data . item position = meta . get nr rows to skip ( ) ;,element gets input
second renderer handles <PLACE_HOLDER> .,map < string @$ integer > second renderer mapped capabilities = new hash map < > ( ) ; second renderer mapped capabilities . put ( english . id @$ format_unsupported_subtype ) ; second renderer mapped capabilities . put ( german . id @$ format_handled ) ; renderer capabilities second renderer capabilities = new fake mapped renderer capabilities ( c . track_type_audio @$ second renderer mapped capabilities ) ; renderer capabilities [ ] renderer capabilities = new renderer capabilities [ ] { first renderer capabilities @$ second renderer capabilities } ;,renderer handles english
number match includes at least the two <PLACE_HOLDER> being checked,if ( num end idx > pos ) { if ( num end idx > next pos ) { next pos = num end idx ; pos = num end idx ; do { pos = move index32 ( f text @$ pos @$ - __num__ ) ; this char = utf16 . char at ( f text @$ pos ) ; } while ( fcm . contains ( this char ) ) ; } continue ; },match includes characters
start a new session and attempt to access jenkins @$ which should cause auto login <PLACE_HOLDER>,wc = j . create web client ( ) ; wc . get cookie manager ( ) . add cookie ( c ) ;,which cause failure
the same . this tests that the resource processor namespaces the r <PLACE_HOLDER> correctly to avoid collisions between the two identical layout values .,java file object model = java file objects . for resource ( __str__ ) ; java file object model with differentr class = java file objects . for resource ( __str__ ) ; java file object generated model = java file objects . for resource ( __str__ ) ; java file object generated model with differentr class = java file objects . for resource ( __str__ ) ; assert_ ( ) . about ( java sources ( ) ) . that ( arrays . as list ( model @$ model with differentr class @$ r @$ r_from_different_package_with_same_value ) ) . processed with ( new epoxy processor ( ) ) . compiles without error ( ) . and ( ) . generates sources ( generated model @$ generated model,processor namespaces class
if this item has a uri <PLACE_HOLDER> @$ try using that .,uri uri = get uri ( ) ; if ( uri != null ) { final content resolver resolver = context . get content resolver ( ) ; asset file descriptor descr = null ; file input stream stream = null ; input stream reader reader = null ; try { try { descr = resolver . open typed asset file descriptor ( uri @$ __str__ @$ null ) ; } catch ( security exception e ) { log . w ( __str__ @$ __str__ @$ e ) ; } catch ( file not found exception | runtime exception e ) { } if ( descr != null ) { try { stream = descr . create input stream ( ) ; reader = new input stream reader,item has value
we need to mark this basic block as changed so that this monitorexit will be visited again . we need to do this to ensure that we have accounted for the possibility that this bytecode will throw an <PLACE_HOLDER> .,basic block bb = get basic block containing ( bci ) ; bb . set changed ( true ) ; bb . _monitor_top = bad_monitors ; if ( trace monitor mismatch ) { report monitor mismatch ( __str__ ) ; },bytecode throw exception
valve use the <PLACE_HOLDER> to log messages,when ( delta session manager . get the context ( ) ) . then return ( mock ( context . class ) ) ; when ( delta session manager . get the context ( ) . get logger ( ) ) . then return ( mock ( log . class ) ) ;,valve use context
setting <PLACE_HOLDER> interval to 1 hour to prevent bp service actor sends <PLACE_HOLDER> periodically to nn during running test case @$ and bp service actor only sends <PLACE_HOLDER> once after startup,conf . set time duration ( dfs_heartbeat_interval_key @$ __num__ @$ time unit . hours ) ; minidfs cluster cluster = new minidfs cluster . builder ( conf ) . build ( ) ; cluster . wait active ( ) ; data node dn = cluster . get data nodes ( ) . get ( __num__ ) ; metrics record builder rb = get metrics ( dn . get metrics ( ) . name ( ) ) ; assert counter ( __str__ @$ __num__ @$ rb ) ;,actor sends heartbeat
can happen if this operator does not carry forward the previous bucketing <PLACE_HOLDER> for e.g . another join operator which does not carry one of the sides ' key <PLACE_HOLDER>,for ( list < string > list bucket cols : grand parent col names ) { if ( list bucket cols . is empty ( ) ) { continue ; } int col count = __num__ ; for ( string col name : parent col names . get ( __num__ ) ) { if ( list bucket cols . size ( ) <= col count ) { return false ; } expr node desc expr node desc = col expr map . get ( col name ) ; if ( expr node desc instanceof expr node column desc ) { if ( ( ( expr node column desc ) expr node desc ) . get column ( ) . equals ( list bucket cols . get ( col,operator carry columns
make <PLACE_HOLDER>ure the ping take<PLACE_HOLDER> more than 1 <PLACE_HOLDER>,thread . sleep ( __num__ ) ; assert false ( volt . handler . got ping ) ;,ping takes time
get the targets from the main app class . only one will be used @$ depending on whether the user is deleting old <PLACE_HOLDER> by date or by row count .,try { timestamp type date target = app . get target date ( ) ; long row target = app . get target rows per partition ( ) ; client response with partition key [ ] responses ; if ( app . config . historyseconds > __num__ ) { responses = app . client . call all partition procedure ( __str__ @$ date target @$ app . config . deletechunksize ) ; } else { responses = app . client . call all partition procedure ( __str__ @$ row target @$ app . config . deletechunksize ) ; } app . update partition count ( responses . length ) ; for ( client response with partition key resp : responses ) { if ( resp . response .,user deleting data
the amount with which to adjust the <PLACE_HOLDER> provided content padding to account for stroke and shape corners .,int content padding offset = ( int ) ( ( include corner padding ? calculate actual corner padding ( ) : __num__ ) - get parent card view calculated corner padding ( ) ) ; material card view . set ancestor content padding ( user content padding . left + content padding offset @$ user content padding . top + content padding offset @$ user content padding . right + content padding offset @$ user content padding . bottom + content padding offset ) ;,which adjust view
let the caller make <PLACE_HOLDER> of it @$ then .,type = type . t package ; return vset ;,caller make use
check for test object <PLACE_HOLDER> . it should be 0,vm1 . invoke ( new cache serializable runnable ( __str__ ) { @ override public void run2 ( ) throws cache exception { assert equals ( __num__ @$ test object . num instance ) ; } } ) ; this . close client ( vm2 ) ; this . close client ( vm3 ) ; this . close client ( vm1 ) ; this . close client ( vm0 ) ;,test object instances
make the user agent tester change its <PLACE_HOLDER> and make sure we do n't get notifications as we 're now unsubscribed .,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = get sample status1 ( ) ;,tester change states
close the socket @$ which releases <PLACE_HOLDER> if it has created any .,if ( socket != null ) { try { socket . close ( ) ; } catch ( exception e ) { } },which releases resources
generate entities that have multiple errors the string a field has a <PLACE_HOLDER> over the length limitation and miss string b fields,if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__ ; i ++ ) { validation demo . union field with inline record union = new validation demo . union field with inline record ( ) ; union . set my enum ( my enum . foofoo ) ; validation demos . add ( new validation demo ( ) . set stringa ( __str__ ) . set inta ( current criteria . get inta ( ) ) . set union field with inline record ( union ) ) ; } } else if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__,string has count
init assembler . each plan assembler requires a new <PLACE_HOLDER> of the plan selector to keep track of the best plan,plan assembler assembler = new plan assembler ( m_db @$ m_partitioning @$ ( plan selector ) m_plan selector . clone ( ) @$ m_is large query ) ;,assembler requires instance
register the jre fonts so that the native platform can access <PLACE_HOLDER> . this is used only on windows so that when printing the printer driver can access the fonts .,access controller . do privileged ( new privileged action ( ) { public object run ( ) { registerjre fonts with platform ( jre font dir name ) ; return null ; } } ) ;,platform access them
if we have throttle threads @$ make sure the user also specified <PLACE_HOLDER>,preconditions . check argument ( large threads > __num__ && small threads > __num__ ) ; final string n = thread . current thread ( ) . get name ( ) ; steal job queue < runnable > steal job queue = new steal job queue < runnable > ( comparator ) ; this . long compactions = new thread pool executor ( large threads @$ large threads @$ __num__ @$ time unit . seconds @$ steal job queue @$ new thread factory builder ( ) . set name format ( n + __str__ ) . set daemon ( true ) . build ( ) ) ; this . long compactions . set rejected execution handler ( new rejection ( ) ) ; this . long compactions .,user specified one
validate the having <PLACE_HOLDER> if any,validate having clause ( root node ) ; broker request broker request = new broker request ( ) ; root node . update broker request ( broker request ) ; if ( enable_pinot_query ) { try { pinot query pinot query = new pinot query ( ) ; root node . update pinot query ( pinot query ) ; if ( validate_converter ) { pinot query2 broker request converter converter = new pinot query2 broker request converter ( ) ; broker request temp broker request = converter . convert ( pinot query ) ; boolean result = broker request comparison utils . validate ( broker request @$ temp broker request ) ; if ( ! result ) { logger . error ( __str__ @$ expression ) ; if,the having statements
orc does n't currently handle <PLACE_HOLDER>,case timestamp : timestamp ts = data type utils . to timestamp ( field value @$ ( ) -> data type utils . get date format ( field data type . get format ( ) ) @$ field name ) ;,orc handle timestamp
this is called only if the peer has junior <PLACE_HOLDER>,this . seeddb . add potential ( peer ) ;,peer has children
let 's add an identical <PLACE_HOLDER> to the child @$ but it 'll appear after the current <PLACE_HOLDER> @$ so has no impact,child . insert ace ( __num__ @$ base permission . delete @$ new principal sid ( auth ) @$ true ) ;,'s add permission
this can contain user code . wrap it in case it throws an <PLACE_HOLDER> .,try { invoker . invoke start bundle ( new do fn start bundle context ( ) ) ; } catch ( throwable t ) { throw wrap user code exception ( t ) ; },code throws exception
create the mapping from index source look up <PLACE_HOLDER> to probe key input,immutable set multimap . builder < symbol @$ integer > builder = immutable set multimap . builder ( ) ; for ( map . entry < symbol @$ symbol > entry : index key trace . entry set ( ) ) { symbol index join symbol = entry . get key ( ) ; symbol index lookup symbol = entry . get value ( ) ; builder . put all ( index lookup symbol @$ index to probe key input . get ( index join symbol ) ) ; } return builder . build ( ) ;,mapping look symbols
null indicates a static method which may still need generics <PLACE_HOLDER>,generics type [ ] generics types = helper method . get generics types ( ) ; if ( generics types != null ) { map < string @$ class node > method spec = generics utils . add method generics ( helper method @$ collections . empty map ( ) ) ; generics type [ ] new gt = generics utils . apply generics context to place holders ( method spec @$ helper method . get generics types ( ) ) ; forwarder . set generics types ( new gt ) ; },which need information
the remaining ids reference <PLACE_HOLDER> that do n't exist in db . they must be deleted from index .,remaining . values ( ) . for each ( item -> bulk indexer . add deletion ( type_active_rule @$ item . get doc id ( ) @$ item . get doc routing ( ) ) ) ; return bulk indexer . stop ( ) ;,the ids rows
set the new mark so that next time we get new <PLACE_HOLDER> since this point .,if ( scan time since mark ms > __num__ ) { u . m bluetooth scan timer . set mark ( elapsed realtime ms ) ; long scan time rx since mark ms = scan time since mark ms ; long scan time tx since mark ms = scan time since mark ms ; if ( normalize scan rx time ) { scan time rx since mark ms = ( rx time ms * scan time rx since mark ms ) / total scan time ms ; } if ( normalize scan tx time ) { scan time tx since mark ms = ( tx time ms * scan time tx since mark ms ) / total scan time ms ; } final controller activity counter impl counter,time get data
on t vs @$ if the app does n't implement <PLACE_HOLDER> @$ we want to launch assist .,if ( ! result && ( get context ( ) . get resources ( ) . get configuration ( ) . ui mode & configuration . ui_mode_type_mask ) == configuration . ui_mode_type_television ) { bundle args = new bundle ( ) ; args . put int ( intent . extra_assist_input_device_id @$ event . get device id ( ) ) ; return ( ( search manager ) get context ( ) . get system service ( context . search_service ) ) . launch legacy assist ( null @$ get context ( ) . get user id ( ) @$ args ) ; },app implement defaults
result of this conversion will exceed the original <PLACE_HOLDER> and cause a newline to be inserted,string data [ ] = { __str__ @$ __str__ @$ __str__ @$ __str__ } ;,result exceed size
original capacity assumes 20 <PLACE_HOLDER> per headers,string builder sb = new string builder ( simple name . length ( ) + __num__ + size * __num__ ) . append ( simple name ) . append ( __str__ ) ; while ( headers it . has next ( ) ) { entry < ? @$ ? > header = headers it . next ( ) ; sb . append ( header . get key ( ) ) . append ( __str__ ) . append ( header . get value ( ) ) . append ( __str__ ) ; } sb . set length ( sb . length ( ) - __num__ ) ; return sb . append ( __str__ ) . to string ( ) ;,capacity assumes bytes
if the existing monitor has already been cancelled @$ then do not apply the <PLACE_HOLDER>,if ( delegate . is cancelled ( ) ) { new delegate . cancel ( ) ; return ; } for ( cancelled listener l : listeners ) { new delegate . add cancelled listener ( l ) ; delegate . remove cancelled listener ( l ) ; } new delegate . set maximum ( delegate . get maximum ( ) ) ; new delegate . set progress ( delegate . get progress ( ) ) ; new delegate . set message ( delegate . get message ( ) ) ; new delegate . set indeterminate ( delegate . is indeterminate ( ) ) ; new delegate . set cancel enabled ( delegate . is cancel enabled ( ) ) ; this . delegate = new delegate ;,then apply callback
make sure the d ns do n't send a <PLACE_HOLDER> for a while @$ so the blocks wo n't actually get completed during lease recovery .,for ( data node dn : cluster . get data nodes ( ) ) { data node test utils . set heartbeats disabled for tests ( dn @$ true ) ; },ns send message
create directories one by one with explicit <PLACE_HOLDER>s to ensure no umask is applied @$ using mkdirs will apply the <PLACE_HOLDER> only to the last directory,stack < path > dirs to make = new stack < > ( ) ; dirs to make . push ( hdfs path ) ; path parent = hdfs path . get parent ( ) ; while ( ! hdfs . exists ( parent ) ) { dirs to make . push ( parent ) ; parent = parent . get parent ( ) ; } while ( ! dirs to make . empty ( ) ) { path dir to make = dirs to make . pop ( ) ; if ( ! file system . mkdirs ( hdfs @$ dir to make @$ new fs permission ( options . get mode ( ) . to short ( ) ) ) ) { return false ; },mkdirs apply permission
the stdouterr file collects all the <PLACE_HOLDER> and system.err writes to disk .,string stdouterrfile = util . extract string option ( __str__ @$ settings ) ;,file collects errors
notify the listeners . do that from the end of the list so that if a listener removes <PLACE_HOLDER> as the result of being called @$ it wo n't mess up with our iteration,if ( m listeners != null ) { int listener count = m listeners . size ( ) ; for ( int i = listener count - __num__ ; i >= __num__ ; i -- ) { m listeners . get ( i ) . on drawer closed ( drawer view ) ; } },listener removes itself
stomp out any bad characters since this is from a circular buffer a corruption is seen sometimes that results in the vm crashing this should prevent <PLACE_HOLDER> and the line will probably fail to parse,for ( int j = start index ; j < end index ; j ++ ) { if ( ( wl buffer [ j ] & __num__ ) != __num__ ) wl buffer [ j ] = ( byte ) __str__ ; } boolean parsed = process . parse proc line ( wl buffer @$ start index @$ end index @$ wakeup_sources ? wakeup_sources_format : proc_wakelocks_format @$ name string array @$ wl data @$ null ) ; name = name string array [ __num__ ] . trim ( ) ; count = ( int ) wl data [ __num__ ] ; if ( wakeup_sources ) { total time = wl data [ __num__ ] * __num__ ; } else { total time = ( wl data [ __num__,results prevent termination
a fourth rebalance should not change <PLACE_HOLDER>,apply assignments ( returned assignments ) ; member configs = member configs ( leader @$ offset @$ assignments ) ; assignor . perform task assignment ( leader @$ offset @$ member configs @$ coordinator @$ protocol version ) ; ++ rebalance num ; returned assignments = assignments capture . get value ( ) ; assert delay ( __num__ @$ returned assignments ) ; expected member configs = member configs ( leader @$ offset @$ returned assignments ) ; assert no reassignments ( member configs @$ expected member configs ) ; assert assignment ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __str__ ) ; verify ( coordinator @$ times ( rebalance num ) ) . config snapshot ( ) ; verify ( coordinator @$ times (,rebalance change assignments
button text can have a different <PLACE_HOLDER>,if ( m ok color == null ) m ok color = m accent color ; m ok button . set text color ( m ok color ) ; if ( m cancel color == null ) m cancel color = m accent color ; m cancel button . set text color ( m cancel color ) ; if ( get dialog ( ) == null ) { view . find view by id ( r . id . mdtp_done_background ) . set visibility ( view . gone ) ; } int circle background = context compat . get color ( context @$ r . color . mdtp_circle_background ) ; int background color = context compat . get color ( context @$ r . color . mdtp_background_color ),text have color
now try <PLACE_HOLDER> and it should work .,admin . split region async ( hri . get region name ( ) ) . get ( __num__ @$ time unit . minutes ) ;,now try 2
find the next index @$ in case the assumed next index is not uni<PLACE_HOLDER>ue . for instance @$ if there is no p @$ then re<PLACE_HOLDER>uest for p 's position actually returns <PLACE_HOLDER> 's . so we need to look ahead to make sure that there is really a <PLACE_HOLDER> at <PLACE_HOLDER> 's position . if not @$ move further down ...,int next next section = next section + __num__ ; while ( next next section < section count && m section indexer . get position for section ( next next section ) == next index ) { next next section ++ ; next section ++ ; },request returns an
should use root entity <PLACE_HOLDER> by default,criteria executor criteria executor = new criteria executor ( ) { protected criteria get criteria ( session s ) { return s . create criteria ( enrolment . class @$ __str__ ) . create alias ( __str__ @$ __str__ @$ criteria . left_join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set fetch mode ( __str__ @$ fetch mode . join ) . set projection ( projections . projection list ( ) . add ( projections . property ( __str__ ) ) . add ( projections . property ( __str__ ) ) ) . add order ( order . asc ( __str__ ) ) ; } } ;,use root transformer
completing the user event listener should terminate the <PLACE_HOLDER>,user event listener instance user event listener instance = cmmn runtime service . create user event listener instance query ( ) . case instance id ( case instance . get id ( ) ) . single result ( ) ; assert that ( user event listener instance . get id ( ) ) . is equal to ( user event listener plan item instance . get id ( ) ) ; cmmn runtime service . complete user event listener instance ( user event listener instance . get id ( ) ) ; assert case instance ended ( case instance ) ;,listener terminate case
verify user from a group which has column family level <PLACE_HOLDER> can read all the data belonging to that family and group which has no <PLACE_HOLDER> ca n't read any data .,grant on table ( test_util @$ testgroup_1_name @$ table name @$ test_family @$ null @$ permission . action . read ) ; verify allowed ( testgroup1_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup1_user1 @$ scan family action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan table action for group with family level access ) ; verify denied ( testgroup2_user1 @$ scan family action for group with family level access ) ;,which has access
we deduced thoughts from the inferences in the intents . the keynote also carries these <PLACE_HOLDER>,this . get actions clone ( ) . for each ( action -> keynote . add action ( action ) ) ;,keynote carries actions
out stream has <PLACE_HOLDER> .,throw new illegal state exception ( ioex ) ;,stream has data
we can not determine <PLACE_HOLDER> method is the most specific because one parameter of the first candidate was more specific and another parameter of the second candidate was more specific .,if ( best match != null && ! potential match . equals ( best match ) ) { return null ; } else { best match = potential match ; },parameter specific which
default to number class in exception details @$ else use the specified number <PLACE_HOLDER> .,return cast to number ( object @$ number . class ) ;,default use type
sql result type <PLACE_HOLDER> @$,finish initialization ( query return type list ) ;,sql result list
use configuration that is same as <PLACE_HOLDER> the transition will change .,use configuration ( immutable map . of ( __str__ @$ __str__ ) ) ; configured target test = get configured target ( __str__ ) ; @ suppress warnings ( __str__ ) configured target dep = iterables . get only element ( ( list < configured target > ) get my info from target ( test ) . get value ( __str__ ) ) ; assert that ( get core options ( test ) . transition directory name fragment ) . is null ( ) ; assert that ( get core options ( dep ) . transition directory name fragment ) . is not null ( ) ;,transition change what
we will be calling a function of index & passing the other index as a parameter . we will be getting list object . each element of the list will contain a two dimesional object <PLACE_HOLDER> . the first row will contain the result objects of the first index & secondrow will contain that of second index . the object contained in each of,query observer observer = query observer holder . get instance ( ) ; context . cache put ( compiled value . index_info @$ indx info ) ;,element contain array
seasonal does n't have boss <PLACE_HOLDER>,if ( index < skills . size ( ) ) { hiscore result . set abyssal sire ( skills . get ( index ++ ) ) ; hiscore result . set alchemical hydra ( skills . get ( index ++ ) ) ; hiscore result . set barrows chests ( skills . get ( index ++ ) ) ; hiscore result . set bryophyta ( skills . get ( index ++ ) ) ; hiscore result . set chambers of xeric ( skills . get ( index ++ ) ) ; hiscore result . set chambers of xeric challenge mode ( skills . get ( index ++ ) ) ; hiscore result . set chaos elemental ( skills . get ( index ++ ) ) ; hiscore result,seasonal have shape
playback play sound <PLACE_HOLDER> .,if ( feature notify and playback devices ) { cnstrnts . gridy = __num__ ; playback play sound button = new j button ( new image icon ( neomedia activator . get resources ( ) . get image in bytes ( __str__ ) ) ) ; playback play sound button . set minimum size ( new dimension ( __num__ @$ __num__ ) ) ; playback play sound button . set preferred size ( new dimension ( __num__ @$ __num__ ) ) ; if ( ( ( device configuration combo box model . capture device ) playback combo . get selected item ( ) ) . info == null ) { playback play sound button . set enabled ( false ) ; } playback play sound button . set,playback play button
no security exception @$ do the <PLACE_HOLDER> .,int source user id = content provider . get user id from uri ( uri @$ m user ) ; uri = content provider . get uri without user id ( uri ) ; uri grants manager . get service ( ) . grant uri permission from owner ( m permission owner @$ src uid @$ dest pkg @$ uri @$ flag_grant_read_uri_permission @$ source user id @$ m user ) ; binder . restore calling identity ( ident ) ;,exception do default
moving a day the notification q calls the commit <PLACE_HOLDER> . now payment is expected,bus handler . push expected events ( next event . invoice @$ next event . payment @$ next event . invoice_payment ) ; clock . add days ( __num__ ) ; assert listener status ( ) ; parent invoice = invoice user api . get invoice ( parent invoice . get id ( ) @$ call context ) ; assert equals ( parent invoice . get status ( ) @$ invoice status . committed ) ; assert equals ( parent invoice . get balance ( ) . compare to ( big decimal . zero ) @$ __num__ ) ;,q calls invoice
simulate a 2 nn beginning a <PLACE_HOLDER> @$ but not finishing . this will cause name 1 to be restored .,cluster . get name node rpc ( ) . roll edit log ( ) ; print storages ( fs image ) ;,nn beginning file
xml parser only does string <PLACE_HOLDER>,return get field value ( ) ;,parser does validation
tests whether the width of the picture changes trough all qualities and every step has a few <PLACE_HOLDER> .,try { int n = __num__ ; int m = __num__ ; int q = __num__ ; int [ ] qualities = { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ; while ( ! end requested ) { frame source = fg . grab frame ( ) ; n ++ ; m ++ ; if ( source . image width != qualities [ q ] ) { q ++ ; assert equals ( source . image width @$ qualities [ q ] ) ; assert true ( m > __num__ ) ; assert true ( m <= __num__ ) ; m = __num__ ; } fr . record ( source ) ; } assert equals ( q @$ qualities,tests has levels
update controllers pose <PLACE_HOLDER>,environment . getv rinput ( ) . update controller states ( ) ;,controllers pose state
the region state node may have no <PLACE_HOLDER> in a test scenario ; allow for this .,update user region location ( region state node . get region info ( ) @$ region state node . get state ( ) @$ region state node . get region location ( ) @$ open seq num @$ region state node . get procedure ( ) != null ? region state node . get procedure ( ) . get proc id ( ) : procedure . no_proc_id ) ;,node have problems
parsing should go all the way to the end of the string . we want the longest match @$ and we do n't care if the plural <PLACE_HOLDER> of the unit matches the plural <PLACE_HOLDER> of the number .,assert equals ( __str__ @$ parse string . length ( ) @$ ppos . get index ( ) ) ;,form matches form
closing the entity manager factory should close the <PLACE_HOLDER>,em . get entity manager factory ( ) . close ( ) ;,factory close endpoint
no fragment may be present on a toolchain rule in retroactive trimming <PLACE_HOLDER> . this is because trimming expects that platform and toolchain resolution uses only the platform configuration . in theory @$ this means toolchains could use platforms @$ but the current expectation is that toolchains should not use anything at all @$ so better to go with the stricter expectation for now,if ( configuration . trim configurations retroactively ( ) && ! target . get configuration key ( ) . get fragments ( ) . is empty ( ) ) { string extra fragment description = target . get configuration key ( ) . get fragments ( ) . stream ( ) . map ( cl -> cl . get simple name ( ) ) . collect ( joining ( __str__ ) ) ; throw new registered toolchains function exception ( new invalid toolchain label exception ( toolchain label @$ __str__ + __str__ + __str__ + extra fragment description + __str__ ) @$ transience . persistent ) ; },configuration trimming case
the data will include corrected <PLACE_HOLDER> :,assert equals ( __num__ @$ histogram . get count at value ( __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get count at value ( ( test value level * __num__ ) / __num__ ) ) ; assert equals ( __num__ @$ histogram . get total count ( ) ) ;,data include samples
the copying thread wo n't generally detect <PLACE_HOLDER>,swin . interrupt ( ) ;,thread detect interruption
no tables have an empty <PLACE_HOLDER>,try ( connection connection = create connection ( ) ) { try ( result set rs = connection . get meta data ( ) . get tables ( __str__ @$ null @$ null @$ null ) ) { assert table metadata ( rs ) ; assert equals ( read rows ( rs ) . size ( ) @$ __num__ ) ; } } try ( connection connection = create connection ( ) ) { try ( result set rs = connection . get meta data ( ) . get tables ( test_catalog @$ __str__ @$ null @$ null ) ) { assert table metadata ( rs ) ; set < list < object > > rows = immutable set . copy of ( read rows ( rs ),tables have marker
native methods may have null <PLACE_HOLDER>,if ( e . get file name ( ) != null ) { assert that ( json ) . contains ( __str__ + e . get file name ( ) + __str__ ) ; },methods have names
note that view.on initialize accessibility node <PLACE_HOLDER> was introduced in ics and we would like to tweak a bit the text that is reported to accessibility services via the accessibility node <PLACE_HOLDER> .,info . set text ( get context ( ) . get string ( r . string . accessibility_delegate_custom_text_added ) ) ;,view.on initialize factory
can only apply if both sides have <PLACE_HOLDER> .,return ;,sides have problems
action has no configuration @$ we just keep this <PLACE_HOLDER> for the future,if ( config != null ) { },configuration keep one
make sure that an app with coarse permissions ca n't get frequent location <PLACE_HOLDER> by calling location manager.get last known location repeatedly .,if ( allowed resolution level < resolution_level_fine ) { location = m last location coarse interval . get ( name ) ; } else { location = m last location . get ( name ) ; },app get updates
just use the first <PLACE_HOLDER> in the batch here to satisfy the <PLACE_HOLDER> cursor . the truth is that we 'll be using the read method which accepts an external <PLACE_HOLDER> anyway so it does n't matter .,try ( page cursor cursor = store . open page cursor for reading ( id ) ) { boolean has next = true ; while ( has next ) { if ( assembler . append ( store @$ cursor @$ batch @$ id @$ i ) ) { i ++ ; } if ( has next = id range . has next ( ) ) { id = id range . next ( ) ; } } } sender . send ( assembler . cut off at ( batch @$ i ) ) ;,which accepts page
always consume the drag so that java does not change the <PLACE_HOLDER>,e . consume ( ) ;,java change context
otherwise register a dummy provider which would provoke the <PLACE_HOLDER> of bug 8139436,boolean provider prepended = false ; string testprovider = system . get property ( __str__ ) ; if ( testprovider != null && ! testprovider . is empty ( ) ) { try { system . out . println ( __str__ + testprovider ) ; class < ? > providerclass = class . for name ( testprovider ) ; object provider = providerclass . new instance ( ) ; security . insert provider at ( ( provider ) provider @$ __num__ ) ; } catch ( exception e ) { system . out . println ( __str__ + testprovider + __str__ ) ; e . print stack trace ( system . out ) ; } provider prepended = true ; system . out . println ( __str__ +,which provoke reset
we catch throwable here because netty uses clever <PLACE_HOLDER> to have method signatures that look like they do not throw checked exceptions @$ but they actually do . the compiler wo n't let us catch them explicitly because in theory they should n't be possible @$ so we have to catch throwable and do our own checks to grab them,throw new port bind exception ( initializer . address ( ) @$ e ) ;,netty uses objects
system apps have <PLACE_HOLDER> over where their default storage context is pointed @$ so we 're always explicit when building paths .,try { final context ce context = create credential protected storage context ( ) ; root dir = ce context . get data dir ( ) . get canonical path ( ) ; files dir = ce context . get files dir ( ) . get canonical path ( ) ; nb files dir = ce context . get no backup files dir ( ) . get canonical path ( ) ; db dir = ce context . get database path ( __str__ ) . get parent file ( ) . get canonical path ( ) ; sp dir = ce context . get shared preferences path ( __str__ ) . get parent file ( ) . get canonical path ( ) ; cache dir = ce context,apps have control
check if the crypto <PLACE_HOLDER>s granted to the app contain a crypto <PLACE_HOLDER> for the requested algorithm that does not require any exemption mechanism to be enforced . return that <PLACE_HOLDER> @$ if present .,permission collection app pc = app perms . get permission collection ( alg ) ; if ( app pc == null ) { return default perm ; } enumeration < permission > enum_ = app pc . elements ( ) ; while ( enum_ . has more elements ( ) ) { crypto permission cp = ( crypto permission ) enum_ . next element ( ) ; if ( cp . get exemption mechanism ( ) == null ) { return cp ; } },check return permission
generate the strongest key using the shared secret assuming the key sizes in aes constants class are in ascending <PLACE_HOLDER>,while ( skey == null && idx >= __num__ ) { if ( keysize >= aes constants . aes_keysizes [ idx ] ) { keysize = aes constants . aes_keysizes [ idx ] ; skey = new secret key spec ( secret @$ __num__ @$ keysize @$ __str__ ) ; } idx -- ; },class ascending order
all surrogate pairs with this lead surrogate have only irrelevant <PLACE_HOLDER>,if ( ( norm32 & mask ) == __num__ ) { return __num__ ; } else { return get norm32 from surrogate pair ( norm32 @$ args . c ) ; },pairs have data
make sure that the pattern matches a <PLACE_HOLDER> by one .,if ( ! b . is java constant ( ) ) { return null ; } java constant b cst = b . as java constant ( ) ; long b value ; if ( b cst . get java kind ( ) == java kind . int ) { b value = b cst . as int ( ) ; } else if ( b cst . get java kind ( ) == java kind . long ) { b value = b cst . as long ( ) ; } else { return null ; } if ( b value == - __num__ ) { return builder -> get arithmeticlir generator ( ) . emit get mask up to lowest set bit ( operand ( a,pattern matches subtraction
otherwise can only duplicate if enough <PLACE_HOLDER> .,return true ;,otherwise duplicate elements
create calendar which omits <PLACE_HOLDER>,for ( action . handler action handler : action handlers ) { gregorian calendar cal = new gregorian calendar ( get time zone ( ) @$ get locale ( ) ) ; cal . clear ( ) ; cal . set ( current calendar . get ( java . util . calendar . year ) @$ current calendar . get ( java . util . calendar . month ) @$ current calendar . get ( java . util . calendar . date ) ) ; date start = cal . get time ( ) ; cal . add ( java . util . calendar . date @$ __num__ ) ; cal . add ( java . util . calendar . second @$ - __num__ ) ; date end,which omits keys
assume root always has a <PLACE_HOLDER>,if ( is root ( path ) ) { return true ; } string parent key = get parent path ( path ) ; return parent key != null && is directory ( parent key ) ;,root has parent
proprietary class format @$ can not set or check logical <PLACE_HOLDER> for now @$ just return,if ( cla < __num__ ) { return ; },format set validity
check each new char to make sure it matches <PLACE_HOLDER> the group referenced matched last time around,int x = i ; for ( int index = __num__ ; index < group size ; index ++ ) { int c1 = character . code point at ( seq @$ x ) ; int c2 = character . code point at ( seq @$ j ) ; if ( c1 != c2 ) { if ( do unicode case ) { int cc1 = character . to upper case ( c1 ) ; int cc2 = character . to upper case ( c2 ) ; if ( cc1 != cc2 && character . to lower case ( cc1 ) != character . to lower case ( cc2 ) ) return false ; } else { if ( ascii . to lower ( c1 ) != ascii,group referenced what
since some records do n't have all possible <PLACE_HOLDER> @$ we initialize average coordinates equal to current centroid coordinates,map < string @$ double > average = centroid . get coordinates ( ) ;,records have values
wild card ipv 6 single broadcast ipv 6 <PLACE_HOLDER> : fe 80 : xx : xx ... loopback ipv 6 <PLACE_HOLDER> site local ipv 6 <PLACE_HOLDER> : fec 0 : xx : xx ...,if ( inet addr . is any local address ( ) || inet addr . is link local address ( ) || inet addr . is loopback address ( ) || inet addr . is site local address ( ) ) { return true ; },card ipv address
count keys as equal if they have the same key <PLACE_HOLDER> .,return o instanceof g infopc && arrays . equals ( _gs @$ ( ( g infopc ) o ) . _gs ) ;,keys have size
renewing <PLACE_HOLDER> and adding it to timer calls are separated purposefully if user provides incorrect <PLACE_HOLDER> then it should not be added for renewal .,if ( ! token list . is empty ( ) ) { for ( delegation token to renew dtr : token list ) { delegation token to renew current dtr = all tokens . put if absent ( dtr . token @$ dtr ) ; if ( current dtr != null ) { current dtr . referring app ids . add ( application id ) ; app tokens . get ( application id ) . add ( current dtr ) ; } else { app tokens . get ( application id ) . add ( dtr ) ; set timer for token renewal ( dtr ) ; } } },user provides token
at this point we are done processing the input . close the record <PLACE_HOLDER>,if ( ! is closed ) { close record processor ( ) ; is closed = true ; },input close processor
interactive bugreports show progress <PLACE_HOLDER> .,assert that ( callback . has received progress ( ) ) . is true ( ) ; assert that ( callback . get error code ( ) ) . is equal to ( bugreport callback . bugreport_error_user_consent_timed_out ) ; assert fds are closed ( m bugreport fd ) ;,bugreports show indication
if namespace parents are implicitly created @$ they wo n't have ac ls . so @$ let 's explicitly create <PLACE_HOLDER> .,curator framework null ns fw = zk client . using namespace ( null ) ; ensure path ensure ns = null ns fw . new namespace aware ensure path ( __str__ + zk client . get namespace ( ) ) ; try { ensure ns . ensure ( null ns fw . get zookeeper client ( ) ) ; } catch ( exception e ) { throw new io exception ( __str__ @$ e ) ; },let create them
no rows to recycle but update the spacer <PLACE_HOLDER>,spacer container . update spacer indexes for row and after ( index @$ index + number of rows - added row count @$ number of rows - added row count ) ; double new rows height = number of rows * default row height ; if ( added row count > __num__ ) { move viewport and content ( null @$ __num__ @$ __num__ @$ new rows height ) ; row visibility changed = true ; } else { move viewport and content ( index @$ new rows height @$ new rows height @$ new rows height ) ; },rows recycle index
the offset is just to get the right stack <PLACE_HOLDER> highlighted in the output,if ( interpreter frame method != null ) { int offset = __num__ ; anno panel . add annotation ( new annotation ( cur frame . address of interpreter frame local ( offset ) @$ cur frame . address of interpreter frame local ( ( int ) interpreter frame method . get max locals ( ) + offset ) @$ __str__ + cur frame . getsp ( ) ) ) ; },right stack size
normalization can not remove leading uplevel <PLACE_HOLDER> @$ so this will be true,assert that ( create ( __str__ ) . contains uplevel references ( ) ) . is true ( ) ;,normalization remove references
the directory entries typically have different <PLACE_HOLDER>s to the files @$ e.g . execute <PLACE_HOLDER> or ca n't cd into it,files . walk file tree ( target @$ new simple file visitor < path > ( ) { public file visit result pre visit directory ( path dir @$ basic file attributes attrs ) throws io exception { files . set posix file permissions ( dir @$ dir permissions ) ; return file visit result . continue ; } public file visit result visit file ( path file @$ basic file attributes attrs ) throws io exception { files . set posix file permissions ( file @$ permissions ) ; return file visit result . continue ; } } ) ;,directory execute permission
decrement llc simultaneous segment <PLACE_HOLDER> .,_server metrics . add value to global gauge ( server gauge . llc_simultaneous_segment_builds @$ - __num__ ) ;,decrement llc builds
prints 0 <PLACE_HOLDER> <PLACE_HOLDER> <PLACE_HOLDER> <PLACE_HOLDER> ... .,no inline ( __str__ ) ;,prints 0 x
make sure gl does not use our <PLACE_HOLDER> before we start computing,if ( syncc ltogl || should init buffers ) { gl finish ( ) ; } if ( should init buffers ) { initgl objects ( ) ; set kernel constants ( ) ; } if ( rebuild ) { build program ( ) ; set kernel constants ( ) ; } computecl ( double precision ) ; rendergl ( ) ;,gl use buffers
the log level would actually allow a <PLACE_HOLDER> to be logged .,if ( log warnings ) { try { walk warnings ( statement . get warnings ( ) @$ handler ) ; } catch ( sql exception sql exception ) { log . debug ( __str__ @$ sql exception ) ; } },level allow warning
assume something generous if we have no <PLACE_HOLDER>,m interarrival time = __num__ ;,something have backpressure
load the texture again . but this time @$ force the gwt <PLACE_HOLDER> of pixmap to move to a canvas representation of the image,pixmap pixmap = new pixmap ( gdx . files . internal ( __str__ ) ) ; pixmap . get pixel ( __num__ @$ __num__ ) ; file texture data data1 = new file texture data ( null @$ pixmap @$ null @$ false ) ; bad texture = new texture ( data1 ) ;,texture force instance
add nodes to cluster @$ so cluster have 20 <PLACE_HOLDER> and 20 vcores,resource new resource = resource . new instance ( __num__ * gb @$ __num__ ) ; rm node node = mock nodes . new node info ( __num__ @$ new resource @$ __num__ @$ __str__ ) ; cs . handle ( new node added scheduler event ( node ) ) ; resource new resource2 = resource . new instance ( __num__ * gb @$ __num__ ) ; rm node node2 = mock nodes . new node info ( __num__ @$ new resource2 @$ __num__ @$ __str__ ) ; cs . handle ( new node added scheduler event ( node2 ) ) ; fi ca scheduler app fi ca app1 = cs . get scheduler applications ( ) . get ( app . get application id ( ) ),cluster have gb
chromium 's info <PLACE_HOLDER> container may add an info <PLACE_HOLDER> immediately during this initialization call @$ so make sure everything in the info <PLACE_HOLDER> container is completely ready beforehand .,m native info bar container = native init ( ) ;,container add bar
items to test <PLACE_HOLDER>,item = new privacy item ( privacy item . type . group . name ( ) @$ false @$ i ) ; item . set value ( group name ) ; item . set filter message ( true ) ; original privacy items [ i ] = item ; i = i + __num__ ;,items test group
user 4 has another global <PLACE_HOLDER> via a group,group dto administrator group3 = db . users ( ) . insert group ( organization2 ) ; db . users ( ) . insert permission on group ( administrator group3 @$ quality_profile_admin ) ; user dto user4 = db . users ( ) . insert user ( with email ( __str__ ) ) ; db . users ( ) . insert member ( administrator group3 @$ user4 ) ; component dto project = db . components ( ) . insert private project ( ) ;,user has permission
hive does not support result set meta data on prepared statement @$ and hive describe does not support <PLACE_HOLDER> @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,hive support queries
check if method has <PLACE_HOLDER> that need profiling,if ( graph . get nodes ( ) . filter ( invoke node . class ) . filter ( ( n ) -> ( ( invoke node ) n ) . get invoke kind ( ) . is indirect ( ) ) . count ( ) > options . simple method indirect calls . get default value ( ) ) { return false ; } return true ;,method has instructions
let 's use the link <PLACE_HOLDER> as default title when no mapping is defined .,if ( link element . has text ( ) && ! this . html mapping . contains key ( __str__ ) ) { doc . set field ( collection schema . title . get solr field name ( ) @$ link element . text ( ) ) ; },let use text
check has audio <PLACE_HOLDER> just in case !,if ( ! has audio control ( ) ) { if ( my debug . log ) log . e ( tag @$ __str__ ) ; return ; } this . close popup ( ) ; shared preferences shared preferences = get default shared preferences ( this ) ; string audio_control = shared preferences . get string ( preference keys . get audio control preference key ( ) @$ __str__ ) ; if ( audio_control . equals ( __str__ ) && speech recognizer != null ) { if ( speech recognizer is started ) { speech recognizer . stop listening ( ) ; speech recognizer stopped ( ) ; } else { preview . show toast ( audio_control_toast @$ r . string . speech_recognizer_started ) ; intent intent,check has control
append will fail when the file size crosses the checksum chunk <PLACE_HOLDER> @$ if append was called with a stale file stat .,do small appends ( file @$ fs @$ __num__ ) ;,size crosses threshold
return the snapshot from the update <PLACE_HOLDER> as another thread may have already flushed the <PLACE_HOLDER>,return my update request . get finished index snapshot ( ) ;,thread flushed cache
noop as lzma output stream throws an <PLACE_HOLDER> in flush,return new flush shield filter output stream ( new lzma output stream ( out @$ get options ( opts ) @$ false ) ) ;,stream throws exception
the class cast exception might occur <PLACE_HOLDER> of the completion stage which might be confusing,return apply ( publisher @$ ( class < r > ) type ) . then apply ( type :: cast ) ;,exception occur outside
first @$ with debug @$ which should not log index writer <PLACE_HOLDER> :,try { parsed document doc = test parsed document ( __str__ @$ null @$ test document with text field ( ) @$ b_1 @$ null ) ; engine . index ( index for doc ( doc ) ) ; engine . flush ( ) ; assert false ( mock appender . saw index writer message ) ; loggers . set level ( root logger @$ level . trace ) ; engine . index ( index for doc ( doc ) ) ; engine . flush ( ) ; assert true ( mock appender . saw index writer message ) ; } finally { loggers . remove appender ( root logger @$ mock appender ) ; mock appender . stop ( ) ; loggers . set level ( root,which log output
if the process wrapper is enabled @$ we use its timeout feature @$ which first interrupts the subprocess and only kills it after a grace period so that the subprocess can output a stack <PLACE_HOLDER> @$ test log or similar @$ which is incredibly helpful for debugging .,if ( use process wrapper ) { process wrapper util . command line builder command line builder = process wrapper util . command line builder ( process wrapper . get path string ( ) @$ spawn . get arguments ( ) ) . set timeout ( context . get timeout ( ) ) . set kill delay ( duration . of seconds ( local execution options . local sigkill grace seconds ) ) ; if ( local execution options . collect local execution statistics ) { statistics path = tmp dir . get relative ( __str__ ) ; command line builder . set statistics path ( statistics path ) ; } args = command line builder . build ( ) ; } else { subprocess builder . set,subprocess output trace
if keys do not exist : plus a new <PLACE_HOLDER>,set ( val @$ keys ) ;,keys exist one
first time just get <PLACE_HOLDER> .,tables info = load hdfs region infos ( ) ;,time get table
scanning the entire table should give us three <PLACE_HOLDER>,meta table accessor . scan meta for table regions ( connection @$ visitor @$ table name ) ; verify ( visitor @$ times ( __num__ ) ) . visit ( ( result ) any object ( ) ) ;,table give records
do n't cache these @$ bumps do n't get updated <PLACE_HOLDER>,return new vertical slider thumb icon ( ) ;,bumps get otherwise
delayed transport has already terminated . terminating the transport terminates the sub<PLACE_HOLDER> @$ which in turn terimates the oob <PLACE_HOLDER> @$ which terminates the <PLACE_HOLDER> .,assert false ( oob1 . is terminated ( ) ) ; verify ( balancer rpc executor pool ) . return object ( balancer rpc executor . get scheduled executor service ( ) ) ; transport info . listener . transport terminated ( ) ; assert true ( oob1 . is terminated ( ) ) ; assert true ( channel . is terminated ( ) ) ; verify ( balancer rpc executor pool @$ times ( __num__ ) ) . return object ( balancer rpc executor . get scheduled executor service ( ) ) ;,which terimates channel
array precedence equals <PLACE_HOLDER>,if ( array . get precedence ( ) > get precedence ( ) ) { res . enclose ( __str__ @$ __str__ ) ; },precedence equals any
the file failed to write @$ try 5 times @$ calling gc and sleep between each iteration sometimes windows takes a <PLACE_HOLDER> to release a lock on a file,while ( ! are byte arrays equal ( data @$ read bytes ( f ) ) && count < __num__ ) { system . gc ( ) ; try { thread . sleep ( __num__ ) ; } catch ( interrupted exception e ) { throw new runtime exception ( __str__ ) ; } write bytes ( f @$ data ) ; count ++ ; },windows takes while
if the user only wants to print the <PLACE_HOLDER> @$ print only the <PLACE_HOLDER>,if ( pod . get selection ( ) ) { print selection ( monitor @$ start date @$ job @$ book @$ lm @$ scale amount @$ scaled height ) ; if ( monitor . is cancelled ( ) ) { return ; } } else if ( pod . get visible ( ) ) { print visible content ( monitor @$ start date @$ job @$ book @$ lm @$ scale amount @$ scaled height ) ; if ( monitor . is cancelled ( ) ) { return ; } } else { int page count = get printable page count ( monitor @$ lm @$ scaled height @$ big integer . zero @$ null ) ; print view ( monitor @$ start date @$ job @$,selection print selection
for nested result maps @$ partial works the <PLACE_HOLDER> as none,sql session factory . get configuration ( ) . set auto mapping behavior ( auto mapping behavior . partial ) ; try ( sql session sql session = sql session factory . open session ( ) ) { mapper mapper = sql session . get mapper ( mapper . class ) ; user user = mapper . get user with pets_ external ( __num__ ) ; assertions . assert equals ( integer . value of ( __num__ ) @$ user . get id ( ) ) ; assertions . assert equals ( __str__ @$ user . get name ( ) ) ; assertions . assert null ( user . get pets ( ) . get ( __num__ ) . get pet name ( ) @$ __str__ ) ;,partial works same
the request has no body @$ or it has a <PLACE_HOLDER> encoding we do not support . in either case @$ we read any data available,log . debug ( __str__ ) ; while ( in . available ( ) > __num__ && ( ( length = in . read ( buffer ) ) != - __num__ ) ) { log . debug ( __str__ @$ length ) ; out . write ( buffer @$ __num__ @$ length ) ; },request has new
each entry has the <PLACE_HOLDER> of the row number,b = new array list < > ( arrays . as list ( big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) ) ) ; f = new array list < > ( arrays . as list ( big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) @$ big integer . value of ( __num__ ) ) ) ; input tableau ( builder ) ; builder . seq ( seq -> { secret tableau . debug info ( seq @$ ps ) ; return null ; } ) ;,entry has value
because the cluster has only 3 <PLACE_HOLDER> @$ and 2 of which are decomm'ed @$ the last block file will remain under replicated .,init exclude hosts ( nodes ) ; refresh nodes ( __num__ ) ;,cluster has nodes
the property ca n't have any <PLACE_HOLDER>,return ;,property have values
worker might have <PLACE_HOLDER> that the master does n't yet know about @$ e.g . ufs specific <PLACE_HOLDER> @$ or <PLACE_HOLDER> from a different version of alluxio .,return new property key . builder ( name ) . set is built in ( false ) . build unregistered ( ) ;,worker have properties
so that everyone sees the <PLACE_HOLDER> .,query contact status ( parent provider . get aim connection ( ) . get screenname ( ) . get formatted ( ) ) ;,everyone sees message
we speculate on the first compilation . the global value numbering will merge the two floating integer exact operation <PLACE_HOLDER> .,installed code code = get code ( method ) ; code . execute varargs ( x @$ y ) ; if ( ! code . is valid ( ) ) { code = get code ( method ) ; code . execute varargs ( x @$ y ) ; assert true ( code . is valid ( ) ) ; },numbering merge calls
every selectable node in the tree is either of type function tree container node or of type function tree function node . only nodes of the first type contain assembler <PLACE_HOLDER> that is displayed .,if ( selected node instanceof function tree block node ) { final function tree block node block node = ( function tree block node ) selected node ; final basic block basic block = block node . get basic block ( ) ; final string builder text = new string builder ( __str__ ) ; for ( final instruction instruction : basic block . get instructions ( ) ) { text . append ( instruction . to string ( ) + __str__ ) ; } m_field . set text ( text . to string ( ) ) ; m_field . set caret position ( __num__ ) ; } else { m_field . set text ( __str__ ) ; },nodes contain input
we are careful here to avoid putting the value id into file metadata if we already have a digest . arbitrary filesystems may do weird <PLACE_HOLDER> with the value id ; a digest is more robust .,return new regular file state value ( stat . get size ( ) @$ digest @$ null ) ;,digest do things
simply return do n't require <PLACE_HOLDER>,return ;,return require cache
e.<PLACE_HOLDER> <PLACE_HOLDER>reat reat<PLACE_HOLDER> so match <PLACE_HOLDER> @$ <PLACE_HOLDER> and reat @$ reat,if ( is scrambled ( input1 @$ input2 @$ start1 @$ start1 + len @$ end2 - len @$ end2 @$ mem map ) && is scrambled ( input1 @$ input2 @$ start1 + len + __num__ @$ end1 @$ start2 @$ end2 - len - __num__ @$ mem map ) ) { mem map . put ( index @$ true ) ; return true ; },reatg match reat
since the work used the immediate <PLACE_HOLDER> @$ it is unaffected by the main looper being paused .,assert equals ( __num__ @$ x . get ( ) ) ; observable . just ( __num__ ) . compose ( observe forui ( ) ) . subscribe ( x :: set ) ;,work used mutator
make sure vm 3 can deserialize the <PLACE_HOLDER>,deserialization future = vm3 . invoke async ( ( ) -> { final region r = cache . get region ( region . separator + get test method name ( ) + __str__ ) ; pdx value result = ( pdx value ) r . get ( key_0 ) ; assert equals ( result @$ new pdx value ( __num__ ) ) ; } ) ; try { deserialization future . await ( __num__ @$ time unit . seconds ) ; fail ( __str__ ) ; } catch ( timeout exception e ) { },vm deserialize value
existing tables and partitions may have <PLACE_HOLDER> in a different order than the writer is providing @$ so build an index to rearrange <PLACE_HOLDER> in the proper order,list < string > file column names = get column names ( schema ) ; list < type > file column types = get column types ( schema ) . stream ( ) . map ( hive type -> hive type . get type ( type manager ) ) . collect ( to list ( ) ) ; int [ ] file input column indexes = file column names . stream ( ) . map to int ( input column names :: index of ) . to array ( ) ; try { file system file system = hdfs environment . get file system ( session . get user ( ) @$ path @$ configuration ) ; output stream output stream = file system . create ( path,tables have columns
change needs <PLACE_HOLDER>,final remote animation adapter adapter = new remote animation adapter ( runner @$ __num__ @$ __num__ @$ true ) ;,change needs snapshot
properties @$ return an <PLACE_HOLDER> of strings representing its groups .,string [ ] test groups = new string [ ] { __str__ @$ __str__ @$ __str__ } ; do test get groups ( arrays . as list ( test groups ) ) ;,properties return set
if the context has the effective <PLACE_HOLDER> @$ we do n't need to schedule an extra task .,if ( effective deadline != null && ! effective deadline . equals ( context . get deadline ( ) ) && deadline cancellation executor != null ) { deadline cancellation future = start deadline timer ( effective deadline ) ; },context has line
this locator will hook <PLACE_HOLDER> up with the first membership to be created,l = internal locator . start locator ( port @$ new file ( __str__ ) @$ null @$ null @$ local host @$ false @$ new properties ( ) @$ null @$ temporary folder . get root ( ) . to path ( ) ) ;,locator hook itself
but not again after we denied ourselves write <PLACE_HOLDER> with an acl update,verify denied ( incrementq2 @$ user_other @$ group_user ) ;,ourselves write access
update should not add new <PLACE_HOLDER> !,assert that ( cursor . get count ( ) ) . is equal to ( users for update . size ( ) ) ;,update add rows
check if contact has additional phone <PLACE_HOLDER> @$ if yes show the call button,meta contact phone util contact phone util = null ; details response listener details listener = null ;,contact has ui
derive flink internal <PLACE_HOLDER> from explicitly configure task heap <PLACE_HOLDER> size and managed <PLACE_HOLDER> size,final memory size task heap memory size = get task heap memory size ( config ) ; final memory size managed memory size = get managed memory size ( config ) ; final memory size framework heap memory size = get framework heap memory size ( config ) ; final memory size framework off heap memory size = get framework off heap memory size ( config ) ; final memory size task off heap memory size = get task off heap memory size ( config ) ; final memory size shuffle memory size ; final memory size total flink exclude shuffle memory size = framework heap memory size . add ( framework off heap memory size ) . add ( task heap memory size ) . add (,derive flink memory
thread which will mess around <PLACE_HOLDER> .,final query index idx = index ( idx_name_1 @$ field ( field_name_1 ) ) ; ignite internal future idx fut = multithreaded async ( new callable < void > ( ) { @ override public void call ( ) throws exception { boolean exists = false ; while ( ! stopped . get ( ) ) { ignite node = grid ( thread local random . current ( ) . next int ( __num__ @$ __num__ ) ) ; ignite internal future fut ; if ( exists ) { fut = query processor ( node ) . dynamic index drop ( cache_name @$ cache_name @$ idx_name_1 @$ true ) ; exists = false ; } else { fut = query processor ( node ) . dynamic index create,which mess indexes
first transition triggers the <PLACE_HOLDER>,state . goto state ( connecting ) ; assert equals ( __num__ @$ executor . run due tasks ( ) ) ; assert equals ( __num__ @$ sink . size ( ) ) ; assert equals ( connecting @$ sink . poll ( ) ) ; assert equals ( __num__ @$ executor . num pending tasks ( ) ) ;,transition triggers callback
we return file so that caller that wants to write knows <PLACE_HOLDER> the next available has location is,if ( desired return type == uri . class ) { return ( t ) file ; },available has what
sequenced event is a package private class @$ which can not be instantiated by importing . so use reflection to create an <PLACE_HOLDER> .,try { class < ? extends awt event > seq class = ( class < ? extends awt event > ) class . for name ( __str__ ) ; constructor < ? extends awt event > seq const = seq class . get constructor ( awt event . class ) ; seq const . set accessible ( true ) ; return seq const . new instance ( wrap me ) ; } catch ( throwable err ) { throw new runtime exception ( __str__ @$ err ) ; },reflection create instance
table follows <PLACE_HOLDER>,if ( opcode == opcodes . switch ) { return false ; },table follows switch
now @$ we have to make sure the finalizer gets run . we will keep allocating more and more memory with the idea that eventually @$ the memory occupied by the big object will get <PLACE_HOLDER> and the finalizer will be run .,list hold alot = new array list ( ) ; for ( int chunk = __num__ ; chunk > __num__ ; chunk = chunk / __num__ ) { if ( finalizer run ) { return ; } try { while ( true ) { hold alot . add ( new byte [ chunk ] ) ; system . err . println ( __str__ + chunk ) ; } } catch ( throwable thrown ) { system . gc ( ) ; } system . run finalization ( ) ; },memory get updated
app : released api 10 <PLACE_HOLDER> : released api 20,verify compute target sdk version ( older_version @$ released @$ true @$ older_version ) ;,app released dev
if the key is the left operand @$ then reflect the <PLACE_HOLDER> before the index lookup,int op = reflect on operator ( index info . _key ( ) ) ;,then reflect operator
if its a managed parent <PLACE_HOLDER> @$ it might not have any children,if ( ( q . children == null || q . children . is empty ( ) ) && ! ( q . parent queue instanceof managed parent queue ) ) { return immutable set . of ( q . queue name ) ; } set < string > leaf queue names = new hash set < > ( ) ; for ( temp queue per partition child : q . children ) { leaf queue names . add all ( get leaf queue names ( child ) ) ; } return leaf queue names ;,a managed queue
it is fully initialized . the previous implementation of this rule did not protect <PLACE_HOLDER> of ha region queue and caused the bug .,return this . owning queue . is queue initialized ( ) ? this . owning queue : null ;,implementation protect instances
and now fire the message received <PLACE_HOLDER> .,fire message received ( message @$ from ) ;,message received event
check memory includes the new <PLACE_HOLDER> .,process tree . update process tree ( ) ; assert . assert equals ( __str__ @$ __num__ @$ process tree . get virtual memory size ( ) ) ; if ( ! smap enabled ) { long cumu rss mem = procfs based process tree . page_size > __num__ ? __num__ * procfs based process tree . page_size : resource calculator process tree . unavailable ; assert . assert equals ( __str__ @$ cumu rss mem @$ process tree . get rss memory size ( ) ) ; } else { assert . assert equals ( __str__ @$ __num__ * kb_to_bytes * __num__ @$ process tree . get rss memory size ( ) ) ; },memory includes configuration
if the jvm is not doing url <PLACE_HOLDER> @$ then the jar files will not be cached either @$ and so they are safe to close,if ( ! get use caches ( ) ) { if ( _jar file != null ) { try { if ( log . is debug enabled ( ) ) log . debug ( __str__ + _jar file . get name ( ) ) ; _jar file . close ( ) ; } catch ( io exception ioe ) { log . ignore ( ioe ) ; } } } _jar file = null ; super . close ( ) ;,jvm doing lookup
expected only metadata update proposed <PLACE_HOLDER> for update schema .,assert custom messages ( __num__ ) ; assert communication messages ( ) ;,metadata update call
delete again should not throw any <PLACE_HOLDER>,try { dlm . delete ( ) ; } catch ( io exception ioe ) { fail ( __str__ ) ; },delete throw exceptions
this index already exists @$ verify meta data <PLACE_HOLDER> with expectations,mutable boolean paged file open = new mutable boolean ( true ) ; boolean success = false ; try { meta meta = read meta ( null @$ paged file ) ; paged file = map with correct page size ( page cache @$ index file @$ paged file @$ meta . get page size ( ) @$ paged file open @$ open options ) ; success = true ; return paged file ; } catch ( illegal state exception e ) { throw new metadata mismatch exception ( __str__ @$ e ) ; } finally { if ( ! success && paged file open . boolean value ( ) ) { paged file . close ( ) ; } },index exists match
the hadoop factory creates various <PLACE_HOLDER>,return __str__ ;,factory creates handlers
now create and destroy all of the entries in the new oplog . this should cause us to remove the crf but leave the drf @$ which has creates in reverse order . now we have garbage <PLACE_HOLDER> which have higher i ds than any crate,region . put ( __str__ @$ __str__ ) ; region . put ( __str__ @$ __str__ ) ; region . put ( __str__ @$ __str__ ) ; region . destroy ( __str__ ) ; region . destroy ( __str__ ) ; region . destroy ( __str__ ) ; region . destroy ( __str__ ) ; store . force roll ( ) ;,which have entries
get <PLACE_HOLDER>s should return the <PLACE_HOLDER>,object [ ] spans = spannable . get spans ( __num__ @$ spannable . length ( ) @$ m class ) ; assert not null ( spans ) ; assert that ( spans @$ array with size ( __num__ ) ) ; assert same ( m watcher @$ spans [ __num__ ] ) ;,spans return span
assigned <PLACE_HOLDER> for task will either contain all <PLACE_HOLDER> for the task or be empty @$ so just add all,assigned partitions . add all ( assigned partitions for task ) ;,partitions contain partitions
if the entity has deleted <PLACE_HOLDER> @$ then update that as well,if ( entry . get deleted state ( ) != null ) { entry . get deleted state ( ) [ lazy property numbers [ j ] ] = lazy property types [ j ] . deep copy ( prop value @$ factory ) ; } return field name . equals ( lazy property names [ j ] ) ;,entity deleted properties
modify the grammar to make checksum comparison detect a <PLACE_HOLDER>,try ( change change = change . of ( base grammar @$ __str__ ) ) { byte [ ] test lexer sum = checksum ( gen test lexer ) ; byte [ ] test parser sum = checksum ( gen test parser ) ; byte [ ] hello sum = checksum ( gen hello ) ; maven . execute mojo ( session @$ project @$ exec ) ; assert false ( arrays . equals ( test lexer sum @$ checksum ( gen test lexer ) ) ) ; assert false ( arrays . equals ( test parser sum @$ checksum ( gen test parser ) ) ) ; assert true ( arrays . equals ( hello sum @$ checksum ( gen hello ) ) ) ; },comparison detect change
method name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( operation id ) ) { logger . warn ( operation id + __str__ + camelize ( sanitize name ( __str__ + operation id ) @$ true ) ) ; operation id = __str__ + operation id ; } return camelize ( sanitize name ( operation id ) @$ true ) ;,name use keyword
process instance create <PLACE_HOLDER>,flowable engine entity event event = ( flowable engine entity event ) listener . get events received ( ) . get ( __num__ ) ; assert equals ( flowable engine event type . entity_created @$ event . get type ( ) ) ; assert equals ( process instance . get id ( ) @$ ( ( process instance ) event . get entity ( ) ) . get id ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get process instance id ( ) ) ; assert equals ( process instance . get id ( ) @$ event . get execution id ( ) ) ; assert equals ( process instance . get process definition id ( ) @$ event,instance create event
allow all scored <PLACE_HOLDER> @$ unless very common,int tagged word iw = new int tagged word ( word @$ null tag ) ; if ( seen counter . get count ( iw ) > smooth in unknowns threshold ) { return rules with word [ word ] . iterator ( ) ; } else { word taggings = new array list < > ( __num__ ) ; for ( int tagged word itw2 : tags ) { int tagged word itw = new int tagged word ( word @$ itw2 . tag ) ; if ( score ( itw @$ loc @$ word index . get ( word ) @$ null ) > float . negative_infinity ) { word taggings . add ( itw ) ; } } },all scored words
destination creation will trigger the <PLACE_HOLDER> since the virtual topic exists,final destination statistics destination statistics = local broker . get destination ( new activemq queue ( test queue name ) ) . get destination statistics ( ) ; final destination statistics remote dest statistics = remote broker . get destination ( new activemq queue ( __str__ ) ) . get destination statistics ( ) ; thread . sleep ( __num__ ) ; assert advisory broker counts ( __num__ @$ __num__ @$ __num__ ) ;,creation trigger gc
if both old and new table does not satisfies the <PLACE_HOLDER> @$ then do n't dump the event .,log . info ( __str__ ) ; return false ;,table satisfies policy
all the containers @$ the container monitor and all the container . the container scheduler may use its own <PLACE_HOLDER> .,return new container scheduler ( cntxt @$ dispatcher @$ metrics ) ;,containers use matcher
if the query contains more than one <PLACE_HOLDER>,if ( query properties . get join count ( ) > __num__ ) { profilescbo . add ( extendedcbo profile . join_reordering ) ; },query contains operator
app is using upgrade key <PLACE_HOLDER> ; make sure all are valid,long [ ] upgrade key sets = old ps . key set data . get upgrade key sets ( ) ; for ( int i = __num__ ; i < upgrade key sets . length ; i ++ ) { if ( ! is id valid key set id ( upgrade key sets [ i ] ) ) { slog . wtf ( tag @$ __str__ + ( old ps . name != null ? old ps . name : __str__ ) + __str__ + upgrade key sets [ i ] + __str__ ) ; return false ; } } return true ;,app using sets
no direct instances present @$ however another two role <PLACE_HOLDER> subs another single role <PLACE_HOLDER> and has instances,entity type another single role entity = tx . get entity type ( __str__ ) ; assert false ( test tx . rule cache ( ) . absent types ( collections . singleton ( another single role entity ) ) ) ;,present subs entity
converts interpolated <PLACE_HOLDER> back to polar .,double lat = atan2 ( z @$ sqrt ( x * x + y * y ) ) ; double lng = atan2 ( y @$ x ) ; return new lat lng ( to degrees ( lat ) @$ to degrees ( lng ) ) ;,converts interpolated vector
new entries should not affect any <PLACE_HOLDER>,assert . assert equals ( record . get record metadata ( __str__ ) @$ __str__ ) ; assert . assert null ( derived1 . get record metadata ( __str__ ) ) ; assert . assert null ( derived2 . get record metadata ( __str__ ) ) ; assert . assert null ( derived3 . get record metadata ( __str__ ) ) ; assert . assert equals ( derived1 . get record metadata ( __str__ ) @$ __str__ ) ; assert . assert null ( record . get record metadata ( __str__ ) ) ; assert . assert null ( derived2 . get record metadata ( __str__ ) ) ; assert . assert null ( derived3 . get record metadata ( __str__ ) ) ; assert . assert equals,entries affect copies
associate generated component virtual file with mocked generated component <PLACE_HOLDER>,psi class mocked generated component cls = mock ( psi class . class ) ; psi file mocked generated component file = mock ( psi file . class ) ; when ( mocked generated component cls . get containing file ( ) ) . then return ( mocked generated component file ) ; virtual file generated component virtual file = create present in scope virtual file ( ) ; when ( mocked generated component file . get virtual file ( ) ) . then return ( generated component virtual file ) ; virtual file present in scope virtual file = create present in scope virtual file ( ) ;,associate generated cls
delta could not be extracted . we would need to send full <PLACE_HOLDER> . or a create operation has a <PLACE_HOLDER> which has delta . but we send full <PLACE_HOLDER> for create . so serialize it .,if ( this . delta bytes == null || is create ( ) ) { this . _value = serialized value = cache server helper . serialize ( latest value ) ; },which has value
call the method from the parent class which initializes the <PLACE_HOLDER> .,super . init debugger ( ) ;,which initializes debugger
only 1 process can have this process <PLACE_HOLDER>,break ;,process have state
attach the original predicate to the table scan <PLACE_HOLDER> for index optimizations that require the pushed predicate before pcr & later optimizations are applied,if ( hive conf . get bool var ( hive conf @$ hive conf . conf vars . hiveoptindexfilter ) ) { table scan desc . set filter expr ( original predicate ) ; },predicate scan operator
chris added this next test in 2017 ; a bit weird @$ but kbp setup does n't have <PLACE_HOLDER> in sentence boundary patterns @$ just in to discard,if ( abstract tokenizer . newline_token . equals ( word ) ) { last token was newline = true ; } if ( debug ) { log . info ( __str__ + word + __str__ + debug text ) ; },setup have constraints
when slow reach the <PLACE_HOLDER> of the loop with k step @$ fast already goes for 2 k steps @$ so it is k steps ahead of the <PLACE_HOLDER> of the loop .,return null ;,slow reach end
collator.get keyword values return the same <PLACE_HOLDER> for both commonly used true and false .,string [ ] all = collator . get keyword values for locale ( __str__ @$ loc @$ false ) ; boolean match all = false ; if ( pref . length == all . length ) { match all = true ; for ( int j = __num__ ; j < pref . length ; j ++ ) { boolean found match = false ; for ( int k = __num__ ; k < all . length ; k ++ ) { if ( pref [ j ] . equals ( all [ k ] ) ) { found match = true ; break ; } } if ( ! found match ) { match all = false ; break ; } } } if ( ! match,values return value
stub notify shared cache <PLACE_HOLDER> to return true,do return ( true ) . when ( spied ) . notify shared cache manager ( isa ( string . class ) @$ isa ( string . class ) ) ; assert true ( spied . call ( ) ) ;,stub notify manager
ensure that snapshot directory will always use the local file <PLACE_HOLDER> instead of the default file <PLACE_HOLDER>,configuration configuration = new configuration ( ) ; configuration . set string ( core options . default_filesystem_scheme @$ __str__ ) ; file system . initialize ( configuration ) ; final file folder root = temporary folder . get root ( ) ; try { file folderb = new file ( folder root @$ string . value of ( uuid . randomuuid ( ) ) ) ; snapshot directory snapshot directoryb = snapshot directory . temporary ( folderb ) ; assert . assert equals ( snapshot directoryb . get file system ( ) @$ file system . get local file system ( ) ) ; } finally { file system . initialize ( new configuration ( ) ) ; },directory use scheme
handle @$ mark as processed @$ unless the <PLACE_HOLDER> handler threw an <PLACE_HOLDER>,exception handler . handle event exception ( ex @$ next sequence @$ event ) ; processed sequence = true ;,handler threw exception
warning the <PLACE_HOLDER> in the finally clause will trump any <PLACE_HOLDER> before,if ( failed ) return failure ;,return trump errors
event with no message has severity level <PLACE_HOLDER>,assert false ( filter . accept ( ev ) @$ __str__ ) ; final severity level error level = severity level . error ; final localized message error message = new localized message ( __num__ @$ __num__ @$ __str__ @$ __str__ @$ null @$ error level @$ null @$ get class ( ) @$ null ) ; final audit event ev2 = new audit event ( this @$ __str__ @$ error message ) ; assert true ( filter . accept ( ev2 ) @$ __str__ + error level ) ; final severity level info level = severity level . info ; final localized message info message = new localized message ( __num__ @$ __num__ @$ __str__ @$ __str__ @$ null @$ info level @$ null @$ get class,event has info
if the new candidate has more <PLACE_HOLDER> than previously tracked mc vs we insert it and bubble others down,int j ; for ( j = num tracked - __num__ ; j > __num__ ; j -- ) { if ( duplicates <= most common value candidates [ j - __num__ ] . count ) { break ; } most common value candidates [ j ] . count = most common value candidates [ j - __num__ ] . count ; most common value candidates [ j ] . first = most common value candidates [ j - __num__ ] . first ; } most common value candidates [ j ] . count = duplicates ; most common value candidates [ j ] . first = i - duplicates ;,candidate has values
we allow one trailing <PLACE_HOLDER>,if ( flavor != config syntax . json && t == tokens . close_square ) { put back ( t ) ; } else { throw parse error ( __str__ + t + __str__ + t + __str__ ) ; },one trailing newline
bob can read <PLACE_HOLDER>,wc = j . create web client ( ) . login ( __str__ ) ; wc . go to ( __str__ ) ;,bob read url
check that the jar does not have an <PLACE_HOLDER> for the removed class,try ( jar file abi jar = new jar file ( abi jar path . to file ( ) ) ) { assert that ( abi jar . stream ( ) . map ( jar entry :: get name ) . collect ( collectors . to set ( ) ) @$ matchers . contains in any order ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; manifest manifest = abi jar . get manifest ( ) ; assert null ( manifest . get attributes ( __str__ ) ) ; },jar have entry
sensless tagging : josm does create a <PLACE_HOLDER> here . we follow the highway tag :,way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . clear tags ( ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . can skip ( ) ) ; way . set tag ( __str__ @$ __str__ ) ; assert true ( encoder . get access ( way ) . is way ( ) ) ; way . clear tags ( ) ; way . set tag ( __str__,josm create line
inherit connect <PLACE_HOLDER> from server pool configuration .,for ( ldap server server : servers ) { if ( server . get connect timeout ( ) == __num__ && timeout != __num__ ) { server . set connect timeout ( timeout ) ; } } return this ;,inherit connect timeout
one in 4 tests generate a sync and volt bulk loader internal state <PLACE_HOLDER>,if ( ! abort1 && rnd . next int ( ) % __num__ == __num__ ) { bulk loader1 . drain ( ) ; assert ( bulk loader1 . get outstanding row count ( ) == __num__ ) ; assert ( bulk loader1 . get completed row count ( ) == row cnt1 ) ; },bulk loader verification
url contains port <PLACE_HOLDER>,if ( - __num__ != colon index ) { if ( baseurl . starts with ( __str__ ) && baseurl . ends with ( __str__ ) ) { baseurl = baseurl . substring ( __num__ @$ colon index ) ; } else if ( baseurl . starts with ( __str__ ) && baseurl . ends with ( __str__ ) ) { baseurl = baseurl . substring ( __num__ @$ colon index ) ; } },url contains number
then : '' collected must remove <PLACE_HOLDER> '',assert that ( tap . block ( ) ) . contains exactly ( __num__ @$ __num__ @$ __num__ ) ;,collected remove duplicates
checking if the current continuous frame contains a correct <PLACE_HOLDER> with the other frames combined,if ( curop == opcode . continuous && current continuous frame != null ) { add to buffer list ( frame . get payload data ( ) ) ; },frame contains length
required features expect activity <PLACE_HOLDER>,wait for latch ( latch ) ; verify ( m mock account manager response ) . on error ( account manager . error_code_invalid_response @$ account manager service test fixtures . error_message ) ; verify ( m mock account manager response @$ never ( ) ) . on result ( any ( bundle . class ) ) ;,features expect launch
identify the x position at which to truncate the final <PLACE_HOLDER> : note : the left position of the <PLACE_HOLDER> is needed for the case of rtl text .,final float ellipsis target = layout width - bounds . width ( ) + new layout . get line left ( ellipsized line number ) ;,which truncate line
create the parts add any <PLACE_HOLDER>,for ( j meter property j meter property : get arguments ( ) ) { http argument arg = ( http argument ) j meter property . get object value ( ) ; string parameter name = arg . get name ( ) ; if ( arg . is skippable ( parameter name ) ) { continue ; } string body string body = new string body ( arg . get value ( ) @$ content type . create ( arg . get content type ( ) @$ charset ) ) ; form body part form part = form body part builder . create ( parameter name @$ string body ) . build ( ) ; multipart entity builder . add part ( form part ) ; },parts add parameters
adjust <PLACE_HOLDER> index to pickup first range which contains <PLACE_HOLDER>,try { record rec = range table . get record at or before ( start ) ; if ( rec != null && rec . get long value ( range_to_col ) >= start ) { start = rec . get key ( ) ; } map rec iter = map table . index iterator ( map_range_key_col @$ new long field ( start ) @$ new long field ( end ) @$ true ) ; } catch ( io exception e ) { err handler . db error ( e ) ; },which contains key
battery stats impl.update cpu <PLACE_HOLDER> locked,m battery stats impl . set power profile ( null ) ; m battery stats impl . update time bases locked ( unplugged @$ screen state @$ up time @$ real time ) ; m battery stats impl . set power profile ( m power profile ) ;,stats impl.update cycles
we only look for a partial match which does n't match on subid because we can crossfade views that match <PLACE_HOLDER> except for subid .,for ( pair < view @$ string > p : transition view pairs ) { if ( tn . partial equals ( transition name . parse ( view compat . get transition name ( p . first ) ) ) ) { it . remove ( ) ; break ; } },which match everything
succeed if user has <PLACE_HOLDER> for all regions and all keys for the given operation,if ( allowed permissions . contains ( new resource permission ( resource permission . resource . data @$ permission . get operation ( ) ) ) ) { return true ; },user has permission
check that the alluxio <PLACE_HOLDER> we 're creating does n't shadow a <PLACE_HOLDER> in the parent ufs,mount table . resolution resolution = m mount table . resolve ( alluxio path ) ; try ( closeable resource < under file system > ufs resource = resolution . acquire ufs resource ( ) ) { string ufs resolved path = resolution . get uri ( ) . get path ( ) ; if ( ufs resource . get ( ) . exists ( ufs resolved path ) ) { throw new io exception ( exception message . mount_path_shadows_parent_ufs . get message ( alluxio path @$ ufs resolved path ) ) ; } },path shadow path
create a walker which walks the <PLACE_HOLDER> in a dfs manner while maintaining the operator stack .,map < rule @$ node processor > op rules = new linked hash map < rule @$ node processor > ( ) ; op rules . put ( new rule reg exp ( __str__ @$ reduce sink operator . get operator name ( ) + __str__ ) @$ new set reducer parallelism ( ) ) ; op rules . put ( new rule reg exp ( __str__ @$ join operator . get operator name ( ) + __str__ ) @$ new convert join map join ( ) ) ; if ( proc ctx . conf . get bool var ( conf vars . hivemapaggrhashminreductionstatsadjust ) ) { op rules . put ( new rule reg exp ( __str__ @$ group by operator . get operator name ( ),which walks tree
if the score of this <PLACE_HOLDER> is higher or equal to that of this factory and some other factory is responsible for it @$ then this factory should not track the <PLACE_HOLDER> because it has no hope of satisfying it .,return ! n . requested && ( n . score < m score || n . factory serial number == m serial number ) && n . request . network capabilities . satisfied by network capabilities ( m capability filter ) && accept request ( n . request @$ n . score ) ;,factory track request
using just the package name is n't great @$ since it disallows having multiple engines in the same package @$ but that 's <PLACE_HOLDER> the existing api does .,engine . name = service . package name ; char sequence label = service . load label ( pm ) ; engine . label = text utils . is empty ( label ) ? engine . name : label . to string ( ) ; engine . icon = service . get icon resource ( ) ; engine . priority = resolve . priority ; engine . system = is system engine ( service ) ; return engine ;,api does what
now the tricky one . 'before a ' leads to 'call activity a ' @$ which calls <PLACE_HOLDER> 02 which terminates,process instance = runtime service . start process instance by key ( __str__ ) ; tasks = assert task names ( process instance @$ arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; task task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( task ) ; task service . complete ( task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ; assert historic process,which calls subprocess
forbid deprecated <PLACE_HOLDER> .,if ( python opts . incompatible use python toolchains ) { if ( opts . python2 path != null ) { reporter . handle ( event . error ( __str__ + __str__ ) ) ; } if ( opts . python3 path != null ) { reporter . handle ( event . error ( __str__ + __str__ ) ) ; } if ( opts . python top != null ) { reporter . handle ( event . error ( __str__ + __str__ + __str__ + __str__ + __str__ ) ) ; } },forbid deprecated usage
since the expression must have a single corresponding node <PLACE_HOLDER> is our default assumption,if ( expr nodes . size ( ) != __num__ ) { return nullness . nullable ; },expression have which
nodes to start holds the <PLACE_HOLDER> of nodes to be started immediately . we do n't want to start the animations in the loop directly because we first need to set up dependencies on all of the nodes . for example @$ we do n't want to start an animation when some other animation also wants to start when the first animation begins .,final array list < node > nodes to start = new array list < node > ( ) ; for ( int i = __num__ ; i < num sorted nodes ; ++ i ) { node node = m sorted nodes . get ( i ) ; if ( m set listener == null ) { m set listener = new animator set listener ( this ) ; } if ( node . dependencies == null || node . dependencies . size ( ) == __num__ ) { nodes to start . add ( node ) ; } else { int num dependencies = node . dependencies . size ( ) ; for ( int j = __num__ ; j < num dependencies ; ++ j ),nodes holds list
signal for factory to start ; sets start <PLACE_HOLDER>,start flag . count down ( ) ;,sets start flag
make <PLACE_HOLDER> keys change multimedia <PLACE_HOLDER> even if music is not playing now,set volume control stream ( audio manager . stream_music ) ; try { play services available = google api availability . get instance ( ) . is google play services available ( this ) == connection result . success ; } catch ( exception ignored ) { } if ( play services available ) init cast ( ) ;,keys change volume
second call should throw <PLACE_HOLDER>,assert null ( shard expression . value ( ) ) ;,call throw exception
ok @$ lets compare this <PLACE_HOLDER> lexicographically,subversion comparision result = v comps1 [ i ] . compare to ( v comps2 [ i ] ) ;,lets compare object
type parameter does n't have declared upper <PLACE_HOLDER>,if ( ! node . has type bound ( ) ) { node . set type definition ( java type definition . for class ( upper_bound @$ object . class ) ) ; } else { super . visit ( node @$ data ) ; rollup type unary ( node ) ; },parameter declared bound
gcm mode needs additional <PLACE_HOLDER>,if ( cipher mode == gcm_mode ) { if ( tag len == - __num__ ) { tag len = galois counter mode . default_tag_len ; } if ( decrypting ) { min bytes = tag len ; } else { require reinit = arrays . equals ( iv bytes @$ last enc iv ) && message digest . is equal ( key bytes @$ last enc key ) ; if ( require reinit ) { throw new invalid algorithm parameter exception ( __str__ ) ; } last enc iv = iv bytes ; last enc key = key bytes ; } ( ( galois counter mode ) cipher ) . init ( decrypting @$ algorithm @$ key bytes @$ iv bytes @$ tag len ) ; },mode needs encoding
some np cs drop <PLACE_HOLDER> onto multiple tiles,final list < item stack > all items = new array list < > ( ) ; for ( int i = __num__ ; i < size ; ++ i ) { for ( int j = __num__ ; j < size ; ++ j ) { final int packed = ( x + i ) << __num__ | ( y + j ) ; final collection < item stack > items = item spawns . get ( packed ) ; all items . add all ( items ) ; } } if ( all items . is empty ( ) ) { return ; } kill points . add ( location ) ; event bus . post ( new npc loot received ( npc @$ all items,np cs items
it 's possible the default tag wo n't exist if the user just changed the app 's <PLACE_HOLDER> @$ in which case default to the first tag in the table,if ( ! reader tag table . tag exists ( tag ) ) { tag = reader tag table . get first tag ( ) ; } set current tag ( tag ) ; if ( build config . information_architecture_available && m is top level ) { if ( tag . is followed sites ( ) ) { m view model . set default subfilter ( ) ; } },user changed language
window do <PLACE_HOLDER> operator does not use a do <PLACE_HOLDER>,this . requires stable input = do fn != null && do fn signatures . get signature ( do fn . get class ( ) ) . process element ( ) . requires stable input ( ) ;,a do fn
finishing the tasks should also set the end <PLACE_HOLDER>,tasks = task service . create task query ( ) . process instance id ( process instance . get id ( ) ) . list ( ) ; assert equals ( __num__ @$ tasks . size ( ) ) ; for ( org . flowable . task . api . task task : tasks ) { task service . complete ( task . get id ( ) ) ; } wait for history job executor to process all jobs ( __num__ @$ __num__ ) ; historic activity instances = history service . create historic activity instance query ( ) . activity id ( __str__ ) . list ( ) ; assert equals ( __num__ @$ historic activity instances . size ( ) ) ; for ( historic activity,tasks set time
the pending intent to launch our activity if the user selects this <PLACE_HOLDER>,pending intent content intent = pending intent . get activity ( this @$ __num__ @$ new intent ( this @$ main activity . class ) @$ __num__ ) ;,user selects one
now test that compare to does what we expect . the exact ordering here does n't matter <PLACE_HOLDER> .,collections . shuffle ( paths ) ; collections . sort ( paths ) ; list < path fragment > expected order = to paths ( immutable list . of ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ; assert that ( paths ) . is equal to ( expected order ) ;,test matter much
user has specified initial context <PLACE_HOLDER> ; try to get it,if ( my props != null && my props . get ( context . initial_context_factory ) != null ) { get default init ctx ( ) ; },user specified factory
indicate the the client supports <PLACE_HOLDER>,join presence . add extension ( new muc initial presence ( password @$ max chars @$ max stanzas @$ seconds @$ since ) ) ;,client supports presence
the file we should write new <PLACE_HOLDER> to .,path output file ; if ( old file != null && old file . exists ( ) && ! is renaming ) { output file = old file ; } else { output file = new file ; },file write content
0 indicates an unrepresentable <PLACE_HOLDER>,return __num__ ;,0 indicates type
if memory does n't contain the target <PLACE_HOLDER>,if ( ! reference . is memory reference ( ) ) { continue ; } if ( ! memory . contains ( target ) ) { continue ; } if ( ignore new pointers . contains ( target ) ) { continue ; },memory contain address
if syntax error occurs then message is printed by error listener and parser throws this runtime <PLACE_HOLDER> to stop parsing . just stop processing current javadoc comment .,if ( parse error message == null ) { parse error message = error listener . get error message ( ) ; },parser throws exception
flow files those are already committed will not get <PLACE_HOLDER> .,session . rollback ( ) ;,files get run
we need to filter the index result if the property is not <PLACE_HOLDER> on some label since the nodes in the index might have both an <PLACE_HOLDER> and a dis<PLACE_HOLDER> label for the property,if ( ! access mode . allows traverse all nodes with label ( label ) || access mode . disallows read property for some label ( prop ) || ! access mode . allows read node property ( ( ) -> labels . from ( label ) @$ prop ) ) { return new node label security filter ( property ids @$ cursor @$ cursors . allocate node cursor ( ) @$ this @$ access mode ) ; },nodes have read
keystore contains a hostname which does n't match <PLACE_HOLDER>,client ssl context factory . set key store path ( __str__ ) ; client ssl context factory . set key store password ( __str__ ) ; queued thread pool client threads = new queued thread pool ( ) ; client threads . set name ( __str__ ) ; client = new http client ( client ssl context factory ) ; client . set executor ( client threads ) ; client . start ( ) ;,which match certificate
inspect the source ce 32 s. just copy <PLACE_HOLDER> if none are modified . otherwise copy to modified c es @$ with modifications .,boolean is modified = false ; for ( int i = __num__ ; i < length ; ++ i ) { ce32 = srcce32s [ src index + i ] ; long ce ; if ( collation . is specialce32 ( ce32 ) || ( ce = modifier . modifyce32 ( ce32 ) ) == collation . no_ce ) { if ( is modified ) { modifiedc es [ i ] = collation . ce fromce32 ( ce32 ) ; } } else { if ( ! is modified ) { for ( int j = __num__ ; j < i ; ++ j ) { modifiedc es [ j ] = collation . ce fromce32 ( srcce32s [ src index + j ] ) ; } is,s. copy them
zap : added the <PLACE_HOLDER> .,int current idx = __num__ ; if ( is exist status code ) { ps insert . set int ( current idx @$ status code ) ; ++ current idx ; },zap added statement
oss capabilities request a chrome <PLACE_HOLDER>,map < string @$ object > payload = immutable map . of ( __str__ @$ immutable map . of ( __str__ @$ __str__ ) @$ __str__ @$ immutable map . of ( __str__ @$ immutable map . of ( __str__ @$ __str__ ) @$ __str__ @$ immutable list . of ( immutable map . of ( __str__ @$ __str__ ) @$ immutable map . of ( __str__ @$ __str__ ) ) ) ) ;,capabilities request request
two 'union with aliases ' fields under different records but with the same field <PLACE_HOLDER> . the generated record representation for these two unions should include the parent record 's <PLACE_HOLDER> to avoid any <PLACE_HOLDER> conflicts .,return new object [ ] [ ] { { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ all modes @$ __str__ @$ null @$ null @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ all modes @$ __str__ @$ empty foo schema @$ empty foo value @$ null } @$ { __str__ @$ translate default @$ __str__ @$ empty foo schema @$ empty foo value @$ null },'union include name
the address should have <PLACE_HOLDER>,deque < property > result address = new array deque < > ( operations . get operation address ( logging configuration ) . as property list ( ) ) ; assert . assert true ( __str__ @$ result address . get last ( ) . get value ( ) . as string ( ) . contains ( __str__ ) ) ; model node handler = logging configuration . get ( __str__ @$ __str__ ) ; assert . assert true ( __str__ @$ handler . is defined ( ) ) ; assert . assert true ( handler . has defined ( __str__ ) ) ; string file name = null ;,address have logging
inline the wsdl as extensibility element write <PLACE_HOLDER> : metadata wrapper,if ( wsdl address != null ) { writer . write start element ( member submission addressing constants . mex_metadata . get prefix ( ) @$ member submission addressing constants . mex_metadata . get local part ( ) @$ member submission addressing constants . mex_metadata . get namespaceuri ( ) ) ; writer . write start element ( member submission addressing constants . mex_metadata_section . get prefix ( ) @$ member submission addressing constants . mex_metadata_section . get local part ( ) @$ member submission addressing constants . mex_metadata_section . get namespaceuri ( ) ) ; writer . write attribute ( member submission addressing constants . mex_metadata_dialect_attribute @$ member submission addressing constants . mex_metadata_dialect_value ) ; write wsdl ( writer @$ service @$ wsdl address ) ; writer,element write element
these ms counts all presume <PLACE_HOLDER>,double [ ] [ ] exp = new double [ ] [ ] { d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d ( __num__ @$ __num__ ) @$ d (,ms counts utc
clear 'links extracted ' <PLACE_HOLDER> .,this . link extractor finished = false ; extra info = null ; out links = null ; this . revisit profile = null ;,clear 'links information
the component has timeout <PLACE_HOLDER> @$ it needs a 'real ' timer service,if ( component description . is timer service required ( ) ) { final string deployment name ; if ( module description . get distinct name ( ) == null || module description . get distinct name ( ) . length ( ) == __num__ ) { deployment name = module description . get application name ( ) + __str__ + module description . get module name ( ) ; } else { deployment name = module description . get application name ( ) + __str__ + module description . get module name ( ) + __str__ + module description . get distinct name ( ) ; } root_logger . debugf ( __str__ @$ component description . get component name ( ) ) ; component description . get,component has request
now we must read and convert this file to the target format with the target size <PLACE_HOLDER> x <PLACE_HOLDER>,try { file new png file = new file ( png file . get absolute path ( ) + __str__ ) ; png file . rename to ( new png file ) ; final image img = image parser . parse ( png file . get absolute path ( ) @$ file utils . read ( new png file ) ) ; if ( img == null ) { return false ; } final image scaled = img . get scaled instance ( width @$ height @$ image . scale_area_averaging ) ; final media tracker media tracker = new media tracker ( new container ( ) ) ; media tracker . add image ( scaled @$ __num__ ) ; try { media tracker . wait forid ( __num__,1024 size x
modal bottom sheets have auto peek <PLACE_HOLDER> by default .,assert that ( behavior . get peek height ( ) @$ is ( bottom sheet behavior . peek_height_auto ) ) ;,sheets have height
we do n't have <PLACE_HOLDER> to read boolean creds s ince the creds have no boolean creds we should get an empty set,try { set priv cred set1 = s . get private credentials ( boolean . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { e . print stack trace ( ) ; throw new runtime exception ( __str__ ) ; } system . out . println ( __str__ ) ;,creds have permission
second @$ check if this work has multiple reduce sinks . if so @$ do <PLACE_HOLDER> .,split base work ( spark work @$ work @$ child works ) ;,second do them
for each aggregation <PLACE_HOLDER> in the reference node @$ create a corresponding aggregation <PLACE_HOLDER> that points to the null row . map the symbols from the aggregations in reference aggregation to the symbols in these new aggregations .,immutable map . builder < symbol @$ symbol > aggregations symbol mapping builder = immutable map . builder ( ) ; immutable map . builder < symbol @$ aggregation node . aggregation > aggregations over null builder = immutable map . builder ( ) ; for ( map . entry < symbol @$ aggregation node . aggregation > entry : reference aggregation . get aggregations ( ) . entry set ( ) ) { symbol aggregation symbol = entry . get key ( ) ; aggregation node . aggregation aggregation = entry . get value ( ) ; if ( ! is using symbols ( aggregation @$ sources symbol mapping . key set ( ) ) ) { return optional . empty ( ) ; } aggregation over,function node node
xlsx needs full <PLACE_HOLDER>,if ( data . wb instanceof xssf workbook ) { formula evaluator evaluator = data . wb . get creation helper ( ) . create formula evaluator ( ) ; for ( int sheet num = __num__ ; sheet num < data . wb . get number of sheets ( ) ; sheet num ++ ) { sheet sheet = data . wb . get sheet at ( sheet num ) ; for ( row r : sheet ) { for ( cell c : r ) { if ( c . get cell type ( ) == cell . cell_type_formula ) { evaluator . evaluate formula cell ( c ) ; } } } } } else if ( data . wb instanceof hssf workbook ) {,xlsx needs layout
this streamer had internal <PLACE_HOLDER> before getting updated block,if ( ! streamer . is healthy ( ) && coordinator . get new blocks ( ) . peek ( streamer . get index ( ) ) != null ) { failed . add ( streamer ) ; },streamer had updates
field field file name <PLACE_HOLDER>,wl field filename = new label ( w file name @$ swt . right ) ; wl field filename . set text ( base messages . get string ( pkg @$ __str__ ) ) ; props . set look ( wl field filename ) ; fdl field filename = new form data ( ) ; fdl field filename . left = new form attachment ( __num__ @$ __num__ ) ; fdl field filename . top = new form attachment ( w file name in field @$ margin ) ; fdl field filename . right = new form attachment ( middle @$ - margin ) ; wl field filename . set layout data ( fdl field filename ) ; w field filename = new c combo ( w file,field file line
test increase container api and make sure requests can reach <PLACE_HOLDER>,test increase container resource ( container ) ; test restart container ( container . get id ( ) ) ; test get container status ( container @$ i @$ container state . running @$ __str__ @$ exit code ) ; wait for container transition count ( container @$ org . apache . hadoop . yarn . server . nodemanager . containermanager . container . container state . running @$ __num__ ) ; if ( i % __num__ == __num__ ) { test re initialize container ( container . get id ( ) @$ clc @$ false ) ; test get container status ( container @$ i @$ container state . running @$ __str__ @$ exit code ) ; wait for container transition count ( container @$ org .,requests reach noise
we do our own adjustment as calendar can not handle a <PLACE_HOLDER> .,time -= value ; value to use = __num__ ;,calendar handle long
check that one of them has the correct match <PLACE_HOLDER>,boolean has match = false ; for ( match action match : patterns . get post patterns ( ) . get ( __num__ ) . get match actions ( ) ) { if ( ! ( match instanceof function start analyzer . context action ) ) { continue ; } has match = true ; assert equals ( __str__ @$ ( ( function start analyzer . context action ) match ) . get name ( ) ) ; assert equals ( new big integer ( __str__ ) @$ ( ( function start analyzer . context action ) match ) . get value ( ) ) ; } assert true ( has match ) ;,one has action
build period offset <PLACE_HOLDER>,offset criteria = new offset criteria . offset criteria builder ( ) . with offset as period ( __str__ ) ; assert . assert false ( offset criteria . is smallest ( ) ) ; assert . assert false ( offset criteria . is largest ( ) ) ; assert . assert true ( offset criteria . is period ( ) ) ; assert . assert false ( offset criteria . is custom ( ) ) ; assert . assert equals ( offset criteria . get offset string ( ) @$ __str__ ) ;,period offset criteria
each zoom level multiplies viewable <PLACE_HOLDER> by 2,for ( int i = default_min_zoom ; i < default_max_zoom ; i ++ ) { max intensity array [ i ] = get max value ( m data @$ m bounds @$ radius @$ ( int ) ( screen_size * math . pow ( __num__ @$ i - __num__ ) ) ) ; if ( i == default_min_zoom ) { for ( int j = __num__ ; j < i ; j ++ ) max intensity array [ j ] = max intensity array [ i ] ; } },level multiplies subimage
if the entity does n't have an <PLACE_HOLDER> @$ it will be created instead of just being updated,rest comment mock mvc . perform ( put ( __str__ ) . content type ( test util . application_json_utf8 ) . content ( test util . convert object to json bytes ( comment ) ) ) . and expect ( status ( ) . is created ( ) ) ;,entity have id
ensure that all headed tests use swing <PLACE_HOLDER> when displaying errors . setting this to false would force errors to only be written to the console .,set errorgui enabled ( true ) ;,tests use capture
weak values allows transform executor services that are no longer in use to be reclaimed . executing transform executor services have a strong <PLACE_HOLDER> to their transform executor service which stops the transform executor services from being prematurely garbage collected,serial executor services = cache builder . new builder ( ) . weak values ( ) . removal listener ( shutdown executor service listener ( ) ) . build ( serial transform executor service cache loader ( ) ) ; this . visible updates = new queue message receiver ( ) ; parallel executor service = transform executor services . parallel ( executor service ) ; executor factory = new direct transform executor . factory ( context @$ registry @$ transform enforcements ) ;,services have reference
load dexposed <PLACE_HOLDER> for hook .,return load dexposed lib ( context ) ;,load dexposed lib
the classes bellow have a static final secure random field . note that if the classes are not found as reachable by the analysis registering them form class initialization rerun does n't have any <PLACE_HOLDER> .,image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( clazz ( access @$ __str__ ) @$ __str__ ) ; image singletons . lookup ( runtime class initialization support . class ) . rerun initialization ( javax . net . ssl . ssl context . class @$ __str__ ) ;,rerun have effect
check whether the callable threw an <PLACE_HOLDER> .,if ( future . is done ( ) ) { try { future . get ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; throw new runtime exception ( e ) ; } it . remove ( ) ; },callable threw exception
add legacy subsystems the regular model will have the new attributes because they are in the xml @$ but the reverse controller model will not because transformation strips <PLACE_HOLDER>,builder . create legacy kernel services builder ( null @$ controller version @$ model version ) . add maven resourceurl ( __str__ + controller version . get maven gav version ( ) ) . configure reverse controller check ( additional initialization . management @$ model node -> { for ( model node node : model node . get ( global_modules ) . as list ( ) ) { if ( __str__ . equals ( node . get ( name ) . as string ( ) ) ) { if ( ! node . has ( annotations ) ) { node . get ( annotations ) . set ( false ) ; } if ( ! node . has ( meta_inf ) ) { node . get ( meta_inf,model strips them
bad value allows <PLACE_HOLDER> .,trigger action in cell editor ( key event . vk_escape ) ; assert not editing field ( ) ; assert equals ( __num__ @$ model . get num selected rows ( ) ) ; assert equals ( __num__ @$ model . get min index selected ( ) ) ; assert equals ( get data type ( __num__ ) @$ dt ) ;,value allows escape
if we attempt to interact with the server too quickly @$ we will get a zoo keeper connection loss exception @$ <PLACE_HOLDER> the provider wraps in an io exception . we will wait 1 second in this case and try again . the test will timeout if this does not succeeed within 20 seconds .,thread . sleep ( __num__ ) ; e . print stack trace ( ) ; assert . fail ( __str__ + e . get class ( ) + __str__ @$ e ) ;,provider wraps which
this could happen when the time zone is not associated with a country @$ and its id is not hierarchical @$ for example @$ cst 6 cdt . we use the canonical id <PLACE_HOLDER> as the location for this case .,if ( location == null ) { location = tzid ; },canonical id itself
create a simple rule which just writes <PLACE_HOLDER> new to the output file .,build target target = build target factory . new instance ( __str__ ) ; build rule rule = new write file ( target @$ filesystem @$ __str__ @$ output @$ false ) ;,which writes something
calls set image <PLACE_HOLDER> internally,super . set image bitmap ( bitmap ) ;,calls set bitmap
first dfs client has no files open so does n't renew <PLACE_HOLDER> .,final dfs client mock client1 = create mock client ( ) ; mockito . do return ( false ) . when ( mock client1 ) . renew lease ( ) ; assert same ( renewer @$ lease renewer . get instance ( fake_authority @$ fake_ugi_a @$ mock client1 ) ) ; long file id = __num__ ; renewer . put ( mock client1 ) ;,files renew lease
make sure any exceptions caused by handlers do n't crash <PLACE_HOLDER>,return noop aware finished span handler . create ( defensive copy @$ noop ) ;,exceptions crash threads
let the application handle the <PLACE_HOLDER> .,return __num__ ;,application handle rest
apply the predicates and fetch the primary key <PLACE_HOLDER>s look up the <PLACE_HOLDER> and convert them to bean,try { return run task ( new query task < list < e > > ( ) { @ override public list < e > handle ( connection connection ) throws exception { list < e > ret = new array list < > ( ) ; if ( ! ids to find . is empty ( ) ) { list < generic json entity > entities ; try ( prepared statement select statement = sql query builder . create find by id statement ( connection @$ generic json entity . class @$ ids to find ) ) { try ( result set result set = select statement . execute query ( ) ) { entities = generic result set mapper . map all ( result set @$,ids look collection
close server 1 and pause so server has <PLACE_HOLDER> to close,close server ( server1 ) ; wait . pause ( __num__ * __num__ ) ; wait for cqs disconnected ( client @$ __str__ @$ __num__ ) ;,server has time
promo snapshot collapses as the panel reaches the maximized <PLACE_HOLDER> .,update appearance ( __num__ - percentage ) ;,panel reaches view
if fifo <PLACE_HOLDER> reaches max <PLACE_HOLDER> we evict the eldest entry .,if ( queue . size ( ) > max size ) { evict one ( ) ; } return result ;,size reaches size
log a warning if the resource uses an unnecessary service error definition <PLACE_HOLDER>,if ( service error def annotation != null && ! resource model . is any service error list defined ( ) ) { log . warn ( string . format ( __str__ + __str__ + __str__ @$ resource class . get name ( ) @$ service error def . class . get simple name ( ) @$ service errors . class . get simple name ( ) @$ param error . class . get simple name ( ) ) ) ; },resource uses annotation
make sure the user can set the file <PLACE_HOLDER> to null @$ to allow the clearing of a value,options options = load search options ( ) ; help location help = null ; file default value = null ; string option name = __str__ ; options . register option ( option name @$ option type . file_type @$ default value @$ help @$ __str__ ) ; file option value = options . get file ( option name @$ null ) ; assert null ( option value ) ; file file = new file ( __str__ ) ; options . put object ( option name @$ file ) ; option value = options . get file ( option name @$ null ) ; assert equals ( file @$ option value ) ;,user set type
calculate the current radius at which to place the selection <PLACE_HOLDER> .,final int sel radius = m selector radius ; final float sel length = m circle radius - math utils . lerp ( hours inset @$ minutes inset @$ m hours to minutes ) ; final double sel angle rad = math . to radians ( math utils . lerp deg ( hours angle deg @$ minutes angle deg @$ m hours to minutes ) ) ; final float sel centerx = mx center + sel length * ( float ) math . sin ( sel angle rad ) ; final float sel centery = my center - sel length * ( float ) math . cos ( sel angle rad ) ;,which place circle
this newly opened jar file has its own <PLACE_HOLDER> @$ merge it into the parent 's <PLACE_HOLDER> @$ taking into account the relative path .,jar index new index = new loader . get index ( ) ; if ( new index != null ) { int pos = jar name . last index of ( __str__ ) ; new index . merge ( this . index @$ ( pos == - __num__ ? null : jar name . substring ( __num__ @$ pos + __num__ ) ) ) ; },file has index
the application should not use the <PLACE_HOLDER> to connect,assert user connected ( wc @$ username ) ;,application use cookie
disabling the single plan item will terminate the <PLACE_HOLDER>,cmmn runtime service . disable plan item instance ( plan item instance . get id ( ) ) ; assert case instance ended ( case instance ) ;,item terminate case
if user provides a config <PLACE_HOLDER> @$ use it as base configs .,if ( string utils . is none empty ( config file path ) ) { log . info ( __str__ + config file path ) ; final file config file = new file ( config file path ) ; final uri config uri = config file . touri ( ) ; final config factory config factory = options . get config factory ( ) . get declared constructor ( ) . new instance ( ) ; log . info ( __str__ + config factory . get class ( ) . get name ( ) ) ; if ( config factory instanceof properties config factory ) { check argument ( config file . exists ( ) @$ __str__ @$ config file path ) ; } config . put all,user provides file
if target has <PLACE_HOLDER> remaining @$ need n't to check the is read only,target . flip ( ) ; assert equals ( __num__ @$ source . read ( target ) ) ;,target has data
make sure task 1 is running and blocking the <PLACE_HOLDER>,assert that ( task1 running . await ( __num__ @$ time unit . seconds ) ) . is true ( ) ;,task running task
check if content matches minimum length <PLACE_HOLDER>,if ( algorithm . equals ( tlsh ) && flow file . get size ( ) < __num__ ) { return false ; } else { return true ; },content matches behavior
say that a null label matches no positive <PLACE_HOLDER> @$ but any negated patern,if ( lab == null ) { return negated pattern ; } else { if ( basic cat ) { lab = basic cat function . apply ( lab ) ; } matcher m = pattern . matcher ( lab ) ; return m . find ( ) != negated pattern ; },label matches pattern
if preflight timed out @$ m result will contain error <PLACE_HOLDER> as int .,if ( total size < __num__ ) { return ( int ) total size ; } if ( more_debug ) { slog . v ( tag @$ __str__ + total size ) ; } i backup transport transport = m transport client . connect or throw ( __str__ ) ; result = transport . check full backup size ( total size ) ; if ( result == backup transport . transport_quota_exceeded ) { if ( more_debug ) { slog . d ( tag @$ __str__ + pkg . package name + __str__ + total size + __str__ + m quota ) ; } remote call . execute ( callback -> agent . do quota exceeded ( total size @$ m quota @$ callback ) @$ m agent timeout,result contain message
this property is n't contributing <PLACE_HOLDER> this happens when you try to map an empty sequence to a property,if ( prop . ref ( ) . is empty ( ) ) return ;,property contributing something
the component has entered the focused <PLACE_HOLDER> either if it is larger than half of the viewport and it occupies at least half of the viewport or if it is smaller than half of the viewport and it is fully visible .,return ( total component area >= half viewport area ) ? ( visible component area >= half viewport area ) : component bounds . equals ( component visible bounds ) ;,component entered range
calculate how many <PLACE_HOLDER>s we need . as each task handles a separate <PLACE_HOLDER> of data @$ so we want the number of <PLACE_HOLDER>s equal to the number of tasks,int workergroup number = conf . get int ( angel conf . angel_workergroup_number @$ angel conf . default_angel_workergroup_number ) ; int task num in worker = conf . get int ( angel conf . angel_worker_task_number @$ angel conf . default_angel_worker_task_number ) ; int split num = workergroup number * task num in worker ; log . info ( __str__ + split num ) ; if ( ! use newapi ) { log . info ( __str__ ) ; org . apache . hadoop . mapred . input split [ ] split array = generate splits use oldapi ( conf @$ split num ) ; log . info ( __str__ + split array . length ) ; if ( log . is debug enabled ( ) ) { int,task handles worker
usually happens when permission denied listing <PLACE_HOLDER> in directory,log . e ( __str__ @$ __str__ + remote path @$ e ) ;,permission denied files
assume properties contain <PLACE_HOLDER>,if ( property value == null ) { if ( require property ) { throw new illegal argument exception ( __str__ + property key + __str__ + property key + __str__ ) ; } else { return null ; } } property value = get slashy path ( property value ) ; property value = correct double slash ( property value @$ property index end @$ str ) ; result += property value ; property index end ++ ; property index start = property index end ;,properties contain paths
avoid output warn : <PLACE_HOLDER> rejected,request config global config = request config . custom ( ) . set cookie spec ( cookie specs . ignore_cookies ) . build ( ) ; builder . set default request config ( global config ) ; closeable http client http client = builder . build ( ) ; return http client ;,output warn cookies
merge input <PLACE_HOLDER> : data <PLACE_HOLDER> override header <PLACE_HOLDER>,if ( fields from header != null ) { if ( ! ( fields from header instanceof point builder ) ) { throw new illegal state exception ( __str__ + fields from header ) ; } fields from data . merge with header ( ( point builder ) fields from header ) ; },fields override fields
if the ancestor has a forced preferred <PLACE_HOLDER> @$ its layout manager may be able to give a good enough estimation .,if ( ancestor . is preferred size set ( ) ) { layout manager ancestor layout = ancestor . get layout ( ) ; if ( ancestor layout != null ) { dimension preferred layout size = ancestor layout . preferred layout size ( ancestor ) ; if ( preferred layout size != null ) { component = ancestor ; width = preferred layout size . width ; height = preferred layout size . height ; } } } else { dimension pref size = ancestor . get preferred size ( ) ; if ( pref size != null ) { component = ancestor ; width = pref size . width ; height = pref size . height ; } },ancestor has size
\u 00 a <PLACE_HOLDER> and \uffe <PLACE_HOLDER> are actually the same symbol @$ just different code points . but the ri returns the \uffe <PLACE_HOLDER> and android returns those with \u 00 a <PLACE_HOLDER>,string [ ] yen = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; string [ ] dollar = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ;,ri returns word
add all udtf columns following sel will do <PLACE_HOLDER> for columns from udtf @$ not adding sel in here,new cols . add all ( from column names ( output cols . sub list ( num sel columns @$ output cols . size ( ) ) ) ) ; op . get conf ( ) . set output internal col names ( to column names ( new cols ) ) ; prune operator ( ctx @$ op @$ new cols ) ; cpp ctx . get pruned col lists ( ) . put ( op @$ cols after replacement ) ; return null ;,sel do projection
for the best integration possible @$ we will ask ognl which property accessor it would use for this target object @$ and then depending on the result apply our equivalent or just <PLACE_HOLDER> to ognl evaluation if it is a custom property accessor we do not implement .,final class < ? > target class = ognl runtime . get target class ( target ) ; final property accessor ognl property accessor = ognl runtime . get property accessor ( target class ) ;,accessor apply fallback
this can only happen if one of our entries has an unsupported <PLACE_HOLDER> .,throw wrapper . invalid ctb converter name ( ucne @$ codeset . get name ( ) ) ;,one has name
walk the args : the various drivers expect unpacked <PLACE_HOLDER> of the elements,object [ ] used args = new object [ args . length ] ; for ( int i = __num__ ; i < args . length ; i ++ ) { used args [ i ] = unpack wrapped element ( args [ i ] ) ; } return used args ;,drivers expect all
view table can not have ttl <PLACE_HOLDER>,if ( m_in strict mat view diff mode ) { return __str__ ; },table have columns
we no longer know what bin has the smallest <PLACE_HOLDER>,_ssx = - __num__ ;,bin has value
proxy servlet will receive an absolute uri from the client @$ and convert it to a relative uri . the connect handler wo n't modify <PLACE_HOLDER> the client sent @$ which must be a relative uri .,assert that ( request . length ( ) @$ matchers . greater than ( __num__ ) ) ; if ( server ssl context factory == null ) assert false ( request . contains ( __str__ ) ) ; else assert false ( request . contains ( __str__ ) ) ; string response = __str__ + __str__ + __str__ ; get end point ( ) . write ( callback . noop @$ byte buffer . wrap ( response . get bytes ( standard charsets . utf_8 ) ) ) ;,client sent something
create dummy get all <PLACE_HOLDER> operation as some of the methods in accumulo store test if the operation is a get all <PLACE_HOLDER> operation and if so set some options . we need those options if operation is returning all the <PLACE_HOLDER> .,if ( operation instanceof getrdd of all elements ) { derived operation = get get all elements ( operation ) ; } else { derived operation = operation ; },operation returning elements
client 1 did not register <PLACE_HOLDER>,await ( ) . until asserted ( ( ) -> { assert that ( client1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ ) ; assert that ( client2 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ ) ; assert that ( server1 . invoke ( ( ) -> get cache ( ) . get region ( region name ) . size ( ) ) ) . is equal to ( __num__ * __num__ ) ; assert that ( server2 . invoke ( (,client register interest
document can contain null <PLACE_HOLDER> by key,if ( doc . contains key ( field ) ) { modify field with dot notation ( doc @$ field ) ; return ; } list < field name and value > new fields = null ; iterator < map . entry < string @$ object > > it = doc . entry set ( ) . iterator ( ) ; while ( it . has next ( ) ) { map . entry < string @$ object > entry = it . next ( ) ; string [ ] origin key nodes = dot . split ( entry . get key ( ) ) ; string [ ] key nodes = exclude numeric items ( origin key nodes ) ; if ( key nodes . length >,document contain values
search for issues of project 1 having <PLACE_HOLDER> than 14 days and project 2 having <PLACE_HOLDER> then 25 days,assert that search returns only ( issue query . builder ( ) . created after by project uuids ( immutable map . of ( project1 . uuid ( ) @$ new issue query . period start ( add days ( now @$ - __num__ ) @$ true ) @$ project2 . uuid ( ) @$ new issue query . period start ( add days ( now @$ - __num__ ) @$ true ) ) ) @$ project1 issue1 . key ( ) @$ project2 issue1 . key ( ) ) ;,days having less
verify that an inv<PLACE_HOLDER>e method works <PLACE_HOLDER> after a pop frames in a thread suspended by an event .,main thread . pop frames ( frame for ( __str__ ) ) ; system . out . println ( __str__ ) ; system . out . println ( __str__ ) ; method invokeee method = ( method ) target class . methods by name ( __str__ ) . get ( __num__ ) ; try { target class . invoke method ( main thread @$ invokeee method @$ new array list ( ) @$ __num__ ) ; } catch ( exception ex ) { failure ( __str__ + ex ) ; ex . print stack trace ( ) ; } system . out . println ( __str__ ) ;,method works ok
entry does not have attributes <PLACE_HOLDER> : ca certs can have no attributes open ssl generates pkcs 12 with no attr for ca certs .,attr set = null ;,entry attributes note
relative paths have no <PLACE_HOLDER> .,return new gcs path ( fs @$ __str__ @$ component ) ;,paths have symbols
test using default value for the <PLACE_HOLDER> . individually specified env properties do not have defaults @$ so this should just get things from the default prop name <PLACE_HOLDER> .,string bogus prop = prop name + __str__ ; apps . set env from input property ( env @$ bogus prop @$ default prop value @$ conf @$ file . path separator ) ;,things prop root
this configuration does not accept <PLACE_HOLDER>,return null ;,configuration accept jwt
more complicated . vh 2 has a higher <PLACE_HOLDER> @$ but has some exceptions that vh 1 does not have .,region version holder vh1 = new region version holder ( member ) ; region version holder vh2 = new region version holder ( member ) ; bit set bs1 = new bit set ( ) ; bs1 . set ( __num__ @$ __num__ ) ; bs1 . set ( __num__ @$ __num__ ) ; record versions ( vh1 @$ bs1 ) ; bit set bs2 = new bit set ( ) ; bs2 . set ( __num__ @$ __num__ ) ; bs2 . set ( __num__ @$ __num__ ) ; record versions ( vh2 @$ bs2 ) ;,vh has version
confirm both iterables contain the same <PLACE_HOLDER> of elements,assert . assert equals ( all points list . size ( ) @$ iterable with size . size ( ) ) ;,iterables contain number
the caller must hold <PLACE_HOLDER> to synchronize the update .,final ref count += n ; if ( final ref count > peak final ref count ) { peak final ref count = final ref count ; },caller hold lock
this is to see if the server has any <PLACE_HOLDER>,try { orb port info [ ] serverorb and port list = entry . lookup ( iiop_clear_text . value ) ; } catch ( exception exc ) { return null ; },server has port
ok @$ so perhaps @$ we can use the <PLACE_HOLDER> from a previous execution ?,string [ ] saved = props . is initialized ( ) ? props . get instance ( ) . get last arguments ( ) : null ;,ok use restart
no rows match purge <PLACE_HOLDER>,verify proc fails ( client @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ @$ __num__ ) ; client response cr = client . call procedure ( __str__ @$ __str__ + __str__ + __str__ ) ; assert equals ( client response . success @$ cr . get status ( ) ) ;,rows match criteria
2 range conditions are used on different columns @$ but not all sql databases properly optimize it . some databases can only use an <PLACE_HOLDER> on one of the columns . an additional condition provides explicit knowledge that 'start ' can not be greater than 'end ' .,list < data segment > matching segments = connector . in read only transaction ( new transaction callback < list < data segment > > ( ) { @ override public list < data segment > in transaction ( final handle handle @$ final transaction status status ) { return handle . create query ( string utils . format ( __str__ + __str__ @$ db tables . get segments table ( ) @$ connector . get quote string ( ) ) ) . set fetch size ( connector . get streaming fetch size ( ) ) . bind ( __str__ @$ data source ) . bind ( __str__ @$ interval . get start ( ) . to string ( ) ) . bind ( __str__ @$ interval .,databases use integer
we treat calls from a profile as if made by its parent as profiles share the accessibility state of the parent . the call below performs the current profile parent <PLACE_HOLDER> .,synchronized ( m lock ) { final int resolved user id = m security policy . resolve calling user id enforcing permissions locked ( user id ) ; user state user state = get user state locked ( resolved user id ) ; client client = new client ( callback @$ binder . get calling uid ( ) @$ user state ) ; if ( m security policy . is caller interacting across users ( user id ) ) { m global clients . register ( callback @$ client ) ; if ( debug ) { slog . i ( log_tag @$ __str__ + binder . get calling pid ( ) ) ; } return int pair . of ( user state . get client state ( ),call performs resolution
announce what cloud we think we are in . publish our <PLACE_HOLDER> as well .,udp heartbeat . build_and_multicast ( cloud @$ hb ) ;,announce publish heartbeat
tests get maximum <PLACE_HOLDER>,stores to versions = get admin client ( ) . readonly ops . getro max version ( __num__ @$ lists . new array list ( __str__ @$ __str__ ) ) ; assert equals ( stores to versions . size ( ) @$ __num__ ) ; assert equals ( stores to versions . get ( __str__ ) . long value ( ) @$ __num__ ) ; assert equals ( stores to versions . get ( __str__ ) . long value ( ) @$ __num__ ) ;,tests get versions
make sure a status update does report <PLACE_HOLDER>,map task status mock status = new map task status ( attemptid @$ __num__ @$ __num__ @$ task status . state . running @$ __str__ @$ __str__ @$ __str__ @$ task status . phase . map @$ new counters ( ) ) ; feedback = listener . status update ( attemptid @$ mock status ) ; assert true ( feedback . get task found ( ) ) ; verify ( hb handler ) . progressing ( eq ( attempt id ) ) ;,update does progress
if obj extends unicast remote <PLACE_HOLDER> @$ set its ref .,if ( obj instanceof unicast remote object ) { ( ( unicast remote object ) obj ) . ref = sref ; } return sref . export object ( obj @$ null @$ false ) ;,obj extends object
this simulates the completion of txnid : id txn update <PLACE_HOLDER>,long write id = txn mgr2 . get table write id ( __str__ @$ __str__ ) ; add dynamic partitions adp = new add dynamic partitions ( txn mgr2 . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr2 . commit txn ( ) ;,txn update 1
if this component already has focus @$ then activate the input method by dispatching a synthesized focus gained <PLACE_HOLDER> .,if ( is focus owner ( ) ) { input context input context = get input context ( ) ; if ( input context != null ) { focus event focus gained event = new focus event ( this @$ focus event . focus_gained ) ; input context . dispatch event ( focus gained event ) ; } } event mask |= awt event . input_methods_enabled_mask ; if ( ( event mask & awt event . input_methods_enabled_mask ) != __num__ ) { input context input context = get input context ( ) ; if ( input context != null ) { input context . end composition ( ) ; input context . remove notify ( this ) ; } } event mask &= ~ awt event . input_methods_enabled_mask,focus gained event
skip notes that match csum but not key add <PLACE_HOLDER> of note to every position in duplicates array corresponding to the current key,if ( key to indexes map . contains key ( note . get key ( ) ) ) { list < integer > output pos = key to indexes map . get ( note . get key ( ) ) ; for ( int i = __num__ ; i < output pos . size ( ) ; i ++ ) { add note to duplicates array ( i > __num__ ? new note info ( note ) : note @$ duplicates @$ output pos . get ( i ) ) ; } },notes add record
if this project has correlated <PLACE_HOLDER> @$ create value generator and produce the correlated variables in the new output .,if ( cm . map ref rel to cor ref . contains key ( rel ) ) { frame = decorrelate input with value generator ( rel ) ; },project correlated reference
case . it consists of two arrays of lines . the first array of lines is the test input @$ and the second one is the expected output . if the second array has a single <PLACE_HOLDER> starting with ! ! then it is expected that import orderer will throw a formatter exception with that message . if a line ends with \ then,string [ ] [ ] [ ] inputs outputs = { { { } @$ { } } @$ { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__,array has element
check that class does not import the same parameterized <PLACE_HOLDER> with two different argument lists .,chk . check class bounds ( tree . pos ( ) @$ c . type ) ; tree . type = c . type ; for ( list < jc type parameter > l = tree . typarams ; l . non empty ( ) ; l = l . tail ) { assert . check non null ( env . info . scope . lookup ( l . head . name ) . scope ) ; },class import type
copy local variables to event scope execution by value . this way @$ the event scope execution references a <PLACE_HOLDER> ' of the local variables,new sub process variable snapshotter ( ) . set variables snapshots ( sub process execution @$ event scope execution ) ;,execution references '
normal template literals still throw <PLACE_HOLDER>,parse error ( __str__ @$ __str__ ) ;,literals throw errors
let 's build the response <PLACE_HOLDER>,datagram packet resp packet = make response packet ( packet ) ;,'s build packet
for local queries returning pdx <PLACE_HOLDER> wrap the resultset with results collection pdx deserializer wrapper which deserializes these pdx <PLACE_HOLDER> .,if ( needspdx deserialization wrapper ( true ) && result instanceof select results ) { result = new results collection pdx deserializer wrapper ( ( select results ) result @$ false ) ; },which deserializes records
figure out what type of glob path was given @$ will convert globbed <PLACE_HOLDER> to match the type to preserve relativity,path type glob type ; uri glob uri = glob path . to uri ( ) ; if ( glob uri . get scheme ( ) != null ) { glob type = path type . has_scheme ; } else if ( ! glob uri . get path ( ) . is empty ( ) && new path ( glob uri . get path ( ) ) . is absolute ( ) ) { glob type = path type . schemeless_absolute ; } else { glob type = path type . relative ; },figure convert uri
special handle <PLACE_HOLDER>,if ( key . equals ( __str__ ) ) { key = resource information . gpu_uri ; },special handle gpu
when the encoded graph has <PLACE_HOLDER> inlining @$ we can already have a proper caller state . if not @$ we set the caller state here .,if ( outer state . outer frame state ( ) == null && method scope . caller != null ) { ensure outer state decoded ( method scope . caller ) ; outer state . set outer frame state ( method scope . caller . outer state ) ; } method scope . outer state = outer state ;,graph has exports
assert the package tracker triggered an <PLACE_HOLDER> .,check update check triggered ( new package versions ) ; check token token2 = m fake intent helper . capture and reset last token ( ) ;,tracker triggered update
each application asks 10 5 gb <PLACE_HOLDER>,am1 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; am2 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; am3 . allocate ( __str__ @$ __num__ * gb @$ __num__ @$ null ) ; capacity scheduler cs = ( capacity scheduler ) rm1 . get resource scheduler ( ) ; rm node rm node1 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ; fi ca scheduler app scheduler app1 = cs . get application attempt ( am1 . get application attempt id ( ) ) ; fi ca scheduler app scheduler app2 = cs . get application attempt ( am2 . get,application asks containers
test reads that cross checksum <PLACE_HOLDER>,stm . seek ( __num__ ) ; assert equals ( stm . get pos ( ) @$ __num__ ) ; stm . read fully ( actual @$ __num__ @$ half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ half_chunk_size ) ; stm . read fully ( actual @$ half_chunk_size @$ block_size - half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ block_size ) ; stm . read fully ( actual @$ block_size @$ bytes_per_sum + half_chunk_size ) ; assert equals ( stm . get pos ( ) @$ block_size + bytes_per_sum + half_chunk_size ) ; stm . read fully ( actual @$ __num__ * block_size - half_chunk_size @$ file_size - ( __num__ * block_size - half_chunk_size ) ) ; assert,test reads checks
null value will always have null <PLACE_HOLDER>,if ( value == null ) { this . value quotes = null ; } else { if ( value quotes == null ) { this . value quotes = attribute value quotes . double ; } else if ( value quotes == attribute value quotes . none && value . length ( ) == __num__ ) { this . value quotes = attribute value quotes . double ; } else { this . value quotes = value quotes ; } },value have value
block until submissions create further <PLACE_HOLDER>,if ( m_clients pending notification . is empty ( ) ) { run submissions ( true ) ; m_limiter = rate limiter . create ( notification_rate @$ warmup_ms @$ time unit . milliseconds ) ; } else { run submissions ( false ) ; },submissions create notifications
if map has a null comparator @$ the keys should have a natural <PLACE_HOLDER> @$ even though k does n't explicitly implement comparable .,if ( comparator == null ) { comparator = ( comparator < ? super k > ) natural_order ; },keys have order
when a runtime exception gets thrown out @$ this provider will get called again if the object is asked for again . this will have the same failed result @$ 'cause when it 's called no parameters will have actually changed . guice will then report the same <PLACE_HOLDER> multiple times @$ which is pretty annoying . cache a null supplier and return that,ret val = suppliers . of instance ( null ) ; throw e ;,times report error
buffer wo n't contain the <PLACE_HOLDER> if the caller does n't have permission .,return - __num__ ;,buffer contain access
always a good idea to set up default <PLACE_HOLDER> ...,blur shader . use ( ) ; blur shader . set uniformf ( __str__ @$ __num__ @$ __num__ ) ;,idea set uniforms
failure during parameter serialization ; call may have reached <PLACE_HOLDER> @$ so call retry not possible .,throw e ;,call reached limit
now remove first region in table t 2 to see if catalogjanitor scan <PLACE_HOLDER> .,list < region info > t2 ris = meta table accessor . get table regions ( test_util . get connection ( ) @$ t2 ) ; meta table accessor . delete region info ( test_util . get connection ( ) @$ t2 ris . get ( __num__ ) ) ; gc = janitor . scan ( ) ; report = janitor . get last report ( ) ; assert false ( report . is empty ( ) ) ; assert equals ( __num__ @$ report . get holes ( ) . size ( ) ) ; assert true ( report . get holes ( ) . get ( __num__ ) . get first ( ) . get table ( ) . equals ( t1 ) ) ; assert,catalogjanitor scan ba
let 's get child <PLACE_HOLDER>,if ( cce . node count ( ) > __num__ ) { add loopx path ( cnode @$ monitor ) ; },'s get nodes
each test will populate the <PLACE_HOLDER> as needed,test visual graph g = new test visual graph ( ) ; return g ;,test populate graph
we call parse with 1 to get only one . this also means if we change the implementation to use 2 @$ it would use every other <PLACE_HOLDER> and so on . not that it is really useful @$ but a person could use it that way if they have a huge gigabyte log file and they only want to use a quarter of,int this count = parser . parse and configure ( __num__ @$ this ) ; if ( this count < __num__ ) { return error result ( new error ( __str__ ) @$ new http sample result ( ) ) ; },person use one
log if the installed pkg has a higher version <PLACE_HOLDER> .,if ( existing pkg info != null ) { if ( existing pkg info . get long version code ( ) == pkg . get long version code ( ) ) { if ( skip if same version ) { log . w ( tag @$ __str__ + pkg . get long version code ( ) + __str__ + package name + __str__ ) ; return ; } else { log . w ( tag @$ __str__ + pkg . get long version code ( ) + __str__ + package name ) ; } } else if ( existing pkg info . get long version code ( ) > pkg . get long version code ( ) ) { if ( skip if lower version ) { log,pkg has code
this map will extract pegasus options.generation <PLACE_HOLDER> to project property,project . get extensions ( ) . get extra properties ( ) . set ( __str__ @$ arrays . stream ( pegasus options . generation mode . values ( ) ) . collect ( collectors . to map ( pegasus options . generation mode :: name @$ function . identity ( ) ) ) ) ; synchronized ( static_project_evaluated_lock ) { if ( ! project . get root project ( ) . has property ( run_once ) || ! boolean . parse boolean ( string . value of ( project . get root project ( ) . property ( run_once ) ) ) ) { project . get gradle ( ) . projects evaluated ( gradle -> gradle . get root project ( ) . subprojects ( subproject,map extract mode
ok we have a real file get the local <PLACE_HOLDER>,this . gpgexe = kettlevfs . get filename ( file ) ; try { if ( file != null ) { file . close ( ) ; } } catch ( exception e ) { },file get name
user specified the <PLACE_HOLDER> for local mode hadoop run,console . print info ( __str__ + hadoop mem + __str__ ) ; variables . put ( hadoop_mem_key @$ string . value of ( hadoop mem ) ) ;,user specified memory
test that offering another element preserves the priority queue <PLACE_HOLDER> .,assert offer ( queue @$ e ) ; assert equals ( __num__ @$ queue . size ( ) ) ; assert same ( a @$ queue . peek ( ) ) ; assert same ( a @$ queue . poll ( ) ) ; assert equals ( __num__ @$ queue . size ( ) ) ;,priority queue size
this request has a bad <PLACE_HOLDER> @$ so we are dismissing it early .,if ( err != keeper exception . code . ok . int value ( ) ) { dec in process ( ) ; reply header rh = new reply header ( request . cxid @$ __num__ @$ err ) ; try { request . cnxn . send response ( rh @$ null @$ null ) ; } catch ( io exception e ) { log . error ( __str__ @$ e ) ; } },request has body
we must create a new <PLACE_HOLDER> @$ otherwise the subclass serializer cache can create concurrency problems,return new pojo serializer < > ( clazz @$ duplicate field serializers @$ fields @$ execution config ) ;,cache create class
user 2 joins the new <PLACE_HOLDER>,try { multi user chat muc2 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc2 . join ( __str__ ) ; multi user chat muc3 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc3 . join ( __str__ ) ; muc . grant ownership ( get barejid ( __num__ ) ) ; muc . grant moderator ( __str__ ) ; collection < affiliate > affiliates = muc . get owners ( ) ; assert equals ( __str__ @$ __num__ @$ affiliates . size ( ) ) ; for ( affiliate affiliate1 : affiliates ) { if ( get barejid ( __num__ ) . equals ( affiliate1 . get jid ( ) ) ) {,user joins room
the file only has one <PLACE_HOLDER>,located block lblock = locatedblocks . get ( __num__ ) ; datanode info [ ] datanodeinfos = lblock . get locations ( ) ; extended block block = lblock . get block ( ) ;,file has block
dismiss the grouped notification if a user dismisses all <PLACE_HOLDER> from a wear device,if ( ! mgcm message handler . has notifications ( ) ) { notification manager compat notification manager = notification manager compat . from ( m context ) ; notification manager . cancel ( group_notification_id ) ; },user dismisses notifications
verify owner has active tx <PLACE_HOLDER>,d lock batch [ ] batches = grantor . get lock batches ( owner ) ; if ( batches . length == __num__ ) { logger . debug ( __str__ @$ owner ) ; return ; } send recovery msgs ( dlock . get distribution manager ( ) @$ batches @$ owner @$ grantor ) ;,owner has locks
visibility @$ like all package groups @$ does n't have a <PLACE_HOLDER>,nested set < package group contents > visibility = convert visibility ( prerequisite map @$ analysis environment . get event handler ( ) @$ target @$ null ) ; if ( target instanceof output file ) { output file output file = ( output file ) target ; target context target context = new target context ( analysis environment @$ target @$ config @$ prerequisite map . get ( dependency resolver . output_file_rule_dependency ) @$ visibility ) ; if ( analysis environment . get skyframe env ( ) . values missing ( ) ) { return null ; } rule configured target rule = ( rule configured target ) target context . find direct prerequisite ( output file . get generating rule ( ) . get label (,visibility have parent
skip gc 'd weak <PLACE_HOLDER>,for ( hash entry < k @$ v > p = e ; p != last run ; p = p . next ) { k key = p . key ref . get ( ) ; if ( key == null ) { reduce ++ ; continue ; } int k = p . hash & size mask ; hash entry < k @$ v > n = new table [ k ] ; new table [ k ] = new hash entry < k @$ v > ( key @$ p . hash @$ n @$ p . value @$ ref queue ) ; },gc 'd refs
puts @$ gets @$ hits @$ <PLACE_HOLDER> @$ <PLACE_HOLDER> .,@ suppress warnings ( __str__ ) atomic integer [ ] counts = { in . caches . put count @$ is ppd ? in . caches . get by expr count : in . caches . get count @$ is ppd ? in . caches . get hit by expr count : in . caches . get hit count @$ is ppd ? in . caches . get count : in . caches . get by expr count @$ is ppd ? in . caches . get hit count : in . caches . get hit by expr count } ; verify splits ( original hs @$ splits ) ; verify call counts ( counts @$ __num__ @$ __num__ @$ __num__ ) ; assert equals ( __num__ @$,puts hits counts
we need to overwrite the default scan <PLACE_HOLDER> @$ which does not expand views . the expanding scan <PLACE_HOLDER> uses the flink planner to translate a view into a rel tree @$ before applying any subsequent rules .,context chain = contexts . chain ( context @$ contexts . of ( rel factories . expanding scan factory ( create flink planner ( current catalog @$ current database ) @$ rel factories . default_table_scan_factory ) ) ) ;,default scan factory
the url has been seen before . notify <PLACE_HOLDER> with the new distance estimate .,if ( m nearby urls . contains ( url info . get url ( ) ) ) { if ( url info . get distance ( ) >= __num__ && m pws result map . contains key ( url info . get url ( ) ) ) { safe notify native listeners on distance changed ( url info . get url ( ) @$ url info . get distance ( ) ) ; } return ; },url notify listeners
it 's fine to just check departure time @$ as the above pass ensures that all stop times have either both <PLACE_HOLDER> and departure times @$ or neither,if ( stop times [ __num__ ] . departure_time == entity . int_missing || stop times [ stop times . length - __num__ ] . departure_time == entity . int_missing ) { throw new first and last stops do not have times ( ) ; },times have stop
the linker matches <PLACE_HOLDER> to bindings by their type .,@ suppress warnings ( __str__ ) binding < t > binding = ( binding < t > ) get injectable type binding ( class loader @$ injectable type key @$ key ) ;,linker matches keys
now let 's allow <PLACE_HOLDER> for foot,foot encoder . set block fords ( boolean . false ) ; node = new reader node ( __num__ @$ - __num__ @$ - __num__ ) ; node . set tag ( __str__ @$ __str__ ) ; assert true ( foot encoder . handle node tags ( node ) == __num__ ) ; node = new reader node ( __num__ @$ - __num__ @$ - __num__ ) ; node . set tag ( __str__ @$ __str__ ) ; assert true ( foot encoder . handle node tags ( node ) == __num__ ) ;,'s allow node
the semantic graph does n't explicitly encode the root <PLACE_HOLDER> @$ so we print that out ourselves,for ( indexed word root : graph . get roots ( ) ) { string rel = grammatical relation . root . get long name ( ) ; rel = rel . replace all ( __str__ @$ __str__ ) ; int source = __num__ ; int target = root . index ( ) ; string source word = __str__ ; string target word = tokens . get ( target - __num__ ) . word ( ) ; final boolean is extra = false ; add dependency info ( dep info @$ rel @$ is extra @$ source @$ source word @$ null @$ target @$ target word @$ null @$ curns ) ; },graph encode collection
the following code uses an aes <PLACE_HOLDER> to encrypt the message .,final buffered block cipher cipher = new padded buffered block cipher ( new cbc block cipher ( new aes fast engine ( ) ) ) ; cipher . init ( true @$ key ) ; final byte [ ] encrypted bytes = new byte [ cipher . get output size ( plain text as bytes . length ) ] ; final int process len = cipher . process bytes ( plain text as bytes @$ __num__ @$ plain text as bytes . length @$ encrypted bytes @$ __num__ ) ; final int do final len = cipher . do final ( encrypted bytes @$ process len ) ;,code uses encoder
this creates the ephemeral sequential node with host id 0 <PLACE_HOLDER> this node already used for itself . just recording that fact .,final int selected host id = select new host id ( m_config . coordinator ip . to string ( ) ) ; if ( selected host id != __num__ ) { org . voltdb . voltdb . crash local voltdb ( __str__ + selected host id @$ false @$ null ) ; },node used which
if baseline provider returns both current <PLACE_HOLDER> and baseline <PLACE_HOLDER> @$ using them as the result,if ( df baseline . contains ( col_current ) ) { df aligned = df baseline ; df aligned . rename series ( col_value @$ col_baseline ) ; } else { data frame df current = this . time series loader . load ( slice view current ) ; df aligned = df current . rename series ( col_value @$ col_current ) . join outer ( df baseline . rename series ( col_value @$ col_baseline ) ) ; },provider returns column
sometimes the network connection notifier lags the actual connection <PLACE_HOLDER> @$ especially when the gcm nm wakes us from doze state . if we are really connected @$ report the connection <PLACE_HOLDER> from android .,if ( connection type == connection type . connection_none ) { connectivity manager cm = ( connectivity manager ) context . get system service ( context . connectivity_service ) ; network info active network = cm . get active network info ( ) ; boolean is connected = active network != null && active network . is connected or connecting ( ) ; if ( is connected ) { connection type = convert android network type to connection type ( active network . get type ( ) ) ; } },type report type
this test requires two task <PLACE_HOLDER> @$ but uberization overrides max to 1,conf . set boolean ( mr job config . job_ubertask_enable @$ false ) ; job job = app . submit ( conf ) ; app . wait for state ( job @$ job state . succeeded ) ; map < task id @$ task > tasks = job . get tasks ( ) ; assert . assert equals ( __str__ @$ __num__ @$ tasks . size ( ) ) ; task task = tasks . values ( ) . iterator ( ) . next ( ) ; assert . assert equals ( __str__ @$ task state . succeeded @$ task . get report ( ) . get task state ( ) ) ; map < task attempt id @$ task attempt > attempts = tasks . values (,test requires progress
complete the task on the same level should not trigger the <PLACE_HOLDER> of the user event listener,cmmn task service . complete ( tasks . get ( __num__ ) . get id ( ) ) ; assert that ( cmmn runtime service . create user event listener instance query ( ) . case instance id ( case instance . get id ( ) ) . count ( ) ) . is equal to ( __num__ ) ;,task trigger creation
split does not affect consumed <PLACE_HOLDER> ; consumed <PLACE_HOLDER> is still 0 .,assert equals ( __num__ @$ consumed parallelism from progress ( iter . get progress ( ) ) @$ __num__ ) ;,split affect parallelism
delegate already handles pk <PLACE_HOLDER>,if ( get factory ( ) . get session factory options ( ) . is comments enabled ( ) ) { insert . set comment ( __str__ + get entity name ( ) ) ; } return insert . to statement string ( ) ;,delegate handles comments
when instance registers second <PLACE_HOLDER>,instance id refresh id = registry . register ( registration . create ( __str__ @$ __str__ ) . build ( ) ) . block ( ) ; assert that ( refresh id ) . is equal to ( id ) ; step verifier . create ( registry . get instance ( id ) ) . assert next ( ( registered ) -> { assert that ( registered . get info ( ) ) . is equal to ( info ) ; assert that ( registered . get status info ( ) ) . is equal to ( status ) ; } ) . verify complete ( ) ;,instance registers request
start at 1 @$ since a miss in the open object int hash map returns a <PLACE_HOLDER>,int i = __num__ ;,map returns number
class file to write @$ if directory then use the <PLACE_HOLDER> of the input,path output = paths . get ( args [ __num__ ] ) ; if ( files . is directory ( output ) ) output = output . resolve ( input . get file name ( ) ) ;,directory use name
if the requestable capabilities have n't changed @$ and the score has n't changed @$ then the change we 're processing ca n't affect any requests @$ it can only affect the listens on this network . we might have been called by rematch network and requests when a network changed foreground <PLACE_HOLDER> .,if ( nai . get current score ( ) == old score && new nc . equal requestable capabilities ( prev nc ) ) { process listen requests ( nai ) ; } else { rematch all networks and requests ( ) ; notify network callbacks ( nai @$ connectivity manager . callback_cap_changed ) ; },network changed state
default lists namenode <PLACE_HOLDER> only,jmx . init ( ) ;,default lists id
client reads <PLACE_HOLDER>,assert equals ( __num__ @$ client . get input stream ( ) . read ( ) ) ;,client reads response
if the request indicates a <PLACE_HOLDER> that is greater than the current <PLACE_HOLDER> then assign that <PLACE_HOLDER> and leader to the current context .,boolean transition = update term and leader ( request . term ( ) @$ null ) ; completable future < vote response > future = completable future . completed future ( log response ( handle vote ( request ) ) ) ; if ( transition ) { raft . transition ( raft server . role . follower ) ; } return future ;,request indicates term
do not invoke class descriptor write <PLACE_HOLDER> with old protocol,if ( protocol == protocol_version_1 ) { desc . write non proxy ( this ) ; } else { write class descriptor ( desc ) ; },descriptor write proxy
process persist <PLACE_HOLDER> .,for ( long file id : m persist requests . key set ( ) ) { if ( thread . interrupted ( ) ) { throw new interrupted exception ( __str__ ) ; } boolean remove = true ; alluxio . time . exponential timer timer = m persist requests . get ( file id ) ; if ( timer == null ) { continue ; } alluxio . time . exponential timer . result timer result = timer . tick ( ) ; if ( timer result == alluxio . time . exponential timer . result . not_ready ) { continue ; } alluxiouri uri = null ; try { try ( locked inode path inode path = m inode tree . lock full inode path (,process persist requests
worker is still running this <PLACE_HOLDER>,task location task location = task runner work item . get location ( ) ; final url url = task runner utils . make task locationurl ( task location @$ __str__ @$ task id ) ; return optional . of ( new byte source ( ) { @ override public input stream open stream ( ) throws io exception { try { return http client . go ( new request ( http method . get @$ url ) @$ new input stream response handler ( ) ) . get ( ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; } catch ( execution exception e ) { throwables . propagate if possible ( e . get cause ( ),worker running task
check that both animators ' listeners have received the animation <PLACE_HOLDER> .,assert true ( l1 . start called ) ; assert true ( l1 . end called ) ; assert false ( a1 . is started ( ) ) ; assert true ( l1 . end time >= l1 . start time ) ; assert true ( l2 . start called ) ; assert true ( l2 . end called ) ; assert false ( a2 . is started ( ) ) ; assert true ( l2 . end time >= l1 . start time ) ;,listeners received event
invalid port number yields an <PLACE_HOLDER> .,check error state ( new ignite callable < void > ( ) { @ override public void call ( ) throws exception { driver manager . get connection ( __str__ ) ; return null ; } } @$ __str__ @$ __str__ ) ;,number yields error
name of the attribute containing the <PLACE_HOLDER> roles . if not specified @$ this defaults to roles .,module options . put ( __str__ @$ __str__ ) ;,name containing module
at this point the clone is complete . next step is enabling the <PLACE_HOLDER> .,string msg = __str__ + snapshot . get name ( ) + __str__ + table name + __str__ ; log . info ( msg ) ; monitor status . set status ( msg + __str__ ) ;,step enabling snapshot
historic rules may contain null <PLACE_HOLDER> when original zoneinfo data includes non transition data .,if ( historic rules != null ) { for ( int i = __num__ ; i < historic rules . length ; i ++ ) { if ( historic rules [ i ] != null ) { size ++ ; } } },rules contain values
xshift and yshift represent the <PLACE_HOLDER> & direction to shift the tab in their respective axis .,for ( int j = start + __num__ ; j <= end ; j ++ ) { int xshift = __num__ ; int yshift = __num__ ; switch ( tab pane . get tab placement ( ) ) { case j tabbed pane . top : case j tabbed pane . bottom : xshift = ltr ? tab overlap : - tab overlap ; break ; case j tabbed pane . left : case j tabbed pane . right : yshift = tab overlap ; break ; default : } rects [ j ] . x += xshift ; rects [ j ] . y += yshift ; rects [ j ] . width += math . abs ( xshift ) ; rects [ j ] . height,xshift represent direction
if we 're becoming visible @$ immediately change client visibility as well . there seem to be some edge cases where we change our visibility but client visibility never gets <PLACE_HOLDER> . if we 're becoming invisible @$ update the client visibility if we are not running an animation . otherwise @$ we 'll update client visibility in on animation finished .,if ( visible || ! is really animating ( ) ) { set client hidden ( ! visible ) ; } if ( ! get display content ( ) . m closing apps . contains ( this ) && ! get display content ( ) . m opening apps . contains ( this ) ) { get display content ( ) . get docked divider controller ( ) . notify app visibility changed ( ) ; m wm service . m task snapshot controller . notify app visibility changed ( this @$ visible ) ; },visibility gets updated
inline editor is n't supported but <PLACE_HOLDER> viewer is enable <PLACE_HOLDER>,if ( array utils . contains ( supported edit types @$ i value controller . edit type . panel ) ) { controller . activate panel ( value viewer panel . panel_id @$ true @$ true ) ; return null ; },viewer enable panel
the number of got <PLACE_HOLDER>,int got = skipped + cis . read ( result @$ __num__ @$ __num__ ) ;,number got bytes
can this left constituent leave <PLACE_HOLDER> for a right constituent ?,boolean i possiblel = ( narrowr < end ) ;,constituent leave space
check if any of the references cross the module <PLACE_HOLDER> .,if ( check modules && ref . module != null ) { if ( ref . module != fn module && ! module graph . depends on ( ref . module @$ fn module ) ) { is removable = false ; check modules = false ; } },any cross boundary
cleanup session log <PLACE_HOLDER> .,cleanup session log dir ( ) ; hive history hive hist = session state . get hive history ( ) ; if ( null != hive hist ) { hive hist . close stream ( ) ; } try { session state . reset thread name ( ) ; session state . close ( ) ; } finally { session state = null ; } if ( session state != null ) { try { session state . reset thread name ( ) ; session state . close ( ) ; } catch ( throwable t ) { log . warn ( __str__ @$ t ) ; } session state = null ; } if ( session hive != null ) { try { session hive . close,session log dir
let evaluation prints stats how often the right output had the highest <PLACE_HOLDER>,evaluation eval = new evaluation ( ) ; eval . eval ( ds . get labels ( ) @$ output ) ; system . out . println ( eval . stats ( ) ) ;,output had priority
if the scan matches all <PLACE_HOLDER> @$ we can throw away the scan nodes and use a truncate delete node .,if ( delete is truncate ( m_parsed delete @$ sub select root ) ) { delete node . set truncate ( true ) ; } else { if ( m_parsed delete . order by columns ( ) . size ( ) > __num__ && ! is single partition plan && ! target table . get isreplicated ( ) ) { throw new planning error exception ( __str__ + __str__ + __str__ + __str__ ) ; } boolean needs order by node = is order by node required ( m_parsed delete @$ sub select root ) ; abstract expression address expr = new tuple address expression ( ) ; node schema proj_schema = new node schema ( ) ; proj_schema . add column ( abstract parsed stmt . temp_table_name,scan matches views
note : each event loop has its own connection <PLACE_HOLDER> .,factory with pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( true ) . build ( ) ; factory without pipelining = client factory . builder ( ) . worker group ( event loop group . get ( ) @$ false ) . use http1 pipelining ( false ) . build ( ) ;,loop has window
this is technically a recursive constraint for cast @$ but type registry.can cast has explicit <PLACE_HOLDER> for row to row cast,super ( cast @$ immutable list . of ( new type variable constraint ( __str__ @$ false @$ false @$ __str__ @$ immutable set . of ( new type signature ( __str__ ) ) @$ immutable set . of ( ) ) @$ with variadic bound ( __str__ @$ __str__ ) ) @$ immutable list . of ( ) @$ new type signature ( __str__ ) @$ immutable list . of ( new type signature ( __str__ ) ) @$ false ) ;,cast has handling
if this instruction has a delay <PLACE_HOLDER> @$ adjust max addr accordingly,if ( instr . get prototype ( ) . has delay slots ( ) ) { max addr = instr . get min address ( ) . add ( instr . get default fall through offset ( ) - __num__ ) ; } v context . set current instruction ( instr ) ; v context . flow to address ( flow from addr @$ max addr ) ; if ( evaluator != null ) { if ( evaluator . evaluate context before ( v context @$ instr ) ) { body . add ( conflicts ) ; return body ; } },instruction has slot
makes it more easy to see which release <PLACE_HOLDER> process in thread dump,thread . current thread ( ) . set name ( __str__ + uri . get host ( ) ) ;,which release thread
we must sort i pv 4 and i pv <PLACE_HOLDER> here,set < string > ipv6 = new hash set < > ( ) ; set < string > ipv4 = new hash set < > ( ) ; for ( string ip : ips ) if ( is properip ( ip ) ) { if ( ip . index of ( __str__ ) >= __num__ ) ipv6 . add ( ip ) ; else ipv4 . add ( ip ) ; } if ( ipv4 . size ( ) == __num__ ) { if ( ipv6 . size ( ) == __num__ ) { this . dna . put ( seed . ip @$ ipv6 . iterator ( ) . next ( ) ) ; this . dna . put ( seed . ip6 @$ __str__ ) ;,pv pv 6
for a normal <PLACE_HOLDER> @$ we can not ensure that the underlying compression stream is closed @$ so we decompress the full record set here . use cases which call for a lower memory footprint can use ` streaming <PLACE_HOLDER> ` at the cost of additional complexity,try ( closeable iterator < record > iterator = compressed iterator ( buffer supplier . no_caching @$ false ) ) { list < record > records = new array list < > ( count ( ) ) ; while ( iterator . has next ( ) ) records . add ( iterator . next ( ) ) ; return records . iterator ( ) ; },footprint use iterator
if an entry already exists for a dataset @$ add it to the current <PLACE_HOLDER> @$ else create a new <PLACE_HOLDER>,for ( string dataset : iterables . filter ( datasets @$ new dataset predicate ( dataset name element . get as string ( ) ) ) ) { if ( dataset specific config map . contains key ( dataset ) ) { dataset specific config map . get ( dataset ) . add all ( state utils . json object to state ( object @$ dataset ) ) ; } else { dataset specific config map . put ( dataset @$ state utils . json object to state ( object @$ dataset ) ) ; } },else create one
make sure processor has <PLACE_HOLDER> to do .,if ( ! is work to do ( ) ) { logger . debug ( __str__ @$ connectable ) ; return invocation result . yield ( __str__ ) ; } if ( num relationships > __num__ ) { final int required number of available relationships = connectable . is trigger when any destination available ( ) ? __num__ : num relationships ; if ( ! repository context . is relationship availability satisfied ( required number of available relationships ) ) { logger . debug ( __str__ @$ connectable ) ; return invocation result . yield ( __str__ ) ; } } logger . debug ( __str__ @$ connectable ) ; final long batch nanos = connectable . get run duration ( time unit . nanoseconds ) ; final,processor has work
use a small long here which will only occupy one property <PLACE_HOLDER>,my node . set property ( __str__ @$ small value ) ; tx . commit ( ) ;,which occupy block
there are two special cases below @$ because 2 cliques have 2 <PLACE_HOLDER>,c . set domain ( domain ) ; if ( clique == cliquec ) { featuresc ( c info @$ loc @$ c ) ; } else if ( clique == clique cpc ) { features cpc ( c info @$ loc @$ c ) ; features cnc ( c info @$ loc - __num__ @$ c ) ; } else if ( clique == clique cp2c ) { features cp2c ( c info @$ loc @$ c ) ; } else if ( clique == clique cp3c ) { features cp3c ( c info @$ loc @$ c ) ; } else if ( clique == clique cp4c ) { features cp4c ( c info @$ loc @$ c ) ; } else if ( clique ==,cliques have domains
if the current <PLACE_HOLDER> of the window do n't match the desired <PLACE_HOLDER> @$ then adjust the min width and min height arrays according to the weights .,diffw = parent . width - r . width ; if ( diffw != __num__ ) { weight = __num__ ; for ( i = __num__ ; i < info . width ; i ++ ) weight += info . weightx [ i ] ; if ( weight > __num__ ) { for ( i = __num__ ; i < info . width ; i ++ ) { int dx = ( int ) ( ( ( ( double ) diffw ) * info . weightx [ i ] ) / weight ) ; info . min width [ i ] += dx ; r . width += dx ; if ( info . min width [ i ] < __num__ ) { r . width -= info,dimensions match width
key value should allow negative <PLACE_HOLDER> for backwards compat . otherwise @$ if the user already has negative <PLACE_HOLDER> in cluster data @$ h base wo n't be able to handle that,try { new key value ( bytes . to bytes ( __num__ ) @$ bytes . to bytes ( __num__ ) @$ bytes . to bytes ( __num__ ) @$ - __num__ @$ bytes . to bytes ( __num__ ) ) ; } catch ( illegal argument exception ex ) { fail ( __str__ ) ; },user has timestamps
check non singleton <PLACE_HOLDER> for types do not eagerly initialize factor <PLACE_HOLDER> when getting bean factory post processor <PLACE_HOLDER>,collection < bean factory post processor > factory post processors = bean factory . get beans of type ( bean factory post processor . class @$ true @$ false ) . values ( ) ; if ( factory post processors . is empty ( ) ) { factory post processors = collections . singleton ( new property placeholder configurer ( ) ) ; } for ( bean factory post processor factory post processor : factory post processors ) { factory post processor . post process bean factory ( bean factory ) ; } abstract engine configuration engine configuration = ( abstract engine configuration ) bean factory . get bean ( bean name ) ; engine configuration . set beans ( new spring bean factory proxy map ( bean,types initialize beans
the ri starts datagram <PLACE_HOLDER> in broadcast mode .,assert true ( ds . get broadcast ( ) ) ;,ri starts stream
only log the message if it 's from a recipient we 're waiting for . it could have been multicast to all members @$ which would give us one <PLACE_HOLDER> per member,if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ this . processor id @$ is present @$ sender @$ this . key @$ serialized @$ version tag ) ; },which give message
if we are unable to write the current buffer to socket channel we should break @$ as we might have reached max socket send buffer <PLACE_HOLDER> .,break ;,socket send size
clear defined <PLACE_HOLDER> command @$ create defined <PLACE_HOLDER> command @$ create index command @$ define index command @$ destroy index command @$ list index command,create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster manage query ) ; create test command ( __str__ @$ cluster read query ) ;,indexes command indexes
we check that cookie manager returns the <PLACE_HOLDER> for the main domain,url url main domain = new url ( __str__ ) ; cookies = cookie handler . get cookies for url ( man . get cookies ( ) @$ url main domain @$ cookie manager . allow_variable_cookies ) ; assert . assert equals ( __num__ @$ cookies . size ( ) ) ;,manager returns cookies
resort file <PLACE_HOLDER> @$ make i node id not sorted .,for ( int j = __num__ ; j < files . length / __num__ ; j ++ ) { path temp path = files [ j ] ; files [ j ] = files [ files . length - __num__ - j ] ; files [ files . length - __num__ - j ] = temp path ; byte [ ] temp bytes = bytes [ __num__ + j ] ; bytes [ __num__ + j ] = bytes [ files . length - __num__ - j + __num__ ] ; bytes [ files . length - __num__ - j + __num__ ] = temp bytes ; },resort file list
resetting max length should not remove maxlength <PLACE_HOLDER>,tf . add focus listener ( event -> { tf . set max length ( __num__ ) ; } ) ;,length remove focus
rather than create a validated network which complicates <PLACE_HOLDER> by registering it 's own network request during startup @$ just bump up the score to cancel out the unvalidated penalty .,test agent . adjust score ( __num__ ) ; cv = test factory . get network stoppedcv ( ) ;,which complicates tests
for an incoming message : shall remove and store the <PLACE_HOLDER> envelope @$ including the delimiter . shall pass the remaining data frames to its calling application . shall wait for a single reply message from its calling application . shall prepend the <PLACE_HOLDER> envelope and delimiter . shall deliver this message back to the originating peer .,for ( string bind address : binds ) { envelope ( ctx @$ bind address @$ zmq . zmq_rep @$ zmq . zmq_dealer ) ; },envelope prepend context
take in two stringified lists @$ and return true if the first list contains <PLACE_HOLDER> that are not in the second list,bi function < string @$ string @$ boolean > has exclusive elements = ( string a @$ string b ) -> { if ( a == null || a . is empty ( ) ) { return false ; } else if ( b == null || b . is empty ( ) ) { return true ; } set < string > b set = stream . of ( b . split ( element sep ) ) . collect ( collectors . to set ( ) ) ; return ! stream . of ( a . split ( element sep ) ) . filter ( ( x ) -> ! b set . contains ( x ) ) . collect ( collectors . to set ( ),list contains elements
also check what happens if a value xml file also contains the same <PLACE_HOLDER> .,merged android data actual = unwritten merged android data . of ( source . resolve ( __str__ ) @$ direct @$ parsed android data builder . empty ( ) ) . write ( merged data writer ) ; assert about ( paths ) . that ( actual . get manifest ( ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve ( __str__ ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve ( __str__ ) ) . exists ( ) ; assert about ( paths ) . that ( actual . get resource dir ( ) . resolve (,check contains id
do n't add trust <PLACE_HOLDER> if not already present @$ the builder will inherit the <PLACE_HOLDER> from its parent @$ and that 's where the trust <PLACE_HOLDER> should be added .,if ( ! builder . has certificates entry refs ( ) ) { return ; } builder . add certificates entry refs ( debug config builder . get certificates entry refs ( ) ) ;,builder inherit chain
case 6 : true if an attribute named primitive float att name of type float has the <PLACE_HOLDER> float <PLACE_HOLDER> we cover javax.management.binary rel query exp with a rel op equal to eq and javax.management.numeric <PLACE_HOLDER> exp,queries . add ( query . eq ( query . attr ( primitive float att name ) @$ query . value ( float value ) ) ) ;,name has value
use modified portions of do fast scale down <PLACE_HOLDER> here since we do not want to allocate a temporary fast hive decimal object .,long compare0 ; long compare1 ; long compare2 ; int scale down ; if ( left scale < right scale ) { scale down = right scale - left scale ; if ( scale down < longword_decimal_digits ) { final long divide factor = power of ten table [ scale down ] ; final long multiply factor = power of ten table [ longword_decimal_digits - scale down ] ; compare0 = right fast0 / divide factor + ( ( right fast1 % divide factor ) * multiply factor ) ; compare1 = right fast1 / divide factor + ( ( right fast2 % divide factor ) * multiply factor ) ; compare2 = right fast2 / divide factor ; } else if ( scale down < two_x_longword_decimal_digits ),portions scale checks
the ws loop consumes <PLACE_HOLDER> from the beginning of each line .,while ( true ) { ws loop : while ( true ) { switch ( c ) { case __str__ : case __str__ : c = in . read ( ) ; break ; case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : case __num__ : default : break ws loop ; } } if ( c == __str__ ) { do { c = in . read ( ) ; } while (,loop consumes 0
apply the offset to the begin <PLACE_HOLDER> to find the absolute <PLACE_HOLDER> .,stats . m last time used = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_active_attr ) ; try { stats . m last time visible = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_visible_attr ) ; } catch ( io exception e ) { log . i ( tag @$ __str__ ) ; } try { stats . m last time foreground service used = stats out . begin time + xml utils . read long attribute ( parser @$ last_time_service_used_attr ) ; } catch ( io exception e ) { log . i ( tag @$ __str__ ) ; } stats . m total time in foreground = xml utils .,offset begin time
handle provisioning internally ; it 'll reset m prepare <PLACE_HOLDER> in progress,int result = handle provisioninig ( uuid ) ;,m prepare operation
client reads <PLACE_HOLDER>,buffer util . clear ( buffer ) ; len = c . client . fill ( buffer ) ; assert equals ( __num__ @$ len ) ; assert equals ( __str__ @$ buffer util . to string ( buffer ) ) ;,client reads data
as subsequent compare and set <PLACE_HOLDER> will fail .,cluster id before deserialize . compare and set ( no cluster id @$ cluster meta . get ( ) ) ; return data ;,subsequent compare operations
fast path @$ if the stream can take a <PLACE_HOLDER> directly just write to it,if ( output stream instanceof buffer writable output stream ) { try { ( ( buffer writable output stream ) output stream ) . write ( buffers ) ; return true ; } catch ( io exception e ) { callback . on exception ( exchange @$ this @$ e ) ; return false ; } },stream take buffer
issue 668 : do n't inline singleton getter methods <PLACE_HOLDER> as this confused class removing logic .,if ( convention . get singleton getter class name ( call node ) != null ) { return false ; },issue inline call
clear non backed up <PLACE_HOLDER> from expected interval stats,m interval stats . active configuration = null ; m interval stats . configurations . clear ( ) ; m interval stats . events . clear ( ) ;,non backed configuration
user 1 sends the <PLACE_HOLDER> that contains the xhtml to user 2,try { chat1 . send message ( msg ) ; } catch ( exception e ) { fail ( __str__ ) ; } packet packet = chat2 . next result ( __num__ ) ; message message = ( message ) packet ; assert not null ( __str__ @$ message . get body ( ) ) ; try { xhtml extension = ( xhtml extension ) message . get extension ( __str__ @$ __str__ ) ; assert not null ( __str__ @$ xhtml extension ) ; assert true ( __str__ @$ xhtml extension . get bodies count ( ) > __num__ ) ; for ( iterator < string > it = xhtml extension . get bodies ( ) ; it . has next ( ) ; ) { string,user sends message
expected since only test role can call that <PLACE_HOLDER>,assert . fail ( __str__ ) ;,role call method
provoke an exception when the nl emits a <PLACE_HOLDER> @$ must bubble up and nl must stop,expected exception . expect message ( __str__ ) ; execute ( __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ) ;,nl emits message
check that the room 's owner can ban a simple <PLACE_HOLDER>,muc . ban user ( get barejid ( __num__ ) @$ __str__ ) ; thread . sleep ( __num__ ) ; assert null ( __str__ @$ muc . get occupant ( room + __str__ ) ) ; assert false ( __str__ @$ muc2 . is joined ( ) ) ;,owner ban user
launcher on a managed profile is referring ot <PLACE_HOLDER> 0 !,run with caller ( launcher_1 @$ user_p0 @$ ( ) -> { m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_2 @$ list ( __str__ @$ __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_3 @$ list ( __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ handle_user_0 ) ; m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_p0 ) ; } ) ; run with caller ( launcher_1 @$ user_10 @$ ( ) -> { m launcher apps . pin shortcuts ( calling_package_1 @$ list ( __str__ @$ __str__ ) @$ handle_user_10 ) ;,launcher referring user
the shard is relocating @$ the <PLACE_HOLDER> it is relocating to will be in initializing state @$ so we do n't count it,if ( shard routing . relocating ( ) ) { compute relocating shards ++ ; },shard relocating shards
test releasing mms network request does not disconnect main cellular network <PLACE_HOLDER>,m cm . unregister network callback ( network callback ) ; mms network agent . expect disconnected ( ) ; verify active network ( transport_cellular ) ;,request disconnect call
check put <PLACE_HOLDER> .,boolean failed = false ; try { dflt cache . put ( __num__ @$ __num__ ) ; } catch ( cache exception e ) { failed = true ; check and wait ( e ) ; } assert true ( failed ) ; dflt cache . put ( __num__ @$ __num__ ) ; return true ; assert true ( ( boolean ) o ) ; assert equals ( __num__ @$ dflt cache . get ( __num__ ) ) ; return true ;,check put operation
this point is only reached if the operation failed <PLACE_HOLDER> than the allowed retry count,throw new io exception ( __str__ + m upload id + __str__ + m key @$ last exception ) ;,operation failed more
note that the only current caller of this method checks first to see if there is a path before calling this method . it is not clear <PLACE_HOLDER> the return value should be here .,return null ;,value be what
last valid y coordinate which did n't have a missing <PLACE_HOLDER>,int last = - __num__ ;,which have component
let the visitor visit the <PLACE_HOLDER> referenced in the constant element value .,enum constant element value . referenced classes accept ( class visitor ) ;,visitor visit classes
this layout has red @$ blue @$ and green <PLACE_HOLDER> from the top .,enter scene ( r . layout . scene3 ) ;,layout has effect
when the animation completes @$ adjust the relative layout params of the recycler to make sure the bar does n't overlap the bottom <PLACE_HOLDER> when showing,long ms delay = ani utils . duration . short . to millis ( this ) ; m handler . post delayed ( new runnable ( ) { @ override public void run ( ) { if ( ! is finishing ( ) ) { relative layout . layout params params = ( relative layout . layout params ) m recycler . get layout params ( ) ; if ( show ) { params . add rule ( relative layout . above @$ r . id . container_selection_bar ) ; } else { params . add rule ( relative layout . above @$ __num__ ) ; } } } } @$ ms delay ) ;,bar overlap strips
retry with the old formatter which uses synchronization <PLACE_HOLDER>,try { parsed = generic formatter . short_day_formatter . parse ( date str @$ __num__ ) . get time ( ) ; } catch ( final parse exception pe ) { parsed = new date ( ) ; },which uses logic
stop automatically reconnecting to this <PLACE_HOLDER> in the future . automatically connecting to a <PLACE_HOLDER> that provides no or limited connectivity is not useful @$ because the user can not use that <PLACE_HOLDER> except through the notification shown by this method @$ and the notification is only shown if the <PLACE_HOLDER> is explicitly selected by the user .,nai . async channel . send message ( network agent . cmd_prevent_automatic_reconnect ) ;,user use port
no param constructors do not require js <PLACE_HOLDER>,test same ( __str__ ) ;,constructors require doc
additionally @$ this condition reduces computation <PLACE_HOLDER>,if ( ! m answer sounds added ) { string answer sound source = remove front side audio ( answer ) ; m sound player . add sounds ( m base url @$ answer sound source @$ sound . sounds_answer ) ; m answer sounds added = true ; },condition reduces time
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( test admin mode from command line . class ) ;,suite using tests
this is what always should happen . a sort sets the items <PLACE_HOLDER> by <PLACE_HOLDER> @$ temporarily breaking the uniqueness requirement .,if ( dupl == __num__ ) { duplicates . remove ( e ) ; } else { duplicates . put ( e @$ dupl - __num__ ) ; },sort sets one
this leads to missing the after command profiles of the other modules in the profile . since the bep currently shuts down at the build complete event @$ we can not just move posting the build tool <PLACE_HOLDER> to after command of this module .,if ( profile path != null ) { try { profiler . instance ( ) . stop ( ) ; event . get result ( ) . get build tool log collection ( ) . add local file ( profile path . get base name ( ) @$ profile path ) ; } catch ( io exception e ) { reporter . handle ( event . error ( __str__ + e . get message ( ) ) ) ; } },the build log
higher priority @$ so accept new work <PLACE_HOLDER>,assert . assert true ( add files ( list @$ __str__ @$ __num__ ) ) ; assert . assert equals ( list . get work units ( ) . size ( ) @$ __num__ ) ;,priority accept units
block any external content resolving actions since we do n't need them and a report says these actions may cause security <PLACE_HOLDER> .,document builder . set entity resolver ( new entity resolver ( ) { @ override public input source resolve entity ( string public id @$ string system id ) throws sax exception @$ io exception { return new input source ( ) ; } } ) ;,actions cause problems
if the query h<PLACE_HOLDER>s <PLACE_HOLDER> h<PLACE_HOLDER>ving cl<PLACE_HOLDER>use <PLACE_HOLDER>nd both h 2 <PLACE_HOLDER>nd pinot h<PLACE_HOLDER>ve no groups @$ th<PLACE_HOLDER>t is expected @$ so we do n't need to comp<PLACE_HOLDER>re the number of docs sc<PLACE_HOLDER>nned,if ( pinot query . contains ( __str__ ) ) { return ; } if ( pinot num records selected != __num__ ) { string failure message = __str__ + pinot num records selected + __str__ ; failure ( pinot query @$ sql queries @$ failure message ) ; },query has a
... if the cache did not contain a color state list @$ try and create <PLACE_HOLDER>,if ( tint == null ) { tint = ( m hooks == null ) ? null : m hooks . get tint list for drawable res ( context @$ res id ) ; if ( tint != null ) { add tint list to cache ( context @$ res id @$ tint ) ; } },and create one
empty control batch should not cause an <PLACE_HOLDER>,default record batch . write empty header ( buffer @$ record batch . magic_value_v2 @$ __num__ @$ ( short ) __num__ @$ - __num__ @$ __num__ @$ __num__ @$ record batch . no_partition_leader_epoch @$ timestamp type . create_time @$ time . milliseconds ( ) @$ true @$ true ) ; current offset += append transactional records ( buffer @$ __num__ @$ current offset @$ new simple record ( time . milliseconds ( ) @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) @$ new simple record ( time . milliseconds ( ) @$ __str__ . get bytes ( ) @$ __str__ . get bytes ( ) ) ) ; commit transaction ( buffer @$ __num__ @$ current offset ) ; buffer .,batch cause exception
when location represents one particular <PLACE_HOLDER>,for ( storage dir dir : tier . get storage dirs ( ) ) { location = dir . to block store location ( ) ; assert equals ( m meta manager . get available bytes ( location ) @$ m metadata view . get available bytes ( location ) ) ; },location represents tier
then : should get created interpreters which have different <PLACE_HOLDER>,md1 = setting ;,which have dependencies
t qk js n 2 q 2 s zv 9 h 2 d q 5 x 2 r sne k ny l qgeg <PLACE_HOLDER>,ec key ec key2 = new ec key ( utils . get random ( ) ) ; byte [ ] low bal address2 = ec key2 . get address ( ) ; ret1 = public methed . sendcoin2 ( low bal address2 @$ __num__ @$ from address @$ test key002 @$ blocking stub full ) ; assert . assert equals ( ret1 . get code ( ) @$ grpcapi . return . response_code . success ) ; witness list witnesslist = blocking stub full . list witnesses ( grpcapi . empty message . new builder ( ) . build ( ) ) ; optional < witness list > result = optional . of nullable ( witnesslist ) ; witness list witness list = result . get ( ),l qgeg a
pre grouped symbols reflect <PLACE_HOLDER> of the input . splitting the aggregation and pushing partial aggregation through the exchange may or may not preserve these <PLACE_HOLDER> . hence @$ it is safest to drop pre grouped symbols here .,plan node partial = new aggregation node ( context . get id allocator ( ) . get next id ( ) @$ node . get source ( ) @$ intermediate aggregation @$ node . get grouping sets ( ) @$ immutable list . of ( ) @$ partial @$ node . get hash symbol ( ) @$ node . get group id symbol ( ) ) ;,properties preserve properties
allow a connection from a new key with the 'always allow ' option <PLACE_HOLDER> to false,run adb test ( test_key_1 @$ true @$ false @$ false ) ;,connection allow set
do not set ms lab to null as scanners may still be reading the <PLACE_HOLDER> here and need to decrease the counter when they finish,this . mem storelab . close ( ) ;,scanners reading data
init logs read <PLACE_HOLDER>,lenient ( ) . do return ( test_log_infos ) . when ( test workspace logs reader ) . get log infos ( ) ; lenient ( ) . do return ( test_read_first_log_command ) . when ( test workspace logs reader ) . get read logs command ( test_workspace_id @$ paths . get ( format ( __str__ @$ path_to_store_logs @$ test_workspace_id @$ first_log_info . get name ( ) ) ) @$ first_log_info . get location inside workspace ( ) ) ; lenient ( ) . do return ( test_read_second_log_command ) . when ( test workspace logs reader ) . get read logs command ( test_workspace_id @$ paths . get ( format ( __str__ @$ path_to_store_logs @$ test_workspace_id @$ second_log_info . get name ( ) ) ) @$ second_log_info .,init logs command
we are setting an empty value to these attributes @$ since now we have a new entity type called hive process execution which captures these <PLACE_HOLDER> . we have to set empty <PLACE_HOLDER> here because these attributes are mandatory attributes for hive process entity type .,ret . set attribute ( attribute_start_time @$ empty_attribute_value ) ; ret . set attribute ( attribute_end_time @$ empty_attribute_value ) ; ret . set attribute ( attribute_user_name @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_text @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_id @$ empty_attribute_value ) ; ret . set attribute ( attribute_query_plan @$ __str__ ) ; ret . set attribute ( attribute_recent_queries @$ collections . singleton list ( query str ) ) ; return ret ;,which captures values
check existing schema contains all the <PLACE_HOLDER> in to validate schema,if ( to validate . get types ( ) . size ( ) != expected . get types ( ) . size ( ) ) { return false ; } hash set < schema > types = new hash set < schema > ( expected . get types ( ) ) ; for ( schema to validate type : to validate . get types ( ) ) { schema equal schema = null ; for ( schema type : types ) { if ( compare ( type @$ to validate type ) ) { equal schema = type ; break ; } } if ( equal schema == null ) { return false ; } types . remove ( equal schema ) ; } return true ;,schema contains types
local get can not be used with mvcc as local node can contain some visible <PLACE_HOLDER> which is not latest .,boolean fast loc get = ! cctx . mvcc enabled ( ) && ( ! force primary || aff nodes . get ( __num__ ) . is local ( ) ) && cctx . reserve for fast local get ( part @$ top ver ) ; if ( fast loc get ) { try { if ( local get ( top ver @$ key @$ part @$ loc vals ) ) return true ; } finally { cctx . release for fast local get ( part @$ top ver ) ; } } return false ;,node contain version
find function name @$ pointed by function . long timestamp can shift if position so use <PLACE_HOLDER> of timestamp to find the function which is <PLACE_HOLDER>ed by ' : ' .,left index = right index + __num__ ; while ( character . is whitespace ( line . char at ( left index ) ) ) { ++ left index ; } right index = line . index of ( __str__ @$ left index ) ; final string function = line . substring ( left index @$ right index ) ; if ( ! function . equals ( function_tracing_mark_write ) ) { continue ; },position use track
we read a further byte @$ which indicates the zmtp <PLACE_HOLDER> .,byte protocol = greeting recv . get ( revision pos ) ; if ( protocol == protocol . v1 . revision || protocol == protocol . v2 . revision ) { greeting send . limit ( v2_greeting_size ) ; greeting send . position ( signature_size + __num__ ) ; greeting send . put ( ( byte ) options . type ) ; outsize += __num__ ; } else { greeting send . limit ( v3_greeting_size ) ; greeting send . position ( signature_size + __num__ ) ; greeting send . put ( ( byte ) __num__ ) ; outsize += __num__ ; greeting send . mark ( ) ; greeting send . put ( new byte [ __num__ ] ) ; assert ( mechanism == mechanisms . null,which indicates version
while it does seem a bit silly @$ we need a check to make sure the document pane is enabled since this method will get called during the processing stage and we do n't want anything below to be fired during so . this checks to make sure the user has selected appropriate <PLACE_HOLDER> for the combine sentences option to be enabled .,if ( e . is popup trigger ( ) && main . document pane . is enabled ( ) ) { if ( main . editor driver . new caret position [ __num__ ] > main . editor driver . new caret position [ __num__ ] ) { start = main . editor driver . new caret position [ __num__ ] ; stop = main . editor driver . new caret position [ __num__ ] ; } else { start = main . editor driver . new caret position [ __num__ ] ; stop = main . editor driver . new caret position [ __num__ ] ; } if ( start == stop ) { right click menu . enable combine sentences ( false ) ; right click,user selected keys
vm <PLACE_HOLDER> locks and frees key <PLACE_HOLDER>,vm1 . invoke ( new serializable runnable ( ) { @ override public void run ( ) { logger . info ( __str__ ) ; connect distributed system ( ) ; d lock service dls = ( d lock service ) distributed lock service . create ( dls name @$ get system ( ) ) ; assert that ( dls . lock ( key1 @$ - __num__ @$ - __num__ ) ) . is true ( ) ; assert that ( dls . is lock grantor ( ) ) . is false ( ) ; assert that ( dls . get token ( key1 ) ) . is not null ( ) ; dls . unlock ( key1 ) ; assert that ( dls . get token (,vm locks 1
flush enough files to get up to the threshold @$ does n't need <PLACE_HOLDER>,for ( int i = __num__ ; i < __num__ ; i ++ ) { put put = new put ( table name . to bytes ( ) ) . add column ( family @$ family @$ table name . to bytes ( ) ) ; region . put ( put ) ; fr = region . flush ( true ) ; assert true ( fr . is flush succeeded ( ) ) ; assert false ( fr . is compaction needed ( ) ) ; },files need compaction
this will change the source text <PLACE_HOLDER> instead @$ which will update the extract text <PLACE_HOLDER> .,mime . on extracted delete text ( start @$ end ) ;,which update view
set new items without calling notify cell data set changed <PLACE_HOLDER> of cell recycler view adapter,m row header recycler view adapter . set items ( new row header @$ ! m enable animation ) ; m cell recycler view adapter . set items ( new column items @$ ! m enable animation ) ; if ( m enable animation ) { final row header sort callback diff callback = new row header sort callback ( old row header @$ new row header ) ; final diff util . diff result diff result = diff util . calculate diff ( diff callback ) ; diff result . dispatch updates to ( m row header recycler view adapter ) ; diff result . dispatch updates to ( m cell recycler view adapter ) ; },data set method
the specific device <PLACE_HOLDER> does not match the generic device <PLACE_HOLDER> .,if ( specific device class . generic device class != generic . not_known && specific device class . generic device class != this . generic device class ) { throw new illegal argument exception ( __str__ ) ; } this . specific device class = specific device class ;,class match class
completing the task should trigger the event <PLACE_HOLDER> . this interupts the main flow .,task service . complete ( task . get id ( ) ) ; task sub one task = task service . create task query ( ) . task name ( __str__ ) . single result ( ) ; assert not null ( sub one task ) ; task service . complete ( sub one task . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ;,task trigger deletion
check whether the caller can read the core <PLACE_HOLDER> ?,sm . check read ( corefile ) ;,caller read file
trigger a bucket creation in vm <PLACE_HOLDER> @$ which should cause server <PLACE_HOLDER> to close it 's cache .,assert that thrown by ( ( ) -> server1 . invoke ( ( ) -> put data ( __num__ @$ __num__ @$ __str__ ) ) ) . is instance of ( rmi exception . class ) . has cause instance of ( distributed system disconnected exception . class ) ; assert that ( server2 . invoke ( ( ) -> get bucket list ( ) ) ) . contains exactly ( __num__ ) ;,which cause 1
the override priority for implementation options p 1 camera event <PLACE_HOLDER> on repeating options p 2 session config options,capture config . builder capture config builder = capture config . builder . from ( capture config ) ; camera event callbacks event callbacks = new camera2 config ( m session config . get implementation options ( ) ) . get camera event callback ( camera event callbacks . create empty callback ( ) ) ; m camera event on repeating options = merge options ( event callbacks . create combo callback ( ) . on repeating ( ) ) ; if ( m camera event on repeating options != null ) { capture config builder . add implementation options ( m camera event on repeating options ) ; } capture request capture request = camera2 capture request builder . build ( capture config builder . build (,priority p callback
reorder only down to the lowest odd level and reorder at an odd min level in a separate @$ simpler loop . see <PLACE_HOLDER> above for why min level is always incremented .,++ min level ; runs = bidi . runs ; levels = bidi . levels ; run count = bidi . run count ;,reorder see comments
run the trash emptier for 120 ms @$ it should run 2 times deletion as the interval is 50 ms. verify the checkpoints <PLACE_HOLDER> when shutting down the emptier .,verify auditable trash emptier ( trash @$ __num__ @$ __num__ ) ;,ms. verify failures
let 's close the first <PLACE_HOLDER> with evict on close turned on,conf . set boolean ( __str__ @$ true ) ; cache conf = new cache config ( conf @$ bc ) ; hsf = new h store file ( this . fs @$ path cow off @$ conf @$ cache conf @$ bloom type . none @$ true ) ; hsf . init reader ( ) ; reader = hsf . get reader ( ) ; reader . close ( cache conf . should evict on close ( ) ) ;,'s close file
making subcluster 0 slow to reply @$ should only get d <PLACE_HOLDER> from nn 1,minidfs cluster dfs cluster = cluster . get cluster ( ) ; name node nn0 = dfs cluster . get name node ( __num__ ) ; simulate slow namenode ( nn0 @$ __num__ ) ; wait update live nodes ( json string2 @$ metrics ) ; final string json string3 = metrics . get live nodes ( ) ; assert equals ( __num__ @$ get num datanodes ( json string3 ) ) ;,1 get ns
nobody has the correct <PLACE_HOLDER>,test protected denied ( url @$ __str__ @$ __str__ ) ; test protected denied ( url @$ __str__ @$ __str__ ) ; test protected denied ( url @$ __str__ @$ __str__ ) ;,nobody has permission
verify the server got the <PLACE_HOLDER> with right size,assert equals ( request size @$ result size ) ;,server got request
if we use <PLACE_HOLDER> @$ setup the dex reporter to notify package manager of any relevant dex loads . the idle maintenance job will use the information reported to optimize the loaded dex files . note that we only need one global reporter per app . make sure we do this before invoking app code for the first time so that we can capture,base dex class loader . set reporter ( dex load reporter . get instance ( ) ) ;,job use api
a stubborn caller will get string reader <PLACE_HOLDER> .,if ( m_bytes == null ) { m_string reader = new string reader ( __str__ ) ; throw new io exception ( string . format ( __str__ @$ m_path ) ) ; },caller get call
russian internal passports use <PLACE_HOLDER> for the name and have a number one digit longer than fits into the standard mrz format,if ( __str__ . equals ( result . issuing country ) && mrz lines [ __num__ ] . char at ( __num__ ) == __str__ ) { result . type = result . type_internal_passport ; string [ ] names = result . first name . split ( __str__ ) ; result . first name = cyrillic to latin ( russian passport translit ( names [ __num__ ] ) ) ; if ( names . length > __num__ ) result . middle name = cyrillic to latin ( russian passport translit ( names [ __num__ ] ) ) ; result . last name = cyrillic to latin ( russian passport translit ( result . last name ) ) ; if ( result . number != null ) result .,passports use spaces
always <PLACE_HOLDER> before installing . we might be downgrading @$ which requires an <PLACE_HOLDER> @$ or we might just want a clean installation .,uninstall agent ( event bus @$ device ) ; agent info = optional . empty ( ) ;,which requires unvalid
run two animators @$ one of which has a start <PLACE_HOLDER> @$ after setting the duration scale to 0,a1 . set start delay ( __num__ ) ; final my listener l1 = new my listener ( ) ; final my listener l2 = new my listener ( ) ; a1 . add listener ( l1 ) ; a2 . add listener ( l2 ) ; m activity rule . run on ui thread ( new runnable ( ) { @ override public void run ( ) { assert false ( l1 . start called ) ; assert false ( l2 . start called ) ; assert false ( l1 . end called ) ; assert false ( l2 . end called ) ; a1 . start ( ) ; a2 . start ( ) ; assert equals ( a2_start_value @$ a2 . get animated value (,one has delay
initial security properties should only contain initial <PLACE_HOLDER> of values,this . server starter . start server ( props @$ this . ls rule . get member ( __num__ ) . get port ( ) ) ; distributed system ds = this . server starter . get cache ( ) . get distributed system ( ) ;,properties contain set
populate the cache and then reset the build <PLACE_HOLDER>,process result cache populating result = workspace . run buck command ( __str__ @$ target . get fully qualified name ( ) ) ; cache populating result . assert success ( ) ;,the build process
only master stream can resolve the data <PLACE_HOLDER>,assert ( m_coordinator . is master ( ) ) ;,stream resolve space
if in zip 64 format or using strict entry numbers @$ use the parsed information as is to read the central directory file <PLACE_HOLDER> .,if ( zip data . is zip64 ( ) || strict entries ) { read central directory file headers ( zip data . get expected entries ( ) @$ zip data . get central directory offset ( ) ) ; } else { long central directory offset = eocd location - zip data . get central directory size ( ) ; if ( ( int ) central directory offset == ( int ) zip data . get central directory offset ( ) ) { read central directory file headers ( central directory offset ) ; } else { read central directory file headers ( zip data . get expected entries ( ) @$ zip data . get central directory offset ( ) ) ; } },directory file headers
use raw socket instead of telnet client here because telnet client sends an extra newline char after each write which causes the <PLACE_HOLDER> to become unresponsive .,socket = new socket ( ) ; socket . connect ( new inet socket address ( connection . get host ( ) @$ connection . get telnet port ( ) ) @$ timeout ) ; socket . set keep alive ( true ) ; socket . set so timeout ( timeout ) ; in = new buffered reader ( new input stream reader ( socket . get input stream ( ) ) ) ; out = new output stream writer ( socket . get output stream ( ) @$ __str__ ) ; connected = true ; callback . listener connected ( ) ;,which causes connection
if multiple threads are doing the <PLACE_HOLDER> at the same time it is not a problem because they will all get to the same result in the end .,if ( format == null ) { format = logging support . get simple format ( ) ; },threads doing initialization
if we have more than one stream @$ lets the <PLACE_HOLDER> be the master,if ( ! master stream set ) { if ( remote descriptions . size ( ) > __num__ ) { if ( media type . audio . equals ( media type ) ) { master stream = true ; master stream set = true ; } } else { master stream = true ; master stream set = true ; } } media stream stream = init stream ( connector @$ dev @$ fmt @$ target @$ direction @$ rtp extensions @$ master stream ) ;,master lets audio
if job is already succeed and h base segment in ready <PLACE_HOLDER> @$ remove the build <PLACE_HOLDER>,if ( executable state . succeed . equals ( job state ) ) { cube segment cube segment = cube instance . get segment ( segment state . get segment name ( ) @$ null ) ; if ( cube segment != null && segment status enum . ready == cube segment . get status ( ) ) { logger . info ( __str__ @$ job id @$ segment state . get segment name ( ) ) ; coordinator . get stream metadata store ( ) . remove segment build state ( cube name @$ segment state . get segment name ( ) ) ; } return false ; },the build state
rose and durant have 5 <PLACE_HOLDER> per game but only rose does not play in okc,iterable < player > filtered players = filter ( players ) . with ( __str__ ) . equals to ( __num__ ) . and ( __str__ ) . not equals to ( __str__ ) . get ( ) ; assert that ( filtered players ) . contains only ( kobe ) ;,durant have players
relay state data may be included with a saml protocol message transmitted with this binding . the value must not exceed 80 <PLACE_HOLDER> in length and should be integrity protected by the entity creating the message independent of any other protections that may or may not exist during message transmission .,if ( relay state != null ) { if ( relay state . length ( ) > __num__ ) { throw new illegal argument exception ( __str__ + relay state . length ( ) ) ; } encoder . add param ( relay_state @$ relay state ) ; },value exceed bytes
inversion does not handle <PLACE_HOLDER> correctly .,test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; invert = true ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ; test in function ( __str__ @$ __str__ ) ;,inversion handle exceptions
else release pause <PLACE_HOLDER>,pause reader ( ) ;,release pause reader
by convention @$ full message <PLACE_HOLDER> extends first message <PLACE_HOLDER> .,if ( content . readable bytes ( ) > max content length - m . content ( ) . readable bytes ( ) ) { @ suppress warnings ( __str__ ) s s = ( s ) current message ; invoke handle oversized message ( ctx @$ s ) ; return ; },type extends type
grid and escalator take care of their own cleanup at removal @$ no <PLACE_HOLDER> to clear details from those . because this removal happens instantly any pending scroll to row or such should not <PLACE_HOLDER> another attempt and unless something else causes such <PLACE_HOLDER> the pending operations should be cleared out .,mark details added or updated for delayed alert to grid ( false ) ;,grid take need
then inner comparison fails as the fields have different <PLACE_HOLDER>,comparison difference value difference = diff ( __str__ @$ witha . inner @$ withb . inner @$ __str__ ) ; verify should be equal by comparing field by field recursively call ( witha @$ withb @$ value difference ) ;,fields have types
we have a real error @$ so do <PLACE_HOLDER> the appropriate action tells us <PLACE_HOLDER> to do ...,coding error action action = result . is unmappable ( ) ? unmappable character action : malformed input action ; if ( action == coding error action . report ) { return result ; } else if ( action == coding error action . replace ) { if ( out . remaining ( ) < replacement bytes . length ) { return coder result . overflow ; } out . put ( replacement bytes ) ; } in . position ( in . position ( ) + result . length ( ) ) ;,action tells what
base set but with label b and different <PLACE_HOLDER>,new equals tester ( ) . add equality group ( dependency . with configuration and aspects ( a @$ host @$ two aspects ) @$ dependency . with configuration and aspects ( a explicit @$ host @$ two aspects ) @$ dependency . with configuration and aspects ( a @$ host @$ inverse aspects ) @$ dependency . with configuration and aspects ( a explicit @$ host @$ inverse aspects ) @$ dependency . with configured aspects ( a @$ host @$ two aspects @$ two aspects host map ) @$ dependency . with configured aspects ( a explicit @$ host @$ two aspects @$ two aspects host map ) ) . add equality group ( dependency . with configuration and aspects ( b @$ host @$ two,base set type
delete notifications do n't need all <PLACE_HOLDER> . hence the special handling for delete operation,if ( instance converter != null ) { if ( operation == entity operation . delete ) { for ( atlas entity header entity header : entity headers ) { ret . add ( new referenceable ( entity header . get guid ( ) @$ entity header . get type name ( ) @$ entity header . get attributes ( ) ) ) ; } } else { for ( atlas entity header entity header : entity headers ) { ret . add ( to referenceable ( entity header . get guid ( ) ) ) ; } } },notifications need attributes
hadoop 's external sort stores the <PLACE_HOLDER> of available memory bytes in an int @$ this prevents integer overflow,check argument ( memorymb < __num__ @$ __str__ ) ; this . memorymb = memorymb ; return this ;,sort stores number
if the message has no <PLACE_HOLDER> @$ or the bodies list is empty,if ( old msg . get body ( ) == null && old msg . get bodies ( ) . size ( ) == __num__ ) { return old msg ; } message new msg = new message ( ) ; new msg . set stanza id ( packet . get stanza id ( ) ) ; new msg . set to ( packet . get to ( ) ) ; new msg . set from ( packet . get from ( ) ) ;,message has body
control all of the rows which has same column <PLACE_HOLDER> .,if ( should fit columns ( column position @$ my position ) ) { if ( m last dx < __num__ ) { log . e ( log_tag @$ __str__ + column position + __str__ + my position + __str__ + __str__ ) ; m cell layout manager . fit width size ( column position @$ true ) ; } else { m cell layout manager . fit width size ( column position @$ false ) ; log . e ( log_tag @$ __str__ + column position + __str__ + my position + __str__ + __str__ ) ; } m need fit for vertical scroll = false ; },which has position
use the new provider api to better leverage the build <PLACE_HOLDER>,_target . get jvm argument providers ( ) . add ( new data template jvm argument provider ( resolver path str @$ get project ( ) . get root dir ( ) ) ) ; _target . get argument providers ( ) . add ( new data template argument provider ( arrays . as list ( _target . get destination dir ( ) . get path ( ) @$ _target . get input dir ( ) . get path ( ) ) ) ) ;,the build compilation
user 1 creates <PLACE_HOLDER>,note note public = notebook . create note ( __str__ @$ new authentication info ( __str__ ) ) ;,user creates note
check that the password is disguised and the toggle button reflects the same <PLACE_HOLDER>,assert not equals ( input_text @$ text input . get layout ( ) . get text ( ) . to string ( ) ) ; on view ( with id ( r . id . textinput_password ) ) . perform ( click icon ( true ) ) ;,button reflects action
we just wait for expected events to arrive . caller will do <PLACE_HOLDER> and throw exception .,return true ;,caller do something
resolution of <PLACE_HOLDER> is funky on windows . python does n't follow <PLACE_HOLDER> @$ but java does so : do an extra resolution in java @$ and see what happens . yay .,path original pwd = pwd ; try { pwd = pwd . to real path ( ) . to absolute path ( ) ; } catch ( io exception e ) { },python follow security
number of state infos that <PLACE_HOLDER> .,int match count = __num__ ; int maxo styles = style . states . length ; for ( int this counter = states . length - __num__ ; this counter >= __num__ ; this counter -- ) { int state = states [ this counter ] . get component state ( ) ; boolean found = false ; for ( int o counter = maxo styles - __num__ - match count ; o counter >= __num__ ; o counter -- ) { if ( state == style . states [ o counter ] . get component state ( ) ) { style . states [ o counter ] = states [ this counter ] . add to ( style . states [ o counter ] ) ; state,number infos match
we need this delay so wm can not treat two <PLACE_HOLDER> on title as double click,robot . delay ( __num__ ) ; util . click on title ( owner_frame @$ robot ) ; util . wait for idle ( robot ) ; system . out . println ( __str__ ) ;,wm treat windows
check if session handler sends a goaway <PLACE_HOLDER> when closing,session handler . write inbound ( close message ) ; assert go away ( session handler . read outbound ( ) @$ local stream id ) ; assert null ( session handler . read outbound ( ) ) ; local stream id += __num__ ;,handler sends frame
this way we can verify the <PLACE_HOLDER> of the runnable as well .,verify ( m network queue ) . put ( m request ) ; assert same ( entry @$ m request . get cache entry ( ) ) ; verify ( m delivery @$ never ( ) ) . post error ( any ( request . class ) @$ any ( volley error . class ) ) ;,way verify behavior
this method will be called by the printer job on a thread other that the application 's thread . we hold on to the graphics until we can rendevous with the application 's thread and hand over the graphics . the application then does all the <PLACE_HOLDER> . when the application is done <PLACE_HOLDER> we rendevous again with the printer job thread and release,int result ;,application does work
when the trailer does not contain orca <PLACE_HOLDER> @$ listener callback will not be invoked .,metadata trailer = new metadata ( ) ; tracer . inbound trailers ( trailer ) ; verify no more interactions ( orca listener1 ) ;,trailer contain report
now @$ send the second request so that the client reuses the <PLACE_HOLDER> .,final http response res2 = client . get ( __str__ ) ;,client reuses connection
evaluate any karate expression even on lhs will throw <PLACE_HOLDER> if variable does not exist,actual = eval karate expression ( expression @$ context ) ; if ( actual . is json like ( ) ) { path = var_root ; },expression throw exception
for every time when a user has not selected a <PLACE_HOLDER> but a basic block this breaks . as it does throw a null pointer exception .,first function . load ( ) ; second function . load ( ) ; final creation thread creation thread = new creation thread ( module @$ source block @$ target block @$ first function @$ second function ) ; progress dialog . show ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ @$ creation thread ) ; if ( ( ! ( creation thread . threw exception ( ) ) ) && ( creation thread . get created view ( ) == null ) ) { message box . show information ( plugin interface . get main window ( ) . get frame ( ) @$ __str__ ) ; } else { new thread ( ) { @ override public void,user selected function
application log <PLACE_HOLDER>,if ( app id != null ) { app log dir present . set true ( ) ; if ( should clean app log dir ( child path @$ now @$ fs @$ retain millis ) ) { delete dir ( child path ) ; } } else { clean app log dir ( child path @$ retain millis @$ app log dir present ) ; },application log dir
insert one <PLACE_HOLDER> @$ so calls below produce just one <PLACE_HOLDER> .,client . call procedure ( __str__ @$ __num__ @$ __num__ ) ;,calls produce row
management server sends back an rds <PLACE_HOLDER> containing route configurations more than requested .,list < any > route configs = immutable list . of ( any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) @$ any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) @$ any . pack ( build route configuration ( __str__ @$ immutable list . of ( build virtual host ( immutable list . of ( __str__ ) @$ __str__ ) ) ) ) ) ;,server sends response
the default <PLACE_HOLDER> provider uses a <PLACE_HOLDER> of 1 for all calls @$ ignoring the processing details @$ so an empty one is fine,processing details empty processing details = new processing details ( time unit . milliseconds ) ; scheduler . add response time ( __str__ @$ mock call @$ empty processing details ) ; return priority ;,provider uses priority
components with their own comp context do their own <PLACE_HOLDER>,if ( naming mode == component naming mode . create && comp binding ) { continue ; },components do binding
update log reg <PLACE_HOLDER> with new portion of data .,logistic regression model mdl2 = trainer . update ( mdl @$ ignite @$ data cache @$ split . get test filter ( ) @$ normalization preprocessor ) ; system . out . println ( __str__ + mdl ) ; double accuracy = evaluator . evaluate ( data cache @$ mdl2 @$ normalization preprocessor @$ metric name . accuracy ) ; system . out . println ( __str__ + accuracy ) ; system . out . println ( __str__ + ( __num__ - accuracy ) ) ; system . out . println ( __str__ ) ;,update log model
everyone can submit <PLACE_HOLDER> on queue c .,verify submit reservation success ( queue_b_user @$ queuec ) ; verify submit reservation success ( queue_b_admin @$ queuec ) ; verify submit reservation success ( queue_a_user @$ queuec ) ; verify submit reservation success ( queue_a_admin @$ queuec ) ; verify submit reservation success ( common_user @$ queuec ) ;,everyone submit reservations
update the context chars and the unsafe backward set while copying @$ in case a character had conditional <PLACE_HOLDER> in the source builder and they were removed later .,modified |= src . modified ;,character had mappings
test echo <PLACE_HOLDER>,echo request proto echo request = echo request proto . new builder ( ) . set message ( __str__ ) . build ( ) ; echo response proto echo response = stub . echo ( null @$ echo request ) ; assert equals ( __str__ @$ echo response . get message ( ) ) ; stub . error ( null @$ empty request ) ; fail ( __str__ ) ; rpc client . close ( ) ;,test echo method
default nine should not appear @$ not checked checkbox ten should not appear @$ disabled eleven should not appear @$ <PLACE_HOLDER>,assert equals ( __str__ @$ data . get ( __num__ ) . to string ( ) ) ;,checkbox appear button
get image cell layout which has image <PLACE_HOLDER> instead of text <PLACE_HOLDER> .,layout = inflater . inflate ( r . layout . table_view_image_cell_layout @$ parent @$ false ) ; return new gender cell view holder ( layout ) ; default :,which has view
default method below so we they can get the default <PLACE_HOLDER> when needed .,return g tree . this . get tool tip text ( event ) ;,method get value
the title is same @$ compare their <PLACE_HOLDER>,if ( result == __num__ ) { result = lhs [ __num__ ] . compare to ignore case ( rhs [ __num__ ] ) ; },title compare names
in hive one can set conflicting <PLACE_HOLDER> for the same property @$ in such case it looks like table properties are used,if ( serde property value != null && table property value != null && ! table property value . equals ( serde property value ) ) { throw new presto exception ( hive_invalid_metadata @$ format ( __str__ @$ key @$ serde property value @$ table property value ) ) ; },one set values
producer is not doing any <PLACE_HOLDER> .,create client ( producer @$ the port @$ host0 ) ; final int size = __num__ ; create values ( producer @$ regions [ __num__ ] @$ size ) ; createcq ( client @$ pool name @$ __str__ @$ cqs [ __num__ ] ) ; executecq ( client @$ __str__ @$ true @$ null ) ; create values ( producer @$ regions [ __num__ ] @$ ( __num__ * size ) ) ; for ( int i = __num__ ; i <= size ; i ++ ) { if ( i % __num__ == __num__ ) wait for updated ( client @$ __str__ @$ key + i ) ; } for ( int i = ( size + __num__ ) ; i <= __num__ * size ; i,producer doing thing
if the declared type can be assigned into the actual type @$ or the expected type @$ then the compiler already has sufficient type <PLACE_HOLDER> .,return false ;,compiler has limits
key 1 value 1 key <PLACE_HOLDER> ...,node key = n . get first child ( ) ; while ( key != null ) { switch ( key . get token ( ) ) { case getter_def : case setter_def : case string_key : case member_function_def : if ( is strip name ( key . get string ( ) ) ) { node next = key . get next ( ) ; n . remove child ( key ) ; node util . mark functions deleted ( key @$ compiler ) ; key = next ; compiler . report change to enclosing scope ( n ) ; break ; } default : key = key . get next ( ) ; } },key value case
if the user specified the <PLACE_HOLDER> @$ then keep that,if ( from parms . _network == network . auto || from parms . _network == null ) { if ( from parms . _network_definition_file != null && ! from parms . _network_definition_file . equals ( __str__ ) ) { if ( ! from parms . _quiet_mode ) log . info ( __str__ ) ; to parms . _network = network . user ; } else { if ( to parms . _problem_type == problem type . image ) to parms . _network = network . inception_bn ; if ( to parms . _problem_type == problem type . text || to parms . _problem_type == problem type . dataset ) { to parms . _network = null ; if ( from parms . _hidden == null ) {,user specified network
test that ` source path ` coercion does n't change the <PLACE_HOLDER> from ` path ` cercion .,assert same message ( path coerce exception @$ get coerce exception ( source path . class @$ invalid path ) ) ;,path change result
we have now read the file header @$ and obtained the position for <PLACE_HOLDER> of the data items . now read <PLACE_HOLDER> in turn @$ first seeking the input stream to the position of the data item .,bytes . reset ( ) ; icu binary . skip bytes ( bytes @$ cfu keys offset ) ; fcfu keys = icu binary . get ints ( bytes @$ cfu keys size @$ __num__ ) ; bytes . reset ( ) ; icu binary . skip bytes ( bytes @$ cfu values offset ) ; fcfu values = icu binary . get shorts ( bytes @$ cfu values size @$ __num__ ) ; bytes . reset ( ) ; icu binary . skip bytes ( bytes @$ cfu string table offset ) ; fcfu strings = icu binary . get string ( bytes @$ cfu string table size @$ __num__ ) ;,position read all
create a temporary <PLACE_HOLDER> @$ as jaudiotagger requires a <PLACE_HOLDER> rather than an input stream,temp file = file . create temp file ( filename @$ __str__ + file ext ) ; long bytes copied = file utils . copy ( source @$ temp file @$ max bytes ) ; partially parsed = bytes copied == max bytes && source . read ( ) != - __num__ ; f = audio fileio . read ( temp file ) ;,jaudiotagger requires file
<PLACE_HOLDER> <PLACE_HOLDER> modified modified added elements added <PLACE_HOLDER> modified elements modified & added <PLACE_HOLDER> modified added <PLACE_HOLDER> <PLACE_HOLDER> <PLACE_HOLDER> added <PLACE_HOLDER> <PLACE_HOLDER>,string incoming = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ;,& added split
click on the input icon means : <PLACE_HOLDER> to a new hop in this case @$ we set the end hop step ...,selected step = null ; start hop step = null ; end hop step = ( step meta ) area owner . get parent ( ) ; candidate hop type = null ; start error hop step = false ;,click means move
roll over the master key do allocate again . the am should get the latest amrm <PLACE_HOLDER>,rm . getrm context ( ) . getamrm token secret manager ( ) . roll master key ( ) ; response = am . allocate ( records . new record ( allocate request . class ) ) ; assert . assert not null ( response . getamrm token ( ) ) ; token < amrm token identifier > amrm token = converter utils . convert from yarn ( response . getamrm token ( ) @$ new text ( response . getamrm token ( ) . get service ( ) ) ) ; assert . assert equals ( amrm token . decode identifier ( ) . get key id ( ) @$ rm . getrm context ( ) . getamrm token secret manager ( ) . get master key,am get token
remote exception contains useful <PLACE_HOLDER> as against the java.lang.reflect exceptions .,if ( cause instanceof io exception ) { throw ( io exception ) cause ; } else if ( cause instanceof runtime exception ) { throw ( runtime exception ) cause ; } else { throw new io exception ( se ) ; },exception contains information
not testing string equality since some browsers return the <PLACE_HOLDER> with quotes around the url argument and some without quotes .,assert true ( background image + __str__ @$ background image . contains ( __str__ ) ) ;,equality return result
a media check on anki droid will also update the media <PLACE_HOLDER>,col . get media ( ) . find changes ( true ) ;,check update view
all rows have this <PLACE_HOLDER>,model = new object table model ( columns @$ calculator . class @$ new functor [ ] { new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) @$ new functor ( __str__ ) } @$ new functor [ ] { null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null @$ null } @$ new class [ ] { string . class @$ integer . class @$ long .,rows have constructor
check if we can use the fast path @$ resuming a session . we can do so iff we have a valid record for that session @$ and the cipher suite for that session was on the list <PLACE_HOLDER> the client requested @$ and if we 're not forgetting any needed authentication on the part of the client .,if ( previous != null ) { resuming session = previous . is rejoinable ( ) ; if ( resuming session ) { protocol version old version = previous . get protocol version ( ) ; if ( old version != mesg . protocol version ) { resuming session = false ; } } if ( resuming session && use extended master secret ) { if ( requested to useems && ! previous . get use extended master secret ( ) ) { resuming session = false ; } else if ( ! requested to useems && previous . get use extended master secret ( ) ) { fatalse ( alerts . alert_handshake_failure @$ __str__ + __str__ ) ; } else if ( ! requested to useems &&,client requested which
asif : take a lock on active cache while removing the connection from the map . it is possible that while the connection is being returned the cleaner thread migh have also removed it from the active map . if that is the case we do n't do anything bcoz the cleaner thread will take <PLACE_HOLDER> of decrementing the total & available connection .,if ( connection object != null ) { synchronized ( connection object ) { if ( this . active cache . contains key ( connection object ) ) { this . active cache . remove ( connection object ) ; returned happened = true ; } } },thread take care
there are context registers to worry about @$ so we need a function start <PLACE_HOLDER> and a function start analyzer.context <PLACE_HOLDER> for each context register,map < string @$ big integer > regs to values = c reg filter . get value map ( ) ; match action [ ] actions = new match action [ __num__ + regs to values . size ( ) ] ; actions [ __num__ ] = func start analyzer . new function start action ( ) ; int match index = __num__ ; for ( string register : regs to values . key set ( ) ) { big integer value = regs to values . get ( register ) ; actions [ match index ] = func start analyzer . new context action ( register @$ value ) ; match index ++ ; } return actions ;,function start action
update the backup if the user id or any of the backed up android preferences <PLACE_HOLDER> .,shared prefs . register on shared preference change listener ( new shared preferences . on shared preference change listener ( ) { @ override public void on shared preference changed ( shared preferences shared preferences @$ string key ) { if ( key . equals ( chrome signin controller . signed_in_account_key ) ) { on backup prefs changed ( ) ; return ; } for ( string pref : chrome backup agent . backup_android_bool_prefs ) { if ( key . equals ( pref ) ) { on backup prefs changed ( ) ; return ; } } } } ) ;,id backed changes
the first page load is used only to cause the web view to fetch the proxy settings . do n't update the url <PLACE_HOLDER> @$ and do n't check if the captive portal is still there .,if ( m pages loaded == __num__ ) return ;,settings update string
each sample has up to three <PLACE_HOLDER> of overhead for the start code that replaces its length . allow ten source samples per output sample @$ like the platform extractor .,int max input size = track sample table . maximum size + __num__ * __num__ ; format format = track . format . copy with max input size ( max input size ) ; if ( track . type == c . track_type_video && track duration us > __num__ && track sample table . sample count > __num__ ) { float frame rate = track sample table . sample count / ( track duration us / __num__ ) ; format = format . copy with frame rate ( frame rate ) ; } format = metadata util . get format with metadata ( track . type @$ format @$ udta metadata @$ mdta metadata @$ gapless info holder ) ; mp4 track . track output . format (,sample has bytes
try to update something that does n't exists should not change existing <PLACE_HOLDER>,repository . update all ( collections . singleton ( generator . get ( ) ) ) ;,something change data
the task manager wo n't send heartbeat <PLACE_HOLDER> to the resource manager,resource manager gateway . heartbeat from task manager ( resourceid @$ heartbeat payload ) ;,manager send messages
if we 're only parsing integers @$ or if we already saw the decimal @$ then do n't parse this <PLACE_HOLDER> .,if ( is parse integer only ( ) || saw decimal ) { break ; } digits . decimal at = digit count ;,then parse one
button.foreground @$ button.shadow @$ button.dark shadow @$ button.disabled forground @$ and button.disabled shadow are only used for windows classic . windows xp will use <PLACE_HOLDER> from the current visual style .,object [ ] defaults = { __str__ @$ null @$ __str__ @$ boolean . value of ( use system font settings ) @$ __str__ @$ field input map @$ __str__ @$ password input map @$ __str__ @$ multiline input map @$ __str__ @$ multiline input map @$ __str__ @$ multiline input map @$ __str__ @$ control font @$ __str__ @$ control background color @$ __str__ @$ control text color @$ __str__ @$ control shadow color @$ __str__ @$ control dark shadow color @$ __str__ @$ control light color @$ __str__ @$ control highlight color @$ __str__ @$ inactive text color @$ __str__ @$ control highlight color @$ __str__ @$ button focus color @$ __str__ @$ new xp value ( integer . value of ( __num__ ) @$ integer,shadow use those
somebody took away our unwanted <PLACE_HOLDER> .,if ( delta <= unused guaranteed ) { int result = ( unused guaranteed -= delta ) ; if ( metrics != null ) { metrics . set wm unused guaranteed ( result ) ; } wm_log . info ( __str__ + result + __str__ + delta ) ; return ; } else { delta -= unused guaranteed ; unused guaranteed = __num__ ; to update = new array list < > ( ) ; int total updated = revoke guaranteed ( delta @$ null @$ to update ) ; if ( metrics != null ) { metrics . set wm unused guaranteed ( __num__ ) ; } wm_log . info ( __str__ + total updated + __str__ + delta ) ; if ( delta != total updated,somebody took allocation
for each dimension read the <PLACE_HOLDER> of each dimension followed by the values,try { for ( int i = __num__ ; i < size ; i ++ ) { int length = in . read int ( ) ; byte [ ] b = new byte [ length ] ; in . read fully ( b ) ; dimension values [ i ] = new string ( b @$ __str__ ) ; } } catch ( exception e ) { logger . info ( arrays . to string ( bytes ) @$ e ) ; throw new runtime exception ( e ) ; } return new dimension key ( dimension values ) ;,dimension read length
we need something that will measure the amount of time since our consumer has seen a <PLACE_HOLDER> ...,time since time since last record = threads . time since ( clock . system ) ;,consumer seen snapshot
platform.run later is necessary or <PLACE_HOLDER>s will be empty since check <PLACE_HOLDER>s adds 2 base <PLACE_HOLDER> later .,platform . run later ( ( ) -> { initialized = true ; selected profile . set ( profiles . stream ( ) . filter ( it -> it . get name ( ) . equals ( config ( ) . get selected profile ( ) ) ) . find first ( ) . or else ( profiles . get ( __num__ ) ) ) ; } ) ; event bus . event_bus . channel ( refreshed versions event . class ) . register weak ( event -> { run infx ( ( ) -> { profile profile = selected profile . get ( ) ; if ( profile != null && profile . get repository ( ) == event . get source ( ) ) { selected,profiles adds profile
any activity on the power button stops the accessibility <PLACE_HOLDER>,cancel pending accessibility shortcut action ( ) ; result &= ~ action_pass_to_user ; is wake key = false ;,activity stops shortcut
check if the grantor matches current <PLACE_HOLDER>,if ( grant info . get grantor ( ) != null && grant info . get grantor ( ) . equals ( user name ) && grant info . get grantor type ( ) == principal type . user ) { priv2priv obj . put ( grant info . get privilege ( ) @$ ms obj priv ) ; },grantor matches user
for annotation type @$ if annotation only contain single mandatory <PLACE_HOLDER> which is called 'value ' it will be automatically turned into constructor parameter,if ( is annotation value attribute ( ) && there are no other mandatory attributes ( ) ) { return constructor_parameter_default_order ; },annotation contain attribute
update application catalog sometimes changes the <PLACE_HOLDER> . the second procedure call will fail in that case @$ so do n't call it a second time .,if ( pre hash && ! proc name . equals ( __str__ ) ) { params . put ( __str__ @$ get hashed password forhttp var ( password @$ client auth scheme . hash_sha1 ) ) ; call proc overjson raw ( params @$ http port @$ expected code @$ session id ) ; },catalog changes password
dalvik vm rejects <PLACE_HOLDER> in an apk that are already defined . framework <PLACE_HOLDER> take precedence over local <PLACE_HOLDER> .,string class name = smali file . get class name ( ) ; if ( is framework class ( class name ) && ! class name . starts with ( __str__ ) ) { log . warn ( __str__ @$ class name ) ; } else { smali files . add ( smali file ) ; },vm rejects classes
local merge source must have a single <PLACE_HOLDER>,context . set driver instance count ( __num__ ) ; plan node source node = get only element ( node . get sources ( ) ) ; local execution plan context sub context = context . create sub context ( ) ; physical operation source = source node . accept ( this @$ sub context ) ; int operators count = sub context . get driver instance count ( ) . or else ( __num__ ) ; list < type > types = get source operator types ( node @$ context . get types ( ) ) ; local exchange factory exchange factory = new local exchange factory ( node . get partitioning scheme ( ) . get partitioning ( ) . get handle ( ) @$ operators,source have driver
second elector joins <PLACE_HOLDER> @$ becomes standby .,electors [ __num__ ] . join election ( app datas [ __num__ ] ) ; mockito . verify ( cbs [ __num__ ] @$ mockito . timeout ( __num__ ) ) . become standby ( ) ; check fatals and reset ( ) ;,elector joins election
need to go through all elements @$ extract data @$ and check distance agains new centroids create <PLACE_HOLDER> of all centroids,int i ; int j ; int k ; int m ; double [ ] temp = new double [ __num__ ] ;,centroids create versions
static clinits are generated in gen @$ so we need to use a fake one . attr creates a fake clinit <PLACE_HOLDER> while attributing lambda expressions used as initializers of static fields @$ so let 's use that one .,if ( is static ) { method symbol clinit = attr . remove clinit ( csym ) ; if ( clinit != null ) { clinits . put ( csym @$ clinit ) ; return clinit ; } clinit = ( method symbol ) clinits . get ( csym ) ; if ( clinit == null ) { clinit = make private synthetic method ( static @$ names . clinit @$ new method type ( list . < type > nil ( ) @$ syms . void type @$ list . < type > nil ( ) @$ syms . method class ) @$ csym ) ; clinits . put ( csym @$ clinit ) ; } return clinit ; } else { for ( symbol s :,one creates instance
when table limit feature is fully supported @$ there needs to be more test cases . generalize this test within a loop @$ maybe . test max <PLACE_HOLDER> 0,vt = client . call procedure ( __str__ @$ __str__ ) . get results ( ) [ __num__ ] ; validate table of scalar longs ( vt @$ new long [ ] { __num__ } ) ; verify proc fails ( client @$ __str__ @$ __str__ @$ __num__ @$ __num__ @$ __num__ ) ; vt = client . call procedure ( __str__ @$ __str__ ) . get results ( ) [ __num__ ] ; validate table of scalar longs ( vt @$ new long [ ] { __num__ } ) ;,cases test limit
previously we cloned the artifact @$ but it is more efficient to just update the artifact scope if problems are later discovered that the original object needs its original artifact scope <PLACE_HOLDER> @$ cloning may again be appropriate,nearest artifact . set scope ( farthest artifact . get scope ( ) ) ;,object needs info
this is okay . the function has a <PLACE_HOLDER> @$ but it is empty .,break ; case param_list :,function has parameter
two different properties can satisfy the routing field <PLACE_HOLDER>,chained field extractor . no value handler routing response = chained field extractor . no value handler . skip ; list < field extractor > routings = new array list < field extractor > ( __num__ ) ; if ( settings . get mapping routing ( ) != null ) { settings . set property ( constant field extractor . property @$ settings . get mapping routing ( ) ) ; field extractor extractor = object utils . < field extractor > instantiate ( settings . get mapping routing extractor class name ( ) @$ settings ) ; routing response = chained field extractor . no value handler . not_found ; routings . add ( extractor ) ; },properties satisfy spec
global declaration must have a <PLACE_HOLDER>,if ( name attr == null ) { report schema error ( __str__ @$ new object [ ] { __str__ @$ __str__ } @$ elm node ) ; name attr = no_name ; } attr grp . f name = name attr ; attr grp . f target namespace = schema doc . f target namespace ;,declaration have name
step 2 relays the <PLACE_HOLDER> from step 1 to step 3,thread step2 = new step2 ( context ) ; step2 . start ( ) ;,step relays flow
wait a little bit to let the delete take <PLACE_HOLDER> .,thread . sleep ( __num__ ) ;,delete take place
wait until the task created the <PLACE_HOLDER>,long deadline = system . current time millis ( ) + __num__ ; while ( ! temp test file . exists ( ) && system . current time millis ( ) < deadline ) { thread . sleep ( __num__ ) ; } assert true ( __str__ @$ temp test file . exists ( ) ) ;,task created file
modifier letter glottal <PLACE_HOLDER>..modifier letter reversed glottal <PLACE_HOLDER>,if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ch == __num__ ) { return true ; } else if ( ch == __num__ ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else if ( ( ch >= __num__ ) && ( ch <= __num__ ) ) { return true ; } else { return false ;,letter reversed code
the top of the stack always represents the <PLACE_HOLDER> filter . when back is pressed @$ the top is popped off and the new top indicates what filter to use . if there are no filters remaining @$ the activity itself is closed .,if ( m back stack . size ( ) > __num__ ) { m back stack . pop ( ) ; m download manager ui . update for url ( m back stack . peek ( ) ) ; } else { if ( ! m back stack . is empty ( ) ) m back stack . pop ( ) ; super . on back pressed ( ) ; },top represents location
partitioned tables do n't have table <PLACE_HOLDER> set on the fetch task . instead they have a list of partition <PLACE_HOLDER> objects @$ each with a table <PLACE_HOLDER> . let 's try to fetch the <PLACE_HOLDER> for the first partition and use it 's deserializer .,if ( td == null && ft . get work ( ) != null && ft . get work ( ) . get part desc ( ) != null ) { if ( ft . get work ( ) . get part desc ( ) . size ( ) > __num__ ) { td = ft . get work ( ) . get part desc ( ) . get ( __num__ ) . get table desc ( ) ; } } if ( td == null ) { log . info ( __str__ ) ; } else { string table name = __str__ ; list < field schema > lst = null ; try { lst = hive meta store utils . get fields from deserializer ( table,tables have description
when clearing the selection after a text change @$ state is not reset correctly so hitting down again will cause it to start from the previous selection point . we still have to send the key down event to let the list view items take <PLACE_HOLDER> @$ but then we select the first item explicitly .,if ( m suggestion list . get selected item position ( ) == list view . invalid_position ) { boolean result = m suggestion list . on key down ( key code @$ event ) ; m suggestion list . set selection ( __num__ ) ; return result ; } else { return m suggestion list . on key down ( key code @$ event ) ; },items take effect
if the option has a <PLACE_HOLDER> and a non blank argname,if ( option . has arg ( ) && ( option . get arg name ( ) == null || option . get arg name ( ) . length ( ) != __num__ ) ) { buff . append ( option . get opt ( ) == null ? long opt separator : __str__ ) ; buff . append ( __str__ ) . append ( option . get arg name ( ) != null ? option . get arg name ( ) : get arg name ( ) ) . append ( __str__ ) ; },option has argname
ensure the insertion logic can handle a <PLACE_HOLDER> appended to the front,do in hibernate ( this :: session factory @$ s -> { query < employee > query = s . create query ( __str__ @$ employee . class ) . set comment ( __str__ ) . add query hint ( __str__ ) . set parameter ( __str__ @$ __str__ ) ; list < employee > results = query . list ( ) ; assert equals ( results . size ( ) @$ __num__ ) ; } ) ; sql statement interceptor . assert executed count ( __num__ ) ; assert that ( sql statement interceptor . get sql queries ( ) . get ( __num__ ) @$ contains string ( __str__ ) ) ; sql statement interceptor . clear ( ) ;,logic handle comment
flag undeclared <PLACE_HOLDER> for checking after the dtd is fully processed,if ( notations . get ( external id . notation ) == null ) notations . put ( external id . notation @$ boolean . true ) ;,flag undeclared annotations
check that slave satisfies min <PLACE_HOLDER> .,if ( cpus < cluster props . min cpu per node ( ) || mem < cluster props . min memory per node ( ) ) { log . log ( level . fine @$ __str__ @$ offer . get resources list ( ) ) ; return null ; } double total cpus = __num__ ; double total mem = __num__ ; double total disk = __num__ ;,slave satisfies requirements
wake up the reader ... there 's stuff to do @$ <PLACE_HOLDER> to read,has read ahead = false ; lock support . unpark ( this ) ;,stuff 's right
let 's test the default <PLACE_HOLDER>,lst . add ( __str__ ) ;,'s test value
the ' : ' at pos p may be either a port divider or a part of an i pv 6 <PLACE_HOLDER>,if ( p > target . last index of ( __str__ ) ) { target = target . substring ( __num__ @$ p ) ; },part pv address
when final state in last <PLACE_HOLDER> is not default @$ it means last <PLACE_HOLDER> has finished . create a new <PLACE_HOLDER> for this node @$ and add it to the <PLACE_HOLDER> list . return this new <PLACE_HOLDER> . when final state in last <PLACE_HOLDER> is default @$ it means last <PLACE_HOLDER> has not finished . just get last <PLACE_HOLDER> .,if ( node allocation . get final allocation state ( ) != allocation state . default ) { node allocation = new node allocation ( nodeid ) ; node allocations . add ( node allocation ) ; },return create allocation
if the test specifies a specific <PLACE_HOLDER> @$ do not re<PLACE_HOLDER> .,return get description ( ) . get annotation ( fix method order . class ) != null ;,test specifies order
simple equals can cause <PLACE_HOLDER> here because of how equals works e.g . between lists and sets .,return collection utils . is equal collection ( state objects @$ that . state objects ) ;,equals cause problems
j 2 se does not support xalan <PLACE_HOLDER>,do exit ( msg ) ;,se support interpretive
then the write behind action gets 3 <PLACE_HOLDER> to write,awaitility . await ( ) . until true ( writer called ) ; assert . assert equals ( __num__ @$ number of entries . int value ( ) ) ;,write gets entries
first read existing <PLACE_HOLDER> from the <PLACE_HOLDER> table . because we 'll be deleting entries for missing <PLACE_HOLDER> as we go @$ we need to query the database in small batches @$ to avoid problems with cursor window positioning .,if ( prescan files ) { long last id = long . min_value ; uri limit uri = m files uri . build upon ( ) . append query parameter ( media store . param_limit @$ __str__ ) . build ( ) ; while ( true ) { selection args [ __num__ ] = __str__ + last id ; if ( c != null ) { c . close ( ) ; c = null ; } c = m media provider . query ( limit uri @$ files_prescan_projection @$ where @$ selection args @$ media store . files . file columns . _id @$ null ) ; if ( c == null ) { break ; } int num = c . get count ( ) ;,first read images
this happens at least if the server is restarted without preserving the session . after restart the client reconnects @$ gets a session expired <PLACE_HOLDER> and then closes the connection and ends up here,get logger ( ) . log ( level . finer @$ __str__ @$ e ) ; return ;,session expired exception
and has a <PLACE_HOLDER>,select conf . set ( csv_input_header @$ csv_header_opt_use ) ; select conf . set boolean ( select_errors_include_sql @$ true ) ; input must ( select conf @$ csv_input_header @$ csv_header_opt_use ) ; input must ( select conf @$ select_input_format @$ select_format_csv ) ; input must ( select conf @$ select_output_format @$ select_format_csv ) ; input must ( select conf @$ select_input_compression @$ compression_opt_gzip ) ;,and has header
robo vm note : seems like the gc can not reclaim the <PLACE_HOLDER> after this method finishes . it 's probably still referenced by some register . clear it to make its elements reclaimable .,list . clear ( ) ; return true ;,gc reclaim object
the anonymous class ca n't have <PLACE_HOLDER> @$ but we may be binding <PLACE_HOLDER> from super classes,if ( clazz . is anonymous class ( ) ) { if ( clazz . get interfaces ( ) . length != __num__ ) { type parameters = clazz . get interfaces ( ) [ __num__ ] . get type parameters ( ) ; } else { type parameters = clazz . get superclass ( ) . get type parameters ( ) ; } } else { type parameters = clazz . get type parameters ( ) ; },class have interfaces
the configuration distinguisher is only set by apple <PLACE_HOLDER> transition and apple binary transition @$ both of which also set the <PLACE_HOLDER> and the cpu to apple ones . so we are fine not doing anything .,if ( apple options . configuration distinguisher != configuration distinguisher . unknown ) { return build options ; },which set binary
accessibility needs to be able to interact with the device . if these settings are already configured @$ we will not overwrite them . if they are already set @$ it means that the user has performed a global <PLACE_HOLDER> to enable accessibility or set these settings in the accessibility portion of the setup wizard @$ and definitely needs these features working after the,switch ( name ) { case settings . secure . accessibility_enabled : case settings . secure . touch_exploration_enabled : case settings . secure . accessibility_display_daltonizer_enabled : case settings . secure . accessibility_display_magnification_enabled : case settings . secure . accessibility_display_magnification_navbar_enabled : return settings . secure . get int ( m context . get content resolver ( ) @$ name @$ __num__ ) != __num__ ; case settings . secure . touch_exploration_granted_accessibility_services : case settings . secure . enabled_accessibility_services : case settings . secure . accessibility_display_daltonizer : return ! text utils . is empty ( settings . secure . get string ( m context . get content resolver ( ) @$ name ) ) ; case settings . secure . accessibility_display_magnification_scale : float default scale = m context . get,user performed action
this is a 'patch configuration ' which considers <PLACE_HOLDER> ' as default collection,remote instance instance = new remote instance ( target baseurl + __str__ @$ null @$ null @$ __num__ @$ trust self signed on authenticated server @$ long . max_value @$ false ) ;,which considers '
tab beyond last <PLACE_HOLDER> : add a <PLACE_HOLDER> to table !,if ( active table row >= maxrows ) { table item item = new table item ( table @$ swt . none @$ active table row ) ; item . set text ( __num__ @$ __str__ ) ; set row nums ( ) ; } if ( sel ) { edit ( active table row @$ active table column ) ; } table . set focus ( ) ;,tab add line
the rollback operation should have rolled back the first nn 's local <PLACE_HOLDER> @$ and the shared dir @$ but not the other nn 's <PLACE_HOLDER> . those have to be done by bootstrapping the standby .,check nn previous dir existence ( cluster @$ __num__ @$ false ) ; check jn previous dir existence ( qj cluster @$ false ) ; if ( fs != null ) { fs . close ( ) ; } if ( qj cluster != null ) { qj cluster . shutdown ( ) ; },operation rolled dirs
test results of within using contains <PLACE_HOLDER>,prefix = __str__ ; sql = __str__ + __str__ + __str__ ; vt1 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; sql = __str__ + __str__ + __str__ ; vt2 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; assert tables are equal ( prefix @$ vt2 @$ vt1 @$ geography_distance_epsilon ) ; sql = __str__ + __str__ + __str__ ; vt1 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__ ] ; sql = __str__ + __str__ + __str__ ; vt2 = client . call procedure ( __str__ @$ sql ) . get results ( ) [ __num__,results contains expression
hosted providers index will match ' p ' attribute in widget 's entry in the xml file being restored if there 's no live entry for this provider @$ add an inactive <PLACE_HOLDER> so that widget i ds referring to them can be properly allocated,string cl = parser . get attribute value ( null @$ __str__ ) ;,index match attribute
res stores the first found abstract <PLACE_HOLDER>,method res = null ; for ( method mi : methods ) { if ( ! modifier . is abstract ( mi . get modifiers ( ) ) ) continue ; if ( mi . get annotation ( traits . implemented . class ) != null ) continue ; try { object . class . get method ( mi . get name ( ) @$ mi . get parameter types ( ) ) ; continue ; } catch ( no such method exception e ) { } if ( res != null ) return null ; res = mi ; },stores found method
scope might have set to database in some previous iteration of loop @$ so reset it to false if database tracker has no <PLACE_HOLDER> .,scope . database = false ;,tracker has tasks
standalone server does n't need myid <PLACE_HOLDER> .,if ( ! my id file . is file ( ) ) { return ; } buffered reader br = new buffered reader ( new file reader ( my id file ) ) ; string my id string ; try { my id string = br . read line ( ) ; } finally { br . close ( ) ; } try { server id = long . parse long ( my id string ) ; mdc . put ( __str__ @$ my id string ) ; } catch ( number format exception e ) { throw new illegal argument exception ( __str__ + my id string + __str__ ) ; },server need file
this estimate will not take into account the <PLACE_HOLDER> saved by inlining the keys .,return vm thin disk region entry off heap object key . class ;,estimate take memory
let 's traverse the <PLACE_HOLDER> and override all text colors !,stack < view > views to process = new stack < > ( ) ; views to process . add ( root ) ; while ( ! views to process . is empty ( ) ) { view v = views to process . pop ( ) ; if ( v instanceof text view ) { text view text view = ( text view ) v ; text view . set text ( contrast color util . clear color spans ( text view . get text ( ) ) ) ; text view . set text color ( text color ) ; } if ( v instanceof view group ) { view group view group = ( view group ) v ; for ( int i = __num__,'s traverse view
'digest ' already has the <PLACE_HOLDER> from the stream @$ just finish the op,byte [ ] sd = digest . digest ( ) ; digest . reset ( ) ;,'digest has data
start tx state update during pre commit <PLACE_HOLDER>,set updating tx state during pre commit ( true ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ + __str__ @$ secondary transactional operations . size ( ) ) ; },update commit phase
recursive evaluation should not trigger a suspended <PLACE_HOLDER>,context . eval ( test source ) ; suspension count . increment and get ( ) ;,evaluation trigger exception
never registered before if null ... also as of current @$ there is ever only 1 notification type per m bean @$ so we do need need a while <PLACE_HOLDER> here,if ( notifications != null ) { set < map . entry < refresh notification type @$ integer > > entries = notifications . entry set ( ) ; for ( map . entry < refresh notification type @$ integer > e : entries ) { integer timer notification id = e . get value ( ) ; if ( null != timer notification id ) { try { refresh timer . remove notification ( timer notification id ) ; } catch ( instance not found exception xptn ) { log stack trace ( level . debug @$ xptn ) ; } } } try { if ( mbean server != null && mbean server . is registered ( refresh timer object name ) ) { mbean server,need need cycle
empty if nobody has the specified global <PLACE_HOLDER>,query = permission query . builder ( ) . set organization uuid ( organization . get uuid ( ) ) . set permission ( __str__ ) . build ( ) ; expect permissions ( query @$ empty list ( ) ) ;,nobody has permission
unknown slot which basically means the exists <PLACE_HOLDER>,if ( ! ( predicates . length == __num__ && predicates [ __num__ ] instanceof exists predicate ) ) { throw new illegal state exception ( __str__ + arrays . to string ( predicates ) ) ; } return true ;,which means predicate
phantom reference does not allow <PLACE_HOLDER> to its object @$ so it is mostly useless to have a phantom reference on the image heap . but some jdk code uses it @$ e.g. @$ for marker values @$ so we can not disallow phantom reference for the image heap .,if ( receiver instanceof phantom reference ) { return null ; },reference allow access
ipv 4 occupies last two <PLACE_HOLDER>,if ( index > octets . length - __num__ || index > __num__ ) { return false ; },ipv occupies words
controller and mode events have status byte 0 x <PLACE_HOLDER> @$ where c is the channel they are sent on .,if ( ( status & __num__ ) == __num__ ) { for ( int i = __num__ ; i < count ; i ++ ) { try { ( ( controller event listener ) event info . get listener ( i ) ) . control change ( event ) ; } catch ( throwable t ) { if ( printer . err ) t . print stack trace ( ) ; } } } return ;,events have 02
set timestamp before moving to cmroot @$ so we can avoid race condition cm remove the <PLACE_HOLDER> before setting timestamp,long now = system . current time millis ( ) ; fs . set times ( path @$ now @$ - __num__ ) ; boolean success = false ; if ( fs . exists ( cm path ) && file check sum . equals ignore case ( checksum for ( cm path @$ fs ) ) ) { success = false ; } else { switch ( type ) { case move : { log . info ( __str__ @$ path . to string ( ) @$ cm path . to string ( ) ) ; success = fs . rename ( path @$ cm path ) ; break ; } case copy : { log . info ( __str__ @$ path . to string ( ) @$,cm remove entry
for mwt created core labels set <PLACE_HOLDER> of them to the character offsets of the mwt,int lastmwt char begin = - __num__ ; int lastmwt char end = - __num__ ; for ( core label cl : core labels ) { if ( sentence . mwt data . contains key ( cl . index ( ) - __num__ ) ) { if ( sentence . mwt data . get ( cl . index ( ) - __num__ ) == processedmwt tokens ) { cl . set begin position ( doc . doc text . length ( ) ) ; doc . doc text += sentence . mwt tokens . get ( processedmwt tokens ) ; cl . set end position ( doc . doc text . length ( ) ) ; lastmwt char begin = cl . begin position ( ) ; lastmwt,labels set each
if the container is running a health <PLACE_HOLDER> @$ any previous image failure has been resolved,reset image failure ( ) ; update state ( healthchecking ) ;,container running check
optimized for code which will add the same value again and again @$ more specifically this is used to add new <PLACE_HOLDER> for watcher @$ and the same watcher may watching thousands or even millions of nodes @$ which will call add the same value of this function @$ check exist using read lock will optimize the performance here .,integer bit = get bit ( value ) ; if ( bit != null ) { return bit ; } rw lock . write lock ( ) . lock ( ) ; try { bit = value2 bit . get ( value ) ; if ( bit != null ) { return bit ; } bit = freed bit set . next set bit ( __num__ ) ; if ( bit > - __num__ ) { freed bit set . clear ( bit ) ; } else { bit = next bit ++ ; } value2 bit . put ( value @$ bit ) ; bit2 value . put ( bit @$ value ) ; return bit ; } finally { rw lock . write lock ( ),which add nodes
inspect the source c es . just copy <PLACE_HOLDER> if none are modified . otherwise copy to modified c es @$ with modifications .,boolean is modified = false ; for ( int i = __num__ ; i < length ; ++ i ) { long srcce = srcc es [ src index + i ] ; long ce = modifier . modifyce ( srcce ) ; if ( ce == collation . no_ce ) { if ( is modified ) { modifiedc es [ i ] = srcce ; } } else { if ( ! is modified ) { for ( int j = __num__ ; j < i ; ++ j ) { modifiedc es [ j ] = srcc es [ src index + j ] ; } is modified = true ; } modifiedc es [ i ] = ce ; } } if ( is modified,es copy elements
mark a fragment as completing @$ but do n't actually complete it yet . the wait queue should now have <PLACE_HOLDER> to accept one more fragment .,task executor service . fragment completing ( r1 . get request id ( ) @$ scheduler fragment completing listener . state . success ) ; submission state = task executor service . schedule ( r4 ) ; assert equals ( scheduler . submission state . accepted @$ submission state ) ; assert equals ( __num__ @$ task executor service . wait queue . size ( ) ) ; assert equals ( __num__ @$ task executor service . completing fragment map . size ( ) ) ; r1 . complete ( ) ; r1 . await end ( ) ;,queue have time
ima sometimes unexpectedly decreases the ad <PLACE_HOLDER> in an ad group .,log . w ( tag @$ __str__ + ad count + __str__ + old ad count ) ;,ima decreases count
label visitor has some legacy special <PLACE_HOLDER> of output files .,if ( target instanceof output file ) { rule rule = ( ( output file ) target ) . get generating rule ( ) ; observe edge ( target @$ null @$ rule ) ; visit ( null @$ null @$ rule @$ depth + __num__ @$ count + __num__ ) ; } label visitation utils . visit target exceptionally ( target @$ edge filter @$ ( from target @$ attribute @$ to label ) -> enqueue target ( target @$ attribute @$ to label @$ depth @$ count ) ) ;,visitor has handling
if right expression is a closure expression @$ store parameter type <PLACE_HOLDER>,if ( left expression instanceof variable expression ) { if ( right expression instanceof closure expression ) { parameter [ ] parameters = ( ( closure expression ) right expression ) . get parameters ( ) ; left expression . put node meta data ( static types marker . closure_arguments @$ parameters ) ; } else if ( right expression instanceof variable expression && ( ( variable expression ) right expression ) . get accessed variable ( ) instanceof expression && ( ( expression ) ( ( variable expression ) right expression ) . get accessed variable ( ) ) . get node meta data ( static types marker . closure_arguments ) != null ) { variable target variable = find target variable ( ( variable expression ),store parameter information
actions will consume key <PLACE_HOLDER> @$ so do n't process them,if ( enable action key bindings ) { return ; },actions consume bindings
set max object size a little less than 1024 1024 @$ because the key of the segment result cache is long if set to 1024 1024 will cause memcached client exceed max size <PLACE_HOLDER>,memcached cache config . set max object size ( __num__ ) ; memcached cache config . set hosts ( config hosts ) ;,client exceed limit
some controls include <PLACE_HOLDER> based on the view dimensions @$ so update now .,update controls ( ) ; surface view sv = ( surface view ) find view by id ( r . id . hardware scaler_surface view ) ; m render thread = new render thread ( sv . get holder ( ) ) ; m render thread . set name ( __str__ ) ; m render thread . start ( ) ; m render thread . wait until ready ( ) ; render handler rh = m render thread . get handler ( ) ; if ( rh != null ) { rh . send set flat shading ( m flat shading checked ) ; rh . send surface created ( ) ; },controls include shadows
only post the callback to stop the service if the service does not have an open <PLACE_HOLDER> .,if ( m opened count . decrement and get ( ) != __num__ ) { return ; },service have connection
send message to all peers to find out who hosts the <PLACE_HOLDER>,if ( ! tx . is real deal local ( ) ) { find remotetx message reply processor processor = send find remotetx message ( server connection . get cache ( ) @$ tx id ) ; try { processor . wait for replies uninterruptibly ( ) ; } catch ( reply exception e ) { e . handle cause ( ) ; } internal distributed member hosting member = processor . get hosting member ( ) ; if ( hosting member != null ) { if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ hosting member ) ; } if ( tx . get target ( ) == null ) { tx . set target ( hosting member ),who hosts transaction
this is necessary to have the rm respect our vcore allocation <PLACE_HOLDER>,conf . set class ( capacity scheduler configuration . resource_calculator_class @$ dominant resource calculator . class @$ resource calculator . class ) ; conf . set boolean ( yarn configuration . nm_disk_health_check_enable @$ false ) ; miniyarn cluster = new miniyarn cluster ( test dynamometer infra . class . get name ( ) @$ __num__ @$ minicluster_num_nms @$ __num__ @$ __num__ ) ; miniyarn cluster . init ( conf ) ; miniyarn cluster . start ( ) ; yarn conf = miniyarn cluster . get config ( ) ; minidfs cluster = new minidfs cluster . builder ( conf ) . format ( true ) . num data nodes ( minicluster_num_dns ) . build ( ) ; minidfs cluster . wait cluster up ( ) ; file system,rm respect config
jls does not allow array <PLACE_HOLDER> to be used as bounds of type variables,return false ;,jls allow indices
grab plugin executions that are bound to the selected lifecycle phases from project . the effective model of the project already contains the plugin executions induced by the project 's packaging type . remember @$ all phases of interest and only those are in the lifecycle mapping @$ if a phase has no <PLACE_HOLDER> in the map @$ we are not interested in any,for ( plugin plugin : project . get build ( ) . get plugins ( ) ) { for ( plugin execution execution : plugin . get executions ( ) ) { if ( execution . get phase ( ) != null ) { map < integer @$ list < mojo execution > > phase bindings = mappings . get ( execution . get phase ( ) ) ; if ( phase bindings != null ) { for ( string goal : execution . get goals ( ) ) { mojo execution mojo execution = new mojo execution ( plugin @$ goal @$ execution . get id ( ) ) ; mojo execution . set lifecycle phase ( execution . get phase ( ) ) ; add mojo,phase has entry
setup filesystem and buck <PLACE_HOLDER> .,path canonical root path = project root . to real path ( ) . normalize ( ) ; immutable map < cell name @$ path > root cell mapping = get cell mapping ( canonical root path ) ; immutable list < string > args = buck args methods . expand at files ( unexpanded command line args @$ root cell mapping ) ;,setup filesystem paths
now @$ try increasing the reservation <PLACE_HOLDER> and it should fail,try { store definition defa = test utils . make store definition ( __str__ @$ __num__ ) ; bdb storage . update ( defa ) ; fail ( __str__ ) ; } catch ( storage initialization exception sie ) { },try increasing limit
iterate over alluxio <PLACE_HOLDER> and process persisted <PLACE_HOLDER> .,for ( map . entry < string @$ inode > inode entry : inode children . entry set ( ) ) { if ( ! inode entry . get value ( ) . is persisted ( ) ) { continue ; } try ( locked inode path descendant = inode path . lock descendant ( inode path . get uri ( ) . join unsafe ( inode entry . get key ( ) ) @$ lock pattern . write_edge ) ) { if ( sync descendant type != descendant type . all ) { sync descendant type = descendant type . none ; } sync result sync result = sync inode metadata ( rpc context @$ descendant @$ sync descendant type @$ status cache ) ; paths to,iterate persisted inodes
do this last because the prior operations could throw <PLACE_HOLDER> .,maybe set up metrics recorder ( context @$ configs ) ;,operations throw exceptions
here client opens initial <PLACE_HOLDER> and fetches topology . only first server in list should be contacted .,try ( grid client client = client ( ) ) { assert equals ( __num__ @$ srvs [ __num__ ] . get connect count ( ) ) ; for ( int i = __num__ ; i < client test rest server . servers_cnt ; i ++ ) assert equals ( __num__ @$ srvs [ i ] . get connect count ( ) ) ; srvs [ __num__ ] . reset counters ( ) ; int contacted srv = __num__ ; for ( int i = __num__ ; i < __num__ ; i ++ ) { int failed = contacted srv ; srvs [ failed ] . fail ( ) ; while ( true ) try { client . compute ( ) . refresh topology ( false @$ false,client opens connection
the two consume the same p <PLACE_HOLDER> and can exist in the same stage .,if ( existing consumers . stream ( ) . all match ( collection consumer -> greedyp collection fusers . is compatible ( collection consumer . consuming transform ( ) @$ new consumer . consuming transform ( ) @$ pipeline ) ) ) { existing consumers . add ( new consumer ) ; found siblings = true ; break ; },two consume data
setup the full outer intersect map join 's input obj <PLACE_HOLDER> to include the small table @$ etc .,intersect map join operator . set input obj inspectors ( intercept test desc . input object inspectors ) ;,setup map inspectors
this is taken care in wrapper which generates xni <PLACE_HOLDER> @$ there are no next events,throw new java . io . eof exception ( ) ;,which generates ns
get twitter created a flow <PLACE_HOLDER> @$ then it 's sent via s 2 s,tc . add lineage ( create lineage ( prs @$ __num__ @$ __num__ @$ __num__ ) ) ; test ( tc ) ; wait notifications get delivered ( ) ; final lineage lineage = get lineage ( ) ; final node flow = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patha = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathb = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathc = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patht = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathi = lineage . find node (,twitter created file
add the input file string and date <PLACE_HOLDER> to lists,file strings . add ( input line ) ; dt objects . add ( calculateddt ) ;,input file string
we 'll be able to remove this once we have proper <PLACE_HOLDER>d key subclasses @$ like frame key . set the new <PLACE_HOLDER> both in schema and in schema v 3 @$ so that they are always in sync .,string new type = __str__ + get keyed class type ( ) + __str__ ; __meta . schema_type = new type ; set schema type_do not call ( new type ) ;,3 set type
ideally @$ we would just install the next phase using the singleton policy @$ however <PLACE_HOLDER> unit phases do not currently support restarts restart the <PLACE_HOLDER> using the attached phase builder @$ but only if a builder was not already attached,if ( unit . put attachment ( attachments . deployment_unit_phase_builder @$ new singleton deployment unit phase builder ( support @$ policy ) ) == null ) { singleton logger . root_logger . singleton deployment detected ( policy ) ; service controller < ? > controller = context . get service registry ( ) . get required service ( unit . get service name ( ) ) ; controller . add listener ( this ) ; controller . set mode ( mode . never ) ; },restarts restart deployment
if the request does n't have a session <PLACE_HOLDER> @$ we 're not going to renew one .,if ( ! request . get cookies ( ) . contains key ( session cookie name ) ) { return ; } cookie request cookie = request . get cookies ( ) . get ( session cookie name ) ; optional < user > optional user = authenticator . authenticate ( request cookie ) ; if ( optional user . is present ( ) ) { session login resource . cookies for user ( optional user . get ( ) ) . for each ( c -> response . get headers ( ) . add ( http headers . set_cookie @$ c ) ) ; },request have cookie
none of the results got the data form the <PLACE_HOLDER> .,blocks . add ( no_data ) ;,data form source
create a role filesystem which does not have read <PLACE_HOLDER> under a path it still has write <PLACE_HOLDER> @$ which can be explored in the final step to delete files and directories .,role config = create assumed role config ( ) ; bind role policy statements ( role config @$ statement_s3guard_client @$ statement_allow_sse_kms_rw @$ statement ( true @$ s3_all_buckets @$ s3_all_operations ) @$ new statement ( effects . deny ) . add actions ( s3_all_get ) . add resources ( directory ( no read dir ) ) ) ; readonlyfs = ( s3a file system ) base path . get file system ( role config ) ; verifys3 guard settings ( readonlyfs @$ __str__ ) ;,which read permissions
we set <PLACE_HOLDER> to empty here explicitly @$ so that the lookup will succeed as user does n't know the exact <PLACE_HOLDER> .,for ( member response member response : response . member responses ( ) ) { member errors . put ( new member identity ( ) . set member id ( join group request . unknown_member_id ) . set group instance id ( member response . group instance id ( ) ) @$ errors . for code ( member response . error code ( ) ) ) ; },user know errors
both came from db @$ no <PLACE_HOLDER> to run this .,return ;,both came need
most commands use the object <PLACE_HOLDER> for p 2 @$ nut not all,int param2 = config . get device ( ) . get properties ( ) . get number ( ) ; logger . debug ( __str__ + config . get object type ( ) ) ; switch ( config . get object type ( ) ) { case unit : { unit unit = ( unit ) config . get device ( ) ; boolean resend to children = false ; if ( command == on off type . on ) { cmd = omni link cmd . cmd_unit_on . get number ( ) ; } else if ( command == on off type . off ) { cmd = omni link cmd . cmd_unit_off . get number ( ) ; } else if ( command == increase decrease,commands use type
run the k means <PLACE_HOLDER>,path answer = new path ( output @$ __str__ ) ; k means driver . run ( conf @$ data @$ initialclusters @$ answer @$ convergence delta @$ max iterations @$ true @$ __num__ @$ false ) ;,k means job
empty string signifies broadest possible <PLACE_HOLDER>,owner . set last name ( __str__ ) ;,string signifies replay
changing the display name has surely brought a change in the order as well so let 's tell the <PLACE_HOLDER>,fire meta contact group event ( find parent meta contact group ( meta contact ) @$ null @$ null @$ meta contact group event . child_contacts_reordered ) ;,'s tell others
start data change <PLACE_HOLDER> from several threads .,final atomic boolean stopped = new atomic boolean ( ) ; ignite internal future update fut = multithreaded async ( new callable < void > ( ) { @ override public void call ( ) throws exception { while ( ! stopped . get ( ) ) { ignite node = grid ( thread local random . current ( ) . next int ( __num__ @$ __num__ ) ) ; int key = thread local random . current ( ) . next int ( __num__ @$ large_cache_size ) ; int val = thread local random . current ( ) . next int ( ) ; binary object key obj = key ( node @$ key ) ; if ( thread local random . current ( ) . next,data change thread
creating the handler starts the preview @$ which can also throw a runtime <PLACE_HOLDER> .,if ( handler == null ) { handler = new capture activity handler ( this @$ decode formats @$ decode hints @$ character set @$ camera manager ) ; } decode or store saved bitmap ( null @$ null ) ;,which throw exception
return false because function types do not have a null <PLACE_HOLDER>,if ( definition argument properties . get ( i ) . get argument type ( ) == function_type ) { if ( invocation argument convention != invocation argument convention . function ) { return false ; } throw new unsupported operation exception ( __str__ ) ; },types have argument
first see if the file matches the regular <PLACE_HOLDER> !,if ( pattern source != null ) { matcher matcher = pattern source . matcher ( filename ) ; unzip = matcher . matches ( ) ; } if ( unzip ) { if ( ! unzip file ( children [ i ] @$ real targetdirectory @$ real wildcard @$ real wildcard exclude @$ result @$ parent job @$ movetodir @$ real movetodirectory ) ) { update errors ( ) ; } else { update success ( ) ; } },file matches expression
manager wants this <PLACE_HOLDER> to be paused,if ( get state ( ) == generic fragment . state_paused ) { return ; } if ( util . sdk_int > __num__ ) { perform initialization ( ) ; },manager wants fragment
js 2 e does not throw this <PLACE_HOLDER>,key pair gen . initialize ( __num__ @$ new secure random ( ) ) ; try { key pair gen . initialize ( __num__ @$ new secure random ( ) ) ; fail ( __str__ ) ; } catch ( invalid parameter exception e ) { } key pair gen . initialize ( __num__ @$ null ) ; assert null ( __str__ @$ key pair gen . generate key pair ( ) ) ; assert null ( __str__ @$ key pair gen . gen key pair ( ) ) ; break ; case __num__ : key pair gen . initialize ( pp @$ new secure random ( ) ) ; key pair gen . initialize ( pp ) ; key pair gen . initialize ( __num__ @$ new,e throw exception
exclusion only makes <PLACE_HOLDER> for a union type .,if ( outcome . boolean values == boolean literal set . empty && get native type ( boolean_type ) . is subtype of ( type ) ) { if ( type . is union type ( ) ) { type = type . to maybe union type ( ) . get restricted union ( get native type ( boolean_type ) ) ; } },exclusion makes sense
the page supplier has incremented the page <PLACE_HOLDER> count @$ and add pages below also increments the <PLACE_HOLDER> count @$ so we need to drop the page supplier <PLACE_HOLDER> . the call de<PLACE_HOLDER> page is performed outside of synchronized to avoid making a callback while holding a lock .,page references = pages supplier . get pages ( max size ) ;,supplier incremented reference
deregister as a listener to reduce computational <PLACE_HOLDER>,remove event listener ( ) ;,listener reduce thread
2 nd time launch service to handle if service exist <PLACE_HOLDER>,system service . launch user service ( user services ) ; verify for launched user services ( ) ;,service exist ok
before inlining happens remove unused code sees one use of inner c @$ which prevents its <PLACE_HOLDER> . after inlining it sees ` this instanceof inner c ` as the only use of inner c. make sure remove unused code recognizes that the value of inner c escapes .,test ( options @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,which prevents removal
first handle special <PLACE_HOLDER> . if one of the special case methods can not handle it @$ it returns null .,vector expression ve = null ; if ( udf instanceof genericudf between ) { ve = get between expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf in ) { ve = get in expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf if ) { ve = get if expression ( ( genericudf if ) udf @$ child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudf when ) { ve = get when expression ( child expr @$ mode @$ return type ) ; } else if ( udf instanceof genericudfop positive ) { ve = get identity expression ( child expr,one handle cases
each polling event will trigger a <PLACE_HOLDER> and comparison of config tree,return optional . of ( instant . now ( ) ) ;,event trigger comparison
by default @$ let 's just shift the <PLACE_HOLDER> offset .,do { final int x change = mx offset direction * burn_in_shift_step ; m last burn inx offset += x change ; if ( m last burn inx offset > m max horizontal burn in offset || m last burn inx offset < m min horizontal burn in offset ) { m last burn inx offset -= x change ; mx offset direction *= - __num__ ; final int y change = my offset direction * burn_in_shift_step ; m last burn iny offset += y change ; if ( m last burn iny offset > m max vertical burn in offset || m last burn iny offset < m min vertical burn in offset ) { m last burn iny offset -= y change ; my offset direction,let shift right
default starts one <PLACE_HOLDER> only .,test_util . get configuration ( ) . set boolean ( load balancer . tables_on_master @$ true ) ;,default starts shard
the token has now changed <PLACE_HOLDER> to having all windows shown ... what to do @$ what to do ?,if ( m freezing screen ) { show all windows locked ( ) ; stop freezing screen ( false @$ true ) ; if ( debug_orientation ) slog . i ( tag @$ __str__ + this + __str__ + m num interesting windows + __str__ + m num drawn windows ) ; set app layout changes ( finish_layout_redo_wallpaper @$ __str__ ) ; } else { set app layout changes ( finish_layout_redo_anim @$ __str__ ) ; if ( ! get display content ( ) . m opening apps . contains ( this ) && can show windows ( ) ) { show all windows locked ( ) ; } },token changed everything
now test <PLACE_HOLDER> should contain only old junit <PLACE_HOLDER> .,test classes . remove all ( new junit classes ) ; throw new j unit exception ( format ( __str__ + __str__ + __str__ @$ test clazz . get name ( ) @$ sorted classes ( new junit classes ) @$ sorted classes ( test classes ) ) ) ;,classes contain classes
this flow covers instance not found exception . actual <PLACE_HOLDER> just eating the exception . i.e actual <PLACE_HOLDER> just printing the stacktrace @$ whenever an exception of type instance not found exception occurs .,mbean container . destroy ( ) ;,exception found code
0 : id 1 : indexed string add <PLACE_HOLDER>,db . execsql ( __str__ + constraint + __str__ + __str__ ) ;,string add row
if this is a call to ya cys special search <PLACE_HOLDER> @$ enhance the query with field assignments,if ( ( response writer instanceof y json response writer || response writer instanceof opensearch response writer ) && __str__ . equals ( mmsp . get ( __str__ @$ __str__ ) ) ) { if ( ! mmsp . get map ( ) . contains key ( __str__ ) ) mmsp . get map ( ) . put ( __str__ @$ new string [ ] { q } ) ; if ( ! mmsp . get map ( ) . contains key ( __str__ ) ) mmsp . get map ( ) . put ( __str__ @$ new string [ ] { collection schema . description_txt . get solr field name ( ) + __str__ + collection schema . h4_txt . get solr field name ( ) +,ya cys service
the bucket move will send a destroy region <PLACE_HOLDER> .,return new distribution message observer ( ) { private volatile boolean done ; @ override public void before send message ( cluster distribution manager dm @$ distribution message message ) { if ( message instanceof destroy region message && ! done ) { task . run ( ) ; done = true ; } } } ;,move send message
test that the from row function simply returns the original <PLACE_HOLDER> back .,simplepojo extracted = registry . get from row function ( simplepojo . class ) . apply ( row ) ; assert same ( pojo @$ extracted ) ;,the returns object
loading child elements modifies the <PLACE_HOLDER> of the attribute set 's underlying parser @$ so it needs to happen after obtaining attributes and extracting <PLACE_HOLDER>s .,if ( dr == null ) { int type ; while ( ( type = parser . next ( ) ) == xml pull parser . text ) { } if ( type != xml pull parser . start_tag ) { throw new xml pull parser exception ( parser . get position description ( ) + item_missing_drawable_error ) ; } if ( parser . get name ( ) . equals ( __str__ ) ) { dr = vector drawable compat . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else if ( sdk_int >= lollipop ) { dr = drawable . create from xml inner ( resources @$ parser @$ attrs @$ theme ) ; } else { dr = drawable,elements modifies state
jump to second write back if not equal . reserve <PLACE_HOLDER> for the two jcc and first write back when calculating target address .,final long second write back offset = instructions . size ( ) + first write back . size ( ) + __num__ ; final string second write back goal = string . format ( __str__ @$ instruction . get address ( ) . to long ( ) @$ second write back offset ) ; instructions . add ( reil helpers . create jcc ( base offset + instructions . size ( ) @$ current size @$ comparison result @$ operand size . address @$ second write back goal ) ) ;,jump write space
now run the k means <PLACE_HOLDER>,fuzzyk means driver . run ( testdata @$ new path ( output @$ __str__ ) @$ fuzzyk means output @$ __num__ @$ __num__ @$ __num__ @$ true @$ true @$ __num__ @$ true ) ; int num iterations = __num__ ; path clusters in = new path ( fuzzyk means output @$ __str__ ) ; representative points driver . run ( conf @$ clusters in @$ new path ( fuzzyk means output @$ __str__ ) @$ fuzzyk means output @$ measure @$ num iterations @$ true ) ; representative points driver . print representative points ( fuzzyk means output @$ num iterations ) ; cluster evaluator evaluator = new cluster evaluator ( conf @$ clusters in ) ;,k means job
when filtering @$ let the server decide the <PLACE_HOLDER> unless we 've set the filter to empty and explicitly said that we want to see the results starting from <PLACE_HOLDER> 0 .,if ( ! filter . equals ( last filter ) ) { if ( filter . is empty ( ) && page != __num__ ) { page = - __num__ ; } else { page = __num__ ; } },server decide page
first assume there are no named capture backreferences @$ because the spec says they should only be recognized if the pattern contains at least <PLACE_HOLDER> named capture group .,class parser { int pos ; int num capturing groups ; set < string > capturing group names = new hash set < > ( ) ; final int limit = pattern . length ( ) ; boolean look for named capture backreferences ; reg exp tree parse top level ( ) { this . pos = __num__ ; this . num capturing groups = __num__ ; this . look for named capture backreferences = false ; reg exp tree out = parse ( ) ; if ( ! capturing group names . is empty ( ) ) { this . pos = __num__ ; this . num capturing groups = __num__ ; this . look for named capture backreferences = true ; out = parse ( ),pattern contains one
test file does n't have an ec <PLACE_HOLDER>,final path file = new path ( dir @$ __str__ ) ; fs . create ( file ) . close ( ) ; assert null ( client . get file info ( file . to string ( ) ) . get erasure coding policy ( ) ) ; contract test utils . assert not erasure coded ( fs @$ file ) ; fs . delete ( file @$ true ) ; final erasure coding policy ec policy1 = get ec policy ( ) ;,file have policy
if component metrics @$ no more information required from tag so break the <PLACE_HOLDER>,if ( tag . name ( ) . equals ( __str__ ) ) { app id = tag . value ( ) ; },metrics break loop
if there are quotes @$ build a <PLACE_HOLDER> wide list,if ( quote annotator . gather quotes ( this . annotation document ) != null ) build document quotes list ( ) ;,list build document
if any the fields of struct are representing <PLACE_HOLDER> @$ then return true,for ( int i = __num__ ; i < struct fields . size ( ) ; i ++ ) { if ( has any null object ( soi . get struct field data ( o @$ struct fields . get ( i ) ) @$ struct fields . get ( i ) . get field object inspector ( ) ) ) { return true ; } } return false ;,fields representing null
the sdk requires an unbounded thread <PLACE_HOLDER> because a step may create x writers each requiring their own thread to perform the writes otherwise a writer may block causing deadlock for the step because the writers buffer is full . also @$ the map task executor launches the steps in reverse order and completes them in forward order thus requiring enough threads so that,return new thread pool executor ( __num__ @$ integer . max_value @$ long . max_value @$ time unit . nanoseconds @$ new synchronous queue < > ( ) @$ thread factory builder . build ( ) ) ;,sdk requires limit
if this next line runs a <PLACE_HOLDER> @$ it fails,lambda l = dummy method to make check style happy ( static fail if clinit runs :: static method ) ; try { l . run ( __num__ @$ __num__ ) ; fail ( __str__ ) ; } catch ( assertion error ae ) { },line runs clinit
fourth try @$ calling stop build with a parameter not matching build <PLACE_HOLDER> does n't interrupt the build reset semaphore and result code,semaphore . drain permits ( ) ; atomic result . set ( __num__ ) ; queue task future < free style build > future build = p . schedule build2 ( __num__ ) ; future build . wait for start ( ) ; web request request = new web request ( new url ( j . geturl ( ) + __str__ ) @$ http method . post ) ; page page = wc . get page ( request ) ; assert equals ( __num__ @$ page . get web response ( ) . get status code ( ) ) ; assert request was not blocked ( ) ;,the build properties
interface has no <PLACE_HOLDER> and is therefore not handled as pojo,if ( modifier . is interface ( clazz . get modifiers ( ) ) ) { return new generic type info < out > ( clazz ) ; },interface has modifiers
the final part of this check is to verify that the <PLACE_HOLDER> does actually indicate a <PLACE_HOLDER> in user .,try { cached security context = security context association . get security context ( ) ; final security context next context = security context factory . create security context ( desired user @$ new current user credential ( connection user . get name ( ) ) @$ new subject ( ) @$ __str__ ) ; security context association . set security context ( next context ) ; context set = true ; remoting context . clear ( ) ; } catch ( exception e ) { logger . error ( __str__ @$ e ) ; throw new ejb access exception ( __str__ ) ; },change indicate user
sorting on due date asc should put the <PLACE_HOLDER> at the end,list < task > tasks = task service . create task query ( ) . order by due date nulls last ( ) . asc ( ) . list ( ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { assert not null ( tasks . get ( i ) . get due date ( ) ) ; } assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get ( __num__ ) . get name ( ) ) ; assert equals ( __str__ @$ tasks . get,sorting put nulls
we start out by loading an initial configuration where we started to write a task update @$ and then compaction cleaned up the earlier <PLACE_HOLDER> .,expect configure ( ) ; list < consumer record < string @$ byte [ ] > > existing records = arrays . as list ( new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$ __num__ @$ connector_config_keys . get ( __num__ ) @$ configs_serialized . get ( __num__ ) ) @$ new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$ __num__ @$ task_config_keys . get ( __num__ ) @$ configs_serialized . get ( __num__ ) ) @$ new consumer record < > ( topic @$ __num__ @$ __num__ @$ __num__ @$ timestamp type . create_time @$ __num__ @$ __num__ @$,compaction cleaned puts
ensure that really late tasks do n't completely saturate the <PLACE_HOLDER> of the task queue,if ( late > period ) { period = __num__ ; } else if ( late > __num__ ) { period -= late ; },tasks saturate rest
subclasses like forall not node might override this <PLACE_HOLDER>,insert child left tuple ( sink @$ trg left tuples @$ left tuple @$ right tuple . get propagation context ( ) @$ true ) ;,subclasses override method
active user count has to decrease from queue 2 due to app has no pending <PLACE_HOLDER>,assert equals ( __num__ @$ queue2 . get abstract users manager ( ) . get num active users ( ) ) ;,count has requests
adapterview does not support click <PLACE_HOLDER>,if ( view instanceof adapter view ) { return ; } view . set on click listener ( new view . on click listener ( ) { @ override public void on click ( view v ) { if ( on click listener == null ) { return ; } on click listener . on click ( dialog plus . this @$ v ) ; } } ) ;,adapterview support listener
if inheriting all <PLACE_HOLDER> then trans executor can initialize <PLACE_HOLDER> from parent trans,assert . assert equals ( parent value @$ internal trans . get variable ( variable name ) ) ;,executor initialize variables
both ruby should pass : ruby.jar has no jdk <PLACE_HOLDER>,collection < component info > cat infos = contents . load components ( __str__ @$ m @$ false ) ; assert equals ( __num__ @$ cat infos . size ( ) ) ; set < url > urls = new hash set < > ( arrays . as list ( test data . resolve ( __str__ ) . to uri ( ) . tourl ( ) @$ test data . resolve ( __str__ ) . to uri ( ) . tourl ( ) ) ) ; iterator < component info > itc = cat infos . iterator ( ) ; component info ci = itc . next ( ) ; assert true ( urls . remove ( ci . get remoteurl ( ) ) ) ; ci =,ruby.jar has dependencies
zap : added the <PLACE_HOLDER> .,extension hook . get hook view ( ) . add option panel ( get options database panel ( ) ) ; extension hook . get hook view ( ) . add option panel ( get options jvm panel ( ) ) ;,zap added listener
unknown backend should get default <PLACE_HOLDER> of registry,thread pool bulkhead bulkhead3 = bulkhead registry . bulkhead ( __str__ ) ; assert that ( bulkhead3 ) . is not null ( ) ; assert that ( bulkhead3 . get bulkhead config ( ) . get core thread pool size ( ) ) . is equal to ( __num__ ) ; assert that ( event consumer registry . get all event consumer ( ) ) . has size ( __num__ ) ;,backend get config
verify that the read delta call will only return deltas when the previous call had null <PLACE_HOLDER> .,m callback . clear ( ) ; final long [ ] [ ] new times3 = increase time ( new times2 ) ; write to file ( m headline + uid lines ( m uids @$ new times3 ) ) ; m reader . read delta ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; ++ i ) { m callback . verify ( m uids [ i ] @$ get active time ( new times3 [ i ] ) - get active time ( new times2 [ i ] ) ) ; } m callback . verify no more interactions ( ) ; assert true ( m test file . delete ( ) ) ;,call had callback
if we have a constraints changed intent in the queue do n't add a second <PLACE_HOLDER> . we are treating this intent as special because every time a worker with constraints is complete it kicks off an update for constraint proxies .,if ( command handler . action_constraints_changed . equals ( action ) && has intent with action ( command handler . action_constraints_changed ) ) { return false ; } intent . put extra ( key_start_id @$ start id ) ; synchronized ( m intents ) { boolean has commands = ! m intents . is empty ( ) ; m intents . add ( intent ) ; if ( ! has commands ) { process command ( ) ; } },intent add one
the following will change the <PLACE_HOLDER> and feel of the toolbar to match the current design,m recycler view . set toolbar background color ( context compat . get color ( context @$ r . color . primary ) ) ; m recycler view . set toolbar spinner text color ( context compat . get color ( context @$ android . r . color . white ) ) ; m recycler view . set toolbar spinner drawable ( r . drawable . ic_dropdown_primary_30_24dp ) ; if ( build config . information_architecture_available && m is top level ) { m recycler view . set toolbar title ( r . string . reader_screen_title @$ get resources ( ) . get dimension pixel size ( r . dimen . margin_extra_large ) ) ; } else { m recycler view . set toolbar left and right padding (,following change look
the keys of probe and build sides are overlapped @$ so there would be none unmatched build elements after probe phase @$ make sure build side outer join <PLACE_HOLDER> well in this case .,final int probe vals per key = __num__ ;,side join matchs
no intersection between clip area and segment computed <PLACE_HOLDER> : bottom left,clippable . init ( ) ; segment clipper . clip ( - __num__ @$ - __num__ @$ __num__ @$ __num__ ) ; assert . assert equals ( __num__ @$ points . size ( ) ) ;,intersection computed corner
no need to reset the <PLACE_HOLDER> ; we 're exiting,if ( _is stopped ) { break ; } logger . warn ( __str__ @$ e ) ; break ;,need reset bit
if invalid label exception is received means the requested label doesnt have <PLACE_HOLDER> so killing job in this case .,string diag msg = __str__ + string utils . stringify exception ( e ) ; log . info ( diag msg ) ; job id job id = this . get job ( ) . getid ( ) ; event handler . handle ( new job diagnostics update event ( job id @$ diag msg ) ) ; event handler . handle ( new job event ( job id @$ job event type . job_kill ) ) ; throw e ;,doesnt have feature
check if the current node has a <PLACE_HOLDER> of the appropriate type if it does : increment the count and proceed otherwise : create a new node @$ and to map and set as a <PLACE_HOLDER>,g tree node current node child = current node child map . get ( key ) ; if ( current node child == null ) { current node child = new function bit patternsg tree node ( key @$ current seq . get instructions ( ) [ level ] @$ current seq . get sizes ( ) [ level ] ) ; ( ( function bit patternsg tree node ) current node child ) . increment count ( __num__ ) ; current node child map . put ( key @$ current node child ) ; current node . add node ( current node child ) ; } else { ( ( function bit patternsg tree node ) current node child ) . increment count ( __num__ ) ;,node has child
verify script listener has done its <PLACE_HOLDER> @$ on create before flowable entity event was fired,assert equals ( __str__ @$ __str__ @$ task from event . get assignee ( ) ) ; assert equals ( __str__ @$ __num__ @$ task from event . get priority ( ) ) ;,listener done job
for bootstrap class <PLACE_HOLDER> bean deployment archive always use the parent resource <PLACE_HOLDER>,if ( module == null ) { new bda services . add ( resource loader . class @$ service registry . get ( resource loader . class ) ) ; },archive use loader
no handlers left so close the actual <PLACE_HOLDER> the done handler needs to be executed on the context that calls close @$ not the context of the actual <PLACE_HOLDER>,actual server . actual close ( completion ) ;,handlers left server
total block count fragment <PLACE_HOLDER> in bytes .,return sb . f_blocks * sb . f_frsize ;,block count size
do delay send coin <PLACE_HOLDER> .,long create account fee = __num__ ; logger . info ( __str__ ) ;,delay send transaction
user can override <PLACE_HOLDER> of spark clone configuration in hive config to true,conf . set ( spark clone configuration @$ __str__ ) ; check spark conf ( conf @$ spark clone configuration @$ __str__ ) ;,user override value
fallback to maven manifest <PLACE_HOLDER>,hash map < string @$ string > maven versions = get maven versions ( ) ; string [ ] maven libs = { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; for ( string lib : maven libs ) { lib versions . put ( lib @$ maven versions . get ( lib ) ) ; } return lib versions ;,fallback manifest version
if list <PLACE_HOLDER> exceeds pref <PLACE_HOLDER> @$ close connection,if ( closed || ( pref size > __num__ && conns . size ( ) > pref size ) ) { d ( __str__ @$ conn ) ; td ( __str__ @$ conn ) ; conns . remove ( entry ) ; conn . close connection ( ) ; } else { d ( __str__ @$ conn ) ; td ( __str__ @$ conn ) ; entry = conns . get ( loc ) ; entry . release ( ) ; },size exceeds size
abort : option to queue a batch with a query which can cause the <PLACE_HOLDER> to abort .,boolean has abort = arg . contains ( __str__ ) ;,which cause query
already scrolling to the correct page @$ but not yet there . only handle instant <PLACE_HOLDER> because then we need to interrupt the current smooth scroll .,if ( item == m current item && smooth scroll ) { return ; },page handle writes
if cr<PLACE_HOLDER> just keep <PLACE_HOLDER>,if ( c == __str__ ) { c = read ( reader ) ; if ( c != __str__ ) { last read = c ; use last read = true ; c = __str__ ; } },crlf keep row
this property access would be an unknown property error unless the polymer pass had successfully parsed the element <PLACE_HOLDER> .,compiler compiler = compile ( options @$ new string [ ] { lines ( __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) } ) ;,pass parsed definition
a client will merge databases if the <PLACE_HOLDER> of databases exceeds the maximum <PLACE_HOLDER> per client times the amount of clients,int max database files = options . get max database files ( ) * all database files map . key set ( ) . size ( ) ; boolean too many database files = number of database files > max database files ; boolean removed old versions = result . get removed old versions count ( ) > __num__ ; return removed old versions || too many database files || options . is force ( ) ;,number exceeds number
h base connector does n't support local date time use <PLACE_HOLDER> as conversion class for now .,for ( int j = __num__ ; j < family type . get arity ( ) ; j ++ ) { class clazz = qualifier types [ j ] . get type class ( ) ; if ( local date time . class . equals ( clazz ) ) { clazz = timestamp . class ; } else if ( local date . class . equals ( clazz ) ) { clazz = date . class ; } else if ( local time . class . equals ( clazz ) ) { clazz = time . class ; } hbase schema . add column ( name @$ qualifier names [ j ] @$ clazz ) ; },connector support introspection
nodes without outgoing edges into the subgraph formed by the selected nodes are unselected . those are just the nodes which have no selected <PLACE_HOLDER> .,graph . iterate selected ( new i node callback < node type > ( ) { @ override public iteration mode next ( final node type node ) { if ( are all children deselected ( node ) ) { if ( ! deselect . contains ( node ) ) { deselect . add ( node ) ; } } return iteration mode . continue ; } } ) ;,which have children
more methods than advertised @$ either broken request or not actually socks 5 do this <PLACE_HOLDER> last so that any waiting for data is already done,return actual readable bytes == ( __num__ + number of authentication methods ) ;,methods do last
we loaded plain <PLACE_HOLDER> and the caller wants styled <PLACE_HOLDER> @$ that is all we have so return it .,if ( styled ) { return text ; } else { return html . escape html ( text ) ; },text wants text
a media period may report a discontinuity at the current playback position to ensure the renderers are flushed . only report the discontinuity <PLACE_HOLDER> if the position changed .,if ( period position us != playback info . position us ) { playback info = playback info . copy with new position ( playback info . period id @$ period position us @$ playback info . content position us @$ get total buffered duration us ( ) ) ; playback info update . set position discontinuity ( player . discontinuity_reason_internal ) ; } renderer position us = media clock . sync and get position us ( ) ; period position us = playing period holder . to period time ( renderer position us ) ; maybe trigger pending messages ( playback info . position us @$ period position us ) ; playback info . position us = period position us ;,period report when
clear res monitor <PLACE_HOLDER>,res monitor work = null ; m_periodic works . clear ( ) ; m_snapshot completion monitor . shutdown ( ) ; m_periodic work thread . shutdown ( ) ; m_periodic work thread . await termination ( __num__ @$ time unit . days ) ; m_periodic priority work thread . shutdown ( ) ; m_periodic priority work thread . await termination ( __num__ @$ time unit . days ) ; if ( m_elastic service != null ) { m_elastic service . shutdown ( ) ; } if ( m_leader appointer != null ) { m_leader appointer . shutdown ( ) ; } m_global service elector . shutdown ( ) ; if ( m_has started sampler . get ( ) ) { m_sampler . set should stop ( ) ;,res monitor work
a volt db extension to support limit partition rows <PLACE_HOLDER>,case tokens . limit : read ( ) ; process alter table add limit constraint ( t @$ cname ) ; return ;,extension support syntax
failed attempt to renew the seed does not have any <PLACE_HOLDER>,assert user connected ( wc @$ alice ) ; user seed property user seed property = alice . get property ( user seed property . class ) ; user seed property . renew seed ( ) ;,attempt have effect
the order of tasks below is waiting @$ pending @$ running to prevent skipping a task @$ it 's the order in which tasks will change <PLACE_HOLDER> if they do while this is code is executing @$ so a task might be counted twice but never skipped,if ( runner task state . waiting . equals ( state ) ) { collection < ? extends task runner work item > runners known tasks = runner . get known tasks ( ) ; set < string > runner known task ids = runners known tasks . stream ( ) . map ( task runner work item :: get task id ) . collect ( collectors . to set ( ) ) ; final list < any task > waiting tasks = new array list < > ( ) ; for ( task runner work item task : all tasks ) { if ( ! runner known task ids . contains ( task . get task id ( ) ) ) { waiting tasks . add (,tasks change them
note : do n't use schema <PLACE_HOLDER> internally we will get stack overflaw because xml schema validator will be instantiating xsd handler ...,f schema grammar description = new xsd description ( ) ;,note use validator
if this rel references cor <PLACE_HOLDER> and now it needs to be rewritten it must have been pulled above the correlate replace the input ref to account for the lhs of the correlate,if ( current rel instanceof logical correlate ) { final int left input field count = ( ( logical correlate ) current rel ) . get left ( ) . get row type ( ) . get field count ( ) ; rel data type new type = input ref . get type ( ) ; if ( project pulled above left correlator ) { new type = type factory . create type with nullability ( new type @$ true ) ; } int pos = input ref . get index ( ) ; rex input ref new input ref = new rex input ref ( left input field count + pos @$ new type ) ; if ( ( is count != null ) && is count,references cor var
all values have the same <PLACE_HOLDER> so they all appear as a single output element,return input . apply ( reify . windows ( ) ) . apply ( with keys . < integer @$ value in single window < t > > of ( __num__ ) . with key type ( new type descriptor < integer > ( ) { } ) ) . apply ( window . into ( new identity window fn < kv < integer @$ value in single window < t > > > ( original window fn . window coder ( ) ) ) . triggering ( never . ever ( ) ) . with allowed lateness ( input . get windowing strategy ( ) . get allowed lateness ( ) ) . discarding fired panes ( ) ) . apply ( group by key . create,values have type
some cases use this absolute <PLACE_HOLDER>,if ( fs != null ) { if ( root dir test enabled ( ) ) { cleanup dir ( path ( __str__ ) ) ; } cleanup dir ( get test base dir ( ) ) ; },cases use path
the compressed stream should reset before compressing stream reset state since in gz reset statue will write <PLACE_HOLDER> in the outputstream .,compressed stream . reset ( ) ; compressing stream . reset state ( ) ; compressing stream . write ( buffer @$ offset @$ length ) ; compressing stream . flush ( ) ; compressed stream . to byte array ( ) ; final long finish time = system . nano time ( ) ;,statue write bytes
deselect will fire the <PLACE_HOLDER>,if ( item ids != null ) { return deselect ( arrays . as list ( item ids ) ) ; } else { throw new illegal argument exception ( __str__ ) ; },deselect fire event
user ca n't edit the <PLACE_HOLDER> .,return false ;,user edit line
create a table which will throw <PLACE_HOLDER> for requests,table name table name = table name . value of ( name . get method name ( ) ) ; h table descriptor table desc = new h table descriptor ( table name ) ; table desc . add coprocessor ( error throwing get observer . class . get name ( ) ) ; table desc . add family ( new h column descriptor ( family ) ) ; table table = util . create table ( table desc @$ null ) ; table . put ( new put ( rowkey ) . add column ( family @$ col @$ bytes . to bytes ( __str__ ) ) ) ; thrifth base service handler hbase handler = create handler ( ) ; thrift metrics metrics = get metrics,which throw exceptions
ok so apparently the index has a node <PLACE_HOLDER> outside node high id,reporter . for index entry ( new index entry ( descriptor @$ context . token name lookup @$ entity id ) ) . node not in use ( record loader . node ( entity id ) ) ;,index has id
developer is explicitly using the dark <PLACE_HOLDER> .,if ( args . get boolean ( base dialog builder . arg_use_dark_theme ) ) { use light theme = false ; } else if ( args . get boolean ( base dialog builder . arg_use_light_theme ) ) { use light theme = true ; },developer using theme
ensure that this real user even has kerberos <PLACE_HOLDER> :,user real user = proxy user provider . get user ( ) ; kerberos principal real principal = real user . get kerberos principal ( ) ; if ( real principal == null ) { throw new es hadoop illegal argument exception ( __str__ + real user . get user name ( ) + __str__ + run as user + __str__ ) ; } if ( log . is debug enabled ( ) ) { log . debug ( __str__ + real user . get user name ( ) + __str__ + run as user + __str__ ) ; } credential user provider = proxy user provider ;,user has principal
if the regular output and reprocess output have the same <PLACE_HOLDER> and format @$ they can share one image reader .,if ( input format == reprocess output format && input size . equals ( reprocess output size ) ) { max images *= __num__ ; m share one image reader = true ; },output have size
user 2 joins the new <PLACE_HOLDER>,try { multi user chat muc2 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc2 . join ( __str__ ) ; muc2 . add user status listener ( new default user status listener ( ) { public void kicked ( string actor @$ string reason ) { super . kicked ( actor @$ reason ) ; answer [ __num__ ] = actor ; answer [ __num__ ] = reason ; } } ) ; multi user chat muc3 = new multi user chat ( get connection ( __num__ ) @$ room ) ; muc3 . join ( __str__ ) ; muc3 . add participant status listener ( new default participant status listener ( ) { public void kicked ( string participant,user joins room
rows with ` technology ` have ` <PLACE_HOLDER> ` in the quality numeric string field,regex filtered dimension spec regex spec = new regex filtered dimension spec ( new default dimension spec ( __str__ @$ __str__ @$ value type . long ) @$ __str__ ) ; list filtered dimension spec list filtered spec = new list filtered dimension spec ( new default dimension spec ( __str__ @$ __str__ @$ value type . float ) @$ sets . new hash set ( __str__ ) @$ true ) ; group by query query = make query builder ( ) . set data source ( query runner test helper . data_source ) . set query segment spec ( query runner test helper . first_to_third ) . set dimensions ( regex spec @$ list filtered spec ) . set dim filter ( new in dim filter (,technology have quality
this code uses a calendar <PLACE_HOLDER> to create a unique name and description for test purposes so that you can easily upload multiple files . you should remove this code from your project and use your own standard names instead .,calendar cal = calendar . get instance ( ) ; snippet . set title ( __str__ + cal . get time ( ) ) ; snippet . set description ( __str__ + __str__ + cal . get time ( ) ) ;,code uses instance
in command line . stop jmx <PLACE_HOLDER> and then start it again with different property values stop jmx <PLACE_HOLDER> again and then start it without property value make sure these properties overridden corectly,system . out . println ( __str__ ) ; something s = do something ( __str__ @$ __str__ + port1 @$ __str__ @$ __str__ ) ; try { test no connect ( port1 ) ; jcmd ( cmd_stop ) ; jcmd ( cmd_start @$ __str__ @$ __str__ + port1 ) ; test connect ( port1 ) ; jcmd ( cmd_stop ) ; jcmd ( cmd_start @$ __str__ + port1 ) ; test no connect ( port1 ) ; } finally { s . stop ( ) ; },overridden stop connection
first things first : let 's flush the buffer to get some more <PLACE_HOLDER>,_flush buffer ( ) ;,buffer get data
insert dummy dimension so all subtotals queries have <PLACE_HOLDER> rows with the same shape . use a field name that does not appear in the main query <PLACE_HOLDER> @$ to assure the <PLACE_HOLDER> will be null .,string dim name = __str__ + i ; while ( query . get result row position lookup ( ) . get int ( dim name ) >= __num__ ) { dim name = __str__ + dim name ; } new dimensions . add ( default dimension spec . of ( dim name ) ) ;,rows have result
only chunks in the old from space have a remembered <PLACE_HOLDER> .,final heap impl heap = heap impl . get heap impl ( ) ; final old generation old gen = heap . get old generation ( ) ; if ( that . get space ( ) == old gen . get from space ( ) ) { if ( ! card table . verify ( get card table start ( that ) @$ get first object table start ( that ) @$ get aligned heap chunk start ( that ) @$ that . get top ( ) ) ) { final log verify log = heap . get heap verifier impl ( ) . get witness log ( ) . string ( __str__ ) ; verify log . string ( __str__ ) . string ( __str__ ) .,chunks have seed
main : 1 finished main : 1 received 0 on 11 received 1 on 11 received <PLACE_HOLDER> on 11,system . out . println ( __str__ + thread . current thread ( ) . get id ( ) ) ;,main received 2
if the order by expression list contains a <PLACE_HOLDER> by expression then we wo n't have to sort by it twice . we sort by the <PLACE_HOLDER> by expressions first @$ and we do n't care what order we sort by them . so @$ find the sort direction in the order by list and use that in the <PLACE_HOLDER> by list @$ and,boolean [ ] dontsort = new boolean [ win expr . get orderby size ( ) ] ; list < abstract expression > order by expressions = win expr . get order by expressions ( ) ; list < sort direction type > order by directions = win expr . get order by directions ( ) ; order by plan node onode = new order by plan node ( ) ; for ( int idx = __num__ ; idx < win expr . get partitionby size ( ) ; ++ idx ) { sort direction type pdir = sort direction type . asc ; abstract expression partition by expression = partition by expressions . get ( idx ) ; int sidx = win expr . get sort index,order contains order
model name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( model name ) ) { logger . warn ( model name + __str__ + camelize ( __str__ + model name ) ) ; model name = __str__ + model name ; },name use keyword
measure with exactly . that way @$ paged tile layout will only use excess <PLACE_HOLDER> and will be measured last @$ after other views and padding is accounted for .,mqs panel . measure ( width measure spec @$ measure spec . make measure spec ( max qs @$ measure spec . exactly ) ) ; int width = mqs panel . get measured width ( ) ; int height = layout params . top margin + layout params . bottom margin + mqs panel . get measured height ( ) + get padding bottom ( ) ; super . on measure ( measure spec . make measure spec ( width @$ measure spec . exactly ) @$ measure spec . make measure spec ( height @$ measure spec . exactly ) ) ;,layout use space
null constraint has no <PLACE_HOLDER>,if ( constraint data != null ) { number flag = ( number ) constraint data . get field value ( __str__ ) ; if ( flag != null ) { this . flag = flag . int value ( ) ; } },constraint has op
when activity is idle @$ we consider the relaunch must be successful @$ so let 's clear the <PLACE_HOLDER> .,r . m relaunch reason = relaunch_reason_none ;,'s clear reason
in the absence of metadata art @$ the controller dialog takes <PLACE_HOLDER> of creating it .,if ( is casting ) { builder . put string ( media metadata compat . metadata_key_display_icon_uri @$ image location ) ; },dialog takes care
load state store with app node split <PLACE_HOLDER> of 2 .,store = zk tester . getrm state store ( create conf for app node split ( __num__ ) ) ; store . setrm dispatcher ( dispatcher ) ; rm state state = store . load state ( ) ; application id app id21 = application id . new instance ( __num__ @$ __num__ ) ; store app ( store @$ dispatcher @$ app id21 @$ submit time @$ start time ) ;,store split level
the my sql binlog always returns a year <PLACE_HOLDER> ...,if ( data instanceof java . time . year ) { r . deliver ( adjust temporal ( java . time . year . of ( ( ( java . time . year ) data ) . get value ( ) ) ) . get ( chrono field . year ) ) ; } else if ( data instanceof java . sql . date ) { r . deliver ( ( ( java . sql . date ) data ) . get year ( ) + __num__ ) ; } else if ( data instanceof string ) { mut data = integer . value of ( ( string ) data ) ; },sql returns clone
passing null matches <PLACE_HOLDER>,if ( argument passed . equals ( type info factory . void type info ) ) { return __num__ ; },null matches everything
no existing note info <PLACE_HOLDER> mapping to same key as the current note so add a new list,if ( sparse array index < __num__ ) { list < note info > duplicates for key = new array list < > ( ) ; duplicates for key . add ( note ) ; duplicates . put ( position @$ duplicates for key ) ; } else { duplicates . value at ( sparse array index ) . add ( note ) ; },no note arrays
ensure that the 2 nn can still perform a <PLACE_HOLDER> .,secondary . do checkpoint ( ) ;,nn perform checkpoint
elf file does not contain an .llvmbc <PLACE_HOLDER>,if ( llvmbc == null ) { return null ; },file contain extension
run the buck kill <PLACE_HOLDER>,buck kill command handler handler = new buck kill command handler ( project @$ buck command . kill ) ; buck build manager . get instance ( project ) . run buck command while connected to buck ( handler @$ action_title @$ buck module ) ;,buck kill command
there is a problem with handling simultaneous auto <PLACE_HOLDER> after restart and manual <PLACE_HOLDER> . to properly catch the moment when cluster <PLACE_HOLDER> has finished we temporary disable auto <PLACE_HOLDER> .,disable auto activation = true ; start grids ( __num__ ) ; ig = grid ( __num__ ) ; ig . cluster ( ) . active ( true ) ; cache = ig . cache ( cache name ) ; grid dht partition full map part map = ig . cachex ( cache name ) . context ( ) . topology ( ) . partition map ( false ) ; for ( int i = __num__ ; i < __num__ ; i ++ ) { ignite ex ig0 = grid ( i ) ; for ( int p = __num__ ; p < __num__ ; p ++ ) assert equals collections ( ig . affinity ( cache name ) . map partition to primary and backups ( p,activation disable initialization
consolidate passed ops at the current slot duration ensuring each <PLACE_HOLDER> is full . to achieve this we put all passed and existing ops in a list and will merge them to ensure each represents a <PLACE_HOLDER> at the current granularity .,final list < historical ops > all ops = new linked list < > ( passed ops ) ; if ( existing ops != null ) { all ops . add all ( existing ops ) ; } if ( debug ) { enforce ops well formed ( all ops ) ; },each represents block
client app left the restore <PLACE_HOLDER> dangling . we know that it ca n't be in the middle of an actual restore operation because the timeout is suspended while a restore is in progress . clean up now .,if ( backup manager service . get active restore session ( ) != null ) { slog . w ( tag @$ __str__ ) ; backup manager service . get active restore session ( ) . mark timed out ( ) ; post ( backup manager service . get active restore session ( ) . new end restore runnable ( backup manager service @$ backup manager service . get active restore session ( ) ) ) ; },app left session
wait for monitoring interval time and verify server is still in running <PLACE_HOLDER>,resume and wait ( monitoring_interval + __num__ ) ; assert equals ( operation mode . running @$ voltdb . instance ( ) . get mode ( ) ) ;,server running mode
if default material button background has been overwritten @$ we will let app compat button handle the <PLACE_HOLDER>,super . set support background tint list ( tint ) ;,button handle tinting
resources missing from used are using <PLACE_HOLDER> of that resource,if ( i >= used . other resources . length ) { return __num__ ; },resources using none
this pass does not do flow <PLACE_HOLDER> .,test same ( __str__ ) ;,pass do validation
assignment of object arrays requires <PLACE_HOLDER> of widening conversion of component types,return is assignable to ( from type @$ to type ) ;,assignment requires availability
we do not want to truncate logs based on these exceptions . since users can influence <PLACE_HOLDER> with config changes the users are able to workaround this if truncations is really needed .,throw e ; if ( fail on corrupted log files ) { throw unable to clean recover ( t ) ; } if ( last transaction != null ) { log entry commit commit entry = last transaction . get commit entry ( ) ; monitor . fail to recover transactions after commit ( t @$ commit entry @$ recovery to position ) ; } else { monitor . fail to recover transactions after position ( t @$ recovery start position ) ; },users influence results
when calling set lock task <PLACE_HOLDER> for locked <PLACE_HOLDER> on both tasks,m lock task controller . start lock task mode ( tr1 @$ false @$ test_uid ) ; m lock task controller . start lock task mode ( tr2 @$ false @$ test_uid ) ;,calling set mode
rm does n't have app <PLACE_HOLDER> and job history server is not configured,client service delegate client service delegate = get client service delegate ( null @$ getrm delegate ( ) ) ; job status job status = client service delegate . get job status ( old job id ) ; assert . assert equals ( __str__ @$ job status . get username ( ) ) ; assert . assert equals ( job status . state . prep @$ job status . get state ( ) ) ;,rm have id
if new if old expected old <PLACE_HOLDER> require old <PLACE_HOLDER>,if ( basic put ( event @$ false @$ false @$ null @$ false ) ) { old value = event . get old value ( ) ; },value require value
we have to compare in this order @$ because the comparator order has special <PLACE_HOLDER> when the 'left side ' is a special key .,int cmp = comparator . compare ( key @$ arr [ mid ] ) ;,order has logic
latest supported <PLACE_HOLDER> for focused test runs,string builder buf = new string builder ( super . get name ( ) ) ; if ( include variant markers in test name || always include variant markers in name ) { buf . append ( __str__ ) . append ( get sdk ( ) . get api level ( ) ) . append ( __str__ ) ; if ( default res mode strategy == res mode strategy . both ) { buf . append ( __str__ ) . append ( resources mode . name ( ) ) . append ( __str__ ) ; } } return buf . to string ( ) ;,latest supported version
is the outermost query and it will actually get run as a native query . druid 's native query layer will finalize <PLACE_HOLDER> for the outermost query even if we do n't explicitly ask it to .,final druid query query = to druid query ( false ) ; if ( query != null ) { return get query maker ( ) . run query ( query ) ; } else { return sequences . empty ( ) ; },layer finalize aggregations
this is an approximation as different host process make significantly different <PLACE_HOLDER>,int tot = __num__ ; for ( host process process : this . get host processes ( ) ) { tot += process . get percentage complete ( ) ; } int latest progress = tot / this . get host processes ( ) . size ( ) ; if ( latest progress != this . progress ) { this . progress = latest progress ; active scan event publisher . publish scan progress event ( this . get id ( ) @$ this . progress ) ; },process make decisions
the agent can read <PLACE_HOLDER> from user content,string content = s . get channel ( ) . call ( new read files2m callable ( target ) ) ;,agent read files
... figure out which direct dependencies can possibly have <PLACE_HOLDER> attached to them ...,multimap < attribute @$ label > deps with possible aspects = ( ( rule ) target ) . get transitions ( ( rule rule @$ attribute attribute ) -> { for ( aspect aspect with parameters : attribute . get aspects ( rule ) ) { if ( ! aspect with parameters . get definition ( ) . get attributes ( ) . is empty ( ) ) { return true ; } } return false ; } ) ;,dependencies have requests
0 x 1002346 : p 1 repeatable <PLACE_HOLDER> contains p 2 repeatable <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . repeatable_comment ) ;,comment contains comment
add a keyed stateful map operator @$ which uses <PLACE_HOLDER> for state serialization,event stream = event stream . key by ( event :: get key ) . map ( create artificial keyed state mapper ( ( map function < event @$ event > ) in -> in @$ ( event event @$ complex payload avro last state ) -> { if ( last state != null && ! last state . get str payload ( ) . equals ( keyed_state_oper_with_avro_ser . get name ( ) ) && last state . get inner pay load ( ) . get sequence number ( ) == ( event . get sequence number ( ) - __num__ ) ) { system . out . println ( __str__ ) ; } complex payload avro payload = new complex payload avro ( ) ; payload .,which uses kryo
0 <PLACE_HOLDER> 10018 a 6 : op 0 has reg ref to esi . 0 <PLACE_HOLDER> 100295 a : op 1 has reg ref to c<PLACE_HOLDER> . 0 <PLACE_HOLDER> 1002 d 0 b : op 0 has reg ref to edi ; op 1 has reg ref to ea<PLACE_HOLDER> . 0 <PLACE_HOLDER> 10033 fe : op 0 both have reg ref to edi .,mtf . initialize ( __str__ @$ new program modifier listener ( ) { @ override public void modify latest ( programdb program ) { int tx id = program . start transaction ( __str__ ) ; boolean commit = false ; try { program context context = program . get program context ( ) ; register esi reg = context . get register ( __str__ ) ; register cx reg = context . get register ( __str__ ) ; reference manager ref mgr = program . get reference manager ( ) ; reference [ ] refs ; reference new ref ; refs = ref mgr . get references from ( addr ( program @$ __str__ ) @$ __num__ ) ; assert equals ( __num__ @$ refs . length,fe 0 x
undefined blocks have no source <PLACE_HOLDER>,assert true ( ranges . is empty ( ) ) ;,blocks have address
cleanup cached <PLACE_HOLDER> before tests,compile utils . compiled_cache . invalidate all ( ) ;,cleanup cached objects
special join <PLACE_HOLDER> that uses the poly join columns,join sequence = get session factory helper ( ) . create join sequence ( implied join @$ property type @$ table alias @$ join type @$ poly join columns ) ;,special join sequence
go from the back of the list to front @$ look for the request closes to the beginning that requests the <PLACE_HOLDER> in which activity will end after all callbacks are executed .,int last requested state = undefined ; int last requesting callback = - __num__ ; for ( int i = callbacks . size ( ) - __num__ ; i >= __num__ ; i -- ) { final client transaction item callback = callbacks . get ( i ) ; final int post execution state = callback . get post execution state ( ) ; if ( post execution state != undefined ) { if ( last requested state == undefined || last requested state == post execution state ) { last requested state = post execution state ; last requesting callback = i ; } else { break ; } } },activity end activity
constant values do n't need <PLACE_HOLDER>,if ( token . is constant encoding ( ) ) { return ; },values need encoding
server socket will accept <PLACE_HOLDER>,socket base server = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( server @$ not null value ( ) ) ; socket base client = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( client @$ not null value ( ) ) ; zmq . set socket option ( server @$ zmq . zmq_zap_domain @$ __str__ ) ; rc = zmq . bind ( server @$ host ) ; assert that ( rc @$ is ( true ) ) ; rc = zmq . connect ( client @$ host ) ; assert that ( rc @$ is ( true ) ) ; int ret = zmq . send ( client @$ __str__ @$ __num__ ) ; assert,socket accept connections
start use bound port will throw <PLACE_HOLDER>,server config server config2 = new server config ( ) ; server config2 . set bound host ( host ) ; server config2 . set port ( port ) ; server config2 . set protocol ( rpc constants . protocol_type_bolt ) ; http2 clear text server server2 = new http2 clear text server ( ) ; server2 . init ( server config2 ) ; boolean error = false ; try { server2 . start ( ) ; } catch ( exception e ) { error = true ; } assert . assert true ( error ) ; server . stop ( ) ; assert . assert false ( server . started ) ; thread . sleep ( __num__ ) ;,port throw exception
<PLACE_HOLDER> x ac <PLACE_HOLDER> f as basic conjoining jamo,string sc ko pat3 = __str__ ;,x ac 2
set zoom . this helps encourage the user to pull back . some devices like the behold have a zoom <PLACE_HOLDER>,if ( max zoom string != null || mot zoom values string != null ) { parameters . set ( __str__ @$ string . value of ( ten desired zoom / __num__ ) ) ; },behold have limit
text node needs special case <PLACE_HOLDER>,if ( f start container . get node type ( ) == node . text_node ) { string s = f start container . get node value ( ) ; string sub = s . substring ( f start offset @$ f end offset ) ; if ( how != clone_contents ) { ( ( text impl ) f start container ) . delete data ( f start offset @$ f end offset - f start offset ) ; collapse ( true ) ; } if ( how == delete_contents ) return null ; frag . append child ( f document . create text node ( sub ) ) ; return frag ; },node needs handling
this filter will keep the <PLACE_HOLDER> from being sent accross the wire . this is good for counting or other scans that are checking for existence and do n't rely on the value .,s . set filter ( new key only filter ( ) ) ;,filter keep keys
after the job ran check to see if the input from the localized cache match the real <PLACE_HOLDER> . check if there are 3 instances or not .,path result = new path ( test_root_dir + __str__ ) ; if ( count != __num__ ) return new test result ( job @$ false ) ;,input match one
value follows <PLACE_HOLDER>,xdr . write boolean ( true ) ;,value follows endpoint
short form has all other details @$ but if we have collected ram from smaller native processes let 's dump a <PLACE_HOLDER> of that .,if ( extra native ram > __num__ ) { append basic mem entry ( short native builder @$ process list . native_adj @$ - __num__ @$ extra native ram @$ extra native memtrack @$ __str__ ) ; short native builder . append ( __str__ ) ; extra native ram = __num__ ; } append mem info ( full java builder @$ mi ) ;,'s dump diff
if we do n't need a thread pool @$ create a dummy <PLACE_HOLDER> that executes the task synchronously noinspection nullable problems,this . executor = create thread pool ? executors . new fixed thread pool ( threads ) : new executor ( ) { @ override public void execute ( runnable command ) { command . run ( ) ; } } ;,problems create executor
we wo n't compare the candidate display <PLACE_HOLDER> against the current item . this is to prevent an validation warning if the user sets the display <PLACE_HOLDER> to what the existing display <PLACE_HOLDER> is,if ( item . get name ( ) . equals ( current job name ) ) { continue ; } else if ( display name . equals ( item . get display name ( ) ) ) { return false ; },user sets name
do not wrap null <PLACE_HOLDER> . we do not want to impede error signaling .,if ( resolver == null ) { return null ; } return new forwarding name resolver ( resolver ) { @ override public string get service authority ( ) { return authority override ; } } ;,wrap null validators
upsert requires a primary <PLACE_HOLDER> on the table to work,if ( is upsert ) { boolean has pkey = false ; for ( constraint c : cat table . get constraints ( ) ) { if ( c . get type ( ) == constraint type . primary_key . get value ( ) ) { has pkey = true ; break ; } } if ( ! has pkey ) { throw new volt abort exception ( string . format ( __str__ + __str__ @$ table name ) ) ; } },upsert requires key
user has <PLACE_HOLDER> disabled,if ( request . get requested session id ( ) == null ) { system messages system messages = get service ( ) . get system messages ( servlet portlet helper . find locale ( null @$ null @$ request ) @$ request ) ; get service ( ) . write uncached string response ( response @$ json constants . json_content_type @$ vaadin service . create critical notificationjson ( system messages . get cookies disabled caption ( ) @$ system messages . get cookies disabled message ( ) @$ null @$ system messages . get cookies disabledurl ( ) ) ) ; return false ; },user has cookies
see if any of the installed providers supply a <PLACE_HOLDER> from the given algorithm name to an oid string,string oid string ; if ( ! init oid table ) { provider [ ] provs = security . get providers ( ) ; for ( int i = __num__ ; i < provs . length ; i ++ ) { for ( enumeration < object > enum_ = provs [ i ] . keys ( ) ; enum_ . has more elements ( ) ; ) { string alias = ( string ) enum_ . next element ( ) ; string upper case alias = alias . to upper case ( locale . english ) ; int index ; if ( upper case alias . starts with ( __str__ ) && ( index = upper case alias . index of ( __str__ @$ __num__ ) ) !=,any supply mapping
restli request options takes <PLACE_HOLDER> over response compression config,return new object [ ] [ ] { { true @$ null @$ restli request options . default_options @$ large id count @$ default_accept_encoding @$ null @$ true } @$ { true @$ null @$ restli request options . default_options @$ small id count @$ default_accept_encoding @$ null @$ false } @$ { true @$ new compression config ( tiny ) @$ restli request options . default_options @$ large id count @$ default_accept_encoding @$ tiny . to string ( ) @$ true } @$ { true @$ new compression config ( tiny ) @$ restli request options . default_options @$ small id count @$ default_accept_encoding @$ tiny . to string ( ) @$ true } @$ { true @$ new compression config ( huge ) @$ restli request,options takes precedence
test when third argument has <PLACE_HOLDER> and repeats,batch = get batch4 long vectors ( ) ; r = ( long column vector ) batch . cols [ __num__ ] ; batch . cols [ __num__ ] . no nulls = false ; batch . cols [ __num__ ] . is null [ __num__ ] = true ; batch . cols [ __num__ ] . is repeating = true ; expr . evaluate ( batch ) ; assert equals ( true @$ r . is null [ __num__ ] ) ; assert equals ( true @$ r . is null [ __num__ ] ) ; assert equals ( - __num__ @$ r . vector [ __num__ ] ) ; assert equals ( - __num__ @$ r . vector [ __num__ ] ) ; assert equals (,argument has nulls
ui.js just sets up the <PLACE_HOLDER>,assert equals ( __str__ @$ messages . get ( __num__ ) . get text ( ) ) ;,ui.js sets message
we need to create the first <PLACE_HOLDER> because combine fn.merger <PLACE_HOLDER>s can modify the first <PLACE_HOLDER>,accumt first = combine fn . create accumulator ( ) ; iterable < accumt > accumulators to merge = iterables . concat ( collections . singleton ( first ) @$ accums and instants for merged window . stream ( ) . map ( x -> x . _1 ( ) ) . collect ( collectors . to list ( ) ) ) ; result . add ( windowed value . of ( combine fn . merge accumulators ( accumulators to merge ) @$ timestamp combiner . combine ( accums and instants for merged window . stream ( ) . map ( x -> x . _2 ( ) ) . collect ( collectors . to list ( ) ) ) @$ merged window @$ pane info . no_firing,accumulators modify accumulator
initializes status <PLACE_HOLDER> components if the given meta contact contains a status <PLACE_HOLDER> .,init display details ( contact . get display details ( ) ) ;,contact contains details
mock server to allow dynamically generated <PLACE_HOLDER> to be accepted,httpsurl connection . set defaultssl socket factory ( new key store factory ( new mock server logger ( ) ) . ssl context ( ) . get socket factory ( ) ) ; mock server = client and server . start client and server ( port factory . find free port ( ) ) ;,server generated certificates
for the first grandkid replace the original <PLACE_HOLDER>,for ( filter grandkid : grand kids ) { if ( first ) { first = false ; children . set ( i @$ grandkid ) ; } else { children . add ( ++ i @$ grandkid ) ; } },grandkid replace parent
now prepare the implementation of the build method . see bean <PLACE_HOLDER> interface,visit build method definition ( annotation metadata @$ collections . empty map ( ) @$ collections . empty map ( ) ) ;,method see factory
go to ocp and check if there a user project has expected <PLACE_HOLDER>,open shift project catalog page . open ( ) ; open shift login page . login ( new_test_user . get name ( ) @$ new_test_user . get password ( ) ) ; open shift project catalog page . wait project ( user_project_name ) ; open shift project catalog page . click on project ( user_project_name ) ; open shift project catalog page . wait resource ( __str__ ) ;,project expected resources
hive does not support result set meta <PLACE_HOLDER> on prepared statement @$ and hive describe does not support queries @$ so we have to execute the query with limit 1,if ( conn type == conn . type . hive ) { string sql = __str__ + select + __str__ ; query query = new query ( sql ) ; exec . execute query ( ctx @$ query @$ conn ) ; if ( ! query . error ( ) ) { result set rs = query . get result set ( ) ; try { result set meta data rm = rs . get meta data ( ) ; int cols = rm . get column count ( ) ; row = new row ( ) ; for ( int i = __num__ ; i <= cols ; i ++ ) { string name = rm . get column name ( i ) ; if ( name,hive support data
hides the popup menu when the parent window loses <PLACE_HOLDER> .,add component listener ( new component adapter ( ) { @ override public void component resized ( component event evt ) { final window parent window ; component parent = get parent ( ) ; if ( parent instanceof j popup menu ) parent window = swing utilities . get window ancestor ( ( ( j popup menu ) parent ) . get invoker ( ) ) ; else parent window = swing utilities . get window ancestor ( get invoker ( ) ) ; if ( parent window != null ) { if ( ! parent window . is active ( ) ) set visible ( false ) ; parent window . add window listener ( new window adapter ( ) { @ override public void window,window loses focus
issues with stables pages @$ hence we move the <PLACE_HOLDER> back one page,long synced bytes = ( ( position at sync / bits . page size ( ) ) - __num__ ) * bits . page size ( ) ; if ( posix advise . sync_file_range_supported ) { final long retval = posix advise . sync_file_range ( fd @$ sync start @$ synced bytes - sync start @$ posix advise . sync_file_range_sync ) ; if ( retval != __num__ ) { logger . error ( __str__ + retval ) ; logger . error ( __str__ + synced bytes + __str__ + ( synced bytes - sync start ) + __str__ + posix advise . sync_file_range_sync ) ; fc . force ( false ) ; } } else { fc . force ( false ) ; } return synced bytes ;,issues move file
otherwise @$ this meeting needs a new <PLACE_HOLDER>,heap . offer ( intervals [ i ] ) ;,meeting needs interval
authenticated user 's account and requires <PLACE_HOLDER> to use an ssl connection .,list < string > scopes = lists . new array list ( __str__ ) ; try { credential credential = auth . authorize ( scopes @$ __str__ ) ; youtube = new you tube . builder ( auth . http_transport @$ auth . json_factory @$ credential ) . set application name ( __str__ ) . build ( ) ; string channel id = get channel id ( ) ; system . out . println ( __str__ + channel id + __str__ ) ; string video id = get video id ( ) ; system . out . println ( __str__ + video id + __str__ ) ; string text = get text ( ) ; system . out . println ( __str__ + text + __str__ ) ;,account requires requests
this entry aggregates multiple <PLACE_HOLDER> .,if ( entry . get journal entries count ( ) > __num__ ) { for ( journal entry e : entry . get journal entries list ( ) ) { apply entry ( e ) ; } } else if ( entry . get sequence number ( ) < __num__ ) { m last primary start sequence number = entry . get sequence number ( ) ; } else if ( entry . to builder ( ) . clear sequence number ( ) . build ( ) . equals ( journal entry . get default instance ( ) ) ) { } else { apply single entry ( entry ) ; },entry aggregates entries
a volt db <PLACE_HOLDER> to support indexed expressions .,boolean has non column exprs = false ; if ( set == null ) { has non column exprs = true ; set = get base column names ( index exprs ) ; },volt db extension
nn should not bind the wildcard <PLACE_HOLDER> by default .,try { conf . set ( dfs_namenode_http_address_key @$ localhost_server_address ) ; cluster = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) . build ( ) ; cluster . wait active ( ) ; string address = cluster . get name node ( ) . get http address ( ) . to string ( ) ; assert false ( __str__ @$ address . starts with ( wildcard_address ) ) ; } finally { if ( cluster != null ) { cluster . shutdown ( ) ; cluster = null ; } } log . info ( __str__ + dfs_namenode_http_bind_host_key ) ;,nn bind address
if any thread has seen <PLACE_HOLDER> or operation failed then we do n't have to process further .,if ( last exception != null || ! operation status ) { log . warn ( __str__ + __str__ @$ this . operation @$ file . get key ( ) ) ; break ; },thread seen exception
so everyone has the same <PLACE_HOLDER> .,return app priority ;,everyone has priority
verify new paths reflect <PLACE_HOLDER> of previous resources,local resource request lr2 = create local resource request ( user @$ __num__ @$ __num__ @$ local resource visibility . application ) ; localizer context lc2 = new localizer context ( user @$ c id1 @$ null ) ; resource event req event2 = new resource request event ( lr2 @$ local resource visibility . application @$ lc2 ) ; tracker . handle ( req event2 ) ; dispatcher . await ( ) ; path hierarchical path2 = tracker . get path for localization ( lr2 @$ local dir @$ null ) ; long localized id2 = long . parse long ( hierarchical path2 . get name ( ) ) ; assert . assert equals ( localized id1 + __num__ @$ localized id2 ) ; if ( dispatcher,paths reflect information
ui mode includes <PLACE_HOLDER> and night ...,int ui mode type = get ui mode type ( configuration ) ; int res tab type = res tab . ui mode type ( ) ; if ( res tab type != res table_config . ui_mode_type_any ) { ui mode type = res tab type ; } int ui mode night = get ui mode night ( configuration ) ; int res tab night = res tab . ui mode night ( ) ; if ( res tab night != res table_config . ui_mode_night_any ) { ui mode night = res tab night ; } configuration . ui mode = ui mode type | ui mode night ; if ( res tab . density != res table_config . density_default ) { set density ( res tab .,mode includes 4
we move the error classification into extensions which allows downstream <PLACE_HOLDER> to see them but still be spec compliant,if ( error classification != null ) { if ( extensions != null ) { extensions = new linked hash map < > ( extensions ) ; } else { extensions = new linked hash map < > ( ) ; } if ( ! extensions . contains key ( __str__ ) ) { extensions . put ( __str__ @$ error classification . to specification ( error ) ) ; } },which allows users
get data will activate deferred <PLACE_HOLDER> if necessary,byte [ ] the header = get data ( ic sig head ) ; int to big endian ( rendering intent @$ the header @$ ic hdr rendering intent ) ;,data activate profiles
if a topic has k <PLACE_HOLDER> @$ and in the previous run @$ each partition recorded its avg time to pull a record @$ then use the geometric mean of these k numbers as the estimated avg time to pull a record in this run .,double est avg millis for topic = geometric mean ( prev avg millis for partitions ) ; this . est avg millis . put ( topic @$ est avg millis for topic ) ; log . info ( string . format ( __str__ @$ topic @$ est avg millis for topic ) ) ; all est avg millis . add ( est avg millis for topic ) ;,topic has values
check can not delete <PLACE_HOLDER> via rest because it contains tables .,response = client . delete ( namespace path ) ; namespace path = __str__ + ns name ; assert equals ( __num__ @$ response . get code ( ) ) ;,check delete namespace
get a sender from the pool @$ or create a new one if the pool is empty if we ca n't create a new connection then route flow files to failure and yield acquire sender will handle the <PLACE_HOLDER> to failure and yielding,channel sender sender = acquire sender ( context @$ session @$ flow file ) ; if ( sender == null ) { return ; } try { string delimiter = context . get property ( message_delimiter ) . evaluate attribute expressions ( flow file ) . get value ( ) ; if ( delimiter != null ) { delimiter = delimiter . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) . replace ( __str__ @$ __str__ ) ; } if ( delimiter == null ) { process single message ( context @$ session @$ flow file @$ sender ) ; } else { process delimited messages ( context @$ session @$ flow file @$ sender @$ delimiter ) ; } } finally {,sender handle fallback
not enough data to decrypt . compact the <PLACE_HOLDER> so that we keep the data we have but prepare the <PLACE_HOLDER> to be written to again .,logger . debug ( __str__ ) ; stream buffer . compact ( ) ; return __num__ ;,data compact buffer
it is possible to turn off the semantic graphs @$ in which case we do n't want to recreate them using the dependency <PLACE_HOLDER> . this might be relevant if using core nlp for a language which does n't have dependencies @$ for example .,if ( sentence . get ( semantic graph core annotations . enhanced plus plus dependencies annotation . class ) != null ) { pw . println ( ) ; pw . println ( __str__ ) ; pw . print ( sentence . get ( semantic graph core annotations . enhanced plus plus dependencies annotation . class ) . to list ( ) ) ; },them using annotations
resolve initial seek <PLACE_HOLDER> .,if ( pending initial seek position != null ) { pair < object @$ long > period position = resolve seek position ( pending initial seek position @$ true ) ; pending initial seek position = null ; if ( period position == null ) { handle source info refresh ended playback ( ) ; return ; } new content position us = period position . second ; new period id = queue . resolve media period id for ads ( period position . first @$ new content position us ) ; } else if ( old content position us == c . time_unset && ! timeline . is empty ( ) ) { pair < object @$ long > default position = get period position ( timeline,initial seek position
no need for lazy details scroll adjuster @$ because the start is always 0 @$ wo n't change a <PLACE_HOLDER> .,register rpc ( grid client rpc . class @$ new grid client rpc ( ) { @ override public void scroll to start ( ) { scheduler . get ( ) . schedule finally ( new scheduled command ( ) { @ override public void execute ( ) { grid . scroll to start ( ) ; } } ) ; } @ override public void scroll to end ( ) { scheduler . get ( ) . schedule finally ( new scheduled command ( ) { @ override public void execute ( ) { grid . scroll to end ( ) ; lazy details scroller . scroll to row ( data source . size ( ) - __num__ @$ scroll destination . end ) ; } },adjuster change value
step 5 : if the class object for c is in an erroneous state @$ then initialization is not possible . release <PLACE_HOLDER> and throw a no class def found error .,if ( is in error state ( ) ) { throw new no class def found error ( __str__ + hub . get name ( ) ) ; },step release lc
msp 3 has <PLACE_HOLDER> which hsql does not support,if ( ! ishsql ( ) ) { vt = client . call procedure ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ ) . get results ( ) ; assert content of table ( new object [ ] [ ] { { __num__ } } @$ vt [ __num__ ] ) ; assert content of table ( new object [ ] [ ] { { __num__ } } @$ vt [ __num__ ] ) ; assert content of table ( new object [ ] [ ] { { __num__ @$ __str__ } @$ { __num__ @$ __str__ } } @$ vt [ __num__ ] ) ; },msp has cases
secondary ce @$ or a ce with a short primary @$ copy the case <PLACE_HOLDER> .,if ( minice1 <= collation fast latin . secondary_mask || collation fast latin . min_short <= minice1 ) { case1 = ( case1 > > ( __num__ - __num__ ) ) + collation fast latin . lower_case ; minice1 |= case1 ; },ce copy 1
null should not impact <PLACE_HOLDER>,current max = integer max kudaf . aggregate ( null @$ current max ) ; assert that ( __num__ @$ equal to ( current max ) ) ;,null impact result
any implementations must register <PLACE_HOLDER>,return super . parse and validate metadata ( target type ) ;,implementations register metadata
redundant store does n't change the <PLACE_HOLDER>,accept ( storage . span consumer ( ) @$ trace ) ; assert that ( row count ( tables . annotations_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_remote_service_name_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_name_index ) ) . is equal to ( __num__ ) ; assert that ( row count ( tables . service_span_name_index ) ) . is equal to ( __num__ ) ;,store change index
output l 2 <PLACE_HOLDER> and ignite <PLACE_HOLDER> stats . you may notice that at this point the object is not yet stored in l 2 <PLACE_HOLDER> @$ because the read was not yet performed .,print stats ( ses factory ) ; system . out . println ( ) ; system . out . println ( __str__ ) ;,output l c
6 th build @$ <PLACE_HOLDER> @$ accumulation continues up to this point,scm . add change ( ) . with author ( __str__ ) ; p . get builders list ( ) . clear ( ) ; b = j . build and assert success ( p ) ; assert culprits ( b @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,th build need
we have specific code that customizes log <PLACE_HOLDER> . use this when the case .,if ( error . starts with ( __str__ ) || error . starts with ( __str__ ) ) { logger . debug ( error @$ e ) ; } else { string message = format ( __str__ @$ default log message . get ( ) @$ e . get class ( ) . get simple name ( ) @$ error ) ; logger . debug ( message @$ e ) ; },customizes log messages
update once at end of iteration to reduce heap write <PLACE_HOLDER>,last ret = cursor = i ; check for comodification ( ) ; return cursor ; return cursor - __num__ ; if ( last ret < __num__ ) throw new illegal state exception ( ) ; check for comodification ( ) ; try { sub list . this . remove ( last ret ) ; cursor = last ret ; last ret = - __num__ ; expected mod count = array list . this . mod count ; } catch ( index out of bounds exception ex ) { throw new concurrent modification exception ( ) ; } if ( last ret < __num__ ) throw new illegal state exception ( ) ; check for comodification ( ) ; try { array list . this . set (,heap write traffic
subsequent error triggers does n't trigger another <PLACE_HOLDER>,tracker . process response ( new response ( __str__ @$ null ) @$ __str__ @$ __num__ ) ; assert tracker mismatch count ( tracker @$ __num__ ) ;,triggers trigger countdown
do not recycle the old bitmap if such as it may be set as an error state to any of the page views . just let the gc take <PLACE_HOLDER> of it .,m error state = new bitmap drawable ( m context . get resources ( ) @$ error bitmap ) ;,gc take care
resolves dependency if setting has a dependency and the calling user has a <PLACE_HOLDER>,if ( s system clone from parent on dependency . contains key ( setting ) && ( parent id = get group parent locked ( user id ) ) != user id ) { string dependency = s system clone from parent on dependency . get ( setting ) ; final long token = binder . clear calling identity ( ) ; try { setting setting obj = get secure setting ( dependency @$ user id ) ; if ( setting obj != null && setting obj . get value ( ) . equals ( __str__ ) ) { return parent id ; } } finally { binder . restore calling identity ( token ) ; } },user has parent
module invoked in do <PLACE_HOLDER>,invoke priv ( logout_method ) ;,module do privileged
a mapped superclass can have no <PLACE_HOLDER> if the id is set below in the hierarchy,if ( mapping type . get identifier mapper ( ) != null ) { @ suppress warnings ( __str__ ) iterator < property > property iterator = mapping type . get identifier mapper ( ) . get property iterator ( ) ; set < singular persistent attribute < ? super x @$ ? > > attributes = build id class attributes ( jpa mapping type @$ property iterator ) ; jpa mapping type . get in flight access ( ) . apply id class attributes ( attributes ) ; },superclass have dependencies
reposition & adjust the <PLACE_HOLDER> for new orientation,if ( m is expanded ) { m expanded view container . set translationy ( get expanded viewy ( ) ) ; m expanded bubble . get expanded view ( ) . update view ( ) ; },& adjust height
the peer is going away <PLACE_HOLDER> is broken,for ( stream holder holder : current streams . values ( ) ) { if ( holder . source channel != null ) { holder . source channel . rst stream ( ) ; } if ( holder . sink channel != null ) { holder . sink channel . rst stream ( ) ; } } frame data . close ( ) ; send go away ( error_no_error ) ; break ;,peer going stream
user can not see these <PLACE_HOLDER>,index for user ( user2 @$ new doc ( metric key @$ __num__ ) @$ new doc ( metric key @$ __num__ ) @$ new doc ( metric key @$ __num__ ) ) ;,user see projects
a volt db <PLACE_HOLDER> to diagnose array out of bounds .,boolean voltd breclaimed = ( reclaimed node pointer != - __num__ ) ;,volt db extension
only the killed case requires a <PLACE_HOLDER> to be sent out to the am .,switch ( result . get end reason ( ) ) { case success : log . debug ( __str__ @$ request id ) ; if ( metrics != null ) { metrics . incr executor total success ( ) ; } break ; case container_stop_requested : log . info ( __str__ @$ request id ) ; if ( metrics != null ) { metrics . incr executor total killed ( ) ; } break ; case kill_requested : log . info ( __str__ @$ request id ) ; if ( killtimer watch . is running ( ) ) { killtimer watch . stop ( ) ; long elapsed = killtimer watch . elapsed ( time unit . milliseconds ) ; log . info ( __str__ @$ elapsed ),case requires terminal
reserved thread must have incremented <PLACE_HOLDER> but not yet added itself to queue . we will spin until it is added .,if ( thread == null ) { thread . yield ( ) ; continue ; },thread incremented count
otherwise use inject bean method instead which triggers reflective <PLACE_HOLDER>,push inject method for index ( inject method visitor @$ inject instance index @$ current method index @$ __str__ ) ;,which triggers injection
register task manager success will trigger monitoring heartbeat <PLACE_HOLDER> between tm and jm,final task manager location task manager location1 = task manager location future . get ( timeout . to milliseconds ( ) @$ time unit . milliseconds ) ; assert that ( task manager location1 @$ equal to ( task manager location ) ) ;,success trigger target
this set contains <PLACE_HOLDER>,for ( string name : common names ) { voltxml diff child diff = compute diff ( before . find child ( name ) @$ after . find child ( name ) ) ; if ( ! child diff . is empty ( ) ) { result . m_changed elements . put ( name @$ child diff ) ; } } return result ;,set contains duplicates
loop over all jump <PLACE_HOLDER> .,int [ ] jump offsets = switch instruction . jump offsets ; for ( int index = __num__ ; index < jump offsets . length ; index ++ ) { evaluate instruction block ( clazz @$ method @$ code attribute @$ offset + jump offsets [ index ] ) ; },loop jump offsets
in the case there is a directory @$ the has <PLACE_HOLDER> value should not be converted . otherwise @$ if the has <PLACE_HOLDER> value is a file @$ convert it using the path and size values .,return fav . get type ( ) . is file ( ) ? file artifact value . create for normal file using path ( path @$ fav . get size ( ) ) : new has digest . byte string digest ( fav . get value fingerprint ( ) . to byte array ( ) ) ;,the has digest
setting address object line <PLACE_HOLDER> by common convention .,address . set address line ( address_line_0_room_desk_floor @$ address line0 ) ; address . set address line ( address_line_1_number_road_suffix_apt @$ address line1 ) ; address . set address line ( address_line_2_city @$ address line2 ) ; address . set address line ( address_line_3_state_postal_code @$ address line3 ) ; address . set address line ( address_line_4_country @$ address line4 ) ;,address object 3
render state list has special loading <PLACE_HOLDER> @$ so we can leave out the null values,if ( o == null ) { if ( ! name . equals ( __str__ ) ) { element before = current element ; append element ( __str__ ) ; current element = before ; } } else { write ( o @$ o . get class ( ) . get name ( ) @$ null ) ; },list has handling
check whether current am could get container complete <PLACE_HOLDER>,rm app recovered app0 = rm2 . getrm context ( ) . getrm apps ( ) . get ( app0 . get application id ( ) ) ; rm app attempt loaded attempt1 = recovered app0 . get current app attempt ( ) ; assert equals ( __num__ @$ loaded attempt1 . get just finished containers ( ) . size ( ) ) ;,am get information
removals in array list wo n't break <PLACE_HOLDER>,boolean changed = new list . remove ( o ) ; list = collections . unmodifiable list ( new list ) ; return changed ;,removals break sorting
no shebang and no elf @$ use standard <PLACE_HOLDER> .,interpreter = termux service . prefix_path + __str__ ;,shebang use code
due to approximations @$ very close angles can give the same cos <PLACE_HOLDER> we make sure outer cos is bellow inner cos .,if ( ( ( int ) packed angle cos ) == ( ( int ) ( outer angle cos * __num__ ) ) ) { outer angle cos -= __num__ ; } packed angle cos += outer angle cos ; if ( packed angle cos == __num__ ) { throw new illegal argument exception ( __str__ ) ; },angles give object
let user choose email <PLACE_HOLDER>,if ( package name == null ) { for ( intent intent : initial intents ) { grant permission ( context @$ intent @$ intent . get package ( ) @$ attachments ) ; } show chooser ( context @$ initial intents ) ; } else if ( attachment intent . resolve activity ( pm ) != null ) { grant permission ( context @$ attachment intent @$ package name @$ attachments ) ; context . start activity ( attachment intent ) ; } else { acra . log . w ( log_tag @$ __str__ ) ; context . start activity ( resolve intent ) ; },user choose email
skip block if block error has no instruction <PLACE_HOLDER>,continue ;,error has target
bitwise or combines the sign bits so any negative value fails the <PLACE_HOLDER> .,if ( ( index | limit | bytes . length - limit ) < __num__ ) { throw new array index out of bounds exception ( string . format ( __str__ @$ bytes . length @$ index @$ limit ) ) ; } long offset = index ; final long offset limit = limit ; if ( state != complete ) { if ( offset >= offset limit ) { return state ; } int byte1 = ( byte ) state ; if ( byte1 < ( byte ) __num__ ) { if ( byte1 < ( byte ) __num__ || unsafe util . get byte ( bytes @$ offset ++ ) > ( byte ) __num__ ) { return malformed ; } } else if ( byte1,value fails check
the user can specify the hadoop <PLACE_HOLDER>,map < string @$ string > variables = new hash map < string @$ string > ( system . getenv ( ) ) ;,user specify variables
might need to reconsider using collector 's accept <PLACE_HOLDER> for link statistics . we need to convert to <PLACE_HOLDER> window 's <PLACE_HOLDER>stamp . if not @$ it may lead to oom due to mismatch in <PLACE_HOLDER>slots .,long timestamp = time window . refine timestamp ( span . get collector accept time ( ) ) ; if ( parent application . get service type ( ) . is user ( ) ) { if ( logger . is trace enabled ( ) ) { logger . trace ( __str__ @$ parent application @$ span . get agent id ( ) @$ span application @$ span . get agent id ( ) ) ; } final link data map source link data = link data duplex map . get source link data map ( ) ; source link data . add link data ( parent application @$ span . get agent id ( ) @$ span application @$ span . get agent id ( ) @$,collector accept time
the map has no guarantees on order @$ but the lists should match up <PLACE_HOLDER> with key : val associations preserved .,list < string > keys = conf ( fake config ) . keys ( ) ; list < string > vals = conf ( fake config ) . vals ( ) ; for ( int i = __num__ ; i < fake config . size ( ) ; i ++ ) { assert that ( fake config . get ( keys . get ( i ) ) @$ is ( vals . get ( i ) ) ) ; } map < string @$ string > map = conf ( keys @$ vals ) . as map ( ) ; for ( int i = __num__ ; i < fake config . size ( ) ; i ++ ) { assert that ( vals . get ( i,lists match correctly
parameter in select list needs a <PLACE_HOLDER> .,ddl error test ( __str__ @$ __str__ + __str__ + __str__ @$ __str__ ) ;,parameter needs parent
if a thread has acquired the <PLACE_HOLDER> and a close is n't pending then use a deferred close . also decrement <PLACE_HOLDER> use count to signal the last thread that releases the <PLACE_HOLDER> to close it .,if ( ! close pending ) { close pending = true ; fd use count -- ; socket pre close ( ) ; },thread acquired fd
check has min run <PLACE_HOLDER>,memory . get bytes ( filler addr @$ prgm filler bytes ) ; if ( arrays . equals ( prgm filler bytes @$ target filler bytes ) ) { int filler len = __num__ ; string undef data string rep = undefined data . get default value representation ( ) ; address set set = new address set ( current program @$ filler addr @$ current program . get max address ( ) ) ; address iterator addr iter = set . get addresses ( filler addr . next ( ) @$ true ) ; while ( addr iter . has next ( ) ) { address next addr = addr iter . next ( ) ; if ( listing . get undefined data at ( next addr,check run size
fall back to default value suppose column label contains table <PLACE_HOLDER>,if ( column label . contains ( __str__ ) ) { table name = string utils . substring before last ( column label @$ __str__ ) ; },label contains name
ensure the declared dependency only has one possible <PLACE_HOLDER>,if ( coordinates != null && ! coordinates . contains ( bundle dependency coordinate ) ) { if ( coordinates . size ( ) == __num__ ) { final bundle coordinate coordinate = coordinates . stream ( ) . find first ( ) . get ( ) ; final optional < bundle > matching dependency id bundle = get bundle ( coordinate ) ; if ( matching dependency id bundle . is present ( ) ) { final string dependency coordinate str = bundle dependency coordinate . get coordinate ( ) ; logger . warn ( string . format ( __str__ @$ bundle detail . get coordinate ( ) . get coordinate ( ) @$ dependency coordinate str @$ coordinate . get coordinate ( ) ) ) ;,dependency has bundle
but our test framework does n't support that <PLACE_HOLDER> directly .,session old kathmandu time zone offset session = session . builder ( this . session ) . set time zone key ( time_zone_key ) . set start time ( new date time ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ date_time_zone ) . get millis ( ) ) . build ( ) ; time zone key europe warsaw time zone key = get time zone key ( __str__ ) ; date time zone europe warsaw time zone = get date time zone ( europe warsaw time zone key ) ; session europe warsaw session winter = session . builder ( this . session ) . set time zone key ( europe warsaw time zone key ) . set start time ( new date,framework support concept
note : m result handler uses main <PLACE_HOLDER> @$ so this must not be blocked .,m result handler = new handler ( context . get main looper ( ) ) ; m closed = false ;,handler uses looper
this first query retrieves the last version for each file history matching the three requested <PLACE_HOLDER> . however @$ it does not guarantee that this version is indeed the last version in that particular file history @$ so we need another query to verify that .,try ( prepared statement prepared statement = get statement ( __str__ ) ) { prepared statement . set string ( __num__ @$ filecontent checksum ) ; prepared statement . set long ( __num__ @$ size ) ; prepared statement . set timestamp ( __num__ @$ new timestamp ( modified date . get time ( ) ) ) ; try ( result set result set = prepared statement . execute query ( ) ) { collection < partial file history > file histories = new array list < > ( ) ; while ( result set . next ( ) ) { string file history id = result set . get string ( __str__ ) ; partial file history file history = get last version by file history id,three requested letters
do n't let a single processor crash the processor <PLACE_HOLDER> .,m logger . w ( __str__ @$ e ) ;,processor crash thread
only set the badge number if it exists in the style . defaulting it to 0 means the badge will incorrectly show <PLACE_HOLDER> when the user may want a numberless badge .,if ( a . has value ( r . styleable . badge_number ) ) { set number ( a . get int ( r . styleable . badge_number @$ __num__ ) ) ; } set background color ( read color from attributes ( context @$ a @$ r . styleable . badge_background color ) ) ;,badge show bulletin
server 2 will close <PLACE_HOLDER> after creating 10 keys,add listener and put all in client1 . await ( ) ;,server close connection
health check satisfies <PLACE_HOLDER> config health conditions,probe result consumer . accept ( new probe result ( probe factory . get workspace id ( ) @$ probe factory . get machine name ( ) @$ probe factory . get server name ( ) @$ probe status . passed ) ) ;,check satisfies probe
not possible to expand since we have more than one chunk with a single segment . this is the case when user wants to append a segment with coarser granularity . case metadata storage has segments with granularity hour and segments to append have day granularity . druid shard specs does not support multiple <PLACE_HOLDER> for same interval with different granularity .,if ( existing chunks . size ( ) > __num__ ) { throw new illegal state exception ( string . format ( __str__ + __str__ @$ data source @$ segment . get interval ( ) @$ existing chunks . size ( ) ) ) ; },specs support segments
set app submit <PLACE_HOLDER> to be consumable by the am .,application id application id = application . get app attempt id ( ) . get application id ( ) ; environment . put ( application constants . app_submit_time_env @$ string . value of ( rm context . getrm apps ( ) . get ( application id ) . get submit time ( ) ) ) ; credentials credentials = new credentials ( ) ; data input byte buffer dibb = new data input byte buffer ( ) ; byte buffer tokens = container . get tokens ( ) ; if ( tokens != null ) { dibb . reset ( tokens ) ; credentials . read token storage stream ( dibb ) ; tokens . rewind ( ) ; },app submit time
intent is explicit and there 's no <PLACE_HOLDER> . this happens @$ e.g . @$ when a system component sends a broadcast to its own runtime receiver @$ and there 's no manifest <PLACE_HOLDER> for it @$ because this method is called twice for each broadcast @$ for runtime <PLACE_HOLDER> and manifest <PLACE_HOLDER> and the later check would find no <PLACE_HOLDER> .,if ( receivers == null || receivers . size ( ) == __num__ ) { return ; },check find region
this overwrites whatever setting the <PLACE_HOLDER> configured in the properties,adjust auto commit config ( properties @$ offset commit mode ) ;,whatever setting user
add formats which match <PLACE_HOLDER> but the encoding second,if ( input media format . matches without ( mf @$ mime type key @$ encoding key ) ) { formats . add ( matching count @$ mf . append ( input media format ) ) ; } else { formats . add ( mf . append ( input media format ) ) ; },which match everything
app read buffer will extended upto current application buffer size we need to read the existing <PLACE_HOLDER> into dst before we can do unwrap again . if there are no space in dst we can break here .,if ( dst . has remaining ( ) ) read += read from app buffer ( dst ) ; else break ;,app read contents
file status.equals only compares path field @$ must explicitly compare all <PLACE_HOLDER>,assert . assert false ( src status . get permission ( ) . equals ( dst status . get permission ( ) ) ) ; assert . assert false ( src status . get owner ( ) . equals ( dst status . get owner ( ) ) ) ; assert . assert false ( src status . get group ( ) . equals ( dst status . get group ( ) ) ) ; assert . assert true ( src status . get access time ( ) == dst status . get access time ( ) ) ; assert . assert true ( src status . get modification time ( ) == dst status . get modification time ( ) ) ;,status.equals compare fields
sdk default policy should honor the <PLACE_HOLDER> config level max error retry,test actual retries ( client_config_level_max_retry ) ;,policy honor client
run the do fn <PLACE_HOLDER> now that all side inputs are ready .,for ( bag state < windowed value < inputt > > elements bag : elements bags ) { iterable < windowed value < inputt > > elements = elements bag . read ( ) ; for ( windowed value < inputt > elem : elements ) { simple do fn runner . process element ( elem ) ; } elements bag . clear ( ) ; } side input fetcher . release blocked windows ( ready windows ) ;,the do runner
asif : remove the <PLACE_HOLDER> from active map so that destroy can be called on it . we will first collect all the <PLACE_HOLDER>s which have timed out & then expire them . in that gap even if the client genuinely closes the <PLACE_HOLDER> @$ it will not be returned to the pool as the active map no longer contains it,if ( ( now - then ) > time out ) { this . active cache . remove ( conn ) ; this . expired conns . add ( conn ) ; ++ num conn timed out ; } else { sleep time = then + time out - now ; to continue = false ; },genuinely closes connection
ugly but this is <PLACE_HOLDER> hive uses internally anyway,try { class . for name ( driver name ) ; } catch ( class not found exception ex ) { throw new runtime exception ( ex ) ; },hive uses what
the last rollback should cause the 1 st <PLACE_HOLDER> to get sent to the dlq,m = ( text message ) consumer . receive ( __num__ ) ; assert not null ( m ) ; assert equals ( __str__ @$ m . get text ( ) ) ; session . commit ( ) ;,rollback cause message
if the caller did n't specify an explicit time <PLACE_HOLDER> @$ we want to continue tracking under any it has .,if ( r . app time tracker == null && source record != null ) { r . app time tracker = source record . app time tracker ; },caller specify tracker
making an uncoordinated call @$ which initialize the <PLACE_HOLDER> to observer node .,dfs . get client ( ) . getha service state ( ) ; dfs . mkdir ( test path @$ fs permission . get default ( ) ) ; assert sent to ( __num__ ) ; thread reader = new thread ( ( ) -> { try { dfs . get file status ( test path ) ; read status . set ( __num__ ) ; } catch ( io exception e ) { e . print stack trace ( ) ; read status . set ( - __num__ ) ; } } ) ;,which initialize connection
check if consumer set fail <PLACE_HOLDER> @$ then let the test fail .,if ( null != fail msg ) { log . error ( fail msg ) ; fail ( fail msg ) ; },set fail msg
first result set has results @$ second has <PLACE_HOLDER>,when ( result set . next ( ) ) . then return ( true ) . then return ( false ) ; when ( result set . get string ( constants . col_trigger_type ) ) . then return ( constants . ttype_simple ) ; operable trigger trigger = jdbc delegate . select trigger ( conn @$ trigger key . trigger key ( __str__ ) ) ; assert null ( trigger ) ; verify ( persistence delegate ) . load extended trigger properties ( any ( connection . class ) @$ any ( trigger key . class ) ) ;,set has results
string all matches fruit <PLACE_HOLDER>,do xquery test ( xml_snippet @$ __str__ @$ arrays . as list ( fruit names ) ) ;,all matches names
the passive provider can not be disabled @$ but the passive provider did n't exist in previous versions of this shadow . for backwards compatibility @$ we let the passive provider be disabled . this also help emulate the situation where an app only has coarse permissions @$ <PLACE_HOLDER> this shadow normally ca n't emulate .,if ( passive_provider . equals ( name ) ) { passive provider enabled = enabled ; return ; },shadow emulate which
one from a <PLACE_HOLDER> if one exists . if there is a newer <PLACE_HOLDER> then use that <PLACE_HOLDER> .,try { get envelope ( ) ; } catch ( soap exception e ) { } return document . get document element ( ) ;,one use one
we ca n't use the same tactic as for intersection since abstract collection only does a <PLACE_HOLDER> on the first element it encounters .,comparator < t > number comparator = new number aware comparator < t > ( ) ; if ( nlgn sort && ( head instanceof comparable ) ) { set < t > answer ; if ( head instanceof number ) { answer = new tree set < t > ( number comparator ) ; answer . add all ( self ) ; for ( t t : self ) { if ( t instanceof number ) { for ( object t2 : remove me ) { if ( t2 instanceof number ) { if ( number comparator . compare ( t @$ ( t ) t2 ) == __num__ ) answer . remove ( t ) ; } } } else { if ( remove me .,collection does remove
cool union should have a byte data <PLACE_HOLDER> as the last component,dtc = union . get component ( __num__ ) ; assert true ( dtc . get data type ( ) . is equivalent ( new byte data type ( ) ) ) ; dtc = union . get component ( __num__ ) ; assert equals ( s @$ dtc . get data type ( ) ) ;,union have type
track the <PLACE_HOLDER>erver reported online <PLACE_HOLDER> in memory .,synchronized ( rs reports ) { rs reports . put ( server name @$ region names ) ; } if ( region names . is empty ( ) ) { log . trace ( __str__ @$ server name ) ; return ; },regionserver reported region
the database to query from db the columns to return from the query from db the columns for the where clause the values for the where clause do n't <PLACE_HOLDER> the rows do n't filter by row <PLACE_HOLDER>s the sort order,cursor cursor = query builder . query ( db @$ projection @$ selection @$ selection args @$ null @$ null @$ sort order ) ;,clause do some
the newly effective conf does not have <PLACE_HOLDER> 0 and <PLACE_HOLDER> 2 .,string [ ] effective volumes = dn . get conf ( ) . get ( dfs_datanode_data_dir_key ) . split ( __str__ ) ; assert equals ( __num__ @$ effective volumes . length ) ; for ( string ev : effective volumes ) { assert that ( new file ( storage location . parse ( ev ) . get uri ( ) ) . get canonical path ( ) @$ is ( not ( any of ( is ( new dirs . get ( __num__ ) ) @$ is ( new dirs . get ( __num__ ) ) ) ) ) ) ; },conf have rs
all splits must have the same query <PLACE_HOLDER>,set < string > actual = split completed events . stream ( ) . map ( split completed event :: get query id ) . collect ( to set ( ) ) ; assert equals ( actual @$ immutable set . of ( query completed event . get metadata ( ) . get query id ( ) ) ) ;,splits have id
the instrumentation runner requires the package <PLACE_HOLDER> of the test,args . add ( __str__ @$ get test package ( ) ) ; args . add ( __str__ @$ get target package ( ) ) ; args . add ( __str__ @$ get test runner ( ) ) ; args . add ( __str__ @$ get path to adb executable ( ) ) ; if ( get test filter ( ) . is present ( ) ) { args . add ( __str__ @$ __str__ + get test filter ( ) . get ( ) ) ; } for ( map . entry < string @$ string > arg pair : get environment overrides ( ) . entry set ( ) ) { args . add ( __str__ @$ string . format ( locale . us @$ __str__,runner requires name
do a temp tag <PLACE_HOLDER> on app 2,eph atm . clean temp containers ( test utils . get mock application id ( __num__ ) ) ; assert . assert equals ( __num__ @$ atm . get node cardinality by op ( node id . from string ( __str__ ) @$ allocation tags . create single app allocation tags ( test utils . get mock application id ( __num__ ) @$ immutable set . of ( __str__ ) ) @$ long :: sum ) ) ;,temp tag operation
overcautiousness : each event logger has its own events list @$ so one bad test wo n't spoil <PLACE_HOLDER> .,m logger events = new array list < > ( ) ; m logger = new event logger ( m logger events ) ; local broadcast manager . register receiver ( m logger @$ intent filter ) ;,test spoil them
window frames may have changed . tell the input <PLACE_HOLDER> about it .,m input monitor . layout input consumers ( dw @$ dh ) ; m input monitor . set update input windows needed lw ( ) ; if ( update input windows ) { m input monitor . update input windows lw ( false ) ; },window tell monitor
this from element will be rendered as part of the origins join <PLACE_HOLDER>,from element . set text ( __str__ ) ; from elements . add ( from element ) ;,part join tree
previously a update readers twice in a row would cause an <PLACE_HOLDER> . in test this would also normally cause an <PLACE_HOLDER> because scan.store is null . so as long as we get through these two calls we are good and the bug was quashed .,try ( store scanner scan = new store scanner ( new scan ( ) @$ scan info @$ get cols ( __str__ @$ __str__ ) @$ scanners ) ) { scan . update readers ( collections . empty list ( ) @$ collections . empty list ( ) ) ; scan . update readers ( collections . empty list ( ) @$ collections . empty list ( ) ) ; scan . peek ( ) ; },readers cause exception
generate the map join <PLACE_HOLDER>,return map join processor . convertsmb join to map join ( physical context . get conf ( ) @$ newsmb join op @$ map join pos @$ true ) ;,map join processor
we 've already found a constructor match before @$ this means that power mock can not determine which method to expect since there are two methods with the same name and the same number of arguments but one is using wrapper <PLACE_HOLDER> .,throw exception when multiple constructor matches found ( new java . lang . reflect . constructor [ ] { potential constructor . get java constructor ( ) @$ constructor . get java constructor ( ) } ) ;,one using types
java does n't allow simple <PLACE_HOLDER> of resources as directories when the resources are inside a jar file . this searches the contents of the jar to get the list,try { url class url = jar hash . class . get resource ( __str__ ) ; if ( class url != null && class url . get protocol ( ) . equals ( __str__ ) ) { string jar path = class url . get path ( ) . substring ( __num__ @$ class url . get path ( ) . index of ( __str__ ) ) ; jar file jar = new jar file ( url decoder . decode ( jar path @$ __str__ ) ) ; enumeration < jar entry > files = jar . entries ( ) ; while ( files . has more elements ( ) ) { string f name = files . next element ( ) . get name ( ),java allow lookup
shut down the platform when user closed log <PLACE_HOLDER> .,platform . run later ( ( ) -> { platform . set implicit exit ( true ) ; launcher . stop without platform ( ) ; } ) ;,user closed logs
method name can not use reserved <PLACE_HOLDER> @$ e.g . return,if ( is reserved word ( sanitized operation id ) ) { logger . warn ( operation id + __str__ + underscore ( __str__ + operation id ) ) ; sanitized operation id = __str__ + sanitized operation id ; } return camelize ( sanitized operation id ) ;,name use keyword
if this is a segment expression @$ it will already have been dealt with in the <PLACE_HOLDER> of the memory access during the call to translate children of node . just return the <PLACE_HOLDER> .,if ( is segment expression ( current node value ) ) { return partial results . get ( __num__ ) ; } else if ( is memory access ( current node value ) ) { return process simple memory access ( environment @$ segment override @$ size @$ load operand @$ partial results . get ( __num__ ) ) ; } else { throw new internal translation exception ( __str__ ) ; },children return result
tests might add scm <PLACE_HOLDER> to indicate which scm should be used to find credential we have to do this because api url might be of wire mock server and not github,if ( api url . starts with ( git hubscm source . github_url ) || ( string utils . is not blank ( scm id ) && scm id . equals ( github scm . id ) ) ) { scm = new github scm ( new reachable ( ) { @ override public link get link ( ) { preconditions . check not null ( organization ) ; return organization . get link ( ) . rel ( __str__ ) ; } } ) ; } else { scm = new github enterprise scm ( ( new reachable ( ) { @ override public link get link ( ) { preconditions . check not null ( organization ) ; return organization . get link ( ) .,tests add id
the ri silently swallows <PLACE_HOLDER> @$ but android has always logged .,system . loge ( __str__ @$ ex ) ;,ri swallows exceptions
no deployment defined repository @$ use the <PLACE_HOLDER>,if ( job repository name != null ) { service builder . add dependency ( support . get capability service name ( capabilities . job_repository_capability . get name ( ) @$ job repository name ) @$ job repository . class @$ service . get job repository injector ( ) ) ; } else { service . get job repository injector ( ) . set value ( new immediate value < > ( job repository ) ) ; },deployment use default
client should have noticed the <PLACE_HOLDER>,assert that ( __str__ @$ client socket . error latch . await ( __num__ @$ seconds ) @$ is ( true ) ) ; assert that ( __str__ @$ client socket . error . get ( ) @$ instance of ( message too large exception . class ) ) ;,client noticed failure
default all lyrics 3 fields to sa<PLACE_HOLDER>e . id 3 <PLACE_HOLDER> 1 fields are indi<PLACE_HOLDER>idual settings . id 3 <PLACE_HOLDER> 2 fields are always looked at to sa<PLACE_HOLDER>e .,iterator < string > iterator = lyrics3v2 fields . get instance of ( ) . get id to value map ( ) . key set ( ) . iterator ( ) ; string field id ; while ( iterator . has next ( ) ) { field id = iterator . next ( ) ; lyrics3 save field map . put ( field id @$ true ) ; } try { add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword ( frame bodycomm . class @$ __str__ ) ; add keyword,all id v
do n't exceed max file compact threshold <PLACE_HOLDER> : file selection starts with largest to smallest .,compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( __num__ @$ __num__ @$ __num__ @$ max size - __num__ ) @$ __num__ @$ __num__ @$ __num__ ) ; compact equals ( sf create ( max size - __num__ @$ max size - __num__ @$ max size -,max file note
the new code browser selection should have <PLACE_HOLDER> of our vertex selections,assert true ( code browser selection . get min address ( ) . equals ( first selection . get min address ( ) ) ) ; assert true ( code browser selection . get max address ( ) . equals ( second selection . get max address ( ) ) ) ;,selection have one
legacy parser was signaling all created index antlr is parsing only those @$ which will make any model <PLACE_HOLDER>,int number of created indexes which not make change on tables model = __num__ ; int number of alter view statements = __num__ ; int number of dropped views = __num__ ; assert that ( listener . total ( ) ) . is equal to ( __num__ - number of altered tables which does not exists - number of indexes on non existing tables - number of created indexes which not make change on tables model + number of alter view statements + number of dropped views ) ; listener . for each ( this :: print event ) ;,which make change
set the usage analyzer as parent to make sure that the usage log contains the subclass <PLACE_HOLDER> .,logger . set parent ( logger . get logger ( resource usage analyzer . class . get name ( ) ) ) ;,log contains class
the splits have an implicit <PLACE_HOLDER> on the base apk . this means that we have to add the base apk file in addition to the shared libraries .,string base apk name = new file ( info . get base code path ( ) ) . get name ( ) ; string base class path = base apk name ;,splits have getter
offline compaction should not have created a new <PLACE_HOLDER> .,assert equals ( original if length @$ if file . length ( ) ) ; connectd sand cache ( ) ; dsf = cache . create disk store factory ( ) ; disk store = dsf . create ( name ) ; af = new attributes factory ( ) ; af . set disk store name ( name ) ; af . set data policy ( data policy . persistent_replicate ) ; r = cache . create region ( __str__ @$ af . create ( ) ) ; assert equals ( __num__ @$ r . size ( ) ) ; assert equals ( __str__ @$ r . get ( __str__ ) ) ; assert equals ( __str__ @$ r . get ( __str__ ) ) ;,compaction created store
triggers update system ui visibility lw which will reset the <PLACE_HOLDER> as needed,int finish post layout policy lw = m display policy . focus changed lw ( m window @$ m window ) ; assert equals ( window manager policy . finish_layout_redo_layout @$ finish post layout policy lw ) ; assert equals ( __num__ @$ m display policy . m last system ui flags ) ; assert equals ( __num__ @$ m window . m attrs . system ui visibility ) ; assert inset by top bottom ( m window . get content frame lw ( ) @$ status_bar_height @$ nav_bar_height ) ;,which reset ui
top left top <PLACE_HOLDER> bottom left bottom <PLACE_HOLDER>,matrix . set poly to poly ( new float [ ] { __num__ @$ __num__ @$ width @$ __num__ @$ __num__ @$ height @$ width @$ height } @$ __num__ @$ m display orientation == __num__ ? new float [ ] { __num__ @$ height @$ __num__ @$ __num__ @$ width @$ height @$ width @$ __num__ } : new float [ ] { width @$ __num__ @$ width @$ height @$ __num__ @$ __num__ @$ __num__ @$ height } @$ __num__ @$ __num__ ) ;,top left right
if src area is beyond the bounds of the image @$ we must clip it . the transform is based on the specified area @$ not the clipped <PLACE_HOLDER> .,if ( sx1 < __num__ ) { sx1 = __num__ ; } else if ( sx1 > img width ) { sx1 = img width ; },the clipped area
embms temp files can contain arbitrary <PLACE_HOLDER> .,return __str__ ;,files contain paths
next load keystore data to build <PLACE_HOLDER> 's,key store ks = key store . get instance ( key store . get default type ( ) ) ; ks . load ( new file input stream ( system . get property ( __str__ @$ __str__ ) + file . separator char + __str__ ) @$ store password ) ; no_store_domain = new protection domain ( new code source ( new url ( __str__ ) @$ ( java . security . cert . certificate [ ] ) null ) @$ null @$ null @$ null ) ;,data build password
add conference invite <PLACE_HOLDER> .,invite button = ui component registry . get button factory ( ) . create invite conference button ( ) ; invite button . set tool tip text ( res . get string ( __str__ ) ) ; chat room . add chat room button ( invite button ) ; invite button . add action listener ( this ) ;,conference invite button
try to encode two c es as one ce <PLACE_HOLDER> .,if ( ces length == __num__ ) { long ce0 = ces [ __num__ ] ; long ce1 = ces [ __num__ ] ; long p0 = ce0 > > > __num__ ; if ( ( ce0 & __num__ ) == collation . common_secondary_ce && ( ce1 & __num__ ) == collation . common_tertiary_ce && p0 != __num__ ) { return ( int ) p0 | ( ( ( int ) ce0 & __num__ ) << __num__ ) | ( ( ( int ) ce1 > > __num__ ) & __num__ ) | collation . special_ce32_low_byte | collation . latin_expansion_tag ; } },one ce one
if header does n't exist @$ method get <PLACE_HOLDER> creates a new random <PLACE_HOLDER>,if ( response packet . get message ( ) . get headers ( ) . get ( av . messageid tag @$ false ) == null ) { string newid = message . generate messageid ( ) ; hl . add ( new string header ( av . messageid tag @$ newid ) ) ; },id creates id
perform cache directive operations using a closed file <PLACE_HOLDER> .,distributed file system dfs1 = ( distributed file system ) cluster . get new file system instance ( __num__ ) ; dfs1 . close ( ) ; try { dfs1 . list cache directives ( null ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic test utils . assert exception contains ( __str__ @$ ioe ) ; } try { dfs1 . add cache directive ( alpha ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic test utils . assert exception contains ( __str__ @$ ioe ) ; } try { dfs1 . modify cache directive ( alpha ) ; fail ( __str__ ) ; } catch ( io exception ioe ) { generic,operations using system
if we 're copying in a model we need a model schema v <PLACE_HOLDER> of the right class to fill into .,model m = model metrics . model ( ) ; if ( m != null ) { this . model = new model keyv3 ( m . _key ) ; this . model_category = m . _output . get model category ( ) ; this . model_checksum = m . checksum ( ) ; },schema v 3
the data is locked in the <PLACE_HOLDER> @$ or we 're ignoring the <PLACE_HOLDER> . bypass the <PLACE_HOLDER> and read from upstream .,if ( next span == null ) { next data source = upstream data source ; next data spec = new data spec ( uri @$ http method @$ http body @$ read position @$ read position @$ bytes remaining @$ key @$ flags ) ; } else if ( next span . is cached ) { uri file uri = uri . from file ( next span . file ) ; long file position = read position - next span . position ; long length = next span . length - file position ; if ( bytes remaining != c . length_unset ) { length = math . min ( length @$ bytes remaining ) ; } next data spec = new data spec ( file uri,cache bypass timeout
case 23 : true if an attribute named primitive int att name of type int has a <PLACE_HOLDER> equal to int <PLACE_HOLDER> multiplicated by one we cover javax.management.binary rel query exp with a rel op equal to times,queries . add ( query . eq ( query . attr ( primitive int att name ) @$ query . times ( query . value ( int value ) @$ query . value ( __num__ ) ) ) ) ;,name has value
check that older snapshot still have the old ec policy <PLACE_HOLDER>,assert null ( __str__ @$ fs . get erasure coding policy ( snap1 ) ) ; assert equals ( __str__ @$ ec63 policy @$ fs . get erasure coding policy ( snap2 ) ) ; assert null ( __str__ @$ fs . get erasure coding policy ( snap3 ) ) ;,snapshot have settings
run <PLACE_HOLDER> without indexes run all <PLACE_HOLDER> .,for ( int i = __num__ ; i < queries . length ; i ++ ) { try { results [ i ] [ __num__ ] = qs . new query ( __str__ + queries [ i ] ) . execute ( ) ; } catch ( exception e ) { throw new runtime exception ( __str__ + queries [ i ] @$ e ) ; } } qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$ __str__ + region name + __str__ ) ; qs . create index ( __str__ @$ __str__ @$,queries run queries
done is always true until the last stream has be put in the queue . then the stream should be spouting good input <PLACE_HOLDER> .,synchronized ( done ) { return ! done . get ( ) || ! queue . is empty ( ) ; } if ( fail . get ( ) != null ) { throw new re ( fail . get ( ) ) ; } try { return dequeue ( ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; throw new runtime exception ( e ) ; },stream spouting streams
the configuration provider will return null instead of throwing out exceptions @$ if there are no configuration <PLACE_HOLDER> provided . rm will not load the remote configuration <PLACE_HOLDER> @$ and should start successfully .,try { rm = new mockrm ( configuration ) ; rm . init ( configuration ) ; rm . start ( ) ; } catch ( exception ex ) { fail ( __str__ ) ; },rm load files
now c 2 should replace c <PLACE_HOLDER> for source tag <PLACE_HOLDER> .,pcm . add constraint ( app id1 @$ source tag1 @$ c2 @$ true ) ; assert . assert equals ( __num__ @$ pcm . get constraints ( app id1 ) . size ( ) ) ; assert . assert equals ( c2 @$ pcm . get constraint ( app id1 @$ source tag1 ) ) ;,c replace 1
synchronized search improves <PLACE_HOLDER> we must create and add new value if there are no needed entry,synchronized ( this . queue ) { current = get entry value ( key @$ hash @$ this . table [ index ( hash @$ this . table ) ] ) ; if ( current != null ) { return current ; } v value = create ( key ) ; objects . require non null ( value @$ __str__ ) ; int index = index ( hash @$ this . table ) ; this . table [ index ] = new cache entry < > ( hash @$ key @$ value @$ this . table [ index ] ) ; if ( ++ this . size >= this . threshold ) { if ( this . table . length == maximum_capacity ) { this . threshold =,search improves stability
setting volume on ui sounds stream type also controls silent <PLACE_HOLDER>,if ( ( ( flags & audio manager . flag_allow_ringer_modes ) != __num__ ) || ( stream == get ui sounds stream type ( ) ) ) { set ringer mode ( get new ringer mode ( stream @$ index @$ flags ) @$ tag + __str__ @$ false ) ; },type controls mode
grid near tx prepare <PLACE_HOLDER> unmarshalling failed test,read cnt . set ( __num__ ) ; fail optimistic ( ) ;,grid prepare request
system can retrieve the <PLACE_HOLDER> .,m context . binder . clear calling identity ( ) ; assert equals ( bug report request time @$ dpm . get last bug report request time ( ) ) ;,system retrieve timestamp
the old rule references another <PLACE_HOLDER> instance and we wa n't to keep the invariant that every rule references the <PLACE_HOLDER> it is contained within,try { rule new rule = builder . create rule ( rule . get label ( ) @$ rule . get rule class object ( ) @$ rule . get location ( ) @$ rule . get attribute container ( ) ) ; new rule . populate output files ( null event handler . instance @$ builder ) ; if ( rule . contains errors ( ) ) { new rule . set contains errors ( ) ; } builder . add rule ( new rule ) ; } catch ( label syntax exception e ) { throw new illegal state exception ( e ) ; },rule references object
make assignments . the host on top of the set has the smallest total <PLACE_HOLDER>,for ( string id : scores . key set ( ) ) { server assignment least loaded = host scores . get ( __num__ ) ; least loaded . score += scores . get ( id ) ; balanced assignments . put ( id @$ least loaded . host ) ; host scores . sort ( comparator . comparing long ( o -> o . score ) ) ; } return balanced assignments ;,host has score
since derby database which is used for unit test does not have <PLACE_HOLDER> in date and time type @$ and put sql converts date string into long representation using local <PLACE_HOLDER> @$ we need to use local <PLACE_HOLDER> .,simple date format time format = new simple date format ( time format string ) ; java . util . date parsed local time = time format . parse ( time str ) ; simple date format date format = new simple date format ( date format string ) ; java . util . date parsed local date = date format . parse ( date str ) ;,database have format
saving the tool saves the <PLACE_HOLDER>,tool = save tool ( env . get project ( ) @$ tool ) ;,tool saves process
we only register the app advanced <PLACE_HOLDER> as all of the infinispan ones are explicitly defined,add reflection for class ( advanced externalizer . class @$ true @$ app only index @$ reflective class @$ collections . empty set ( ) ) ;,app advanced extensions
exclude locals which do n't have valid <PLACE_HOLDER>,if ( ! var . is valid ( ) ) { continue ; },which have variables
delete local file to make sure that the get requests <PLACE_HOLDER> from ha,file blob file = server . get storage location ( job id @$ key ) ; assert true ( blob file . delete ( ) ) ;,the get work
batch @$ legacy @$ jdk <PLACE_HOLDER>,options . set streaming ( false ) ; system . set property ( __str__ @$ __str__ ) ; assert that ( get container image for job ( options ) @$ equal to ( __str__ ) ) ;,batch jdk version
system processes can do <PLACE_HOLDER> @$ and when they do we want to have them trim their memory after the user leaves the <PLACE_HOLDER> . to facilitate this @$ here we need to determine whether or not it is currently showing <PLACE_HOLDER> .,app . system no ui = true ; if ( app == top_app ) { app . system no ui = false ; app . set current scheduling group ( process list . sched_group_top_app ) ; app . adj type = __str__ ; } else if ( app . has top ui ( ) ) { app . system no ui = false ; app . adj type = __str__ ; } else if ( wpc . has visible activities ( ) ) { app . system no ui = false ; },processes do ui
pi array node implements array length <PLACE_HOLDER> and value proxy . we want to treat it as an array length <PLACE_HOLDER> @$ therefore we check this case first .,do { if ( current instanceof array length provider ) { return ( ( array length provider ) current ) . find length ( mode @$ constant reflection ) ; } else if ( current instanceof value phi node ) { return phi array length ( ( value phi node ) current @$ mode @$ constant reflection ) ; } else if ( current instanceof value proxy node ) { value proxy node proxy = ( value proxy node ) current ; value node length = array length ( proxy . get original node ( ) @$ mode @$ constant reflection ) ; if ( mode == array length provider . find length mode . canonicalize_read && length != null && ! length . is constant ( ),node implements provider
make a store file and write <PLACE_HOLDER> to it .,store file writer writer = new store file writer . builder ( conf @$ cache conf @$ fs ) . with file path ( f ) . with file context ( meta ) . build ( ) ; write store file ( writer ) ; writer . close ( ) ; reader context context = new reader context builder ( ) . with file system and path ( fs @$ f ) . build ( ) ; h file info file info = new h file info ( context @$ conf ) ; store file reader reader = new store file reader ( context @$ file info @$ cache conf @$ new atomic integer ( __num__ ) @$ conf ) ; file info . init meta and index,store file data
this can only happen for a throwable thats not an exception @$ i.e . an <PLACE_HOLDER>,if ( ! success && ! state changed ) { unlock ( ) ; },throwable thats exception
make sure both servers can deserialize the <PLACE_HOLDER>,serverb . invoke ( ( ) -> assert equals ( new simple class ( __num__ @$ ( byte ) __num__ ) @$ get cache ( ) . get region ( __str__ ) . get ( __str__ ) ) ) ; servera . invoke ( ( ) -> assert equals ( new simple class ( __num__ @$ ( byte ) __num__ ) @$ get cache ( ) . get region ( __str__ ) . get ( __str__ ) ) ) ;,servers deserialize type
assertion on auto onboard another dummy data <PLACE_HOLDER>,metadata source config another dummymd source = ds to onboards map . get ( __str__ ) . get ( __num__ ) . get metadata source config ( ) ; assert . assert equals ( another dummymd source . get class name ( ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . size ( ) @$ __num__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __str__ ) ; assert . assert equals ( another dummymd source . get properties ( ) . get ( __str__ ) @$ __num__ ) ;,assertion onboard source
bacause script pid utils.is pid running do n't support <PLACE_HOLDER>,if ( ! os . is family unix ( ) || os . is family mac ( ) ) { return ; } final int shard count = __num__ ; final string job name = __str__ ; job config job config = new job config ( ) ; job config . set job name ( job name ) ; job config . set cron ( __str__ ) ; job config . set job type ( job type . shell_job . to string ( ) ) ; job config . set job class ( longtime java job . class . get canonical name ( ) ) ; job config . set timeout seconds ( __num__ ) ; job config . set sharding total count ( shard count ) ;,utils.is support mac
drawable container ' constant state has drawables <PLACE_HOLDER> . in order to leave the constant state intact in the cache @$ we need to create a new drawable container after added to cache .,if ( dr instanceof drawable container ) { needs new drawable after cache = true ; },state drawables impl
if overwritten not specified @$ it should be <PLACE_HOLDER> user specified,verify queue mapping ( new queue mapping ( mapping type . group @$ __str__ @$ __str__ @$ __str__ ) @$ __str__ @$ __str__ @$ __str__ @$ true ) ;,user specified which
0 x 100415 a : p 1 and p 2 have same plate <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . plate_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . plate_comment ) ; program merge = new program merge manager ( p1 @$ p2 @$ task monitor adapter . dummy_monitor ) ; program merge . set diff filter ( new program diff filter ( program diff filter . comment_diffs ) ) ; program merge . set merge filter ( new program merge filter ( program merge filter . comments @$ program merge filter . merge ) ) ; address set as = new address set ( ) ; as . add range ( addr ( __num__ ) @$ addr ( __num__ ) ) ; as . add range (,0 have comments
no key id specified @$ return no <PLACE_HOLDER>,if ( sought keyid == null ) { return mono . just ( collections . empty list ( ) ) ; },id return results
a user in role contractor can only do a <PLACE_HOLDER> if confidential,response = _connector . get response ( __str__ + __str__ + encoded chris + __str__ + __str__ ) ; assert that ( response @$ starts with ( __str__ ) ) ; assert that ( response @$ contains string ( __str__ ) ) ;,user do request
if the callback executes <PLACE_HOLDER> @$ we close the dialog,if ( callback != null ) { close = callback . execute ( old master password @$ get new password ( ) ) ; } if ( close ) { dialog = null ; dispose ( ) ; },callback executes ok
p 2 post comments contain the p 1 comment <PLACE_HOLDER> .,program builder1 . create comment ( __str__ @$ __str__ @$ code unit . post_comment ) ; program builder2 . create comment ( __str__ @$ __str__ @$ code unit . post_comment ) ; check comment difference ( __num__ ) ;,comments contain string
we fake only once the expired iterator exception at the specified get records attempt <PLACE_HOLDER>,if ( ( integer . value of ( shard iterator ) == order of call to expire - __num__ ) && ! expired once already ) { expired once already = true ; throw new expired iterator exception ( __str__ ) ; } else if ( expired once already && ! expired iterator refreshed ) { throw new runtime exception ( __str__ ) ; } else { return new get records result ( ) . with records ( shard itr to record batch . get ( shard iterator ) ) . with millis behind latest ( millis behind latest ) . with next shard iterator ( ( integer . value of ( shard iterator ) == total num of get records calls - __num__ ) ? null :,records attempt time
we use recursive algorithm with pure complexity and do n't want it to take forever stacked barcode can have up to 11 <PLACE_HOLDER> @$ so 25 seems reasonable enough,if ( this . rows . size ( ) > __num__ ) { this . rows . clear ( ) ; return null ; },barcode have rows
this flag will be set to false after first incremental load is done . this flag is used by repl copy task to check if duplicate file check is required or not . this flag is used by compaction to check if compaction can be done for this database or not . if compaction is done before first incremental then duplicate check will fail,parameters . put ( repl utils . repl_first_inc_pending_flag @$ __str__ ) ; return parameters ;,compaction change rule
old line spacing . all lines should get their <PLACE_HOLDER> and descents from the first font .,static layout layout = static layout . builder . obtain ( text @$ __num__ @$ text . length ( ) @$ paint @$ para width ) . set include pad ( false ) . set use line spacing from fallbacks ( false ) . build ( ) ; assert equals ( __num__ @$ layout . get line count ( ) ) ; assert equals ( - text size @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * text size @$ layout . get line descent ( __num__ ) ) ; assert equals ( - text size @$ layout . get line ascent ( __num__ ) ) ; assert equals ( __num__ * text size @$ layout . get line descent (,lines get ascent
m device idle controller.step idle state locked does n't handle the active <PLACE_HOLDER> @$ so the state should stay as active .,verify state conditions ( state_active ) ;,state handle state
jwt proxy container does n't need proxy <PLACE_HOLDER> since it never does any outbound requests @$ and setting of it may fail accessing internal addresses .,k8s env . get pods data ( ) . entry set ( ) . stream ( ) . filter ( entry -> ! entry . get key ( ) . equals ( jwt_proxy_pod_name ) ) . flat map ( entry -> stream . concat ( entry . get value ( ) . get spec ( ) . get containers ( ) . stream ( ) @$ entry . get value ( ) . get spec ( ) . get init containers ( ) . stream ( ) ) ) . for each ( container -> proxy env vars . for each ( ( k @$ v ) -> container . get env ( ) . add ( new env var ( k @$ v @$ null ) ),container need access
zero distance means a <PLACE_HOLDER>,if ( distance == __num__ ) { byte x = input [ in offset + ip - __num__ ] ; while ( ip < ip bound ) { if ( input [ in offset + ref ++ ] != x ) { break ; } else { ip ++ ; } } } else { for ( ; ; ) { if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ] ) { break ; } if ( input [ in offset + ref ++ ] != input [ in offset + ip ++ ],distance means null
new user gets registration <PLACE_HOLDER>,new user . set activation key ( random util . generate activation key ( ) ) ; set < authority > authorities = new hash set < > ( ) ; authority repository . find by id ( authorities constants . user ) . if present ( authorities :: add ) ; new user . set authorities ( authorities ) ; user repository . save ( new user ) ; log . debug ( __str__ @$ new user ) ; return new user ;,user gets key
we can leave the nested loop . every <PLACE_HOLDER> in the query can match at most one <PLACE_HOLDER> in the term .,term character match found = true ;,character match character
some types do n't have a static <PLACE_HOLDER> @$ that is ok,have looked for static initializer = true ;,types have initializer
if remote client have <PLACE_HOLDER>,if ( local video port > __num__ ) { if ( settings manager . get local preferences ( ) . get video device ( ) != null && ! __str__ . equals ( settings manager . get local preferences ( ) . get video device ( ) ) ) { video media session video media session = media manager . create video media session ( call . get remote sdp description ( ) . to string ( ) @$ local video port ) ; if ( video media session != null ) { video media session . start trasmit ( ) ; video media session . start receive ( ) ; call . set video media session ( video media session ) ; } } } evt .,client have video
h<PLACE_HOLDER>yes modem with dos configur<PLACE_HOLDER>tor,hayes . accept ( con dos ) ;,instructions addis o
our randomly generated <PLACE_HOLDER> significantly .,return name . length ( ) == __num__ ;,our generated name
in a <PLACE_HOLDER> class @$ this.chardata has one <PLACE_HOLDER> @$ that is to say @$ a pair of surrogates is composed and stored to this.chardata .,if ( this . context == s_inbrackets ) { switch ( ch ) { case __str__ : ret = t_backsolidus ; if ( this . offset >= this . regexlen ) throw ex ( __str__ @$ this . offset - __num__ ) ; this . chardata = this . regex . char at ( this . offset ++ ) ; break ; case __str__ : if ( this . offset < this . regexlen && this . regex . char at ( this . offset ) == __str__ ) { this . offset ++ ; ret = t_xmlschema_cc_subtraction ; } else ret = t_char ; break ; case __str__ : if ( ! this . is set ( regular expression . xmlschema_mode ) && this . offset <,this.chardata has character
j 2 se does not support xalan <PLACE_HOLDER>,sax transformer factory stf = ( sax transformer factory ) tfactory ;,se support interpretive
a change in priority is only relevant for static rr os : specifically @$ a regular rro should not have its state <PLACE_HOLDER> only because a change in priority,if ( the truth . is static overlay package ( ) && the truth . overlay priority != old settings . priority ) { return true ; } return false ;,rro have change
emergency calling requires voice <PLACE_HOLDER> .,if ( m is voice capable ) { if ( is in call ( ) ) { visible = true ; } else { final boolean sim locked = keyguard update monitor . get instance ( m context ) . is sim pin voice secure ( ) ; if ( sim locked ) { visible = m enable emergency call while sim locked ; } else { visible = m lock pattern utils . is secure ( keyguard update monitor . get current user ( ) ) ; } } },calling requires support
if the focus changes very quickly before the first <PLACE_HOLDER> is returned each focus change triggers a new partition and we end up with many duplicate partitions . this is enhanced as the focus change can be much faster than the taking of the assist structure . hence remove the currently queued <PLACE_HOLDER> and replace it with the one queued after the structure is,cancel current request locked ( ) ; try { final bundle receiver extras = new bundle ( ) ; receiver extras . put int ( extra_request_id @$ request id ) ; final long identity = binder . clear calling identity ( ) ; try { if ( ! activity task manager . get service ( ) . request autofill data ( m assist receiver @$ receiver extras @$ m activity token @$ flags ) ) { slog . w ( tag @$ __str__ + m activity token ) ; } } finally { binder . restore calling identity ( identity ) ; } } catch ( remote exception e ) { },one fill request
does this target accept the drop action <PLACE_HOLDER> being dropped on it ?,int da = e . get drop action ( ) ; if ( ( da & drop actions ) == __num__ ) { return false ; },target accept any
if the new kproject has a different <PLACE_HOLDER> than the original one it has to be initialized,if ( class loader != k project . get class loader ( ) ) { k project . init ( ) ; },kproject has classloader
resets the content view scroll <PLACE_HOLDER> when swiping the panel down after being maximized .,if ( m content != null && ty > __num__ && get panel state ( ) == panel state . maximized ) { m content . reset content view scroll ( ) ; },content view position
separately handle the case where the grid has no table <PLACE_HOLDER> . in this situation @$ there is a single default column .,if ( column count == __num__ ) { this . column = ( column == __num__ ) ? __num__ : - __num__ ; layout ( ) ; return ; } if ( this . column > - __num__ && this . column < column count ) { this . column = - __num__ ; } if ( column < __num__ || column >= grid . get column count ( ) ) return ; this . column = column ; layout ( ) ;,grid has properties
we have converted the sax events generated by tika into a dom object so we can now use the usual html resources from nutch get meta <PLACE_HOLDER>,html meta processor . get meta tags ( meta tags @$ root @$ base ) ; if ( log . is trace enabled ( ) ) { log . trace ( __str__ + base + __str__ + meta tags . to string ( ) ) ; },resources get tags
fail silently . apparently some other event closed the debug <PLACE_HOLDER> .,if ( ! debugger . is connected ( ) ) { return ; },event closed broker
get the partition id and its crc and validate it . validating the partition id for the chunk separately makes it possible to continue processing <PLACE_HOLDER> from other partitions if only one partition has corrupt <PLACE_HOLDER> in the file .,final checksum partition idcrc = m_checksum type == checksum type . crc32c ? new pure java crc32c ( ) : new pure java crc32 ( ) ; chunk lengthb . mark ( ) ; final int next chunk partition id = chunk lengthb . get int ( ) ; final int next chunk partition idcrc = chunk lengthb . get int ( ) ; chunk lengthb . reset ( ) ; byte partition id bytes [ ] = new byte [ __num__ ] ; chunk lengthb . get ( partition id bytes ) ; partition idcrc . update ( partition id bytes @$ __num__ @$ partition id bytes . length ) ; int generated value = ( int ) partition idcrc . get value ( ) ; if,partition has data
the monitor we are exiting is not verifiably the one on the top of our monitor stack . this causes a monitor <PLACE_HOLDER> .,if ( ! actual . is lock reference ( ) || ! expected . equal ( actual ) ) { _monitor_top = bad_monitors ; _monitor_safe = false ; basic block bb = get basic block containing ( bci ) ; bb . set changed ( true ) ; bb . _monitor_top = bad_monitors ; if ( trace monitor mismatch ) { report monitor mismatch ( __str__ ) ; } } else { replace allcts matches ( actual @$ cell type state . make line ref ( bci ) ) ; },monitor causes failure
create the index scan <PLACE_HOLDER> with all its metadata,scan node . set lookup type ( path . lookup type ) ; scan node . set bindings ( path . bindings ) ; scan node . set end expression ( expression util . combine predicates ( expression type . conjunction_and @$ path . end exprs ) ) ; if ( ! path . index . get predicatejson ( ) . is empty ( ) ) { try { scan node . set partial index predicate ( abstract expression . fromjson string ( path . index . get predicatejson ( ) @$ table scan ) ) ; } catch ( json exception e ) { throw new planning error exception ( e . get message ( ) @$ __num__ ) ; } } scan node . set predicate,index scan node
check strip hidden configurations removes the <PLACE_HOLDER>,configuration conf2 = new configuration ( conf ) ; conf2 . set ( hidden config @$ __str__ ) ; conf . strip hidden configurations ( conf2 ) ;,configurations removes stacktrace
<PLACE_HOLDER> url must match <PLACE_HOLDER> url must not match <PLACE_HOLDER> ip must match <PLACE_HOLDER> ip must not match <PLACE_HOLDER> country must match <PLACE_HOLDER> no depth limit match index url must match index url must not match index content must match index content must not match,this . default surrogate profile = new crawl profile ( crawl_profile_surrogate @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ crawl profile . match_all_string @$ crawl profile . match_never_string @$ __num__ @$ false @$ crawl profile . get recrawl date ( crawl_profile_surrogate_recrawl_cycle ) @$ - __num__ @$ true @$ true @$ false @$ false @$ true @$ false @$ false @$ false @$ - __num__ @$ false @$ true @$ crawl profile . match_never_string @$ cache strategy . nocache @$ __str__ + crawl_profile_surrogate @$ client identification . yacy intranet crawler agent name @$ null @$ null,country match crawler
now compare <PLACE_HOLDER> of row .,lpart = ( left far delimiter < __num__ ? llength + loffset : left far delimiter ) - left delimiter ; rpart = ( right far delimiter < __num__ ? rlength + roffset : right far delimiter ) - right delimiter ; result = super . compare rows ( left @$ left delimiter @$ lpart @$ right @$ right delimiter @$ rpart ) ; if ( result != __num__ ) { return result ; } else { if ( left delimiter < __num__ && right delimiter >= __num__ ) { return - __num__ ; } else if ( right delimiter < __num__ && left delimiter >= __num__ ) { return __num__ ; } else if ( left delimiter < __num__ && right delimiter < __num__ ) { return,now compare middlesection
replication factor does <PLACE_HOLDER>,assert true ( bc known10 . compare to ( bc known1000 ) < __num__ ) ; assert true ( bc unknown10 . compare to ( bc unknown1000 ) < __num__ ) ;,factor does match
these key managers may implement <PLACE_HOLDER> ... see above ...,for ( src = __num__ @$ dst = __num__ ; src < tma . length ; ) { if ( ! ( tma [ src ] instanceof javax . net . ssl . trust manager ) ) { if ( tma [ src ] instanceof x509 trust manager ) { tmaw [ dst ] = ( javax . net . ssl . trust manager ) new x509 trust manager javax wrapper ( ( x509 trust manager ) tma [ src ] ) ; dst ++ ; } } else { tmaw [ dst ] = ( javax . net . ssl . trust manager ) tma [ src ] ; dst ++ ; } src ++ ; },managers implement nulls
call this first @$ to let the parent prepare the <PLACE_HOLDER>,new loading dialog ( this @$ r . string . afc_msg_loading @$ false ) { @ override protected void on pre execute ( ) { super . on pre execute ( ) ; switch ( display prefs . get view type ( file chooser activity . this ) ) { case grid : display prefs . set view type ( file chooser activity . this @$ view type . list ) ; break ; case list : display prefs . set view type ( file chooser activity . this @$ view type . grid ) ; break ; } setup view files ( ) ; if ( build . version . sdk_int >= build . version_codes . honeycomb ) activity compat . invalidate options menu ( file chooser,parent prepare results
this point is only reached if the operation failed <PLACE_HOLDER> than the allowed retry count,log . warn ( __str__ + __str__ + __str__ @$ m key @$ m upload id @$ m bucket name @$ property key . underfs_cleanup_enabled . get name ( ) @$ last exception ) ;,operation failed more
else this schema index provider does n't have any persistent <PLACE_HOLDER> to delete .,delete obsolete indexes = true ;,provider have indexes
hopefully given collection wo n't grow more than 10 <PLACE_HOLDER> between now and the synchronized block ...,int initial = math . min ( c . size ( ) + __num__ @$ max ) ; int count = __num__ ; array list < t > list = new array list < t > ( initial ) ; synchronized ( c ) { iterator < t > iter = c . iterator ( ) ; while ( iter . has next ( ) && ( count < max ) ) { list . add ( iter . next ( ) ) ; count ++ ; } } return list ;,collection grow bytes
selecting a item with enter will lose the <PLACE_HOLDER> and selected item @$ which means that further keyboard selection wo n't work unless we do this :,if ( event . get type int ( ) == event . onkeydown && event . get key code ( ) == key codes . key_enter ) { final menu item item = get selected item ( ) ; super . on browser event ( event ) ; scheduler . get ( ) . schedule deferred ( new scheduled command ( ) { @ override public void execute ( ) { select item ( item ) ; focus ( ) ; } } ) ; } else { super . on browser event ( event ) ; },item lose focus
tree display prefs <PLACE_HOLDER>,box tree display prefs = box . create vertical box ( ) ; tree display prefs . set border ( border factory . create titled border ( __str__ ) ) ; j panel tree display options = new j panel ( ) ; tree display options . set layout ( new grid layout ( __num__ @$ __num__ ) ) ; j label font name = new j label ( __str__ ) ; final j combo box font picker = new j combo box ( graphics environment . get local graphics environment ( ) . get available font family names ( ) ) ; font picker . set selected item ( preferences . get font ( ) ) ; j label size label = new j label ( __str__,display prefs box
state change listener sets final query <PLACE_HOLDER> and then clears scheduler when the query finishes .,sql query scheduler scheduler = query scheduler . get ( ) ; optional < query info > final query info = state machine . get final query info ( ) ; if ( final query info . is present ( ) ) { return final query info . get ( ) . get query stats ( ) . get total memory reservation ( ) ; } if ( scheduler == null ) { return new data size ( __num__ @$ byte ) ; } return succinct bytes ( scheduler . get total memory reservation ( ) ) ;,listener sets info
see if we only have a mnemonic and no operands . if so @$ then just add the mnemonic to the preview <PLACE_HOLDER> and increment our counter .,if ( metadata != null ) { if ( metadata . get operands ( ) == null || metadata . get operands ( ) . size ( ) == __num__ ) { mask container mask container = metadata . get mask container ( ) ; if ( mask container != null && mask container . get value ( ) != null ) { instr size = mask container . get value ( ) . length ; } } else if ( metadata . get operands ( ) != null ) { operand metadata operand = metadata . get operands ( ) . get ( __num__ ) ; if ( operand == null ) { continue ; } instr size = operand . get mask container ( ) .,counter add bar
check that the listener on the animation that was being clone receive the animation lifecycle <PLACE_HOLDER> for the clones .,assert true ( only contains ( started animators @$ s1 @$ s2 @$ s3 ) ) ; assert true ( only contains ( canceled animators @$ s3 ) ) ; assert true ( only contains ( ended animators @$ s1 @$ s2 @$ s3 ) ) ;,animation lifecycle events
the ipport does not represent any existing h 2 o cloud <PLACE_HOLDER> or client,throw new illegal argument exception ( __str__ + node idx + __str__ ) ;,ipport represent node
another call to get <PLACE_HOLDER> should give 3 <PLACE_HOLDER> instead of 2,assert that ( groups . get groups ( __str__ ) . size ( ) ) . is equal to ( __num__ ) ;,call give groups
verify the distance is zero as long as two nodes have the same <PLACE_HOLDER> . they do n't need to refer to the same object .,node base node1 = new node base ( data nodes [ __num__ ] . get host name ( ) @$ data nodes [ __num__ ] . get network location ( ) ) ; node base node2 = new node base ( data nodes [ __num__ ] . get host name ( ) @$ data nodes [ __num__ ] . get network location ( ) ) ; assert equals ( __num__ @$ cluster . get distance ( node1 @$ node2 ) ) ;,nodes have hostname
default constructor requires same <PLACE_HOLDER> flags . exception : enum constructor which is always private,if ( ! is enum && ( ( class acces flags & accessibility_flags ) != ( method access flags & accessibility_flags ) ) ) { return false ; } int count = __num__ ; for ( struct method mt : cl . get methods ( ) ) { if ( code constants . init_name . equals ( mt . get name ( ) ) ) { if ( ++ count > __num__ ) { return false ; } } } return true ;,constructor requires access
class we must guarantee that only one thread at a time uses our number <PLACE_HOLDER> .,synchronized ( number format ) { return ( number format ) number format . clone ( ) ; },thread uses format
quick exit : if the filesystem does not support <PLACE_HOLDER> @$ we can exit early .,if ( ! is encryption supported ( ) ) { return device policy manager . encryption_status_unsupported ; },filesystem support encryption
every adapter has an unique index <PLACE_HOLDER>,for ( adapter adapter : adapters ) { adapter data observer observer = new adapter data observer ( m total @$ m index gen == null ? m index ++ : m index gen . increment and get ( ) ) ; adapter . register adapter data observer ( observer ) ; has stable ids = has stable ids && adapter . has stable ids ( ) ; layout helper helper = adapter . on create layout helper ( ) ; helper . set item count ( adapter . get item count ( ) ) ; m total += helper . get item count ( ) ; helpers . add ( helper ) ; pair = pair . create ( observer @$ adapter ) ; m index ary,adapter has generator
thread does not have a preference so return <PLACE_HOLDER>,if ( b == null ) { return ! share sockets ; } else { return b ; },preference return 0
kudu does n't support general not predicates @$ but some not operators can be converted into kudu predicates . see <PLACE_HOLDER> to predicates below .,translate ( root . get children ( ) . get ( __num__ ) @$ leaves @$ ! is not @$ schema @$ results ) ; return ; case leaf : predicate leaf leaf = leaves . get ( root . get leaf ( ) ) ; if ( schema . has column ( leaf . get column name ( ) ) ) { results . add all ( leaf to predicates ( leaf @$ is not @$ schema ) ) ; } return ; case constant : return ;,some see aliases
change the default <PLACE_HOLDER> for byte data type ; should not affect the typedef default <PLACE_HOLDER>,settings . set long ( __str__ @$ format settings definition . binary ) ; bdefs [ __num__ ] . copy setting ( settings @$ default settings ) ; def settings = td . get default settings ( ) ; assert null ( def settings . get value ( __str__ ) ) ;,settings affect settings
before or after the boolean flag introduces the <PLACE_HOLDER> .,construction exception e = assert throws ( __str__ + __str__ @$ construction exception . class @$ ( ) -> construct ( example prefixed foo options . class @$ example bar was named foo option . class ) ) ; assert that ( e ) . has cause that ( ) . is instance of ( duplicate option declaration exception . class ) ; assert that ( e ) . has message that ( ) . contains ( __str__ ) ; e = assert throws ( __str__ + __str__ @$ construction exception . class @$ ( ) -> construct ( example bar was named foo option . class @$ example prefixed foo options . class ) ) ; assert that ( e ) . has cause that ( ),flag introduces alias
if user code does n't call end <PLACE_HOLDER> @$ the framework automatically does with no cause .,log . end response ( ) ; assert that ( log . response duration nanos ( ) ) . is zero ( ) ; assert that ( log . response cause ( ) ) . is same as ( error ) ;,code call response
replace spaces because importer ca n't read attributes <PLACE_HOLDER> in quotes,writer . append ( __str__ ) ;,importer read themselves
recycler view has also the same <PLACE_HOLDER> to provide better performance .,m column header recycler view . set has fixed size ( has fixed width ) ;,view has width
now @$ using error.two inner profile @$ which uses another <PLACE_HOLDER> :,assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ; assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ; assert equals ( __str__ @$ props . get value ( __str__ @$ __str__ ) ) ;,which uses usage
no known conditional special case <PLACE_HOLDER> @$ use a normal <PLACE_HOLDER>,if ( has slot ( exc word @$ exc_full_mappings ) ) { long value = get slot value and offset ( exc word @$ exc_full_mappings @$ exc offset ) ; full = ( int ) value & __num__ ; exc offset = ( int ) ( value > > __num__ ) + __num__ ; exc offset += full & full_lower ; full >>= __num__ ; exc offset += full & __num__ ; full >>= __num__ ; if ( upper not title ) { full &= __num__ ; } else { exc offset += full & __num__ ; full = ( full > > __num__ ) & __num__ ; } if ( full != __num__ ) { try { out . append ( exceptions @$ exc offset @$ exc,case use position
system scoped <PLACE_HOLDER> which should,d . set scope ( artifact . scope_system ) ; file file = new file ( get basedir ( ) @$ __str__ ) ; assert true ( file . exists ( ) ) ; d . set system path ( file . get canonical path ( ) ) ; artifact = repository system . create dependency artifact ( d ) ;,system scoped artifact
let linger run its <PLACE_HOLDER> .,callback . assert no callback ( ) ; final int linger timeout ms = m service . m linger delay ms + m service . m linger delay ms / __num__ ; callback . expect callback ( callback entry . lost @$ m cell network agent @$ linger timeout ms ) ;,linger run course
verify only the other tenant has the new state <PLACE_HOLDER>,final tenant key value empty tenant key1 = tenant api . get plugin payment state machine config ( plugin_name @$ request options for original tenant ) ; assert . assert equals ( empty tenant key1 . get values ( ) . size ( ) @$ __num__ ) ; final tenant key value tenant key1 other tenant = tenant api . get plugin payment state machine config ( plugin_name @$ request options ) ; assert . assert equals ( tenant key1 other tenant . get key ( ) @$ tenantkv . tenant key . plugin_payment_state_machine_ . to string ( ) + plugin_name ) ; assert . assert equals ( tenant key1 other tenant . get values ( ) . size ( ) @$ __num__ ) ;,tenant has machine
share ud set includes <PLACE_HOLDER> that share same upgrade domain with another datanode .,list < t > shareud set = get shareud set ( ud map ) ;,domain includes keys
no room on current <PLACE_HOLDER> . so start a new <PLACE_HOLDER> .,pw . println ( ) ; line length = __num__ ;,room start room
ok let 's set the default parameter exclusion list evaluate the <PLACE_HOLDER> to load it from an external file ...,if ( this . excluded params . is empty ( ) ) { add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_undefined @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_post_data @$ __str__ ) ; add scanner param filter ( __str__ @$ name value pair . type_post_data @$ __str__ ) ; add scanner param,list evaluate vertex
bar refcount has reached <PLACE_HOLDER> @$ a destroying task is scheduled,assert equals ( __num__ @$ scheduled destroy tasks . size ( ) ) ; scheduled destroy task = scheduled destroy tasks . poll ( ) ; assert equals ( shared resource holder . destroy_delay_seconds @$ scheduled destroy task . get delay ( time unit . seconds ) ) ;,refcount reached 0
delete table once more ; the resource not found <PLACE_HOLDER> swallowed silently,ddbms . destroy ( ) ; verify table not exist ( table name @$ dynamodb ) ; intercept ( io exception . class @$ __str__ @$ __str__ @$ ( ) -> ddbms . list children ( test path ) ) ; ddbms . destroy ( ) ; intercept ( file not found exception . class @$ __str__ @$ __str__ @$ ( ) -> ddbms . prune ( prune mode . all_by_modtime @$ __num__ ) ) ; destroy ( ddbms ) ;,resource found exception
a very slow export decoder must have noticed the export <PLACE_HOLDER> shutting down,if ( m_closed ) { export log . info ( __str__ ) ; container . internal discard ( ) ; } else { m_pending container . set ( container ) ; },decoder noticed container
the input object inspectors describe the <PLACE_HOLDER> and values of the big table and small table .,big table standard object inspector = object inspector factory . get standard struct object inspector ( big table column name list @$ big table object inspector list ) ; small table standard object inspector = object inspector factory . get standard struct object inspector ( small table column name list @$ small table object inspector list ) ; input object inspectors = new object inspector [ ] { big table standard object inspector @$ small table standard object inspector } ;,inspectors describe columns
pkix validation has failed . throw an <PLACE_HOLDER> .,if ( expecting trust manager . has exception ( ) ) { throw expecting trust manager . get exception ( ) ; },pkix throw exception
this should be a rare case : normally this <PLACE_HOLDER> is already captured . but it does happen if the upper bound of a type variable contains a <PLACE_HOLDER>,if ( type instanceof wildcard type ) { return ( ( wildcard type ) type ) . get upper bounds ( ) ; } else if ( type instanceof capture type ) { return ( ( capture type ) type ) . get upper bounds ( ) ; } else if ( type instanceof generic array type ) { return get array exact direct super types ( type ) ; } else { throw new runtime exception ( __str__ + type ) ; },bound contains type
historic rules may contain null entries when original zoneinfo <PLACE_HOLDER> includes non transition <PLACE_HOLDER> .,if ( historic rules != null ) { for ( int i = __num__ ; i < historic rules . length ; i ++ ) { if ( historic rules [ i ] != null ) { size ++ ; } } },data includes data
test validation expects this <PLACE_HOLDER>,factory . set concurrency checks enabled ( false ) ;,validation expects behavior
set alpha will call invalidate <PLACE_HOLDER> and drive the animation .,int partial alpha = ( int ) ( alpha * normalized ) ; super . set alpha ( partial alpha ) ; super . draw ( canvas ) ; super . set alpha ( alpha ) ;,alpha call state
the main menu does a <PLACE_HOLDER> to the local crawler page @$ but in case this table is empty @$ the overview page is shown,if ( post != null && post . contains key ( __str__ ) && tabletype == event origin . local_crawling && resultur ls . get stack size ( event origin . local_crawling ) == __num__ ) { tabletype = ( resultur ls . get stack size ( event origin . surrogates ) == __num__ ) ? event origin . unknown : event origin . surrogates ; },menu does redirect
no previous id @$ always accept this <PLACE_HOLDER>,if ( requested session id == null ) { requested session id = id ; session = s ; } else if ( requested session id . equals ( id ) ) { } else if ( session == null || ! is valid ( session ) ) { requested session id = id ; session = s ; } else { if ( s != null && is valid ( s ) ) throw new bad message exception ( __str__ + requested session id + __str__ + id ) ; },id accept one
this gnarly block of code will release all <PLACE_HOLDER> and all thread @$ even if any close fails .,try { server socket . close ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ; } for ( iterator < socket > s = open client sockets . key set ( ) . iterator ( ) ; s . has next ( ) ; ) { try { s . next ( ) . close ( ) ; s . remove ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ; } } try { executor . shutdown ( ) ; } catch ( throwable e ) { logger . log ( level . warning @$ __str__ @$ e ) ;,block release sockets
we copy all returned <PLACE_HOLDER> from the get value call in case an optimised model is reusing one object to return many <PLACE_HOLDER> . the number subclasses in the jdk are immutable and so will not be used in this way but other subclasses of number might want to do this to save space and avoid unnecessary heap allocation .,if ( type . get superclass ( ) == java . lang . number . class ) { number n1 = ( number ) data . get value at ( row1 @$ column ) ; double d1 = n1 . double value ( ) ; number n2 = ( number ) data . get value at ( row2 @$ column ) ; double d2 = n2 . double value ( ) ; if ( d1 < d2 ) { return - __num__ ; } else if ( d1 > d2 ) { return __num__ ; } else { return __num__ ; } } else if ( type == java . util . date . class ) { date d1 = ( date ) data . get value at,all returned values
async wait <PLACE_HOLDER> .,thread . sleep ( __num__ ) ;,async wait operation
row set log should contain <PLACE_HOLDER> as well as performance logs,verify fetched log ( row set log @$ expected logs execution ) ; verify fetched log ( row set log @$ expected logs performance ) ; verify missing contents in fetched log ( row set log @$ expected logs verbose ) ;,log contain warnings
default root queue allows <PLACE_HOLDER> to have admin acl,if ( is capacity scheduler ) { capacity scheduler configuration csconf = new capacity scheduler configuration ( ) ; csconf . set acl ( __str__ @$ queueacl . administer_queue @$ __str__ ) ; csconf . set acl ( __str__ @$ queueacl . administer_queue @$ __str__ ) ; rm . get resource scheduler ( ) . reinitialize ( csconf @$ rm . getrm context ( ) ) ; },queue allows anyone
check that the meta group changed its <PLACE_HOLDER> .,assert equals ( __str__ @$ new meta group . get group name ( ) @$ renamed group name ) ;,group changed name
instantiate the class specified in the code or object section of the m let <PLACE_HOLDER>,object o ; object instance obj inst ; if ( code != null && ser name != null ) { final string msg = __str__ + __str__ ; mlet_logger . logp ( level . finer @$ m let . class . get name ( ) @$ mth @$ msg ) ; mbeans . add ( new error ( msg ) ) ; continue ; } if ( code == null && ser name == null ) { final string msg = __str__ + __str__ ; mlet_logger . logp ( level . finer @$ m let . class . get name ( ) @$ mth @$ msg ) ; mbeans . add ( new error ( msg ) ) ; continue ; } try { if ( code !=,class let method
the filter ca n't accept more <PLACE_HOLDER> and is effectively broken,overflowed = true ; throw new overflowed error ( ) ;,filter accept data
extension registry may be either extension registry or extension registry lite . since the type we are parsing is a full message @$ only a full extension registry could possibly contain <PLACE_HOLDER> of it . otherwise we will treat the registry as if it were empty .,if ( type . is extension number ( field number ) ) { if ( extension registry instanceof extension registry ) { final extension registry . extension info extension = target . find extension by number ( ( extension registry ) extension registry @$ type @$ field number ) ; if ( extension == null ) { field = null ; } else { field = extension . descriptor ; default instance = extension . default instance ; if ( default instance == null && field . get java type ( ) == descriptors . field descriptor . java type . message ) { throw new illegal state exception ( __str__ + field . get full name ( ) ) ; } } } else { field =,registry contain extensions
the class throttles every 5 <PLACE_HOLDER> @$ so the first time it is called is true . the second time is throttled and returns false .,clock . set time ( clock . current time millis ( ) + duration . standard minutes ( __num__ ) . get millis ( ) ) ; assert false ( hot key logger . is throttled ( ) ) ; assert true ( hot key logger . is throttled ( ) ) ;,class throttles seconds
binder.java closes the <PLACE_HOLDER> for us .,@ suppress warnings ( __str__ ) final indenting print writer pw = new indenting print writer ( writer @$ __str__ ) ; if ( ! dump utils . check dump permission ( m context @$ tag @$ pw ) ) return ; pw . println ( __str__ ) ; pw . increase indent ( ) ; pw . println ( __str__ ) ; pw . increase indent ( ) ; final tethering configuration cfg = m config ; cfg . dump ( pw ) ; pw . decrease indent ( ) ; pw . println ( __str__ ) ; pw . increase indent ( ) ; m entitlement mgr . dump ( pw ) ; pw . decrease indent ( ) ; synchronized ( m public sync ),binder.java closes stream
the script could create an <PLACE_HOLDER> that persists and retains a reference to the bindings,bindings . put ( __str__ @$ null ) ; bindings . put ( __str__ @$ null ) ;,script create identity
for local servant case just generate the <PLACE_HOLDER> @$ but for others generate the stubs also,if ( ! ( i . is local ( ) ) ) { generate skeleton ( ) ; arguments the arguments = ( arguments ) compile . compiler . arguments ; if ( ( the arguments . tie server == true ) && ( the arguments . emit == the arguments . all ) ) { the arguments . tie server = false ; generate skeleton ( ) ; the arguments . tie server = true ; } generate stub ( ) ; },others generate skeleton
the two crawlers should have different storage <PLACE_HOLDER> for their intermediate data .,config1 . set crawl storage folder ( crawl storage folder + __str__ ) ; config2 . set crawl storage folder ( crawl storage folder + __str__ ) ; config1 . set politeness delay ( __num__ ) ; config2 . set politeness delay ( __num__ ) ; config1 . set max pages to fetch ( __num__ ) ; config2 . set max pages to fetch ( __num__ ) ;,crawlers have dir
this pointer may be in work <PLACE_HOLDER>s because archiver did not copy the <PLACE_HOLDER> yet @$ check that it is not too far forward .,cur wal segm idx = start . index ( ) ;,archiver copy file
this node has been inactive @$ but other node has more recent <PLACE_HOLDER> .,if ( ! is inactive ) { updated latest success transfer = latest reported cluster activity ; },node has information
set the socket buffer <PLACE_HOLDER> for the underlying socket .,if ( options . sndbuf != __num__ ) { tcp utils . set tcp send buffer ( fd @$ options . sndbuf ) ; } if ( options . rcvbuf != __num__ ) { tcp utils . set tcp receive buffer ( fd @$ options . rcvbuf ) ; },socket buffer limits
this sets up a channel for each consumer thread . we do n't track <PLACE_HOLDER> @$ as the connection will close its <PLACE_HOLDER> implicitly,try { channel channel = connection . create channel ( ) ; rabbitmq span consumer consumer = new rabbitmq span consumer ( channel @$ collector @$ metrics ) ; channel . basic consume ( builder . queue @$ true @$ consumer tag @$ consumer ) ; } catch ( io exception e ) { throw new illegal state exception ( __str__ + consumer tag @$ e ) ; },connection close content
let 's sort <PLACE_HOLDER> !,collections . sort ( nodes ) ;,'s sort them
and location manager which are in our process . the callbacks in these components may require <PLACE_HOLDER> our remote caller does not have .,final long identity = binder . clear calling identity ( ) ; try { final int callback count = callbacks . size ( ) ; for ( int i = __num__ ; i < callback count ; i ++ ) { final active callback callback = callbacks . value at ( i ) ; try { callback . m callback . op active changed ( code @$ uid @$ package name @$ active ) ; } catch ( remote exception e ) { } } } finally { binder . restore calling identity ( identity ) ; },callbacks require permissions
sort to get a deterministic canonical order . this probably is n't necessary because the options parser will do its own <PLACE_HOLDER> when canonicalizing @$ but it seems like it ca n't hurt .,incompatible changes . sort ( null ) ; return immutable list . copy of ( incompatible changes ) ;,parser do cleanup
add 1 element @$ which should use same <PLACE_HOLDER> .,m layout manager . expect layouts ( __num__ ) ; m test adapter . add and notify ( __num__ ) ; m layout manager . wait for layout ( __num__ ) ; m activity rule . run on ui thread ( new runnable ( ) { @ override public void run ( ) { assert true ( __str__ @$ target child [ __num__ ] == m recycler view . get child at ( __num__ ) ) ; assert equals ( expected important for accessibility @$ view compat . get important for accessibility ( target child [ __num__ ] ) ) ; } } ) ;,which use size
local class b must also capture the <PLACE_HOLDER> and pass them to a 's constructor .,assert translated lines ( translation @$ __str__ @$ __str__ @$ __str__ ) ;,b capture definitions
we do n't have permission to read boolean <PLACE_HOLDER> s ince the <PLACE_HOLDER> have no boolean <PLACE_HOLDER> we should get an empty set,try { set priv cred set1 = s . get private credentials ( boolean . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { e . print stack trace ( ) ; throw new runtime exception ( __str__ ) ; } system . out . println ( __str__ ) ;,creds have privilege
address 2 will hit the <PLACE_HOLDER>,assert . assert equals ( access privilege . read_write @$ matcher . get access privilege ( address2 @$ hostname2 ) ) ; thread . sleep ( __num__ ) ;,address hit cache
user defined <PLACE_HOLDER>,if ( source inclusions . length > __num__ ) { return path pattern . create ( source inclusions ) ; },user defined interfaces
wrap the inner parts of the loop in a catch throwable so that any errors in the loop do n't doom the entire <PLACE_HOLDER> .,try { handle = txn handler . get mutexapi ( ) . acquire lock ( txn store . mutex_key . initiator . name ( ) ) ; started at = system . current time millis ( ) ; long compaction interval = ( prev start < __num__ ) ? prev start : ( started at - prev start ) / __num__ ; prev start = started at ; show compact response current compactions = txn handler . show compact ( new show compact request ( ) ) ; set < compaction info > potentials = txn handler . find potential compactions ( aborted threshold @$ compaction interval ) . stream ( ) . filter ( ci -> check compaction elig ( ci ) ) . collect ( collectors,errors doom recursion
map works the <PLACE_HOLDER> as collection,if ( param instanceof map ) { for ( map . entry < ? @$ ? > i : ( ( map < ? @$ ? > ) param ) . entry set ( ) ) { deferred parameter key = load object instance ( i . get key ( ) @$ existing @$ i . get key ( ) . get class ( ) ) ; deferred parameter val = i . get value ( ) != null ? load object instance ( i . get value ( ) @$ existing @$ i . get value ( ) . get class ( ) ) : null ; setup steps . add ( new serialzation step ( ) { @ override public void handle ( method context context,map works same
let app <PLACE_HOLDER> manager package manage app <PLACE_HOLDER>,dpm . set application restrictions managing package ( admin1 @$ app restrictions manager package ) ; assert equals ( app restrictions manager package @$ dpm . get application restrictions managing package ( admin1 ) ) ;,package manage restrictions
verify data object attributes <PLACE_HOLDER>,attributes = get data object attributes ( data obj ) ; assert equals ( __num__ @$ attributes . size ( ) ) ; for ( string key : attributes . key set ( ) ) { if ( key . equals ( __str__ ) ) { assert true ( __str__ . equals ( attributes . get ( key ) ) ) ; } else if ( key . equals ( __str__ ) ) { assert true ( __str__ . equals ( attributes . get ( key ) ) ) ; } else { fail ( __str__ ) ; } },object attributes extension
now we know the monitor has observed the initial <PLACE_HOLDER> @$ so if we want to test notify behaviour we can trigger by exceeding the threshold .,if ( when == when . in_notify ) { final thread tested thread = new thread ( sensitive thing ) ; final atomic integer notif count = new atomic integer ( ) ; final notification listener listener = new notification listener ( ) { public void handle notification ( notification n @$ object h ) { tested thread . start ( ) ; try { tested thread . join ( ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; } notif count . increment and get ( ) ; } } ; mbs . add notification listener ( monitor name @$ listener @$ null @$ null ) ; observed proxy . set thing ( __num__ ) ; system .,monitor observed value
we 've found the start of a rule or id . <PLACE_HOLDER> is its first <PLACE_HOLDER>hara<PLACE_HOLDER>ter @$ and pos points past <PLACE_HOLDER> .,-- pos ;,pos points p
force an extra begin <PLACE_HOLDER> in case any guarded <PLACE_HOLDER>s are inputs to the state split,if ( current next instanceof abstract begin node && current next instanceof state split && ( ( state split ) current next ) . state after ( ) != null ) { begin node begin = graph ( ) . add ( new begin node ( ) ) ; begin . set node source position ( get no deopt successor position ( ) ) ; begin . set next ( current next ) ; current next = begin ; },extra begin node
address pointers will only be created if extent of table contains only undefined <PLACE_HOLDER> or pointers,builder . create memory ( __str__ @$ __str__ @$ __num__ ) ;,extent contains tables
no primary key @$ but contains only public key <PLACE_HOLDER> .,keyset valid keyset = keyset . new builder ( ) . add key ( keyset . key . new builder ( ) . set key data ( test util . create key data ( key data . new builder ( ) . build ( ) @$ __str__ @$ key data . key material type . asymmetric_public ) ) . set key id ( __num__ ) . set status ( key status type . enabled ) . set output prefix type ( output prefix type . tink ) . build ( ) ) . build ( ) ; try { util . validate keyset ( valid keyset ) ; } catch ( general security exception e ) { fail ( __str__ + e ) ; },key contains data
set the table write <PLACE_HOLDER> in all of the ac<PLACE_HOLDER> file sinks,if ( ! driver context . get plan ( ) . get acid sinks ( ) . is empty ( ) ) { list < file sink desc > acid sinks = new array list < > ( driver context . get plan ( ) . get acid sinks ( ) ) ; acid sinks . sort ( ( file sink desc fsd1 @$ file sink desc fsd2 ) -> fsd1 . get dir name ( ) . compare to ( fsd2 . get dir name ( ) ) ) ; for ( file sink desc desc : acid sinks ) { table desc table info = desc . get table info ( ) ; final table name tn = hive table name . of nullable ( table,table write ids
now let 's build an identical <PLACE_HOLDER> for get,binary object builder bldr = grid ( __num__ ) . binary ( ) . builder ( __str__ ) ; bldr . set field ( __str__ @$ __num__ ) ; bldr . set field ( __str__ @$ __str__ ) ; binary object bin key = bldr . build ( ) ; assert equals ( __str__ @$ c . get ( bin key ) ) ;,'s build key
if the value tag does not specify a <PLACE_HOLDER> which is a valid field and it is not used within the comments of a valid field @$ return null .,return null ;,tag specify type
force refresh since file id should get <PLACE_HOLDER>,repository item = null ;,id get updated
print found <PLACE_HOLDER> only for those that apply correctly or were already applied,if ( found data ) { println ( __str__ + data name + __str__ + current program . get executable path ( ) + __str__ + found list . get ( i ) . to string ( ) ) ; num data found ++ ; },print found data
the peer may have closed the <PLACE_HOLDER> because of the unmatched server name indication .,ssl socket . close ( ) ; ssl server socket . close ( ) ;,peer closed socket
tell the user which methods force this <PLACE_HOLDER> to be abstract .,modifiers |= m_abstract ;,methods force implementation
management server encounters an <PLACE_HOLDER> and closes the stream .,response observer . on error ( status . unknown . as exception ( ) ) ;,server encounters error
this call to load <PLACE_HOLDER> may eventually call find <PLACE_HOLDER> again @$ in case the parent does n't find anything .,class < ? > c ; try { c = super . load class ( name @$ resolve ) ; } catch ( class not found exception e ) { c = load class from system ( name ) ; } catch ( security exception e ) { c = load class from system ( name ) ; } return c ;,parent find class
validate selected controller services implement the <PLACE_HOLDER> required by the processor,final collection < validation result > referenced service validation results = validate referenced controller services ( validation context ) ; validation results . add all ( referenced service validation results ) ; logger . debug ( __str__ @$ validation context @$ validation results ) ; return validation results ;,services implement interface
rebuilding will use the new <PLACE_HOLDER>,msg = builder . build ( ) ; assert that ( msg . repeated_double ) . is equal to ( doubles ) ;,rebuilding use logic
this is a direct call to a method that the static analysis did not see as invoked . this can happen when the receiver is always null . in most cases @$ the method profile also has a <PLACE_HOLDER> of 0 and the below code to kill the invoke would trigger . but not all methods have profiles @$ for example methods with manually,if ( call target . invoke kind ( ) . is direct ( ) && ! ( ( hosted method ) call target . target method ( ) ) . get wrapped ( ) . is simply implementation invoked ( ) ) { unreachable invoke ( graph @$ invoke @$ call target ) ; continue ; },profile has depth
puff @$ everything went <PLACE_HOLDER>,return return value ;,everything went ok
when drag is enabled mouse drags wo n't change the <PLACE_HOLDER> in the list @$ so we only set the is adjusting flag when it 's not enabled,list . set value is adjusting ( true ) ;,drags change items
if the user has not used the default group <PLACE_HOLDER> @$ add it,if ( ! group list . contains ( default group name ) ) { group list . add ( default group name ) ; } for ( int i = __num__ ; i < packages . length ; i ++ ) { package doc pkg = packages [ i ] ; string pkg name = pkg . name ( ) ; string group name = pkg name group map . get ( pkg name ) ; if ( group name == null ) { group name = reg exp group name ( pkg name ) ; } if ( group name == null ) { group name = default group name ; } get pkg list ( group package map @$ group name ) . add ( pkg,user used name
transition nn <PLACE_HOLDER> to active even though nn 1 still thinks it 's active,banner ( __str__ ) ; name node adapter . abort edit logs ( nn1 ) ; name node adapter . enter safe mode ( nn1 @$ false ) ; cluster . transition to active ( __num__ ) ;,transition nn 2
remove all modifies the <PLACE_HOLDER> that is being called on . so create a copy,collection < string > nodes diff = new array list < string > ( all node names . size ( ) ) ; nodes diff . add all ( all node names ) ; nodes diff . remove all ( nodes in result ) ; if ( nodes diff . size ( ) > __num__ ) { check result = false ; for ( string node name : nodes diff ) { system . err . println ( key name + __str__ + node name ) ; } } return check result ;,all modifies list
check if the instruction is generalizing a simple <PLACE_HOLDER> to a different type .,invocation offset = offset ; clazz . constant pool entry accept ( constant instruction . constant index @$ parameter checker ) ; break ;,instruction generalizing enum
shuffling spreads the <PLACE_HOLDER> out across physical hosts better,collections . shuffle ( warehouse ids ) ; m_available warehouse ids . add all ( warehouse ids ) ; boolean do make replicated = true ; for ( load thread load thread : m_load threads ) { load thread . start ( do make replicated ) ; do make replicated = false ; } for ( int ii = __num__ ; ii < m_load threads . length ; ii ++ ) { try { m_load threads [ ii ] . join ( ) ; } catch ( interrupted exception e ) { e . print stack trace ( ) ; system . exit ( - __num__ ) ; } } try { m_volt client . drain ( ) ; } catch ( interrupted exception e ) { return,shuffling spreads store
listener added later so unneeded <PLACE_HOLDER> not thrown,text menu . add action listener ( this ) ;,listener added exception
if we need to hold the x<PLACE_HOLDER> @$ then we need to make sure that no one holds any <PLACE_HOLDER> @$ including the shared <PLACE_HOLDER> @$ otherwise @$ we just need to make sure that no one holds the x<PLACE_HOLDER>,return xlock req ? ! s . is locked ( ) : ! s . has exclusive lock ( ) ;,one holds lock
'is file name in field ' will always return <PLACE_HOLDER> ',concat fields meta = new concat fields meta ( ) ; concat fields meta . set file name in field ( true ) ; assert false ( concat fields meta . is file name in field ( ) ) ; concat fields meta . set file name in field ( false ) ; assert false ( concat fields meta . is file name in field ( ) ) ;,name return '
initialize <PLACE_HOLDER>s if eos is turned on @$ which will block if the previous <PLACE_HOLDER> has not completed yet ; do not start the first <PLACE_HOLDER> until the topology has been initialized later,if ( eos enabled ) { initialize transactions ( ) ; },transactions start transaction
process variables should have no <PLACE_HOLDER>,task service . set variable ( current task . get id ( ) @$ __str__ @$ __str__ ) ; current task = task service . create task query ( ) . single result ( ) ; assert equals ( __num__ @$ ( ( counting task entity ) current task ) . get variable count ( ) ) ; map < string @$ object > local vars = new hash map < > ( ) ; local vars . put ( __str__ @$ __str__ ) ; local vars . put ( __str__ @$ __str__ ) ; local vars . put ( __str__ @$ __str__ ) ; task service . set variables local ( current task . get id ( ) @$ local vars ) ; current task = task,variables have effect
custom proguard <PLACE_HOLDER>,pro guard obfuscate step . create ( test android platform target factory . create ( ) @$ java compilation constants . default_java_command_prefix @$ new fake project filesystem ( ) @$ optional . empty ( ) @$ __str__ @$ optional . empty ( ) @$ immutable set . of ( proguard config . get filesystem ( ) . resolve ( proguard config . get relative path ( ) ) @$ proguard config . get filesystem ( ) . resolve ( aapt proguard dir . resolve ( __str__ ) ) ) @$ pro guard obfuscate step . sdk proguard type . none @$ pro guard obfuscate step . default_optimization_passes @$ optional . empty ( ) @$ immutable map . of ( build target paths . get gen path ( library,custom proguard configs
does this node support <PLACE_HOLDER> at all ?,if ( ! supported command classes . contains key ( command class . security ) ) { result = false ; } else { final int command class code = ( byte ) serial message . get message payload byte ( __num__ ) & __num__ ; final command class command class of message = command class . get command class ( command class code ) ; if ( command class of message == null ) { logger . warn ( string . format ( __str__ @$ get node id ( ) @$ command class code @$ serial message ) ) ; result = false ; } else if ( command class . security == command class of message ) { final byte message code = byte . value,node support security
this command also supports retrieving <PLACE_HOLDER> for an importing job .,string importing jobid = request . get parameter ( __str__ ) ; if ( importing jobid != null ) { long jobid = long . parse long ( importing jobid ) ; importing job job = importing manager . get job ( jobid ) ; if ( job != null ) { project = job . project ; } } if ( project == null ) { project = get project ( request ) ; } response . set header ( __str__ @$ __str__ ) ; map < string @$ language info > prefixes map = new hash map < > ( ) ; for ( string language prefix : meta parser . get language prefixes ( ) ) { language info info = meta parser . get,command supports rows
when prepare deps of patterns function completes <PLACE_HOLDER> @$,evaluation context evaluation context = evaluation context . new builder ( ) . set keep going ( keep going ) . set num threads ( loading_phase_threads ) . set event hander ( new reporter ( new event bus ( ) @$ event collector ) ) . build ( ) ; evaluation result < sky value > evaluation result = get skyframe executor ( ) . get driver ( ) . evaluate ( singleton target pattern @$ evaluation context ) ;,function completes evaluation
second @$ we must ensure that this method must be statically checked for example @$ in a mixed mode where only some methods are statically checked we must not visit a method which used dynamic dispatch . we do not check for an annotation because some other ast transformations may use this <PLACE_HOLDER> without the annotation being explicitely set,if ( ! type checking context . methods to be visited . is empty ( ) && ! type checking context . methods to be visited . contains ( node ) ) return ;,transformations use visitor
previous status must be load now as measure matrix mutate the live measure <PLACE_HOLDER> which are passed to it,metric . level previous status = load previous status ( metrics @$ db measures ) ; configuration config = project configuration loader . load project configuration ( db session @$ project ) ; debt rating grid debt rating grid = new debt rating grid ( config ) ; measure matrix matrix = new measure matrix ( components @$ metrics per id . values ( ) @$ db measures ) ; formula context impl context = new formula context impl ( matrix @$ debt rating grid ) ; long beginning of leak = get beginning of leak period ( last analysis @$ branch ) ; components . for each ( c -> { issue counter issue counter = new issue counter ( db client . issue dao ( ),matrix mutate matches
for now @$ we can use the deprecated get credentials list <PLACE_HOLDER> .,return client . get credentials list ( ) ;,deprecated get method
footer has an invisible first <PLACE_HOLDER>,c10 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c10 . get text ( ) ) ; assert equals ( __str__ @$ __str__ @$ c10 . get attribute ( __str__ ) ) ; grid cell element c11 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c11 . get text ( ) ) ; grid cell element c20 = get grid element ( ) . get footer cell ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ c20 . get text ( ) ) ; grid cell element c21 = get grid element ( ) . get footer cell ( __num__,footer has column
the procedure runner nt generator has all of the dangerous and slow stuff in it . like <PLACE_HOLDER> @$ instantiation @$ and reflection .,runner generator map . put ( procedure . get type name ( ) @$ new procedure runnernt generator ( clz ) ) ;,runner like storages
test a call where one partition has an <PLACE_HOLDER> .,api error value = api error . from throwable ( new cluster authorization exception ( null ) ) ; list < replica election result > election results = new array list < > ( ) ; replica election result election result = new replica election result ( ) ; election result . set topic ( topic1 . topic ( ) ) ;,partition has error
a field filter @$ hence skip the mappings filtering <PLACE_HOLDER> as a whole @$ as it requires parsing mappings into a map .,if ( first == mapper plugin . noop_field_filter ) { return second ; } if ( second == mapper plugin . noop_field_filter ) { return first ; } return index -> { predicate < string > first predicate = first . apply ( index ) ; predicate < string > second predicate = second . apply ( index ) ; if ( first predicate == mapper plugin . noop_field_predicate ) { return second predicate ; } if ( second predicate == mapper plugin . noop_field_predicate ) { return first predicate ; } return first predicate . and ( second predicate ) ; } ;,filter skip logic
returning an empty array essentially means that the element does n't declare any <PLACE_HOLDER> @$ but we know that it is not true since the reason the annotation parsing failed is because some annotation referenced a missing class . however @$ this allows us to defend against crashing the image builder if the above jdk bug is encountered in user code or if the,return new annotation [ __num__ ] ;,element declare annotations
before sending client ready @$ lets make sure the stats already reflect 0 queued <PLACE_HOLDER>,check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ; check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ; check cq stat on server ( server1vm @$ durable client id @$ __str__ @$ __num__ ) ;,stats reflect queries
our host did not provide a custom flutter <PLACE_HOLDER> . create a flutter <PLACE_HOLDER> to back our flutter view .,log . d ( tag @$ __str__ + __str__ ) ; is flutter engine from host = false ;,host provide engine
do not overwrite interface <PLACE_HOLDER> with instance <PLACE_HOLDER> do not overwrite private <PLACE_HOLDER> note : private <PLACE_HOLDER> from parent classes are not shown here @$ but when doing the multimethod connection step @$ we overwrite <PLACE_HOLDER> of the parent class with <PLACE_HOLDER> of a subclass and in that case we want to keep the private <PLACE_HOLDER>,if ( match . is private ( ) || ( ! is non real method ( match ) && match . get declaring class ( ) . is interface ( ) && ! method . get declaring class ( ) . is interface ( ) && ! method . is static ( ) ) ) { } else { cached class methodc = method . get declaring class ( ) ; cached class matchc = match . get declaring class ( ) ; if ( methodc == matchc ) { if ( is non real method ( method ) ) { list . set ( found @$ method ) ; } } else if ( ! methodc . is assignable from ( matchc . get the class (,methods overwrite methods
since the bulk create <PLACE_HOLDER> out on a single failure @$ this has to be done as a workaround,batch create types ( types def ) ;,bulk create returns
note : parse offset <PLACE_HOLDER> will validate the given <PLACE_HOLDER> and throws illegal argument exception when <PLACE_HOLDER> is not valid,object [ ] parsed items = parse offset pattern ( gmt offset patterns [ idx ] @$ t . required ( ) ) ; gmt offset pattern items [ idx ] = parsed items ;,pattern validate pattern
normalization scale signed <PLACE_HOLDER>,assert divide all signs ( new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } @$ new int [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) ;,scale signed range
bean may have acquired new weak <PLACE_HOLDER>,target = result . get node ( ) ; assert . assert equals ( count ++ @$ result . get value ( ) . int value ( ) ) ;,bean acquired affinity
second attempt should have the same <PLACE_HOLDER> as exponential backoff is disabled,assert equals ( reconnect backoff ms test @$ delay ) ;,attempt have delay
we encounter a vm error . this includes for example out of memory exceptions . in such a case we silently swallow the error . if it happens again the application thread will most likely encounter the same <PLACE_HOLDER> . if not the watchdog thread will no longer monitor the compilation and thus the error can not happen again .,thread . sleep ( spin_timeout_ms ) ;,thread encounter error
if we 're a relaunch we may need to adjust button enable <PLACE_HOLDER>,if ( icicle != null ) { m did acknowledge = icicle . get boolean ( key_did_acknowledge @$ false ) ; m allow button . set enabled ( ! m did acknowledge ) ; m deny button . set enabled ( ! m did acknowledge ) ; },button enable state
self gateway should adapt its fencing <PLACE_HOLDER>,assert equals ( new fencing token @$ fenced gateway . get fencing token ( ) ) ; assert equals ( new fencing token @$ fenced testing endpoint . get fencing token ( ) ) ; rpc utils . terminate rpc endpoint ( fenced testing endpoint @$ timeout ) ;,gateway adapt token
sort operator create a new <PLACE_HOLDER> for the moment,return true ;,operator create one
first result set has results @$ second has <PLACE_HOLDER>,when ( result set . next ( ) ) . then return ( true ) . then return ( false ) ; when ( result set . get string ( constants . col_trigger_type ) ) . then return ( constants . ttype_blob ) ; operable trigger trigger = jdbc delegate . select trigger ( conn @$ trigger key . trigger key ( __str__ ) ) ; assert null ( trigger ) ;,set has results
client eventually sees broken <PLACE_HOLDER>,assert throws ( io exception . class @$ ( ) -> { int i = __num__ ; for ( i = __num__ ; i < __num__ ; i ++ ) { client . get output stream ( ) . write ( __num__ ) ; } } ) ;,client sees connection
second gbk forces the <PLACE_HOLDER> to be materialized,p assert . that ( result ) . contains in any order ( kv . of ( __str__ @$ immutable list . of ( __num__ @$ __num__ @$ __num__ ) ) @$ kv . of ( __str__ @$ immutable list . of ( __num__ ) ) ) ;,gbk forces store
ca of the certificate @$ for which this <PLACE_HOLDER> is checked @$ has also signed <PLACE_HOLDER> @$ so skip the path validation @$ because is already done,if ( signing cert . equals ( defaultcrl sign cert ) ) { valid certs . add ( signing cert ) ; valid keys . add ( defaultcrl sign key ) ; continue ; } try { cert path builder builder = cert path builder . get instance ( __str__ @$ bouncy castle provider . provider_name ) ; selector = new x509 cert store selector ( ) ; selector . set certificate ( signing cert ) ; extendedpkix parameters temp = ( extendedpkix parameters ) paramspkix . clone ( ) ; temp . set target cert constraints ( selector ) ; extendedpkix builder parameters params = ( extendedpkix builder parameters ) extendedpkix builder parameters . get instance ( temp ) ; if ( cert path certs . contains,certificate signed key
verify the puts reach <PLACE_HOLDER> 2,vm2 . invoke ( ( ) -> wan test base . validate region size ( get test method name ( ) + __str__ @$ __num__ ) ) ;,puts reach size
see if the input row contains the filename <PLACE_HOLDER> ...,int idx = get input row meta ( ) . index of value ( meta . get accepting field ( ) ) ; if ( idx < __num__ ) { throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ meta . get accepting field ( ) ) ) ; },row contains information
make sure the type matches <PLACE_HOLDER> the return statements are returning .,type type = val needed ? this . type : type . t void ; expression e = new inline method expression ( where @$ type @$ field @$ new compound statement ( where @$ body ) ) ; return val needed ? e . inline value ( env @$ ctx ) : e . inline ( env @$ ctx ) ;,statements returning what
dw streams specifies the <PLACE_HOLDER> of streams in the file . for example @$ a file with audio and video has two streams .,d . write int ( ( int ) largest buffer size ) ;,streams specifies number
set the screen 's skin using the previously generated ui skin <PLACE_HOLDER> .,widget . set skin ( assets . generate asset ( data @$ ui skin . class ) ) ; selected screen box . set content ( widget ) ;,skin using format
confirm that we report at least as much throttle time as our server sent <PLACE_HOLDER> for . we will actually report more due to backoff in restarting streams .,assert true ( this . client . get and reset throttle time ( ) > throttle time ) ; stream . close ( ) ; assert true ( stream . await termination ( __num__ @$ time unit . seconds ) ) ;,time sent requests
all other errors indicate a bigger <PLACE_HOLDER> @$ so just stop the task .,if ( thread . interrupted ( ) ) { return true ; } else if ( ! response . is valid ( ) ) { return response . get error ( ) . get error code ( ) != error code . io_exception ; } else { return false ; },errors indicate timeout
system apps have <PLACE_HOLDER> over where their default storage context is pointed @$ so we 're always explicit when building paths .,final context ce context = context . create credential protected storage context ( ) ; files_dir = ce context . get files dir ( ) ; database_dir = ce context . get database path ( __str__ ) . get parent file ( ) ; root_dir = ce context . get data dir ( ) ; sharedpref_dir = ce context . get shared preferences path ( __str__ ) . get parent file ( ) ; cache_dir = ce context . get cache dir ( ) ; nobackup_dir = ce context . get no backup files dir ( ) ; final context de context = context . create device protected storage context ( ) ; device_files_dir = de context . get files dir ( ) ; device_database_dir = de context,apps have control
replace load method counters node with resolve method and load counters node @$ expose klass <PLACE_HOLDER> .,replace load method counters ( graph @$ state mapper @$ context ) ;,counters node instance
remove existent <PLACE_HOLDER> should remove ownership <PLACE_HOLDER>,cache . remove owner from stream ( stream @$ owner @$ __str__ ) ; assert null ( __str__ + owner + __str__ @$ cache . get owner ( stream ) ) ; ownership map = cache . get stream owner mapping ( ) ; assert equals ( __str__ + ( num proxies * num streams per proxy - __num__ ) + __str__ @$ num proxies * num streams per proxy - __num__ @$ ownership map . size ( ) ) ; ownership distribution = cache . get stream ownership distribution ( ) ; assert equals ( __str__ + num proxies + __str__ @$ num proxies @$ ownership distribution . size ( ) ) ; set < string > owned streams = ownership distribution . get ( owner ),mapping remove stream
get the tenant root <PLACE_HOLDER> . if the tenant root <PLACE_HOLDER> does not exist then exit .,tenant root folder = repository file dao . get file by absolute path ( server repository paths . get tenant root folder path ( the tenant ) ) ; if ( tenant root folder != null ) { tenant home folder = repository file dao . get file by absolute path ( server repository paths . get tenant home folder path ( the tenant ) ) ; if ( tenant home folder == null ) { string owner id = user name utils . get principle id ( the tenant @$ username ) ; repository file sid owner sid = new repository file sid ( owner id @$ type . user ) ; string tenant authenticated role id = role name utils . get principle id ( the,tenant root folder
send key action does n't need any incoming cec <PLACE_HOLDER> @$ hence does not consume it .,return false ;,action need message
actual overlap @$ just skip the <PLACE_HOLDER> .,if ( last event . event equals ( cur event ) ) { if ( debug ) slog . d ( tag @$ __str__ + last nanos ) ; } else { assign log id ( cur event ) ; m pending logs . add ( cur event ) ; if ( debug ) slog . d ( tag @$ __str__ + last nanos ) ; },overlap skip event
and finally @$ find the variation @$ caused by the fact that the sun 's gravitational pull on the moon varies depending on which <PLACE_HOLDER> of the earth the moon is on,double variation = __num__ * pi / __num__ * math . sin ( __num__ * ( moon longitude - sun long ) ) ; moon longitude += variation ;,moon on level
decorate outgoing exceptions with basic tree <PLACE_HOLDER> . this is similar to how the constructor appends its <PLACE_HOLDER> @$ but the constructor has read more <PLACE_HOLDER> at that point so this one is a bit more sparse on <PLACE_HOLDER> .,with message ( t @$ t . get message ( ) + __str__ + format ( __str__ @$ index file ) ) ; throw t ;,constructor read information
0 means <PLACE_HOLDER> already set and we should not block any threads .,return get state ( ) == __num__ ? __num__ : - __num__ ;,0 means state
then draw the arc . finally @$ as the optimizer has n't stroke <PLACE_HOLDER> the path @$ we close and fill it @$ and mark the stroke as closed . note : the lineto to the centre of the object is required @$ because the fill only fills the arc . skipping this includes an extra chord @$ which is n't correct . peter,close block ( ) ; draw arc ( x @$ y @$ w @$ h @$ sa @$ aa ) ; lineto ( x + ( w > > __num__ ) @$ y + ( h > > __num__ ) ) ; close block ( __str__ ) ;,optimizer has all
aapt should issue an <PLACE_HOLDER> @$ but do a bit of sanity checking here just in case .,if ( old value != null && ! old value . equals ( value ) ) { integer lower = old value . or null ( ) ; integer higher = value . or null ( ) ; if ( ordering . natural ( ) . compare ( old value . or null ( ) @$ value . or null ( ) ) > __num__ ) { lower = higher ; higher = old value . or null ( ) ; } logger . warning ( string . format ( __str__ @$ type @$ name @$ lower @$ higher ) ) ; },aapt issue error
if the receiver is not included in the contract @$ unfreeze frozen balance for this <PLACE_HOLDER> . otherwise @$ unfreeze delegated frozen balance provided this <PLACE_HOLDER> .,if ( ! array utils . is empty ( receiver address ) && dynamic store . supportdr ( ) ) { if ( arrays . equals ( receiver address @$ owner address ) ) { throw new contract validate exception ( __str__ ) ; } if ( ! decode util . address valid ( receiver address ) ) { throw new contract validate exception ( __str__ ) ; } account capsule receiver capsule = account store . get ( receiver address ) ; if ( dynamic store . get allow tvm constantinople ( ) == __num__ && receiver capsule == null ) { string readable receiver address = string util . create readable string ( receiver address ) ; throw new contract validate exception ( __str__ + readable,balance provided address
check if two vh have the same <PLACE_HOLDER> @$ if so @$ print that as an error,final int child count = m child helper . get child count ( ) ; for ( int i = __num__ ; i < child count ; i ++ ) { view view = m child helper . get child at ( i ) ; view holder other = get child view holder int ( view ) ; if ( other == holder ) { continue ; } final long other key = get changed holder key ( other ) ; if ( other key == key ) { if ( m adapter != null && m adapter . has stable ids ( ) ) { throw new illegal state exception ( __str__ + __str__ + __str__ + other + __str__ + holder ) ; } else {,vh have key
the set of <PLACE_HOLDER> with unknown leader contains <PLACE_HOLDER> with leader election pending as well as <PLACE_HOLDER> which may have expired . add the topic again to metadata to ensure it is included and request metadata update @$ since there are messages to send to the topic .,if ( ! result . unknown leader topics . is empty ( ) ) { for ( string topic : result . unknown leader topics ) this . metadata . add ( topic @$ now ) ; log . debug ( __str__ @$ result . unknown leader topics ) ; this . metadata . request update ( ) ; },set contains topics
language agnostic inlining expects this particular <PLACE_HOLDER> to be present in the graph,assert . assert equals ( __num__ @$ graph . get nodes ( inline decision node . type ) . count ( ) ) ; for ( inline decision node decision node : graph . get nodes ( inline decision node . type ) ) { assert . assert equals ( __num__ @$ decision node . usages ( ) . count ( ) ) ; for ( node usage : decision node . usages ( ) ) { if ( usage instanceof integer equals node ) { final node if node = usage . usages ( ) . first ( ) ; assert . assert true ( if node instanceof if node ) ; final fixed node invoke = ( ( if node ) if node ) . false,inlining expects node
get all records inclusive the <PLACE_HOLDER> before the startdate,for ( long f : files ) { if ( start long <= f && f <= end long ) { result as long . add ( f ) ; } },records inclusive one
brand new <PLACE_HOLDER> @$ not refresh @$ directly add the <PLACE_HOLDER>,zn record segment metadata zn record = _pinot helix resource manager . get segment metadata zn record ( offline table name @$ segment name ) ; if ( segment metadata zn record == null ) { logger . info ( __str__ @$ segment name @$ raw table name ) ; string crypter = headers . get header string ( file upload download client . custom headers . crypter ) ; process new segment ( segment metadata @$ final segment locationuri @$ current segment location @$ zk downloaduri @$ crypter @$ raw table name @$ segment name @$ move segment to final location ) ; return ; } logger . info ( __str__ @$ segment name @$ raw table name ) ; process existing segment ( segment metadata @$,segment add segment
let 's check partition meta <PLACE_HOLDER> .,if ( rec instanceof page snapshot ) { page snapshot snp rec = ( page snapshot ) rec ; assert false ( snapshots . contains key ( snp rec . full page id ( ) ) ) ; snapshots . put ( snp rec . full page id ( ) @$ snp rec ) ; } else if ( rec instanceof meta page update partition data record ) { meta page update partition data record meta rec = ( meta page update partition data record ) rec ; assert true ( snapshots . contains key ( meta rec . full page id ( ) ) ) ; },check partition pages
only used if framework does not support the object <PLACE_HOLDER>,callbacks [ __num__ ] = new password callback ( __str__ @$ false ) ;,framework support classic
a spawn exec a blocks b if a produces an <PLACE_HOLDER> consumed by b,multimap < spawn exec @$ spawn exec > blocked by = multimap builder . hash keys ( ) . array list values ( ) . build ( ) ; multimap < spawn exec @$ spawn exec > blocking = multimap builder . hash keys ( ) . array list values ( ) . build ( ) ; for ( spawn exec ex : inputs ) { for ( file s : ex . get inputs list ( ) ) { if ( output producer . contains key ( s . get path ( ) ) ) { spawn exec blocker = output producer . get ( s . get path ( ) ) ; blocked by . put ( ex @$ blocker ) ; blocking . put (,a produces output
produce a message to broker 1 's test queue and verify that broker 1 's memory <PLACE_HOLDER> has increased @$ but broker 2 still has no memory <PLACE_HOLDER> .,send messages ( __str__ @$ test queue @$ __num__ ) ; assert true ( broker1 test queue . get memory usage ( ) . get usage ( ) > __num__ ) ; assert equals ( __num__ @$ broker2 test queue . get memory usage ( ) . get usage ( ) ) ;,broker has usage
animate each child moving and changing <PLACE_HOLDER> to match their final locations .,array list < animator > animators = new array list < animator > ( ) ; value animator child animator = value animator . of float ( __num__ @$ __num__ ) ; animators . add ( child animator ) ; for ( int i = __num__ ; i < m layout . get child count ( ) ; i ++ ) { if ( m initial tops . get ( i ) . compare to ( final child tops . get ( i ) ) == __num__ && m initial tops . get ( i + __num__ ) . compare to ( final child tops . get ( i + __num__ ) ) == __num__ ) { continue ; } final view child = m layout . get,child moving tables
if we need to hold the x<PLACE_HOLDER> @$ then we need to make sure that no one holds any <PLACE_HOLDER> @$ including the shared <PLACE_HOLDER> @$ otherwise @$ we just need to make sure that no one holds the x<PLACE_HOLDER>,return xlock req ? ! s . is locked ( ) : ! s . has exclusive lock ( ) ;,one holds lock
<PLACE_HOLDER>scriber connects and creates durable <PLACE_HOLDER>,mqtt client client = create client ( false @$ __str__ @$ listener ) ; final string account_prefix = __str__ ; client . subscribe ( account_prefix + __str__ ) ; client . subscribe ( account_prefix + __str__ ) ; client . subscribe ( account_prefix + __str__ ) ; assert true ( client . get pending delivery tokens ( ) . length == __num__ ) ; string expected result = __str__ ; client . publish ( account_prefix + __str__ @$ expected result . get bytes ( standard charsets . utf_8 ) @$ __num__ @$ false ) ;,subscriber connects sub
bt configures the interface elsewhere : only start <PLACE_HOLDER> .,final inet4 address srv addr = ( inet4 address ) parse numeric address ( bluetooth_iface_addr ) ; return configure dhcp ( enabled @$ srv addr @$ bluetooth_dhcp_prefix_length ) ;,configures start gpu
note : make sure the array has a <PLACE_HOLDER> of at least 1,int array size = math . max ( default_size @$ list size ) ; list = new light [ array size ] ; dist to owner = new float [ array size ] ; for ( int i = __num__ ; i < list size ; i ++ ) { list [ i ] = lights . get ( i ) ; } arrays . fill ( dist to owner @$ float . negative_infinity ) ;,array has size
let the first client receive all <PLACE_HOLDER>,msgs client1 += receive all messages ( client1 ) ; client1 . close ( ) ;,client receive messages
rr <PLACE_HOLDER> matches but app <PLACE_HOLDER> does not . rr <PLACE_HOLDER> should be set to app <PLACE_HOLDER>,rr . set node label expression ( enforced label1 ) ; scheduler utils . enforce partition exclusivity ( rr @$ enforced exclusive label set @$ null ) ; assert . assert null ( rr . get node label expression ( ) ) ; rr . set node label expression ( enforced label2 ) ; scheduler utils . enforce partition exclusivity ( rr @$ enforced exclusive label set @$ app label ) ; assert . assert equals ( app label @$ rr . get node label expression ( ) ) ;,label does label
set paint style @$ style.fill will fill the <PLACE_HOLDER> @$ style.stroke will stroke the <PLACE_HOLDER>,paint . set style ( paint . style . fill ) ; canvas . draw rect ( get dimension in pixel ( __num__ ) @$ get height ( ) - ( __num__ + random . next int ( ( int ) ( get height ( ) / __num__ ) - __num__ ) ) @$ get dimension in pixel ( __num__ ) @$ get height ( ) - __num__ @$ paint ) ; canvas . draw rect ( get dimension in pixel ( __num__ ) @$ get height ( ) - ( __num__ + random . next int ( ( int ) ( get height ( ) / __num__ ) - __num__ ) ) @$ get dimension in pixel ( __num__ ) @$ get height ( ) - __num__,style.fill fill color
native wrappers for methods do n't have a <PLACE_HOLDER>,if ( c != null ) { c . setn method ( nm ) ; },wrappers have name
whether new am could get container complete <PLACE_HOLDER>,allocate response allocate response = am2 . allocate ( new array list < resource request > ( ) @$ new array list < container id > ( ) ) ; list < container status > container statuses = allocate response . get completed containers statuses ( ) ; if ( is container id in container status ( container statuses @$ container id2 ) == false ) { assert . fail ( ) ; } container statuses = attempt2 . get just finished containers ( ) ; if ( is container id in container status ( container statuses @$ container id2 ) ) { assert . fail ( ) ; },am get msg
we wo n't compare the candidate display name against the current item . this is to prevent an validation warning if the user sets the display name to <PLACE_HOLDER> the existing display name is,if ( item . get name ( ) . equals ( current job name ) ) { continue ; } else if ( display name . equals ( item . get display name ( ) ) ) { return false ; },name is whatever
first request has no continuation <PLACE_HOLDER> .,state tag state tag = new state tag ( state tag . kind . bag @$ encoded tag @$ state family ) ;,request has sequence
change link properties should have updated tcp buffer <PLACE_HOLDER> .,link properties lp = new link properties ( ) ; lp . set tcp buffer sizes ( test tcp buffer sizes ) ; m cell network agent . send link properties ( lp ) ; network callback . expect callback ( callback entry . link_properties_changed @$ m cell network agent ) ; verify tcp buffer size change ( test tcp buffer sizes ) ;,properties have sizes
60 fps would require 16 msec <PLACE_HOLDER> here .,uninterruptibles . sleep uninterruptibly ( __num__ @$ time unit . milliseconds ) ;,fps require delay
add the parceler instance which uses the <PLACE_HOLDER> to parcel the field data,type name parceler type = parameterized type name . get ( class name . get ( package_parceler @$ __str__ ) @$ class name ) ; builder . add field ( field spec . builder ( parceler type @$ __str__ @$ modifier . static @$ modifier . final ) . initializer ( __str__ @$ parceler type @$ entity generator . type_name ) . build ( ) ) ;,which uses generator
metrics undock <PLACE_HOLDER>,toggle split screen mode ( - __num__ @$ - __num__ ) ;,metrics undock test
repeatedly bounce a receiving site member which will cause partition offline <PLACE_HOLDER>,async invocation < integer > close open invocation = vm3 . invoke async ( ( ) -> close recreate cache ( ny port @$ region name @$ __num__ ) ) ;,which cause exception
services make no <PLACE_HOLDER> between actual & valid,fmt . set locale ( uloc @$ uloc ) ;,services make distinction
validate <PLACE_HOLDER> here instead of in port mapping parser to guaruntee every instance has a valid <PLACE_HOLDER>,if ( ip != null && ! inet addresses . is inet address ( this . ip ) ) { throw new illegal argument exception ( ip + __str__ ) ; } this . internal port = internal port ; this . external port = external port ; this . protocol = optional . from nullable ( protocol ) . or ( tcp ) ;,instance has ip
constructor is public @$ but class has package <PLACE_HOLDER>,system . out . println ( __str__ ) ; system . err . println ( __str__ ) ; ex . print stack trace ( ) ;,class has name
it is a good practice to remove location requests when the activity is in a paused or stopped state . doing so helps battery <PLACE_HOLDER> and is especially recommended in applications that request frequent location updates .,m fused location client . remove location updates ( m location callback ) . add on complete listener ( this @$ new on complete listener < void > ( ) { @ override public void on complete ( @ non null task < void > task ) { m requesting location updates = false ; set buttons enabled state ( ) ; } } ) ;,state helps performance
mark the view drawn to ensure that it gets invalidated properly the next <PLACE_HOLDER> it is visible and gets invalidated,m private flags |= pflag_drawn ;,view invalidated time
this simulates the completion of txnid : id txn update <PLACE_HOLDER>,long write id = txn mgr2 . get table write id ( __str__ @$ __str__ ) ; add dynamic partitions adp = new add dynamic partitions ( txn mgr2 . get current txn id ( ) @$ write id @$ __str__ @$ __str__ @$ collections . singleton list ( __str__ ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr2 . commit txn ( ) ;,txn update 1
check if remainder of href contains any illegal <PLACE_HOLDER> before proceeding,if ( i < len ) { for ( int j = i ; j < len ; ++ j ) { ch = href . char at ( j ) ; if ( ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) || ( ch >= __num__ && ch <= __num__ ) ) { continue ; } if ( xml char . is high surrogate ( ch ) && ++ j < len ) { int ch2 = href . char at ( j ) ; if ( xml char . is low surrogate ( ch2 ) ) { ch2 = xml char . supplemental ( (,remainder contains characters
the registry now owns native <PLACE_HOLDER> @$ even if registration threw an exception .,s proxy map . set ( i binder @$ result ) ;,registry owns binder
handle negatives @$ which means last n <PLACE_HOLDER>,if ( start < __num__ ) { start = str . length ( ) + start ; },which means text
firsts contains at least one entry @$ always contains everything else . let 's combine <PLACE_HOLDER> into the stream to form a unified set of capabilities . woohoo !,from oss = firsts . stream ( ) . map ( first -> immutable map . < string @$ object > builder ( ) . put all ( always ) . put all ( first ) . build ( ) ) . map ( this :: apply transforms ) . map ( map -> map . entry set ( ) . stream ( ) . filter ( entry -> accepted_w3c_patterns . test ( entry . get key ( ) ) ) . collect ( immutable map . to immutable map ( map . entry :: get key @$ map . entry :: get value ) ) ) ; from oss = stream . of ( ) ;,'s combine them
if one reader has not processed any <PLACE_HOLDER> @$ its 'last offset ' will be null . in this case @$ it must the be the lagging reader .,if ( a offset == null ) { a reader leading = false ; } else if ( b offset == null ) { a reader leading = true ; } else { document a document = source info . create document from offset ( a offset ) ; document b document = source info . create document from offset ( b offset ) ; a reader leading = source info . is position at or before ( b document @$ a document @$ binlog readera . context . gtid source filter ( ) ) ; } if ( a reader leading ) { logger . info ( __str__ ) ; } else { logger . info ( __str__ ) ; },reader processed data
some tables do not use dynamic <PLACE_HOLDER>,if ( ! ( cm instanceof g table column model ) ) { super . create default columns from model ( ) ; return ; },tables use columns
this value must not exceed 4 <PLACE_HOLDER>,list buffer . add ( encode ( default_timecode_scale @$ true ) ) ;,value exceed bits
nt <PLACE_HOLDER> will call a <PLACE_HOLDER> that does n't exist,system . out . flush ( ) ; system . out . println ( __str__ ) ; try { client response impl cri = ( client response impl ) client . call procedure ( __str__ @$ run on all partitionsnt proc . call_missing_proc ) ; system . err . println ( cri . tojson string ( ) ) ; fail ( ) ; } catch ( proc call exception e ) { response = ( client response impl ) e . get client response ( ) ; assert equals ( client response . unexpected_failure @$ response . get status ( ) ) ; system . out . println ( __str__ ) ; system . out . println ( response . tojson string ( ) ) ; },proc call procedure
a proper exit will change this <PLACE_HOLDER> .,this . s exit = crawl status . finished_abnormal ; if ( get pause at start ( ) ) { complete pause ( ) ; } else { get frontier ( ) . run ( ) ; },exit change state
extra element to make sure we have the same <PLACE_HOLDER> to compute the length of each element of the array .,start position = new int [ fields . length + __num__ ] ;,element have number
charset encoder icu has taken ownership ; its finalizer will do the <PLACE_HOLDER> .,address = __num__ ;,finalizer do free
verify that the specified packages matches the provided <PLACE_HOLDER> .,int user id = user handle . get user id ( uid ) ; try { application info app info = mi package manager . get application info ( package name @$ __num__ @$ user id ) ; if ( app info == null ) { log . w ( log_tag @$ string . format ( __str__ @$ package name ) ) ; return false ; } else if ( uid != app info . uid ) { string message = string . format ( __str__ @$ package name @$ app info . uid @$ uid ) ; log . w ( log_tag @$ message ) ; throw new security exception ( message ) ; } } catch ( remote exception e ) { log . e (,packages matches users
not all supported andorid version have this <PLACE_HOLDER>,if ( m selector position field == null ) { for ( int i = __num__ ; i < get child count ( ) ; i ++ ) { if ( get child at ( i ) . get bottom ( ) == m selector rect . bottom ) { return i + get fixed first visible item ( ) ; } } } else { try { return m selector position field . get int ( this ) ; } catch ( illegal argument exception e ) { e . print stack trace ( ) ; } catch ( illegal access exception e ) { e . print stack trace ( ) ; } },version have option
while processing an admin request @$ hdfs failed lock could take long <PLACE_HOLDER> because of multiple hdfs operations @$ especially when the name node is in a different data center . so extend <PLACE_HOLDER>out to 5 minutes .,admin client config admin config = new admin client config ( ) . set max connections per node ( cluster . get number of nodes ( ) ) . set max backoff delay ms ( max backoff delay ms ) . set admin socket timeout sec ( __num__ * __num__ ) ;,lock take time
there is a case that can lead us here . the caller is moving the top activity that is in a <PLACE_HOLDER> that has multiple activities to pip mode . for that the caller is creating a new <PLACE_HOLDER> to host the activity so that we only move the top activity to pip mode and keep other activities in the previous <PLACE_HOLDER> . there,if ( root == null ) { return result_skip ; },caller creating level
pointers to polyphase <PLACE_HOLDER>,int [ ] polyp = new int [ ilbc_constants . enh_ups0 ] ;,pointers polyphase data
use random ids so that one app can not know that another app creates <PLACE_HOLDER>,int session id ; int tries = __num__ ; do { tries ++ ; if ( tries > max_session_id_create_tries ) { slog . w ( tag @$ __str__ + max_session_id_create_tries + __str__ ) ; return null ; } session id = math . abs ( s random . next int ( ) ) ; } while ( session id == __num__ || session id == no_session || m sessions . index of key ( session id ) >= __num__ ) ; assert caller locked ( component name @$ compat mode ) ;,app creates them
check correctness of cross if udf returns right input <PLACE_HOLDER>,final execution environment env = execution environment . get execution environment ( ) ; data set < tuple3 < integer @$ long @$ string > > ds = collection data sets . get small3 tuple data set ( env ) ; data set < tuple5 < integer @$ long @$ integer @$ string @$ long > > ds2 = collection data sets . get small5 tuple data set ( env ) ; data set < tuple5 < integer @$ long @$ integer @$ string @$ long > > cross ds = ds . cross ( ds2 ) . with ( new tuple5 return right ( ) ) ; list < tuple5 < integer @$ long @$ integer @$ string @$ long > > result = cross ds .,udf returns objects
prefer entry destroyed <PLACE_HOLDER> over statistics disabled <PLACE_HOLDER>,check entry destroyed ( ) ; checktx ( ) ; if ( ! this . local region . statistics enabled ) { throw new statistics disabled exception ( string . format ( __str__ @$ this . local region . get full path ( ) ) ) ; },entry destroyed exception
this makes the dialog take up the full <PLACE_HOLDER>,window manager . layout params lp = new window manager . layout params ( ) ; lp . copy from ( get window ( ) . get attributes ( ) ) ; lp . width = window manager . layout params . match_parent ; lp . height = window manager . layout params . wrap_content ; get window ( ) . set attributes ( lp ) ; account = new account ( get intent ( ) . get string extra ( key_account_name ) @$ get intent ( ) . get string extra ( key_account_type ) ) ; package name = get intent ( ) . get string extra ( key_android_package_name ) ; service = get intent ( ) . get string extra ( key_authtoken ) ; if (,dialog take window
snap to edge which has no <PLACE_HOLDER> nodes,iter = expl . set base node ( __num__ ) ; iter . next ( ) ; res = create location result ( __num__ @$ __num__ @$ iter @$ __num__ @$ edge ) ; query graph query graph6 = lookup ( res ) ; assert equals ( new gh point ( __num__ @$ __num__ ) @$ res . get snapped point ( ) ) ; assert equals ( __num__ @$ res . get closest node ( ) ) ; assert equals ( __num__ @$ get points ( query graph6 @$ __num__ @$ __num__ ) . get size ( ) ) ; assert equals ( __num__ @$ get points ( query graph6 @$ __num__ @$ __num__ ) . get size ( ) ) ;,which has super
the leak detector should report no leaked <PLACE_HOLDER> as the query is still running,leak detector . check for memory leaks ( ( ) -> immutable list . of ( create query info ( test query . get id ( ) @$ running ) ) @$ immutable map . of ( test query @$ __num__ ) ) ; assert equals ( leak detector . get number of leaked queries ( ) @$ __num__ ) ;,detector report queries
system tables have no <PLACE_HOLDER>,execute ( __str__ ) ; for ( object [ ] row : response . rows ( ) ) { assert null ( row [ __num__ ] ) ; } execute ( __str__ ) ; for ( object [ ] row : response . rows ( ) ) { assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( ( map < string @$ object > ) row [ __num__ ] ) . contains key ( __str__ ) ) ; assert true ( ( (,tables have name
it would be better if the on site changed provided the <PLACE_HOLDER> of changed sites .,if ( get selected site ( ) == null && m site store . has site ( ) ) { set selected site ( m site store . get sites ( ) . get ( __num__ ) ) ; } if ( get selected site ( ) == null ) { return ; } site model site = m site store . get site by local id ( get selected site ( ) . get id ( ) ) ; if ( site != null ) { m selected site = site ; } if ( get my site fragment ( ) != null ) { get my site fragment ( ) . on site changed ( site ) ; },site provided set
how many header <PLACE_HOLDER> ?,int argnr = rep . count nr job entry attributes ( id_jobentry @$ __str__ ) ; allocate ( argnr ) ; for ( int a = __num__ ; a < argnr ; a ++ ) { header name [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; header value [ a ] = rep . get job entry attribute string ( id_jobentry @$ a @$ __str__ ) ; },many header name
top horizontal non croppped <PLACE_HOLDER>,height = this . crop zone rect . y - this . image rect . y ; if ( height > __num__ ) { g . fill rect ( this . crop zone rect . x @$ this . image rect . y @$ this . crop zone rect . width @$ height ) ; },non croppped part
do n't let the user click restore if the <PLACE_HOLDER> area contains the current wallet <PLACE_HOLDER> @$ or are an invalid set @$ or if the date field is n't set @$ or if it 's in the future .,restore button . disable property ( ) . bind ( or ( or ( not ( validator . valid ) @$ equal ( orig words @$ words area . text property ( ) ) ) @$ date picker is invalid ) ) ;,area contains text
check that pause resume wo n't call the end <PLACE_HOLDER> prematurely,resp . pause ( ) ; resp . resume ( ) ;,resume call method
for single partition key column table @$ we can merge multiple partitions into a single split by using in clause in a single select query if the partitions have the same host <PLACE_HOLDER> . for multiple partition key columns table @$ we ca n't merge them into a single select query @$ so keep them in a separate split .,boolean single partition key column = true ; string partition key column name = null ; if ( ! partitions . is empty ( ) ) { single partition key column = partitions . get ( __num__ ) . get tuple domain ( ) . get domains ( ) . get ( ) . size ( ) == __num__ ; if ( single partition key column ) { string partition id = partitions . get ( __num__ ) . get partition id ( ) ; partition key column name = partition id . substring ( __num__ @$ partition id . last index of ( __str__ ) - __num__ ) ; } } map < set < string > @$ set < string > > hosts to partition keys,partitions have name
test case 3 set io permission granted to code that was signed by first signer set factory permission granted to code that was signed by second signer keystore that contains only first keypairs code was singed by first signer and second signer expect access control <PLACE_HOLDER> for set factory permission,system . out . println ( __str__ ) ; cmd = constructcmd ( __str__ @$ policy2 @$ __str__ @$ __str__ ) ; process tools . execute test jvm ( cmd ) . should have exit value ( __num__ ) ;,signer expect exception
due how xml pull parser <PLACE_HOLDER> @$ the xml is fully loaded on the ram,super ( false @$ true @$ algorithm_ttml_converter ) ;,xml pull exceptions
if enable exceed throttle <PLACE_HOLDER> @$ make sure that region server throttle <PLACE_HOLDER>s are in seconds time unit . because once previous requests exceed their <PLACE_HOLDER> and consume region server <PLACE_HOLDER> @$ <PLACE_HOLDER> in other time units may be refilled in a long time @$ this may affect later requests .,list < pair < boolean @$ timed quota > > list = arrays . as list ( pair . new pair ( throttle . has req num ( ) @$ throttle . get req num ( ) ) @$ pair . new pair ( throttle . has read num ( ) @$ throttle . get read num ( ) ) @$ pair . new pair ( throttle . has write num ( ) @$ throttle . get write num ( ) ) @$ pair . new pair ( throttle . has req size ( ) @$ throttle . get req size ( ) ) @$ pair . new pair ( throttle . has read size ( ) @$ throttle . get read size ( ) ) @$ pair,requests exceed quota
second split contains the <PLACE_HOLDER> 3 and <PLACE_HOLDER> 4 @$ however @$ the locations is undetermined .,if ( split . equals ( splits . get ( __num__ ) ) ) { assert equals ( __num__ @$ file split . get num paths ( ) ) ; expected . clear ( ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ __num__ ) ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ blocksize ) ) ; expected . add ( new split ( file3 . get name ( ) @$ blocksize @$ blocksize * __num__ ) ) ; expected . add ( new split ( file4 . get name ( ) @$ blocksize @$ __num__ ) ) ; expected . add ( new split ( file4 . get,split contains file
some already compressed <PLACE_HOLDER>,return arrays . stream ( new object [ ] [ ] { { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip handler . gzip } @$ { __str__ @$ __str__ @$ gzip,some compressed runtimes
version of refine which wrote the <PLACE_HOLDER>,reader . read line ( ) ;,which wrote line
ok @$ one or both lists had extra <PLACE_HOLDER>,if ( message == null ) { message = __str__ + __str__ + __str__ + expected copy + __str__ + actual copy + __str__ ; } fail ( message ) ;,ok had information
function will always processes the first <PLACE_HOLDER>,dictionary block ineffective block = create dictionary block ( __num__ @$ __num__ ) ; test project range ( ineffective block @$ dictionary block . class @$ projection @$ force yield @$ produce lazy block ) ; test project fast return ignore yield ( ineffective block @$ projection @$ produce lazy block ) ;,function processes dictionary
each job gets its own metric log <PLACE_HOLDER>,path metrics log dir = new path ( properties . get property ( configuration keys . metrics_log_dir_key ) @$ this . get name ( ) ) ; if ( ! fs . exists ( metrics log dir ) && ! fs . mkdirs ( metrics log dir ) ) { logger . error ( __str__ + this . get name ( ) ) ; return ; },job gets dir
this routine is to process the plus sign in the dial string by loop through the network portion @$ post dial portion 1 @$ post dial portion 2 ... <PLACE_HOLDER> . if applied,do { string network dial str ; if ( use nanp ) { network dial str = extract network portion ( temp dial str ) ; } else { network dial str = extract network portion alt ( temp dial str ) ; } network dial str = process plus code ( network dial str @$ use nanp ) ; if ( ! text utils . is empty ( network dial str ) ) { if ( ret str == null ) { ret str = network dial str ; } else { ret str = ret str . concat ( network dial str ) ; } } else { rlog . e ( __str__ @$ network dial str ) ; return dial str ; } post dial,routine is 3
keys in record schema @$ string @$ boolean @$ bytes <PLACE_HOLDER>,object inputs [ ] [ ] = { { as map ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ @$ __str__ @$ __num__ ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __str__ @$ __str__ @$ true @$ __str__ @$ byte string . copy avro string ( __str__ @$ false ) ) @$ __str__ @$ __str__ } @$ { as map ( __str__ @$ __str__ @$ __str__ @$ true @$ __str__ @$ byte string . copy avro string ( __str__ @$ false ) @$ __str__ @$,keys bytes types
asif : a <PLACE_HOLDER> level of zero means exact <PLACE_HOLDER> .,if ( index expression . equals ( other expression ) && index type == other type ) { match level = get match level ( other definitions @$ index definitions @$ mapping ) ; },level means match
learn that a specific host has accessed a specific <PLACE_HOLDER>,access path = __str__ ;,host accessed url
onload will set appropriate <PLACE_HOLDER> later,if ( icon instanceof image icon ) { icon . set width ( __str__ ) ; icon . set height ( __str__ ) ; },onload set size
spinner model did n't like new <PLACE_HOLDER> @$ reset,try { ( ( j formatted text field ) source ) . set value ( last value ) ; } catch ( illegal argument exception iae2 ) { },model like value
user can see these <PLACE_HOLDER>,index for user ( user1 @$ new doc ( ) . set languages ( singleton list ( __str__ ) ) @$ new doc ( ) . set languages ( as list ( __str__ @$ __str__ ) ) ) ;,user see projects
make sure the user has not requested an insane <PLACE_HOLDER> of txns .,int max txns = metastore conf . get int var ( conf @$ conf vars . txn_max_open_batch ) ; if ( num txns > max txns ) num txns = max txns ; stmt = db conn . create statement ( ) ; list < long > txn ids = open txns ( db conn @$ stmt @$ rqst ) ; log . debug ( __str__ ) ; db conn . commit ( ) ; return new open txns response ( txn ids ) ; close ( null @$ stmt @$ db conn ) ; unlock internal ( ) ;,user requested number
should never happen @$ dumpstate always set the <PLACE_HOLDER> .,if ( bugreport file == null ) { log . wtf ( tag @$ __str__ + extra_bugreport + __str__ + intent ) ; return ; },dumpstate set flag
group has no <PLACE_HOLDER> @$ mapping has been disabled or no role mappers were found : simply return the group contents .,if ( group contents . is empty ( ) ) { for ( principal role principal : group principals ) { group contents . add ( role principal . get name ( ) ) ; } },group has roles
ensure calculation type getter returns correct <PLACE_HOLDER> and value,cal . set calculation type ( calculation type . islamic_umalqura ) ; object ct obj = cal . get calculation type ( ) ; if ( ct obj instanceof calculation type ) { calculation type ct = ( calculation type ) ct obj ; if ( ct != calculation type . islamic_umalqura ) { errln ( __str__ ) ; } } else { errln ( __str__ ) ; } date now = new date ( ) ; cal . set time ( now ) ; date then = cal . get time ( ) ; if ( ! now . equals ( then ) ) { errln ( __str__ ) ; } logln ( then . to string ( ) ) ; cal . add ( calendar .,returns correct id
whether device owner enforces camera <PLACE_HOLDER> .,boolean disallow camera globally = false ; if ( is device owner ) { final active admin device owner = get device owner admin locked ( ) ; if ( device owner == null ) { return ; } user restrictions = device owner . user restrictions ; disallow camera globally = device owner . disable camera ; } else { final active admin profile owner = get profile owner admin locked ( user id ) ; user restrictions = profile owner != null ? profile owner . user restrictions : null ; },owner enforces restrictions
if the merged anomaly 's create <PLACE_HOLDER> is before last notify <PLACE_HOLDER> @$ discard,if ( snapshot . contains key ( snapshot key ) ) { long last notify time = alert snap shot . get latest status ( snapshot @$ snapshot key ) . get last notify time ( ) ; if ( merged anomaly . get created time ( ) < last notify time ) { iterator . remove ( ) ; } },last notify time
the new one should have the current @$ max and total <PLACE_HOLDER> that we had when we wrote it,assert equals ( __num__ @$ full . get current duration ms locked ( __num__ ) ) ; assert equals ( __num__ @$ full . get max duration ms locked ( __num__ ) ) ; assert equals ( __num__ @$ full . get total duration ms locked ( __num__ ) ) ;,one have duration
there should never be more than 1 intersecting vertex . but if it happens as a fallback simply skip <PLACE_HOLDER> .,m arr temp vertices . add ( n ) ; m arr temp vertices . add all ( intersections ) ;,fallback skip them
if frame queue is full @$ let 's drop everything . if frame queue accepts this frame @$ let 's recycle the <PLACE_HOLDER> as well .,if ( m frame queue . offer ( frame ) ) { int curr size = buffer . length ; int req size = m buffer size ; if ( curr size == req size ) { if ( m buffer mode == buffer_mode_dispatch ) { m buffer callback . on buffer available ( buffer ) ; } else { m buffer queue . offer ( buffer ) ; } } },'s recycle buffer
all log <PLACE_HOLDER> whose transaction id is not less than provided transaction id,if ( segment . get first tx id ( ) >= transaction id ) { final first tx id not less than selector selector = new first tx id not less than selector ( transaction id ) ; async read record from entries ( log name @$ reader @$ segment @$ executor service @$ new single entry scan context ( __num__ ) @$ selector ) . when complete ( new future event listener < log record withdlsn > ( ) { @ override public void on success ( log record withdlsn value ) { promise . complete ( optional . of ( selector . result ( ) ) ) ; } @ override public void on failure ( throwable cause ) { promise . complete exceptionally ( cause,all log records
length of row is different @$ copy <PLACE_HOLDER> except family,if ( current . last common prefix < bytes . sizeof_short ) { int old row length with size = current . row length with size ; current buffer . get ( current . key buffer @$ current . last common prefix @$ bytes . sizeof_short - current . last common prefix ) ; current . row length with size = bytes . to short ( current . key buffer @$ __num__ ) + bytes . sizeof_short ; system . arraycopy ( current . key buffer @$ old row length with size @$ current . key buffer @$ current . row length with size @$ current . family length with size ) ; current buffer . get ( current . key buffer @$ bytes . sizeof_short @$ current,length different everything
parent window can be null if the child is detached from it 's parent already @$ but someone still has a <PLACE_HOLDER> to access it . so @$ we return the top parent value we already have instead of null .,if ( current != null ) { top parent = current ; },someone has chance
instrument a logger to block the <PLACE_HOLDER> of a remove sub advisory simulate a slow thread,slow down appender = new default test appender ( ) { @ override public void do append ( logging event logging event ) { if ( level . debug . equals ( logging event . get level ( ) ) ) { string message = logging event . get message ( ) . to string ( ) ; if ( message . starts with ( __str__ ) && message . contains ( __str__ ) ) { try { consumer demand exists . count down ( ) ; system . err . println ( __str__ + message ) ; time unit . seconds . sleep ( __num__ ) ; } catch ( exception ignored ) { } } } } } ;,logger block start
this user has the email <PLACE_HOLDER> and could be stored @$ but the schema is still incompatible so the entire stream is rejected,record incompatible user = new record ( incompatible ) ; incompatible user . put ( __str__ @$ __num__ ) ; incompatible user . put ( __str__ @$ __str__ ) ; incompatible user . put ( __str__ @$ __str__ ) ; test runner runner = test runners . new test runner ( store in kite dataset . class ) ; runner . set property ( store in kite dataset . kite_dataset_uri @$ dataset uri ) ; runner . assert valid ( ) ; runner . enqueue ( stream for ( incompatible user ) ) ; runner . run ( ) ; runner . assert all flow files transferred ( __str__ @$ __num__ ) ;,user has extension
1 | 1 not yet visible @$ should n't immediately expand <PLACE_HOLDER>,assert equals ( __num__ @$ grid . get row count ( ) ) ; assert cell texts ( __num__ @$ __num__ @$ new string [ ] { __str__ @$ __str__ @$ __str__ } ) ; select menu path ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,1 expand row
stolen from sp scheduler . need to update the duplicate <PLACE_HOLDER> associated with any every partition tasks cleanup duplicate <PLACE_HOLDER> and collect done <PLACE_HOLDER> in this list for further processing .,return new long [ __num__ ] ;,counters duplicate counters
a list that is expanded with builder methods has the added <PLACE_HOLDER> at the end,p collection list < long > with one create = pc list . and ( create two ) ; assert that ( with one create . get all ( ) @$ contains ( bounded count @$ max read time count @$ unbounded count @$ create two ) ) ;,list has one
and finally @$ let 's allow <PLACE_HOLDER> to be converted too,if ( t == json token . value_string ) { string text = p . get text ( ) . trim ( ) ; if ( __str__ . equals ( text ) || __str__ . equals ( text ) ) { _verify string for scalar coercion ( ctxt @$ text ) ; return boolean . true ; } if ( __str__ . equals ( text ) || __str__ . equals ( text ) ) { _verify string for scalar coercion ( ctxt @$ text ) ; return boolean . false ; } if ( text . length ( ) == __num__ ) { return ( boolean ) _coerce empty string ( ctxt @$ _primitive ) ; } if ( _has textual null ( text ) ) { return,'s allow strings
start the modify procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new modify table procedure ( proc exec . get environment ( ) @$ new td ) ) ; int last step = __num__ ;,procedure kill executor
copy the source <PLACE_HOLDER> to cmroot . as the client will move the source <PLACE_HOLDER> to another location @$ we should make a copy of the <PLACE_HOLDER> to cmroot instead of moving it .,if ( need cm recycle ) { cm . recycle ( source path @$ recycle type . copy @$ true ) ; },client move path
registry has an empty <PLACE_HOLDER>,return __str__ ;,registry has entries
if any aggregate is distinct @$ bail out if any aggregate is the grouping id @$ bail out if any aggregate call has a filter @$ bail out if any aggregate functions do not support <PLACE_HOLDER> @$ bail out,final immutable bit set bottom aggregate group set = aggregate . get group set ( ) ; final list < aggregate call > top aggregate calls = new array list < > ( ) ; for ( int i = __num__ ; i < aggregate . get agg call list ( ) . size ( ) ; i ++ ) { aggregate call aggregate call = aggregate . get agg call list ( ) . get ( i ) ; if ( aggregate call . is distinct ( ) ) { return ; } if ( aggregate call . get aggregation ( ) . equals ( hive groupingid . instance ) ) { return ; } if ( aggregate call . filter arg >= __num__ ) { return,functions support filter
this will block until zk comes back @$ at <PLACE_HOLDER> point @$ shutdown will complete,store . shutdown ( new callback < none > ( ) { @ override public void on error ( throwable e ) { warn ( _log @$ __str__ ) ; } @ override public void on success ( none result ) { info ( _log @$ __str__ ) ; } } ) ;,point complete which
the suite made here will all be using the <PLACE_HOLDER> from this class,multi config suite builder builder = new multi config suite builder ( testsql features suite . class ) ;,suite using tests
for single click @$ we handle editing file <PLACE_HOLDER>,if ( evt . get click count ( ) == __num__ && source instanceof j list ) { if ( ( ! fc . is multi selection enabled ( ) || fc . get selected files ( ) . length <= __num__ ) && index >= __num__ && list selection model . is selected index ( index ) && get edit index ( ) == index && edit file == null ) { edit file name ( index ) ; } else { if ( index >= __num__ ) { set edit index ( index ) ; } else { reset edit index ( ) ; } } } else if ( evt . get click count ( ) == __num__ ) { reset edit index ( ),editing file names
in case of multithreaded evaluation the composite partition aware object sink adapter used by the ot ns will take <PLACE_HOLDER> of enqueueing this inseretion on the propagation queues of the different agendas,if ( partitions enabled ) { propagation entry . insert . execute ( handle @$ context @$ working memory @$ object type conf ) ; } else { working memory . add propagation ( new propagation entry . insert ( handle @$ context @$ working memory @$ object type conf ) ) ; },adapter take care
return no more than latest detail metrics sz <PLACE_HOLDER> .,if ( detail metrics sz > __num__ ) { if ( detail metrics . size ( ) > detail metrics sz ) { grid bounded priority queue < grid cache query detail metrics adapter > latest metrics = new grid bounded priority queue < > ( detail metrics sz @$ qry_detail_metrics_priority_new_cmp ) ; latest metrics . add all ( detail metrics . values ( ) ) ; return latest metrics ; } return new array list < > ( detail metrics . values ( ) ) ; },metrics sz etc
we just return all dependencies in the days that fall within end ts and lookback as dependency links themselves do n't have <PLACE_HOLDER> .,list < string > indices = index name formatter . format type and range ( type_dependency @$ begin millis @$ end ts ) ; if ( indices . is empty ( ) ) return call . empty list ( ) ; return search . new call ( search request . create ( indices ) @$ body converters . dependency_links ) ;,themselves have them
getting actions for the null <PLACE_HOLDER> @$ which in this case means the body <PLACE_HOLDER>,for ( handler ah : action handlers ) { final action [ ] actions = ah . get actions ( null @$ this ) ; if ( actions != null ) { for ( action action : actions ) { action set . add ( action ) ; keys . add ( action mapper . key ( action ) ) ; } } },which means handler
if its all in one read then we can just take it all @$ otherwise take only the current frame size and the next iteration starts a new <PLACE_HOLDER> .,if ( current buffer != null ) { if ( current buffer . remaining ( ) >= plain . remaining ( ) ) { current buffer . put ( plain ) ; } else { byte [ ] fill = new byte [ current buffer . remaining ( ) ] ; plain . get ( fill ) ; current buffer . put ( fill ) ; } if ( current buffer . has remaining ( ) ) { return ; } else { current buffer . flip ( ) ; object command = wire format . unmarshal ( new data input stream ( new nio input stream ( current buffer ) ) ) ; do consume ( command ) ; next frame size = - __num__ ; current,size starts one
if the incoming vector to copy is random @$ then adding items from the iterator can degrade <PLACE_HOLDER> dramatically if the number of elements is large as this vector tries to stay in order as items are added @$ so it 's better to sort the other vector 's elements by index and then add them to this,copy sorted random access sparse vector ( other ) ;,vector degrade performance
disable the usual optimizations for <PLACE_HOLDER>ing join output by outer table only . in case of full join @$ the unmatched inner table tuples get appended to the end of the join 's output table thus invalidating the outer table join <PLACE_HOLDER> .,if ( m_join type == join type . full ) { m_sort direction = sort direction type . invalid ; return ; },table join sort
if we explicitly name the <PLACE_HOLDER> with a name which is both a hash name and a tree name @$ we always get a tree index . this is true even if the column type is hashable .,list < pair < string @$ index type > > passing = arrays . as list ( pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of ( __str__ @$ index type . balanced_tree ) @$ pair . of,which name column
if old table satisfies the <PLACE_HOLDER> @$ but the new table does not @$ then the old table should be dropped . this should be done @$ only if the old table is not in the list of tables to be bootstrapped which is a multi rename case . in case of multi rename @$ only the first rename should do the drop .,if ( ! repl utils . table included in repl scope ( within context . repl scope @$ new name ) ) { if ( old table in bootstrap list ) { return false ; } else { scenario = scenario . drop ; log . info ( __str__ + old name + __str__ + new name ) ; return true ; } },table satisfies policy
if permissions need a <PLACE_HOLDER> before any of the app components can run @$ we launch the <PLACE_HOLDER> activity and pass a pending intent to start the activity we are to launching now after the <PLACE_HOLDER> is completed .,if ( a info != null ) { if ( m service . get package manager internal locked ( ) . is permissions review required ( a info . package name @$ user id ) ) { i intent sender target = m service . get intent sender locked ( activity manager . intent_sender_activity @$ calling package @$ calling uid @$ user id @$ null @$ null @$ __num__ @$ new intent [ ] { intent } @$ new string [ ] { resolved type } @$ pending intent . flag_cancel_current | pending intent . flag_one_shot @$ null ) ; intent new intent = new intent ( intent . action_review_permissions ) ; int flags = intent . get flags ( ) ; flags |= intent . flag_activity_exclude_from_recents ;,permissions need review
an empty query to read <PLACE_HOLDER> .,query query = query . new builder ( ) . set limit ( int32 value . new builder ( ) . set value ( num entities ) ) . build ( ) ;,query read entities
wait till the socket has sent the <PLACE_HOLDER>,barrier . await ( __num__ @$ time unit . milliseconds ) ; return sent packet ;,socket sent packet
if the action bar did n't provide an action <PLACE_HOLDER> @$ start the emulated window one,if ( m action mode == null ) { m action mode = start support action mode from window ( wrapped callback ) ; },bar provide mode
we can only operate on encodings map because ` fr ` could not have target <PLACE_HOLDER> at all,double global mean for target class = calculate prior mean ( encoding map ) ;,fr have class
remove & save client return <PLACE_HOLDER> and insert the protocol header and service name @$ then rewrap <PLACE_HOLDER> .,z frame client = msg . unwrap ( ) ; msg . add first ( service frame . duplicate ( ) ) ; msg . add first ( mdp . c_client . new frame ( ) ) ; msg . wrap ( client ) ; msg . send ( socket ) ;,& save envelope
a sync vector only has one <PLACE_HOLDER> if not recovered from persistence @$ no valid version for the vector 's owner,if ( ! my id . is disk store id ( ) ) { return true ; },vector has key
we need the table and region to determine if this is from a mob region we do n't need to worry about hfilelink back references @$ because the hfilelink cleaner will retain <PLACE_HOLDER> .,path family = file . get parent ( ) ; path region = family . get parent ( ) ; path table = region . get parent ( ) ; table name table name = fs utils . get table name ( table ) ; string mob region = mob_regions . get ( table name ) ; if ( mob region == null ) { string tmp = mob utils . get mob region info ( table name ) . get encoded name ( ) ; if ( tmp == null ) { log . error ( __str__ @$ table name ) ; return false ; } mob region = mob_regions . put if absent ( table name @$ tmp ) ; if ( mob region == null,cleaner retain these
limiter should delay <PLACE_HOLDER> by one buffer @$ and there should almost no different in output v.s . input,for ( int i = __num__ ; i < n1a . length ; i ++ ) { if ( math . abs ( out1a [ i ] ) > __num__ ) throw new exception ( __str__ ) ; if ( math . abs ( out2a [ i ] ) > __num__ ) throw new exception ( __str__ ) ; } synth . close ( ) ;,limiter delay audio
one property block can only contains at most <PLACE_HOLDER> x 8 byte parts @$ one for header and 3 for coordinates,if ( coordinate . length > geometry type . get max supported dimensions ( ) ) { throw new unsupported operation exception ( __str__ + geometry type . get max supported dimensions ( ) + __str__ ) ; },block contains 3
minimal delay until the java process object <PLACE_HOLDER> that the process is gone this will not let the test fail predictably if the process is actually in fact going away @$ but it would create frequent failures . not ideal @$ but the best we can do without severely prolonging the test,thread . sleep ( __num__ ) ;,process object deletes
at this point there should be : 1 k flow <PLACE_HOLDER> in the active queue 9@$001 flow <PLACE_HOLDER> in the swap queue 10 k flow <PLACE_HOLDER> swapped to disk,for ( int i = __num__ ; i < __num__ ; i ++ ) { final flow file record flow file = queue . poll ( exp ) ; assert not null ( flow file ) ; assert equals ( __num__ @$ queue . get queue diagnostics ( ) . get local queue partition diagnostics ( ) . get unacknowledged queue size ( ) . get object count ( ) ) ; assert equals ( __num__ @$ queue . get queue diagnostics ( ) . get local queue partition diagnostics ( ) . get unacknowledged queue size ( ) . get byte count ( ) ) ; queue . acknowledge ( collections . singleton ( flow file ) ) ; assert equals ( __num__ @$ queue . get,files files file
something crashed . stop the <PLACE_HOLDER> .,common utils . sleep ms ( __num__ ) ;,something stop app
has next call above sets <PLACE_HOLDER> up,return m_inner itr . next ( ) ;,call sets state
validate requested <PLACE_HOLDER>,for ( extension config req ext : request . get extensions ( ) ) { if ( ! extension registry . is available ( req ext . get name ( ) ) ) { throw new illegal argument exception ( __str__ + req ext . get name ( ) + __str__ ) ; } } if ( log . is debug enabled ( ) ) log . debug ( __str__ @$ websocket @$ to uri ) ; init ( ) ; web socket upgrade request ws req = new web socket upgrade request ( this @$ http client @$ request ) ; ws req . set upgrade listener ( upgrade listener ) ; return ws req . send async ( ) ;,validate requested extensions
if a vm bug has caused the <PLACE_HOLDER> to get freed without the reference getting cleared @$ looking it up @$ assigning it to a local and doing a gc should cause some sort of exception .,try { string s = string ref . get ( ) ; system . gc ( ) ; test object finalized = true ; } catch ( throwable t ) { error = new assertion failed error ( __str__ + t + __str__ ) ; },bug caused object
iterate through all the transitions until first millis is reached . use the name key and savings for whatever rule reaches the <PLACE_HOLDER> .,long millis = long . min_value ; int save millis = __num__ ; transition first = null ; transition next ; while ( ( next = next transition ( millis @$ save millis ) ) != null ) { millis = next . get millis ( ) ; if ( millis == first millis ) { first = new transition ( first millis @$ next ) ; break ; } if ( millis > first millis ) { if ( first == null ) { for ( rule rule : copy ) { if ( rule . get save millis ( ) == __num__ ) { first = new transition ( first millis @$ rule @$ i standard offset ) ; break ; } } } if (,key reaches end
execute binding . notify <PLACE_HOLDER> that the object is now unlocked,fire property change ( __str__ @$ false @$ true ) ;,binding notify listeners
see if our node matches the given key <PLACE_HOLDER> according to the match attribute on xsl : key .,x path match expr = kd . get match ( ) ; double score = match expr . get match score ( xctxt @$ test node ) ; if ( score == kd . get match ( ) . match_score_none ) continue ; return dtm iterator . filter_accept ;,node matches expression
the device is receiving <PLACE_HOLDER> from the wireless charger . update the rest position asynchronously .,if ( is powered && plug type == battery manager . battery_plugged_wireless ) { m powered wirelessly = true ; m must update rest position = true ; start detection locked ( ) ; } else { m powered wirelessly = false ; if ( m at rest ) { if ( plug type != __num__ && plug type != battery manager . battery_plugged_wireless ) { m must update rest position = false ; clear at rest locked ( ) ; } else { start detection locked ( ) ; } } },device receiving data
logger tree should not influnce <PLACE_HOLDER>,assert true ( __str__ + sizea + __str__ + sizeb @$ ( sizea - sizeb ) < __num__ ) ;,tree influnce size
exception is thrown if the context is a receiver restricted context object . as receiver restricted context is not public @$ the context type can not be checked before calling register receiver . the most likely scenario in which the exception would be thrown would be when a broadcast receiver creates a <PLACE_HOLDER> to show the user .,m screen off receiver = null ;,receiver creates screen
if the dto does n't have an <PLACE_HOLDER> for the metric @$ add with a value of 0 .,final map < string @$ long > dto metrics = snapshot dto . get status metrics ( ) ; final string field = descriptor . get field ( ) ; if ( ! dto metrics . contains key ( field ) ) { dto metrics . put ( field @$ __num__ ) ; },dto have entry
enter the started <PLACE_HOLDER> .,final completable future < string > start future = start stop . start ( true ) ; assert that ( start future . join ( ) ) . is equal to ( __str__ ) ;,the started phase
this will disable stack trace <PLACE_HOLDER> inside jul . if someone wants location info @$ they can use their own formatter or use a different logging framework like sl 4 j @$ or log 4 j,record . set source class name ( null ) ; record . set parameters ( params ) ; logger . log ( record ) ;,stack trace output
read the keep <PLACE_HOLDER> .,while ( true ) { read next word ( __str__ + configuration constants . class_keyword + __str__ + java constants . acc_interface + __str__ + java constants . acc_enum + __str__ @$ false @$ true ) ; if ( ! configuration constants . argument_separator_keyword . equals ( next word ) ) { break ; } read next word ( __str__ + configuration constants . allow_shrinking_suboption + __str__ + configuration constants . allow_optimization_suboption + __str__ + configuration constants . allow_obfuscation_suboption + __str__ ) ; if ( configuration constants . include_descriptor_classes_suboption . starts with ( next word ) ) { mark descriptor classes = true ; } else if ( configuration constants . allow_shrinking_suboption . starts with ( next word ) ) { allow shrinking = true ; } else,the keep record
check that method under test throws <PLACE_HOLDER>,try { epki . get key spec ( ( key ) null @$ ( provider ) null ) ; fail ( get name ( ) + __str__ ) ; } catch ( null pointer exception ok ) { },method throws npe
the process will throw an escalation <PLACE_HOLDER> @$ which is caught and escalated by a user org.flowable.task.service.task,assert equals ( __num__ @$ task service . create task query ( ) . task definition key ( __str__ ) . count ( ) ) ; task task = task service . create task query ( ) . single result ( ) ; assert equals ( __str__ @$ task . get name ( ) ) ;,process throw event
so let 's start thge <PLACE_HOLDER>,this . start block ( ) ; try { return get result set ( ) ; } catch ( sql exception e ) { throw new dbc exception ( e @$ connection . get data source ( ) ) ; } finally { this . end block ( ) ; },'s start block
two labels : positive or negative because we are dealing with reviews of different lengths and only one output at the final time step : use padding arrays mask arrays contain <PLACE_HOLDER> if data is present at that time step for that example @$ or 0 if data is just padding,ind array features mask = nd4j . zeros ( reviews . size ( ) @$ max length ) ; ind array labels mask = nd4j . zeros ( reviews . size ( ) @$ max length ) ; int [ ] temp = new int [ __num__ ] ; for ( int i = __num__ ; i < reviews . size ( ) ; i ++ ) { list < string > tokens = all tokens . get ( i ) ; temp [ __num__ ] = i ; for ( int j = __num__ ; j < tokens . size ( ) && j < max length ; j ++ ) { string token = tokens . get ( j ) ; ind array vector = word,arrays contain 1
if cluster management service is enabled and user did not specify a member id @$ then we will find the applicable members based on the what <PLACE_HOLDER> this region is on,if ( cc service != null && member name orid == null ) { region name = get valid region name ( region path @$ cms ) ; set < string > calculated groups = get groups containing region ( cms @$ region name ) ; if ( calculated groups . is empty ( ) ) { return result model . create error ( __str__ + region name + __str__ ) ; } if ( groups != null && ! calculated groups . contains all ( arrays . as list ( groups ) ) ) { return result model . create error ( __str__ + region name + __str__ ) ; } if ( groups == null ) { groups = calculated groups . stream ( ) .,region on group
the bounds should equal the defined app <PLACE_HOLDER> and height,assert equals ( info . app width @$ app bounds . width ( ) ) ; assert equals ( info . app height @$ app bounds . height ( ) ) ;,bounds equal width
we can not determine <PLACE_HOLDER> method is the most specific because one parameter of the first candidate was more specific and another parameter of the second candidate was more specific .,if ( best match != null && ! potential match . equals ( best match ) ) { return null ; } else { best match = potential match ; },parameter specific which
the only way we can find out if we need to quote the properties is by checking an object <PLACE_HOLDER> that we 've constructed .,if ( object name . is domain pattern ( ) ) { domain = object name . quote ( domain ) ; } if ( object name . is property value pattern ( __str__ ) ) { properties . put ( __str__ @$ object name . quote ( name ) ) ; } if ( object name . is property value pattern ( __str__ ) ) { properties . put ( __str__ @$ object name . quote ( type ) ) ; } object name = new object name ( domain @$ properties ) ; return object name ;,way checking name
we assume that only a switch to available and chat indicates user <PLACE_HOLDER> since other mode changes could be also a result of some sort of automatism,reset idle time ( ) ; break ; default : break ;,switch indicates indication
this one will overflow <PLACE_HOLDER> .,wseg = buf . offer ( size ) ; bbuf = wseg . buffer ( ) ; bbuf . put long ( __num__ ) ; wseg . release ( ) ;,one overflow buffer
since the source ca n't be split @$ the first subtask index will read <PLACE_HOLDER>,if ( should have readers ) { mockito . when ( mock . get index of this subtask ( ) ) . then return ( __num__ ) ; } else { mockito . when ( mock . get index of this subtask ( ) ) . then return ( parallelism - __num__ ) ; },index read them
even if ' v ' is a dom node @$ it always derive from object @$ so the get bean info <PLACE_HOLDER> bean info for object,if ( bi . jaxb type == object . class && dom handler != null ) w . write dom ( v @$ dom handler @$ o @$ field name ) ; else bi . serialize root ( v @$ w ) ;,info get returns
verify that thread identifier does not remove <PLACE_HOLDER> as data is lying,assert not null ( __str__ @$ events map . get ( thread id ) ) ;,identifier remove itself
register an error handler which detects validation <PLACE_HOLDER>,if ( is validating ( ) ) { result . set error handler ( new default handler ( ) { @ override public void error ( sax parse exception ex ) throws sax exception { throw ex ; } } ) ; },which detects errors
in each case get file link status returns the same file status as get file status since we 're not calling it on a link and file status <PLACE_HOLDER> are compared by path .,assert equals ( wrapper . get file status ( file ) @$ wrapper . get file link status ( file ) ) ; assert equals ( wrapper . get file status ( file via link ) @$ wrapper . get file link status ( file via link ) ) ; assert equals ( wrapper . get file status ( file via link ) @$ wrapper . get file link status ( file ) ) ;,status file fields
stopship do proper <PLACE_HOLDER> in split user mode,return code_ok ; if ( ! m injector . user manager is split system user ( ) ) { if ( device owner user id != user handle . user_system ) { return code_not_system_user ; } if ( has user setup completed ( user handle . user_system ) ) { return code_user_setup_completed ; } } else { },stopship do validation
entry has no <PLACE_HOLDER>,if ( attributes == null ) { return null ; } array list < certificate [ ] > cert chains = new array list < certificate [ ] > ( ) ; iterator < map . entry < string @$ hash map < string @$ attributes > > > it = signatures . entry set ( ) . iterator ( ) ; while ( it . has next ( ) ) { map . entry < string @$ hash map < string @$ attributes > > entry = it . next ( ) ; hash map < string @$ attributes > hm = entry . get value ( ) ; if ( hm . get ( name ) != null ) { string signature file = entry .,entry has digest
superclass does not use <PLACE_HOLDER>,return sample result . class . equals ( arg0 ) ;,superclass use types
the parsed <PLACE_HOLDER> represents the <PLACE_HOLDER> @$ the process definitions @$ and the bpmn resource @$ parse @$ and model associated with each process definition .,parsed deployment parsed deployment = parsed deployment builder factory . get builder for deployment and settings ( deployment @$ deployment settings ) . build ( ) ; bpmn deployment helper . verify process definitions do not share keys ( parsed deployment . get all process definitions ( ) ) ; bpmn deployment helper . copy deployment values to process definitions ( parsed deployment . get deployment ( ) @$ parsed deployment . get all process definitions ( ) ) ; bpmn deployment helper . set resource names on process definitions ( parsed deployment ) ;,deployment represents deployment
mark all inferred concepts that are required for the insert for persistence explicitly can avoid this potentially expensive <PLACE_HOLDER> if there are n't any inferred concepts to start with,stream < concept map > explicitly persisted = inserted . peek ( concept map -> { if ( transaction cache . get inferred instances ( ) . find any ( ) . is present ( ) ) { mark concepts for persistence ( concept map . concepts ( ) ) ; } } ) ;,concepts avoid lookup
doubling from the expected size will cause exactly one <PLACE_HOLDER> @$ except near minimum capacity .,if ( size > __num__ ) { grow using put all ( m @$ size ) ; assert capacity ( m @$ __num__ * initial capacity ) ; },doubling cause resize
on windows @$ java must keep the environment <PLACE_HOLDER>ed . order is random on unix @$ so this test does the <PLACE_HOLDER> .,if ( ! windows . is ( ) ) output = sort by lines windowsly ( output ) ; equal ( output @$ expected ) ;,test does same
note that we ca n't use 'instanceof ' because this class has a <PLACE_HOLDER> .,if ( obj . get class ( ) != different real path file value with stored chain . class ) { return false ; } different real path file value with stored chain other = ( different real path file value with stored chain ) obj ; return real rooted path . equals ( other . real rooted path ) && real file state value . equals ( other . real file state value ) && logical chain during resolution . equals ( other . logical chain during resolution ) ;,note has subclass
purge on empty with reset clears <PLACE_HOLDER> .,assert equals ( __num__ @$ queue view . get queue size ( ) ) ; assert equals ( __num__ @$ queue view . get enqueue count ( ) ) ; assert equals ( __num__ @$ queue view . get dequeue count ( ) ) ; produce ( __num__ ) ; consume ( __num__ ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ; assert equals ( __num__ @$ queue view . get enqueue count ( ) ) ; assert equals ( __num__ @$ queue view . get dequeue count ( ) ) ; execute purge ( __str__ + get destination name ( ) ) ;,purge clears everything
interrupted exception can not be thrown by runnable.run @$ so we must wrap it . interrupts can be caught by both the evaluator and the abstract queue visitor . the former will unwrap the <PLACE_HOLDER> and propagate it as is ; the latter will throw a new <PLACE_HOLDER> .,throw scheduler exception . of interruption ( ie @$ sky key ) ;,former unwrap ie
the first update is capturing the start <PLACE_HOLDER> @$ the second update is capturing the error,verify ( workspace dao @$ times ( __num__ ) ) . update ( workspace captor . capture ( ) ) ; workspace ws = workspace captor . get all values ( ) . get ( __num__ ) ; assert not null ( ws . get attributes ( ) . get ( stopped_attribute_name ) ) ; assert true ( boolean . value of ( ws . get attributes ( ) . get ( stopped_abnormally_attribute_name ) ) ) ; assert equals ( ws . get attributes ( ) . get ( error_message_attribute_name ) @$ __str__ ) ;,update capturing time
restart so that new config takes <PLACE_HOLDER>,after class ( ) ; before class ( ) ; final table name table name = table name . value of ( name . get method name ( ) ) ; try ( admin admin = connection . get admin ( ) ; region locator rl = connection . get region locator ( table name ) ) { util . create table ( table name @$ __str__ ) ; h region location loc = rl . get all region locations ( ) . get ( __num__ ) ; region info parent = loc . get region ( ) ; long rid = __num__ ; byte [ ] split key = bytes . to bytes ( __str__ ) ; region info splita = region info builder . new builder,config takes effect
let 's give a namespace to our application window . this way @$ if someone uses the same <PLACE_HOLDER> for different applications @$ we do n't get unwanted style conflicts .,listing . set column alignment ( __str__ @$ table . align_right ) ;,someone uses namespace
use jni handle <PLACE_HOLDER> as id,write objectid ( get address value ( handle addr ) ) ;,jni handle address
only the owning user can change the <PLACE_HOLDER> .,if ( owning user id != calling user id ) { return false ; },user change setting
this method exists because heap is the place clients should ask this <PLACE_HOLDER> @$ and to aggregate all the reasons allocation might be disallowed .,return no allocation verifier . is active ( ) || gc impl . collection in progress . get state ( ) ;,clients ask method
os gi class loaders must implement bundle <PLACE_HOLDER>,final class loader class loader = info . get class loader ( ) ; settings . put ( org . hibernate . cfg . available settings . scanner @$ new osgi scanner ( ( ( bundle reference ) class loader ) . get bundle ( ) ) ) ; osgi class loader . add class loader ( class loader ) ; final class loader prevcl = thread . current thread ( ) . get context class loader ( ) ; try { thread . current thread ( ) . set context class loader ( class loader ) ; return bootstrap . get entity manager factory builder ( info @$ settings @$ new os gi class loader service impl ( osgi class loader @$ osgi service util ) ),loaders implement scanner
toolbar buttons pass a null <PLACE_HOLDER> to this method . tool bar or function compare panel .,if ( is toolbar button action || function comparison panel . is ancestor of ( source component ) ) { listing code comparison panel dual listing panel = function comparison panel . get dual listing panel ( ) ; boolean is showing dual listing = ( dual listing panel != null ) && dual listing panel . is visible ( ) ; boolean source isa dual field panel = is showing dual listing && dual listing panel . is ancestor of ( source component ) && ( source component instanceof field panel ) ; listing panel listing panel = null ; if ( source isa dual field panel ) { listing panel = dual listing panel . get listing panel ( ( field panel ) source component ),buttons pass value
now turn back on and get data . here we also get the previously sent <PLACE_HOLDER> of urgent data as it is still in the urgent buffer,received string = new string ( my bytes @$ __num__ @$ total bytes read ) ;,previously sent part
at this point we are ready to add another rowset to 'this ' object check the size of vec join type and vec row <PLACE_HOLDER> in join,vec row sets injoin . add ( c rowset ) ;,size join sets
update already notified <PLACE_HOLDER> .,synchronized ( this ) { if ( observed attribute != null && observed attribute . equals ( attribute ) ) return ; observed attribute = attribute ; cleanup is complex type attribute ( ) ; int index = __num__ ; for ( observed object o : observed objects ) { reset already notified ( o @$ index ++ @$ observed_attribute_error_notified | observed_attribute_type_error_notified ) ; } },update notified values
the content provider does not support canonical <PLACE_HOLDER> so we backup the default,if ( canonical sound == null ) { return settings . system . default_notification_uri ; },provider support uri
entities now prefers shorted <PLACE_HOLDER> if aliased,assert equals ( __str__ @$ custom out ) ;,entities prefers names
fetch the multi auto complete text <PLACE_HOLDER> and set an adapter and tokenizer,multi auto complete text view mactv = find view by id ( r . id . widgets_multiautocompletetextview ) ; mactv . set tokenizer ( new multi auto complete text view . comma tokenizer ( ) ) ; mactv . set adapter ( new array adapter < > ( this @$ android . r . layout . simple_dropdown_item_1line @$ cheeses . s cheese strings ) ) ;,auto complete view
order matters @$ as the cluster <PLACE_HOLDER> modifies the event bus <PLACE_HOLDER> .,if ( allow clustering ) { set event bus options ( conf @$ options ) ; initialize cluster options ( conf @$ options ) ; },options modifies options
no restrictions to default <PLACE_HOLDER> or vr 2 d <PLACE_HOLDER> .,if ( display id == default_display || ( display id != invalid_display && display id == m service . m vr2d display id ) ) { return true ; },restrictions display display
credentials were verified . verify that the principal has all allowed <PLACE_HOLDER> for authentication,if ( ! has allowed authentication rules ( info . get principals ( ) @$ ldap context factory ) ) { throw new naming exception ( __str__ ) ; } return info ;,principal allowed characters
no name @$ kids carry <PLACE_HOLDER>,break ;,name carry index
setters also accept unknown enum value <PLACE_HOLDER> .,builder . set field ( optional nested enum field @$ unknown6543 ) ; builder . set repeated field ( repeated nested enum field @$ __num__ @$ unknown4321 ) ; builder . set repeated field ( packed nested enum field @$ __num__ @$ unknown5432 ) ; message = builder . build ( ) ;,setters accept types
have the factory build the <PLACE_HOLDER>,final object object = this . expression object factory . build object ( this . context @$ name ) ;,factory build object
request to allocate two reduce priority <PLACE_HOLDER>,final string [ ] locations = new string [ ] { host } ; allocator . send request ( create request ( job id @$ __num__ @$ resource . new instance ( __num__ @$ __num__ ) @$ locations @$ false @$ true ) ) ; allocator . schedule all reduces ( ) ; allocator . make remote request ( ) ; nm . node heartbeat ( true ) ; rm . drain events ( ) ; allocator . send request ( create request ( job id @$ __num__ @$ resource . new instance ( __num__ @$ __num__ ) @$ locations @$ false @$ false ) ) ; int assigned container ; for ( assigned container = __num__ ; assigned container < __num__ ; ) { assigned container +=,two reduce containers
step 3 : determine final launch bounds based on resolved windowing mode and activity requested orientation . we set bounds to empty for fullscreen mode and keep bounds as is for all other windowing modes that 's not freeform mode . one can read <PLACE_HOLDER> in relevant methods to further understand this step . we skip making adjustments if the params are fully resolved,final int resolved mode = ( launch mode != windowing_mode_undefined ) ? launch mode : display . get windowing mode ( ) ; if ( fully resolved current param ) { if ( resolved mode == windowing_mode_freeform ) { if ( current params . m preferred display id != display id ) { adjust bounds to fit in display ( display @$ out params . m bounds ) ; } adjust bounds to avoid conflict in display ( display @$ out params . m bounds ) ; } } else { if ( source != null && source . in freeform windowing mode ( ) && resolved mode == windowing_mode_freeform && out params . m bounds . is empty ( ) && source . get display id (,one read comments
variable column table models have default <PLACE_HOLDER> and 'found ' <PLACE_HOLDER> . we only want to create a key based upon the default <PLACE_HOLDER>,return variable table model . get default column count ( ) ;,models have value
all four static methods must get distinct <PLACE_HOLDER> @$ so that moo.a resolves correctly to foo.a,test ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) @$ lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,methods get names
if the object is externalizable @$ call write external . else do serializable <PLACE_HOLDER> .,if ( current class desc . is externalizable ( ) ) { orb stream . write_octet ( stream format version ) ; externalizable ext = ( externalizable ) obj ; ext . write external ( this ) ; } else { if ( current class desc . for class ( ) . get name ( ) . equals ( __str__ ) ) { this . writeutf ( ( string ) obj ) ; return ; } int stack mark = class desc stack . size ( ) ; try { object stream class next ; while ( ( next = current class desc . get superclass ( ) ) != null ) { class desc stack . push ( current class desc ) ; current class desc =,external do deserialization
after the initial data encryption <PLACE_HOLDER> expires @$ <PLACE_HOLDER> manager should regenerate a valid data encryption <PLACE_HOLDER> using the current block <PLACE_HOLDER> .,final data encryption key dek after expiration = key manager . new data encryption key ( ) ; assert not equals ( __str__ @$ dek @$ dek after expiration ) ; assert true ( __str__ @$ dek after expiration . expiry date > fake timer . now ( ) ) ;,manager regenerate key
start the delete procedure & & kill the <PLACE_HOLDER>,long proc id = proc exec . submit procedure ( new delete table procedure ( proc exec . get environment ( ) @$ table name ) ) ;,procedure kill executor
only one write <PLACE_HOLDER> at a time @$ but there is a chance two writers race after commit : thus synchronize,synchronized ( tx commit count lock ) { commit count ++ ; if ( debug tx write ) { system . out . println ( __str__ + commit count + __str__ + ( entity type ids affected != null ? entity type ids affected . length : __num__ ) ) ; } },one write entry
the project manager maintains the <PLACE_HOLDER> of the projects with the most recent being first in the list,for ( url project view : recent views ) { string url path = ghidraurl . get display string ( project view ) ; docking action action = new recent view plugin action ( url path ) ; reopen views list . add ( new view info ( action @$ project view ) ) ; tool . add action ( action ) ; },manager maintains order
reading the merged bag gets both the <PLACE_HOLDER>,assert that ( bag1 . read ( ) @$ contains in any order ( __str__ @$ __str__ @$ __str__ ) ) ; assert that ( bag2 . read ( ) @$ matchers . empty iterable ( ) ) ;,bag gets contents
verify the successful flow file has the expected <PLACE_HOLDER>,final mock flow file mock flow file = test runner . get flow files for relationship ( putorc . rel_success ) . get ( __num__ ) ; mock flow file . assert attribute equals ( putorc . absolute_hdfs_path_attribute @$ orc file . get parent ( ) . to string ( ) ) ; mock flow file . assert attribute equals ( core attributes . filename . key ( ) @$ filename ) ; mock flow file . assert attribute equals ( putorc . record_count_attr @$ __str__ ) ;,file has attributes
broken glass fish @$ which call modify handshake before get endpoint <PLACE_HOLDER> !,if ( end point . get ( ) == null ) { h request . set ( request ) ; } else { end point . get ( ) . handshake request ( request ) ; end point . set ( null ) ; },fish get information
dns ur is never have a dns <PLACE_HOLDER>,if ( curi . getuuri ( ) . get scheme ( ) . equals ( __str__ ) ) { curi . set prerequisite ( true ) ; return false ; } else if ( curi . getuuri ( ) . get scheme ( ) . equals ( __str__ ) ) { return false ; },ur have uri
check that start captive portal app sends the expected <PLACE_HOLDER> to network monitor .,m cm . start captive portal app ( wifi network ) ; wait for idle ( ) ; verify ( m wi fi network agent . m network monitor ) . launch captive portal app ( ) ;,check sends message
db must have <PLACE_HOLDER> . older versions of anki droid did n't create them for new collections .,fix integrity progress ( progress callback @$ current task ++ @$ total tasks ) ; int ixs = m db . query scalar ( __str__ ) ; if ( ixs < __num__ ) { problems . add ( __str__ ) ; storage . add indices ( m db ) ; } m db . get database ( ) . end transaction ( ) ;,db have problems
schedule delayed <PLACE_HOLDER> killing if graceful stopping will be not finished within timeout .,executor . schedule ( new runnable ( ) { @ override public void run ( ) { if ( state ( name ) == ignite state . started ) { u . error ( null @$ __str__ + timeout ms + __str__ ) ; runtime . get runtime ( ) . halt ( ignition . kill_exit_code ) ; } } } @$ timeout ms @$ time unit . milliseconds ) ;,schedule delayed execution
close the fd via the parent stream 's close <PLACE_HOLDER> . the parent will reinvoke our close <PLACE_HOLDER> @$ which is defined in the superclass abstract interruptible channel @$ but the is open logic in that <PLACE_HOLDER> will prevent this <PLACE_HOLDER> from being reinvoked .,if ( parent != null ) { ( ( java . io . closeable ) parent ) . close ( ) ; } else { nd . close ( fd ) ; },method prevent method
every one of our providers will call this <PLACE_HOLDER> @$ so only execute the logic once .,if ( initialization state != initialization state . uninitialized ) { return initialization state != initialization state . has_errors ; },one call method
need to talk about include all <PLACE_HOLDER> .,inc tag list . set prototype cell value ( __str__ ) ; inc tag scroller = new j scroll pane ( inc tag list ) ; inc tag scroller . set horizontal scroll bar policy ( javax . swing . j scroll pane . horizontal_scrollbar_as_needed ) ; inc tag scroller . set vertical scroll bar policy ( javax . swing . j scroll pane . vertical_scrollbar_as_needed ) ;,need include tags
response account <PLACE_HOLDER>,m ams . add account as user ( m mock account manager response @$ null @$ __str__ @$ null @$ true @$ null @$ user handle . user_system ) ;,response account type
there is one adjp unary rewrite to ad but otherwise all have <PLACE_HOLDER> or adjp,non terminal info . put ( __str__ @$ new string [ ] [ ] { { right @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } ) ;,all have jj
app 1 gets all <PLACE_HOLDER> in default partition,assert . assert equals ( __num__ @$ scheduler node2 . get num containers ( ) ) ;,app gets containers
let explicit command line override the <PLACE_HOLDER> .,m_port = ports . next client ( ) ; m_admin port = ports . next admin ( ) ; m_internal port = ports . next ( ) ; m_zk interface = __str__ + ports . next ( ) ;,line override default
upadate <PLACE_HOLDER> resource should n't be called before start <PLACE_HOLDER> @$ otherwise @$ node manager can not find the <PLACE_HOLDER>,try { client . update container resource ( container ) ; fail ( __str__ ) ; } catch ( yarn exception e ) { assert true ( __str__ @$ e . get message ( ) . contains ( __str__ ) ) ; },resource find container
this should be a relatively uncommon case @$ window for this happening is a few milliseconds when an admin explicitly creates a <PLACE_HOLDER> @$ after the index has been populated . we can improve this later on by replicating the <PLACE_HOLDER> validation logic down here @$ or rethinking where we validate <PLACE_HOLDER>s . for now @$ we just kill these transactions .,throw new transaction failure exception ( status . transaction . constraints changed @$ __str__ + __str__ + __str__ @$ latest constraint introducing tx @$ last committed tx when transaction started ) ;,admin creates transaction
else : no new winner @$ ergo no reason to send <PLACE_HOLDER> to neighbors,messenger . send message ( incident message scope @$ shortest distance seen on this iteration ) ;,reason send message
style rules should always have the lowest <PLACE_HOLDER> .,return - __num__ ;,rules have priority
if the stream is sorted then it should also be <PLACE_HOLDER>ed so the following will also preserve the sort <PLACE_HOLDER>,return new reference pipeline . stateful op < t @$ t > ( upstream @$ stream shape . reference @$ stream op flag . is_distinct | stream op flag . not_sized ) { < p_in > node < t > reduce ( pipeline helper < t > helper @$ spliterator < p_in > spliterator ) { terminal op < t @$ linked hash set < t > > reduce op = reduce ops . < t @$ linked hash set < t > > make ref ( linked hash set :: new @$ linked hash set :: add @$ linked hash set :: add all ) ; return nodes . node ( reduce op . evaluate parallel ( helper @$ spliterator ) ) ; } @ override public,following preserve order
write 1000 lines and then save @$ this will reduce disk writing frequency and at the same time will keep <PLACE_HOLDER> under control,if ( ( line count % __num__ ) == __num__ ) { output . flush ( ) ; },time keep freelist
this overload has no uncompression <PLACE_HOLDER>,w . write object ( __num__ @$ value ) ;,overload has problem
invocation in the code under test uses different <PLACE_HOLDER> and should fail immediately this helps with debugging and is essential for mockito strictness,production code . simple method ( mock @$ __num__ ) ;,invocation uses implementation
did n't get m unthemed entries <PLACE_HOLDER> @$ bail out ...,if ( s themed resource cache_m unthemed entries field == null ) { return ; },m unthemed field
add an expect <PLACE_HOLDER> to make sure that the item does n't already exist @$ since it 's supposed to be new,if ( get local save behavior ( ) != save behavior . clobber && ! internal expected value assertions . contains key ( field . name ( ) ) && field . get generate strategy ( ) != dynamodb auto generate strategy . always ) { internal expected value assertions . put ( field . name ( ) @$ new expected attribute value ( ) . with exists ( false ) ) ; },an expect call
check system properties for the default <PLACE_HOLDER> then use secure settings <PLACE_HOLDER> @$ if any .,string default delay = system properties . get ( __str__ + settings . global . pac_change_delay @$ default_delays ) ; string val = settings . global . get string ( cr @$ settings . global . pac_change_delay ) ; return ( val == null ) ? default delay : val ;,properties use delay
the user asked for stats to be collected . some stats like number of rows require a <PLACE_HOLDER> of the data however @$ some other stats @$ like number of files @$ do not require a complete <PLACE_HOLDER> update the stats which do not require a complete <PLACE_HOLDER> .,task < ? > stat task = null ; if ( conf . get bool var ( hive conf . conf vars . hivestatsautogather ) ) { basic stats work basic stats work = new basic stats work ( load table work ) ; basic stats work . set no stats aggregator ( true ) ; basic stats work . set clear aggregator stats ( true ) ; stats work column stats work = new stats work ( ts . table handle @$ basic stats work @$ conf ) ; stat task = task factory . get ( column stats work ) ; } if ( stat task != null ) { child task . add dependent task ( stat task ) ; },which require scan
if relationship label is null @$ then <PLACE_HOLDER> label is mentioned at both ends @$ use the respective end 's <PLACE_HOLDER> label as relationship label,if ( relationship label == null ) { relationship label = get legacy edge label ( entity type @$ attr name ) ; } if ( attribute == null ) { cardinality cardinality = end def . get cardinality ( ) ; boolean is optional = true ; atlas constraint def constraint = null ; if ( cardinality == cardinality . set ) { attr type name = atlas base type def . get array type name ( attr type name ) ; } if ( relationship def . get relationship category ( ) == relationship category . composition ) { if ( end def . get is container ( ) ) { constraint = new atlas constraint def ( constraint_type_owned_ref ) ; } else { is optional,label use edge
event listener registry defines 3 <PLACE_HOLDER> to register listeners :,event listener registry . add duplication strategy ( new custom duplication strategy ( ) ) ;,listener defines ways
when opearting with groups . the group must have <PLACE_HOLDER> so changes to take effect . otherwise group will be lost after loggingout,try { this . op set pers presence1 . subscribe ( group @$ this . fixture . userid2 ) ; synchronized ( o ) { o . wait ( __num__ ) ; } } catch ( exception ex ) { fail ( __str__ + group . get group name ( ) + __str__ + ex . get message ( ) ) ; },group have entries
last file should have 1 <PLACE_HOLDER>,mff = runner . get flow files for relationship ( select hiveql . rel_success ) . get ( __num__ ) ; in = new byte array input stream ( mff . to byte array ( ) ) ; assert equals ( __num__ @$ get number of records from stream ( in ) ) ; mff . assert attribute exists ( __str__ ) ; assert equals ( integer . to string ( __num__ ) @$ mff . get attribute ( __str__ ) ) ; assert equals ( __str__ @$ mff . get attribute ( __str__ ) ) ; runner . clear transfer state ( ) ;,file have record
filter does not change the <PLACE_HOLDER> ordering . filter rel does not permute the <PLACE_HOLDER> . all cor vars produced by filter will have the same output positions in the <PLACE_HOLDER> rel .,return register ( rel @$ rel builder . build ( ) @$ frame . old to new outputs @$ frame . cor def outputs ) ;,rel permute input
we return the first found <PLACE_HOLDER> .,if ( paths . size ( ) > __num__ ) { final_symlink = paths . get ( __num__ ) + symlink [ symlink . length - __num__ ] ; } else { final_symlink = symlink [ symlink . length - __num__ ] ; },first found dir
note : this test encodes the <PLACE_HOLDER> that limit spec sorts numbers like strings ; we might want to change this in the future .,assert . assert equals ( immutable list . of ( test rows list . get ( __num__ ) @$ test rows list . get ( __num__ ) ) @$ limit fn . apply ( sequences . simple ( test rows list ) ) . to list ( ) ) ;,test encodes implementations
this estimate will not take into account the <PLACE_HOLDER> saved by inlining the keys .,return versioned stats disklru region entry off heap object key . class ;,estimate take memory
it does not sound like it makes a lot of sense to fail the getting of the presence status of the specified signin name just because one of the possibly many operation set presence instances has failed . additionally @$ the native counterpart will swallow any java <PLACE_HOLDER> anyway .,if ( t instanceof thread death ) throw ( thread death ) t ; else t . print stack trace ( system . err ) ;,counterpart swallow exceptions
this loop has child <PLACE_HOLDER> . loop peeling could explode graph size .,if ( loop . loop ( ) . get children ( ) . size ( ) > __num__ ) { return false ; },loop has elements
loopback produces empty <PLACE_HOLDER> .,if ( hw addr != null && hw addr . length > __num__ ) { string mac = byte array2 hex string ( hw addr ) ; if ( ! macs . contains ( mac ) ) macs . add ( mac ) ; },loopback produces list
ensure that the schema has the virtual <PLACE_HOLDER> added,virtual column provider factory . add built in virtual columns to schema ( schema ) ;,schema has columns
note : a null search name will match the first pu <PLACE_HOLDER> found,string search name = null ;,name match object
other users may already have a <PLACE_HOLDER> on this combining value .,accum = combine fn . create accumulator ( ) ; is cleared = true ;,users have handle
prevent npe and make the unit test go equals ordinal <PLACE_HOLDER>,other . set indextype ( index type . normal ) ; index meta . set indextype ( index type . normal ) ; assertions . assert not equals ( index meta @$ other ) ; other = new index meta ( ) ; other . set indextype ( index type . normal ) ; index meta . set indextype ( index type . normal ) ; assertions . assert equals ( index meta @$ other ) ;,test go index
user certificate store @$ does not bypass static <PLACE_HOLDER> .,if ( info . target sdk version <= build . version_codes . m && ! info . is privileged app ( ) ) { builder . add certificates entry ref ( new certificates entry ref ( user certificate source . get instance ( ) @$ false ) ) ; },store bypass defaults
use this setting to improve performance if you know that changes in content do not change the layout <PLACE_HOLDER> of the recycler view,m recycler view . set has fixed size ( true ) ; recycler view . layout manager layout manager = new linear layout manager ( get activity ( ) ) ; m recycler view . set layout manager ( layout manager ) ; m recycler view . set adapter ( adapter ) ; m recycler view . add item decoration ( new spaces item decoration ( quick return utils . dp2px ( get activity ( ) @$ __num__ ) ) ) ; int header height = get resources ( ) . get dimension pixel size ( r . dimen . facebook_header_height ) ; int footer height = get resources ( ) . get dimension pixel size ( r . dimen . facebook_footer_height ) ; int header translation =,changes change size
note : scala and wsdl have no <PLACE_HOLDER>,return arrays . as list ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ;,scala have attributes
end probe phase @$ iterator build side <PLACE_HOLDER> .,collector . collect ( build iter . get row ( ) ) ; while ( build iter . advance next ( ) ) { collector . collect ( build iter . get row ( ) ) ; },iterator build effects
the highest one bit should be 1 @$ which means the <PLACE_HOLDER> as a number is negative,return word < __num__ ;,which means string
dispatch the callback with empty arrays which means a <PLACE_HOLDER> .,on request permissions result ( request code @$ new string [ __num__ ] @$ new int [ __num__ ] ) ; return ;,which means failure
a volt db <PLACE_HOLDER> to avoid using exceptions for flow control .,if ( ! is option ) { if ( ! prefer to throw ) { return unexpected token ( ) ; } throw unexpected token ( ) ; },volt db extension
user has specified a <PLACE_HOLDER> @$ we can continue ...,if ( connection != null ) { database db = new database ( this @$ connection ) ; db . share variables with ( this ) ; try { db . connect ( parent job . get transaction id ( ) @$ null ) ; string real schemaname = environment substitute ( schemaname ) ; string real tablename = environment substitute ( tablename ) ; if ( db . check table exists ( real tablename ) ) { if ( log . is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ ) + real tablename + base messages . get string ( pkg @$ __str__ ) ) ; } if ( schemaname != null ) { real tablename =,user specified connection
also append sub 1 file <PLACE_HOLDER>,dfs test util . append file ( hdfs @$ sub1file2 @$ blocksize ) ; hdfs . create snapshot ( dir @$ __str__ ) ; out . close ( ) ;,1 file 2
add all added <PLACE_HOLDER> to this graph,for ( filter filter : added filters ) { add filter ( filter ) ; },all added filters
the application has both 64 and 32 bit bundled libraries . we check here that the app declares multi arch support @$ and warn if it does n't . we will be lenient here and record both ab is . the primary will be the <PLACE_HOLDER> that 's higher on the list @$ i.e @$ a device that 's configured to prefer 64 bit,if ( has32 bit libs && has64 bit libs ) { if ( ( pkg . application info . flags & application info . flag_multiarch ) == __num__ ) { slog . e ( package manager service . tag @$ __str__ + pkg + __str__ ) ; } if ( vm runtime . is64 bit instruction set ( get preferred instruction set ( ) ) ) { primary cpu abi = build . supported_64_bit_abis [ __num__ ] ; secondary cpu abi = build . supported_32_bit_abis [ __num__ ] ; } else { primary cpu abi = build . supported_32_bit_abis [ __num__ ] ; secondary cpu abi = build . supported_64_bit_abis [ __num__ ] ; } } else { primary cpu abi = null ; secondary cpu abi =,device see one
if the next sublist has no <PLACE_HOLDER> @$ add this one and then break @$ otherwise just break,if ( next index - curr index == __num__ ) { next index ++ ; },sublist has children
since the db is running @$ exclude the <PLACE_HOLDER> files,return ! __str__ . equals ( sub path ) && ! sub path . ends with ( __str__ ) ;,files exclude root
this assignment should n't cause <PLACE_HOLDER> ' to be inferred as undefined above .,test types with common externs ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,assignment cause '
bind and start to accept incoming <PLACE_HOLDER> .,server bootstrap . bind ( new inet socket address ( address @$ port ) ) ;,bind accept connections
the following boolean logic implements the <PLACE_HOLDER> above,if ( left == left2 && right == right2 && key length <= r2 . key length && r2 . pattern . region matches ( __num__ @$ pattern @$ __num__ @$ len ) ) { return ( flags == r2 . flags ) || ( ! ( ( flags & anchor_start ) != __num__ ) && ! ( ( flags & anchor_end ) != __num__ ) ) || ( ( ( r2 . flags & anchor_start ) != __num__ ) && ( ( r2 . flags & anchor_end ) != __num__ ) ) ; },logic implements comment
close all the given file media <PLACE_HOLDER> on error case .,if ( error string != null ) { for ( media item item : playlist ) { if ( item instanceof file media item ) { ( ( file media item ) item ) . increase ref count ( ) ; ( ( file media item ) item ) . decrease ref count ( ) ; } } throw new illegal argument exception ( error string ) ; },given file items
must iterate over every identifier @$ since we do n't know which ones will match any given <PLACE_HOLDER>,for ( identifier id : identifiers . values ( ) ) { if ( ! id . is class ) continue ; for ( int i = __num__ ; i < starting classes . size ( ) ; ++ i ) { if ( starting patterns [ i ] . matcher ( id . name ) . matches ( ) ) { dep queue . add last ( id ) ; closure . add ( id ) ; matched [ i ] = true ; if ( verbose ) { log . info ( __str__ + id . name ) ; } break ; } } } for ( int i = __num__ ; i < starting classes . size ( ) ; ++ i ) { if,ones match pattern
effective host name minus domain must not contain any <PLACE_HOLDER>,string effective host without domain = host . substring ( __num__ @$ host . length ( ) - cookie domain . length ( ) ) ; return effective host without domain . index of ( __str__ ) == - __num__ ;,name contain dots
visit the class type after interface types which is the order the obj c compiler visits the <PLACE_HOLDER> .,type mirror class type = null ; for ( type mirror super type : direct supertypes ( type ) ) { if ( ! result ) { return false ; } if ( is class ( super type ) ) { class type = super type ; } else { visit type hierarchy objc order ( super type @$ visitor ) ; } } if ( class type != null && result ) { result = visit type hierarchy objc order ( class type @$ visitor ) ; } return result ;,compiler visits inspector
! f create entity ref nodes move <PLACE_HOLDER> of entity ref before the entity ref . remove entity ref .,f current node index = f deferred document impl . get parent node ( f current node index @$ false ) ;,ref nodes nodes
remove core attributes since the user does not want <PLACE_HOLDER> @$ unless they are in the attribute list . attribute list always wins,for ( string core attribute : core attributes ) { if ( ! attributes . contains ( core attribute ) ) { result . remove ( core attribute ) ; } },user want them
unit test version does not handle launch wake <PLACE_HOLDER>,do nothing ( ) . when ( this ) . acquire launch wakelock ( ) ; do return ( m keyguard controller ) . when ( this ) . get keyguard controller ( ) ; m launching activity wake lock = mock ( power manager . wake lock . class ) ; initialize ( ) ;,version handle lock
if padding <PLACE_HOLDER> equals background <PLACE_HOLDER> @$ return null to indicate we do n't have to paint it .,if ( padding color == background color ) { return null ; } return padding color ;,color equals color
process height points reversed <PLACE_HOLDER> @$ becase for same pos x @$ we always want the highest point,priority queue < integer > queue = new priority queue < integer > ( __num__ @$ collections . reverse order ( ) ) ; queue . offer ( __num__ ) ; int prev = queue . peek ( ) ; for ( height point p : height points ) { if ( p . height < __num__ ) { queue . offer ( - p . height ) ; } else { queue . remove ( p . height ) ; } int curr peak = queue . peek ( ) ; if ( curr peak != prev ) { rst . add ( new int [ ] { p . x @$ curr peak } ) ; prev = curr peak ; } } return rst ;,points reversed order
find the place the new entry should go @$ ensuring an entry with the same name does n't already exist along the <PLACE_HOLDER>,directory entry prev = null ; directory entry curr = table [ index ] ; while ( curr != null ) { if ( curr . name ( ) . equals ( entry . name ( ) ) ) { if ( overwrite existing ) { if ( prev != null ) { prev . next = entry ; } else { table [ index ] = entry ; } entry . next = curr . next ; curr . next = null ; entry . file ( ) . increment link count ( ) ; return ; } else { throw new illegal argument exception ( __str__ + entry . name ( ) + __str__ ) ; } } prev = curr ; curr = curr .,entry exist path
check if font has correct <PLACE_HOLDER> of letters,if ( font . length != __num__ ) { system . out . println ( __str__ + font . length + __str__ ) ; system . out . println ( __str__ ) ; } for ( int i = __num__ ; i < font . length ; i ++ ) { letter = font [ i ] ; b = __num__ ; if ( ( letter . length != n ) && ( n == - __num__ ) ) { n = letter . length ; } else if ( letter . length != n ) { system . out . println ( __str__ + i + __str__ + letter . length + __str__ + n + __str__ ) ; system . out . println ( __str__ ),font has numbers
delete <PLACE_HOLDER> is complete before <PLACE_HOLDER> so not relevant @$ get next delete <PLACE_HOLDER>,delete range = delete range it . has next ( ) ? delete range it . next ( ) : null ; break ; case range1_completely_after_range2 :,range get range
at this point either m whitelisted country <PLACE_HOLDER> or m blacklisted country <PLACE_HOLDER> is null . we assume no countries are to be excluded . here @$ we correct this assumption based on the contents of either lists .,set < string > excluded countries = new hash set < > ( ) ; if ( m whitelisted country isos == null ) { excluded countries . add all ( m blacklisted country isos ) ; } else { excluded countries . add all ( country info map . key set ( ) ) ; excluded countries . remove all ( m whitelisted country isos ) ; },m whitelisted os
broker requires a concrete member <PLACE_HOLDER> to be allowed to join the group . update member <PLACE_HOLDER> and send another join group request in next cycle .,if ( error == errors . member_id_required ) { synchronized ( abstract coordinator . this ) { abstract coordinator . this . generation = new generation ( offset commit request . default_generation_id @$ join response . data ( ) . member id ( ) @$ null ) ; abstract coordinator . this . reset state and rejoin ( ) ; } future . raise ( error ) ; } else { log . error ( __str__ @$ error . message ( ) ) ; future . raise ( new kafka exception ( __str__ + error . message ( ) ) ) ; },broker requires id
args can contain <PLACE_HOLDER> like location macros @$ so extract any inputs we find .,for ( arg arg : preprocessor flags . get other flags ( ) . get all flags ( ) ) { buildable support . derive inputs ( arg ) . for each ( input consumer ) ; },args contain things
this is not the last <PLACE_HOLDER> @$ so our offset should record the next <PLACE_HOLDER> to be used ...,if ( event row number < ( total number of rows - __num__ ) ) { this . current row number = event row number ; this . restart rows to skip = this . current row number + __num__ ; return offset using position ( this . restart rows to skip ) ; },offset record row
the users of this method do not need a nano <PLACE_HOLDER>,return clock . system ( zone ) ;,users need time
this method will be called on rotation suggestion changes even if the proposed rotation is not valid for the top app . use invalid rotation <PLACE_HOLDER> as a signal to remove the rotate button if shown .,if ( ! is valid ) { set rotate suggestion button state ( false ) ; return ; },method use values
and it is not always true that a bigger array uses more <PLACE_HOLDER> than a smaller one,return ( ) -> arrays . stream ( value type . array types ( ) ) . filter ( t -> t != value type . string_array && t != value type . string_alphanumeric_array && t != value type . string_ascii_array && t != value type . string_bmp_array ) . iterator ( ) ;,array uses memory
the test program creates a class file from the header stored above and adding the content of a source debug extension attribute made of the character 0 x 02 repeated 68000 times . this attribute does n't follow the <PLACE_HOLDER> specified in jsr 45 but it 's fine because this test just checks that the jvm is able to load a class file with,byte [ ] buf = new byte [ header . length + attr size ] ; for ( int i = __num__ ; i < header . length ; i ++ ) { buf [ i ] = header [ i ] ; } for ( int i = __num__ ; i < attr size ; i ++ ) { buf [ header . length + i ] = ( byte ) __num__ ; } class c = loader . define class ( __str__ @$ buf @$ __num__ @$ buf . length ) ; system . out . println ( __str__ ) ;,attribute follow rules
we hold the wake lock as long as the service is processing <PLACE_HOLDER> .,synchronized ( this ) { if ( ! m service processing ) { m service processing = true ; try { m run wake lock . acquire ( __num__ * __num__ * __num__ ) ; m launch wake lock . release ( ) ; } catch ( throwable e ) { file log . e ( e ) ; m service processing = false ; } } },service processing callbacks
customers do n't see this <PLACE_HOLDER>,if ( ! ( boolean ) result ) { throw new entry not found exception ( __str__ ) ; },customers see entry
check if the workers go <PLACE_HOLDER> ...,assert true ( appender . log contains ( __str__ ) ) ; assert true ( appender . log contains ( __str__ ) ) ; assert true ( appender . log contains ( __str__ ) ) ;,workers go room
tests throwable proxy util.ste <PLACE_HOLDER> to step <PLACE_HOLDER>,new throwable proxy ( t ) ;,tests throwable calls
only labels changed . since we do n't know which properties this entity has let 's include all <PLACE_HOLDER> for the changed labels .,if ( properties . length == __num__ ) { set . matching descriptors ( descriptors @$ changed entity tokens ) ; } else if ( changed entity tokens . length == __num__ ) { set . matching descriptors for partial list of properties ( descriptors @$ unchanged entity tokens @$ properties ) ; } else { set . matching descriptors ( descriptors @$ changed entity tokens ) ; set . matching descriptors for partial list of properties ( descriptors @$ unchanged entity tokens @$ properties ) ; },'s include indexes
we catch everything because right now the masters do not require authentication . so delay reporting errors to the user until the servers return 401 <PLACE_HOLDER> .,log . debug ( __str__ + __str__ + __str__ @$ e ) ;,servers return codes
negative offset means <PLACE_HOLDER>,return cursor offset >= __num__ ;,offset means precision
open a new region which uses this <PLACE_HOLDER>,table descriptor htd = table descriptor builder . new builder ( table name . value of ( __str__ ) ) . set column family ( column family descriptor builder . of ( b ) ) . build ( ) ; region info hri = region info builder . new builder ( htd . get table name ( ) ) . build ( ) ; chunk creator . initialize ( mem storelab impl . chunk_size_default @$ false @$ __num__ @$ __num__ @$ __num__ @$ null ) ; test_util . create localh region ( hri @$ htd @$ wal ) . close ( ) ; region server services rs services = mock ( region server services . class ) ; when ( rs services . get server name ( ),which uses table
rank 2 denotes the second largest <PLACE_HOLDER> .,i2 = rankx >= __num__ ? __num__ : __num__ ; j2 = ranky >= __num__ ? __num__ : __num__ ; k2 = rankz >= __num__ ? __num__ : __num__ ; l2 = rankw >= __num__ ? __num__ : __num__ ;,rank denotes coordinate
script object mirror has some object equality <PLACE_HOLDER> for entries,if ( o instanceof map ) { if ( depth > __num__ || ! seen . add ( o ) ) { return true ; } map map = ( map ) o ; map . for each ( ( k @$ v ) -> { if ( recurse cyclic ( depth + __num__ @$ v @$ seen ) ) { map . put ( k @$ __str__ + v . get class ( ) . get name ( ) ) ; } } ) ; } else if ( o instanceof list ) { if ( depth > __num__ || ! seen . add ( o ) ) { return true ; } list list = ( list ) o ; int count = list . size,mirror has rules
json stringer can cause an <PLACE_HOLDER> when the json to handle is too big .,response . result = null ; response . error = m object mapper . convert value ( e . get message ( ) @$ json object . class ) ; json object = m object mapper . convert value ( response @$ json object . class ) ; response string = json object . to string ( ) ;,stringer cause exception
harmony expected 1<PLACE_HOLDER> @$ but the ri and android report <PLACE_HOLDER> .,assert equals ( __num__ @$ f . get channel ( ) . position ( ) ) ;,ri report byte
should flush latch 1 and latch <PLACE_HOLDER> and then run latch 3 immediately,assert true ( latch3 . await ( __num__ @$ time unit . milliseconds ) ) ; assert equals ( __num__ @$ latch1 . get count ( ) ) ; assert equals ( __num__ @$ latch2 . get count ( ) ) ;,flush latch 2
the map reduce tokens are provided so that tasks can spawn <PLACE_HOLDER> if they wish to . the tasks authenticate to the job tracker via the map reduce delegation tokens .,if ( system . getenv ( __str__ ) != null ) { conf . set ( __str__ @$ system . getenv ( __str__ ) ) ; } return conf ;,tasks spawn them
add a custom equality assertion because the resulting geometry may have its constituent <PLACE_HOLDER> in a different order,bi function < object @$ object @$ boolean > equality function = ( left @$ right ) -> { if ( left == null && right == null ) { return true ; } if ( left == null || right == null ) { return false ; } ogc geometry left geometry = ogc geometry . from text ( left . to string ( ) ) ; ogc geometry right geometry = ogc geometry . from text ( right . to string ( ) ) ; return left geometry . difference ( right geometry ) . is empty ( ) && right geometry . difference ( left geometry ) . is empty ( ) ; } ;,geometry have sides
base length next line handles both positive <PLACE_HOLDER> and also scale mismatch,if ( scale != - lhs . exp ) reqdig = ( reqdig + scale ) + lhs . exp ; reqdig = ( reqdig - ( ( rhs . mant . length - __num__ ) ) ) - rhs . exp ;,length handles signs
need to set the system property that the log 4 j 2 configuration reads in order to determine the script log file <PLACE_HOLDER> . once that 's set @$ the log configuration must be 'kicked ' to pick up the change .,system . set property ( __str__ @$ file . get absolute path ( ) ) ; if ( initialized ) { ( ( logger context ) log manager . get context ( false ) ) . reconfigure ( ) ; },log file location
stream still has <PLACE_HOLDER> . buffer starvation occurred . audio decoder thread will fill the <PLACE_HOLDER> and start the channel again .,reclaim channel = true ;,stream has data
stream must not receive <PLACE_HOLDER> sent by server after reset .,assert false ( stream1 data latch . await ( __num__ @$ time unit . seconds ) ) ;,stream receive data
gms join leave mistakenly uses an old view <PLACE_HOLDER> when joining @$ making it a rogue member,gms join leave member id . set vm view id ( - __num__ ) ; member identifier previous member id = services . get member factory ( ) . create ( member data builder . new builder ( gms join leave member id . get inet address ( ) @$ gms join leave member id . get host name ( ) ) . set membership port ( gms join leave member id . get membership port ( ) ) . build ( ) ) ; previous member id . set vm view id ( __num__ ) ; previous member id . get member data ( ) . setuuid ( gms join leave member id . get member data ( ) . getuuid ( ) ) ; gms membership,member uses id
make sure the master has <PLACE_HOLDER> of the reports,waiter . wait for ( test_util . get configuration ( ) @$ __num__ * __num__ @$ new predicate < exception > ( ) { @ override public boolean evaluate ( ) throws exception { map < region info @$ long > region sizes = quota manager . snapshot region sizes ( ) ; log . trace ( __str__ + region sizes ) ; return num regions == count regions for table ( tn @$ region sizes ) && table size <= get table size ( tn @$ region sizes ) ; } } ) ; map < table name @$ long > sizes = test_util . get admin ( ) . get space quota table sizes ( ) ; long size = sizes . get ( tn ),master has all
get namespace edits dirs removes <PLACE_HOLDER> across edits and shared.edits,collection < uri > edits dirs = fs namesystem . get namespace edits dirs ( conf ) ; assert equals ( __num__ @$ edits dirs . size ( ) ) ;,dirs removes duplicates
user 3 has the <PLACE_HOLDER>,user dto user1 = db . users ( ) . insert user ( ) ; user dto user2 = db . users ( ) . insert user ( ) ; user dto user3 = db . users ( ) . insert user ( ) ; organization dto organization = db . organizations ( ) . insert ( ) ; group dto group1 = db . users ( ) . insert group ( organization ) ; db . users ( ) . insert permission on group ( group1 @$ administer ) ; db . users ( ) . insert permission on group ( group1 @$ provision_projects ) ; db . users ( ) . insert member ( group1 @$ user1 ) ; db . users ( ) . insert,user has permission
class d has a directly applied parameter <PLACE_HOLDER> whose value includes to an <PLACE_HOLDER> class that is missing .,test parameter annotation ( d . class . get declared method ( __str__ @$ object . class ) @$ true ) ;,d has annotation
seems awkward to have the stream wrap <PLACE_HOLDER> .,super ( wrapped @$ wrapped . get http request ( ) ) ; this . wrapped = wrapped ;,stream wrap method
if there are more new <PLACE_HOLDER> than how many new dom <PLACE_HOLDER> got added @$ the top row logical index necessarily gets shifted down by that difference because recycling does n't replace any logical <PLACE_HOLDER> @$ just shifts them off the visual range @$ and the inserted <PLACE_HOLDER> that do n't fit to the visual range also push the other <PLACE_HOLDER> down . if,if ( number of rows > added row count ) { update top row logical index ( number of rows - added row count ) ; },rows push rows
this situation is possible only if a callee throws an <PLACE_HOLDER> which type extends throwable directly,if ( cause instanceof command action execution exception ) { command action execution exception command action execution exception = ( command action execution exception ) cause ; cause = command action execution exception . get cause ( ) ; },callee throws exception
the response should contain the exact <PLACE_HOLDER> that the schema registry has .,assert . assert equals ( __num__ @$ metadata schemas list . size ( ) ) ; for ( register response metadata entry r : metadata schemas list ) { if ( r . get version ( ) == __num__ ) { assert . assert equals ( metadata schema1 @$ r . get schema ( ) ) ; assert . assert true ( arrays . equals ( meta schema digest1 @$ r . get crc32 ( ) ) ) ; } else { assert . assert equals ( metadata schema2 @$ r . get schema ( ) ) ; assert . assert true ( arrays . equals ( meta schema digest2 @$ r . get crc32 ( ) ) ) ; } } easy mock . verify ( mock,response contain data
unlike status.from throwable which returns the unknown <PLACE_HOLDER> for these .,status st = status proto . from throwable ( e ) ; return st != null ? st : status . new builder ( ) . set code ( code . internal . get number ( ) ) . set message ( e . get message ( ) ) . build ( ) ;,which returns status
list includes all <PLACE_HOLDER>,try ( byte provider byte provider = get byte provider ( __str__ @$ __str__ + __str__ + __str__ + __str__ ) ) { check validxml load spec ( byte provider @$ loader . find supported load specs ( byte provider ) @$ __str__ @$ endian . little @$ __str__ @$ null @$ __str__ ) ; },list includes languages
this assumes the adapter has already cleaned up any partially created <PLACE_HOLDER> .,if ( overlay ) { check remove address space ( start . get address space ( ) ) ; } throw new cancelled exception ( ) ; program . db error ( e ) ;,adapter cleaned blocks
this branch coordinates fragment task or completed transaction task @$ holds the <PLACE_HOLDER> until all the sites on the node receive the task . task with newer sp handle will,if ( task . need coordination ( ) && m_scoreboard enabled ) { coordinated task queue offer ( task ) ; } else { task queue offer ( task ) ; },task holds lock
failed to read the file . there are two possibilities . either this file is in old format which does not have a magic <PLACE_HOLDER> in the beginning or this is not a valid file at all . try reading it as a file in old format,fis . close ( ) ; fis = new file input stream ( f ) ; dis = new data input stream ( new buffered input stream ( fis @$ __num__ * __num__ ) ) ; read disk store record ( dis @$ f ) ;,which have seq
only let the system discover remote display <PLACE_HOLDER> for now .,if ( client record != null ) { if ( ! client record . m trusted ) { route types &= ~ media router . route_type_remote_display ; } if ( client record . m route types != route types || client record . m active scan != active scan ) { if ( debug ) { slog . d ( tag @$ client record + __str__ + integer . to hex string ( route types ) + __str__ + active scan ) ; } client record . m route types = route types ; client record . m active scan = active scan ; client record . m user record . m handler . send empty message ( user handler . msg_update_discovery_request ) ; } },system discover types
set the low threshold to 50 instead of 60 set the high threshold to 60 instead of 70 node 2 now should not have new <PLACE_HOLDER> allocated to it @$ and <PLACE_HOLDER> can not remain,disk settings = settings . builder ( ) . put ( disk threshold settings . cluster_routing_allocation_disk_threshold_enabled_setting . get key ( ) @$ true ) . put ( disk threshold settings . cluster_routing_allocation_low_disk_watermark_setting . get key ( ) @$ __str__ ) . put ( disk threshold settings . cluster_routing_allocation_high_disk_watermark_setting . get key ( ) @$ __str__ ) . put ( disk threshold settings . cluster_routing_allocation_disk_flood_stage_watermark_setting . get key ( ) @$ __str__ ) . build ( ) ; deciders = new allocation deciders ( new hash set < > ( arrays . as list ( new same shard allocation decider ( settings . empty @$ cluster settings ) @$ make decider ( disk settings ) ) ) ) ; strategy = new allocation service ( deciders @$ new test,2 have shards
verify append called or <PLACE_HOLDER>,verify ( wal @$ expect append ? times ( __num__ ) : never ( ) ) . append data ( ( h region info ) any ( ) @$ ( wal key impl ) any ( ) @$ ( wal edit ) any ( ) ) ;,append called not
if the text wrapper does n't have a start <PLACE_HOLDER>,if ( quotes . get ( i ) . start index == - __num__ ) { if ( closing quote == - __num__ ) { closing quote = i ; } else { if ( quotes . get ( i ) . end index < quotes . get ( closing quote ) . end index ) { closing quote = i ; } } },wrapper have index
table mutation requires a current <PLACE_HOLDER> to mutate,assert ( change type == change type . create || this . old schema != null ) ;,mutation requires table
insert the video . the command sends three arguments . the first specifies which <PLACE_HOLDER> the api request is setting and which <PLACE_HOLDER> the api response should return . the second argument is the video resource that contains metadata about the new video . the third argument is the actual video content .,you tube . videos . insert video insert = youtube . videos ( ) . insert ( __str__ @$ video object defining metadata @$ media content ) ;,request setting information
make sure the preferences has only one sorted <PLACE_HOLDER>,table sort state saved sort state = get sort state from preference state ( preference state @$ column model state ) ; assert equals ( __num__ @$ saved sort state . get sorted column count ( ) ) ; column sort state column sort state = saved sort state . get column sort state ( column zero ) ; assert not null ( column sort state ) ; sort direction sort direction = column sort state . get sort direction ( ) ; assert true ( sort direction . is ascending ( ) ) ;,preferences sorted column
open the first file & read the required rows in the buffer @$ stop if it fails @$ exception will stop process <PLACE_HOLDER>,open next file ( ) ;,exception stop process
while wbmp reader does not support ext wbmp <PLACE_HOLDER>,if ( type != __num__ || fix header field != __num__ ) { return false ; },reader support header
go through each message and make sure the unmarshalled fields are null then call the getters which will unmarshall the <PLACE_HOLDER> again to show the marshalled <PLACE_HOLDER> exists,for ( message reference ref : messages ) { activemq text message message = ( activemq text message ) ref . get message ( ) ; field properties field = message . class . get declared field ( __str__ ) ; properties field . set accessible ( true ) ; field text field = activemq text message . class . get declared field ( __str__ ) ; text field . set accessible ( true ) ; assert null ( text field . get ( message ) ) ; assert null ( properties field . get ( message ) ) ; assert not null ( message . get properties ( ) ) ; assert not null ( message . get text ( ) ) ; } consumer . close,which unmarshall value
no need to sync because noone has <PLACE_HOLDER> to new info yet,new info . policy entries . add ( pe ) ;,noone has access
list applicable <PLACE_HOLDER>s may return <PLACE_HOLDER> which was already added explicitly above .,return stream . concat ( roles @$ list applicable roles ( principal @$ list role grants ) . map ( role grant :: get role name ) . filter ( predicate . is equal ( admin_role_name ) . negate ( ) ) ) . distinct ( ) ;,roles return role
make sure the new comment has the same <PLACE_HOLDER> as the old one,new comment . level = this . get ( index ) . level ; this . set ( index @$ new comment ) ; return true ;,comment has level
the new tree does n't have this <PLACE_HOLDER> . we only need to remove all its children from the list .,if ( new root is null ) { final int current items count = current root . get count ( ) ; removed components . add ( current root ) ; final change set change set = change set . acquire change set ( current root . get count ( ) @$ new root @$ enable stats ) ; for ( int i = __num__ ; i < current items count ; i ++ ) { change set . add change ( change . remove ( __num__ ) ) ; } return change set ; },tree have item
optional validation if the response contains a <PLACE_HOLDER>,if ( has text ( saml response . get destination ( ) ) && ! recipient . equals ( saml response . get destination ( ) ) ) { throw auth exception ( invalid_destination @$ __str__ + saml response . get destination ( ) ) ; } string issuer = saml response . get issuer ( ) . get value ( ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ + issuer ) ; } if ( ! has text ( issuer ) || ( ! issuer . equals ( token . get idp entity id ( ) ) ) ) { string message = string . format ( __str__ @$ issuer @$ token . get idp entity id,response contains host
slow mode starts an async <PLACE_HOLDER>,if ( mode == getrss mode . ps ) { if ( thread != null ) { if ( thread . is alive ( ) ) return ; else thread = null ; } thread = new thread ( new runnable ( ) { @ override public void run ( ) { sample system now ( medium @$ large ) ; } } ) ; thread . start ( ) ; } else { sample system now ( medium @$ large ) ; },mode starts thread
<PLACE_HOLDER> nodes will contain the both our <PLACE_HOLDER> @$ if it exists @$ and any nested stat <PLACE_HOLDER>s . ours will always be first,if ( description nodes . get length ( ) > __num__ ) { element description node = ( element ) description nodes . item ( __num__ ) ; if ( description node . get parent node ( ) . get node name ( ) . equals ( type node . get node name ( ) ) ) { description = extract description ( description node ) ; } },nodes contain description
put connect controls into environment . copy them first since caller owns the <PLACE_HOLDER> .,if ( conn ctls != null ) { control [ ] copy = new control [ conn ctls . length ] ; system . arraycopy ( conn ctls @$ __num__ @$ copy @$ __num__ @$ conn ctls . length ) ; env . put ( bind_controls_property @$ copy ) ; },caller owns array
n has <PLACE_HOLDER> and m has 1,if ( ! is bitn && is bitm ) { int mask = ~ ( m << k ) ; m = m & mask ; },n has 1
any user can execute table <PLACE_HOLDER> ; queries like ` select 1 ` might be used to do simple connection checks,return null ;,user execute statement
note that we can not do any size checking here @$ as the correct component count depends on the drawing step . gl should catch such <PLACE_HOLDER> then @$ and we will report them to the user .,if ( m values != null ) { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ __num__ ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m values ) ; } else { gles20 . gl bind buffer ( gles20 . gl_array_buffer @$ m vbo ) ; gles20 . gl vertex attrib pointer ( m index @$ m components @$ m type @$ m should normalize @$ m stride @$ m offset ) ; },note catch exceptions
multiload 3 items and ensure that multiload pulls <PLACE_HOLDER> from the database & 1 from the cache .,list < simple entity > entities = session . by multiple ids ( simple entity . class ) . with ( cache mode . normal ) . enable session check ( true ) . enable ordered return ( true ) . enable return of deleted entities ( true ) . multi load ( ids ( __num__ ) ) ; assert equals ( __num__ @$ entities . size ( ) ) ; simple entity deleted entity = entities . get ( __num__ ) ; assert not null ( deleted entity ) ; final entity entry entry = ( ( shared session contract implementor ) session ) . get persistence context ( ) . get entry ( deleted entity ) ; assert true ( entry . get status ( ),multiload pulls 2
keep glyph cache panel width from ever going down so the canvas game container does n't change <PLACE_HOLDER> and flicker .,glyph cache panel = new j panel ( ) { private int max width ; public dimension get preferred size ( ) { dimension size = super . get preferred size ( ) ; max width = math . max ( max width @$ size . width ) ; size . width = max width ; return size ; } } ;,container change size
create server validator eagerly so that we can conveniently get the trusted certificates clients need it anyway eventually @$ and servers will not mind the little extra <PLACE_HOLDER>,validator v = get validator ( validator . var_tls_server ) ; trusted certs = v . get trusted certificates ( ) ; server validator = v ; if ( debug != null && debug . is on ( __str__ ) ) { show trusted certs ( ) ; },servers mind precedence
make sure constantinople config is not using parent 's block <PLACE_HOLDER>,return new constants adapter ( super . get constants ( ) ) { @ override public big integer getblock_reward ( ) { return big integer . ten ; } } ;,config using reward
for delayed binding step partitioning meta does not achieve schema <PLACE_HOLDER> when using in constructor so we have to set it explicitly . see equals implementation for step partitioning meta .,step partitioning meta part meta = new step partitioning meta ( __str__ @$ schema ) ;,meta achieve definition
to prevent task stack resize <PLACE_HOLDER> may flicking when playing app transition <PLACE_HOLDER> & ime window enter <PLACE_HOLDER> in parallel @$ make sure app transition is done and then start to animate for ime .,if ( m animating for ime && ! m display content . m app transition . is running ( ) ) { return animate for ime ( now ) ; },task stack animation
empty input produces empty <PLACE_HOLDER> .,string [ ] [ ] [ ] inputs outputs = { { { } @$ { } } @$ { { __str__ @$ __str__ } @$ { __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } @$ { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } } @$ { { __str__ @$ __str__ @$ __str__ @$ __str__,input produces string
one write only @$ valid <PLACE_HOLDER> ; space in the beginning @$ transform quoted,list < object [ ] > parameters = arrays . < object [ ] > as list ( new object [ ] { new string item ( __str__ ) @$ __str__ @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . state ) } @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . command ) } @$ null } @$ new object [ ] { string item with state ( __str__ @$ new string type ( __str__ ) ) @$ __str__ @$ new itemio connection [ ] { new itemio connection ( __str__ @$ __num__ @$ io type . state ) } @$ new itemio connection [ ] { new itemio,one write range
queue c 111 has <PLACE_HOLDER> and aq @$ both from parent,assert true ( c111 . has access ( queueacl . administer_queue @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . administer_queue @$ __str__ ) ) ; assert true ( c111 . has access ( queueacl . submit_applications @$ user ) ) ; assert true ( has queueacl ( acl infos @$ queueacl . submit_applications @$ __str__ ) ) ; reset ( c ) ;,111 has permission
column q 1 covers version at 123 fr which user 2 do not have <PLACE_HOLDER>,p . add column ( test_family1 @$ q1 @$ __num__ @$ value ) ; p . add column ( test_family1 @$ q2 @$ value ) ; t . put ( p ) ; fail ( user . get short name ( ) + __str__ ) ;,user have users
unless this is the last replacment <PLACE_HOLDER>s @$ we pop the used <PLACE_HOLDER> . the lastreplacement <PLACE_HOLDER> applies any additional entries .,if ( names . size ( ) > __num__ ) { names . remove ( __num__ ) ; } callback . copy ( null ) ;,names pop name
say xml has a type <PLACE_HOLDER> that is removed in the new version,string existingxml = __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ ; cache config cache config = service . un marshall ( existingxml ) ; list elements = cache config . get custom cache elements ( ) ; assert that ( elements . get ( __num__ ) ) . is instance of ( element one . class ) ;,xml has tag
let handle timeout take <PLACE_HOLDER> of finishing the page,if ( ! timeout flag ) handler . post delayed ( new web view status checker ( ) @$ __num__ ) ;,timeout take care
add document level entity mentions <PLACE_HOLDER>,if ( doc . contains key ( mentions annotation . class ) ) { for ( core map mention : doc . get ( mentions annotation . class ) ) { builder . add mentions ( to proto mention ( mention ) ) ; } keys to serialize . remove ( mentions annotation . class ) ; builder . set has entity mentions annotation ( true ) ; } else { builder . set has entity mentions annotation ( false ) ; },entity mentions info
instead of expiring the session directly here @$ accumulate a list of session ids that need to be expired . this is an efficiency measure : as the expiration involves the session data <PLACE_HOLDER> doing a delete @$ it is most efficient if it can be done as a bulk operation to eg reduce roundtrips to the persistent <PLACE_HOLDER> . only do this if,if ( session . is expired at ( now ) ) { if ( _session id manager . get session house keeper ( ) != null && _session id manager . get session house keeper ( ) . get interval sec ( ) > __num__ ) { _candidate session ids for expiry . add ( session . get id ( ) ) ; if ( log . is debug enabled ( ) ) log . debug ( __str__ @$ session . get id ( ) ) ; } } else { _session cache . check inactive session ( session ) ; },expiration involves store
anchor changed @$ qualified name of children needs an <PLACE_HOLDER>,update child categories ( store object @$ store object . get children categories ( ) @$ impacted categories @$ false ) ;,name needs refactoring
initialize the input method manager and start a daemon thread if the user has multiple input <PLACE_HOLDER> to choose from . otherwise @$ just keep the instance .,if ( imm . has multiple input methods ( ) ) { imm . initialize ( ) ; thread imm thread = new thread ( imm @$ thread name ) ; imm thread . set daemon ( true ) ; imm thread . set priority ( thread . norm_priority + __num__ ) ; imm thread . start ( ) ; } input method manager = imm ;,user has methods
removing any quad <PLACE_HOLDER> to that <PLACE_HOLDER> will remove the <PLACE_HOLDER>,final quad item < t > quad item = new quad item < > ( item ) ; synchronized ( m quad tree ) { m items . remove ( quad item ) ; m quad tree . remove ( quad item ) ; },item remove item
both items use the same <PLACE_HOLDER>,return __num__ ;,items use params
no incoming flow file containing a query @$ and an exception causes no outbound <PLACE_HOLDER> . there should be no flow files on either relationship,runner . assert all flow files transferred ( abstract executesql . rel_failure @$ __num__ ) ; runner . assert all flow files transferred ( abstract executesql . rel_success @$ __num__ ) ;,exception causes flowfile
this is strange xml stream reader throws xml stream <PLACE_HOLDER> xml event reader does n't throw xml stream <PLACE_HOLDER>,boolean next = false ; try { next = fxml reader . has next ( ) ; } catch ( xml stream exception ex ) { return false ; } return next ;,reader throw exception
check that the vertices received the trigger checkpoint <PLACE_HOLDER> for the second checkpoint,verify ( vertex1 . get current execution attempt ( ) @$ times ( __num__ ) ) . trigger checkpoint ( eq ( checkpoint2 id ) @$ eq ( timestamp + __num__ ) @$ any ( checkpoint options . class ) ) ; verify ( vertex2 . get current execution attempt ( ) @$ times ( __num__ ) ) . trigger checkpoint ( eq ( checkpoint2 id ) @$ eq ( timestamp + __num__ ) @$ any ( checkpoint options . class ) ) ;,vertices received message
control multicast advertise <PLACE_HOLDER> to inject proxy,agent = multicast discovery agent factory . create discovery agent ( new uri ( discovery address ) ) ; agent . register service ( proxy . get url ( ) . to string ( ) ) ; agent . start ( ) ; do reconnect ( ) ;,multicast advertise service
get the calculated base dirs which includes the <PLACE_HOLDER>,str = props . get property ( __str__ ) ; if ( ! string util . is blank ( str ) ) { resource collection bases = new resource collection ( string util . csv split ( str ) ) ; web app . set war ( null ) ; web app . set base resource ( bases ) ; },which includes library
the <PLACE_HOLDER> of call gets the <PLACE_HOLDER> of the input callback argument .,size += size of ( get original callback argument ( ) ) ; return size ;,size gets size
since byte buffers can point all <PLACE_HOLDER> of crazy places it 's harder to keep track of which blocks are kept alive by what byte buffer . so we make a guess .,if ( c instanceof byte buffer extended cell ) { byte buffer extended cell bb cell = ( byte buffer extended cell ) c ; byte buffer bb = bb cell . get value byte buffer ( ) ; if ( bb != last block ) { context . increment response block size ( bb . capacity ( ) ) ; last block = bb ; } } else { byte [ ] value array = c . get value array ( ) ; if ( value array != last block ) { context . increment response block size ( value array . length ) ; last block = value array ; } },buffers point kinds
this is not needed @$ the underlying functor does not have <PLACE_HOLDER> to this,throw new unsupported operation exception ( __str__ ) ;,functor have access
record job reply <PLACE_HOLDER> .,if ( ! internal && ctx . event ( ) . is recordable ( evt_job_failed ) ) evts = add event ( evts @$ evt_job_failed @$ __str__ + job ) ; try { byte [ ] res bytes = null ; byte [ ] ex bytes = null ; byte [ ] attr bytes = null ; boolean loc = ctx . local node id ( ) . equals ( snd node . id ( ) ) && ! ctx . config ( ) . is marshal local jobs ( ) ; map < object @$ object > attrs = job ctx . get attributes ( ) ; if ( ! loc ) { try { res bytes = u . marshal ( marsh @$ res ) ;,job reply event
test that a null element throws null pointer <PLACE_HOLDER>,caught = false ; try { f inst . redefine classes ( new class definition [ ] { null } ) ; } catch ( null pointer exception npe ) { caught = true ; } assert true ( caught ) ;,element throws exception
single selection model should n't have selection <PLACE_HOLDER> to begin with,assert false ( __str__ @$ get grid element ( ) . get cell ( __num__ @$ __num__ ) . is element present ( by . tag name ( __str__ ) ) ) ; set selection model single ( ) ; header = get grid element ( ) . get header cell ( __num__ @$ __num__ ) ; assert false ( __str__ @$ header . is element present ( by . tag name ( __str__ ) ) ) ;,model have items
return to the last opened <PLACE_HOLDER> if triggered from the content area .,if ( m gaia service type != account management screen helper . gaia_service_type_none ) { if ( is added ( ) ) get activity ( ) . finish ( ) ; } return true ;,return opened item
big query encodes numeric values to avro using the bytes type with the decimal logical type . avro coder ca n't apply logical <PLACE_HOLDER> to schemas directly @$ so we need to get the schema for the bird class defined below @$ then replace the field used to test numeric with a field that has the appropriate schema .,big decimal birthday money = new big decimal ( __str__ ) ; schema birthday money schema = schema . create ( type . bytes ) ; logical type birthday money logical type = logical types . decimal ( birthday money . precision ( ) @$ birthday money . scale ( ) ) ;,coder apply values
test user 1 can not get user 2 's <PLACE_HOLDER>,authorization service . set owners ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set readers ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set runners ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; authorization service . set writers ( note id @$ new hash set < > ( arrays . as list ( user2 id ) ) ) ; list < paragraph info > paragraph list1 = null ; try { paragraph list1 = notebook server . get paragraph list ( user1 id,user get note
process command line property definitions these can potentially occur multiple <PLACE_HOLDER>,list < cl option > cl options = parser . get arguments ( ) ; for ( cl option option : cl options ) { string name = option . get argument ( __num__ ) ; string value = option . get argument ( __num__ ) ; switch ( option . get descriptor ( ) . get id ( ) ) { case cl option . text_argument : throw new illegal argument exception ( __str__ + option . get argument ( ) ) ; case propfile2_opt : log . info ( __str__ @$ name ) ; try ( file input stream fis = new file input stream ( new file ( name ) ) ) { properties tmp = new properties ( ) ; tmp . load (,these occur times
represents separator @$ which has no temporal <PLACE_HOLDER>,verify pattern parsing ( __str__ @$ new array list < > ( list . of ( null @$ chrono field . year @$ null @$ chrono field . month_of_year @$ null @$ chrono field . day_of_month @$ null ) ) ) ;,which has index
at this point @$ the given <PLACE_HOLDER> does not match any session <PLACE_HOLDER>,throw new api exception ( api exception . type . illegal_parameter @$ action_param_session ) ; case action_unset_active_session : site = extension . get http sessions site ( api utils . get authority ( params . get string ( action_param_site ) ) @$ false ) ; if ( site == null ) { throw new api exception ( api exception . type . illegal_parameter @$ action_param_site ) ; } site . unset active session ( ) ; return api response element . ok ; case action_add_session_token : extension . add http session token ( api utils . get authority ( params . get string ( action_param_site ) ) @$ params . get string ( action_param_token_name ) ) ; return api response element . ok ; case action_remove_session_token : extension,name match parameters
create boring attribute which creates a relation <PLACE_HOLDER>,attribute type < string > attribute type = tx . put attribute type ( __str__ @$ attribute type . data type . string ) ; attribute < string > attribute = attribute type . create ( __str__ ) ; entity type entity type = tx . put entity type ( __str__ ) . has ( attribute type ) ; entity entity = entity type . create ( ) ; entity . has ( attribute ) ; relation impl relation = relation impl . from ( entity . relations ( ) . iterator ( ) . next ( ) ) ;,which creates impl
create back buffer when not printing @$ and its graphics 2 d then set drawing <PLACE_HOLDER> for that graphics 2 d object,if ( is printing ) g2 = ( graphics2d ) g ; else { back buffer = ( buffered image ) this . create image ( w @$ h ) ; g2 = back buffer . create graphics ( ) ; g2 . set color ( color . white ) ; g2 . fill rect ( __num__ @$ __num__ @$ w @$ h ) ; g2 . set color ( color . black ) ; },printing set color
only one <PLACE_HOLDER> must match the local <PLACE_HOLDER>,if ( found > __num__ ) { string msg = __str__ + __str__ + dfs_nameservice_id + __str__ + dfs_ha_namenode_id_key ; throw new hadoop illegal argument exception ( msg ) ; },address match address
table has <PLACE_HOLDER>,t capabilities = t capabilities . replace all ( __str__ @$ __str__ ) . to upper case ( ) ;,table has capabilities
the <PLACE_HOLDER> in the linear axis will not have <PLACE_HOLDER> after the decimal point .,vertical axis . set label format ( __str__ ) ; categorical axis horizontal axis = new categorical axis ( ) ; horizontal axis . set label interval ( __num__ ) ; horizontal axis . set label fit mode ( axis label fit mode . multi_line ) ; area series . set vertical axis ( vertical axis ) ; area series . set horizontal axis ( horizontal axis ) ;,values have values
query success @$ get the right <PLACE_HOLDER> @$ bandwidth and the account name .,account query result = query account ( test key002 @$ blocking stub full ) ;,success get balance
this view ddl ca n't be used if the table already contains <PLACE_HOLDER> @$ only use it on empty tables,if ( this . new schema . view rep != null && this . start == __num__ ) { batch . add ( this . new schema . view rep . ddl for view ( ) ) ; } this . start = system . nano time ( ) ; if ( change type == change type . create ) { log . info ( __str__ ) ; } else { log . info ( __str__ ) ; } return batch . execute ( ) ;,table contains data
this handler reads a single <PLACE_HOLDER> from the message and prints it to the standard output .,try { string string = text message . get text ( ) ; system . out . println ( string ) ; } catch ( javax . jms . jms exception jmse ) { jmse . print stack trace ( ) ; } system . out . println ( __str__ + __str__ ) ;,handler reads string
check the json itself makes <PLACE_HOLDER>,json object json obj = new json object ( responsejson ) ; json array results = json obj . getjson array ( __str__ ) ; assert equals ( __num__ @$ response . results . length ) ; json object table = results . getjson object ( __num__ ) ; json array data = table . getjson array ( __str__ ) ; assert equals ( __num__ @$ data . length ( ) ) ; json array row = data . getjson array ( __num__ ) ; assert equals ( __num__ @$ row . length ( ) ) ; long value = row . get long ( __num__ ) ; assert equals ( __num__ @$ value ) ;,itself makes sense
the compressed bzip 2 stream should start with the identifying <PLACE_HOLDER> bz . caller of cb zip 2 output stream i.e . this class must write these <PLACE_HOLDER> .,if ( super . out != null ) { out . write ( header . get bytes ( standard charsets . utf_8 ) ) ; },bzip write characters
now let 's build an identical <PLACE_HOLDER> for get,binary object builder bldr = grid ( __num__ ) . binary ( ) . builder ( __str__ ) ; bldr . set field ( __str__ @$ __num__ ) ; bldr . set field ( __str__ @$ __num__ ) ;,'s build key
provide an empty key <PLACE_HOLDER> since trust manager impl does n't support null key <PLACE_HOLDER>s . trust manager impl will use cert <PLACE_HOLDER> to lookup certificates .,key store store = key store . get instance ( key store . get default type ( ) ) ; store . load ( null ) ; m delegate = new trust manager impl ( store @$ null @$ cert store ) ;,impl use store
user has not issue admin <PLACE_HOLDER> on these 2 issues,issue dto not authorized issue1 = db . issues ( ) . insert ( rule @$ project2 @$ project2 @$ i -> i . set type ( bug ) . set status ( status_open ) . set resolution ( null ) ) ; issue dto not authorized issue2 = db . issues ( ) . insert ( rule @$ project2 @$ project2 @$ i -> i . set type ( bug ) . set status ( status_open ) . set resolution ( null ) ) ; bulk change ws response response = call ( builder ( ) . set issues ( as list ( authorized issue1 . get key ( ) @$ not authorized issue1 . get key ( ) @$ not authorized issue2 . get key (,user has permission
create a verifier but do not require exact verification because the verifier sockets may receive <PLACE_HOLDER> out of order @$ due to the fact that socket exporter acknowledges <PLACE_HOLDER> when written on the sending socket @$ not when received on the destination host .,m_verifier = new export test expected data ( m_server sockets @$ false @$ false @$ kfactor + __num__ ) ;,sockets receive rows
get user inputs and set the map source this bit gets the <PLACE_HOLDER> from the manifest,alert dialog builder . set cancelable ( false ) . set positive button ( __str__ @$ new dialog interface . on click listener ( ) { public void on click ( dialog interface dialog @$ int id ) { map box tile source b = new map box tile source ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __str__ ) ; b . set mapbox mapid ( user input box id . get text ( ) . to string ( ) ) ; b . set access token ( user input token . get text ( ) . to string ( ) ) ; m map view . set tile source ( b ) ; } } ) . set negative button ( __str__ @$ new dialog,bit gets tokens
start editing when a key is typed . ui classes can disable this <PLACE_HOLDER> by setting the client property j table.auto starts edit to boolean.false .,if ( ! ret value && condition == when_ancestor_of_focused_component && is focus owner ( ) && ! boolean . false . equals ( get client property ( __str__ ) ) ) { component editor component = get editor component ( ) ; if ( editor component == null ) { if ( e == null || e . getid ( ) != key event . key_pressed ) { return false ; } int code = e . get key code ( ) ; if ( code == key event . vk_shift || code == key event . vk_control || code == key event . vk_alt ) { return false ; } int lead row = get selection model ( ) . get lead selection index ( ) ;,classes disable property
concurrent hash map does not need <PLACE_HOLDER> . here,for ( abstract thread group thread group : groups ) { stopped all = stopped all && thread group . verify threads stopped ( ) ; } return stopped all ;,map need synch
a @$ should retrieve b 's conflicting <PLACE_HOLDER>,clienta . down ( ) ; assert file list equals ( clienta . get local files exclude locked and no read ( ) @$ clientb . get local files exclude locked and no read ( ) ) ; assert sql database equals ( clienta . get database file ( ) @$ clientb . get database file ( ) ) ;,a retrieve files
this is tough for hotspot @$ but graal eliminates all <PLACE_HOLDER> .,consume ( el ) ;,graal eliminates calls
data set <PLACE_HOLDER>,final list < adapter item > data set one = new array list < > ( ) ; adapter item data set one0 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one1 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one2 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one3 = new adapter item ( __num__ @$ __str__ ) ; adapter item data set one4 = new adapter item ( __num__ @$ __str__ ) ; data set one . add ( data set one0 ) ; data set one . add ( data set one1 ) ; data set one . add ( data set one2 ) ; data set,data set one
case when 2 txn have same <PLACE_HOLDER> and keys,string key = __str__ ; string old val = __str__ ; map < string @$ string > key vals1 = new hash map < string @$ string > ( ) ; key vals1 . put ( key @$ old val ) ; map < string @$ string > key vals2 = new hash map < string @$ string > ( ) ; string new val = __str__ ; key vals2 . put ( key @$ new val ) ; list < transaction state . per source transactional update > old db updates = generate updates for schema2 ( source ids @$ key vals1 @$ scn ) ; list < transaction state . per source transactional update > new db updates = generate updates for schema2 ( source ids,txn have source
if the class is null @$ the driver has no user <PLACE_HOLDER>,if ( user code function type != null ) { this . stub = init stub ( user code function type ) ; },driver has code
the map will represent the request <PLACE_HOLDER>,map < string @$ string > request = new hash map < > ( ) ; request . put ( __str__ @$ _sessionid ) ; int random = _server . get random ( __num__ ) ; int uri = random % __num__ ; boolean blocking = ( random / __num__ ) > __num__ ; int delay = ( blocking && uri % __num__ == __num__ ) ? random / __num__ : __num__ ; request . put ( __str__ @$ uri + __str__ ) ;,map represent count
if the user 's settng a <PLACE_HOLDER> @$ warn if they do n't allow conditions .,if ( null != query ) { if ( query . index of ( substitute_token ) == - __num__ ) { log . warn ( __str__ + substitute_token + __str__ + query + __str__ ) ; } },user settng query
destroy is called on each member . if the <PLACE_HOLDER> destroy is successful on one member @$ we deem the destroy action successful @$ since if one member destroy successfully @$ the subsequent destroy on a another member would probably throw <PLACE_HOLDER> destroyed exception,list < cli function result > results list = execute and get function result ( region destroy function . instance @$ region path @$ region members list ) ; result model result = result model . create member status result ( results list ) ; xml entity xml entity = find xml entity ( results list ) ;,destroy throw region
modern apps always support <PLACE_HOLDER> .,application density = display metrics . density_device ; application scale = __num__ ; application inverted scale = __num__ ; final int expandable = __num__ ; final int large_screens = __num__ ; final int xlarge_screens = __num__ ; int size info = __num__ ;,apps support windows
create a simple rule which just writes a <PLACE_HOLDER> .,build target target = build target factory . new instance ( __str__ ) ; build rule params params = test build rule params . create ( ) ; rule key input rule key = new rule key ( __str__ ) ; build rule rule = new failing input rule key build rule ( target @$ filesystem @$ params ) ; graph builder . add to index ( rule ) ;,which writes file
m total length contains the <PLACE_HOLDER> already,child left = get padding left ( ) + right - left - m total length ; break ; case gravity . center_horizontal :,length contains padding
ensure id <PLACE_HOLDER> is not blocking id 2 @$ and id 2 is following id <PLACE_HOLDER>,twitter1 . destroy block ( rw private . id ) ; rw private message . create friendship ( id1 . id ) ; string message = __str__ + new date ( ) . to string ( ) ; direct message sent = twitter1 . send direct message ( rw private . id @$ message ) ; assert equals ( rw private . id @$ sent . get recipient id ( ) ) ; assert equals ( id1 . id @$ sent . get sender id ( ) ) ; assert equals ( message @$ sent . get text ( ) ) ; assert equals ( sent @$ twitter object factory . create direct message ( twitter object factory . get rawjson ( sent ) ) ) ;,id following 1
these should only affect the given <PLACE_HOLDER>,session s = open session ( ) ; transaction t = s . begin transaction ( ) ; int count = s . create query ( __str__ ) . set string ( __str__ @$ __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; count = s . create query ( __str__ ) . execute update ( ) ; assert equals ( __str__ @$ __num__ @$ count ) ; t . commit ( ) ; s . close ( ) ; data . cleanup ( ) ;,these affect ugi
node 0 has 2 <PLACE_HOLDER>,assert equals ( __num__ @$ node0 . get leaf count ( ) ) ;,node has children
the partition <PLACE_HOLDER> field stores the table <PLACE_HOLDER> .,_table name with type = message . get partition name ( ) ; _logger = logger factory . get logger ( _table name with type + __str__ + timeboundary refresh message handler . class ) ;,field stores name
recursively collect <PLACE_HOLDER> for all superinterfaces .,for ( class doc superintf : intf . interfaces ( ) ) { if ( ! collect remote methods ( superintf @$ table ) ) { errors = true ; } } return ! errors ;,recursively collect methods
somone killed this <PLACE_HOLDER> @$ behave as if timer cancelled,synchronized ( queue ) { new tasks may be scheduled = false ; queue . clear ( ) ; },somone killed thread
create a number of consumers to read of the messages and start them with a handler which simply stores the message <PLACE_HOLDER> in a map and checks for a duplicate,for ( int i = __num__ ; i < consumer_count ; i ++ ) { receivers [ i ] = new threaded message receiver ( test_queue_name @$ new i message handler ( ) { @ override public void on message ( message message ) throws exception { synchronized ( lock ) { int current = message count . increment and get ( ) ; if ( current % __num__ == __num__ ) { logger . info ( __str__ + message . getjms messageid ( ) + __str__ + ( ( text message ) message ) . get text ( ) ) ; } if ( messages . contains key ( message . getjms messageid ( ) ) ) { duplicate signal . count down ( ) ; logger,which stores id
verify that the text field has the min <PLACE_HOLDER> since the current offset is 0,assert equals ( program . get min address ( ) @$ ai . get address ( ) ) ; run swing ( ( ) -> ai . set value ( __str__ ) ) ; press button by text ( d . get component ( ) @$ __str__ ) ;,field has address
the following will add an <PLACE_HOLDER> into the spark preferences window,preference mypreference = new otr preferences ( ) ; spark manager . get preference manager ( ) . add preference ( mypreference ) ;,following add preference
we had a previous <PLACE_HOLDER> of who had focus . clear it .,if ( m focused != child ) { if ( m focused != null ) { m focused . un focus ( focused ) ; } m focused = child ; } if ( m parent != null ) { m parent . request child focus ( this @$ focused ) ; },who had behavior
if application was stopped and execution services did not finish <PLACE_HOLDER> @$ these codes will be executed .,platform . run later ( ( ) -> { if ( controllers . get stage ( ) != null ) { controllers . get stage ( ) . close ( ) ; emit status ( loading state . done ) ; } } ) ;,services finish termination
should trigger work <PLACE_HOLDER> of worker 5,worker1 . dispose ( ) ;,trigger work right
we documented that using distance in the where clause utilizes the <PLACE_HOLDER> which is n't precise so treating lte & lt the same should be acceptable,switch ( parent operator name ) { case lte operator . name : case lt operator . name : return lat lon point . new distance query ( column name @$ lon lat . gety ( ) @$ lon lat . getx ( ) @$ distance ) ; case gte operator . name : if ( distance - geo utils . tolerance <= __num__ ) { return queries . new match all query ( ) ; } case gt operator . name : return queries . not ( lat lon point . new distance query ( column name @$ lon lat . gety ( ) @$ lon lat . getx ( ) @$ distance ) ) ; case eq operator . name : return eq distance ( parent,clause utilizes iterator
first do a sanity check and see if this address has any <PLACE_HOLDER> .,if ( ! has conflict ( addr ) ) { return ; } monitor . set message ( __str__ ) ; boolean ask user = chosen conflict option == ask_user ;,address has conflicts
if the current cluster does n't contain the <PLACE_HOLDER> @$ fallback to something machine local and then rack local .,list < node > rack nodes = get network topology ( ) . get datanodes in rack ( network location ) ; if ( rack nodes != null ) { for ( node rack node : rack nodes ) { if ( ( ( datanode descriptor ) rack node ) . get ip addr ( ) . equals ( host ) ) { node = ( datanode descriptor ) rack node ; break ; } } if ( node == null && ! rack nodes . is empty ( ) ) { node = ( datanode descriptor ) ( rack nodes . get ( thread local random . current ( ) . next int ( rack nodes . size ( ) ) ) ) ; } },cluster contain host
check that the job name contains only allowed <PLACE_HOLDER>,if ( ! name_version_pattern . matcher ( job id name ) . matches ( ) ) { errors . add ( format ( __str__ @$ recomputed id . get name ( ) ) ) ; },name contains characters
ensure comments are stripped . these cause syntax <PLACE_HOLDER> if not stripped .,final source file src file1 = source file . from code ( __str__ @$ line_joiner . join ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,these cause errors
free and that values can move past them . we do n't need to be concerned with exposing the getter or setter here but the decomposer does not have a <PLACE_HOLDER> of exposing properties @$ only variables .,helper move expression ( __str__ @$ __str__ @$ __str__ ) ; helper move expression ( __str__ @$ __str__ @$ __str__ ) ;,decomposer have way
now @$ <PLACE_HOLDER>ck should show healthy <PLACE_HOLDER> and should not show any open files,out str = run fsck ( conf @$ __num__ @$ true @$ open file . to string ( ) @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ; assert true ( out str . contains ( namenode fsck . healthy_status ) ) ; assert false ( out str . contains ( __str__ ) ) ; assert false ( out str . contains ( __str__ ) ) ; assert false ( out str . contains ( __str__ + num all units ) ) ; assert true ( out str . contains ( __str__ + num all units ) ) ; util . cleanup ( fs @$ top dir ) ;,fsck show fs
this collation implies that this serial aggregate requires its <PLACE_HOLDER> to be sorted in an order that is one of permutations of the fields from this collation,immutable bit set group by = aggr . get group set ( ) ; list < rel data type field > row type list = aggr . get row type ( ) . get field list ( ) ; list < rel field collation > collation fields = new array list < > ( ) ; for ( int index = group by . next set bit ( __num__ ) ; index != - __num__ ; index = group by . next set bit ( index + __num__ ) ) { preconditions . check state ( index < row type list . size ( ) ) ; collation fields . add ( new rel field collation ( index ) ) ; } return rel collations . of (,aggregate requires results
p 1 has primary key <PLACE_HOLDER> on column a : group by c should not use its <PLACE_HOLDER> to speed up .,check seq scan ( pn @$ __str__ @$ __str__ @$ __str__ ) ; assert not null ( aggregate plan node . get inline aggregation node ( pn ) ) ; assert null ( pn . get inline plan node ( plan node type . limit ) ) ;,group use index
check only for <PLACE_HOLDER> @$ that 's enough because default ca n't exceed <PLACE_HOLDER>,if ( maximum application lifetime <= __num__ ) { return ( lifetime requested by app <= __num__ ) ? default application lifetime : lifetime requested by app ; } if ( lifetime requested by app <= __num__ ) { return default application lifetime ; } else if ( lifetime requested by app > maximum application lifetime ) { return maximum application lifetime ; } return lifetime requested by app ; read lock . unlock ( ) ;,default exceed 4
if this is a bulk op @$ and concurrency checks are enabled @$ we need to save the version <PLACE_HOLDER> in case we retry . make record bulk op version <PLACE_HOLDER> after record sequence number @$ so that record bulk op start in a retry bulk op would not incorrectly remove the saved version <PLACE_HOLDER> in recorded bulk op version <PLACE_HOLDER>s,if ( lr . get concurrency checks enabled ( ) && ( event . get operation ( ) . is put all ( ) || event . get operation ( ) . is remove all ( ) ) && lr . get server proxy ( ) == null ) { record bulk op event ( event @$ membershipid ) ; },start remove tag
be aware the legacy parser is not signaling truncate <PLACE_HOLDER>,parser . signal truncate table ( table id @$ ctx ) ; super . enter truncate table ( ctx ) ;,parser signaling event
form should not have a description <PLACE_HOLDER>,check tooltip ( $ ( form element . class ) . first ( ) @$ null ) ;,form have field
for some reason ` test 0 <PLACE_HOLDER> ` and ` test 1 <PLACE_HOLDER> ` are n't equal . this is good @$ but unexpected .,assert that ( test1 type ) . is not same instance as ( test0 type ) ;,reason test type
note : no need to call inc failed accept here because it will be done in our caller . no need to log error here since caller will log <PLACE_HOLDER>,if ( connection != null && ! finished connecting ) { close con ( __str__ @$ connection ) ; connection = null ; },caller log error
end of the post fork <PLACE_HOLDER> .,trace . trace end ( trace . trace_tag_activity_manager ) ; if ( parsed args . m invoke with != null ) { wrapper init . exec application ( parsed args . m invoke with @$ parsed args . m nice name @$ parsed args . m target sdk version @$ vm runtime . get current instruction set ( ) @$ pipe fd @$ parsed args . m remaining args ) ; throw new illegal state exception ( __str__ ) ; } else { if ( ! is zygote ) { return zygote init . zygote init ( parsed args . m target sdk version @$ parsed args . m remaining args @$ null ) ; } else { return zygote init . child zygote init ( parsed args,end fork event
check that a field of type string does not accept <PLACE_HOLDER>,try { completed form . set answer ( __str__ @$ true ) ; fail ( __str__ ) ; } catch ( illegal argument exception e ) { },field accept exception
make sure the subject has a <PLACE_HOLDER>,assert false ( client principals . is empty ( ) ) ;,subject has principal
to do : handle if user does n't provide <PLACE_HOLDER> best,if ( flags . usek best ) { int k = flags . k best ; crf . classify and write answersk best ( test file @$ k @$ reader and writer ) ; } else if ( flags . print label value ) { crf . print label information ( test file @$ reader and writer ) ; } else { log . info ( __str__ ) ; crf . classify and write answers ( test file @$ reader and writer @$ true ) ; },user provide k
allow that balance is not exact . fyi @$ get region server <PLACE_HOLDER>s does not include master <PLACE_HOLDER> though it is a regionserver so we have to check master and then below the regionservers .,for ( jvm cluster util . region server thread rst : cluster . get region server threads ( ) ) { regions = rst . get region server ( ) . get regions ( ) ; int rs actual count = regions . size ( ) ; check count ( rs actual count @$ rs count ) ; } h master old master = cluster . get master ( ) ; cluster . kill master ( old master . get server name ( ) ) ; old master . join ( ) ; while ( cluster . get master ( ) == null || cluster . get master ( ) . get server name ( ) . equals ( old master . get server name ( ) ),threads include thread
if no restrictions were saved @$ m user manager.get application restrictions returns null @$ but dpm method should return an empty <PLACE_HOLDER> as per java doc,return bundle != null ? bundle : bundle . empty ; m injector . binder restore calling identity ( id ) ;,method return bundle
partition node <PLACE_HOLDER> from node 2 .,partition ( member1 @$ member2 ) ;,partition node 1
this test case generates a relation <PLACE_HOLDER> where multiple positions share the same symbol,assert query ( __str__ ) ;,case generates plan
extended relocations add an addition <PLACE_HOLDER>,if ( nreloc > __num__ ) { nreloc ++ ; } file offset += ( nreloc * image_relocation . totalsize ) ;,relocations add amount
schedule an event immediately to ensure the <PLACE_HOLDER> are being honored .,handler . post ( new runnable ( ) { @ override public void run ( ) { events . add ( __str__ ) ; } } ) ; assert true ( latch . await ( __num__ @$ seconds ) ) ; assert equals ( as list ( __str__ @$ __str__ ) @$ events ) ;,event ensure requirements
also the follower may have just sent a leader check @$ which receives no <PLACE_HOLDER>,cluster . stabilise ( math . max ( default millis ( follower_check_timeout_setting ) + default millis ( follower_check_interval_setting ) + default_delay_variability + default_cluster_state_update_delay + default_cluster_state_update_delay @$ default millis ( leader_check_timeout_setting ) + default millis ( leader_check_interval_setting ) + default_delay_variability ) ) ;,which receives response
authorization header must have a <PLACE_HOLDER>,if ( auth header base64 string == null || auth header base64 string . is empty ( ) ) { throw new http authentication exception ( __str__ + __str__ ) ; } return auth header base64 string ;,header have payload
finally @$ the default overall toolchain <PLACE_HOLDER> .,return toolchain . get default platform ( ) ;,default toolchain target
if we have a <PLACE_HOLDER> for a compressed apk @$ copy it to the reference location . note that copying the <PLACE_HOLDER> here will cause it to override the reference <PLACE_HOLDER> every ota even though the existing reference <PLACE_HOLDER> may have more data . we ca n't copy during decompression since the directories are not set up at that point .,if ( profile file . exists ( ) ) { try { if ( ! m installer . copy system profile ( profile file . get absolute path ( ) @$ pkg . application info . uid @$ pkg . package name @$ art manager . get profile name ( null ) ) ) { log . e ( tag @$ __str__ ) ; } else { use profile for dexopt = true ; } } catch ( exception e ) { log . e ( tag @$ __str__ + profile file . get absolute path ( ) + __str__ @$ e ) ; } },profile have profile
validate that other vm no longer hosts colocated <PLACE_HOLDER>,host . get host ( __num__ ) . getvm ( othervm ) . invoke ( new serializable runnable ( ) { @ override public void run ( ) { for ( int i = __num__ ; i < region path . length ; i ++ ) { partitioned region pr = ( partitioned region ) get cache ( ) . get region ( region path [ i ] ) ; bucket bucket = pr . get region advisor ( ) . get bucket ( __num__ ) ; assert false ( __str__ @$ bucket . is hosting ( ) ) ; bucket region bucket region = bucket . get bucket advisor ( ) . get proxy bucket region ( ) . get hosted bucket region ( ) ; assert,vm hosts bucket
call into chatty asserts test <PLACE_HOLDER> to get the nice formatting .,if ( ! actual . starts with ( expected ) ) { assert that ( actual ) . is equal to ( expected ) ; },call asserts methods
no reports right after we created this <PLACE_HOLDER> .,assert equals ( __num__ @$ get region reports for table ( quota manager . snapshot region sizes ( ) @$ tn ) ) ;,reports created region
we need to strip 'delete ' or else jpa operations.delete will generate the wrong <PLACE_HOLDER>,string delete query string = query string . substring ( __str__ . length ( ) ) ; result handle delete count ; if ( use named params ) { result handle parameters = generate parameters object ( named parameter to index @$ method creator ) ; delete count = method creator . invoke static method ( method descriptor . of method ( jpa operations . class @$ __str__ @$ long . class @$ class . class @$ string . class @$ parameters . class ) @$ method creator . read instance field ( entity class field descriptor @$ method creator . get this ( ) ) @$ method creator . load ( delete query string ) @$ parameters ) ; } else { result handle params array =,'delete generate output
by default @$ we should see no restriction ; the unicode set should allow all <PLACE_HOLDER> .,set = sc . get allowed chars ( ) ; tmp set = new unicode set ( __num__ @$ __num__ ) ; assert equals ( __str__ @$ tmp set @$ set ) ;,set allow characters
if the context url has a different <PLACE_HOLDER> @$ discard it because we ca n't use it .,if ( protocol != null && context != null && ! protocol . equals ( context . protocol ) ) { context = null ; },url has protocol
note that chunk <PLACE_HOLDER> only changes chunk <PLACE_HOLDER> in case we actually save chunks otherwise there is a risk file and chunks are not compatible,if ( ! saved chunks ) { try { save chunks ( chunk size ) ; } catch ( io exception ioe ) { throw new mongo exception ( __str__ @$ ioe ) ; } } super . save ( ) ;,size changes size
open a connection @$ thus we can not set <PLACE_HOLDER> here !,key chain . choose private key alias ( m activity @$ new key chain alias callback ( ) { @ override public void alias ( string alias ) { timber . d ( __str__ @$ alias ) ; set alias ( alias ) ; } } @$ null @$ null @$ null @$ - __num__ @$ get alias ( ) ) ;,connection set aliases
the above method will perform the <PLACE_HOLDER> as long as the user does not cancel the request,if ( policy tool . collator . compare ( e . get action command ( ) @$ tool window . save_policy_file ) == __num__ ) { string filename = ( ( j text field ) tw . get component ( tool window . mw_filename_textfield ) ) . get text ( ) ; if ( filename == null || filename . length ( ) == __num__ ) { tool dialog td = new tool dialog ( policy tool . get message ( __str__ ) @$ tool @$ tw @$ true ) ; td . display save as dialog ( tool dialog . noaction ) ; } else { try { tool . save policy ( filename ) ; message format form = new message format ( policy tool .,method perform quit
the test is just checking to make sure thaat the <PLACE_HOLDER> and consumer does not hang due to the network hops take to route the message form the <PLACE_HOLDER> to the consumer .,assert true ( __str__ @$ r > __num__ ) ; assert true ( __str__ @$ p > __num__ ) ;,message form address
the following 'deltas ' includes all <PLACE_HOLDER> of delta files including insert & delete deltas .,final list < parsed delta > deltas = new array list < parsed delta > ( ) ; list < parsed delta > working = new array list < parsed delta > ( ) ; list < path > original directories = new array list < > ( ) ; final list < path > obsolete = new array list < > ( ) ; final list < path > aborted directories = new array list < > ( ) ; list < hdfs file status with id > children with id = try list located hdfs status ( use file ids @$ fs @$ candidate directory ) ; txn base best base = new txn base ( ) ; final list < hdfs file status with id,'deltas includes kinds
rename tables to make them satisfy the <PLACE_HOLDER> and enable acid tables .,primary . run ( __str__ + primary db name ) . run ( __str__ ) . run ( __str__ ) . run ( __str__ ) ; dump with clause = arrays . as list ( __str__ + hive conf . conf vars . repl_bootstrap_acid_tables . varname + __str__ @$ __str__ + repl utils . repl_dump_include_acid_tables + __str__ ) ; replicated tables = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ } ; bootstrap tables = new string [ ] { __str__ @$ __str__ @$ __str__ @$ __str__ } ; replicate and verify ( repl policy @$ null @$ last repl id @$ dump with clause @$ null @$ bootstrap tables @$ replicated tables ) ;,them satisfy filter
if the left index has not reached the right <PLACE_HOLDER> of array must now sort the right partition .,if ( lo <= hi0 ) quick sort ( lo @$ hi0 @$ key @$ is ascending ) ;,index reached side
processor has <PLACE_HOLDER>,capabilities . clear ( ) ; capabilities . add ( __str__ ) ; sethms client ( __str__ @$ ( string [ ] ) ( capabilities . to array ( new string [ __num__ ] ) ) ) ; parts = client . get partitions by names ( db name @$ tbl name @$ part values @$ false @$ null ) ; for ( partition part : parts ) { assert equals ( __str__ @$ - __num__ @$ part . get sd ( ) . get num buckets ( ) ) ; } tbl name = __str__ ; properties = new string builder ( ) ; properties . append ( __str__ ) ; properties . append ( __str__ ) ; properties . append ( capabilities_key ) . append (,processor has capabilities
now format first nn and copy the storage <PLACE_HOLDER> from that node to the others .,int nn index = nn counter ; collection < uri > prevnn dirs = null ; for ( nn conf nn : nameservice . getn ns ( ) ) { init name node conf ( conf @$ ns id @$ ns counter @$ nn . get nn id ( ) @$ manage name dfs dirs @$ manage name dfs dirs @$ nn index ) ; collection < uri > namespace dirs = fs namesystem . get namespace dirs ( conf ) ; if ( format ) { for ( uri name dir uri : namespace dirs ) { file name dir = new file ( name dir uri ) ; if ( name dir . exists ( ) && ! file util . fully delete ( name dir,format nn dirs
ensure an empty file is created if there is no string map . this avoids confusing some build <PLACE_HOLDER> that expect to see the file @$ even if it is empty .,if ( compiler . get string map ( ) == null ) { if ( ! ( new file ( config . string map output path ) . create new file ( ) ) ) { throw new io exception ( __str__ + config . string map output path ) ; } } else { compiler . get string map ( ) . save ( config . string map output path ) ; },some build tests
verify that just changing the build <PLACE_HOLDER> <PLACE_HOLDER> key changes the calculated <PLACE_HOLDER> key .,assert not equals ( build result ( create builder ( graph builder ) . set reflectively ( __str__ @$ explicit build target source path . of ( fake1 . get build target ( ) @$ paths . get ( __str__ ) ) ) ) @$ build result ( create builder ( graph builder ) . set reflectively ( __str__ @$ explicit build target source path . of ( fake2 . get build target ( ) @$ paths . get ( __str__ ) ) ) ) ) ;,the build target
validate that the first savepoint does not discard its private <PLACE_HOLDER> .,verify ( subtask state1 @$ never ( ) ) . discard state ( ) ; verify ( subtask state2 @$ never ( ) ) . discard state ( ) ;,savepoint discard state
test empty group id and use site url that causes uri syntax <PLACE_HOLDER>,url = __str__ ; mockito . when ( mock result . get site url ( ) ) . then return ( url ) ; mockito . when ( mock result . get group id ( ) ) . then return ( __str__ ) ; assert . assert equals ( url @$ utils . get group id ( mock result ) ) ;,causes uri exception
validate property has <PLACE_HOLDER> that can not be resolved,object [ ] [ ] input = { { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ @$ __str__ @$ __str__ } @$ { __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,property has properties
list contains an <PLACE_HOLDER> from another thread .,list . add ( obj from another thread . get ( ) ) ;,list contains object
inner loop tries all 9 compression <PLACE_HOLDER> .,for ( int i = __num__ ; i <= __num__ ; i ++ ) { system . out . println ( __str__ + j + __str__ + i + __str__ ) ; byte [ ] zipped = new byte [ __num__ * data_size ] ; deflater deflater = new deflater ( i ) ; deflater . set input ( input ) ; deflater . finish ( ) ; deflater . deflate ( zipped ) ; deflater . end ( ) ; byte [ ] output = new byte [ data_size ] ; inflater inflater = new inflater ( ) ; inflater . set input ( zipped ) ; inflater . finished ( ) ; inflater . inflate ( output ) ; inflater . end ( ) ; assert,loop tries methods
verify that interim <PLACE_HOLDER> mask includes one or more <PLACE_HOLDER> not included in the <PLACE_HOLDER> mask,boolean one or more = false ; for ( int i = __num__ ; i < interim reasons mask . length && ! one or more ; i ++ ) { if ( interim reasons mask [ i ] && ! ( i < reasons mask . length && reasons mask [ i ] ) ) { one or more = true ; } } if ( ! one or more ) { return false ; },mask includes reasons
if an user used customizations for projects he perhaps just used the key <PLACE_HOLDER> for project without a name but the code expects a name for the project . therefore we fill the name according to the project key which is the same .,for ( entry < string @$ project > entry : cfg . get projects ( ) . entry set ( ) ) { if ( entry . get value ( ) . get name ( ) == null ) { entry . get value ( ) . set name ( entry . get key ( ) ) ; } },user used column
if scope is limited to this iterator @$ then do n't check any more <PLACE_HOLDER> in this scope,if ( scope . get limit ( ) == itr ) { continue next_scope ; },then check elements
keep first row in deduplicate will not send <PLACE_HOLDER>,list < object > expected output = new array list < > ( ) ; expected output . add ( record ( __str__ @$ __num__ @$ __num__ ) ) ; expected output . add ( record ( __str__ @$ __num__ @$ __num__ ) ) ; assertor . assert output equals sorted ( __str__ @$ expected output @$ test harness . get output ( ) ) ; test harness . close ( ) ;,row send retraction
regular expression check <PLACE_HOLDER> .,assert button state ( __str__ @$ true @$ false ) ; @ suppress warnings ( __str__ ) j combo box < charset > combo box = ( j combo box < charset > ) find component by name ( pane @$ __str__ ) ; assert not null ( combo box ) ;,expression check case
reset namenode backup address because windows does not release <PLACE_HOLDER> used previously properly .,backup_config . set ( dfs config keys . dfs_namenode_backup_address_key @$ this_host ) ;,windows release resources
if a meta key is active but the lookup with the meta key did not produce <PLACE_HOLDER> @$ try some other meta keys @$ because the user might have pressed shift when they meant alt @$ or vice versa .,if ( meta != __num__ ) { key data kd = new key data ( ) ; char [ ] accepted = get accepted chars ( ) ; if ( event . get key data ( kd ) ) { for ( int i = __num__ ; i < kd . meta . length ; i ++ ) { if ( ok ( accepted @$ kd . meta [ i ] ) ) { return kd . meta [ i ] ; } } } },lookup produce anything
set <PLACE_HOLDER> should cover the previous <PLACE_HOLDER> @$ so we call clear here .,if ( assigned != null && assigned . length > __num__ ) { actions . clear ( ) ; actions . add all ( arrays . as list ( assigned ) ) ; },actions cover actions
compiler options also validates <PLACE_HOLDER> @$ but uses preconditions and therefore wo n't generate a useful exception .,if ( flags . defines != null ) { validate primitive types ( flags . defines ) ; options . set define replacements ( to map ( flags . defines ) ) ; },options validates definitions
zipkin did n't store the <PLACE_HOLDER> @$ as they should n't have been readable @$ due to the error,assert that ( zipkin . get traces ( ) ) . is empty ( ) ;,zipkin store spans
still an override might change the <PLACE_HOLDER> of the repository .,if ( rule == null ) { repository delegator function . repository_overrides . get ( env ) ; return ; },override change behavior
yes @$ we are assuming that test queries do n't contain quoted question <PLACE_HOLDER> .,int param count = string utils . count matches ( sql @$ __str__ ) ; boolean m_infer = m_by default infer partitioning ; boolean m_forcesp = m_by default infer partitioning ; m_by default infer partitioning = false ; m_by default plan for single partition = true ; abstract plan node pn = compilesp with join order ( sql @$ param count @$ null ) ; m_by default infer partitioning = m_infer ; m_by default plan for single partition = m_forcesp ; return pn ;,queries contain marks
the last 4 bytes of the file encode the major and minor <PLACE_HOLDER> universally,baos dos . write int ( materialize version ( major version @$ minor version ) ) ; baos . write to ( output stream ) ;,bytes encode version
compute coordinates of point in box coordinate <PLACE_HOLDER>,temp vars vars = temp vars . get ( ) ; vector3f closest = vars . vect1 ; point . subtract ( center @$ closest ) ;,coordinates coordinate space
available everywhere . let 's note different <PLACE_HOLDER> .,super public . public method ( ) ;,'s note types
will call create blob <PLACE_HOLDER> on the underlying connection,method m = connection . class . get method ( __str__ @$ new class [ ] { } ) ;,call create wrapper
in role editor each object may have different privilege <PLACE_HOLDER>,if ( is role editor ( ) ) { permission table . remove all ( ) ; if ( ! common utils . is empty ( objects ) ) { class < ? > object type = objects . get ( __num__ ) . get class ( ) ; for ( postgre privilege type pt : postgre privilege type . values ( ) ) { if ( ! pt . is valid ( ) || ! pt . supports type ( object type ) ) { continue ; } table item priv item = new table item ( permission table @$ swt . left ) ; priv item . set text ( __num__ @$ pt . name ( ) ) ; priv item . set data ( pt,object have types
we know check if both member received <PLACE_HOLDER>,assert equals ( __str__ + __str__ @$ __num__ @$ op set1 room collector . collected events . size ( ) ) ; assert equals ( __str__ + __str__ @$ __num__ @$ op set2 room collector . collected events . size ( ) ) ; chat room member presence change event member event = ( chat room member presence change event ) op set1 room collector . collected events . get ( __num__ ) ; assert equals ( __str__ @$ chat room member presence change event . member_joined @$ member event . get event type ( ) ) ; assert equals ( __str__ @$ fixture . userid2 @$ member event . get chat room member ( ) . get contact address ( ) ) ; assert equals ( __str__,member received events
verify that this connection supports <PLACE_HOLDER>,connection . verify can update ( ) ;,connection supports update
inspect the memstore contents to see whether the memstore contains only <PLACE_HOLDER> with seq id smaller than the flush seq id . if so @$ we can discard those <PLACE_HOLDER> .,drop mem store contents for seq id ( flush . get flush sequence number ( ) @$ null ) ;,memstore contains edits
illegal access error is expected note : logback stops context <PLACE_HOLDER> after shutdown initiated . it is problematic to see log output system out could help,logger . warn ( __str__ + e . get message ( ) ) ;,logback stops processing
the replica sets have different <PLACE_HOLDER> ...,if ( ! this . replica sets by name . key set ( ) . equals ( prior state . replica sets by name . key set ( ) ) ) { return true ; },sets have names
test that toggling does n't notify the <PLACE_HOLDER> .,parole listener . rearm latch ( ) ; set app idle enabled ( m controller @$ true ) ; parole listener . await on latch ( stable_charging_threshold * __num__ / __num__ ) ; assert true ( parole listener . m on parole ) ; assert equals ( last update time @$ parole listener . get last parole change time ( ) ) ; parole listener . rearm latch ( ) ; set app idle enabled ( m controller @$ false ) ; parole listener . await on latch ( stable_charging_threshold * __num__ / __num__ ) ; assert true ( parole listener . m on parole ) ; assert equals ( last update time @$ parole listener . get last parole change time ( ) ) ;,toggling notify listeners
both output pages must have the same <PLACE_HOLDER>,assert equals ( dictionary block2 . get dictionary ( ) @$ dictionary block . get dictionary ( ) ) ;,pages have dictionary
use properties file <PLACE_HOLDER>,use properties file = false ; properties file = __str__ ;,properties file paths
check if first column contains <PLACE_HOLDER>,if ( first col zero ) { for ( int i = __num__ ; i < matrix . length ; ++ i ) { matrix [ i ] [ __num__ ] = __num__ ; } },column contains 0
ensure process owner creates queued command <PLACE_HOLDER>,if ( ! cmd dir . exists ( ) ) { cmd dir . mkdir ( ) ; return ; },owner creates directory
inline fragments may not have a type <PLACE_HOLDER>,string type condition = type name == null ? __str__ : wrap ( __str__ @$ type ( type name ) @$ __str__ ) ; string directives = directives ( node . get directives ( ) ) ; string selection set = node ( node . get selection set ( ) ) ; out . printf ( __str__ @$ comments ( node ) ) ; out . printf ( __str__ @$ spaced ( __str__ @$ type condition @$ directives @$ selection set ) ) ;,fragments have expression
nested groupby only requires time <PLACE_HOLDER> for inner most query,test query ( planner_config_require_time_condition @$ __str__ + __str__ + __str__ + __str__ @$ calcite tests . regular_user_auth_result @$ immutable list . of ( group by query . builder ( ) . set data source ( new query data source ( group by query . builder ( ) . set data source ( calcite tests . datasource1 ) . set interval ( query segment spec ( intervals . utc ( date times . of ( __str__ ) . get millis ( ) @$ joda utils . max_instant ) ) ) . set granularity ( granularities . all ) . set dimensions ( dimensions ( new default dimension spec ( __str__ @$ __str__ ) ) ) . set aggregator specs ( aggregators ( new long sum aggregator factory ( __str__,groupby requires condition
no more room to display the comments below ; do n't process <PLACE_HOLDER>,if ( total comments found > max display lines ) { return ; },comments process them
the test table has 500 <PLACE_HOLDER> @$ so total query time should be ~ 2500 ms,try { stmt . execute query ( __str__ + __str__ + table name + __str__ + table name + __str__ ) ; fail ( __str__ ) ; } catch ( sql timeout exception e ) { assert not null ( e ) ; system . err . println ( e . to string ( ) ) ; } catch ( sql exception e ) { fail ( __str__ + e ) ; e . print stack trace ( ) ; },table has rows
if user enters the <PLACE_HOLDER> of the project manually and leaves off the extension @$ try to open or create using the extension,if ( ! create && filename . last index of ( __str__ ) > path . last index of ( file . separator ) ) { msg . show error ( get class ( ) @$ tool . get tool frame ( ) @$ __str__ @$ __str__ + file . get name ( ) + __str__ ) ; continue ; },user enters definition
optional union field @$ field value is custom any <PLACE_HOLDER>,object [ ] [ ] inputs = { { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ + __str__ @$ __str__ + __str__ @$ __str__ + __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__ @$ __str__ + __str__ + __str__ @$ __str__ + __str__ + __str__ } @$ { __str__ @$ __str__,field custom attribute
need constantly invalidate view in order to get max redraw <PLACE_HOLDER> .,m layout . get view tree observer ( ) . add on pre draw listener ( this ) ;,max redraw time
rollback should n't be called before start <PLACE_HOLDER> @$ otherwise @$ node manager can not find the <PLACE_HOLDER>,try { client . rollback last re initialization ( container . get id ( ) ) ; fail ( __str__ ) ; } catch ( yarn exception e ) { assert true ( __str__ @$ e . get message ( ) . contains ( __str__ ) ) ; },rollback find container
the dummy protocol suite has the nice <PLACE_HOLDER> that it can be run by just one player .,int no of parties = __num__ ; run test ( f @$ eval strategy @$ log performance @$ no of parties ) ;,suite has identity
we want to avoid looking into the future . so @$ if the cells of the operation specify a <PLACE_HOLDER> @$ or the operation itself specifies a <PLACE_HOLDER> @$ then we use the maximum ts found . otherwise @$ we bound the get to the current server time . we add 1 to the timerange since the upper bound of a timerange is exclusive,long latest ts = math . max ( op ts @$ latest cell ts ) ; if ( latest ts == __num__ || latest ts == h constants . latest_timestamp ) { latest ts = environment edge manager . current time ( ) ; } get . set time range ( __num__ @$ latest ts + __num__ ) ;,itself specifies timestamp
store exists default ssl <PLACE_HOLDER> to restore after test .,final ssl context dflt ssl ctx = ssl context . get default ( ) ;,store exists context
if we could n't find the region because the cache is closed @$ throw a cache closed <PLACE_HOLDER>,if ( rgn == null ) { if ( cache . is closed ( ) ) { throw new cache closed exception ( ) ; } throw new region not found exception ( string . format ( __str__ @$ this . region path ) ) ; },cache closed exception
some problems in key conversion @$ so the params do not match the key <PLACE_HOLDER>,continue ;,params match params
seed hash of the target peer @$ needed for network stability check if we are the right target and requester has correct <PLACE_HOLDER> about this peer,if ( ( sb . peers . my seed ( ) == null ) || ( ! ( sb . peers . my seed ( ) . hash . equals ( youare ) ) ) ) { return prop ; },hash has information
the super implementation does not handle the following <PLACE_HOLDER>,identification protocol = params . get endpoint identification algorithm ( ) ; algorithm constraints = params . get algorithm constraints ( ) ; prefer local cipher suites = params . get use cipher suites order ( ) ; collection < sni matcher > matchers = params . getsni matchers ( ) ; if ( matchers != null ) { sni matchers = params . getsni matchers ( ) ; },implementation handle parameters
reset pig <PLACE_HOLDER> @$ otherwise you may get the pig <PLACE_HOLDER> of last job in the same thread because pig <PLACE_HOLDER> is thread local variable,pig stats . start ( pig server . get pig context ( ) . get execution engine ( ) . instantiate pig stats ( ) ) ; pig script listener script listener = new pig script listener ( ) ; script state . get ( ) . register listener ( script listener ) ; listener map . put ( context . get paragraph id ( ) @$ script listener ) ; pig server . register script ( tmp script file . get absolute path ( ) ) ; schema schema = pig server . dump schema ( alias ) ; boolean schema known = ( schema != null ) ; if ( schema known ) { for ( int i = __num__ ; i < schema . size,stats get stats
binding is used to group related field setters together . it is essential for action insert fact col and action set field col 52 columns as these represent single fields and need to be grouped together it is not essential for i action 's as these contain their own list of fields . if a brl fragment does not set the binding use a,if ( binding == null ) { binding = action . to string ( ) ; } final labelled action a = new labelled action ( ) ; a . bound name = binding ; a . action = action ; a . is update = is update ; actions . add ( a ) ;,fragment set these
cnxns typically have many <PLACE_HOLDER> @$ so use default cap here,if ( paths == null ) { paths = new hash set < > ( ) ; watch2 paths . put ( watcher @$ paths ) ; },cnxns have paths
this code is only synchronously calling a single native <PLACE_HOLDER> to trigger and asynchronous sync cycle @$ so 5 minutes is generous .,try { if ( ! semaphore . try acquire ( __num__ @$ time unit . minutes ) ) { log . w ( tag @$ __str__ ) ; sync result . stats . num io exceptions ++ ; } } catch ( interrupted exception e ) { log . w ( tag @$ __str__ @$ e ) ; sync result . stats . num io exceptions ++ ; },code calling method
the line matched the regular <PLACE_HOLDER> .,string class name = null ; string source file = null ; int line number = __num__ ; string type = null ; string field name = null ; string method name = null ; string arguments = null ;,line matched expression
make sure browser context menu does not block the <PLACE_HOLDER>,get command executor ( ) . execute script ( __str__ @$ e @$ x @$ y ) ; new actions ( get driver ( ) ) . move to element ( e @$ getx offset ( e @$ x coord ) @$ gety offset ( e @$ y coord ) ) . context click ( ) . move by offset ( - __num__ @$ - __num__ ) . click ( ) . perform ( ) ;,menu block execution
could try comparing exact message @$ but since it 's informational try <PLACE_HOLDER> :,if ( ! desc . contains ( __str__ ) ) { fail ( __str__ + desc ) ; } if ( ! desc . contains ( __str__ ) ) { fail ( __str__ + desc ) ; },informational try name
some required local <PLACE_HOLDER> @$ which are matched exactly,global properties gp = new global properties ( ) ; local properties lp = local properties . for grouping ( new field list ( __num__ @$ __num__ ) ) ; requested local properties req lp = new requested local properties ( ) ; req lp . set grouped fields ( new field list ( __num__ @$ __num__ ) ) ; to map1 . set required global props ( null ) ; to map1 . set required local props ( req lp ) ; to map2 . set required global props ( null ) ; to map2 . set required local props ( null ) ; feedback properties meet requirements report report = map2 . check partial solution properties met ( target @$ gp @$ lp ) ; assert,some required properties
export the data @$ which causes a file <PLACE_HOLDER> to be shown,execute on swing without blocking ( ( ) -> key binding utils . export key bindings ( options ) ) ; file selected file = find and test file chooser ( null @$ test_filename ) ; return selected file ;,which causes selector
enable a system ime . no need to show a security warning <PLACE_HOLDER> @$ but we might need to prompt if it 's not direct boot aware . tv does n't does n't need to worry about this @$ but other platforms should show a warning .,if ( m imi . is system ( ) ) { if ( m imi . get service info ( ) . direct boot aware || is tv ( ) ) { set checked internal ( true ) ; } else if ( ! is tv ( ) ) { show direct boot warn dialog ( ) ; } } else { show security warn dialog ( ) ; },platforms show dialog
calculator corner radius <PLACE_HOLDER>,float corner radius top left ; float corner radius top right ; float corner radius bottoml right ; float corner radius bottom left ; if ( skeleton attribute child . get corner radius ( ) != integer . min_value ) { corner radius top left = corner radius top right = corner radius bottoml right = corner radius bottom left = get corner radius ( rectangle rect @$ skeleton attribute child . get corner radius ( ) ) ; } else { corner radius top left = skeleton attribute child . get corner radius top left ( ) != integer . min_value ? get corner radius ( rectangle rect @$ skeleton attribute child . get corner radius top left ( ) ) : __num__ ; corner radius top,corner radius right
preserve any <PLACE_HOLDER> already associated with the video . if the video does not have any <PLACE_HOLDER> @$ create a new array . append the provided localization to the list of <PLACE_HOLDER> associated with the video .,map < string @$ video localization > localizations = video . get localizations ( ) ; if ( localizations == null ) { localizations = new array map < string @$ video localization > ( ) ; video . set localizations ( localizations ) ; } video localization video localization = new video localization ( ) ; video localization . set title ( title ) ; video localization . set description ( description ) ; localizations . put ( language @$ video localization ) ;,video have localizations
work correctly @$ so let 's indicate <PLACE_HOLDER> right away,java type value type = _target type ;,let indicate probability
stack trace of the cause should contain three <PLACE_HOLDER> : test error do fn.nested function beta test error do fn.nested function alpha test error do fn.start bundle,assert that ( stack trace frame strings ( exn . get cause ( ) ) @$ contains ( contains string ( __str__ ) @$ contains string ( __str__ ) @$ contains string ( __str__ ) ) ) ; assert that ( exn . to string ( ) @$ contains string ( __str__ ) ) ;,trace contain frames
starting a pulse while docking should suppress wakeup <PLACE_HOLDER>,m status bar . m doze service host . pulse while dozing ( mock ( doze host . pulse callback . class ) @$ doze log . pulse_reason_docking ) ; verify ( m status bar window view ) . suppress wake up gesture ( eq ( true ) ) ;,docking suppress gestures
the ri explicitly guarantees this <PLACE_HOLDER> in the socket options.set option documentation .,if ( on && timeout < __num__ ) { throw new illegal argument exception ( __str__ ) ; } if ( on ) { impl . set option ( socket options . so_linger @$ integer . value of ( timeout ) ) ; } else { impl . set option ( socket options . so_linger @$ boolean . false ) ; },ri guarantees idiocy
chunked encoding only makes <PLACE_HOLDER> to do when the payload is signed,if ( ! is payload signing enabled ( request ) || is chunked encoding disabled ( request ) ) { return false ; } if ( request . get original request object ( ) instanceof put object request || request . get original request object ( ) instanceof upload part request ) { return true ; } return false ;,encoding makes sense
let the gridmix record fill <PLACE_HOLDER> .,return new record writer < k @$ gridmix record > ( ) { @ override public void write ( k ignored @$ gridmix record value ) throws io exception { value . write ( file out ) ; } @ override public void close ( task attempt context ctxt ) throws io exception { file out . close ( ) ; } } ;,record fill parent
support the xa rollback to do to write xa recover log and judge xa <PLACE_HOLDER> to judge if send xa end,if ( session . get xatxid ( ) != null && conn instanceof mysql connection ) { mysql connection mysql con = ( mysql connection ) conn ; string xa tx id = session . get xatxid ( ) ; coordinator log entry coordinator log entry = multi node coordinator . in memory repository . get ( xa tx id ) ; if ( coordinator log entry != null ) { write check point = true ; for ( int i = __num__ ; i < coordinator log entry . participants . length ; i ++ ) { if ( coordinator log entry . participants [ i ] . resource name . equals ( conn . get schema ( ) ) ) { coordinator log entry . participants,xa recover state
ensure that record provided <PLACE_HOLDER> to a data db record,if ( rec != null ) { if ( ! rec . has same schema ( datadb adapter . data_schema ) ) { return true ; } dt = code mgr . get data type ( rec ) ; if ( dt == null ) { msg . error ( this @$ __str__ + address ) ; } } else { dt = code mgr . get data type ( addr ) ; },record provided corresponds
only proxy cache implementation needs a json <PLACE_HOLDER> that has reference to user attributes,return new json formatter ( ) ;,implementation needs formatter
partition ca n't have this <PLACE_HOLDER>,reserved partition values . add ( hive conf . get var ( conf @$ conf vars . defaultpartitionname ) ) ; reserved partition values . add ( hive conf . get var ( conf @$ conf vars . default_zookeeper_partition_name ) ) ;,partition have parameter
finally check if either sender threw an <PLACE_HOLDER>,exception e ; e = s1 . get exception ( ) ; if ( e != null ) throw e ; e = s2 . get exception ( ) ; if ( e != null ) throw e ;,sender threw exception
old clients do n't send the <PLACE_HOLDER> .,checksum = new byte [ __num__ ] ;,clients send checksum
uh oh ... it looks like the provider 's <PLACE_HOLDER> has been killed on us . we need to wait for a new <PLACE_HOLDER> to be started @$ and make sure its death does n't kill our <PLACE_HOLDER> .,if ( ! success ) { slog . i ( tag @$ __str__ + cpr . name . flatten to short string ( ) + __str__ + r ) ; boolean last ref = dec provider count locked ( conn @$ cpr @$ token @$ stable ) ; check time ( start time @$ __str__ ) ; app died locked ( cpr . proc ) ; check time ( start time @$ __str__ ) ; if ( ! last ref ) { return null ; } provider running = false ; conn = null ; } else { cpr . proc . verified adj = cpr . proc . set adj ; },death kill device
check get namespace <PLACE_HOLDER> as xml @$ json and protobuf .,string namespace path = __str__ + ns name ; response = client . get ( namespace path ) ; assert equals ( __num__ @$ response . get code ( ) ) ; response = client . get ( namespace path @$ constants . mimetype_xml ) ; assert equals ( __num__ @$ response . get code ( ) ) ; namespaces instance model model = fromxml ( response . get body ( ) ) ; check namespace properties ( model . get properties ( ) @$ ns properties ) ; response = client . get ( namespace path @$ constants . mimetype_json ) ; assert equals ( __num__ @$ response . get code ( ) ) ; model = json mapper . read value ( response . get body,check get properties
set up key for wikidict ; if caseless use lower case <PLACE_HOLDER> of surface form,string mention surface form key ; if ( wikidict caseless ) mention surface form key = surface form . to lower case ( ) ; else mention surface form key = surface form ;,caseless use version
only consider entries with absolute path names . this allows storing ur is in the database without the media scanner removing <PLACE_HOLDER> .,if ( path != null && path . starts with ( __str__ ) ) { boolean exists = false ; try { exists = os . access ( path @$ android . system . os constants . f_ok ) ; } catch ( errno exception e1 ) { } if ( ! exists && ! mtp constants . is abstract object ( format ) ) { string mime type = media file . get mime type for file ( path ) ; if ( ! media file . is play list mime type ( mime type ) ) { deleter . delete ( row id ) ; if ( path . to lower case ( locale . us ) . ends with ( __str__ ) ) { deleter,ur removing them
items to test <PLACE_HOLDER>,item = new privacy item ( privacy item . type . subscription . name ( ) @$ true @$ i ) ; item . set value ( privacy rule . subscription_both ) ; original privacy items [ i ] = item ; i = i + __num__ ; item = new privacy item ( privacy item . type . subscription . name ( ) @$ false @$ i ) ; item . set value ( privacy rule . subscription_from ) ; original privacy items [ i ] = item ; i = i + __num__ ; item = new privacy item ( privacy item . type . subscription . name ( ) @$ true @$ i ) ; item . set value ( privacy rule . subscription_to ),items test group
partitioned tables do n't have table desc set on the fetch task . instead they have a <PLACE_HOLDER> of partition desc objects @$ each with a table desc . let 's try to fetch the desc for the first partition and use it 's deserializer .,if ( td == null && ft . get work ( ) != null && ft . get work ( ) . get part desc ( ) != null ) { if ( ft . get work ( ) . get part desc ( ) . size ( ) > __num__ ) { td = ft . get work ( ) . get part desc ( ) . get ( __num__ ) . get table desc ( ) ; } } if ( td == null ) { log . info ( __str__ ) ; } else { string table name = __str__ ; list < field schema > lst = null ; try { lst = hive meta store utils . get fields from deserializer ( table,tables have lot
given on lock screen and stack scroller has a nonzero <PLACE_HOLDER>,given lock screen ( ) ; m notification stack height = __num__ ; m keyguard status height = empty_height ;,scroller has height
if the string does n't include the year <PLACE_HOLDER> for some reason @$ then return the gregorian string .,if ( p == - __num__ ) { return s ; } p += year field . length ( ) ; string builder sb = new string builder ( s . substring ( __num__ @$ p ) ) ;,string include field
now we check if the rectangle of this actor in screen coordinates is in the rectangle spanned by the camera 's view . this assumes that the camera has no <PLACE_HOLDER> and is not rotated !,actor rect . set ( stagex @$ stagey @$ get width ( ) @$ get height ( ) ) ; cam rect . set ( camera . position . x - camera . viewport width / __num__ @$ camera . position . y - camera . viewport height / __num__ @$ camera . viewport width @$ camera . viewport height ) ; visible = cam rect . overlaps ( actor rect ) ; return ! visible ;,camera has bounds
this block of code can include <PLACE_HOLDER> if it 's parent <PLACE_HOLDER> is included,if ( annotation type matches ( element @$ parent element @$ include annotations @$ false @$ include jackson annotations @$ seen annotations @$ lines @$ imports resolver @$ element type @$ nullability ) ) { return true ; },block include element
<PLACE_HOLDER> time requested <PLACE_HOLDER>,final plan phase effective plan phase = catalog . find phase ( plan phase . get name ( ) @$ effective date @$ last change plan date ) ; return compute usages ( is cancelled or blocked @$ effective plan phase ) ;,time requested change
getter also creates the rigid <PLACE_HOLDER>,physics . get rigid body ( entity ) ;,getter creates body
result will have our <PLACE_HOLDER> @$ and elapsed time between snapshots,final entry entry = new entry ( ) ; final network stats result ; if ( recycle != null && recycle . capacity >= left . size ) { result = recycle ; result . size = __num__ ; result . elapsed realtime = delta realtime ; } else { result = new network stats ( delta realtime @$ left . size ) ; } for ( int i = __num__ ; i < left . size ; i ++ ) { entry . iface = left . iface [ i ] ; entry . uid = left . uid [ i ] ; entry . set = left . set [ i ] ; entry . tag = left . tag [ i ] ; entry .,result have size
this captures all logged <PLACE_HOLDER> @$ allowing us to verify log message was written .,final log interceptor log interceptor = new log interceptor ( ) ; test helper . drop all schemas ( ) ; test helper . executeddl ( __str__ ) ; configuration . builder config builder = test helper . default config ( ) . with ( postgres connector config . snapshot_mode @$ snapshot mode . initial_only . get value ( ) ) ; start ( postgres connector . class @$ config builder . build ( ) ) ; assert connector is running ( ) ; wait for available records ( __num__ @$ time unit . milliseconds ) ; stop connector ( value -> assert that ( log interceptor . contains warn message ( no_monitored_tables_warning ) ) . is false ( ) ) ;,all logged messages
the following two 'applys ' create multiple <PLACE_HOLDER> to our pipeline @$ one for each of our two input sources .,p collection < table row > events table = p . apply ( big queryio . read table rows ( ) . from ( gdelt_events_table ) ) ; p collection < table row > country codes = p . apply ( big queryio . read table rows ( ) . from ( country_codes ) ) ; p collection < string > formatted results = join events ( events table @$ country codes ) ; formatted results . apply ( textio . write ( ) . to ( options . get output ( ) ) ) ; p . run ( ) . wait until finish ( ) ;,'applys create events
this level has no more set <PLACE_HOLDER> @$ pop back up a level .,if ( next active slot bit number > __num__ ) { int index shift above = index shift + __num__ ; virtual index += __num__ << index shift above ; virtual index &= ~ ( ( __num__ << index shift above ) - __num__ ) ; return - virtual index ; },level set index
optionally check the byte after this frame matches sync <PLACE_HOLDER> .,if ( ! try read ( pes buffer @$ adts scratch . data @$ __num__ ) ) { return true ; } adts scratch . set position ( __num__ ) ; int frame size = adts scratch . read bits ( __num__ ) ; if ( frame size <= __num__ ) { return false ; },frame matches header
the method contains <PLACE_HOLDER> which need to be canonicalized,clause buffer . insert ( __num__ @$ __str__ ) ; compiled value cv = null ; for ( int j = this . args . size ( ) ; j > __num__ ; ) { cv = ( compiled value ) this . args . get ( -- j ) ; cv . generate canonicalized expression ( clause buffer @$ context ) ; clause buffer . insert ( __num__ @$ __str__ ) ; } clause buffer . delete char at ( __num__ ) . insert ( __num__ @$ __str__ ) . insert ( __num__ @$ this . method name ) ;,method contains parameters
vpn is using cell | wi <PLACE_HOLDER> .,m service . set underlying networks for vpn ( new network [ ] { m cell network agent . get network ( ) @$ m wi fi network agent . get network ( ) } ) ; wait for idle ( ) ;,vpn using fi
client should close the <PLACE_HOLDER> @$ but let 's hold it open .,assert equals ( - __num__ @$ client . get input stream ( ) . read ( ) ) ;,client close stream
<PLACE_HOLDER> is call not permitted <PLACE_HOLDER>,assert that ( result . failed ( ) . get ( ) ) . is instance of ( call not permitted exception . class ) ;,exception call exception
verify that a second call should still return absolute <PLACE_HOLDER>,final long [ ] [ ] times2 = increase time ( times1 ) ; write to file ( uid lines ( m uids @$ times2 ) ) ; m reader . read absolute ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; i ++ ) { m callback . verify ( m uids [ i ] @$ times2 [ i ] ) ; } m callback . verify no more interactions ( ) ; m callback . clear ( ) ; assert true ( m test file . delete ( ) ) ;,call return values
the contact already exist its <PLACE_HOLDER>,try { op set presence2 . subscribe ( fixture . userid1 ) ; } catch ( operation failed exception ex ) { if ( ex . get error code ( ) != operation failed exception . subscription_already_exists ) { throw ex ; } else { } },contact exist ok
ensure payload generation does n't throw an <PLACE_HOLDER>,byte [ ] serialized = new exec checking security manager ( ) . call wrapped ( new callable < byte [ ] > ( ) { public byte [ ] call ( ) throws exception { final string command = args . length > __num__ && args [ __num__ ] != null ? args [ __num__ ] : get default test cmd ( ) ; system . out . println ( __str__ + command + __str__ ) ; object payload < ? > payload = clazz . new instance ( ) ; final object obj before = payload . get object ( command ) ; system . out . println ( __str__ ) ; byte [ ] ser = serializer . serialize ( obj before ) ; utils,generation throw exception
make sure the fs and the found root dir have the same <PLACE_HOLDER>,log . debug ( __str__ + file system . get default uri ( fs . get file system ( ) . get conf ( ) ) ) ; log . debug ( __str__ + file system . get default uri ( fs . get configuration ( ) ) ) ;,fs have config
cocoa dragged event has the <PLACE_HOLDER> about which mouse button is being dragged . use it to determine the peer that should receive the dragged event .,if ( id == mouse event . mouse_dragged ) { target peer = mouse down target [ target idx ] ; mouse click buttons &= ~ modifiers ; } else if ( id == mouse event . mouse_released ) { target peer = mouse down target [ target idx ] ; if ( ( modifiers & event button mask ) == __num__ ) { mouse down target [ target idx ] = null ; } },event has information
image alt attribute numeric attachment <PLACE_HOLDER> of the image in the site 's media library,add metadata property ( metadata @$ __str__ @$ __str__ ) ; add metadata property ( metadata @$ __str__ @$ __str__ ) ;,alt attribute path
create the base list of classes which have possible <PLACE_HOLDER> to be overloaded,this . class list = new linked hash set < class > ( ) ; this . class list . add ( super class ) ; if ( generate delegate field ) { class list . add ( delegate class ) ; collections . add all ( this . class list @$ delegate class . get interfaces ( ) ) ; } if ( interfaces != null ) { collections . add all ( this . class list @$ interfaces ) ; } this . proxy name = proxy name ( ) ; this . empty body = empty body ;,which have methods
test <PLACE_HOLDER> builder constructs test runner <PLACE_HOLDER> with a 'null ' shell path only when we use the native test wrapper . something clearly went wrong .,if ( os . get current ( ) == os . windows && ! action . is using test wrapper instead of test setup script ( ) ) { preconditions . check not null ( action . get sh executable maybe ( ) @$ __str__ @$ action ) ; args . add ( action . get sh executable maybe ( ) . get path string ( ) ) ; args . add ( __str__ ) ; args . add ( __str__ ) ; },constructs test action
for mvcc caches we need to wait until updated value becomes visible for consequent readers . when mvcc <PLACE_HOLDER> completes @$ it 's updates are not visible immediately for the new <PLACE_HOLDER>s . this is caused by the lag between <PLACE_HOLDER> completes on the node and mvcc coordinator removes this <PLACE_HOLDER> from the active list .,grid test utils . run async ( new runnable ( ) { @ override public void run ( ) { string v ; while ( ! thread . current thread ( ) . is interrupted ( ) ) { v = cache . get ( key ) ; if ( v == null ) do sleep ( __num__ ) ; else { log . info ( __str__ + id ) ; ( ( ( grid future adapter ) ( ( ignite future impl ) promise ) . internal future ( ) ) ) . on done ( __str__ ) ; break ; } } } } ) ;,coordinator removes connection
& & ! special named arg <PLACE_HOLDER>,if ( no arg && ! super list . is empty ( ) && ! has no arg constructor ( c node ) ) { create no arg constructor ( c node @$ modifiers ) ; },& named constructor
hash at height 0 is just the regular tx hash <PLACE_HOLDER> .,if ( height == __num__ ) { return hashes . get ( pos ) ; },tx hash code
adding null where the complete tasks header simplifies a <PLACE_HOLDER> of logic for us,m tasks . add ( null ) ;,header simplifies lot
which segment contains the <PLACE_HOLDER>,bucket segment index = bucket > > > num buckets per segment bits ;,segment contains bucket
while we still have <PLACE_HOLDER> which have no outgoing edges,while ( no outgoing . size ( ) > __num__ ) { final graph . vertex < integer > current = no outgoing . remove ( __num__ ) ; sorted . add ( current ) ; int i = __num__ ; while ( i < edges . size ( ) ) { final graph . edge < integer > e = edges . get ( i ) ; final graph . vertex < integer > from = e . get from vertex ( ) ; final graph . vertex < integer > to = e . get to vertex ( ) ; if ( to . equals ( current ) ) { edges . remove ( e ) ; from . get edges ( ) . remove (,which have edges
delete the rows inserted from cache utils create <PLACE_HOLDER> @$ otherwise conflict in pk 's,jta obj . delete rows ( this . tbl name ) ; context ctx = cache . getjndi context ( ) ; user transaction ta = ( user transaction ) ctx . lookup ( __str__ ) ; connection conn = null ; try { ta . begin ( ) ; data source da = ( data source ) ctx . lookup ( __str__ ) ; conn = da . get connection ( ) ; statement stmt = conn . create statement ( ) ; string sqlstr = __str__ + this . tbl name + __str__ + this . tblid fld + __str__ + __str__ + this . tbl name fld + __str__ + __str__ ; stmt . execute update ( sqlstr ) ; ta . commit ( ),rows create table
then check if total estimated file size exceeds user specified <PLACE_HOLDER>,if ( total estimated export size > user specified limit ) { string builder sb = new string builder ( ) ; sb . append ( __str__ ) . append ( total estimated export size ) . append ( __str__ ) . append ( cli strings . export_logs__filesizelimit ) . append ( __str__ ) . append ( user specified limit ) . append ( __str__ ) ; return result model . create error ( sb . to string ( ) ) ; },size exceeds limit
determine if this update is changing the <PLACE_HOLDER> for the processor,if ( ! existing coordinate . equals ( incoming coordinate ) ) { if ( ! existing coordinate . get group ( ) . equals ( incoming coordinate . get group ( ) ) || ! existing coordinate . get id ( ) . equals ( incoming coordinate . get id ( ) ) ) { throw new illegal argument exception ( string . format ( __str__ @$ get identifier ( ) @$ existing coordinate . get coordinate ( ) @$ incoming coordinate . get coordinate ( ) ) ) ; } },update changing coordinate
test database meta data queries which do not have a parent <PLACE_HOLDER>,database meta data md = this . con . get meta data ( ) ; assert true ( md . get connection ( ) == this . con ) ; rs = md . get catalogs ( ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get columns ( null @$ null @$ null @$ null ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get functions ( null @$ null @$ null ) ; assert null ( rs . get statement ( ) ) ; rs . close ( ) ; rs = md . get imported keys (,which have statement
parse named <PLACE_HOLDER> .,int color start = start ; for ( int i = start + __num__ ; i < end ; i ++ ) { char ch = str . char at ( i ) ; if ( ch != __str__ ) continue ; color named color = colors . get ( str . sub sequence ( color start @$ i ) . to string ( ) ) ; if ( named color == null ) return - __num__ ; color color = color pool . obtain ( ) ; color stack . add ( color ) ; color . set ( named color ) ; return i - start ; },parse named colors
no handlers left so close the actual <PLACE_HOLDER> the done handler needs to be executed on the context that calls close @$ not the context of the actual <PLACE_HOLDER>,actual server . actual close ( context @$ completion ) ;,handlers left server
if it is no longer pending someone called unschedule async write so we do n't need to write the <PLACE_HOLDER> @$ but if we have a version tag we need to record the operation to update the rvv,if ( tag != null ) { disk entry . helper . do async flush ( tag @$ region ) ; },async write entries
device idle controller adds <PLACE_HOLDER> to local services in the constructor @$ so we have to remove them after each test @$ otherwise @$ subsequent tests will fail .,local services . remove service for test ( app state tracker . class ) ; local services . remove service for test ( device idle controller . local service . class ) ;,controller adds tables
fully recovery needs much longer <PLACE_HOLDER>,assert equals ( load balancer simulator . get point ( __str__ @$ default partition accessor . default_partition_id @$ uri1 ) @$ __num__ ) ; assert equals ( load balancer simulator . get point ( __str__ @$ default partition accessor . default_partition_id @$ uri2 ) @$ __num__ ) ;,recovery needs time
gana ya gana small ya kana ya kana small ya gana yu gana small yu kana yu kana small yu gana <PLACE_HOLDER> gana small <PLACE_HOLDER> kana <PLACE_HOLDER> kana small <PLACE_HOLDER>,return new object [ ] [ ] { { __str__ @$ __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__ + __str__,kana yo kana
check for a vertex hit first @$ otherwise @$ we get edge <PLACE_HOLDER> when we are hovering over a vertex @$ due to how edges are interpreted as existing all the way to the center point of a vertex,v vertex = get pick support ( ) . get vertex ( viewer layout @$ p . getx ( ) @$ p . gety ( ) ) ; if ( vertex != null ) { return new vertex tool tip info ( vertex @$ event ) ; } e edge = get pick support ( ) . get edge ( viewer layout @$ p . getx ( ) @$ p . gety ( ) ) ; if ( edge != null ) { return new edge tool tip info ( edge @$ event ) ; },check get exceptions
does the package have <PLACE_HOLDER> ? if not @$ there wo n't be any artifacts .,if ( ! package dex optimizer . can optimize package ( pkg ) ) { continue ; } if ( pkg . code path == null ) { slog . w ( tag @$ __str__ + pkg + __str__ ) ; continue ; },package have java
empty bundles do n't impact <PLACE_HOLDER> and should n't trigger downstream execution @$ so filter them out,if ( ! iterables . is empty ( committed . get elements ( ) ) ) { completed . add ( committed ) ; },bundles impact output
ie does n't fire popstate correctly with certain hash changes . simulate the missing <PLACE_HOLDER> with history handler .,if ( browser info . get ( ) . isie ( ) ) { history . add value change handler ( evt -> { final string new location = browser . get window ( ) . get location ( ) . to string ( ) ; if ( ! new location . equals ( current location ) ) { current location = new location ; get rpc proxy ( ui server rpc . class ) . popstate ( browser . get window ( ) . get location ( ) . to string ( ) ) ; } } ) ; current location = browser . get window ( ) . get location ( ) . to string ( ) ; },ie simulate camera
should return empty <PLACE_HOLDER>,resp = post ( __str__ @$ collections . empty_map ) ; assert not null ( resp ) ; map m = ( map ) resp . get ( __str__ ) ; assert true ( m . is empty ( ) ) ; resp = post ( __str__ @$ immutable map . of ( __str__ @$ immutable list . of ( __str__ @$ test pipeline impl . class . get name ( ) ) ) ) ; assert not null ( resp ) ; m = ( map ) resp . get ( __str__ ) ; assert not null ( m ) ; assert equals ( __num__ @$ m . size ( ) ) ; map v = ( map ) m . get ( __str__ ) ; assert,return empty map
determine if we are working with a dlgtemplate or dlgtemplateex structure . the first 4 bytes will have specific <PLACE_HOLDER> if it 's a dlgtemplateex .,try { boolean ex = mem buffer . get short ( __num__ ) == __num__ && mem buffer . get short ( __num__ ) == - __num__ ; temp offset = add dlg template structure ( mem buffer @$ comps @$ temp offset @$ ex ) ; temp offset = add dialog menu array ( mem buffer @$ comps @$ temp offset ) ; temp offset = add dialog class array ( mem buffer @$ comps @$ temp offset ) ; temp offset = add dialog title array ( mem buffer @$ comps @$ temp offset ) ; byte get style = mem buffer . get byte ( __num__ ) ; if ( ( get style & ds_setfont ) > __num__ ) { temp offset = add dialog,bytes have information
now @$ iterate over all the discovered <PLACE_HOLDER>,for ( class < ? > iface : interfaces ) { for ( membert member : getter . get members ( iface ) ) { if ( member . is annotation present ( anno ) ) { matches . add ( member ) ; } } } return matches ;,iterate discovered interfaces
this may happen if an app has a recorded <PLACE_HOLDER> @$ but has been uninstalled .,continue ;,app has status
this methods monitors a <PLACE_HOLDER> for new files to read in for streaming .,javad stream < string > log data = jssc . text file stream ( flags . get instance ( ) . get logs directory ( ) ) ; javad stream < apache access log > access logsd stream = log data . map ( new functions . parse from log line ( ) ) . cache ( ) ; final log analyzer total log analyzer total = new log analyzer total ( ) ; final log analyzer windowed log analyzer windowed = new log analyzer windowed ( ) ;,methods monitors stream
default generator does n't provide any <PLACE_HOLDER> for emitting supporting files other than by a mustache template @$ so we 're obliged to serialize the caches to json strings and use templates to write them .,if ( load test data from file ) { try { if ( test data cache . root ( ) . is dirty ( ) ) { byte array output stream out = new byte array output stream ( ) ; test data cache . root ( ) . flush ( out ) ; string test data json = new string ( out . to byte array ( ) @$ __str__ ) ; objs . put ( __str__ @$ test data json ) ; supporting files . add ( new supporting file ( __str__ @$ test data file . get absolute path ( ) ) ) ; } } catch ( cache exception | unsupported encoding exception e ) { logger . error ( __str__ + test data,generator provide methods
virtual or physical memory over limit . fail the <PLACE_HOLDER> and remove the corresponding process tree,if ( is memory over limit ) { log . warn ( msg ) ; if ( ! p tree . check pid pgrpid for match ( ) ) { log . error ( __str__ + __str__ @$ p id ) ; } event dispatcher . get event handler ( ) . handle ( new container kill event ( container id @$ container exit status @$ msg ) ) ; tracking containers . remove ( container id ) ; log . info ( __str__ @$ p id ) ; },memory fail intent
130 hz <PLACE_HOLDER> .,if ( eq values [ __num__ ] == __num__ ) { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) __num__ ) ; } else if ( eq values [ __num__ ] < __num__ ) { if ( eq values [ __num__ ] == __num__ ) { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) - __num__ ) ; } else { m equalizer helper . get equalizer2 ( ) . set band level ( one thirty hertz band @$ ( short ) ( - ( __num__ - eq values [ __num__ ] ) * __num__ ) ) ; } } else if,130 hz band
test the same data and <PLACE_HOLDER> with prior @$ should get the same <PLACE_HOLDER> except for the intercept,glm = new glm ( params ) ; model3 = glm . train model ( ) . get ( ) ; double lambda = model3 . _output . _submodels [ model3 . _output . _best_lambda_idx ] . lambda_value ; params . _lambda_search = false ; params . _lambda = new double [ ] { lambda } ; model metrics mm3 = model metrics . get fromdkv ( model3 @$ fr ) ; assert equals ( __str__ + model3 . _output . _training_metrics . _mse + __str__ + mm3 . _mse @$ model3 . _output . _training_metrics . _mse @$ mm3 . _mse @$ __num__ ) ; assert equals ( __str__ + ( ( model metrics binomialglm ) model3 . _output . _training_metrics ) . _res dev + __str__,data get model
fails with primitive <PLACE_HOLDER>es ; need the wrapper <PLACE_HOLDER> . thanks @$ java .,a = ( e [ ] ) array . new instance ( afclz @$ splits . length ) ;,fails need struct
fragment factory needs to be set before calling the super.on create @$ otherwise the activity crashes when it is recreating and there is a fragment which has no default <PLACE_HOLDER> .,super . on create ( saved instance state ) ;,which has state
catch <PLACE_HOLDER>s since set daemon can cause a security <PLACE_HOLDER> to be thrown under netscape in the applet mode,set daemon ( true ) ;,daemon cause exception
to avoid race condition of testcase @$ atleast check 40 <PLACE_HOLDER> with sleep of 50 ms,verify entity for timelinev2 ( app entity file @$ application metrics constants . finished_event_type @$ __num__ @$ __num__ @$ __num__ @$ false ) ;,atleast check times
reached end of string . always a break <PLACE_HOLDER> .,if ( p2 >= f text . length ( ) ) { break ; },a break position
if the app is undergoing <PLACE_HOLDER> @$ tell the <PLACE_HOLDER> manager about it,final backup record backup target = m backup targets . get ( app . user id ) ; if ( backup target != null && app . pid == backup target . app . pid ) { if ( debug_backup || debug_cleanup ) slog . d ( tag_cleanup @$ __str__ + backup target . app info + __str__ ) ; m handler . post ( new runnable ( ) { @ override public void run ( ) { try { i backup manager bm = i backup manager . stub . as interface ( service manager . get service ( context . backup_service ) ) ; bm . agent disconnected for user ( app . user id @$ app . info . package name ) ; } catch,app undergoing backup
if the metafile references a <PLACE_HOLDER> @$ remove it from the map .,try { dbus event buffer meta info mi = new dbus event buffer meta info ( f ) ; mi . load meta info ( ) ; if ( mi . is valid ( ) ) { string session id = mi . get session id ( ) ; session file map . remove ( session id ) ; } } catch ( dbus event buffer meta info . dbus event buffer meta info exception e ) { log . warn ( __str__ + f . get name ( ) + __str__ @$ e ) ; return ; },metafile references session
remove the receive <PLACE_HOLDER> and the order by <PLACE_HOLDER> and replace them with a merge receive <PLACE_HOLDER> . leave the order by <PLACE_HOLDER> inline in the merge receive <PLACE_HOLDER> @$ since we need it to calculate the merge .,plan . clear children ( ) ; receive node . remove from graph ( ) ; merge receive plan node mrnode = new merge receive plan node ( ) ; mrnode . add inline plan node ( onode ) ; mrnode . add and link child ( send node ) ; plan . add and link child ( mrnode ) ; return plan ;,node receive node
skip due to android devices that have broken scrolltop will may get odd <PLACE_HOLDER> here .,if ( browser info . get ( ) . is touch device ( ) ) { return ; },will get comments
note : we shouldnt ' have to do this @$ since j file chooser adds the <PLACE_HOLDER> to the choosable <PLACE_HOLDER>s list when the <PLACE_HOLDER> is set . lets be paranoid just in case someone overrides set file <PLACE_HOLDER> in j file chooser .,file filter current filter = get file chooser ( ) . get file filter ( ) ; boolean found = false ; if ( current filter != null ) { for ( file filter filter : filters ) { if ( filter == current filter ) { found = true ; } } if ( ! found ) { get file chooser ( ) . add choosable file filter ( current filter ) ; } } return get file chooser ( ) . get file filter ( ) ;,chooser adds filter
query must produce only <PLACE_HOLDER> from single region .,validate clients ( region id @$ clients2 ) ; if ( region id == unmapped_region ) fail ( ) ;,query produce results
this test verifies the baseline <PLACE_HOLDER> used in subsequent tests . if this fails @$ the rest will fail .,project dependency graph graph = three projects depending ona single ( ) ; final list < maven project > sorted projects = graph . get sorted projects ( ) ; assert equals ( a project @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender1 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender2 @$ sorted projects . get ( __num__ ) ) ; assert equals ( depender3 @$ sorted projects . get ( __num__ ) ) ;,test verifies values
reopen implies the <PLACE_HOLDER> of the reopened session for the same query that we gave it out for ; so @$ as we would have failed an active query @$ fail the <PLACE_HOLDER>r before it 's started .,future . set exception ( new runtime exception ( __str__ + session . get reason for kill ( ) ) ) ; return ;,reopen implies future
for some reason @$ the <PLACE_HOLDER> is not stored @$ so the restored values use the <PLACE_HOLDER> from the default jvm timezone .,if ( time as timestamp remappingh2 dialect . class . equals ( get remapping dialect class ( ) ) ) { return get original property value ( ) . with offset same local ( offset date time . now ( ) . get offset ( ) ) ; } else { return get original property value ( ) . with nano ( __num__ ) . with offset same local ( offset date time . now ( ) . get offset ( ) ) ; },values use offset
remove all items using <PLACE_HOLDER>,iterator iterator = observable collection . iterator ( ) ; while ( iterator . has next ( ) ) { iterator . next ( ) ; iterator . remove ( ) ; },items using iterator
this will obtain any waiting outbound <PLACE_HOLDER> @$ or will process the outbound app <PLACE_HOLDER> .,try { synchronized ( write lock ) { hs status = write record ( output record @$ ea ) ; } } catch ( ssl exception e ) { throw e ; } catch ( io exception e ) { throw new ssl exception ( __str__ @$ e ) ; },any waiting connections
removing these calls and the three location assertions above will cause the test to fail due to the strict stubs . without the alterations to stack trace cleaner provider @$ the failure messages will contain an <PLACE_HOLDER> in the stack trace inside the powermock libraries .,assert that ( something with static method . do static one ( ) @$ is ( __str__ ) ) ; assert that ( something with static method . do static two ( ) @$ is ( __str__ ) ) ; something with static method . do static void ( ) ;,messages contain error
producer will do it 's own destination <PLACE_HOLDER> so always use the destination based send method otherwise we might violate a jms rule .,message producer . send ( destination @$ message @$ delivery mode @$ priority @$ time to live ) ;,producer do lookup
check if the caller has enough <PLACE_HOLDER> to embed activities and launch to private displays .,final int start any perm = m service . check permission ( internal_system_window @$ calling pid @$ calling uid ) ; if ( start any perm == permission_granted ) { if ( debug_tasks ) slog . d ( tag @$ __str__ + __str__ ) ; return true ; },caller has permissions
trigger the creation of a bucket @$ which should trigger the <PLACE_HOLDER> of this vm .,assert that thrown by ( ( ) -> region . put ( __str__ @$ __str__ ) ) . is instance of ( cancel exception . class ) ;,which trigger right
first see if the file matches the regular <PLACE_HOLDER> !,if ( pattern != null ) { matcher matcher = pattern . matcher ( item . get name ( ) . geturi ( ) ) ; get it = matcher . matches ( ) ; } if ( patternexclude != null ) { matcher matcherexclude = patternexclude . matcher ( item . get name ( ) . geturi ( ) ) ; get itexclude = matcherexclude . matches ( ) ; } boolean take = take this file ( item @$ new file name ) ; if ( get it && ! get itexclude && take ) { if ( log . is detailed ( ) ) { log detailed ( base messages . get string ( pkg @$ __str__ @$ item . get name ( ) .,file matches expression
since file download task wraps the actual <PLACE_HOLDER> with download <PLACE_HOLDER> . we should extract it letting the error message clearer .,if ( ! is dependents succeeded ( ) ) { exception t = task . get exception ( ) ; if ( t instanceof download exception ) throw new library download exception ( library @$ t . get cause ( ) ) ; else throw new library download exception ( library @$ t ) ; } else { if ( xz ) unpack library ( jar @$ files . read all bytes ( xz file . to path ( ) ) ) ; if ( ! checksum valid ( jar @$ library . get checksums ( ) ) ) { jar . delete ( ) ; throw new io exception ( __str__ + library ) ; } },task wraps exception
unmatched files contains any files that had no association to a program give the user a <PLACE_HOLDER> to search the project for it @$ and import it if not found,if ( ! unmatched files . is empty ( ) ) { unmatched files = search project for matching files or fail ( unmatched files @$ program manager @$ monitor @$ programs opened ) ; } return unmatched files ;,files give chance
the new code can collapse extra separator <PLACE_HOLDER>,compare ( __str__ @$ __str__ ) ; compare ( __str__ @$ __str__ ) ;,code collapse characters
if the client supplied connection credentials @$ the <PLACE_HOLDER> filter will perform a normal <PLACE_HOLDER> @$ so we should exit immediately :,if ( credentials available ( conn ) ) { return ; },filter perform shutdown
wait a minute and you should get other 6 <PLACE_HOLDER> executed,wait minute quota ( ) ;,minute get requests
did n't enclose table pattern within single quotes . table name and include list not allowed . table name and exclude list not allowed . abrubtly ended <PLACE_HOLDER> . with square brackets two <PLACE_HOLDER>s with empty list more than two list,string [ ] invalid repl policies = new string [ ] { primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ @$ primary db name + __str__ } ;,name ended policy
finally check account notes did get <PLACE_HOLDER>,assert . assert null ( retrieved account . get notes ( ) ) ;,notes get updated
cooked string can be null only for tagged template literals . a tagged template literal would hit the default <PLACE_HOLDER> below .,if ( node . has one child ( ) ) { return check not null ( node . get first child ( ) . get cooked string ( ) ) ; } else { throw new malformed exception ( __str__ @$ node ) ; },literal hit check
any <PLACE_HOLDER> but primary & proxy can contain any <PLACE_HOLDER> :,if ( m != primary && m != proxy ) for ( igfs mode n : igfs mode . values ( ) ) assert true ( igfs utils . can contain ( m @$ n ) ) ;,primary contain node
async write the <PLACE_HOLDER> back .,member . remote . send string ( data @$ null ) ;,async write data
crawl job needs to be sure all beans have received finished <PLACE_HOLDER> before teardown,this . is stop complete = true ; app ctx . publish event ( new stop complete event ( this ) ) ;,beans received intent
any errors here should let the <PLACE_HOLDER> come to a halt and be recognized by the writer,flush all ( ) ;,errors let stream
we keep a separate list of <PLACE_HOLDER> for each object newly obtained from a view @$ and perform a shallow copy during get clone . that way the list of <PLACE_HOLDER> performed contains all <PLACE_HOLDER> performed on the view by the tree of nodes initialized from it . note that initializing two nodes with the same view will not merge the two lists @$,shadow obtained . performed action and args list = new array list < > ( ) ; shadow obtained . view = view ; s allocation count ++ ; if ( shadow obtained . m origin node id == __num__ ) { shadow obtained . m origin node id = s allocation count ; } strict equality node wrapper wrapper = new strict equality node wrapper ( obtained instance ) ; obtained instances . put ( wrapper @$ thread . current thread ( ) . get stack trace ( ) ) ; ordered instances . put ( s allocation count @$ wrapper ) ; return obtained instance ;,list contain actions
we should recalculate geometry just before calculation of the thumb movement direction . it is important for the case @$ when j slider is a cell editor in j table . see <PLACE_HOLDER> .,calculate geometry ( ) ; final boolean first click = ( current mousex == - __num__ ) && ( current mousey == - __num__ ) ; current mousex = e . getx ( ) ; current mousey = e . gety ( ) ; if ( slider . is request focus enabled ( ) ) { slider . request focus ( ) ; } boolean is mouse event in thumb = thumb rect . contains ( current mousex @$ current mousey ) ;,calculation see 6348946
skip non union r dot <PLACE_HOLDER>,fake buildable context buildable context = new fake buildable context ( ) ; list < step > steps = dummyr dot java . get build steps ( fake build context . noop_context @$ buildable context ) ; assert equals ( __str__ @$ __num__ @$ steps . size ( ) ) ; path r dot java src folder = dummyr dot java . getr dot java src folder ( dummyr dot java . get build target ( ) @$ filesystem ) ; path r dot java bin folder = compiler output paths . get classes dir ( dummyr dot java . get build target ( ) @$ filesystem ) ; path r dot java output folder = dummyr dot java . get path to output dir ( dummyr dot,r dot java
the immersive mode confirmation took the focus from m last focused <PLACE_HOLDER> which was controlling the system ui visibility . so if m last focused <PLACE_HOLDER> can still receive keys @$ we let it keep controlling the visibility .,if ( win candidate . get attrs ( ) . token == m immersive mode confirmation . get window token ( ) ) { final boolean last focus can receive keys = ( m last focused window != null && m last focused window . can receive keys ( ) ) ; win candidate = is status bar keyguard ( ) ? m status bar : last focus can receive keys ? m last focused window : m top fullscreen opaque window state ; if ( win candidate == null ) { return __num__ ; } },focus focused window
reset the complex type attribute <PLACE_HOLDER> such that it is recalculated again .,cleanup is complex type attribute ( ) ; int index = __num__ ; for ( observed object o : observed objects ) { reset already notified ( o @$ index ++ @$ observed_attribute_error_notified | observed_attribute_type_error_notified ) ; },type attribute information
verify ask after registration but before launch . do n't kill @$ should be null . now put a <PLACE_HOLDER> with the id,listener . register pending task ( task @$ wid ) ; result = listener . get task ( context ) ; assert null ( result ) ;,registration put task
string object 2 <PLACE_HOLDER> and a reference on a string,int string size = jvm size utils . get object header size ( ) + ( __num__ * __num__ ) + jvm size utils . get reference size ( ) ;,string object checks
src 1 has no <PLACE_HOLDER> to accommodate new rename node,fs . set quota ( src1 . get parent ( ) @$ __num__ @$ hdfs constants . quota_dont_set ) ; create file ( src1 ) ; fs . set quota ( src1 . get parent ( ) @$ __num__ @$ hdfs constants . quota_dont_set ) ; rename ( dst1 @$ src1 @$ false @$ true @$ rename . overwrite ) ;,src has quota
the rest li response data <PLACE_HOLDER> . this can potentially be refactored .,throwable error ; try { rest li service exception service exception = response data . get response envelope ( ) . get exception ( ) ; final rest li response response = _response handler . build partial response ( _method @$ response data ) ; error = new rest li response exception ( service exception @$ response ) ; } catch ( throwable throwable ) { logger . error ( __str__ @$ response data . get response envelope ( ) . get exception ( ) ) ; error = throwable ; } _wrapped callback . on error ( error ) ;,rest li itself
for every pixel in this row get the <PLACE_HOLDER>,while ( off < row limit ) { pixels [ off ++ ] = index into gradients arrays ( g ) ; g += dgdx ; },pixel get accumulator
note : other parent readers init <PLACE_HOLDER> in ctor @$ but union does it in start stripe .,this . tags = new run length byte reader ( data stream ) ;,readers init them
sql server and oracle does n't normally support <PLACE_HOLDER> by in subqueries ...,if ( ! ( get dialect ( ) instanceof sql server dialect ) && ! ( get dialect ( ) instanceof oracle8i dialect ) ) { dc4 . get executable criteria ( session ) . add order ( order . asc ( __str__ ) ) . list ( ) ; } session . create criteria ( enrolment . class @$ __str__ ) . add ( subqueries . eq ( __str__ @$ dc4 ) ) . list ( ) ; session . delete ( enrolment2 ) ; session . delete ( gavin ) ; session . delete ( course ) ; t . commit ( ) ; session . close ( ) ;,server support order
ca n't use high watermark directly @$ as the partition map may have different <PLACE_HOLDER> . for example @$ high watermark may be specified to seconds @$ but partition map could be specified to hour or date .,long highest watermark = collections . max ( partition map . values ( ) ) ; for ( map . entry < long @$ long > entry : partition map . entry set ( ) ) { long partition high watermark = entry . get value ( ) ; if ( partition high watermark . equals ( highest watermark ) ) { partitions . add ( new partition ( entry . get key ( ) @$ partition high watermark @$ true @$ has user specified high watermark ) ) ; } else { partitions . add ( new partition ( entry . get key ( ) @$ partition high watermark @$ false ) ) ; } },map have timestamps
preserve the same order that github org api <PLACE_HOLDER>,map < string @$ scm organization > org map = new linked hash map < > ( ) ;,github org methods
parse float allows an <PLACE_HOLDER> or a sign,if ( s . matches ( __str__ ) ) { throw new number format exception ( __str__ ) ; } try { float value = float . parse float ( s ) ; if ( value < __num__ || value > __num__ ) { throw new number format exception ( __str__ ) ; } return value ; } catch ( number format exception e ) { throw new number format exception ( __str__ ) ; },float allows sign
if n has 1 and m has <PLACE_HOLDER>,if ( is bitn && ! is bitm ) { m = m | ( m << k ) ; } else if ( ! is bitn && is bitm ) { int mask = ~ ( m << k ) ; m = m & mask ; },n has 1
deep copy the matchers and logs @$ which allows the <PLACE_HOLDER> to be reused .,for ( matcher and error m : matchers and logs ) { this . matchers and logs . add ( m ) ; },which allows them
now generate another <PLACE_HOLDER> @$ but without having updated the package . the package tracker should recognize the last <PLACE_HOLDER> failed and trigger again .,simulate package installation ( package versions ) ;,tracker recognize check
fancier parser would give line <PLACE_HOLDER> and column ...,if ( ! actual str . equals ignore case ( expected str ) ) { throw new io exception ( __str__ + printable ( expected str ) + __str__ + printable ( actual str ) ) ; },parser give name
no instance has correct <PLACE_HOLDER> configured,try { driver . assign instances ( instance partitions type . offline @$ instance configs ) ; fail ( ) ; } catch ( illegal state exception e ) { assert equals ( e . get message ( ) @$ __str__ ) ; } for ( int i = __num__ ; i < num instances ; i ++ ) { instance config instance config = instance configs . get ( i ) ; if ( i < num instances / __num__ ) { instance config . get record ( ) . set map field ( instance . pool_key @$ collections . singleton map ( offline_tag @$ __str__ ) ) ; } else { instance config . get record ( ) . set map field ( instance . pool_key,instance has pool
traverse the children which can be either just include tables list or both include and exclude tables <PLACE_HOLDER> .,tree old policy tables list node = old repl policy tree . get child ( __num__ ) ; assert ( old policy tables list node . get type ( ) == tok_repl_tables ) ; set repl dump tables list ( old policy tables list node @$ old repl scope ) ;,list include lists
this function needs its stack looked at . set its stack <PLACE_HOLDER> to invalid @$ so wo n't be added to list again if called recursively .,func . set stack purge size ( function . invalid_stack_depth_change ) ; func list . add ( __num__ @$ func ) ;,stack set purge
instead of getting this directly from the lp @$ use reflection so that a package which uses grammatical <PLACE_HOLDER> does n't necessarily have to use lexicalized parser,try { method method = lp . get class ( ) . get method ( __str__ ) ; params = ( treebank lang parser params ) method . invoke ( lp ) ; params . set generate original dependencies ( generate original dependencies ) ; } catch ( exception cnfe ) { throw new runtime exception ( cnfe ) ; },which uses debuggers
we can now utilize shader program 's hash map which may be better than gl get uniform <PLACE_HOLDER>,shader program . set strict mode ( false ) ;,gl get map
if specified enable <PLACE_HOLDER>,try { if ( ! kerberos . is empty ( ) ) { config . enable kerberos authentication ( kerberos ) ; } m_client = get client ( config @$ servers @$ port ) ; } catch ( exception exc ) { system . err . println ( exc . get message ( ) ) ; return - __num__ ; },specified enable authentication
and will throw exception . for cases when users will do <PLACE_HOLDER> as result of that as well we need to support chain of <PLACE_HOLDER> calls but still fail on <PLACE_HOLDER> @$ commit,if ( ! is open ( ) && failure ) { return ; } failure ( ) ; close transaction ( ) ;,users do sync
this special create table is called locally to master . therefore @$ no rpc means no <PLACE_HOLDER> to use nonce to detect duplicated rpc call .,long proc id = this . procedure executor . submit procedure ( new create table procedure ( procedure executor . get environment ( ) @$ table descriptor @$ new regions ) ) ; return proc id ;,rpc means need
download attributes and return file <PLACE_HOLDER> only if the blob exists .,if ( null != blob && blob . exists ( get instrumented context ( ) ) ) { log . debug ( __str__ @$ key ) ; try { blob . download attributes ( get instrumented context ( ) ) ; blob properties properties = blob . get properties ( ) ; if ( retrieve folder attribute ( blob ) ) { log . debug ( __str__ @$ key ) ; return new file metadata ( key @$ properties . get last modified ( ) . get time ( ) @$ get permission status ( blob ) @$ blob materialization . explicit @$ hadoop block size ) ; } else { log . debug ( __str__ @$ key ) ; return new file metadata ( key @$ get,attributes file metadata
verify new file is using it 's own <PLACE_HOLDER> @$ with new keyversions @$ and can be decrypted correctly .,assert key version changed ( recreated @$ fei orig ) ; final string content = dfs test util . read file ( fs @$ recreated ) ; assert equals ( content orig @$ content ) ;,file using copy
this call not only toggles the error <PLACE_HOLDER> on @$ but also wires our special error display wrapper .,set errorgui enabled ( true ) ;,call toggles dialog
server socket will accept <PLACE_HOLDER>,socket base server = zmq . socket ( ctx @$ zmq . zmq_dealer ) ; assert that ( server @$ not null value ( ) ) ; zmq . set socket option ( server @$ zmq . zmq_curve_server @$ true ) ; zmq . set socket option ( server @$ zmq . zmq_curve_secretkey @$ server secret ) ; zmq . set socket option ( server @$ zmq . zmq_identity @$ __str__ ) ; rc = zmq . bind ( server @$ host ) ; assert that ( rc @$ is ( true ) ) ; host = ( string ) zmq . get socket option ext ( server @$ zmq . zmq_last_endpoint ) ; int port = test utils . port ( host ) ;,socket accept connections
unmanaged a ms do <PLACE_HOLDER> amrm token,assert . assert not null ( rm client . getamrm token ( app id ) ) ; return app id ;,ms do return
create second expired certificate whose key usage extension does not allow code <PLACE_HOLDER>,create alias ( second_key_alias ) ; issue cert ( second_key_alias @$ __str__ @$ __str__ @$ __str__ @$ __str__ + validity * __num__ + __str__ @$ __str__ @$ integer . to string ( validity ) ) ;,extension allow signing
in case default constructor someday does <PLACE_HOLDER>,this ( ) ;,constructor does work
we allow one fewer output buffer due to the way that media codec r<PLACE_HOLDER>erer and the underlying decoders handle the <PLACE_HOLDER> of stream . this should be tightened up in the future .,decoder counters util . assert total buffer count ( tag + audio_tag_suffix @$ audio counters @$ audio counters . input buffer count - __num__ @$ audio counters . input buffer count ) ; decoder counters util . assert total buffer count ( tag + video_tag_suffix @$ video counters @$ video counters . input buffer count - __num__ @$ video counters . input buffer count ) ;,renderer handle type
make sure vm 1 did n't create the <PLACE_HOLDER>,assert that ( vm1 . invoke ( ( ) -> get bucket list ( partitioned region name ) ) ) . is empty ( ) ; vm1 . invoke ( ( ) -> get cache ( ) . close ( ) ) ; async invocation < void > create partitioned region onvm0 = vm0 . invoke async ( ( ) -> create partitioned region ( __num__ @$ - __num__ @$ __num__ @$ true ) ) ; create partitioned region onvm0 . join ( seconds . to millis ( __num__ ) ) ;,vm create bucket
if there is no existing named view add <PLACE_HOLDER>,add to cache ( named view detail @$ false ) ; return ;,view add one
if we have found the account <PLACE_HOLDER> for this protocol provider return this <PLACE_HOLDER>,if ( index != null ) { return integer . parse int ( index ) ; } else { return create account index ( protocol provider @$ account root prop name ) ; },index return index
support for automatic static linking of standard libraries . this works because all of the jdk uses system.load <PLACE_HOLDER> or jdk.internal.loader.boot loader with literal string arguments . if such a <PLACE_HOLDER> is in our list of static standard libraries @$ add the <PLACE_HOLDER> to the linker command .,if ( libname != null && native library support . singleton ( ) . is preregistered builtin library ( libname ) && registered libraries . put if absent ( libname @$ boolean . true ) != boolean . true ) { native libraries . add library ( libname @$ true ) ; },all uses class
mock suspend @$ which stops <PLACE_HOLDER> 1 and sets suspended state in metadata @$ flipping to <PLACE_HOLDER> 2 in test <PLACE_HOLDER> spec implementation of create suspended spec,reset all ( ) ; metadata supervisor manager . insert ( easy mock . eq ( __str__ ) @$ easy mock . capture ( captured insert ) ) ; supervisor2 . start ( ) ; supervisor1 . stop ( true ) ; replay all ( ) ; manager . suspend or resume supervisor ( __str__ @$ true ) ; assert . assert equals ( __num__ @$ manager . get supervisor ids ( ) . size ( ) ) ; assert . assert equals ( captured insert . get value ( ) @$ manager . get supervisor spec ( __str__ ) . get ( ) ) ; assert . assert true ( captured insert . get value ( ) . suspended ) ; verify all ( ) ;,which stops supervisor
nm 1 do 50 <PLACE_HOLDER>,capacity scheduler cs = ( capacity scheduler ) rm1 . get resource scheduler ( ) ; rm node rm node1 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm1 . get node id ( ) ) ; rm node rm node2 = rm1 . getrm context ( ) . getrm nodes ( ) . get ( nm2 . get node id ( ) ) ; scheduler node scheduler node1 = cs . get scheduler node ( nm1 . get node id ( ) ) ;,nm do heartbeats
if <PLACE_HOLDER> is declared to be thrown by the proxy method @$ then no catch blocks are necessary @$ because the invoke can @$ at most @$ throw <PLACE_HOLDER> anyway .,if ( ex . is assignable from ( throwable . class ) ) { unique list . clear ( ) ; break ; } else if ( ! throwable . class . is assignable from ( ex ) ) { continue ; },invoke throw exception
check size will open a new <PLACE_HOLDER> if we exceeded the max bytes setting,writer . check size ( ) ; if ( writer . get position ( ) != position ) { position = writer . get position ( ) ; },size open file
use in alert prevents <PLACE_HOLDER> of x,test same ( lines ( __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ) ;,use prevents removal
asif asif : the iter operands passed are null @$ as a not null value can exists only if there exists a single <PLACE_HOLDER> operand in original group junction,filter results = filter . filter evaluate ( context @$ ! is conditioning needed ? intermediate results : null @$ this . complete expansion @$ null @$ this . indpndnt itr @$ _operator == literal_and @$ is conditioning needed @$ false ) ;,value exists condition
a media player created by a video view should already have its m subtitle controller <PLACE_HOLDER> .,if ( m subtitle controller == null ) { set subtitle anchor ( ) ; } if ( ! m subtitle controller . has renderer for ( f format ) ) { context context = activity thread . current application ( ) ; m subtitle controller . register renderer ( new srt renderer ( context @$ m event handler ) ) ; },player have instance
the time we accessed wrapper.reference count @$ the wrapper was tombstoned by a pending release task . this race condition is highly unlikely to happen as there is no systematic coding practice which can cause this error because of ttl . however @$ even in very unlikely case when it happen we have the retry which get a valid <PLACE_HOLDER> . note : there,for ( int retry = __num__ ; retry < max_retry ; retry ++ ) { wrapped context wrapper = get cache ( ) . compute if absent ( job info . job id ( ) @$ job id -> { try { return new wrapped context ( job info @$ creator . apply ( job info ) ) ; } catch ( exception e ) { throw new runtime exception ( __str__ + job info . job id ( ) @$ e ) ; } } ) ; synchronized ( wrapper ) { if ( wrapper . reference count != null ) { wrapper . reference count . increment and get ( ) ; return wrapper ; } } },which get context
calls to functions that have no side effects have the no side effect <PLACE_HOLDER> set .,if ( ! function call has side effects ( n ) ) { break ; },calls have flag
mp 3 live streams commonly have seekable <PLACE_HOLDER> @$ despite being unseekable .,if ( icy headers != null && extractor instanceof mp3 extractor ) { ( ( mp3 extractor ) extractor ) . disable seeking ( ) ; } if ( pending extractor seek ) { extractor . seek ( position @$ seek time us ) ; pending extractor seek = false ; } while ( result == extractor . result_continue && ! load canceled ) { load condition . block ( ) ; result = extractor . read ( input @$ position holder ) ; if ( input . get position ( ) > position + continue loading check interval bytes ) { position = input . get position ( ) ; load condition . close ( ) ; handler . post ( on continue loading requested runnable ),streams have headers
redex access the constructed proguard command line and then goes and opens a <PLACE_HOLDER> of the files listed there .,return immutable list . < source path > builder ( ) . add all ( classpath entries to dex source paths ) . add all ( rich stream . from ( proguard config ) . collect ( collectors . to list ( ) ) ) . add all ( proguard configs ) . build ( ) ;,access goes file
test read 0 <PLACE_HOLDER> .,test array . set position ( __num__ ) ; result [ __num__ ] = __num__ ; test array . read bits ( result @$ __num__ @$ __num__ ) ; assert that ( result [ __num__ ] ) . is equal to ( ( byte ) __num__ ) ;,test read bits
successful connect request <PLACE_HOLDER> in a response with empty body .,if ( status code == __num__ ) { if ( http method . connect . equals ( method ) ) { done = true ; queue . clear ( ) ; return true ; } },successful connect results
killed jobs might not have <PLACE_HOLDER>,if ( total counters != null ) { json object j groups = new json object ( ) ; for ( counter group counter group : total counters ) { string group name = counter group . get name ( ) ; counter group total group = total counters . get group ( group name ) ; counter group map group = map counters . get group ( group name ) ; counter group reduce group = reduce counters . get group ( group name ) ; iterator < counter > ctr itr = total group . iterator ( ) ; json array j group = new json array ( ) ; while ( ctr itr . has next ( ) ) { json object j counter =,jobs have counters
we are tolerant here because frameworks such as avro accept a boxed <PLACE_HOLDER> even though the field is primitive,if ( ! box primitive ( parameter type ) . equals ( box primitive ( field type ) ) ) { return null ; },frameworks accept primitive
this is too long @$ and the ri just returns the input <PLACE_HOLDER> ...,string long input = make puny string ( __num__ ) ; assert equals ( long input @$ idn . to unicode ( long input ) ) ;,ri returns string
simple date format has this <PLACE_HOLDER>,date time formatter test = fmt . with locale ( locale . english ) . with decimal style ( decimal style . standard ) ; format format = test . to format ( ) ; parse position pos = new parse position ( __num__ ) ; format . parse object ( ( string ) null @$ pos ) ;,format has behavior
the healthy path will never see this synchronized <PLACE_HOLDER>,synchronized ( resource holder . bad_filters ) { long last time = resource holder . bad_filters . get ( f ) ; if ( last time == null || last time + time unit . minutes . to millis ( interval ) < system . current time millis ( ) ) { resource holder . bad_filters . put ( f @$ system . current time millis ( ) ) ; return level . warning ; } else { return level . fine ; } },path see block
let basic date click handler handle calendar <PLACE_HOLDER> @$ and update only the other parts of ui here,calendar component . set handler ( new basic date click handler ( ) { @ override public void date click ( date click event event ) { super . date click ( event ) ; switch to day view ( ) ; } } ) ;,handler handle dates
this will actually mark the parent as a dir @$ so that lists of that dir will pick up the <PLACE_HOLDER>,put file ( child @$ now @$ null ) ;,lists pick file
check whether a provider can provide an implementation that 's closer to the requested locale than <PLACE_HOLDER> the java runtime itself can provide .,try { return new simple date format ( time style @$ date style @$ loc ) ; } catch ( missing resource exception e ) { return new simple date format ( __str__ ) ; },java runtime what
some tests only run on linux @$ those wo n't create a <PLACE_HOLDER> on other os,if ( client != null ) client . close ( ) ;,tests create proxy
now new job requests should succeed as list operation has no cancel <PLACE_HOLDER> .,job runnable = concurrent list jobs ( __num__ @$ config @$ false @$ false @$ list job helper . get delayed resonse answer ( __num__ @$ new array list < job item bean > ( ) ) ) ; assert true ( job runnable . exception == null ) ;,operation has call
we bypass the standard equals <PLACE_HOLDER> that resolve the hostname,assert that ( slave . get channel ( ) . call ( new url builder callable ( __str__ ) ) @$ not ( equal to ( slave . get channel ( ) . call ( new url builder callable ( __str__ ) ) ) ) ) ;,standard equals method
expressions propagate linked <PLACE_HOLDER> ; statements do not .,return node util . is statement ( parent ) ? empty : propagate ( false ) ;,expressions propagate lines
for joda time <PLACE_HOLDER>s @$ return sql <PLACE_HOLDER> for java.util.date .,if ( raw type instanceof class && abstract instant . class . is assignable from ( ( class < ? > ) raw type ) ) { return type factory . create java type ( date . class ) ; } else if ( raw type instanceof class && byte string . class . is assignable from ( ( class < ? > ) raw type ) ) { return type factory . create java type ( byte [ ] . class ) ; } return type factory . create java type ( ( class ) raw type ) ;,return sql type
check any changes do not break already encoded <PLACE_HOLDER>,string password = __str__ ; string obfuscate = __str__ ; assert equals ( password @$ password . deobfuscate ( obfuscate ) ) ;,changes break values
at this point all sub<PLACE_HOLDER>s are destroyed and this <PLACE_HOLDER> has been marked as destroyed and post destroy <PLACE_HOLDER> has been called for each <PLACE_HOLDER> . the only detail left is unhooking this <PLACE_HOLDER> from the parent sub<PLACE_HOLDER> map @$ and sending listener events,assert . assert true ( is destroyed ) ;,left unhooking list
put records into the queues using the queue <PLACE_HOLDER> . each worker will pull and process .,try { record record = records . next ( ) ; id distributor . distribute ( record @$ record consumer ) ; progress . add ( __num__ ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; break ; },queues using producer
this <PLACE_HOLDER> to shutdown will eventually make a <PLACE_HOLDER> to population cancelled on the monitor below,dbms . shutdown ( ) ;,call make call
make sure the fragment either has no <PLACE_HOLDER> or dependencies have been dealt with,set < node > node with fragment = connected nodes . stream ( ) . filter ( node -> ! node . get fragments without dependency ( ) . is empty ( ) || ! node . get fragments with dependency visited ( ) . is empty ( ) ) . collect ( collectors . to set ( ) ) ;,fragment has edges
cut off to maximum queue <PLACE_HOLDER> if update <PLACE_HOLDER> is exceeding queue <PLACE_HOLDER> .,new expire time . put ( application timeout type . lifetime @$ updatedlifetime in millis ) ; new timeout iniso8601 format . put ( application timeout type . lifetime @$ times . formatiso8601 ( updatedlifetime in millis . long value ( ) ) ) ;,lifetime exceeding lifetime
if the expanded child has the same <PLACE_HOLDER> as the collapsed one we hide it .,if ( m expanded child != null && m expanded child . get height ( ) != __num__ ) { if ( ( ! m is heads up && ! m heads up animating away ) || m heads up child == null || ! m containing notification . can show heads up ( ) ) { if ( m expanded child . get height ( ) <= m contracted child . get height ( ) ) { expandable = false ; } } else if ( m expanded child . get height ( ) <= m heads up child . get height ( ) ) { expandable = false ; } } if ( m expanded child != null ) { m expanded wrapper . update expandability,child has height
create a job launcher instance depending on the configuration . the same <PLACE_HOLDER> object is used for both system and job configuration <PLACE_HOLDER> because azkaban puts configuration <PLACE_HOLDER> in the .job file and in the .<PLACE_HOLDER> file into the same <PLACE_HOLDER> object .,this . job launcher = this . closer . register ( job launcher factory . new job launcher ( job props @$ job props @$ null @$ metadata tags ) ) ;,azkaban puts files
nor the admin have that <PLACE_HOLDER>,jenkins rule . web client wc = j . create web client ( ) ; wc . login ( __str__ ) ; wc . get options ( ) . set throw exception on failing status code ( false ) ; page page = wc . go to ( __str__ + __num__ + __str__ ) ; assert equals ( __num__ @$ page . get web response ( ) . get status code ( ) ) ; assert request was blocked and reset flag ( ) ;,admin have permission
we 'd like to have 20 buckets per member @$ but what we 'll find is that member 1 will have 15 and 2 and 3 will have <PLACE_HOLDER> and 18 .,for ( partition member info details : model . get partitioned member details ( __str__ ) ) { assert equals ( __num__ @$ details . get bucket count ( ) ) ; },member have 17
server did not perform the <PLACE_HOLDER> @$ so do n't leave an invalid entry here,if ( ! force new entry && event . no version received from server ( ) ) { return false ; },server perform invalidation
rotate the queue so that each filter gets the <PLACE_HOLDER> in a different order,for ( filter filter : filters ) { mutating descriptions . add last ( mutating descriptions . poll first ( ) ) ; for ( description description : descriptions ) { if ( filter . should run ( description ) ) { add description for filter to map ( descriptions run @$ filter @$ description ) ; } } },filter gets comments
server socket will accept <PLACE_HOLDER>,socket base req = zmq . socket ( ctx @$ bind type ) ; assert that ( req @$ not null value ( ) ) ; boolean rc = zmq . bind ( req @$ address ) ; assert that ( rc @$ is ( true ) ) ; socket base router = zmq . socket ( ctx @$ connect type ) ; assert that ( router @$ not null value ( ) ) ; string host = ( string ) zmq . get socket option ext ( req @$ zmq . zmq_last_endpoint ) ; assert that ( host @$ not null value ( ) ) ; rc = zmq . connect ( router @$ host ) ; assert that ( rc @$ is ( true ) ),socket accept connections
tag change from 'unresolved ' to 'string ' does not happen <PLACE_HOLDER> . we just look at the object at the corresponding index and decide based on the oop type .,if ( ctag . is string ( ) ) { symbol sym = cpool . get unresolved string at ( cp index ) ; return __str__ + sym . as string ( ) + __str__ ; } else if ( ctag . is klass ( ) || ctag . is unresolved klass ( ) ) { constant pool . cp slot obj = cpool . get slot at ( cp index ) ; if ( obj . is resolved ( ) ) { klass k = obj . get klass ( ) ; return __str__ + k . get name ( ) . as string ( ) + __str__ + k . get address ( ) + __str__ ; } else if ( obj . is unresolved (,change happen atomically
<PLACE_HOLDER> null means no <PLACE_HOLDER> @$ so the tag is irrelevant since it is used to reset a settings subset their <PLACE_HOLDER>s . also it is irrelevant if the system set the canonical <PLACE_HOLDER> .,if ( default value == null ) { tag = null ; default from system = false ; },system set default
make sure the <PLACE_HOLDER> that we send to the peer does have an accurate region version <PLACE_HOLDER> for our local version,return create copy ( this . my id @$ cloned holders @$ this . local version . get ( ) @$ gc versions @$ this . localgc version . get ( ) @$ false @$ cloned local holder ) ;,holder have list
white rectangle detector returns points <PLACE_HOLDER> of the rectangle . i want points on the edges .,float centerx = ( pointa . getx ( ) + pointb . getx ( ) + pointc . getx ( ) + pointd . getx ( ) ) / __num__ ; float centery = ( pointa . gety ( ) + pointb . gety ( ) + pointc . gety ( ) + pointd . gety ( ) ) / __num__ ; pointa = move away ( pointa @$ centerx @$ centery ) ; pointb = move away ( pointb @$ centerx @$ centery ) ; pointc = move away ( pointc @$ centerx @$ centery ) ; pointd = move away ( pointd @$ centerx @$ centery ) ; result point point bs ; result point point ds ;,returns points outside
check if the statements contains a <PLACE_HOLDER> to check not null or require non null,if ( is expression ) { expression expression = ( expression ) stat ; if ( expression instanceof assignment ) expression = ( ( assignment ) expression ) . expression ; if ( ! ( expression instanceof message send ) ) return null ; message send invocation = ( message send ) expression ; if ( ! arrays . equals ( invocation . selector @$ check_not_null ) && ! arrays . equals ( invocation . selector @$ require_non_null ) ) return null ; if ( invocation . arguments == null || invocation . arguments . length == __num__ ) return null ; expression first argument = invocation . arguments [ __num__ ] ; if ( ! ( first argument instanceof single name reference ) ) return null ;,statements contains call
then we should be able to start a transaction @$ though perhaps not be able to finish it . this is <PLACE_HOLDER> the individual test methods will be doing . the test passes when transaction.close completes within the test timeout @$ that is @$ it did n't deadlock .,start log rotation latch . await ( ) ;,methods doing what
let 's add a <PLACE_HOLDER> of constant rows to test the rle,row . set field value ( __num__ @$ null ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; row . set field value ( __num__ @$ null ) ; for ( int i = __num__ ; i < __num__ ; ++ i ) { writer . add row ( row ) ; } union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; writer . add row ( row ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ) ) ; writer . add row ( row ) ; union . set ( ( byte ) __num__ @$ new int writable ( __num__ ),'s add lot
noop build rule implements supports <PLACE_HOLDER> based rule key @$ which can not add build rule for rule keys .,return false ;,implements supports column
make copy of ground items because we are going to modify them here @$ and the array list supports our desired <PLACE_HOLDER> here,if ( plugin . is hot key pressed ( ) ) { ground item list = new array list < > ( ground item list ) ; final java . awt . point awt mouse pos = new java . awt . point ( mouse pos . getx ( ) @$ mouse pos . gety ( ) ) ; ground item ground item = null ; for ( ground item item : ground item list ) { item . set offset ( offset map . compute ( item . get location ( ) @$ ( k @$ v ) -> v != null ? v + __num__ : __num__ ) ) ; if ( ground item != null ) { continue ; } if ( plugin . get,list supports format
do n't let focus issues hide the popup <PLACE_HOLDER>,panel . set ignore focus ( true ) ; return p ;,issues hide bar
this member has a lower <PLACE_HOLDER> that ca n't fit buckets of <PLACE_HOLDER> 30,partition member info impl details3 = build details ( member3 @$ __num__ @$ __num__ @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } @$ new long [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ) ; model . add region ( __str__ @$ arrays . as list ( details1 @$ details2 @$ details3 ) @$ new fake offline details ( ) @$ true ) ; assert equals ( __num__ @$ do moves ( new composite director ( false @$ false @$ true @$ true ) @$ model ) ) ; assert equals ( collections . empty list ( ) @$ bucket operator . creates ) ;,member has weight
minimum two level stored artifact <PLACE_HOLDER>,try ( in memory artifact cache in memory artifact cache = new in memory artifact cache ( ) ; two level artifact cache decorator two level cache = new two level artifact cache decorator ( in memory artifact cache @$ test project filesystems . create project filesystem ( tmp . get root ( ) ) @$ buck event bus for tests . new instance ( ) @$ true @$ __num__ @$ optional . empty ( ) ) ) { lazy path dummy file = lazy path . of instance ( tmp . new file ( ) ) ; string test metadata key = __str__ ; two level cache . store ( artifact info . builder ( ) . add rule keys ( dummy rule key ) . set,level stored size
treat this as deterministic for reporting purposes : delete statements produce just one <PLACE_HOLDER> that is the number of <PLACE_HOLDER>s affected,boolean order is deterministic = true ; boolean has limit or offset = m_parsed delete . has limit or offset ( ) ;,statements produce row
if this thread created mid key @$ block until the other thread adds a <PLACE_HOLDER> on it .,if ( order == order . before && thread . current thread ( ) . equals ( first thread . get ( ) ) ) { tracking awaiter . instance . await latch and track exceptions ( other thread winning @$ __str__ ) ; } else if ( order == order . after && ! thread . current thread ( ) . equals ( first thread . get ( ) ) ) { other thread winning . count down ( ) ; },thread adds lock
if primitive just use the primitive <PLACE_HOLDER> not the boxed version,if ( type mirror . get kind ( ) . is primitive ( ) ) { class type = type name . get ( type mirror ) ; },primitive use class
another thread already locked this <PLACE_HOLDER> and is processing it . wait for the other thread to finish @$ by locking the existing read lock .,if ( existing lock != null ) { write lock . unlock ( ) ; write lock = null ; read lock = existing lock . read lock ( ) ; read lock . lock ( ) ; if ( m cache . get if present ( alluxio uri . get path ( ) ) != null ) { return false ; } } else { mount table . resolution resolution = m mount table . resolve ( alluxio uri ) ; if ( resolution . get mount id ( ) != mount info . get mount id ( ) ) { return false ; } boolean exists in ufs ; try ( closeable resource < under file system > ufs resource = resolution . acquire ufs resource,thread locked uri
queue level max allocation ca n't exceed the cluster <PLACE_HOLDER>,capacity scheduler cs = new capacity scheduler ( ) ; cs . set conf ( new yarn configuration ( ) ) ; cs . setrm context ( resource manager . getrm context ( ) ) ; capacity scheduler configuration conf = new capacity scheduler configuration ( ) ; setup queue configuration ( conf ) ; set max alloc mb ( conf @$ yarn configuration . default_rm_scheduler_maximum_allocation_mb ) ; set max alloc vcores ( conf @$ yarn configuration . default_rm_scheduler_maximum_allocation_vcores ) ; long larger mem = yarn configuration . default_rm_scheduler_maximum_allocation_mb + __num__ ; long larger vcores = yarn configuration . default_rm_scheduler_maximum_allocation_vcores + __num__ ; cs . init ( conf ) ; cs . start ( ) ; cs . reinitialize ( conf @$ mock context ) ; check queue,allocation exceed config
the output stream must be flushed before the <PLACE_HOLDER> is obtained as the flush can move the <PLACE_HOLDER> forward .,section output stream . flush ( ) ; long length = file channel . position ( ) - sub section offset ; if ( length == __num__ ) { log . warn ( __str__ + __str__ @$ name . to string ( ) ) ; return ; } summary . add sections ( file summary . section . new builder ( ) . set name ( name . name ) . set length ( length ) . set offset ( sub section offset ) ) ; sub section offset += length ;,flush move header
java ca n't have a root whose value is null . instead of setting null @$ the method sets user <PLACE_HOLDER> so that other methods are able to know the root should be nil .,if ( new_root == context . nil ) { get document ( ) . get document element ( ) . set user data ( nokogiri helpers . root_node_invalid @$ boolean . true @$ null ) ; return new_root ; } xml node new root = as xml node ( context @$ new_root ) ; i ruby object root = root ( context ) ; if ( root . is nil ( ) ) { node new root node ; if ( get document ( ) == new root . get owner document ( ) ) { new root node = new root . node ; } else { new root node = get document ( ) . import node ( new root . node @$ true ) ;,method sets data
backslash and double quote need double the <PLACE_HOLDER> for both java and haskell,special char replacements . remove ( __str__ ) ; special char replacements . remove ( __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ; special char replacements . put ( __str__ @$ __str__ ) ;,backslash double handling
the primary zygote did n't match . try the <PLACE_HOLDER> .,if ( m zygote secondary socket address != null ) { attempt connection to secondary zygote ( ) ; if ( secondary zygote state . matches ( abi ) ) { return secondary zygote state ; } },primary try protocol
incorrect for not yet ported 3 rd party storage <PLACE_HOLDER> .,return call . empty list ( ) ;,incorrect ported graph
authorization check for kill query will be in kill query impl as both admin or <PLACE_HOLDER> owner can perform the <PLACE_HOLDER> . which is not directly supported in authorizer,if ( driver context . get query state ( ) . get hive operation ( ) != hive operation . kill_query ) { command authorizer . do authorization ( driver context . get query state ( ) . get hive operation ( ) @$ sem @$ context . get cmd ( ) ) ; } session state . get perf logger ( ) . perf log end ( class_name @$ perf logger . do_authorization ) ;,owner perform policy
dont do actual <PLACE_HOLDER> in check mode .,boolean configcheck = boolean . parse boolean ( config . get property ( export manager . config_check_only @$ __str__ ) ) ; if ( configcheck ) { return ; } m_is krb = boolean . parse boolean ( config . get property ( __str__ @$ __str__ ) ) ; if ( m_is krb ) { m_context = new login context ( __str__ ) ; m_context . login ( ) ; } connect ( ) ; if ( m_is hdfs && endpoint expander . has date conversion ( m_endpoint ) ) { runnable rotator = new runnable ( ) { @ override public void run ( ) { try { roll ( ) ; } catch ( throwable t ) { m_logger . error ( __str__ + throwables .,dont do config
wait for the present complete semaphore to be signaled to ensure that the image wo n't be rendered to until the presentation engine has fully released <PLACE_HOLDER> to the application @$ and it is okay to render to the image .,demo_draw_build_cmd ( ) ; long buffer lp2 = stack . malloc long ( __num__ ) ; vk submit info submit_info = vk submit info . malloc stack ( stack ) . s type ( vk_structure_type_submit_info ) . p next ( null ) . wait semaphore count ( __num__ ) . p wait semaphores ( lp . put ( __num__ @$ image acquired semaphore ) ) . p wait dst stage mask ( ip . put ( __num__ @$ vk_pipeline_stage_bottom_of_pipe_bit ) ) . p command buffers ( pp . put ( __num__ @$ draw_cmd ) ) . p signal semaphores ( lp2 . put ( __num__ @$ draw complete semaphore ) ) ; check ( vk queue submit ( queue @$ submit_info @$ vk_null_handle ) ) ; vk present,engine released access
this flow covers instance not found exception . actual <PLACE_HOLDER> just eating the exception . i.e actual <PLACE_HOLDER> just printing the stacktrace @$ whenever an exception of type instance not found exception occurs .,mbean container . bean removed ( null @$ managed ) ;,exception found code
the entries in the original logs are alternating <PLACE_HOLDER> considering the sequence file header @$ the middle corruption should affect at least half of the entries,int good entries = ( num_writers - __num__ ) * entries ; int first half entries = ( int ) math . ceil ( entries / __num__ ) - __num__ ; int all regions count = split and count ( num_writers @$ - __num__ ) ; assert true ( __str__ @$ regions . size ( ) * ( good entries + first half entries ) <= all regions count ) ;,entries alternating generators
we can assume here that the request contains all the <PLACE_HOLDER> needed for authentication etc .,try { o auth2 access token token = retrieve token ( request @$ resource @$ get parameters for token request ( resource @$ request ) @$ get headers for token request ( request ) ) ; if ( token == null ) { throw new user redirect required exception ( resource . get user authorization uri ( ) @$ request . to single value map ( ) ) ; } return token ; } catch ( user redirect required exception e ) { throw new user redirect required exception ( e . get redirect uri ( ) @$ request . to single value map ( ) ) ; },request contains information
get tunnel interface record ; if no such interface is found @$ will throw illegal argument <PLACE_HOLDER>,tunnel interface record tunnel interface info = user record . m tunnel interface records . get resource or throw ( tunnel resource id ) ; try { m srv config . get netd instance ( ) . interface add address ( tunnel interface info . m interface name @$ local addr . get address ( ) . get host address ( ) @$ local addr . get prefix length ( ) ) ; } catch ( remote exception e ) { throw e . rethrow from system server ( ) ; },record throw exception
this drop target does n't accept any <PLACE_HOLDER> .,return false ;,target accept tasks
fast source : source produce 8 before @$ and 8 after invocation . buffered source should return all <PLACE_HOLDER> at once .,mock source = new mock split source ( ) . set batch size ( __num__ ) ; try ( split source source = new buffering split source ( mock source @$ __num__ ) ) { mock source . increase available splits ( __num__ ) ; listenable future < next batch result > next batch future = get next batch ( source @$ __num__ ) ; assert false ( next batch future . is done ( ) ) ; mock source . increase available splits ( __num__ ) ; require future value ( next batch future ) . assert size ( __num__ ) . assert no more splits ( false ) ; },source return batches
check that cluster status reports the correct <PLACE_HOLDER> and backup masters,assert not null ( active ) ; cluster metrics status = active . get cluster metrics ( ) ; assert true ( status . get master name ( ) . equals ( active name ) ) ; assert equals ( __num__ @$ status . get backup master names ( ) . size ( ) ) ;,status reports active
although the streaming ops support multiple <PLACE_HOLDER> @$ we only query one key here,byte array key ; try { if ( key format . equals ( admin parser utils . opt_json ) ) { object key object ; string key serializer name = key serializer def . get name ( ) ; if ( is avro schema ( key serializer name ) ) { schema key schema = schema . parse ( key serializer def . get current schema info ( ) ) ; json decoder decoder = new json decoder ( key schema @$ key string ) ; generic datum reader < object > datum reader = new generic datum reader < object > ( key schema ) ; key object = datum reader . read ( null @$ decoder ) ; } else if ( key serializer name .,ops support keys
sorani kurdish <PLACE_HOLDER>,if ( __str__ . equals ignore case ( language ) ) { return new sorani stem filter ( token stream ) ; } else if ( __str__ . equals ignore case ( language ) ) { return new snowball filter ( token stream @$ new swedish stemmer ( ) ) ; } else if ( __str__ . equals ignore case ( language ) || __str__ . equals ignore case ( language ) ) { return new swedish light stem filter ( token stream ) ; } else if ( __str__ . equals ignore case ( language ) ) { return new snowball filter ( token stream @$ new turkish stemmer ( ) ) ; },sorani kurdish maker
verify that the configuration now has three <PLACE_HOLDER> .,configuration = s3 client . get bucket lifecycle configuration ( bucket name ) ; system . out . println ( __str__ + configuration . get rules ( ) . size ( ) ) ;,configuration has rules
validate router failure <PLACE_HOLDER> matches nn failure <PLACE_HOLDER> .,method m = client protocol . class . get method ( __str__ @$ string . class @$ string . class @$ enum set writable . class ) ; string bad path = __str__ ; enum set writable < create flag > create flag writable = new enum set writable < create flag > ( create flag ) ; compare responses ( router protocol @$ nn protocol @$ m @$ new object [ ] { bad path @$ __str__ @$ create flag writable } ) ;,response matches response
allocation of array may have caused gc @$ which may have caused additional <PLACE_HOLDER> to go stale . removing these <PLACE_HOLDER> from the reference queue will make them eligible for reclamation .,while ( queue . poll ( ) != null ) { },which caused entries
does the handler type map have a <PLACE_HOLDER> and address .,if ( handler type count == __num__ || handler type map address == null || ( is relative ( ) && image base address . equals ( handler type map address ) ) ) { throw new invalid data type exception ( get name ( ) + __str__ ) ; },map have image
next task takes less than 10 <PLACE_HOLDER> and should be only aggregated,try ( silent closeable c2 = profiler . profile ( profiler task . action_check @$ __str__ ) ) { profiler . log simple task ( blaze clock . instance ( ) . nano time ( ) @$ profiler task . vfs_stat @$ __str__ ) ; long start time = blaze clock . instance ( ) . nano time ( ) ; clock . advance millis ( __num__ ) ; profiler . log simple task ( start time @$ profiler task . vfs_stat @$ __str__ ) ; },task takes ms
agg spout <PLACE_HOLDER>,if ( is spout ) { map mm = new hash map ( ) ; map acked = client stats util . get map by key ( stats @$ acked ) ; for ( object win : acked . key set ( ) ) { mm . put ( win @$ agg spout lat and count ( ( map ) comp lat stats . get ( win ) @$ ( map ) acked . get ( win ) ) ) ; } mm = swap map order ( mm ) ; w2comp lat wgt avg = client stats util . get map by key ( mm @$ comp_lat_total ) ; w2acked = client stats util . get map by key ( mm @$ acked ) ; } else,agg spout stats
disable opening assist <PLACE_HOLDER> during setup,if ( ! is user setup complete ( ) ) { return ; },opening assist plugins
if the value is 1 byte and the byte represents null @$ <PLACE_HOLDER> to create the entry . this test needs to be moved to data serializer or data serializer.null needs to be publicly accessible .,boolean result = false ; if ( value == null ) { result = region . basic bridge create ( key @$ null @$ true @$ callback arg @$ server connection . get proxyid ( ) @$ true @$ new eventid holder ( event id ) @$ false ) ; } else { result = region . basic bridge put ( key @$ value @$ null @$ is object @$ callback arg @$ server connection . get proxyid ( ) @$ true @$ new eventid holder ( event id ) ) ; },byte represents attempt
in the worst possible case all the unique values come in consecutive order & hence only 5 iterations will yield the <PLACE_HOLDER>,query observer old = query observer holder . set instance ( new query observer adapter ( ) { @ override public void after iteration evaluation ( object result ) { num [ __num__ ] += __num__ ; } @ override public void before iteration evaluation ( compiled value ritr @$ object curr object ) { if ( data . contains ( curr object ) ) { num repeat [ __num__ ] += __num__ ; } else { data . add ( curr object ) ; } } } ) ; string query string = __str__ ; query = qs . new query ( query string ) ; result = ( select results ) query . execute ( ) ; assert equals ( ( __num__ + num repeat [,values yield result
photos and albums have i <PLACE_HOLDER> starting from 1,get album summary ( resp writer @$ ( long ) new random ( ) . next int ( __num__ ) + __num__ ) ; purge all photos ( resp writer ) ; try { latch . await ( ) ; } catch ( interrupted exception e ) { resp writer . println ( e . get message ( ) ) ; },photos i ds
now that we know its not a primitive @$ lets just allow the passed <PLACE_HOLDER> to handle the request .,return class . for name ( class name @$ false @$ cl ) ;,lets allow class
parse root object <PLACE_HOLDER>,document mapper . builder doc builder = new document mapper . builder ( ( root object mapper . builder ) root object type parser . parse ( type @$ mapping @$ parser context ) @$ mapper service ) ; iterator < map . entry < string @$ object > > iterator = mapping . entry set ( ) . iterator ( ) ;,root object type
generate entities that their string a field has a <PLACE_HOLDER> over the length limitation,if ( current criteria . get inta ( ) == __num__ ) { for ( int i = __num__ ; i < __num__ ; i ++ ) { validation demo . union field with inline record union = new validation demo . union field with inline record ( ) ; union . set my enum ( my enum . foofoo ) ; validation demos . add ( new validation demo ( ) . set stringa ( __str__ ) . set inta ( current criteria . get inta ( ) ) . set stringb ( __str__ ) . set union field with inline record ( union ) ) ; } } else if ( current criteria . get inta ( ) == __num__ ) { for ( int i,string has value
put i ps in h in such a way that we believe the mostfront have more <PLACE_HOLDER> to get connected,h . add ( domains . chop zoneid ( ipx ) ) ;,mostfront have time
this version of hadoop does not support ec <PLACE_HOLDER> for mr,return ( field != null ) ;,version support cols
releasing composite should release the remaining <PLACE_HOLDER>,new composite . release ( ) ; assert equals ( __num__ @$ new composite . ref cnt ( ) ) ; assert equals ( __num__ @$ s1 . ref cnt ( ) ) ; assert equals ( __num__ @$ s2 . ref cnt ( ) ) ; assert equals ( __num__ @$ s3 . ref cnt ( ) ) ; assert equals ( __num__ @$ b1 . ref cnt ( ) ) ;,composite release components
given that we do not delete @$ an empty slot means no <PLACE_HOLDER> .,if ( value ref == __num__ ) { return - __num__ ; },slot means match
optionally leave the <PLACE_HOLDER> truncated between early truncates @$ but always restore the rows towards the end of the iterations .,if ( leave truncated -- <= __num__ ) { volt queuesql ( renewbase0 ) ; volt executesql ( ) ; volt queuesql ( captureview1 ) ; volt queuesql ( captureview2 ) ; after views = volt executesql ( ) ; validate same ( before views [ __num__ ] @$ after views [ __num__ ] ) ; validate same ( before views [ __num__ ] @$ after views [ __num__ ] ) ; } else { volt queuesql ( captureview1 ) ; volt queuesql ( captureview2 ) ; after views = volt executesql ( ) ; validate purged ( after views ) ; },optionally leave view
let 's use whatever is currently thrown exception ... may change <PLACE_HOLDER>,verify exception ( e @$ __str__ ) ;,use change something
create a new x path result <PLACE_HOLDER> reuse result <PLACE_HOLDER> passed in ? the constructor will check the compatibility of type and xobj and throw an exception if they are not compatible .,return new x path result impl ( type @$ xobj @$ context node @$ m_xpath ) ;,result object object
if both match @$ take <PLACE_HOLDER>,if ( got positive && got negative ) { if ( positive suffix . length ( ) > negative suffix . length ( ) ) { got negative = false ; } else if ( positive suffix . length ( ) < negative suffix . length ( ) ) { got positive = false ; } },both match precedence
avoid having to make sure that the array has <PLACE_HOLDER> below .,if ( stop times . length == __num__ ) return collections . empty list ( ) ;,array has elements
move all the if node 's following <PLACE_HOLDER> .,move all following ( if node @$ if node . get parent ( ) @$ new dest block ) ; report change to enclosing scope ( if node ) ;,node following nodes
problem when retrieving a simple role : either role not found or not readable @$ so raises a role not found <PLACE_HOLDER> .,if ( ! ( multi role flg ) ) { try { relation service . throw role problem exception ( pb type @$ role name ) ; return null ; } catch ( invalid role value exception exc ) { throw new runtime exception ( exc . get message ( ) ) ; } } else { result = new role unresolved ( role name @$ null @$ pb type ) ; },role found exception
the request should n't affect the key event <PLACE_HOLDER> .,new thread ( new runnable ( ) { public void run ( ) { try { thread . sleep ( __num__ ) ; } catch ( exception ex ) { } system . out . println ( __str__ + t2 ) ; t2 . request focus ( ) ; } } ) . start ( ) ; f . set visible ( true ) ; util . wait for idle ( robot ) ; test ( ) ; if ( passed ) system . out . println ( __str__ ) ;,request affect listener
actual data does n't matter . just send more than 4 k <PLACE_HOLDER>,byte b = __str__ ; for ( int i = __num__ ; i < ( __num__ + __num__ ) ; i ++ ) os . write ( b ) ; os . close ( ) ; t . close ( ) ;,data send worth
one last sync whose transactions are not expected to be seen in the input streams because the journal nodes have not updated their <PLACE_HOLDER> of the committed transaction id yet,write txns ( stm @$ __num__ @$ __num__ ) ; future throws ( new io exception ( ) ) . when ( spies . get ( __num__ ) ) . get journaled edits ( __num__ @$ quorum journal manager . qjm_rpc_max_txns_default ) ; future throws ( new io exception ( ) ) . when ( spies . get ( __num__ ) ) . get journaled edits ( __num__ @$ quorum journal manager . qjm_rpc_max_txns_default ) ; list < edit log input stream > streams = new array list < > ( ) ; qjm . select input streams ( streams @$ __num__ @$ true @$ true ) ;,nodes updated stats
find which one supplies type <PLACE_HOLDER> and return it,type variable < ? > tv = base class . get type parameters ( ) [ __num__ ] ; while ( ! superclasses . is empty ( ) ) { current type = superclasses . pop ( ) ; if ( current type instanceof parameterized type ) { parameterized type pt = ( parameterized type ) current type ; class < ? > raw type = ( class < ? > ) pt . get raw type ( ) ; int arg index = arrays . as list ( raw type . get type parameters ( ) ) . index of ( tv ) ; if ( arg index > - __num__ ) { type type arg = pt . get actual type arguments ( ) [ arg,one supplies parameters
important ? if source has native <PLACE_HOLDER> @$ need to store,if ( ! _has native type ids ) { _has native type ids = other . can write type id ( ) ; } if ( ! _has native object ids ) { _has native object ids = other . can write object id ( ) ; } _may have native ids = _has native type ids | _has native object ids ; json parser p = other . as parser ( ) ; while ( p . next token ( ) != null ) { copy current structure ( p ) ; } return this ;,source has ids
interceptor changes return <PLACE_HOLDER> .,interceptor . ret interceptor = new before remove interceptor ( new ignite bi tuple ( false @$ __num__ ) ) ;,changes return type
events can start coming in the moment the input initializer is created . the pruner must be setup and initialized here so that it sets up it 's structures to start accepting events . setting it up in initialize leads to a window where events may come in before the pruner is initialized @$ which may cause it to drop events . no dynamic,pruner = null ;,events start values
verify that the comparison gives the correct <PLACE_HOLDER> for all values in both directions .,for ( int i = __num__ ; i < test_keys . length ; ++ i ) { for ( int j = __num__ ; j < test_keys . length ; ++ j ) { byte key left = test_keys [ i ] ; byte key right = test_keys [ j ] ; int cmp = left . compare to ( right ) ; if ( i < j && ! ( cmp < __num__ ) ) { fail ( string . format ( __str__ @$ left @$ right @$ cmp @$ i @$ j ) ) ; } else if ( i == j && ! ( cmp == __num__ ) ) { fail ( string . format ( __str__ @$ left @$ right @$ cmp @$ i,comparison gives result
registers a callback to be invoked whenever a user changes a <PLACE_HOLDER> .,get preference screen ( ) . get shared preferences ( ) . register on shared preference change listener ( this ) ;,user changes preference
when the target process is reset @$ the breakpoint deleted message will never arrive . it is therefore necessary to delete the deleting breakpoints manually . see <PLACE_HOLDER> 2109 for an example of what can happen .,if ( manager . get breakpoint status ( address @$ breakpoint type . regular ) == breakpoint status . breakpoint_deleting ) { addresses to remove . add ( address ) ; } else if ( manager . get breakpoint status ( address @$ breakpoint type . regular ) != breakpoint status . breakpoint_disabled ) { addresses to disable . add ( address ) ; },message see bug
mark set possible <PLACE_HOLDER> to true for all events in this bucket before it becomes primary on the node,while ( itr . has next ( ) ) { object key = itr . next ( ) ; object sender event = get nolru ( key @$ true @$ false @$ false ) ; if ( sender event != null ) { ( ( gateway sender event impl ) sender event ) . set possible duplicate ( true ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( __str__ @$ sender event ) ; } } i ++ ; },mark set duplicate
validate the xml file and return the <PLACE_HOLDER>,boolean status = validator . validate ( ) ; if ( ! status ) { log . log error ( validator . get error message ( ) ) ; result . set result ( false ) ; result . set nr errors ( validator . get nr errors ( ) ) ; result . set log text ( validator . get error message ( ) ) ; },xml file errors
do n't add the original policy if it was an expansion flag @$ which have no <PLACE_HOLDER> @$ but do add it if there was either no expansion or if it was a <PLACE_HOLDER>d flag with implicit requirements .,if ( ! is expansion ) { expanded policies . add ( original policy ) ; } return expanded policies ;,which have modifier
get twitter created a flow <PLACE_HOLDER> @$ then it 's sent via s 2 s,tc . add lineage ( create lineage ( prs @$ __num__ @$ __num__ @$ __num__ ) ) ; test ( tc ) ; wait notifications get delivered ( ) ; final lineage lineage = get lineage ( ) ; final node flow = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patha = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathb = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathc = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node patht = lineage . find node ( __str__ @$ __str__ @$ __str__ ) ; final node pathi = lineage . find node (,twitter created file
assuming the default configuration has the correct <PLACE_HOLDER> set . users can specify a particular factory by providing a configuration .,if ( conf == null ) { conf = default conf ; },configuration has default
aet should override any <PLACE_HOLDER> set unless it is zero,message . set time to live ( __num__ ) ; message . set text ( __str__ ) ; sender . send ( message ) ; sender . close ( ) ; assert equals ( __num__ @$ queue view . get queue size ( ) ) ; thread . sleep ( __num__ ) ;,aet override timer
now we want to save the normalizer to a binary file . for doing this @$ one can use the normalizer <PLACE_HOLDER> .,normalizer serializer serializer = normalizer serializer . get default ( ) ;,one use string
turn off persistent ipc @$ so that the dfs client can survive nn <PLACE_HOLDER>,conf . set int ( common configuration keys public . ipc_client_connection_maxidletime_key @$ __num__ ) ; minidfs cluster cluster = null ; fs data output stream stream ; try { cluster = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) . build ( ) ; file system fs = cluster . get file system ( ) ; dfs util client . getnn address ( conf ) . get port ( ) ; stream = fs . create ( file_path @$ true @$ block_size @$ ( short ) __num__ @$ block_size ) ; stream . write ( data_before_restart ) ; stream . write ( ( byte ) __num__ ) ; stream . hflush ( ) ; cluster . restart name node ( ),client survive restart
let the hash builder operators reduce their accounted <PLACE_HOLDER>,partitions no longer needed . set ( null ) ; lock . write lock ( ) . lock ( ) ; try { arrays . fill ( partitions @$ null ) ; lookup source supplier = null ; close cached lookup sources ( ) ; } finally { lock . write lock ( ) . unlock ( ) ; },operators reduce bytes
delete items that do have a keychain item <PLACE_HOLDER> .,for ( enumeration e = deleted entries . keys ( ) ; e . has more elements ( ) ; ) { string alias = ( string ) e . next element ( ) ; object entry = deleted entries . get ( alias ) ; if ( entry instanceof trusted cert entry ) { if ( ( ( trusted cert entry ) entry ) . cert ref != __num__ ) { _remove item from keychain ( ( ( trusted cert entry ) entry ) . cert ref ) ; _release keychain item ref ( ( ( trusted cert entry ) entry ) . cert ref ) ; } } else { certificate cert elem ; key entry key entry = ( key entry ) entry ; if,items have reference
if a build file has been added or removed @$ reconstruct the build file <PLACE_HOLDER> .,build file trees . invalidate ( cell ) ;,the build tree
if a topic has k partitions @$ and in the previous run @$ each partition recorded its avg <PLACE_HOLDER> to pull a record @$ then use the geometric mean of these k numbers as the estimated avg <PLACE_HOLDER> to pull a record in this run .,double est avg millis for topic = geometric mean ( prev avg millis for partitions ) ; this . est avg millis . put ( topic @$ est avg millis for topic ) ; log . info ( string . format ( __str__ @$ topic @$ est avg millis for topic ) ) ; all est avg millis . add ( est avg millis for topic ) ;,partition recorded millis
just pressing the button does not change the <PLACE_HOLDER> ...,set button pressed ( true ) ; assert equals ( empty border button . get border ( ) @$ empty border button . no_button_border ) ;,button change color
does the cell contain special <PLACE_HOLDER> which indicates that the replicated cell visiblilty <PLACE_HOLDER>s have been modified,if ( is system or super user ( ) ) { tag modified tag = null ; iterator < tag > tags iterator = private cell util . tags iterator ( cell ) ; while ( tags iterator . has next ( ) ) { tag tag = tags iterator . next ( ) ; if ( tag . get type ( ) == tag type . string_vis_tag_type ) { modified tag = tag ; break ; } } pair . set first ( true ) ; pair . set second ( modified tag ) ; return pair ; },cell contain tag
since the system will disconnect and attempt to reconnect a new system the old reference to dtc.system can cause <PLACE_HOLDER> @$ so we first null it out .,serializable callable fd = new serializable callable ( __str__ ) { @ override public object call ( ) throws exception { null system ( ) ; final locator old locator = locator . get locator ( ) ; final distributed system msys = cache . get distributed system ( ) ; membership manager helper . crash distributed system ( msys ) ; if ( old locator != null ) { wait criterion wc = new wait criterion ( ) { @ override public boolean done ( ) { return msys . is reconnecting ( ) || msys . get reconnected system ( ) != null ; } @ override public string description ( ) { return __str__ + old locator ; } } ; geode awaitility . await,reference cause deadlock
now flush messages and message pull <PLACE_HOLDER> .,for ( command msg : message cache . values ( ) ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ + ( msg . is message ( ) ? ( ( message ) msg ) . get message id ( ) : msg ) ) ; } transport . oneway ( msg ) ; },messages pull listeners
although we are in doze and would normally allow the device to suspend @$ the doze service has explicitly requested the <PLACE_HOLDER> to remain in the on state which means we should hold the <PLACE_HOLDER> suspend blocker .,if ( m display power request . policy == display power request . policy_doze && m display power request . doze screen state == display . state_on ) { return true ; },service requested screen
framework classes should be loaded from smalivm 's generated framework jar . this is because the object instantiator will expect an empty default <PLACE_HOLDER> and one is added whenever classes are built .,if ( class manager . get framework class names ( ) . contains ( internal name ) ) { class < ? > klazz = cached classes . get ( name ) ; if ( klazz != null ) { return klazz ; } klazz = find class ( name ) ; cached classes . put ( name @$ klazz ) ; return klazz ; } return super . load class ( name @$ resolve ) ;,instantiator expect method
interceptor disables remove and changes return <PLACE_HOLDER> .,interceptor . ret interceptor = new before remove interceptor ( new ignite bi tuple ( true @$ __num__ ) ) ;,disables remove value
make sure decompiler receives <PLACE_HOLDER>,native out . flush ( ) ;,decompiler receives response
open ssl only supports pkcs 5 <PLACE_HOLDER> .,native crypto . evp_cipher_ctx_set_padding ( cipher ctx . get context ( ) @$ padding == padding . pkcs5padding ) ; mode block size = native crypto . evp_cipher_ctx_block_size ( cipher ctx . get context ( ) ) ; called update = false ;,ssl supports padding
the message indicated some error trying to start : do call handle stop <PLACE_HOLDER> .,handle stop keepalive ( nai @$ slot @$ reason ) ;,call handle wal
we take the max of the default and whatever the user put in here . each node 's <PLACE_HOLDER> can be the sum of several operations @$ so the simplest thing to do is get the max . the situation we want to avoid is that the user sets low <PLACE_HOLDER> on one node @$ and when that node is combined with a bunch,if ( on heap == null ) { on heap = on heap default ; } else { on heap = math . max ( on heap . double value ( ) @$ on heap default . double value ( ) ) ; } if ( off heap == null ) { off heap = off heap default ; } else { off heap = math . max ( off heap . double value ( ) @$ off heap default . double value ( ) ) ; } if ( cpu load == null ) { cpu load = cpu load default ; } else { cpu load = math . max ( cpu load . double value ( ) @$ cpu load default . double value (,user sets weight
this security realm does n't support <PLACE_HOLDER> registration . just report an error,req . get view ( this @$ __str__ ) . forward ( req @$ rsp ) ;,realm support privileges
parquet has <PLACE_HOLDER> logging at info level,parquet logger = logger . get logger ( __str__ ) ; parquet logger . set level ( level . warning ) ;,parquet has none
compilation mutates the <PLACE_HOLDER> . this is unrelated to testing recoverable js ast .,return new recoverable js ast ( ast @$ true ) ;,compilation mutates js
we need to make sure the section goes all the <PLACE_HOLDER> to the shelf,if ( section == last section ) { min bottom position = ( int ) ( view state . get final translationy ( m shelf ) + m shelf . get intrinsic height ( ) ) ; },section goes way
if this method did not throw remote <PLACE_HOLDER> as required @$ generate the error but continue @$ so that multiple such errors can be reported .,if ( ! has remote exception ) { env . error ( __str__ @$ intf . qualified name ( ) @$ method . name ( ) + method . signature ( ) ) ; errors = true ; continue next method ; },method throw exceptions
because entries collection is empty @$ remove and decrement <PLACE_HOLDER>,if ( entries . is empty ( ) ) { synchronized ( entries ) { if ( entries . is empty ( ) ) { if ( value to entries map . remove ( new key @$ entries ) ) { num index keys . decrement and get ( ) ; internal index stats . inc num keys ( - __num__ ) ; } } } },collection empty count
this should never happen but the api has marked <PLACE_HOLDER> as nullable,iterable < field > fields new = iterables . transform ( fields @$ new function < field @$ field > ( ) { @ override public schema . field apply ( field input ) { if ( null == input ) { return null ; } field field = new field ( input . name ( ) @$ input . schema ( ) @$ input . doc ( ) @$ input . default value ( ) @$ input . order ( ) ) ; return field ; } } ) ;,api marked fields
set the mode to streaming first so relay will inspect the <PLACE_HOLDER>,cp . set consumption mode ( dbus client mode . online_consumption ) ;,relay inspect result
if slider has number <PLACE_HOLDER>,if ( number indicator != null ) { number indicator . indicator . x = x ; number indicator . indicator . finaly = utils . get relative top ( this ) - get height ( ) / __num__ ; number indicator . indicator . final size = get height ( ) / __num__ ; number indicator . number indicator . set text ( __str__ ) ; },slider has indicator
return null since neither dm nor dls are shutting down can not call <PLACE_HOLDER> in progress because it 's abstract,return null ;,null call anything
vp 9 introduced profiles around 2016 @$ so some vp 9 codecs may not advertise <PLACE_HOLDER> supported profiles . determine the level for them using the info they provide .,if ( prof levs . length == __num__ && m mime . equals ignore case ( media format . mimetype_video_vp9 ) ) { codec profile level prof lev = new codec profile level ( ) ; prof lev . profile = codec profile level . vp9 profile0 ; prof lev . level = video capabilities . equivalentvp9 level ( info ) ; prof levs = new codec profile level [ ] { prof lev } ; } profile levels = prof levs ; if ( m mime . to lower case ( ) . starts with ( __str__ ) ) { m audio caps = audio capabilities . create ( info @$ this ) ; m audio caps . get default format ( m default format ) ;,codecs advertise video
cdh uses different <PLACE_HOLDER> for parquet,if ( __str__ . equals ( input format name ) || __str__ . equals ( input format name ) ) { return mapred parquet input format . class ; } class < ? > clazz = conf . get class by name ( input format name ) ; return ( class < ? extends input format < ? @$ ? > > ) clazz . as subclass ( input format . class ) ;,cdh uses names
do a get file <PLACE_HOLDER> with empty dir flag,s3a file status status = get status with empty dir flag ( fs @$ base ) ; assert non empty dir ( status ) ; if ( isddb ) { list metric . assert diff equals ( __str__ @$ __num__ ) ; get metric . assert diff equals ( __str__ @$ __num__ ) ; log . info ( __str__ ) ; },a get status
audio track.release can take some <PLACE_HOLDER> @$ so we call it on a background thread .,final audio track to release = audio track ; audio track = null ; if ( pending configuration != null ) { configuration = pending configuration ; pending configuration = null ; } audio track position tracker . reset ( ) ; releasing condition variable . close ( ) ; new thread ( ) { @ override public void run ( ) { try { to release . flush ( ) ; to release . release ( ) ; } finally { releasing condition variable . open ( ) ; } } } . start ( ) ;,track.release take time
complete task c @$ which will start task <PLACE_HOLDER>,cmmn runtime service . trigger plan item instance ( get plan item instance id by name and state ( plan item instances @$ __str__ @$ active ) ) ; plan item instances = get plan item instances ( case instance . get id ( ) ) ; assert plan item instance state ( plan item instances @$ __str__ @$ active ) ; assert no plan item instance ( plan item instances @$ __str__ ) ;,which start d
now new job requests should succeed as status operation has no cancel <PLACE_HOLDER> .,job runnable = concurrent jobs status ( __num__ @$ config @$ false @$ false @$ status job helper . get delayed resonse answer ( __num__ @$ status bean ) ) ; assert true ( job runnable . exception == null ) ;,operation has call
sets max <PLACE_HOLDER> to 1 so cache metrics have correct <PLACE_HOLDER> .,try ( ignite data streamer < integer @$ long > ldr = g . data streamer ( default_cache_name ) ) { ldr . per node parallel operations ( __num__ ) ; ldr . receiver ( new incrementing updater ( ) ) ; for ( int i = __num__ ; i < cnt ; i ++ ) ldr . add data ( i % ( cnt / __num__ ) @$ __num__ ) ; },sets max values
initializes the amazon pinpoint <PLACE_HOLDER> .,amazon pinpoint pinpoint client = amazon pinpoint client builder . standard ( ) . with region ( regions . us_east_1 ) . build ( ) ; system . out . format ( __str__ @$ endpoints file name @$ application id ) ; try { create import job result import result = pinpoint client . create import job ( create import job request ) ; string job id = import result . get import job response ( ) . get id ( ) ; get import job result get import job result = null ; string job status = null ; do { get import job result = pinpoint client . get import job ( new get import job request ( ) . with job id ( job id,amazon pinpoint client
junit do n't provide any <PLACE_HOLDER> to order tests,pre auth ( ) ; missing auth ( ) ; valid auth ( ) ; valid auth2 ( ) ;,junit provide credentials
fetch the certificates via ldap . ldap cert store has its own caching <PLACE_HOLDER> @$ see the class description for more info . safe cast since xsel is an x 509 certificate selector .,return ( collection < x509 certificate > ) ldap cert store . get certificates ( xsel ) ;,store has mechanism
if new available range contains the <PLACE_HOLDER> @$ try again,if ( range . with length ( first row index @$ number of rows ) . contains ( row index ) ) { registration . get ( ) . remove ( ) ; wait until visible ( row index @$ destination @$ when visible ) ; },range contains row
there is possibility that we 'll replay txns for a node which was created and then deleted in the fuzzy range @$ and it 's not exist in the snapshot @$ so replay the creation might revert the <PLACE_HOLDER> and pzxid @$ need to check and only update when it 's larger .,if ( parentc version > parent . stat . get cversion ( ) ) { parent . stat . set cversion ( parentc version ) ; parent . stat . set pzxid ( zxid ) ; } data node child = new data node ( data @$ longval @$ stat ) ; parent . add child ( child name ) ; nodes . post change ( parent name @$ parent ) ; node data size . add and get ( get node size ( path @$ child . data ) ) ; nodes . put ( path @$ child ) ; ephemeral type ephemeral type = ephemeral type . get ( ephemeral owner ) ; if ( ephemeral type == ephemeral type . container ) { containers .,replay revert cversion
make an empty path match <PLACE_HOLDER> .,if ( path == null ) { path = __str__ ; },path match success
test new configuration should clear other <PLACE_HOLDER>,conf = new configuration ( ) ; conf . set ( default_key_acl_prefix + __str__ @$ __str__ ) ; acls . set keyac ls ( conf ) ; assert default key acl ( acls @$ key op type . decrypt_eek @$ __str__ ) ; assert . assert true ( acls . key acls . is empty ( ) ) ; assert . assert true ( acls . whitelist key acls . is empty ( ) ) ; assert . assert equals ( __str__ + acls . default key acls @$ __num__ @$ acls . default key acls . size ( ) ) ;,configuration clear acls
can specify orientation if app does n't fill <PLACE_HOLDER> .,assert equals ( screen_orientation_landscape @$ m token . get orientation ( ) ) ; m token . set fills parent ( true ) ; m token . set hidden ( true ) ; m token . sending to bottom = true ;,app fill parent
list contains an unmanaged <PLACE_HOLDER>,list . add ( unmanaged ) ;,list contains object
as we do n't have a bind we need a unique name . let 's use the <PLACE_HOLDER> as the generated invoker,identifier = generated class name ; context . add declaration ( generated class name @$ object . class ) ;,name use name
nn should not bind the wildcard <PLACE_HOLDER> by default .,j cluster = new mini journal cluster . builder ( conf ) . format ( true ) . num journal nodes ( num_jn ) . build ( ) ; jn = j cluster . get journal node ( __num__ ) ; string address = get rpc server address ( jn ) ; assert that ( __str__ @$ address @$ not ( __str__ + wildcard_address ) ) ; log . info ( __str__ + dfs_journalnode_rpc_bind_host_key ) ;,nn bind address
set last saved <PLACE_HOLDER> or string by default :,if ( saved type != - __num__ && saved type < type combo box . get item count ( ) ) { type combo box . set selected index ( saved type ) ; } else { type combo box . set selected item ( new supported column type wrapper ( string . class ) ) ; },set saved type
activity scopes often include transient <PLACE_HOLDER> like task id . make sure bundler service runner is n't stymied by that .,new process ( __str__ ) ; activity = new faux activity ( ) ; activity . create ( bundle ) ; assert that ( activity . root bundler . last loaded ) . is not null ( ) ; assert that ( activity . child bundler . last loaded ) . is not null ( ) ;,scopes include state
remain only first item as exo player does n't support adaptive <PLACE_HOLDER> for url list,return list . sub list ( __num__ @$ __num__ ) ;,player support action
add all nodes from the follow pos of the start node to the follow pos set of the end node @$ which will have the <PLACE_HOLDER> of letting matches transition from a match state at end node to the second char of a match starting with start node .,end node . f follow pos . add all ( start node . f follow pos ) ;,which have affect
special case : java.lang.annotation.target must not have repeated <PLACE_HOLDER> in its value member,if ( a . annotation type . type . tsym != syms . annotation target type . tsym || a . args . tail == null ) return is valid ; if ( ! a . args . head . has tag ( assign ) ) return false ;,java.lang.annotation.target have names
control case : admin can use default <PLACE_HOLDER> .,j . submit ( wc . with basic api token ( admin ) . get page ( p @$ __str__ ) . get form by name ( __str__ ) ) ; j . wait until no activity ( ) ; free style build b1 = p . get last build ( ) ; assert equals ( __num__ @$ b1 . get number ( ) ) ; j . assert log contains ( __str__ @$ j . assert build status success ( b1 ) ) ;,admin use value
foo loads bar own secondary <PLACE_HOLDER> .,list < string > bar secondaries = m bar user0 . get secondary dex paths ( ) ; notify dex load ( m foo user0 @$ bar secondaries @$ m user0 ) ;,loads bar files
lower android versions have a reference <PLACE_HOLDER> with 1024 entries only,for ( int i = __num__ ; i < __num__ ; i ++ ) { assert true ( jni test . create and delete int array ( ) ) ; system . out . print ( i ) ; },versions have cache
do this so that the editor is referencing the current renderer from the tree . the renderer can potentially change each time laf <PLACE_HOLDER> .,set default editor ( tree table model . class @$ new tree table cell editor ( ) ) ;,renderer change display
if the appliation did n't set the status code @$ set the default <PLACE_HOLDER> .,if ( con . get status ( ) == __num__ ) { con . set status ( response message . is fault ( ) ? httpurl connection . http_internal_error : httpurl connection . http_ok ) ; },appliation set one
this function calls <PLACE_HOLDER> recursively,iso run p last iso run = bd . iso runs [ bd . iso run last ] ; opening q opening ; int k @$ opening position @$ closing position ; for ( k = opening index + __num__ ; k < p last iso run . limit ; k ++ ) { q opening = bd . openings [ k ] ; if ( q opening . match >= __num__ ) continue ; if ( new prop position < q opening . context pos ) break ; if ( new prop position >= q opening . position ) continue ; if ( new prop == q opening . context dir ) break ; opening position = q opening . position ; dir props [ opening position,function calls itself
there are 12 issues in total @$ with 10 issues per page @$ the page 2 should only contain 2 <PLACE_HOLDER>,search response result = under test . search ( query . build ( ) @$ new search options ( ) . set page ( __num__ @$ __num__ ) ) ; assert that ( result . get hits ( ) . get hits ( ) ) . has size ( __num__ ) ; assert that ( result . get hits ( ) . get total hits ( ) ) . is equal to ( __num__ ) ; result = under test . search ( issue query . builder ( ) . build ( ) @$ new search options ( ) . set offset ( __num__ ) . set limit ( __num__ ) ) ; assert that ( result . get hits ( ) . get hits ( ) ),issues contain pages
the application will call <PLACE_HOLDER> when waiting for a message @$ which will in turn call this on the transport thread .,stream . transport state ( ) . request messages from deframer ( __num__ ) ;,application call notification
the <PLACE_HOLDER> session was aborted @$ clean up the pending <PLACE_HOLDER> .,if ( ! success ) { delete rollback ( rollback ) ; return null ; },session clean transaction
check if the given spark <PLACE_HOLDER> has a child spark <PLACE_HOLDER> that contains the target map work if it does not @$ then remove the target from dpp op,if ( ! task contains dependent map work ( task @$ target map work ) ) { to remove . add ( target info ) ; pruning sink op . remove from source event ( target map work @$ target info . part key @$ target info . column name @$ target info . column type ) ; log . info ( __str__ + target map work . get name ( ) + __str__ + base work . get name ( ) + __str__ ) ; },child spark task
getting action for the null <PLACE_HOLDER> @$ which in this case means the body <PLACE_HOLDER>,for ( handler ah : action handlers ) { final action [ ] aa = ah . get actions ( null @$ this ) ; if ( aa != null ) { for ( int ai = __num__ ; ai < aa . length ; ai ++ ) { final string akey = action mapper . key ( aa [ ai ] ) ; action set . add ( aa [ ai ] ) ; keys . add ( akey ) ; } } },which means key
fs dir <PLACE_HOLDER> because fs stats uses fs dir <PLACE_HOLDER>,final service metric event . builder builder = builder ( ) . set dimension ( __str__ @$ dir ) ;,stats uses xmx
empty clozes use first <PLACE_HOLDER>,if ( ords . is empty ( ) && allow empty ) { return new array list < > ( arrays . as list ( new integer [ ] { __num__ } ) ) ; },clozes use row
emit max uncommitted offsets messages @$ and fail all of <PLACE_HOLDER> . then ensure that the spout will retry <PLACE_HOLDER> when the retry backoff has passed,try ( simulated time simulated time = new simulated time ( ) ) { kafka spout < string @$ string > spout = spout with mocked consumer setup helper . setup spout ( spout config @$ conf @$ context mock @$ collector mock @$ consumer mock @$ partition ) ; map < topic partition @$ list < consumer record < string @$ string > > > records = new hash map < > ( ) ; int num records = spout config . get max uncommitted offsets ( ) ; records . put ( partition @$ spout with mocked consumer setup helper . create records ( partition @$ __num__ @$ num records ) ) ; when ( consumer mock . poll ( any long ( ) ) ),spout retry them
any change to an input file may affect program <PLACE_HOLDER> @$ even if only by changing line numbers in error messages .,path fragment extension file = extension label . to path fragment ( ) ; try ( mutability mutability = mutability . create ( __str__ @$ extension file ) ) { starlark thread thread = rule class provider . create rule class starlark thread ( extension label @$ mutability @$ starlark semantics @$ event handler @$ file . get content hash code ( ) @$ import map @$ package factory . get native module ( in workspace ) @$ repository mapping ) ; exec and export ( file @$ extension label @$ event handler @$ thread ) ; event . replay events on ( env . get listener ( ) @$ event handler . get events ( ) ) ; for ( postable post : event handler . get,change affect variables
utf requires java string <PLACE_HOLDER>,buffered string tmp str = new buffered string ( ) ; for ( int i = __num__ ; i < chk . _len ; i ++ ) { if ( chk . isna ( i ) ) new chk . addna ( ) ; else { string str = chk . at str ( tmp str @$ i ) . to string ( ) ; new chk . add num ( calc entropy ( str ) ) ; } },utf requires methods
checks the status of the rpc call @$ throws an <PLACE_HOLDER> in case of error,check status ( resp . get status ( ) ) ; operation state op state = operation state . get operation state ( resp . get operation state ( ) ) ; hivesql exception op exception = null ; if ( op state == operation state . error ) { op exception = new hivesql exception ( resp . get error message ( ) @$ resp . get sql state ( ) @$ resp . get error code ( ) ) ; } return new operation status ( op state @$ resp . get task status ( ) @$ resp . get operation started ( ) @$ resp . get operation completed ( ) @$ resp . is set has result set ( ) ? resp . is,checks throws exception
those following presences would have leak <PLACE_HOLDER> that there is some file satisfying that pattern inside,assert that ( workspace content @$ all of ( not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) @$ not ( contains string ( __str__ ) ) ) ) ;,presences have information
and if still not found @$ let 's choose <PLACE_HOLDER> ?,if ( name == null ) { name = _default type id ( cls ) ; },'s choose one
this check is required because <PLACE_HOLDER> impl currently implements soap body <PLACE_HOLDER>,if ( ( element instanceof soap body element ) && ! ( element . get class ( ) . equals ( element impl . class ) ) ) { return ( soap element ) element ; } else { return replace element withsoap element ( element @$ ( element impl ) create body element ( name impl . copy element name ( element ) ) ) ; },impl implements element
now write the attributes representing the configuration <PLACE_HOLDER> .,configuration . write xml attrs ( xml @$ config stats . m configuration ) ; xml . end tag ( null @$ config_tag ) ;,attributes representing summary
wait for all non cancelled <PLACE_HOLDER> to be completed,try { boolean acquired = sem . try acquire ( max_waiting_time @$ time unit . milliseconds ) ; assert . assert true ( __str__ @$ acquired ) ; assert . assert equals ( __str__ @$ list . size ( ) @$ nb_add ) ; for ( int i = __num__ ; i < nb_add ; i ++ ) { assert . assert true ( __str__ @$ i < nb_add ) ; } } catch ( interrupted exception e ) { assert . assert false ( __str__ @$ true ) ; },non cancelled operations
if the id is a generated unique id then this could affect .q file golden <PLACE_HOLDER> for tests that run explain queries .,return __str__ + id + __str__ ;,.q file buffers
check if client reconnected . if so @$ notify apps . ralam <PLACE_HOLDER> 35 @$ 2018,log . debug ( __str__ @$ username + __str__ + user id + __str__ @$ client id ) ; sip peer manager . hangup ( peer id @$ client id @$ notify apps ) ;,client ralam bug
if the number of posts on this blog that use this <PLACE_HOLDER> is higher than previous @$ set this as the most popular <PLACE_HOLDER> @$ and set the second most popular <PLACE_HOLDER> to the current most popular <PLACE_HOLDER>,int post count = json this tag . opt int ( __str__ ) ; if ( post count > popular count ) { next most popular tag = most popular tag ; most popular tag = this tag name ; popular count = post count ; } else if ( next most popular tag == null ) { next most popular tag = this tag name ; },number set tag
now round robin assignment with the modified servers list should return the <PLACE_HOLDER> as the regionserver assignee,assignment map = balancer . round robin assignment ( regions @$ servers ) ; set < server name > server with primary = assignment map . key set ( ) ; assert true ( server before . contains all ( server with primary ) ) ;,assignment return same
verify that the includes file contains all <PLACE_HOLDER>,path file resource = new path ( config ) ; conf . add resource ( file resource ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; assert equals ( __str__ @$ conf . get ( __str__ ) ) ; tear down ( ) ;,file contains properties
assume indirect definitions references use the <PLACE_HOLDER>,for ( node n : refs ) { if ( reference map . is call target ( n ) ) { node call node = reference map . get call or new node for target ( n ) ; if ( node util . is expression result used ( call node ) ) { return false ; } seen use = true ; } else if ( is candidate definition ( n ) ) { seen candidate definiton = true ; } else { if ( ! optimize calls . is allowed reference ( n ) ) { return false ; } } },references use property
since hot swap passes run one <PLACE_HOLDER> at a time @$ namespaces seen will not include any provides earlier than this current <PLACE_HOLDER> .,if ( ! in hot swap && import type . must be ordered ( ) && ! namespaces seen . contains ( namespace ) ) { t . report ( call @$ late_provide_error @$ namespace ) ; },passes run run
expected ip matches the given ip <PLACE_HOLDER> @$ return,if ( inet address . get by name ( expectedip ) . equals ( inet address . get by name ( ip address ) ) ) { return ; },ip matches address
first run <PLACE_HOLDER> 1 @$ <PLACE_HOLDER> 3 are able to run <PLACE_HOLDER> 2 is blocked by <PLACE_HOLDER> 1,wait and assert timestamp ( p1keya @$ __num__ @$ __num__ ) ; wait and assert timestamp ( p2keya @$ __num__ @$ - __num__ ) ; wait and assert timestamp ( p3keyb @$ __num__ @$ __num__ ) ; assert equals ( true @$ locka . get ( ) ) ; assert equals ( true @$ lockb . get ( ) ) ;,1 run thread
we need to pass on input method events since some host input method adapters send them through the java event queue instead of directly to the component @$ and the input context also handles the java composition <PLACE_HOLDER>,if ( are input methods enabled ( ) ) { if ( ( ( e instanceof input method event ) && ! ( this instanceof composition area ) ) || ( e instanceof input event ) || ( e instanceof focus event ) ) { input context input context = get input context ( ) ; if ( input context != null ) { input context . dispatch event ( e ) ; if ( e . is consumed ( ) ) { if ( ( e instanceof focus event ) && focus log . is loggable ( platform logger . level . finest ) ) { focus log . finest ( __str__ + e ) ; } return ; } } } } else { if (,context handles information
note : semgrex can match <PLACE_HOLDER> named nodes to the same node . in this case @$ we simply @$ check the named nodes @$ and if there are any collisions @$ we throw out this match .,while ( matcher . find ( ) ) { set < string > node names = matcher . get node names ( ) ; set < indexed word > seen = generics . new hash set ( ) ; for ( string name : node names ) { indexed word curr = matcher . get node ( name ) ; if ( seen . contains ( curr ) ) break next match ; seen . add ( curr ) ; } if ( predicate test != null ) { if ( ! predicate test . test ( matcher ) ) continue ; } semantic graph tgt = semantic graph factory . duplicate keep nodes ( sg ) ; node map = generics . new hash map ( ),semgrex match many
easiest ways is using json <PLACE_HOLDER>,json object json to encode = new json object ( ) ; json to encode . put ( __str__ @$ custom message . get status code ( ) ) ; json to encode . put ( __str__ @$ custom message . get result code ( ) ) ; json to encode . put ( __str__ @$ custom message . get summary ( ) ) ;,ways using encoding
register a pre commit <PLACE_HOLDER> that will touch the collection and delete the entity,( ( event source ) s ) . get action queue ( ) . register process ( new before transaction completion process ( ) { @ override public void do before transaction completion ( session implementor session ) { q to delete . get fums ( ) . size ( ) ; } } ) ; s . delete ( q to delete ) ; boolean ok = false ; try { s . get transaction ( ) . commit ( ) ; } catch ( lazy initialization exception e ) { ok = true ; s . get transaction ( ) . rollback ( ) ; } catch ( transaction exception te ) { if ( te . get cause ( ) instanceof lazy initialization exception ),pre commit action
we used to report an error here if the <PLACE_HOLDER> was not resolved . having moved the call to 'check supers ' from 'basic check ' into 'resolve type structure ' @$ the errors reported here should have already been reported . furthermore @$ error recovery can null out the <PLACE_HOLDER> @$ which would cause a spurious error from the test here .,env . dt exit ( __str__ + this ) ;,recovery null node
if permissions need a <PLACE_HOLDER> before any of the app components can run @$ we drop the broadcast and if the calling app is in the foreground and the broadcast is explicit we launch the <PLACE_HOLDER> ui passing it a pending intent to send the skipped broadcast .,if ( ! request start target permissions review if needed locked ( r @$ filter . package name @$ filter . owning user id ) ) { r . delivery [ index ] = broadcast record . delivery_skipped ; return ; } r . delivery [ index ] = broadcast record . delivery_delivered ;,permissions need review
check types so we can make sure our exported externs have type <PLACE_HOLDER> .,options . set check symbols ( true ) ; return options ;,types have information
default transfer handler does n't support <PLACE_HOLDER> so we do n't want <PLACE_HOLDER> handling,if ( tree . get drop target ( ) instanceof ui resource ) { tree . set drop target ( null ) ; },handler support drop
eat the exception as it does n't affect the state of existing tables . expect @$ user to manually drop this path when exception and so logging a <PLACE_HOLDER> .,log . warn ( __str__ @$ delete old data loc @$ dbname + __str__ + name ) ;,exception logging warning
reset attempts count and update cumulative wait <PLACE_HOLDER> .,backoff = reset backoff ( duration @$ nano clock @$ start nanos ) ;,attempts count time
use a label if it is marked with one trailing star @$ even if the label string sorts the <PLACE_HOLDER> when all contractions are suppressed .,if ( item . char at ( item . length ( ) - __num__ ) == __str__ && item . char at ( item . length ( ) - __num__ ) != __str__ ) { item = item . substring ( __num__ @$ item . length ( ) - __num__ ) ; check distinct = false ; } else { check distinct = true ; },string sorts longs
the answer does n't match the <PLACE_HOLDER> . that 's not good .,if ( ! query . get question ( ) . equals ( response . get question ( ) ) ) { badresponse = true ; badresponse_error = __str__ ; log manager . i ( this @$ __str__ + badresponse_error ) ; return ; },answer match query
this function does n't use 'att index ' . we are adding the attribute later after we have figured out that current attribute is not namespace declaration since scan attribute value does n't use att index <PLACE_HOLDER> therefore we can safely add the attribute later..,xml string tmp str = get string ( ) ; string localpart = f attributeq name . localpart ; string prefix = f attributeq name . prefix != null ? f attributeq name . prefix : xml symbols . empty_string ; boolean isns decl = f bind namespaces & ( prefix == xml symbols . prefix_xmlns || prefix == xml symbols . empty_string && localpart == xml symbols . prefix_xmlns ) ; scan attribute value ( tmp str @$ f temp string2 @$ f attributeq name . rawname @$ attributes @$ attr index @$ isvc @$ f current element . rawname @$ isns decl ) ; string value = null ;,value use parameter
init logic get <PLACE_HOLDER>,local < member > method = code . get parameter ( __num__ @$ member type id ) ; local < xposed bridge . additional hook info > hook info = code . get parameter ( __num__ @$ hook info type id ) ;,logic get parameters
some drivers do not report an incorrect <PLACE_HOLDER> . then the name is '\0 ' terminated .,if ( location == shader program constants . location_invalid ) { length = __num__ ; while ( length < shader program . name_container_size && shader program . name_container [ length ] != __str__ ) { length ++ ; } name = new string ( shader program . name_container @$ __num__ @$ length ) ; location = gles20 . gl get attrib location ( this . m programid @$ name ) ; if ( location == shader program constants . location_invalid ) { throw new shader program link exception ( __str__ + name + __str__ ) ; } },drivers report length
write out the keep <PLACE_HOLDER> and keep method options @$ if any .,if ( class specification . field specifications != null || class specification . method specifications != null ) { writer . print ( __str__ ) ; writer . println ( configuration constants . open_keyword ) ; write field specification ( class specification . field specifications ) ; write method specification ( class specification . method specifications ) ; writer . println ( configuration constants . close_keyword ) ; } else { writer . println ( ) ; },the keep fields
see if clazz extends <PLACE_HOLDER> of the configured base classes for this method,for ( string baseclass name : preserve overrides . get ( method name ) ) { class < ? > baseclass = load from internal ( baseclass name ) ; check state ( ! baseclass . is interface ( ) @$ __str__ @$ baseclass name ) ; if ( ! baseclass . is assignable from ( clazz ) ) { continue ; } for ( method m : clazz . get superclass ( ) . get methods ( ) ) { if ( method name . equals ( m . get name ( ) ) && descriptor . equals ( type . get method descriptor ( m ) ) && baseclass . equals ( m . get declaring class ( ) ) ) { return true ; },clazz extends any
trigger check to see if parent size has changed @$ recalculate <PLACE_HOLDER>,resize timer = new timer ( ) { @ override public void run ( ) { perform size check ( ) ; resize timer . schedule ( monitor_parent_timer_interval ) ; } } ;,size changed timer
only first or second operator contains dpp <PLACE_HOLDER>,if ( dpps op1 . size ( ) != dpps op2 . size ( ) ) { return false ; },first contains semantics
using random uuid ensures that multiple clusters can be launched by a same test @$ if it stops & starts <PLACE_HOLDER>,path test dir = get data test dir ( __str__ + get randomuuid ( ) . to string ( ) ) ; cluster test dir = new file ( test dir . to string ( ) ) . get absolute file ( ) ;,& starts execution
now do the 1 big <PLACE_HOLDER> .,file . seek ( next write batch . offset ) ; if ( max stat > __num__ ) { if ( stat idx < max stat ) { stats [ stat idx ++ ] = sequence . get length ( ) ; } else { long all = __num__ ; for ( ; stat idx > __num__ ; ) { all += stats [ -- stat idx ] ; } log . trace ( __str__ @$ all / max stat ) ; } } file . write ( sequence . get data ( ) @$ sequence . get offset ( ) @$ sequence . get length ( ) ) ; replication target replication target = journal . get replication target ( ) ; if ( replication target !=,now do write
action client env contains the <PLACE_HOLDER> where values from action <PLACE_HOLDER> are overridden .,action client env . put all ( client env ) ; if ( command . builds ( ) ) { for ( map . entry < string @$ string > entry : options . get options ( core options . class ) . action environment ) { if ( entry . get value ( ) == null ) { visible action env . add ( entry . get key ( ) ) ; } else { visible action env . remove ( entry . get key ( ) ) ; action client env . put ( entry . get key ( ) @$ entry . get value ( ) ) ; } } for ( map . entry < string @$ string > entry : options . get,env contains values
currently project <PLACE_HOLDER> can not have derived <PLACE_HOLDER> .,if ( view . get configuration ( ) . get module ( ) == null ) { return views ; } final string query = __str__ ; try ( prepared statement statement = provider . get connection ( ) . get connection ( ) . prepare statement ( query ) ) { final list < i navi view > module views = view . get configuration ( ) . get module ( ) . get content ( ) . get view container ( ) . get views ( ) ; statement . set int ( __num__ @$ view . get configuration ( ) . get id ( ) ) ; final result set result set = statement . execute query ( ) ; if ( result set ==,views derived views
if the mapping list contains the generic type <PLACE_HOLDER> @$ begin to check this generic type <PLACE_HOLDER> class .,if ( lite pal attr . get instance ( ) . get class names ( ) . contains ( generic type name ) ) { class < ? > reverse dynamic class = class . for name ( generic type name ) ; field [ ] reverse fields = reverse dynamic class . get declared fields ( ) ; boolean reverse associations = false ; for ( field reverse field : reverse fields ) { if ( ! modifier . is static ( reverse field . get modifiers ( ) ) ) { class < ? > reverse field type class = reverse field . get type ( ) ; if ( class name . equals ( reverse field type class . get name ( ) ) ),list contains name
now that the store is sane @$ try and get all the <PLACE_HOLDER> sent,return receive messages ( messages expected @$ session @$ one phase ) ;,store sane messages
replication causes many implicit alter database <PLACE_HOLDER> @$ so metastore will see some alter table events as well .,events map . put ( alter database event . class . get name ( ) @$ null ) ; events map . put ( create table event . class . get name ( ) @$ new hash set < > ( arrays . as list ( __str__ ) ) ) ; events map . put ( alter table event . class . get name ( ) @$ new hash set < > ( arrays . as list ( __str__ @$ __str__ @$ __str__ ) ) ) ; events map . put ( drop table event . class . get name ( ) @$ new hash set < > ( arrays . as list ( __str__ ) ) ) ; return events map ;,some alter operations
the two approaches should produce the same hash <PLACE_HOLDER>,assert false ( seq add tokens . is empty ( ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert true ( seq add tokens . values ( ) . contains ( __num__ ) ) ; assert equals ( batch add tokens @$ seq add tokens ) ;,approaches produce code
should not work : <PLACE_HOLDER> on string,try { tuple ds . aggregate ( aggregations . sum @$ __num__ ) ; assert . fail ( ) ; } catch ( unsupported aggregation type exception iae ) { },not work aggregate
use the record <PLACE_HOLDER> processor list if it is configured . this list can contain both all record <PLACE_HOLDER> processor types,if ( ! this . record stream processors . is empty ( ) ) { for ( record stream processor stream processor : this . record stream processors ) { stream = stream processor . process stream ( stream @$ this . task state ) ; } } else { if ( this . converter instanceof multi converter ) { for ( converter cverter : ( ( multi converter ) this . converter ) . get converters ( ) ) { stream = cverter . process stream ( stream @$ this . task state ) ; } } else { stream = this . converter . process stream ( stream @$ this . task state ) ; } },list contain stream
make the user agent tester change its <PLACE_HOLDER> and make sure we do n't get notifications as we 're now unsubscribed .,logger . debug ( __str__ ) ; presence status old status = operation set presence2 . get presence status ( ) ; presence status new status = supported status set2 . get ( jabber status enum . free_for_chat ) ;,tester change states
the second time should contain the <PLACE_HOLDER>,has certificate = info . get transport context ( ) instanceof x509 certificate [ ] ;,time contain certificate
some systems send update <PLACE_HOLDER> before call is established to update call screening @$ if we do not process them and just send 501 we end the dialog and the call will fail to establish @$ so we just send ok,return false ;,systems send codecs
new chunk requires a new <PLACE_HOLDER>,statement dsg = new declaration statement group ( this @$ module ) ; dsg stack . push ( statement dsg ) ;,chunk requires declaration
vm <PLACE_HOLDER> locks and frees key <PLACE_HOLDER>,vm1 . invoke ( new serializable runnable ( ) { @ override public void run ( ) { logger . info ( __str__ ) ; connect distributed system ( ) ; d lock service dls = ( d lock service ) d lock service . create ( dls name @$ get system ( ) @$ true @$ true @$ false ) ; assert that ( dls . lock ( key1 @$ - __num__ @$ - __num__ ) ) . is true ( ) ; logger . info ( __str__ ) ; dls . unlock ( key1 ) ; assert that ( dls . get token ( key1 ) ) . is not null ( ) ; dls . free resources ( key1 ) ; d lock token token,vm locks 1
fails for e.g . broken sockets silently swallow <PLACE_HOLDER> and just record the failed path,fails . add ( path ) ;,sockets swallow exceptions
noinspection unchecked ca n't know target entity <PLACE_HOLDER> .,if ( relation info . to one getter != null ) { to one to one = relation info . to one getter . get to one ( entity ) ; if ( to one != null ) { to one . get target ( ) ; } } else { if ( relation info . to many getter == null ) { throw new illegal state exception ( __str__ + relation info ) ; } list to many = relation info . to many getter . get to many ( entity ) ; if ( to many != null ) { to many . size ( ) ; } },unchecked know type
let the visitor visit the <PLACE_HOLDER> referenced in the local variable .,local variable info . referenced class accept ( class visitor ) ;,visitor visit class
if we found ourselves closed after the <PLACE_HOLDER> increment @$ decrement the <PLACE_HOLDER> again and do not forward the request,if ( this . closed || this . request queue . is closed ( ) ) { this . requests not returned . decrement and get ( ) ; final notification listener listener ; synchronized ( listener lock ) { listener = all requests processed listener ; all requests processed listener = null ; } if ( listener != null ) { listener . on notification ( ) ; } throw new io exception ( __str__ + request ) ; },request decrement count
we have finished processing this element . decrement the <PLACE_HOLDER> .,int new depth = depth . decrement and get ( ) ;,element decrement depth
in all chunks except the last this chunk also takes <PLACE_HOLDER> of the detection in the seam @$ but for the last one there 's no seam at the end .,long end = last ? to exclusive - __num__ : to exclusive ; for ( long i = from inclusive ; i < end ; i ++ ) { detect ( same group detector @$ i ) ; if ( ++ local progress == __num__ ) { progress . add ( local progress ) ; local progress = __num__ ; } } progress . add ( local progress ) ;,chunk takes care
sensors do n't generate <PLACE_HOLDER> .,m_manifold . point count = __num__ ; evaluate ( m_manifold @$ xfa @$ xfb ) ; touching = m_manifold . point count > __num__ ;,sensors generate points
another wrapping panel so the borderlayout 'd source list <PLACE_HOLDER> panel does n't get forced to take up the full east cell of the containing panel .,j panel button wrapper panel = new j panel ( ) ; button wrapper panel . add ( source list buttons panel ) ; source list panel . add ( button wrapper panel @$ border layout . east ) ; source list model . add list data listener ( new list data listener ( ) { @ override public void interval removed ( list data event e ) { contents changed ( e ) ; } @ override public void interval added ( list data event e ) { contents changed ( e ) ; } @ override public void contents changed ( list data event e ) { boolean has selection = source list . get selected indices ( ) . length > __num__ ; remove source,panel 'd buttons
we explicitly do n't close 'out ' because we must not close the backing <PLACE_HOLDER> . the file output stream will not close it implicitly .,@ suppress warnings ( __str__ ) data output stream out = new data output stream ( fos ) ; out . write int ( m current blob version ) ; final int n = ( state != null ) ? state . size ( ) : __num__ ; out . write int ( n ) ; for ( int i = __num__ ; i < n ; i ++ ) { final string key = state . key at ( i ) ; final long checksum = state . value at ( i ) . long value ( ) ; if ( debug ) { log . i ( tag @$ __str__ + key + __str__ + checksum ) ; } out . writeutf ( key ) ;,stream close fd
wait till client gets the <PLACE_HOLDER>,test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { int events = counting consumer . get num data events ( ) ; log . info ( __str__ + events + __str__ ) ; return events == __num__ ; } } @$ __str__ @$ __num__ * __num__ @$ log ) ;,client gets event
do the reduce <PLACE_HOLDER>,t res = reducer . reduce ( match @$ record ) ;,the reduce operation
bring up wifi @$ then validate it . previous versions would immediately tear down <PLACE_HOLDER> @$ but it 's arguably correct to linger it @$ since it was the default network before it validated .,m wi fi network agent = new test network agent wrapper ( transport_wifi ) ; m wi fi network agent . connect ( true ) ; callback . expect available callbacks unvalidated ( m wi fi network agent ) ;,versions tear bindings
target tier <PLACE_HOLDER> :,assert equals ( input tier1 . size ( ) @$ __num__ ) ; assert equals ( input tier1 . get ( __num__ ) . get amount ( ) @$ new big decimal ( __str__ ) ) ;,target tier 1
most qjmha cluster tests do n't need data <PLACE_HOLDER> @$ so we 'll make this the default,this . dfs builder = new minidfs cluster . builder ( conf ) . num data nodes ( __num__ ) ;,tests need nodes
host @$ port @$ ssl @$ ssl socket factory @$ ssl parameters @$ hostname <PLACE_HOLDER>,jedis utils . add set end point interceptor ( target @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) ; return target . to bytecode ( ) ;,factory hostname verifier
if the arc identifies a readable <PLACE_HOLDER> @$ then two cases :,if ( is readable ( arc ) ) { if ( pos == ( length - __num__ ) ) { result = new long [ depth + __num__ ] ; result [ depth + __num__ ] = __num__ ; result [ depth ] = arc ; checker . add ( depth @$ result @$ depth @$ __num__ ) ; try { checker . check current oid ( ) ; } catch ( snmp status exception e ) { throw new snmp status exception ( snmp status exception . no such object ) ; } finally { checker . remove ( depth @$ __num__ ) ; } handlers . add ( this @$ depth @$ varbind ) ; return result ; } } else if ( is nested arc,arc identifies object
parse fail <PLACE_HOLDER> @$ if present,if ( status info . data . available ( ) > __num__ ) { this . failure info = status info . data . get unaligned bit string ( ) . to boolean array ( ) ; },parse fail info
say hello to equinox who has its own <PLACE_HOLDER> . we use introspection like there is no tomorrow to get access to the file,if ( url . get protocol ( ) . equals ( __str__ ) ) { url connection con = url . open connection ( ) ; con . set use caches ( resource . get default use caches ( ) ) ; if ( bundle_entry_field == null ) { bundle_entry_field = con . get class ( ) . get declared field ( __str__ ) ; bundle_entry_field . set accessible ( true ) ; } object bundle entry = bundle_entry_field . get ( con ) ; if ( match ( bundle entry . get class ( ) . get name ( ) @$ file_bundle_entry_classes ) ) { if ( file_field == null ) { file_field = bundle entry . get class ( ) . get declared field ( __str__,who has bundle
we do n't have permission to read <PLACE_HOLDER> creds we should get an empty set even though the private creds has an <PLACE_HOLDER> cred . no security exception either !,try { set priv cred set1 = s . get private credentials ( integer . class ) ; if ( priv cred set1 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set1 . size ( ) ) ; } } catch ( security exception e ) { system . out . println ( __str__ ) ; } system . out . println ( __str__ ) ; set priv cred set2 = s . get private credentials ( ) ; if ( priv cred set2 . size ( ) != __num__ ) { throw new runtime exception ( __str__ + priv cred set2 . size ( ) ) ; },creds has entry
a volt db extension to disable variable scale <PLACE_HOLDER>,scale = __num__ ;,extension disable parameters
if input has <PLACE_HOLDER>,if ( text . matches ( __str__ ) ) { assert equals ( pos . get error index ( ) >= __num__ @$ true ) ; assert equals ( pos . get index ( ) @$ __num__ ) ; assert equals ( parsed @$ null ) ; } else { assert equals ( pos . get index ( ) @$ expected index @$ __str__ + lc text ) ; assert equals ( pos . get error index ( ) @$ expected error index @$ __str__ + lc text ) ; if ( expected != null ) { assert equals ( parsed . query ( temporal queries . zone id ( ) ) @$ expected ) ; assert equals ( parsed . query ( temporal queries . offset (,input has text
server <PLACE_HOLDER> should not retrieve local <PLACE_HOLDER> .,verify no more interactions ( local logs ) ;,logs retrieve logs
when <PLACE_HOLDER> is on @$ do <PLACE_HOLDER> based user auth check,try { if ( authorization enabled && access controller available && ! is system or super user ( ) ) { user user = visibility utils . get active user ( ) ; throw new access denied exception ( __str__ + ( user != null ? user . get short name ( ) : __str__ ) + __str__ ) ; } if ( authorization enabled ) { check calling user auth ( ) ; } for ( byte string authbs : auths ) { label auths . add ( authbs . to byte array ( ) ) ; } operation status [ ] op status = this . visibility label service . clear auths ( request user @$ label auths ) ; log result ( true @$ __str__,check do user
the above connection request should trigger a 'connecting ' state <PLACE_HOLDER> to be replicated,while ( node statuses . is empty ( ) ) { thread . sleep ( __num__ ) ; } final node connection status connecting status = node statuses . get ( __num__ ) ; assert equals ( node connection state . connecting @$ connecting status . get state ( ) ) ; assert equals ( requested node id @$ connecting status . get node identifier ( ) ) ;,request trigger update
start get <PLACE_HOLDER> .,executors . execute ( new runnable ( ) { @ override public void run ( ) { try { list < string > keys = new array list < string > ( test entries . key set ( ) ) ; while ( ! rebalancing token . get ( ) ) { int index = ( int ) ( math . random ( ) * keys . size ( ) ) ; try { list < versioned < byte [ ] > > values = server side routing storerw . get ( new byte array ( byte utils . get bytes ( keys . get ( index ) @$ __str__ ) ) @$ null ) ; assert equals ( __str__ @$ __num__ @$ values . size ( ),start get operation
simultaneously null implies we are not connected to the db @$ implies undesirable <PLACE_HOLDER> so throw exception,if ( conn == null && ps == null && rs == null ) { throw new sql exception ( res bundle . handle get object ( __str__ ) . to string ( ) ) ; },exception implies behavior
the user specified an empty <PLACE_HOLDER> to skip the copy,if ( s . trim ( ) . ends with ( __str__ ) || s . trim ( ) . length ( ) == __num__ ) { continue ; },user specified string
ensure root has the correct inode <PLACE_HOLDER> last inode <PLACE_HOLDER> should be root inode <PLACE_HOLDER> and inode map size should be 1,int inode count = __num__ ; long expected last inode id = i node id . root_inode_id ; assert equals ( fsn . dir . root dir . get id ( ) @$ i node id . root_inode_id ) ; assert equals ( expected last inode id @$ last id ) ; assert equals ( inode count @$ fsn . dir . get inode map size ( ) ) ;,root has id
try to load the <PLACE_HOLDER> as a pem encoded public <PLACE_HOLDER>,if ( pem reader . is pem ( data ) ) { try { return new loaded key ( pem reader . load public key ( data ) ) ; } catch ( runtime exception | general security exception e ) { throw new signature exception ( __str__ @$ e ) ; } },pem encoded key
chrome always allows insertion <PLACE_HOLDER> .,return true ;,chrome allows events
update permission on an outdated acl @$ retry should keep <PLACE_HOLDER> going,principal sid user1 = new principal sid ( __str__ ) ; mutable acl record child acl2 = acl service . upsert ace ( child acl outdated @$ user1 @$ acl permission . administration ) ; assert . assert equals ( parent oid @$ child acl2 . get acl record ( ) . get parent domain object info ( ) ) ; assert . assert equals ( acl permission . administration @$ child acl2 . get acl record ( ) . get permission ( user1 ) ) ;,retry keep looping
note that 'rollback ' closes the <PLACE_HOLDER> .,if ( writer != null ) { writer . rollback ( ) ; },'rollback closes writer
we do n't want the packager closing the <PLACE_HOLDER> . v 1 creates a tar output <PLACE_HOLDER> @$ which then gets closed @$ which in turn closes the underlying output <PLACE_HOLDER> @$ and we want to protect ourselves against that .,try ( final output stream buffered out = new buffered output stream ( raw out ) ) { final output stream out = new non closeable output stream ( buffered out ) ; for ( final flow file flow file : contents ) { bin . get session ( ) . read ( flow file @$ false @$ new input stream callback ( ) { @ override public void process ( final input stream raw in ) throws io exception { try ( final input stream in = new buffered input stream ( raw in ) ) { final map < string @$ string > attributes = new hash map < > ( flow file . get attributes ( ) ) ; attributes . put ( __str__ @$,which closes stream
let 's take the <PLACE_HOLDER> of item geopoint x field ...,data . index of fielditempointx = data . input row meta . index of value ( meta . get geo point lat ( ) ) ; if ( data . index of fielditempointx < __num__ ) { log error ( base messages . get string ( pkg @$ __str__ @$ meta . get geo point lat ( ) ) ) ; throw new kettle exception ( base messages . get string ( pkg @$ __str__ @$ meta . get geo point lat ( ) ) ) ; },'s take index
the abort slow consumer strategy kicks in here and sends a message down to the client to close itself . the client does not close because it is in the middle of the transaction . meanwhile the failover transport detects the close <PLACE_HOLDER> and removes the consumer from its state .,assert true ( __str__ @$ wait . wait for ( new wait . condition ( ) { @ override public boolean is satisified ( ) throws exception { return aborting slow consumer ; } } @$ __num__ * __num__ ) ) ;,transport detects problem
bk commit time safety <PLACE_HOLDER>,int thread sleep time = idle reader error threshold - __num__ - __num__ ;,bk commit check
query the superclass @$ which triggers argument <PLACE_HOLDER> .,final path p = make qualified ( path ) ; switch ( validate path capability args ( p @$ capability ) ) { case common path capabilities . fs_acls : case common path capabilities . fs_append : case common path capabilities . fs_concat : case common path capabilities . fs_permissions : case common path capabilities . fs_snapshots : case common path capabilities . fs_storagepolicy : case common path capabilities . fs_xattrs : return true ; case common path capabilities . fs_symlinks : return false ; default : return super . has path capability ( p @$ capability ) ; },which triggers validation
add pending remote input <PLACE_HOLDER> after starting work challenge @$ as starting work challenge will clear all previous pending re<PLACE_HOLDER> <PLACE_HOLDER>,m pending work remote input view = clicked ;,challenge clear view
a<PLACE_HOLDER><PLACE_HOLDER>ert that root doe<PLACE_HOLDER> not contain <PLACE_HOLDER> 1,assert true ( ! root . contains ( frag one ) ) ;,root contain fr
does not search <PLACE_HOLDER>,if ( scope activity . get activities ( ) . contains ( current activity ) || scope activity . equals ( current activity ) ) { candiadate execution = execution ; } else if ( current activity != null && current activity . contains ( ( activity impl ) scope activity ) ) { break ; },not search resources
we have backed up this app before . check whether the <PLACE_HOLDER> of the backup matches the <PLACE_HOLDER> of the current app ; if they do n't match @$ the app has been updated and we need to store its metadata again . in either case @$ take it out of m existing so that we do n't consider it deleted later .,if ( m existing . contains ( pack name ) ) { m existing . remove ( pack name ) ; if ( info . get long version code ( ) == m state versions . get ( pack name ) . version code ) { continue ; } },version matches version
basic split pane divider.get preferred <PLACE_HOLDER> for the reason . i leave it in hopes of having this used at some point .,if ( c != basic split pane divider . this || split pane == null ) { return new dimension ( __num__ @$ __num__ ) ; } dimension button min size = null ; if ( split pane . is one touch expandable ( ) && left button != null ) { button min size = left button . get minimum size ( ) ; } insets insets = get insets ( ) ; int width = get divider size ( ) ; int height = width ; if ( orientation == j split pane . vertical_split ) { if ( button min size != null ) { int size = button min size . height ; if ( insets != null ) { size += insets .,split pane size
otherwise @$ no display text @$ just use the executable <PLACE_HOLDER>,string program info = text [ __num__ ] ; return get display text for file path or name ( program info ) ;,text use name
skip pass all the finally <PLACE_HOLDER> because both the fall through and return will also trigger all the finally <PLACE_HOLDER> .,node prefinally follows = follow ; follow = skip finally nodes ( follow ) ; if ( prefinally follows != follow ) { if ( ! is pure ( exit expr ) ) { return n ; } },return trigger nodes
the selector returned prematurely many <PLACE_HOLDER> in a row . rebuild the selector to work around the problem .,if ( selector_auto_rebuild_threshold > __num__ && select cnt >= selector_auto_rebuild_threshold ) { logger . warn ( __str__ @$ select cnt @$ selector ) ; rebuild selector ( ) ; return true ; },selector returned fields
test what happens if a user acc<PLACE_HOLDER>entally uses the same <PLACE_HOLDER> in multiple layouts too .,parsed android data direct = android data builder . of ( source ) . add resource ( __str__ @$ android data builder . resource type . layout @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . layout @$ __str__ @$ __str__ @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . value @$ __str__ @$ __str__ ) . add resource ( __str__ @$ android data builder . resource type . value @$ __str__ ) . create manifest ( __str__ @$ __str__ @$ __str__ ) . build parsed ( ) ;,user uses id
a volt db extension to avoid a <PLACE_HOLDER> when a is not an instance of double,if ( ! ( a instanceof double ) ) { a = new double ( a . to string ( ) ) ; },extension avoid crash
j rockit throws this exception instead of returning null as the javadocs say it should . see <PLACE_HOLDER> 36348,this . set collection usage unsupported ( mp ) ;,36348 see bug
an inactive context may not have a <PLACE_HOLDER> so we use our <PLACE_HOLDER> to call all of the context 's listeners instead,if ( context instanceof abstract application context ) { for ( application listener < ? > listener : ( ( abstract application context ) context ) . get application listeners ( ) ) { this . initial multicaster . add application listener ( listener ) ; } } this . initial multicaster . set error handler ( new logging error handler ( ) ) ; this . initial multicaster . multicast event ( event ) ;,context have listener
only one geometry @$ let 's not wrap it in another node unless the node has <PLACE_HOLDER> .,if ( primitives . length == __num__ && children == null ) { spatial = primitives [ __num__ ] ; } else { node node = new node ( ) ; for ( geometry primitive : primitives ) { node . attach child ( primitive ) ; } spatial = node ; },node has one
where the code forgot the <PLACE_HOLDER> !,assert equals ( expected @$ format information . decode format information ( unmasked_test_format_info @$ masked_test_format_info ) ) ;,code forgot name
create all connectors before adding @$ so a broken connector does not leave the system half <PLACE_HOLDER>,materialized connector connector = new materialized connector ( catalog name @$ create connector ( catalog name @$ factory @$ properties ) ) ; connector handle resolver connector handle resolver = connector . get connector ( ) . get handle resolver ( ) . or else get ( factory . get connector factory ( ) :: get handle resolver ) ; check argument ( connector handle resolver != null @$ __str__ @$ factory ) ; materialized connector information schema connector = new materialized connector ( create information schema catalog name ( catalog name ) @$ new information schema connector ( catalog name . get catalog name ( ) @$ node manager @$ metadata manager @$ access control manager ) ) ; catalog name system id = create system tables,connector leave room
we can only set up the real icq test suites when the <PLACE_HOLDER>.properties file defines the two test <PLACE_HOLDER>,if ( icq test agent name != null ) { icq slick fixture . tester agent = new icq tester agent ( icq test agent name ) ; string icq test agent pwd = system . get property ( testing_impl_pwd_prop_name @$ null ) ; if ( icq slick fixture . tester agent . register ( icq test agent pwd ) ) { if ( ! icq slick fixture . online testing disabled ) { icq slick fixture . tester agent . set authorization required ( ) ; try { initialize tested contact list ( ) ; } catch ( exception ex ) { logger . error ( __str__ @$ ex ) ; } string offline msg body = __str__ + __str__ ; icq slick fixture . offline msg,file defines activities
wait a little bit to let the delete take <PLACE_HOLDER> .,thread . sleep ( __num__ ) ;,delete take place
note the api requires a source file <PLACE_HOLDER> because the html and xml formatters display a page of code annotated with coverage information . having the source files is not actually needed for generating the lcov report ...,visitor . visit bundle ( bundle coverage @$ new i source file locator ( ) { @ override public reader get source file ( string package name @$ string file name ) throws io exception { return null ; } @ override public int get tab width ( ) { return __num__ ; } } ) ;,source file indexor
class cast <PLACE_HOLDER>,try { set perms = new hash set ( ) ; perms . add ( new object ( ) ) ; view . set permissions ( perms ) ; throw new runtime exception ( __str__ ) ; } catch ( class cast exception x ) { },class cast exception
more complicated . vh 2 has a higher version @$ but has some <PLACE_HOLDER> that vh 1 does not have .,region version holder vh1 = new region version holder ( member ) ; region version holder vh2 = new region version holder ( member ) ; bit set bs1 = new bit set ( ) ; bs1 . set ( __num__ @$ __num__ ) ; bs1 . set ( __num__ @$ __num__ ) ; record versions ( vh1 @$ bs1 ) ; bit set bs2 = new bit set ( ) ; bs2 . set ( __num__ @$ __num__ ) ; bs2 . set ( __num__ @$ __num__ ) ; record versions ( vh2 @$ bs2 ) ;,vh has exceptions
before conf object is passed in @$ rm has already processed it and used rm specific <PLACE_HOLDER> to overwrite hadoop common ones . hence we just need to source hadoop.proxyuser <PLACE_HOLDER> here .,map < string @$ string > filter config = authentication filter initializer . get filter config map ( conf @$ config prefix ) ;,ones rm configurations
in order to allow programs to use a single <PLACE_HOLDER> as the display for multiple tabs @$ we will not change the visible compnent if the currently selected tab has a null <PLACE_HOLDER> . this is a bit dicey @$ as we do n't explicitly state we support this in the spec @$ but since programs are now depending on this @$ we 're,if ( selected component != null ) { if ( selected component != visible component && visible component != null ) { if ( swing utilities . find focus owner ( visible component ) != null ) { should change focus = true ; } } set visible component ( selected component ) ; } final rectangle bounds = tab pane . get bounds ( ) ; final int num children = tab pane . get component count ( ) ; if ( num children > __num__ ) { switch ( tab placement ) { case left : total tab width = calculate tab area width ( tab placement @$ run count @$ max tab width ) ; cx = insets . left + total tab width +,tab has component
only class object type can use property <PLACE_HOLDER>,if ( ! ( object type instanceof class object type ) ) { left declared mask = all set bit mask . get ( ) ; return ; },type use specific
middle of an existing chain download . create a <PLACE_HOLDER> of peers .,peer group . start ( ) ;,middle create lot
nb : if rule acts as a sub @$ do not include type <PLACE_HOLDER>,boolean sub head = head . get atom ( ) . is type ( ) ; if ( sub head ) { body . get atoms ( ) . stream ( ) . filter ( atomic :: is type ) . filter ( at -> at . get var name ( ) . equals ( head . get atom ( ) . get var name ( ) ) ) . for each ( all atoms :: remove ) ; } all atoms . add ( head . get atom ( ) ) ; return reasoner query factory . create ( all atoms ) ;,nb include parameters
if <PLACE_HOLDER>loading failed @$ other thread may have called <PLACE_HOLDER> loader which will null out <PLACE_HOLDER> @$ hence we check for it .,synchronized ( this ) { if ( image != null ) { if ( ( new state & width_flag ) == width_flag || width == __num__ ) { width = new width ; } if ( ( new state & height_flag ) == height_flag || height == __num__ ) { height = new height ; } } else { create text = true ; if ( ( new state & width_flag ) == width_flag ) { width = new width ; } if ( ( new state & height_flag ) == height_flag ) { height = new height ; } } state = state | new state ; state = ( state | loading_flag ) ^ loading_flag ; },which null image
singles have special <PLACE_HOLDER>,object [ ] [ ] should fail = { { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ } @$ { __str__ @$ parse exception . class } @$ { __str__ } @$ { __str__ } @$ { __str__ @$ parse exception . class } @$ { __str__ @$ parse exception . class } @$ { __str__ } @$,singles have handling
desired schema does not include virtual <PLACE_HOLDER> or partition <PLACE_HOLDER> .,type description result = type description . create struct ( ) ; for ( int i = __num__ ; i < schema evolution type descrs . size ( ) ; i ++ ) { result . add field ( schema evolution column names . get ( i ) @$ schema evolution type descrs . get ( i ) ) ; } return result ;,schema include columns
hmmm @$ did this member have a restart ? determine <PLACE_HOLDER> member dir might be a match for us,if ( ! baseline dir . exists ( ) ) { baseline dir = find baseline for this member ( member backup location dir . get parent ( ) @$ disk store ) ; },dir match which
thread a 1 gets <PLACE_HOLDER> a with thread a 1,if ( threada1 . in event loop ( ) ) { await ( arrival barrier ) ; return poola1 ; } else if ( threada2 . in event loop ( ) ) { await ( arrival barrier ) ; await ( release barrier ) ; return poola2 ; },1 gets pool
visiting the configuration page should n't change <PLACE_HOLDER>,html page pg = wc . go to ( __str__ ) ; j . submit ( pg . get form by name ( __str__ ) ) ; p = u . get property ( last granted authorities property . class ) ; assert authorities ( p @$ __str__ ) ; assert authorities ( u . impersonate ( ) @$ __str__ ) ;,page change authorities
string <PLACE_HOLDER> follows str index <PLACE_HOLDER>,int st off = sit off + numb strings * __num__ ;,table follows table
the test succeeds if we get the no class def found <PLACE_HOLDER> .,try { in . read object ( ) ; } catch ( no class def found error e ) { if ( e == ncdfe ) { system . err . println ( __str__ + e . to string ( ) ) ; } else { throw e ; } },def found error
semijoins may have created task level <PLACE_HOLDER> @$ examine those,connect terminal ops ( proc ctx . parse context ) ; boolean cycle free = false ; while ( ! cycle free ) { cycle free = true ; set < set < operator < ? > > > components = get components ( proc ctx ) ; for ( set < operator < ? > > component : components ) { if ( log . is debug enabled ( ) ) { log . debug ( __str__ ) ; for ( operator < ? > co : component ) { log . debug ( __str__ + co . get name ( ) + __str__ + co . get identifier ( ) ) ; } } if ( component . size ( ) != __num__ ) { log,semijoins created operators
should n't happen if this is only being called by a single thread . plumber.add should be swapping out <PLACE_HOLDER> before they fill up .,throw new ise ( e @$ __str__ ) ;,plumber.add swapping threads
assert that the total sockets quota has a reasonable <PLACE_HOLDER> .,assert true ( __str__ @$ ! open udp encap sockets . is empty ( ) ) ; assert true ( __str__ @$ open udp encap sockets . size ( ) < max_num_encap_sockets ) ;,quota has limit
this member has a base type that itself has multiple <PLACE_HOLDER> : we need to go deeper !,final type member tree node nested node = new type member tree node ( member ) ; member nodes . put ( member @$ nested node ) ; nested struct nodes . put ( member . get base type ( ) @$ nested node ) ; current node . add ( nested node ) ; create type nodes ( nested node @$ member . get base type ( ) ) ; break ; default : navi logger . warning ( __str__ @$ member . get base type ( ) . get category ( ) ) ; break ;,itself has members
also non colored cumulatives have <PLACE_HOLDER> !,if ( m has colored cumulative ) { used color = new color wrap ( themes . get color from attr ( m chart view . get context ( ) @$ m colors [ i - __num__ ] ) ) ; } else { if ( m chart type == stats . chart type . intervals ) { name = m chart view . get resources ( ) . get string ( r . string . stats_cumulative_percentage ) ; } },cumulatives have colors
if the dto is null @$ it is an indication that the user does not have <PLACE_HOLDER> . however @$ we do n't want to just throw an access denied exception because we would rather ensure that all of the appropriate actions are taken by the pluggable authorizer . as a result @$ we attempt to find the component as a processor and fall,if ( dto == null ) { authorizable authorizable ; try { authorizable = lookup . get processor ( entity . get id ( ) ) . get authorizable ( ) ; } catch ( final resource not found exception rnfe ) { authorizable = lookup . get controller service ( entity . get id ( ) ) . get authorizable ( ) ; } if ( require read ) { authorizable . authorize ( authorizer @$ request action . read @$ user ) ; } if ( require write ) { authorizable . authorize ( authorizer @$ request action . write @$ user ) ; } } else if ( affected componentdto . component_type_processor . equals ( dto . get reference type ( ) ) ) {,user have access
if there are remote deps @$ block on them fragment <PLACE_HOLDER>s indicating failure will throw an exception which will propagate out of handle received frag <PLACE_HOLDER> and cause procedure runner to do the right thing and cause rollback .,while ( ! check done receiving frag responses ( ) ) { fragment response message msg = poll for responses ( ) ; if ( trace log != null ) { final int batch idx = m_remote work . get current batch index ( ) ; trace log . add ( ( ) -> volt trace . end async ( __str__ @$ misc utils . hs id pair txn id to string ( m_mbox . geths id ( ) @$ msg . m_sourcehs id @$ txn id @$ batch idx ) @$ __str__ @$ byte . to string ( msg . get status code ( ) ) ) ) ; } boolean expected msg = handle received frag response ( msg ) ; if ( expected msg ),block received response
else the initial subsystem install is not complete ; messaging subsystem add will add a handler that calls add queue <PLACE_HOLDER>,sb . install ( ) ;,calls add configs
for distinct count @$ because multiple groups have same <PLACE_HOLDER> @$ so there is no guarantee on the order of groups @$ just check the <PLACE_HOLDER>,group by result distinct count group by result = distinct count group by result list . get ( i ) ; assert . assert equals ( distinct count group by result . get value ( ) @$ expected group index / ( num_groups / max_size_of_set ) + __num__ @$ error_message ) ;,groups have result
dom level <PLACE_HOLDER> : namespace uri is never empty string .,this . namespaceuri = namespaceuri ; if ( namespaceuri != null ) { this . namespaceuri = ( namespaceuri . length ( ) == __num__ ) ? null : namespaceuri ; } int colon1 = qname . index of ( __str__ ) ; int colon2 = qname . last index of ( __str__ ) ; owner document . check namespacewf ( qname @$ colon1 @$ colon2 ) ; if ( colon1 < __num__ ) { local name = qname ; if ( owner document . error checking ) { owner document . checkq name ( null @$ local name ) ; if ( qname . equals ( __str__ ) && ( namespaceuri == null || ! namespaceuri . equals ( namespace context . xmlns_uri ) ) || (,dom level 3
disable quota checks <PLACE_HOLDER> in standby .,dir . disable quota checks ( ) ; edit log tailer = new edit log tailer ( this @$ conf ) ; edit log tailer . start ( ) ; if ( ! is observer && standby should checkpoint ) { standby checkpointer = new standby checkpointer ( conf @$ this ) ; standby checkpointer . start ( ) ; },quota checks checkpoint
inspect contained <PLACE_HOLDER> to see if we need to migrate extras . we do n't promote clip data to the parent @$ since chooser activity will already start the picked item as the caller @$ and we ca n't combine the flags in a safe way .,if ( action_chooser . equals ( action ) ) { boolean migrated = false ; try { final intent intent = get parcelable extra ( extra_intent ) ; if ( intent != null ) { migrated |= intent . migrate extra stream to clip data ( ) ; } } catch ( class cast exception e ) { } try { final parcelable [ ] intents = get parcelable array extra ( extra_initial_intents ) ; if ( intents != null ) { for ( int i = __num__ ; i < intents . length ; i ++ ) { final intent intent = ( intent ) intents [ i ] ; if ( intent != null ) { migrated |= intent . migrate extra stream to clip data,inspect contained apps
the iterator already returned it 's load next batch <PLACE_HOLDER> @$ we must complete it exceptionally,current page . complete exceptionally ( t ) ; return ;,iterator returned instance
the root object can omit the surrounding <PLACE_HOLDER> . this token should be the first field 's key @$ or part of it @$ so put it back .,put back ( t ) ; missing curly = true ; result = parse object ( false ) ;,object omit quotes
internally @$ the code does n't permit nullable <PLACE_HOLDER>s @$ so we lazily initialize dummy instances if the developer did n't supply a real <PLACE_HOLDER> .,m on drag initiated listener = ( m on drag initiated listener != null ) ? m on drag initiated listener : new on drag initiated listener ( ) { @ override public boolean on drag initiated ( @ non null motion event e ) { return false ; } } ; m on item activated listener = ( m on item activated listener != null ) ? m on item activated listener : new on item activated listener < k > ( ) { @ override public boolean on item activated ( @ non null item details lookup . item details < k > item @$ @ non null motion event e ) { return false ; } } ; m on context click listener = (,developer supply listener
stream buffer size <PLACE_HOLDER> in model .,igfs cfg props . add ( __str__ ) ;,buffer size information
remove the stack trace so we do not pollute the build <PLACE_HOLDER> .,return exceptions . clear trace ( new file service exception ( ) ) ;,the build groups
if the list accepts the key events and the key event was a click @$ the text view gets the selected <PLACE_HOLDER> from the drop down as its content,switch ( key code ) { case key event . keycode_enter : case key event . keycode_dpad_center : case key event . keycode_tab : if ( event . has no modifiers ( ) ) { perform completion ( ) ; } return true ; },view gets item
now scheduler health records last <PLACE_HOLDER> allocated @$ aggregated allocation account will not be changed,assert . assert equals ( __num__ @$ sh . get allocation count ( ) . long value ( ) ) ; assert . assert equals ( resource . new instance ( __num__ * __num__ @$ __num__ ) @$ sh . get resources allocated ( ) ) ; assert . assert equals ( __num__ @$ sh . get aggregate allocation count ( ) . long value ( ) ) ; assert . assert equals ( __str__ @$ sh . get last allocation details ( ) . get node id ( ) . to string ( ) ) ; assert . assert equals ( __str__ @$ sh . get last allocation details ( ) . get queue ( ) ) ; task task_0_2 = new task ( application_0 @$ priority_0,health records resource
exception could occur if a symbol element is missing an important <PLACE_HOLDER> such as address or length,string message = e . get message ( ) ; if ( message == null ) { message = e . get class ( ) . get simple name ( ) ; } message = __str__ + message ; msg . error ( this @$ message @$ e ) ; throw new io exception ( message @$ e ) ;,element missing field
a list of all available logback log <PLACE_HOLDER> .,final list < string > log levels = arrays . as list ( level . all . to string ( ) @$ level . off . to string ( ) @$ level . error . to string ( ) @$ level . warn . to string ( ) @$ level . info . to string ( ) @$ level . debug . to string ( ) @$ level . trace . to string ( ) ) ; final argument parser parser = argument parsers . new argument parser ( __str__ ) . default help ( true ) . description ( __str__ + __str__ ) ; parser . add argument ( __str__ @$ __str__ ) . type ( string . class ) . nargs ( __str__ ) . required,list log levels
make sure the env supports bulk <PLACE_HOLDER> with generated ids ...,if ( ! supports bulk insert id generation ( integer versioned . class ) ) { skip log . report skip ( __str__ @$ __str__ ) ; return ; } session s = open session ( ) ; transaction t = s . begin transaction ( ) ; integer versioned entity = new integer versioned ( __str__ ) ; s . save ( entity ) ; s . create query ( __str__ ) . list ( ) ; t . commit ( ) ; s . close ( ) ; long initial id = entity . get id ( ) ; int initial version = entity . get version ( ) ; s = open session ( ) ; t = s . begin transaction ( ) ;,env supports inserts
no costs known . use the same <PLACE_HOLDER> as above on the heuristic costs,final long sampled = ( long ) ( heuristic_cost_base * __num__ ) ; costs . add heuristic network cost ( heuristic_cost_base + sampled ) ; costs . add heuristic disk cost ( __num__ * sampled ) ;,costs use value
single interface enabled <PLACE_HOLDER> .,string single interface enabled prop = util activator . get resources ( ) . get settings string ( single_window_interface_enabled ) ; boolean is enabled = false ; if ( single interface enabled prop != null ) is enabled = boolean . parse boolean ( single interface enabled prop ) ; else is enabled = boolean . parse boolean ( util activator . get resources ( ) . get settings string ( __str__ ) ) ;,interface enabled property
verify that a second call will only return <PLACE_HOLDER> .,m callback . clear ( ) ; final long [ ] [ ] new times1 = increase time ( times ) ; write to file ( m headline + uid lines ( m uids @$ new times1 ) ) ; m reader . read delta ( m callback ) ; for ( int i = __num__ ; i < m uids . length ; ++ i ) { m callback . verify ( m uids [ i ] @$ get active time ( new times1 [ i ] ) - get active time ( times [ i ] ) ) ; } m callback . verify no more interactions ( ) ;,call return deltas
add all the <PLACE_HOLDER>s with proper context add first <PLACE_HOLDER>,outputqueue = chain . create blocking queue ( ) ; chain . add mapper ( context @$ outputqueue @$ __num__ ) ;,mappers add mapper
safe to delegate since this reader does not alter the <PLACE_HOLDER>,return in . get reader cache helper ( ) ;,reader alter result
the text lines total height is larger than this view @$ snap <PLACE_HOLDER> to the top and bottom of the view .,if ( line1 height + line2 height > height ) { if ( child != m text line1 ) { vertical offset = height - line2 height ; } } else { vertical offset = ( height - line1 height - line2 height ) / __num__ ; if ( child == m text line2 ) { vertical offset += line1 height ; if ( m suggestion . has answer ( ) && m suggestion . get answer ( ) . get second line ( ) . has image ( ) ) { vertical offset += get resources ( ) . get dimension pixel offset ( r . dimen . omnibox_suggestion_answer_line2_vertical_spacing ) ; } } if ( line1 height != line2 height ) { vertical offset += ( line2,lines snap them
current dest has no distinct <PLACE_HOLDER> .,if ( current distinct keys . is empty ( ) ) { list < expr node desc > combined list = combine expr node lists ( target spray keys @$ target distinct keys ) ; if ( ! match expr lists ( combined list @$ current spray keys ) ) { continue ; } } else { if ( target distinct keys . is empty ( ) ) { list < expr node desc > combined list = combine expr node lists ( current spray keys @$ current distinct keys ) ; if ( ! match expr lists ( combined list @$ target spray keys ) ) { continue ; } else { new distinct key lists . remove ( i ) ; new spray key lists .,dest has keys
uri may contain <PLACE_HOLDER> @$ so make sure we check it too by converting ours @$ if necessary,string to match host = to match . get host ( ) ;,uri contain host
we set 0 this time as the last updated : can happen <PLACE_HOLDER> . when we use an old dynamo table,pm . set last updated ( __num__ ) ; metadata store ms = mock ( metadata store . class ) ; when ( ms . get ( path @$ false ) ) . then return ( pm ) ; i ttl time provider time provider = mock ( i ttl time provider . class ) ; when ( time provider . get now ( ) ) . then return ( __num__ ) ; when ( time provider . get metadata ttl ( ) ) . then return ( __num__ ) ;,updated happen i
the classic translator does not know this <PLACE_HOLDER>,return null ;,translator know method
we called close on the underlying socket above to make doubly sure all resources got released . we do n't finalize self in the case of overlain sockets @$ that 's a different object <PLACE_HOLDER> the gc will finalize separately .,super . finalize ( ) ;,gc finalize which
no ties top <PLACE_HOLDER>,arrays . fill ( hits @$ __num__ ) ; actual_label = __num__ ; pred_dist = new double [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ } ; update hits ( __num__ @$ actual_label @$ pred_dist @$ hits ) ; assert . assert true ( arrays . equals ( hits @$ new double [ ] { __num__ @$ __num__ @$ __num__ @$ __num__ } ) ) ;,ties top point
packing saves <PLACE_HOLDER>,long test packed date time = packed local date time . pack ( test date ) ;,packing saves time
assert that the tab layout did not update and add the new <PLACE_HOLDER>,assert not equals ( __str__ @$ new item count @$ tab layout . get tab count ( ) ) ;,layout update items
checks that a <PLACE_HOLDER>et contains a primary key @$ except when it contains only public <PLACE_HOLDER> .,if ( ! has primary key && ! contains only public key material ) { throw new general security exception ( __str__ ) ; },checks contains key
list result may contain continuation <PLACE_HOLDER>,if ( ( answer . status != ldap client . ldap_success ) || ( answer . referrals != null ) ) { process return code ( answer @$ name ) ; } return new ldap naming enumeration ( this @$ answer @$ name @$ cont ) ;,result contain key
motion events get the raw float <PLACE_HOLDER> :,final float x = is horizontal ? m requested drag distance : __num__ ; final float y = is horizontal ? __num__ : m requested drag distance ; m recycler view . scroll by ( offsetx @$ offsety ) ; add fake motion event ( time @$ motion event . action_move @$ x @$ y ) ; return true ;,events get values
computing the canonical path of a win 32 file should expose the true <PLACE_HOLDER> of filenames @$ rather than just using the input <PLACE_HOLDER>,file y = new file ( user dir @$ f . get path ( ) ) ; string ans = y . get path ( ) ; check ( ans @$ __str__ ) ; check ( ans @$ __str__ ) ; check ( ans @$ __str__ ) ;,file expose list
if a topic was not pulled in the previous run @$ use this.avg est avg millis as the estimated avg time to pull a <PLACE_HOLDER> in this run @$ which is the geometric mean of all topics whose avg times to pull a <PLACE_HOLDER> in the previous run are known .,this . avg est avg millis = geometric mean ( all est avg millis ) ;,times pull message
find all the methods with the specific name and which accept just 1 <PLACE_HOLDER> .,if ( methods == null || methods . is empty ( ) ) { methods = class reflection index util . find all methods ( reflection index @$ class index @$ name @$ __num__ ) ; },which accept parameter
sometimes the former <PLACE_HOLDER> and the newer <PLACE_HOLDER> are execute in same millisecond which causes the later <PLACE_HOLDER> to fail with obsolete version exception . add 5 ms sleep .,thread . sleep ( __num__ ) ;,which causes operation
initial method of data file <PLACE_HOLDER>,set property ( hsqldb_nio_data_file @$ true ) ;,method file type
table has <PLACE_HOLDER>,tbl name = __str__ ; table_params = new string builder ( ) ; table_params . append ( capabilities_key ) . append ( __str__ ) . append ( __str__ ) ; t props . put ( __str__ @$ tbl name ) ; t props . put ( __str__ @$ table_params . to string ( ) ) ; try { table = create table with capabilities ( t props ) ; log . info ( __str__ ) ; } catch ( exception e ) { log . info ( __str__ ) ; fail ( __str__ + e . get message ( ) + __str__ ) ; },table has capabilities
instrumentation can kill and relaunch even persistent <PLACE_HOLDER>,force stop package locked ( ii . target package @$ - __num__ @$ true @$ false @$ true @$ true @$ false @$ user id @$ __str__ ) ;,instrumentation kill resources
decode gif pixel <PLACE_HOLDER> .,datum = bits = count = first = top = pi = bi = __num__ ; for ( i = __num__ ; i < npix ; ) { if ( top == __num__ ) { if ( bits < code_size ) { if ( count == __num__ ) { count = read block ( ) ; if ( count <= __num__ ) { break ; } bi = __num__ ; } datum += ( ( ( int ) block [ bi ] ) & __num__ ) << bits ; bits += __num__ ; bi ++ ; count -- ; continue ; } code = datum & code_mask ; datum >>= code_size ; bits -= code_size ; if ( ( code > available ) || ( code == end_of_information,decode gif blocks
retrieve the replication configuration and verify that the configuration matches the <PLACE_HOLDER> we just set .,bucket replication configuration replication config = s3 client . get bucket replication configuration ( source bucket name ) ; replication rule rule = replication config . get rule ( __str__ ) ; system . out . println ( __str__ + rule . get destination config ( ) . get bucketarn ( ) ) ; system . out . println ( __str__ + rule . get priority ( ) ) ; system . out . println ( __str__ + rule . get status ( ) ) ;,configuration matches one
we found counter objects which imply <PLACE_HOLDER>,if ( ! verify unexpected values ( counters ) ) { success = false ; },which imply error
we need to make a copy here since get previous work unit <PLACE_HOLDER> returns immutable work unit <PLACE_HOLDER> for which add all is not supported,if ( state . get prop as boolean ( configuration keys . overwrite_configs_in_statestore @$ configuration keys . default_overwrite_configs_in_statestore ) ) { work unit state work unit state copy = new work unit state ( work unit state . get workunit ( ) @$ state ) ; work unit state copy . add all ( work unit state ) ; work unit state copy . override with ( state ) ; previous work unit states . add ( work unit state copy ) ; } else { previous work unit states . add ( work unit state ) ; },states returns state
read message length will throw stream corrupted <PLACE_HOLDER> if the marker bytes incorrect,int msg size = tcp transport . read message length ( new bytes array ( minimal header ) ) ; if ( msg size == - __num__ ) { socket . get output stream ( ) . flush ( ) ; } else { final byte [ ] buffer = new byte [ msg size ] ; input . read fully ( buffer ) ; int expected size = tcp header . marker_bytes_size + tcp header . message_length_size + msg size ; try ( bytes stream output output = new releasable bytes stream output ( expected size @$ big arrays ) ) { output . write ( minimal header ) ; output . write ( buffer ) ; consume network reads ( mock channel @$ output . bytes,length throw exception
although this is the same size as the <PLACE_HOLDER> that is likely already loaded @$ the lifecycle is different and interactions are on a different thread . thus to simplify @$ this source will decode its own <PLACE_HOLDER> .,bitmap preview = decode preview ( source @$ preview size ) ; if ( preview . get width ( ) <= gl_size_limit && preview . get height ( ) <= gl_size_limit ) { m preview = new bitmap texture ( preview ) ; } else { log . w ( tag @$ string . format ( __str__ + __str__ @$ m width @$ m height @$ preview . get width ( ) @$ preview . get height ( ) ) ) ; },source decode preview
check that iterator always contains all the <PLACE_HOLDER> .,try { int iter cnt = __num__ ; for ( int i = __num__ ; i < iter cnt ; i ++ ) { collection < integer > cp = new hash set < > ( original ) ; cp . remove all ( linked map . key set ( ) ) ; assert true ( __str__ + cp @$ cp . is empty ( ) ) ; } info ( __str__ + ( system . current time millis ( ) - start ) + __str__ ) ; } finally { run . set ( false ) ; fut . get ( ) ; },iterator contains data
if the app is honeycomb mr 1 or earlier @$ switch its async task <PLACE_HOLDER> to use the pool executor . normally @$ we use the serialized executor as the default . this has to happen in the main thread so the main looper is set right .,if ( data . app info . target sdk version <= android . os . build . version_codes . honeycomb_mr1 ) { async task . set default executor ( async task . thread_pool_executor ) ; },executor switch impl
only has 10 <PLACE_HOLDER> .,buffered writer . write ( string . format ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; buffered writer . write ( string . format ( __str__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ @$ __num__ ) ) ; buffered writer . close ( ) ;,only has rows
mini hs 2 cluster is up .. let it run until someone kills the <PLACE_HOLDER>,while ( true ) { thread . sleep ( __num__ ) ; },someone kills executor
flags @$ currently we never calculate the crc and if we dont calculate them cant keep orig <PLACE_HOLDER> . tags are not experimental and we never create extended header to keep things simple .,extended = false ; experimental = false ; footer = false ;,them keep values
this means that the embedded finder could not parse a <PLACE_HOLDER> .,if ( version == null ) { return null ; },finder parse response
this check prevent <PLACE_HOLDER> when removal already happened during finishing animation,if ( j < additions . size ( ) ) { additions . remove ( j ) ; } if ( additions . is empty ( ) ) { m additions list . remove ( additions ) ; },check prevent removal
test that it emits when time passed the time <PLACE_HOLDER>,consumer . accept ( value in global window ( new byte [ __num__ ] ) ) ;,time passed limit
calendar should have undefined <PLACE_HOLDER>,assert true ( calendar . get width ( ) < __num__ ) ; assert true ( calendar . get height ( ) < __num__ ) ;,calendar have dimensions
the main thread 's context class <PLACE_HOLDER> is set to the current thread 's context class <PLACE_HOLDER> which is a native image class <PLACE_HOLDER> . the class <PLACE_HOLDER> feature object replacer will unwrap the original app class <PLACE_HOLDER> from the native image class <PLACE_HOLDER> .,main thread = new thread ( main group @$ __str__ ) ; main thread . set daemon ( false ) ;,replacer unwrap name
<PLACE_HOLDER> and private classes must share <PLACE_HOLDER> to be visible to each other,if ( ! objects . equals ( root invoker class . get package ( ) @$ root target class . get package ( ) ) ) { return false ; },package share package
verify set <PLACE_HOLDER> on the stores as well .,for ( quota type quota type : new quota type [ ] { quota type . storage_space @$ quota type . get_throughput } ) { long new quota = __num__ + new random ( ) . next int ( __num__ ) ; admin client . quota mgmt ops . set quota ( new store name @$ quota type @$ new quota @$ service ) ; for ( integer node id : cluster . get node ids ( ) ) { long retrieved quota = get quota for node ( new store name @$ quota type @$ node id ) ; assert equals ( __str__ @$ new quota @$ retrieved quota . long value ( ) ) ; } },verify set quota
the variable reference violates a declared module <PLACE_HOLDER> .,if ( module graph . depends on ( var module @$ curr module ) ) { t . report ( n @$ violated_module_dep_error @$ curr module . get name ( ) @$ var module . get name ( ) @$ var name ) ; } else { t . report ( n @$ missing_module_dep_error @$ curr module . get name ( ) @$ var module . get name ( ) @$ var name ) ; },reference violates restriction
if the version is 0 then we are upgrading from a file format that did not know about periodic syncs . in that case do n't clear the <PLACE_HOLDER> since we want the default @$ which is a daily periodic sync . otherwise clear out this default <PLACE_HOLDER> since we will populate it later with the periodic sync descriptions that are read from the,if ( version > __num__ ) { authority . periodic syncs . clear ( ) ; } event log . write event ( __num__ @$ __str__ @$ - __num__ @$ __str__ + info . account + __str__ + authority name + __str__ + user id ) ;,case clear history
release app 2 's am <PLACE_HOLDER> on node 2 .,scheduler . handle ( app removed event2 ) ; assert equals ( __str__ @$ __num__ @$ queue1 . get am resource usage ( ) . get memory size ( ) ) ; scheduler . update ( ) ;,release app container
do not check the port number 0 because a user may want his or her <PLACE_HOLDER> to be bound on multiple arbitrary ports .,if ( p . local address ( ) . get port ( ) > __num__ ) { for ( int i = __num__ ; i < distinct ports . size ( ) ; i ++ ) { final server port port = distinct ports . get ( i ) ; if ( port . local address ( ) . equals ( p . local address ( ) ) ) { final server port merged = new server port ( port . local address ( ) @$ sets . union ( port . protocols ( ) @$ p . protocols ( ) ) ) ; distinct ports . set ( i @$ merged ) ; found = true ; break ; } } } if ( ! found ),user want endpoints
need to breakdown commandline into parts @$ as spaces in command line will cause <PLACE_HOLDER> .,list < string > exec commands = split and unescape command line ( cmd line ) ; system . out . printf ( __str__ @$ cmd line ) ; system . out . printf ( __str__ @$ jetty home dir . get absolute path ( ) ) ; pb cmd = new process builder ( exec commands ) ; pid = pb cmd . start ( ) ; console parser parser = new console parser ( ) ; list < string [ ] > jmx list = parser . new pattern ( __str__ @$ __num__ ) ; list < string [ ] > conn list = parser . new pattern ( __str__ @$ __num__ ) ;,spaces cause errors
expect errors . only create <PLACE_HOLDER> of two necessary configurability rules :,scratch . file ( __str__ @$ __str__ @$ __str__ @$ __str__ ) ; write hello rules ( true ) ;,errors create one
for ssl the exception may not come since the server can close <PLACE_HOLDER> before handshake message is sent from client . however exception should come in any region operations .,if ( gen . class code ( ) . equals ( credential generator . class code . ssl ) ) { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ no_exception ) ) ; client2 . invoke ( ( ) -> do puts ( __num__ @$ other_exception ) ) ; } else { client2 . invoke ( ( ) -> create cache client ( null @$ null @$ null @$ port1 @$ port2 @$ __num__ @$ multi user @$ authreq_exception ) ) ; },server close socket
record the sequence of seeks and reads which trigger a <PLACE_HOLDER> .,int [ ] seeks = new int [ __num__ ] ; int [ ] reads = new int [ __num__ ] ; try ( fs data input stream stm = get file system ( ) . open ( random seek file ) ) { for ( int i = __num__ ; i < limit ; i ++ ) { int seek off = r . next int ( buf . length ) ; int to read = r . next int ( math . min ( buf . length - seek off @$ __num__ ) ) ; seeks [ i % seeks . length ] = seek off ; reads [ i % reads . length ] = to read ; verify read ( stm @$ buf @$,which trigger split
source produces some different <PLACE_HOLDER> .,if ( testing mode . equals ( mode_source_based ) ) { return ; },source produces output
the activity that we are finishing may be over the lock screen . in this case @$ we do not want to consider activities that can not be shown on the lock screen as running and should proceed with finishing the activity if there is no valid next top running activity . note that if this finishing activity is floating <PLACE_HOLDER> @$ we do,final activity display display = get display ( ) ; final activity record next = display . top running activity ( true ) ;,activity floating penalty
shared libraries get a null <PLACE_HOLDER> : this has the side effect of having canonicalized shared libraries using application loaders cache @$ which is the behavior we want .,return application loaders . get default ( ) . get shared library class loader with shared libraries ( jars @$ m application info . target sdk version @$ is bundled app @$ library search path @$ library permitted path @$ null @$ null @$ shared libraries ) ;,libraries get classloader
my sql connector currently does not support <PLACE_HOLDER> on table,assert query fails ( __str__ @$ __str__ ) ;,connector support comment
let 's actually create <PLACE_HOLDER> on non affinity server .,srv3 . cache ( query utils . create table cache name ( query utils . dflt_schema @$ __str__ ) ) ;,let create table
parent name only makes <PLACE_HOLDER> when there is a single obvious parent,if ( refed parent names . size ( ) == __num__ ) { logger . warn ( __str__ + __str__ @$ composed schema . get name ( ) ) ; return refed parent names . get ( __num__ ) ; } return null ;,name makes sense
cleanup flavor c : the user does not want any buffered <PLACE_HOLDER> to persist between panes .,if ( should discard ) { reduce fn . clear state ( renamed context ) ; },user want context
template location . this is the location which templates will be read from . the generator will use the resource <PLACE_HOLDER> to attempt to read the templates .,embedded template dir = template dir = template directory ;,generator use stream
frequency with which to collect <PLACE_HOLDER> for later plotting,int plot everyn minibatches = __num__ ;,which collect data
